{"pr_number": 8496, "pr_title": "KAFKA-9748: Add Streams eos-beta integration test", "pr_createdAt": "2020-04-16T03:52:52Z", "pr_url": "https://github.com/apache/kafka/pull/8496", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY5MzEwOQ==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409693109", "bodyText": "nit: we could define this transition list in a variable to be reused.", "author": "abbccdda", "createdAt": "2020-04-16T16:33:11Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - crash only: the rebalance should result in a commit\n+        // 6.  write 5 record per input topic partition and verify that the result was committed\n+        // 7.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - one transaction is base on a task producer; the other transaction is based on a thread producer\n+        // 8.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only: verify that the stopped client did commit its pending transaction during shutdown\n+        //      - the first client will still have an pending transaction\n+        // 9.  restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        // 10. write 5 record per input topic partition and verify that the result was committed\n+\n+        try (final KafkaStreams streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            final KafkaStreams streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA)) {\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+            streams1Alpha.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+            streams2Alpha.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+            streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+            // phase 1:\n+            final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient1;\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                    )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not startup on time.\"\n+            );\n+            stateTransitions1.clear();\n+            errorInjected = null;\n+\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY5MzU2Mw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409693563", "bodyText": "What about having separate wait condition, or at least have a way to detect which instance gets stuck?", "author": "abbccdda", "createdAt": "2020-04-16T16:33:54Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - crash only: the rebalance should result in a commit\n+        // 6.  write 5 record per input topic partition and verify that the result was committed\n+        // 7.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - one transaction is base on a task producer; the other transaction is based on a thread producer\n+        // 8.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only: verify that the stopped client did commit its pending transaction during shutdown\n+        //      - the first client will still have an pending transaction\n+        // 9.  restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        // 10. write 5 record per input topic partition and verify that the result was committed\n+\n+        try (final KafkaStreams streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            final KafkaStreams streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA)) {\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+            streams1Alpha.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+            streams2Alpha.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+            streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+            // phase 1:\n+            final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient1;\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                    )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not startup on time.\"\n+            );\n+            stateTransitions1.clear();\n+            errorInjected = null;\n+\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY5NjQxOA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409696418", "bodyText": "Could we define 10 as a constant COMMIT_MARKER value, so that people understand that it has a special meaning to indicate a commit request?", "author": "abbccdda", "createdAt": "2020-04-16T16:38:21Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - crash only: the rebalance should result in a commit\n+        // 6.  write 5 record per input topic partition and verify that the result was committed\n+        // 7.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - one transaction is base on a task producer; the other transaction is based on a thread producer\n+        // 8.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only: verify that the stopped client did commit its pending transaction during shutdown\n+        //      - the first client will still have an pending transaction\n+        // 9.  restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        // 10. write 5 record per input topic partition and verify that the result was committed\n+\n+        try (final KafkaStreams streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            final KafkaStreams streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA)) {\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+            streams1Alpha.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+            streams2Alpha.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+            streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+            // phase 1:\n+            final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient1;\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                    )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not startup on time.\"\n+            );\n+            stateTransitions1.clear();\n+            errorInjected = null;\n+\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+\n+\n+            // phase 2:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT\n+            // p-1: 10 records + COMMIT\n+            final List<KeyValue<Long, Long>> committedInputDataBeforeUpgrade = prepareData(0L, 10L, 0L, 1L);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg5MjgxMQ==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409892811", "bodyText": "Not sure if this buys much? The test is complex and one needs to read the comments anyway. We also have a commit marker after 20 and 30 messages and I think using\nprepareData(15L, 2 * COMMIT_MARKER, 0L, 1L);\n\ndoes not improve readablility?", "author": "mjsax", "createdAt": "2020-04-16T22:49:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY5NjQxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY5Nzc3MA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409697770", "bodyText": "Could we just use a boolean flag as parameter to determine whether to only read committed data?", "author": "abbccdda", "createdAt": "2020-04-16T16:40:22Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - crash only: the rebalance should result in a commit\n+        // 6.  write 5 record per input topic partition and verify that the result was committed\n+        // 7.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - one transaction is base on a task producer; the other transaction is based on a thread producer\n+        // 8.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only: verify that the stopped client did commit its pending transaction during shutdown\n+        //      - the first client will still have an pending transaction\n+        // 9.  restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        // 10. write 5 record per input topic partition and verify that the result was committed\n+\n+        try (final KafkaStreams streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            final KafkaStreams streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA)) {\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+            streams1Alpha.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+            streams2Alpha.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+            streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+            // phase 1:\n+            final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient1;\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                    )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not startup on time.\"\n+            );\n+            stateTransitions1.clear();\n+            errorInjected = null;\n+\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+\n+\n+            // phase 2:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT\n+            // p-1: 10 records + COMMIT\n+            final List<KeyValue<Long, Long>> committedInputDataBeforeUpgrade = prepareData(0L, 10L, 0L, 1L);\n+\n+            final List<KeyValue<Long, Long>> allData = new ArrayList<>(committedInputDataBeforeUpgrade);\n+\n+            writeInputData(committedInputDataBeforeUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\"\n+            );\n+\n+            final List<KeyValue<Long, Long>> committedOutputDataBeforeUpgrade = readResult(committedInputDataBeforeUpgrade.size(), CONSUMER_GROUP_ID);\n+            checkResultPerKey(committedOutputDataBeforeUpgrade, computeExpectedResult(committedInputDataBeforeUpgrade, new HashMap<>()));\n+\n+            // capture committed state for later\n+            final ReadOnlyKeyValueStore<Long, Long> storeOneBeforeUpgrade =\n+                streams1Alpha.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            final Map<Long, Long> currentStateOne = new HashMap<>();\n+            final long keyFilterFirstUpgrade;\n+            try (final KeyValueIterator<Long, Long> it = storeOneBeforeUpgrade.all()) {\n+                keyFilterFirstUpgrade = it.peekNextKey();\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateOne.put(row.key, row.value);\n+                }\n+            }\n+            final ReadOnlyKeyValueStore<Long, Long> storeTwoBeforeUpgrade =\n+                streams2Alpha.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            final Map<Long, Long> currentStateTwo = new HashMap<>();\n+            try (final KeyValueIterator<Long, Long> it = storeTwoBeforeUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateTwo.put(row.key, row.value);\n+                }\n+            }\n+\n+            // phase 3:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records (pending)\n+            // p-1: 10 records + COMMIT + 5 records (pending)\n+            stateTransitions2.clear();\n+            final AtomicBoolean errorInjectedClient2 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient2;\n+            if (injectError) {\n+                errorInjectedClient1.set(true);\n+            }\n+            final List<KeyValue<Long, Long>> uncommittedInputDataBeforeUpgrade = prepareData(10L, 15L, 0L, 1L);\n+            allData.addAll(uncommittedInputDataBeforeUpgrade);\n+\n+            writeInputData(uncommittedInputDataBeforeUpgrade);\n+\n+            final List<KeyValue<Long, Long>> allOutputDataBeforeUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataBeforeUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            // phase 4:\n+            // expected end state per output partition:\n+            // stop case:\n+            //   p-0: 10 records + COMMIT + 5 records + COMMIT\n+            //   p-1: 10 records + COMMIT + 5 records (pending)\n+            // crash case:\n+            //   p-0: 10 records + COMMIT + 5 records (pending)\n+            //   p-1: 10 records + COMMIT + 5 records (pending)\n+            if (!injectError) {\n+                streams1Alpha.close();\n+            }\n+            waitForCondition(\n+                () -> stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not stabilize on time.\"\n+            );\n+            errorInjected = null;\n+\n+            final List<KeyValue<Long, Long>> allOutputDataAfterFirstUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataAfterFirstUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataAfterFirstUpgrade =\n+                    uncommittedInputDataBeforeUpgrade.stream().filter(pair -> pair.key == keyFilterFirstUpgrade).collect(Collectors.toList());\n+\n+                final List<KeyValue<Long, Long>> committedOutputDataAfterFirstUpgrade = readResult(committedInputDataAfterFirstUpgrade.size(), CONSUMER_GROUP_ID);\n+                checkResultPerKey(committedOutputDataAfterFirstUpgrade, computeExpectedResult(committedInputDataAfterFirstUpgrade, currentStateOne));\n+            }\n+\n+            // phase 5:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT\n+            // p-1: 10 records + COMMIT + 5 records (+ maybe commit if tasks would have been switch; we don't care and don't verify)\n+            commitRequested.set(0);\n+            stateTransitions1.clear();\n+            stateTransitions2.clear();\n+            streams1Beta.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+\n+            // capture committed state for later\n+            final Map<Long, Long> currentStateAfterFirstUpgrade = new HashMap<>(currentStateTwo);\n+            final ReadOnlyKeyValueStore<Long, Long> storeOneAfterUpgrade =\n+                streams1Beta.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            try (final KeyValueIterator<Long, Long> it = storeOneAfterUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    if (row.key == keyFilterFirstUpgrade) {\n+                        currentStateAfterFirstUpgrade.put(row.key, row.value);\n+                    }\n+                }\n+            }\n+            try (final KeyValueIterator<Long, Long> it = storeTwoBeforeUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    if (row.key == keyFilterFirstUpgrade) {\n+                        currentStateAfterFirstUpgrade.put(row.key, row.value);\n+                    }\n+                }\n+            }\n+\n+            // phase 6:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT\n+            // p-1: 10 records + COMMIT + 5 records + (maybe COMMIT) + 5 records + COMMIT\n+            final List<KeyValue<Long, Long>> committedInputDataDuringUpgrade = prepareData(15L, 20L, 0L, 1L);\n+            allData.addAll(committedInputDataDuringUpgrade);\n+\n+            writeInputData(committedInputDataDuringUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\");\n+\n+            final List<KeyValue<Long, Long>> allCommittedInputDataDuringUpgrade = new ArrayList<>();\n+            final Map<Long, Long> currentStateBeforeSecondUpgrade = new HashMap<>();\n+            if (!injectError) {\n+                allCommittedInputDataDuringUpgrade.addAll(\n+                    uncommittedInputDataBeforeUpgrade.stream().filter(pair -> pair.key != keyFilterFirstUpgrade).collect(Collectors.toList())\n+                );\n+                currentStateBeforeSecondUpgrade.putAll(currentStateAfterFirstUpgrade);\n+            } else {\n+                allCommittedInputDataDuringUpgrade.addAll(uncommittedInputDataBeforeUpgrade);\n+                currentStateBeforeSecondUpgrade.putAll(currentStateOne);\n+                currentStateBeforeSecondUpgrade.putAll(currentStateTwo);\n+            }\n+            allCommittedInputDataDuringUpgrade.addAll(committedInputDataDuringUpgrade);\n+\n+            final List<KeyValue<Long, Long>> committedOutputDataBeforeSecondUpgrade = readResult(allCommittedInputDataDuringUpgrade.size(), CONSUMER_GROUP_ID);\n+            checkResultPerKey(committedOutputDataBeforeSecondUpgrade, computeExpectedResult(allCommittedInputDataDuringUpgrade, currentStateBeforeSecondUpgrade));\n+\n+            // capture committed state for later\n+            currentStateOne.clear();\n+            try (final KeyValueIterator<Long, Long> it = storeOneAfterUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateOne.put(row.key, row.value);\n+                }\n+            }\n+            currentStateTwo.clear();\n+            final long keyFilterSecondUpgrade;\n+            try (final KeyValueIterator<Long, Long> it = storeTwoBeforeUpgrade.all()) {\n+                keyFilterSecondUpgrade = it.peekNextKey();\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateTwo.put(row.key, row.value);\n+                }\n+            }\n+\n+            // phase 7:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT + 5 records (pending)\n+            // p-1: 10 records + COMMIT + 5 records + 5 records + COMMIT  + 5 records (pending)\n+            stateTransitions1.clear();\n+            if (injectError) {\n+                errorInjectedClient2.set(true);\n+            }\n+            final List<KeyValue<Long, Long>> uncommittedInputDataDuringUpgrade = prepareData(20L, 25L, 0L, 1L);\n+            allData.addAll(uncommittedInputDataDuringUpgrade);\n+\n+            writeInputData(uncommittedInputDataDuringUpgrade);\n+\n+            final List<KeyValue<Long, Long>> allOutputDataBeforeSecondUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataBeforeSecondUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            // phase 8:\n+            // expected end state per output partition:\n+            // stop case:\n+            //   p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT + 5 records (pending)\n+            //   p-1: 10 records + COMMIT + 5 records + 5 records + COMMIT  + 5 records + COMMIT\n+            // crash case:\n+            //   p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT + 5 records (pending)\n+            //   p-1: 10 records + COMMIT + 5 records + 5 records + COMMIT  + 5 records (pending)\n+            if (!injectError) {\n+                streams2Alpha.close();\n+            }\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                        KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                        KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not stabilize on time.\"\n+            );\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataAfterSecondUpgrade =\n+                    uncommittedInputDataDuringUpgrade.stream().filter(pair -> pair.key == keyFilterSecondUpgrade).collect(Collectors.toList());\n+\n+                final List<KeyValue<Long, Long>> committedOutputDataAfterSecondUpgrade = readResult(committedInputDataAfterSecondUpgrade.size(), CONSUMER_GROUP_ID);\n+                checkResultPerKey(committedOutputDataAfterSecondUpgrade, computeExpectedResult(committedInputDataAfterSecondUpgrade, currentStateTwo));\n+            }\n+\n+            final List<KeyValue<Long, Long>> allOutputDataAfterSecondUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataAfterSecondUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            // phase 9:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT + 5 records (+ maybe commit if tasks would have been switch; we don't care and don't verify)\n+            // p-1: 10 records + COMMIT + 5 records + 5 records + COMMIT  + 5 records + COMMIT\n+            commitRequested.set(0);\n+            stateTransitions1.clear();\n+            stateTransitions2.clear();\n+            streams2Beta.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+\n+            // capture committed state\n+            final Map<Long, Long> currentStateAfterSecondUpgrade = new HashMap<>(currentStateOne);\n+            try (final KeyValueIterator<Long, Long> it = storeOneAfterUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    if (row.key == keyFilterSecondUpgrade) {\n+                        currentStateAfterSecondUpgrade.put(row.key, row.value);\n+                    }\n+                }\n+            }\n+            final ReadOnlyKeyValueStore<Long, Long> storeTwoAfterUpgrade =\n+                streams2Beta.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            try (final KeyValueIterator<Long, Long> it = storeTwoAfterUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    if (row.key == keyFilterSecondUpgrade) {\n+                        currentStateAfterSecondUpgrade.put(row.key, row.value);\n+                    }\n+                }\n+            }\n+\n+            // phase 10:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT + 5 records + 5 records + COMMIT\n+            // p-1: 10 records + COMMIT + 5 records + 5 records + COMMIT  + 5 records + COMMIT + 5 records + COMMIT\n+            final List<KeyValue<Long, Long>> committedInputDataAfterUpgrade = prepareData(25L, 30L, 0L, 1L);\n+            allData.addAll(committedInputDataAfterUpgrade);\n+\n+            final List<KeyValue<Long, Long>> allCommittedInputDataAfterUpgrade = new ArrayList<>();\n+            final Map<Long, Long> lastCommittedState;\n+            if (!injectError) {\n+                allCommittedInputDataAfterUpgrade.addAll(\n+                    uncommittedInputDataDuringUpgrade.stream().filter(pair -> pair.key != keyFilterSecondUpgrade).collect(Collectors.toList())\n+                );\n+                lastCommittedState = currentStateAfterSecondUpgrade;\n+            } else {\n+                allCommittedInputDataAfterUpgrade.addAll(uncommittedInputDataDuringUpgrade);\n+                lastCommittedState = currentStateBeforeSecondUpgrade;\n+            }\n+            allCommittedInputDataAfterUpgrade.addAll(committedInputDataAfterUpgrade);\n+\n+            writeInputData(committedInputDataAfterUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\");\n+\n+            final List<KeyValue<Long, Long>> committedOutputDataAfterUpgrade = readResult(allCommittedInputDataAfterUpgrade.size(), CONSUMER_GROUP_ID);\n+            checkResultPerKey(committedOutputDataAfterUpgrade, computeExpectedResult(allCommittedInputDataAfterUpgrade, lastCommittedState));\n+        }\n+    }\n+\n+    private KafkaStreams getKafkaStreams(final String appDir,\n+                                         final String processingGuarantee) {\n+        commitRequested = new AtomicInteger(0);\n+        final StreamsBuilder builder = new StreamsBuilder();\n+\n+        final String[] storeNames = new String[] {storeName};\n+        final StoreBuilder<KeyValueStore<Long, Long>> storeBuilder = Stores\n+            .keyValueStoreBuilder(Stores.persistentKeyValueStore(storeName), Serdes.Long(), Serdes.Long())\n+            .withCachingEnabled();\n+\n+        builder.addStateStore(storeBuilder);\n+\n+        final KStream<Long, Long> input = builder.stream(MULTI_PARTITION_INPUT_TOPIC);\n+        input.transform(new TransformerSupplier<Long, Long, KeyValue<Long, Long>>() {\n+            @SuppressWarnings(\"unchecked\")\n+            @Override\n+            public Transformer<Long, Long, KeyValue<Long, Long>> get() {\n+                return new Transformer<Long, Long, KeyValue<Long, Long>>() {\n+                    ProcessorContext<Object, Object> context;\n+                    KeyValueStore<Long, Long> state = null;\n+                    AtomicBoolean crash;\n+\n+                    @Override\n+                    public void init(final ProcessorContext<Object, Object> context) {\n+                        this.context = context;\n+                        state = (KeyValueStore<Long, Long>) context.getStateStore(storeName);\n+                        crash = errorInjected;\n+                    }\n+\n+                    @Override\n+                    public KeyValue<Long, Long> transform(final Long key, final Long value) {\n+                        if ((value + 1) % 10 == 0) {\n+                            context.commit();\n+                            commitRequested.incrementAndGet();\n+                        }\n+\n+                        Long sum = state.get(key);\n+                        if (sum == null) {\n+                            sum = value;\n+                        } else {\n+                            sum += value;\n+                        }\n+                        state.put(key, sum);\n+                        state.flush();\n+\n+                        if (crash != null && crash.compareAndSet(true, false)) {\n+                            // only tries to fail once on one of the task\n+                            throw new RuntimeException(\"Injected test exception.\");\n+                        }\n+\n+                        return new KeyValue<>(key, state.get(key));\n+                    }\n+\n+                    @Override\n+                    public void close() { }\n+                };\n+            } }, storeNames)\n+            .to(MULTI_PARTITION_OUTPUT_TOPIC);\n+\n+        final Properties properties = new Properties();\n+        properties.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, processingGuarantee);\n+        properties.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, Long.MAX_VALUE);\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.METADATA_MAX_AGE_CONFIG), \"1000\");\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), \"earliest\");\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG), 5 * 1000);\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG), 5 * 1000 - 1);\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG), MAX_POLL_INTERVAL_MS);\n+        properties.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+        properties.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath() + File.separator + appDir);\n+\n+        final Properties config = StreamsTestUtils.getStreamsConfig(\n+            applicationId,\n+            CLUSTER.bootstrapServers(),\n+            Serdes.LongSerde.class.getName(),\n+            Serdes.LongSerde.class.getName(),\n+            properties\n+        );\n+\n+        final KafkaStreams streams = new KafkaStreams(builder.build(), config);\n+\n+        streams.setUncaughtExceptionHandler((t, e) -> {\n+            if (uncaughtException != null) {\n+                e.printStackTrace(System.err);\n+                fail(\"Should only get one uncaught exception from Streams.\");\n+            }\n+            uncaughtException = e;\n+        });\n+\n+        return streams;\n+    }\n+\n+    private List<KeyValue<Long, Long>> prepareData(final long fromInclusive,\n+                                                   final long toExclusive,\n+                                                   final Long... keys) {\n+        final List<KeyValue<Long, Long>> data = new ArrayList<>();\n+\n+        for (final Long k : keys) {\n+            for (long v = fromInclusive; v < toExclusive; ++v) {\n+                data.add(new KeyValue<>(k, v));\n+            }\n+        }\n+\n+        return data;\n+    }\n+\n+    private void writeInputData(final List<KeyValue<Long, Long>> records) throws Exception {\n+        IntegrationTestUtils.produceKeyValuesSynchronously(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            records,\n+            TestUtils.producerConfig(CLUSTER.bootstrapServers(), LongSerializer.class, LongSerializer.class),\n+            CLUSTER.time\n+        );\n+    }\n+\n+    private List<KeyValue<Long, Long>> readResult(final int numberOfRecords,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4MzYxNw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409883617", "bodyText": "+1 It seems we do not need the actual groupId here, just a boolean flag.", "author": "guozhangwang", "createdAt": "2020-04-16T22:24:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY5Nzc3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY5ODczOA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409698738", "bodyText": "Could we pass a Collections.emptyMap() here instead?", "author": "abbccdda", "createdAt": "2020-04-16T16:41:53Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - crash only: the rebalance should result in a commit\n+        // 6.  write 5 record per input topic partition and verify that the result was committed\n+        // 7.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - one transaction is base on a task producer; the other transaction is based on a thread producer\n+        // 8.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only: verify that the stopped client did commit its pending transaction during shutdown\n+        //      - the first client will still have an pending transaction\n+        // 9.  restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        // 10. write 5 record per input topic partition and verify that the result was committed\n+\n+        try (final KafkaStreams streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            final KafkaStreams streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA)) {\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+            streams1Alpha.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+            streams2Alpha.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+            streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+            // phase 1:\n+            final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient1;\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                    )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not startup on time.\"\n+            );\n+            stateTransitions1.clear();\n+            errorInjected = null;\n+\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+\n+\n+            // phase 2:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT\n+            // p-1: 10 records + COMMIT\n+            final List<KeyValue<Long, Long>> committedInputDataBeforeUpgrade = prepareData(0L, 10L, 0L, 1L);\n+\n+            final List<KeyValue<Long, Long>> allData = new ArrayList<>(committedInputDataBeforeUpgrade);\n+\n+            writeInputData(committedInputDataBeforeUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\"\n+            );\n+\n+            final List<KeyValue<Long, Long>> committedOutputDataBeforeUpgrade = readResult(committedInputDataBeforeUpgrade.size(), CONSUMER_GROUP_ID);\n+            checkResultPerKey(committedOutputDataBeforeUpgrade, computeExpectedResult(committedInputDataBeforeUpgrade, new HashMap<>()));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg5NDA1Mg==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409894052", "bodyText": "That does not work, because computeExpectedResult will modify the passed in HashMap but Collections.emptyMap() returns an immutable map.", "author": "mjsax", "createdAt": "2020-04-16T22:52:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY5ODczOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTY5OTE0Mw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409699143", "bodyText": "nit: s/state/states", "author": "abbccdda", "createdAt": "2020-04-16T16:42:26Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - crash only: the rebalance should result in a commit\n+        // 6.  write 5 record per input topic partition and verify that the result was committed\n+        // 7.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - one transaction is base on a task producer; the other transaction is based on a thread producer\n+        // 8.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only: verify that the stopped client did commit its pending transaction during shutdown\n+        //      - the first client will still have an pending transaction\n+        // 9.  restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        // 10. write 5 record per input topic partition and verify that the result was committed\n+\n+        try (final KafkaStreams streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            final KafkaStreams streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA)) {\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+            streams1Alpha.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+            streams2Alpha.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+            streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+            // phase 1:\n+            final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient1;\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                    )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not startup on time.\"\n+            );\n+            stateTransitions1.clear();\n+            errorInjected = null;\n+\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+\n+\n+            // phase 2:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT\n+            // p-1: 10 records + COMMIT\n+            final List<KeyValue<Long, Long>> committedInputDataBeforeUpgrade = prepareData(0L, 10L, 0L, 1L);\n+\n+            final List<KeyValue<Long, Long>> allData = new ArrayList<>(committedInputDataBeforeUpgrade);\n+\n+            writeInputData(committedInputDataBeforeUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\"\n+            );\n+\n+            final List<KeyValue<Long, Long>> committedOutputDataBeforeUpgrade = readResult(committedInputDataBeforeUpgrade.size(), CONSUMER_GROUP_ID);\n+            checkResultPerKey(committedOutputDataBeforeUpgrade, computeExpectedResult(committedInputDataBeforeUpgrade, new HashMap<>()));\n+\n+            // capture committed state for later", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTcwODM1OQ==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409708359", "bodyText": "nit: extra space after second COMMIT", "author": "abbccdda", "createdAt": "2020-04-16T16:57:00Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - crash only: the rebalance should result in a commit\n+        // 6.  write 5 record per input topic partition and verify that the result was committed\n+        // 7.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - one transaction is base on a task producer; the other transaction is based on a thread producer\n+        // 8.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only: verify that the stopped client did commit its pending transaction during shutdown\n+        //      - the first client will still have an pending transaction\n+        // 9.  restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        // 10. write 5 record per input topic partition and verify that the result was committed\n+\n+        try (final KafkaStreams streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            final KafkaStreams streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA)) {\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+            streams1Alpha.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+            streams2Alpha.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+            streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+            // phase 1:\n+            final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient1;\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                    )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not startup on time.\"\n+            );\n+            stateTransitions1.clear();\n+            errorInjected = null;\n+\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+\n+\n+            // phase 2:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT\n+            // p-1: 10 records + COMMIT\n+            final List<KeyValue<Long, Long>> committedInputDataBeforeUpgrade = prepareData(0L, 10L, 0L, 1L);\n+\n+            final List<KeyValue<Long, Long>> allData = new ArrayList<>(committedInputDataBeforeUpgrade);\n+\n+            writeInputData(committedInputDataBeforeUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\"\n+            );\n+\n+            final List<KeyValue<Long, Long>> committedOutputDataBeforeUpgrade = readResult(committedInputDataBeforeUpgrade.size(), CONSUMER_GROUP_ID);\n+            checkResultPerKey(committedOutputDataBeforeUpgrade, computeExpectedResult(committedInputDataBeforeUpgrade, new HashMap<>()));\n+\n+            // capture committed state for later\n+            final ReadOnlyKeyValueStore<Long, Long> storeOneBeforeUpgrade =\n+                streams1Alpha.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            final Map<Long, Long> currentStateOne = new HashMap<>();\n+            final long keyFilterFirstUpgrade;\n+            try (final KeyValueIterator<Long, Long> it = storeOneBeforeUpgrade.all()) {\n+                keyFilterFirstUpgrade = it.peekNextKey();\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateOne.put(row.key, row.value);\n+                }\n+            }\n+            final ReadOnlyKeyValueStore<Long, Long> storeTwoBeforeUpgrade =\n+                streams2Alpha.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            final Map<Long, Long> currentStateTwo = new HashMap<>();\n+            try (final KeyValueIterator<Long, Long> it = storeTwoBeforeUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateTwo.put(row.key, row.value);\n+                }\n+            }\n+\n+            // phase 3:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records (pending)\n+            // p-1: 10 records + COMMIT + 5 records (pending)\n+            stateTransitions2.clear();\n+            final AtomicBoolean errorInjectedClient2 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient2;\n+            if (injectError) {\n+                errorInjectedClient1.set(true);\n+            }\n+            final List<KeyValue<Long, Long>> uncommittedInputDataBeforeUpgrade = prepareData(10L, 15L, 0L, 1L);\n+            allData.addAll(uncommittedInputDataBeforeUpgrade);\n+\n+            writeInputData(uncommittedInputDataBeforeUpgrade);\n+\n+            final List<KeyValue<Long, Long>> allOutputDataBeforeUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataBeforeUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            // phase 4:\n+            // expected end state per output partition:\n+            // stop case:\n+            //   p-0: 10 records + COMMIT + 5 records + COMMIT\n+            //   p-1: 10 records + COMMIT + 5 records (pending)\n+            // crash case:\n+            //   p-0: 10 records + COMMIT + 5 records (pending)\n+            //   p-1: 10 records + COMMIT + 5 records (pending)\n+            if (!injectError) {\n+                streams1Alpha.close();\n+            }\n+            waitForCondition(\n+                () -> stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not stabilize on time.\"\n+            );\n+            errorInjected = null;\n+\n+            final List<KeyValue<Long, Long>> allOutputDataAfterFirstUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataAfterFirstUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataAfterFirstUpgrade =\n+                    uncommittedInputDataBeforeUpgrade.stream().filter(pair -> pair.key == keyFilterFirstUpgrade).collect(Collectors.toList());\n+\n+                final List<KeyValue<Long, Long>> committedOutputDataAfterFirstUpgrade = readResult(committedInputDataAfterFirstUpgrade.size(), CONSUMER_GROUP_ID);\n+                checkResultPerKey(committedOutputDataAfterFirstUpgrade, computeExpectedResult(committedInputDataAfterFirstUpgrade, currentStateOne));\n+            }\n+\n+            // phase 5:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT\n+            // p-1: 10 records + COMMIT + 5 records (+ maybe commit if tasks would have been switch; we don't care and don't verify)\n+            commitRequested.set(0);\n+            stateTransitions1.clear();\n+            stateTransitions2.clear();\n+            streams1Beta.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+\n+            // capture committed state for later\n+            final Map<Long, Long> currentStateAfterFirstUpgrade = new HashMap<>(currentStateTwo);\n+            final ReadOnlyKeyValueStore<Long, Long> storeOneAfterUpgrade =\n+                streams1Beta.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            try (final KeyValueIterator<Long, Long> it = storeOneAfterUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    if (row.key == keyFilterFirstUpgrade) {\n+                        currentStateAfterFirstUpgrade.put(row.key, row.value);\n+                    }\n+                }\n+            }\n+            try (final KeyValueIterator<Long, Long> it = storeTwoBeforeUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    if (row.key == keyFilterFirstUpgrade) {\n+                        currentStateAfterFirstUpgrade.put(row.key, row.value);\n+                    }\n+                }\n+            }\n+\n+            // phase 6:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT\n+            // p-1: 10 records + COMMIT + 5 records + (maybe COMMIT) + 5 records + COMMIT\n+            final List<KeyValue<Long, Long>> committedInputDataDuringUpgrade = prepareData(15L, 20L, 0L, 1L);\n+            allData.addAll(committedInputDataDuringUpgrade);\n+\n+            writeInputData(committedInputDataDuringUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\");\n+\n+            final List<KeyValue<Long, Long>> allCommittedInputDataDuringUpgrade = new ArrayList<>();\n+            final Map<Long, Long> currentStateBeforeSecondUpgrade = new HashMap<>();\n+            if (!injectError) {\n+                allCommittedInputDataDuringUpgrade.addAll(\n+                    uncommittedInputDataBeforeUpgrade.stream().filter(pair -> pair.key != keyFilterFirstUpgrade).collect(Collectors.toList())\n+                );\n+                currentStateBeforeSecondUpgrade.putAll(currentStateAfterFirstUpgrade);\n+            } else {\n+                allCommittedInputDataDuringUpgrade.addAll(uncommittedInputDataBeforeUpgrade);\n+                currentStateBeforeSecondUpgrade.putAll(currentStateOne);\n+                currentStateBeforeSecondUpgrade.putAll(currentStateTwo);\n+            }\n+            allCommittedInputDataDuringUpgrade.addAll(committedInputDataDuringUpgrade);\n+\n+            final List<KeyValue<Long, Long>> committedOutputDataBeforeSecondUpgrade = readResult(allCommittedInputDataDuringUpgrade.size(), CONSUMER_GROUP_ID);\n+            checkResultPerKey(committedOutputDataBeforeSecondUpgrade, computeExpectedResult(allCommittedInputDataDuringUpgrade, currentStateBeforeSecondUpgrade));\n+\n+            // capture committed state for later\n+            currentStateOne.clear();\n+            try (final KeyValueIterator<Long, Long> it = storeOneAfterUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateOne.put(row.key, row.value);\n+                }\n+            }\n+            currentStateTwo.clear();\n+            final long keyFilterSecondUpgrade;\n+            try (final KeyValueIterator<Long, Long> it = storeTwoBeforeUpgrade.all()) {\n+                keyFilterSecondUpgrade = it.peekNextKey();\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateTwo.put(row.key, row.value);\n+                }\n+            }\n+\n+            // phase 7:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT + 5 records (pending)\n+            // p-1: 10 records + COMMIT + 5 records + 5 records + COMMIT  + 5 records (pending)\n+            stateTransitions1.clear();\n+            if (injectError) {\n+                errorInjectedClient2.set(true);\n+            }\n+            final List<KeyValue<Long, Long>> uncommittedInputDataDuringUpgrade = prepareData(20L, 25L, 0L, 1L);\n+            allData.addAll(uncommittedInputDataDuringUpgrade);\n+\n+            writeInputData(uncommittedInputDataDuringUpgrade);\n+\n+            final List<KeyValue<Long, Long>> allOutputDataBeforeSecondUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataBeforeSecondUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            // phase 8:\n+            // expected end state per output partition:\n+            // stop case:\n+            //   p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT + 5 records (pending)\n+            //   p-1: 10 records + COMMIT + 5 records + 5 records + COMMIT  + 5 records + COMMIT\n+            // crash case:\n+            //   p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT + 5 records (pending)\n+            //   p-1: 10 records + COMMIT + 5 records + 5 records + COMMIT  + 5 records (pending)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg3MjM4OA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409872388", "bodyText": "Why the second client will have two pending transactions? Upon migrated the task the initTxn should cause the pending transaction failed.", "author": "guozhangwang", "createdAt": "2020-04-16T21:57:44Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg5OTg0Ng==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409899846", "bodyText": "The second client is the \"healthy\" one: it will host one task and has one open transaction before the crash. The first client also hosts one task before the crash. When the first client fails, it does not have a pending TX because it did not write any output records yet (thus there is nothing to be aborted) and its task will migrate to the second client. The second client won't \"touch\" its original task and only create a second task, start a new TX for the new task (it would call init and would abort a pending TX if there would be any, but is also starts it's own new TX) and process 5 records (ie, retry the not yet processed record 10 to 14) without committing the TX because it would only commit after 10 records. Hence, it \"stabilizes\" with 2 pending transactions with 5 writes each.\nDoes this make sense?\nThinking about it: should we inject the error only after the first client wrote some pending output records to the output topic to verify if the TX gets aborted?", "author": "mjsax", "createdAt": "2020-04-16T23:08:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg3MjM4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkzNjA5Nw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409936097", "bodyText": "I thought the first client would only crash after it has processed 5 records and hence there's an ongoing transaction already?", "author": "guozhangwang", "createdAt": "2020-04-17T01:11:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg3MjM4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk1NDk2MQ==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409954961", "bodyText": "Not in the current way the test is written...", "author": "mjsax", "createdAt": "2020-04-17T02:23:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg3MjM4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg3MzUzMg==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409873532", "bodyText": "Not sure why it's the case? I think the previous pending txn should have aborted in step 4.", "author": "guozhangwang", "createdAt": "2020-04-16T22:00:10Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - crash only: the rebalance should result in a commit", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkwMjA1Nw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409902057", "bodyText": "Good catch. The commit happens for both cases, because when we migrate the task to the restarted client, the \"old\" client would commit the task. Will update the comment.", "author": "mjsax", "createdAt": "2020-04-16T23:15:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg3MzUzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg3OTQ3Mw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409879473", "bodyText": "In the current settings, with either alpha or beta, we will have one producer per thread since each thread would host one task only, right? Should we have 4 partitions so that under alpha we will have two producers and two txns per thread?", "author": "guozhangwang", "createdAt": "2020-04-16T22:14:06Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg5NzM0OQ==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409897349", "bodyText": "That is correct. I did consider using 4 partitions for the same reason, but was not sure if it would add value to the test? In the end, the \"gap\" between eos-alpha and eos-beta is not the number to open transaction, but the usage of different transactional.ids between a task-producer and a thread-producer and this gap is closed via \"fetch offset fencing\". Hence, if the \"fetch offset fencing\" works for one task-producer vs one thread-producer (both using different txId), it also works for two task-producers vs one thread-producer?\nThoughts?", "author": "mjsax", "createdAt": "2020-04-16T23:01:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg3OTQ3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkzNzM0Ng==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409937346", "bodyText": "What I'm thinking is when there are two running clients, one with eos-alpha and another with eos-beta (upgraded from eos-alpha) the number of transaction.ids is actually reduced, and hence the number of max in-flight txns, and logically I agree they should not have much impact, but hey without the testing we don't know about what we don't know right? If you think such \"changes of number of txn.ids and hence number of txns\" has been covered in other system tests then probably it's fine. But if using 4 partitions isn't going to make the test more complex / takes much longer, could we be a bit over-cautious here?", "author": "guozhangwang", "createdAt": "2020-04-17T01:16:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg3OTQ3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk1NTE2NA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409955164", "bodyText": "Sure. Works for me.", "author": "mjsax", "createdAt": "2020-04-17T02:24:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg3OTQ3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4MTYwMQ==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409881601", "bodyText": "Should we make this a concurrent linked list since concurrent threads may be adding at the same time?", "author": "guozhangwang", "createdAt": "2020-04-16T22:19:27Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - crash only: the rebalance should result in a commit\n+        // 6.  write 5 record per input topic partition and verify that the result was committed\n+        // 7.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - one transaction is base on a task producer; the other transaction is based on a thread producer\n+        // 8.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only: verify that the stopped client did commit its pending transaction during shutdown\n+        //      - the first client will still have an pending transaction\n+        // 9.  restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        // 10. write 5 record per input topic partition and verify that the result was committed\n+\n+        try (final KafkaStreams streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            final KafkaStreams streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA)) {\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkwMzY0NA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409903644", "bodyText": "There should not be any concurrent threads, because stream1Alpha and streams1Beta don't run at the same time but strictly one after each other.", "author": "mjsax", "createdAt": "2020-04-16T23:20:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4MTYwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkzNzQxOQ==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409937419", "bodyText": "Ah, you're right.", "author": "guozhangwang", "createdAt": "2020-04-17T01:16:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4MTYwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4MjE2Mw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409882163", "bodyText": "Seems we can just keep errorInjected to null in phase 1 since we do not need to inject failures?", "author": "guozhangwang", "createdAt": "2020-04-16T22:20:54Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - crash only: the rebalance should result in a commit\n+        // 6.  write 5 record per input topic partition and verify that the result was committed\n+        // 7.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - one transaction is base on a task producer; the other transaction is based on a thread producer\n+        // 8.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only: verify that the stopped client did commit its pending transaction during shutdown\n+        //      - the first client will still have an pending transaction\n+        // 9.  restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        // 10. write 5 record per input topic partition and verify that the result was committed\n+\n+        try (final KafkaStreams streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            final KafkaStreams streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA)) {\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+            streams1Alpha.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+            streams2Alpha.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+            streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+            // phase 1:\n+            final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient1;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkwNDQxNA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409904414", "bodyText": "This is a little tricky... We get a handle on the AtomicBoolean within the Processor#init() method. In phase 1 we create the Processor and thus we need to prepare the error injection at this point already. In the later phases, Processor#init() is not called for the processors hosted by client-1 any more.\nDoes this make sense?", "author": "mjsax", "createdAt": "2020-04-16T23:22:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4MjE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkzNzUxNw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409937517", "bodyText": "Got it, makes sense.", "author": "guozhangwang", "createdAt": "2020-04-17T01:17:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4MjE2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4NDE1NA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409884154", "bodyText": "After it resumes to running, p-0 should be aborted right?", "author": "guozhangwang", "createdAt": "2020-04-16T22:25:54Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - crash only: the rebalance should result in a commit\n+        // 6.  write 5 record per input topic partition and verify that the result was committed\n+        // 7.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - one transaction is base on a task producer; the other transaction is based on a thread producer\n+        // 8.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only: verify that the stopped client did commit its pending transaction during shutdown\n+        //      - the first client will still have an pending transaction\n+        // 9.  restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        // 10. write 5 record per input topic partition and verify that the result was committed\n+\n+        try (final KafkaStreams streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            final KafkaStreams streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA)) {\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+            streams1Alpha.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+            streams2Alpha.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+            streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+            // phase 1:\n+            final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient1;\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                    )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not startup on time.\"\n+            );\n+            stateTransitions1.clear();\n+            errorInjected = null;\n+\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+\n+\n+            // phase 2:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT\n+            // p-1: 10 records + COMMIT\n+            final List<KeyValue<Long, Long>> committedInputDataBeforeUpgrade = prepareData(0L, 10L, 0L, 1L);\n+\n+            final List<KeyValue<Long, Long>> allData = new ArrayList<>(committedInputDataBeforeUpgrade);\n+\n+            writeInputData(committedInputDataBeforeUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\"\n+            );\n+\n+            final List<KeyValue<Long, Long>> committedOutputDataBeforeUpgrade = readResult(committedInputDataBeforeUpgrade.size(), CONSUMER_GROUP_ID);\n+            checkResultPerKey(committedOutputDataBeforeUpgrade, computeExpectedResult(committedInputDataBeforeUpgrade, new HashMap<>()));\n+\n+            // capture committed state for later\n+            final ReadOnlyKeyValueStore<Long, Long> storeOneBeforeUpgrade =\n+                streams1Alpha.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            final Map<Long, Long> currentStateOne = new HashMap<>();\n+            final long keyFilterFirstUpgrade;\n+            try (final KeyValueIterator<Long, Long> it = storeOneBeforeUpgrade.all()) {\n+                keyFilterFirstUpgrade = it.peekNextKey();\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateOne.put(row.key, row.value);\n+                }\n+            }\n+            final ReadOnlyKeyValueStore<Long, Long> storeTwoBeforeUpgrade =\n+                streams2Alpha.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            final Map<Long, Long> currentStateTwo = new HashMap<>();\n+            try (final KeyValueIterator<Long, Long> it = storeTwoBeforeUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateTwo.put(row.key, row.value);\n+                }\n+            }\n+\n+            // phase 3:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records (pending)\n+            // p-1: 10 records + COMMIT + 5 records (pending)\n+            stateTransitions2.clear();\n+            final AtomicBoolean errorInjectedClient2 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient2;\n+            if (injectError) {\n+                errorInjectedClient1.set(true);\n+            }\n+            final List<KeyValue<Long, Long>> uncommittedInputDataBeforeUpgrade = prepareData(10L, 15L, 0L, 1L);\n+            allData.addAll(uncommittedInputDataBeforeUpgrade);\n+\n+            writeInputData(uncommittedInputDataBeforeUpgrade);\n+\n+            final List<KeyValue<Long, Long>> allOutputDataBeforeUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataBeforeUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            // phase 4:\n+            // expected end state per output partition:\n+            // stop case:\n+            //   p-0: 10 records + COMMIT + 5 records + COMMIT\n+            //   p-1: 10 records + COMMIT + 5 records (pending)\n+            // crash case:\n+            //   p-0: 10 records + COMMIT + 5 records (pending)\n+            //   p-1: 10 records + COMMIT + 5 records (pending)\n+            if (!injectError) {\n+                streams1Alpha.close();\n+            }\n+            waitForCondition(\n+                () -> stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkwNTQyOA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409905428", "bodyText": "Well, there is no TX for p-0 on client-1 as it crashes before it writes a first output record and thus never starts a TX. (Compare my comment from above.)", "author": "mjsax", "createdAt": "2020-04-16T23:25:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4NDE1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4NTA3Mw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409885073", "bodyText": "Here p-0 may end with COMMIT (close) or ABORT (crash) right?", "author": "guozhangwang", "createdAt": "2020-04-16T22:28:24Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - crash only: the rebalance should result in a commit\n+        // 6.  write 5 record per input topic partition and verify that the result was committed\n+        // 7.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - one transaction is base on a task producer; the other transaction is based on a thread producer\n+        // 8.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only: verify that the stopped client did commit its pending transaction during shutdown\n+        //      - the first client will still have an pending transaction\n+        // 9.  restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        // 10. write 5 record per input topic partition and verify that the result was committed\n+\n+        try (final KafkaStreams streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            final KafkaStreams streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA)) {\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+            streams1Alpha.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+            streams2Alpha.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+            streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+            // phase 1:\n+            final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient1;\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                    )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not startup on time.\"\n+            );\n+            stateTransitions1.clear();\n+            errorInjected = null;\n+\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+\n+\n+            // phase 2:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT\n+            // p-1: 10 records + COMMIT\n+            final List<KeyValue<Long, Long>> committedInputDataBeforeUpgrade = prepareData(0L, 10L, 0L, 1L);\n+\n+            final List<KeyValue<Long, Long>> allData = new ArrayList<>(committedInputDataBeforeUpgrade);\n+\n+            writeInputData(committedInputDataBeforeUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\"\n+            );\n+\n+            final List<KeyValue<Long, Long>> committedOutputDataBeforeUpgrade = readResult(committedInputDataBeforeUpgrade.size(), CONSUMER_GROUP_ID);\n+            checkResultPerKey(committedOutputDataBeforeUpgrade, computeExpectedResult(committedInputDataBeforeUpgrade, new HashMap<>()));\n+\n+            // capture committed state for later\n+            final ReadOnlyKeyValueStore<Long, Long> storeOneBeforeUpgrade =\n+                streams1Alpha.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            final Map<Long, Long> currentStateOne = new HashMap<>();\n+            final long keyFilterFirstUpgrade;\n+            try (final KeyValueIterator<Long, Long> it = storeOneBeforeUpgrade.all()) {\n+                keyFilterFirstUpgrade = it.peekNextKey();\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateOne.put(row.key, row.value);\n+                }\n+            }\n+            final ReadOnlyKeyValueStore<Long, Long> storeTwoBeforeUpgrade =\n+                streams2Alpha.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            final Map<Long, Long> currentStateTwo = new HashMap<>();\n+            try (final KeyValueIterator<Long, Long> it = storeTwoBeforeUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateTwo.put(row.key, row.value);\n+                }\n+            }\n+\n+            // phase 3:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records (pending)\n+            // p-1: 10 records + COMMIT + 5 records (pending)\n+            stateTransitions2.clear();\n+            final AtomicBoolean errorInjectedClient2 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient2;\n+            if (injectError) {\n+                errorInjectedClient1.set(true);\n+            }\n+            final List<KeyValue<Long, Long>> uncommittedInputDataBeforeUpgrade = prepareData(10L, 15L, 0L, 1L);\n+            allData.addAll(uncommittedInputDataBeforeUpgrade);\n+\n+            writeInputData(uncommittedInputDataBeforeUpgrade);\n+\n+            final List<KeyValue<Long, Long>> allOutputDataBeforeUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataBeforeUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            // phase 4:\n+            // expected end state per output partition:\n+            // stop case:\n+            //   p-0: 10 records + COMMIT + 5 records + COMMIT\n+            //   p-1: 10 records + COMMIT + 5 records (pending)\n+            // crash case:\n+            //   p-0: 10 records + COMMIT + 5 records (pending)\n+            //   p-1: 10 records + COMMIT + 5 records (pending)\n+            if (!injectError) {\n+                streams1Alpha.close();\n+            }\n+            waitForCondition(\n+                () -> stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not stabilize on time.\"\n+            );\n+            errorInjected = null;\n+\n+            final List<KeyValue<Long, Long>> allOutputDataAfterFirstUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataAfterFirstUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataAfterFirstUpgrade =\n+                    uncommittedInputDataBeforeUpgrade.stream().filter(pair -> pair.key == keyFilterFirstUpgrade).collect(Collectors.toList());\n+\n+                final List<KeyValue<Long, Long>> committedOutputDataAfterFirstUpgrade = readResult(committedInputDataAfterFirstUpgrade.size(), CONSUMER_GROUP_ID);\n+                checkResultPerKey(committedOutputDataAfterFirstUpgrade, computeExpectedResult(committedInputDataAfterFirstUpgrade, currentStateOne));\n+            }\n+\n+            // phase 5:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkwNzk1OQ==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409907959", "bodyText": "Same comment as above: the is no pending TX for p-0 when a crash happens. But the comment here is still simplfied. The end state would be (in full details):\n            // phase 5:\n            // expected end state per output partition:\n            // stop case (original task migrated back):\n            //   p-0: 10 records + COMMIT + 5 records + COMMIT\n            //   p-1: 10 records + COMMIT + 5 records (pending)\n            // stop case (task \"switch\"):\n            //   p-0: 10 records + COMMIT + 5 records + COMMIT\n            //   p-1: 10 records + COMMIT + 5 records + COMMIT\n            // crash case (original task migrates back):\n            //   p-0: 10 records + COMMIT + 5 records + COMMIT\n            //   p-1: 10 records + COMMIT + 5 records (pending)\n            // crash case (task \"switch\"):\n            //   p-0: 10 records + COMMIT + 5 records (pending)\n            //   p-1: 10 records + COMMIT + 5 records + COMMIT", "author": "mjsax", "createdAt": "2020-04-16T23:33:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4NTA3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkzODE3OA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409938178", "bodyText": "I thought that before the client crashed there's a pending txn because we've processed 5 records already in stage 3):\n3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n        //      - both pending transactions are based on task producers\n\nIs that not right?", "author": "guozhangwang", "createdAt": "2020-04-17T01:19:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4NTA3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTk1NjgxNQ==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409956815", "bodyText": "In the current implementation, we set the inject error flag to true, before we write the 5 input records. Hence, we crash when processing the first record -- therefore, no output record is written and no TX is started (as we start TX lazily).\nWe cannot set the inject error flag to true after we sent the input data, because we would be subject to a race condition -- all 5 records might have been processed before we set the flag and thus, so error would happen as process() won't be called any more. We can of course change the test to add an additional condition in the Processor to only throw an exception if the flag is set to true and if, eg, value % 10 == 4 (ie, we throw when processing the last record, and thus get 4 pending writes).", "author": "mjsax", "createdAt": "2020-04-17T02:31:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4NTA3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg5MDI0NA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409890244", "bodyText": "This is a meta comment: I think the key here is to check that the offsets sent via \"sendOffsetsToTxn\" would not be seen to the new owner of the task, i.e. the fencing would take effects. But here since we do not commit (only 5 out of 10 records) this logic would never be checked.\nOn the other hand, inside StreamsProducer we always call\n            producer.sendOffsetsToTransaction(offsets, consumerGroupMetadata);\n            producer.commitTransaction();\n\ntogether, it is a bit hard to check with the given producer.\nI'd suggest doing a bit different here: we use a customized client-supplier that generate an extended producer which, depending on the failure injection boolean, throws an exception in commitTransaction immediately, so that we made sure the sendOffsetsToTransaction completes (i.e. the request acked from group coordinator already) before we crash, instead of crashing in the middle of the processing where we'd not send any offsets to group coordinator anyways.", "author": "guozhangwang", "createdAt": "2020-04-16T22:42:15Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 5 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 2;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private AtomicBoolean errorInjected;\n+    private AtomicInteger commitRequested;\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 2 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - both pending transactions are based on task producers\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have an pending transaction\n+        //      - crash case:\n+        //        * the second client will have two pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - crash only: the rebalance should result in a commit\n+        // 6.  write 5 record per input topic partition and verify that the result was committed\n+        // 7.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - one transaction is base on a task producer; the other transaction is based on a thread producer\n+        // 8.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only: verify that the stopped client did commit its pending transaction during shutdown\n+        //      - the first client will still have an pending transaction\n+        // 9.  restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        // 10. write 5 record per input topic partition and verify that the result was committed\n+\n+        try (final KafkaStreams streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            final KafkaStreams streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            final KafkaStreams streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA)) {\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+            streams1Alpha.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+\n+            final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+            streams2Alpha.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+            streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+            // phase 1:\n+            final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient1;\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                    )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not startup on time.\"\n+            );\n+            stateTransitions1.clear();\n+            errorInjected = null;\n+\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+\n+\n+            // phase 2:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT\n+            // p-1: 10 records + COMMIT\n+            final List<KeyValue<Long, Long>> committedInputDataBeforeUpgrade = prepareData(0L, 10L, 0L, 1L);\n+\n+            final List<KeyValue<Long, Long>> allData = new ArrayList<>(committedInputDataBeforeUpgrade);\n+\n+            writeInputData(committedInputDataBeforeUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\"\n+            );\n+\n+            final List<KeyValue<Long, Long>> committedOutputDataBeforeUpgrade = readResult(committedInputDataBeforeUpgrade.size(), CONSUMER_GROUP_ID);\n+            checkResultPerKey(committedOutputDataBeforeUpgrade, computeExpectedResult(committedInputDataBeforeUpgrade, new HashMap<>()));\n+\n+            // capture committed state for later\n+            final ReadOnlyKeyValueStore<Long, Long> storeOneBeforeUpgrade =\n+                streams1Alpha.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            final Map<Long, Long> currentStateOne = new HashMap<>();\n+            final long keyFilterFirstUpgrade;\n+            try (final KeyValueIterator<Long, Long> it = storeOneBeforeUpgrade.all()) {\n+                keyFilterFirstUpgrade = it.peekNextKey();\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateOne.put(row.key, row.value);\n+                }\n+            }\n+            final ReadOnlyKeyValueStore<Long, Long> storeTwoBeforeUpgrade =\n+                streams2Alpha.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            final Map<Long, Long> currentStateTwo = new HashMap<>();\n+            try (final KeyValueIterator<Long, Long> it = storeTwoBeforeUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateTwo.put(row.key, row.value);\n+                }\n+            }\n+\n+            // phase 3:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records (pending)\n+            // p-1: 10 records + COMMIT + 5 records (pending)\n+            stateTransitions2.clear();\n+            final AtomicBoolean errorInjectedClient2 = new AtomicBoolean(false);\n+            errorInjected = errorInjectedClient2;\n+            if (injectError) {\n+                errorInjectedClient1.set(true);\n+            }\n+            final List<KeyValue<Long, Long>> uncommittedInputDataBeforeUpgrade = prepareData(10L, 15L, 0L, 1L);\n+            allData.addAll(uncommittedInputDataBeforeUpgrade);\n+\n+            writeInputData(uncommittedInputDataBeforeUpgrade);\n+\n+            final List<KeyValue<Long, Long>> allOutputDataBeforeUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataBeforeUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            // phase 4:\n+            // expected end state per output partition:\n+            // stop case:\n+            //   p-0: 10 records + COMMIT + 5 records + COMMIT\n+            //   p-1: 10 records + COMMIT + 5 records (pending)\n+            // crash case:\n+            //   p-0: 10 records + COMMIT + 5 records (pending)\n+            //   p-1: 10 records + COMMIT + 5 records (pending)\n+            if (!injectError) {\n+                streams1Alpha.close();\n+            }\n+            waitForCondition(\n+                () -> stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not stabilize on time.\"\n+            );\n+            errorInjected = null;\n+\n+            final List<KeyValue<Long, Long>> allOutputDataAfterFirstUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataAfterFirstUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataAfterFirstUpgrade =\n+                    uncommittedInputDataBeforeUpgrade.stream().filter(pair -> pair.key == keyFilterFirstUpgrade).collect(Collectors.toList());\n+\n+                final List<KeyValue<Long, Long>> committedOutputDataAfterFirstUpgrade = readResult(committedInputDataAfterFirstUpgrade.size(), CONSUMER_GROUP_ID);\n+                checkResultPerKey(committedOutputDataAfterFirstUpgrade, computeExpectedResult(committedInputDataAfterFirstUpgrade, currentStateOne));\n+            }\n+\n+            // phase 5:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT\n+            // p-1: 10 records + COMMIT + 5 records (+ maybe commit if tasks would have been switch; we don't care and don't verify)\n+            commitRequested.set(0);\n+            stateTransitions1.clear();\n+            stateTransitions2.clear();\n+            streams1Beta.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+\n+            // capture committed state for later\n+            final Map<Long, Long> currentStateAfterFirstUpgrade = new HashMap<>(currentStateTwo);\n+            final ReadOnlyKeyValueStore<Long, Long> storeOneAfterUpgrade =\n+                streams1Beta.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            try (final KeyValueIterator<Long, Long> it = storeOneAfterUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    if (row.key == keyFilterFirstUpgrade) {\n+                        currentStateAfterFirstUpgrade.put(row.key, row.value);\n+                    }\n+                }\n+            }\n+            try (final KeyValueIterator<Long, Long> it = storeTwoBeforeUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    if (row.key == keyFilterFirstUpgrade) {\n+                        currentStateAfterFirstUpgrade.put(row.key, row.value);\n+                    }\n+                }\n+            }\n+\n+            // phase 6:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT\n+            // p-1: 10 records + COMMIT + 5 records + (maybe COMMIT) + 5 records + COMMIT\n+            final List<KeyValue<Long, Long>> committedInputDataDuringUpgrade = prepareData(15L, 20L, 0L, 1L);\n+            allData.addAll(committedInputDataDuringUpgrade);\n+\n+            writeInputData(committedInputDataDuringUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\");\n+\n+            final List<KeyValue<Long, Long>> allCommittedInputDataDuringUpgrade = new ArrayList<>();\n+            final Map<Long, Long> currentStateBeforeSecondUpgrade = new HashMap<>();\n+            if (!injectError) {\n+                allCommittedInputDataDuringUpgrade.addAll(\n+                    uncommittedInputDataBeforeUpgrade.stream().filter(pair -> pair.key != keyFilterFirstUpgrade).collect(Collectors.toList())\n+                );\n+                currentStateBeforeSecondUpgrade.putAll(currentStateAfterFirstUpgrade);\n+            } else {\n+                allCommittedInputDataDuringUpgrade.addAll(uncommittedInputDataBeforeUpgrade);\n+                currentStateBeforeSecondUpgrade.putAll(currentStateOne);\n+                currentStateBeforeSecondUpgrade.putAll(currentStateTwo);\n+            }\n+            allCommittedInputDataDuringUpgrade.addAll(committedInputDataDuringUpgrade);\n+\n+            final List<KeyValue<Long, Long>> committedOutputDataBeforeSecondUpgrade = readResult(allCommittedInputDataDuringUpgrade.size(), CONSUMER_GROUP_ID);\n+            checkResultPerKey(committedOutputDataBeforeSecondUpgrade, computeExpectedResult(allCommittedInputDataDuringUpgrade, currentStateBeforeSecondUpgrade));\n+\n+            // capture committed state for later\n+            currentStateOne.clear();\n+            try (final KeyValueIterator<Long, Long> it = storeOneAfterUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateOne.put(row.key, row.value);\n+                }\n+            }\n+            currentStateTwo.clear();\n+            final long keyFilterSecondUpgrade;\n+            try (final KeyValueIterator<Long, Long> it = storeTwoBeforeUpgrade.all()) {\n+                keyFilterSecondUpgrade = it.peekNextKey();\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    currentStateTwo.put(row.key, row.value);\n+                }\n+            }\n+\n+            // phase 7:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT + 5 records (pending)\n+            // p-1: 10 records + COMMIT + 5 records + 5 records + COMMIT  + 5 records (pending)\n+            stateTransitions1.clear();\n+            if (injectError) {\n+                errorInjectedClient2.set(true);\n+            }\n+            final List<KeyValue<Long, Long>> uncommittedInputDataDuringUpgrade = prepareData(20L, 25L, 0L, 1L);\n+            allData.addAll(uncommittedInputDataDuringUpgrade);\n+\n+            writeInputData(uncommittedInputDataDuringUpgrade);\n+\n+            final List<KeyValue<Long, Long>> allOutputDataBeforeSecondUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataBeforeSecondUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            // phase 8:\n+            // expected end state per output partition:\n+            // stop case:\n+            //   p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT + 5 records (pending)\n+            //   p-1: 10 records + COMMIT + 5 records + 5 records + COMMIT  + 5 records + COMMIT\n+            // crash case:\n+            //   p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT + 5 records (pending)\n+            //   p-1: 10 records + COMMIT + 5 records + 5 records + COMMIT  + 5 records (pending)\n+            if (!injectError) {\n+                streams2Alpha.close();\n+            }\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                        KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                        KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not stabilize on time.\"\n+            );\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataAfterSecondUpgrade =\n+                    uncommittedInputDataDuringUpgrade.stream().filter(pair -> pair.key == keyFilterSecondUpgrade).collect(Collectors.toList());\n+\n+                final List<KeyValue<Long, Long>> committedOutputDataAfterSecondUpgrade = readResult(committedInputDataAfterSecondUpgrade.size(), CONSUMER_GROUP_ID);\n+                checkResultPerKey(committedOutputDataAfterSecondUpgrade, computeExpectedResult(committedInputDataAfterSecondUpgrade, currentStateTwo));\n+            }\n+\n+            final List<KeyValue<Long, Long>> allOutputDataAfterSecondUpgrade = readResult(allData.size(), null);\n+            checkResultPerKey(allOutputDataAfterSecondUpgrade, computeExpectedResult(allData, new HashMap<>()));\n+\n+            // phase 9:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT + 5 records (+ maybe commit if tasks would have been switch; we don't care and don't verify)\n+            // p-1: 10 records + COMMIT + 5 records + 5 records + COMMIT  + 5 records + COMMIT\n+            commitRequested.set(0);\n+            stateTransitions1.clear();\n+            stateTransitions2.clear();\n+            streams2Beta.start();\n+            waitForCondition(\n+                () -> stateTransitions1.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )) && stateTransitions2.equals(Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                    KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+\n+            // capture committed state\n+            final Map<Long, Long> currentStateAfterSecondUpgrade = new HashMap<>(currentStateOne);\n+            try (final KeyValueIterator<Long, Long> it = storeOneAfterUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    if (row.key == keyFilterSecondUpgrade) {\n+                        currentStateAfterSecondUpgrade.put(row.key, row.value);\n+                    }\n+                }\n+            }\n+            final ReadOnlyKeyValueStore<Long, Long> storeTwoAfterUpgrade =\n+                streams2Beta.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));\n+            try (final KeyValueIterator<Long, Long> it = storeTwoAfterUpgrade.all()) {\n+                while (it.hasNext()) {\n+                    final KeyValue<Long, Long> row = it.next();\n+                    if (row.key == keyFilterSecondUpgrade) {\n+                        currentStateAfterSecondUpgrade.put(row.key, row.value);\n+                    }\n+                }\n+            }\n+\n+            // phase 10:\n+            // expected end state per output partition:\n+            // p-0: 10 records + COMMIT + 5 records + COMMIT + 5 records + COMMIT + 5 records + 5 records + COMMIT\n+            // p-1: 10 records + COMMIT + 5 records + 5 records + COMMIT  + 5 records + COMMIT + 5 records + COMMIT", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkwODU4Nw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r409908587", "bodyText": "Correct. I was aware that this case would not be covered, but did not have a good idea how to test it. Your idea is great! Will work on that.", "author": "mjsax", "createdAt": "2020-04-16T23:35:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg5MDI0NA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczNzk1Mg==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r417737952", "bodyText": "Minor bug: this value was hard coded and it was not possible to overwrite it.", "author": "mjsax", "createdAt": "2020-04-30T03:49:35Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -1283,9 +1284,6 @@ private void verifyMaxInFlightRequestPerConnection(final Object maxInFlightReque\n         // add client id with stream client id prefix\n         props.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId);\n \n-        // Reduce the transaction timeout for quicker pending offset expiration on broker side.\n-        props.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, 10000);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczODA3Mw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r417738073", "bodyText": "The test fails here... (cf. over TODO)", "author": "mjsax", "createdAt": "2020-04-30T03:50:11Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -388,6 +388,7 @@ public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n \n                 final List<KeyValue<Long, Long>> expectedCommittedResult =\n                     computeExpectedResult(committedInputDataDuringFirstUpgrade, committedState);\n+                // TODO: if we don't use the custom partitioner, the test hangs here until TX times out and is aborted\n                 verifyCommitted(expectedCommittedResult);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczODE5Mw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r417738193", "bodyText": "This is the workaround that make the test (clean run) pass. For the error injection run, the test passed w/ and w/o this partitioner.", "author": "mjsax", "createdAt": "2020-04-30T03:50:51Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -881,6 +882,10 @@ public void close() { }\n         properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG), 5 * 1000);\n         properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG), 5 * 1000 - 1);\n         properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG), MAX_POLL_INTERVAL_MS);\n+        // TODO\n+        //   if we don't use this custom partitioner the test fails for the non-error case\n+        //   unclear why -- see other TODO\n+        properties.put(StreamsConfig.producerPrefix(ProducerConfig.PARTITIONER_CLASS_CONFIG), KeyPartitioner.class);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczODM3MQ==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r417738371", "bodyText": "This is just needed to make the partitioner work for writing into input topics and to use within KS to write into output topic.", "author": "mjsax", "createdAt": "2020-04-30T03:51:38Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -1064,14 +1071,16 @@ private void addAllKeys(final Set<Long> allKeys, final List<KeyValue<Long, Long>\n \n     // must be public to allow KafkaProducer to instantiate it\n     public static class KeyPartitioner implements Partitioner {\n+        private final static LongDeserializer LONG_DESERIALIZER = new LongDeserializer();\n+\n         @Override\n         public int partition(final String topic,\n                              final Object key,\n                              final byte[] keyBytes,\n                              final Object value,\n                              final byte[] valueBytes,\n                              final Cluster cluster) {\n-            return ((Long) key).intValue() % NUM_TOPIC_PARTITIONS;\n+            return LONG_DESERIALIZER.deserialize(topic, keyBytes).intValue() % NUM_TOPIC_PARTITIONS;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODIwNTQxMw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r418205413", "bodyText": "Hmm, this sounds to me that the StreamProducer's own partitionsFor did not return the num.partitions so we ended up calling send with partition == null, since otherwise we will get the partition as\npartition = partitioner.partition(topic, key, value, partitions.size());\n\nwhere partitioner is the StreamsPartitioner and the producer's own partitioner should not be used.", "author": "guozhangwang", "createdAt": "2020-04-30T18:25:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczODM3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODIwOTI4NQ==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r418209285", "bodyText": "I don't think so. The original impl (just for the upstream producer to write into the input topics) was:\nreturn ((Long) key).intValue() % NUM_TOPIC_PARTITIONS;\n\nHowever, this assumes that key is of type Long what is not true when used within streams, because Streams does serialize all data upfront and key and value type is byte[] -- thus, we need to deserialize  to get the original key object.", "author": "mjsax", "createdAt": "2020-04-30T18:32:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczODM3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODIxNDE2Mg==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r418214162", "bodyText": "What I was asking is for the necessity of\nproperties.put(StreamsConfig.producerPrefix(ProducerConfig.PARTITIONER_CLASS_CONFIG), KeyPartitioner.class);\n\nAs I mentioned, Streams has its own StreamsPartitioner, and if it can get the actual not-null partition value passing to the send call, then the embedded producer's partitioner would not be used. Maybe I missed something critical here --- did you mean this config is only used for sending data to the source topics? If yes why put it into a streams props?", "author": "guozhangwang", "createdAt": "2020-04-30T18:41:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczODM3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczODgwMQ==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r417738801", "bodyText": "We use 6 clients now, to do some error injection in \"mixed mode\". To avoid JXM warnings, we cannot create all clients at the same time and thus cannot use try-with-resources...", "author": "mjsax", "createdAt": "2020-04-30T03:53:51Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,1123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Partitioner;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 100 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> TWO_REBALANCES_STARTUP =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> TWO_REBALANCES_RUNNING =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> SINGLE_REBALANCE =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CLOSE =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.PENDING_SHUTDOWN),\n+                KeyValue.pair(KafkaStreams.State.PENDING_SHUTDOWN, KafkaStreams.State.NOT_RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CRASH =\n+        Collections.unmodifiableList(\n+            Collections.singletonList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.ERROR)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CLOSE_CRASHED =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.ERROR, KafkaStreams.State.PENDING_SHUTDOWN),\n+                KeyValue.pair(KafkaStreams.State.PENDING_SHUTDOWN, KafkaStreams.State.NOT_RUNNING)\n+            )\n+        );\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 4;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+    private final AtomicBoolean errorInjectedClient2 = new AtomicBoolean(false);\n+    private final AtomicBoolean commitErrorInjectedClient1 = new AtomicBoolean(false);\n+    private final AtomicBoolean commitErrorInjectedClient2 = new AtomicBoolean(false);\n+    private final AtomicInteger commitCounterClient1 = new AtomicInteger(-1);\n+    private final AtomicInteger commitCounterClient2 = new AtomicInteger(-1);\n+    private final AtomicInteger commitRequested = new AtomicInteger(0);\n+\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // Additionally, we inject errors while one client is on eos-alpha while the other client is on eos-beta:\n+        // For this case, we inject the error during task commit phase, i.e., after offsets are appended to a TX,\n+        // and before the TX is committed. The goal is to verify that the written but uncommitted offsets are not\n+        // picked up, i.e., GroupCoordinator fencing works correctly.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 4 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - all 4 pending transactions are based on task producers\n+        //      - we will get only 4 pending writes for one partition for the crash case as we crash processing the 5th record\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have two pending transaction\n+        //      - crash case:\n+        //        * the pending transactions of the crashed client got aborted\n+        //        * the second client will have four pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - the rebalance should result in a commit of all tasks\n+        // 6.  write 5 record per input topic partition\n+        //       - stop case:\n+        //         * verify that the result was committed\n+        //       - crash case:\n+        //         * fail the second (i.e., eos-alpha) client during commit\n+        //         * the eos-beta client should not pickup the pending offsets\n+        //         * verify uncommitted and committed result\n+        // 7.  only for crash case:\n+        //     7a. restart the second client in eos-alpha mode and wait until rebalance stabilizes\n+        //     7b. write 10 records per input topic partition\n+        //         * fail the first (i.e., eos-beta) client during commit\n+        //         * the eos-alpha client should not pickup the pending offsets\n+        //         * verify uncommitted and committed result\n+        //     7c. restart the first client in eos-beta mode and wait until rebalance stabilizes\n+        // 8.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - 2 transaction are base on a task producer; one transaction is based on a thread producer\n+        //      - we will get 4 pending writes for the crash case as we crash processing the 5th record\n+        // 9.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the first client will still have one pending transaction\n+        //      - crash case:\n+        //        * the pending transactions of the crashed client got aborted\n+        //        * the first client will have one pending transactions\n+        // 10. restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - the rebalance should result in a commit of all tasks\n+        // 11. write 5 record per input topic partition and verify that the result was committed\n+\n+        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+        KafkaStreams streams1Alpha = null;\n+        KafkaStreams streams1Beta = null;\n+        KafkaStreams streams1BetaTwo = null;\n+\n+        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+        KafkaStreams streams2Alpha = null;\n+        KafkaStreams streams2AlphaTwo = null;\n+        KafkaStreams streams2Beta = null;\n+//        streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA);\n+//        streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+        try {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczOTA2OA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r417739068", "bodyText": "This is new: for the crash case, in inject more errors in mixed mode, after we called sendOffsetsToTransaction() but before actually committing.", "author": "mjsax", "createdAt": "2020-04-30T03:55:03Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,1123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Partitioner;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 100 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> TWO_REBALANCES_STARTUP =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> TWO_REBALANCES_RUNNING =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> SINGLE_REBALANCE =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CLOSE =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.PENDING_SHUTDOWN),\n+                KeyValue.pair(KafkaStreams.State.PENDING_SHUTDOWN, KafkaStreams.State.NOT_RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CRASH =\n+        Collections.unmodifiableList(\n+            Collections.singletonList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.ERROR)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CLOSE_CRASHED =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.ERROR, KafkaStreams.State.PENDING_SHUTDOWN),\n+                KeyValue.pair(KafkaStreams.State.PENDING_SHUTDOWN, KafkaStreams.State.NOT_RUNNING)\n+            )\n+        );\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 4;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+    private final AtomicBoolean errorInjectedClient2 = new AtomicBoolean(false);\n+    private final AtomicBoolean commitErrorInjectedClient1 = new AtomicBoolean(false);\n+    private final AtomicBoolean commitErrorInjectedClient2 = new AtomicBoolean(false);\n+    private final AtomicInteger commitCounterClient1 = new AtomicInteger(-1);\n+    private final AtomicInteger commitCounterClient2 = new AtomicInteger(-1);\n+    private final AtomicInteger commitRequested = new AtomicInteger(0);\n+\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // Additionally, we inject errors while one client is on eos-alpha while the other client is on eos-beta:\n+        // For this case, we inject the error during task commit phase, i.e., after offsets are appended to a TX,\n+        // and before the TX is committed. The goal is to verify that the written but uncommitted offsets are not\n+        // picked up, i.e., GroupCoordinator fencing works correctly.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 4 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - all 4 pending transactions are based on task producers\n+        //      - we will get only 4 pending writes for one partition for the crash case as we crash processing the 5th record\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have two pending transaction\n+        //      - crash case:\n+        //        * the pending transactions of the crashed client got aborted\n+        //        * the second client will have four pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - the rebalance should result in a commit of all tasks\n+        // 6.  write 5 record per input topic partition\n+        //       - stop case:\n+        //         * verify that the result was committed\n+        //       - crash case:\n+        //         * fail the second (i.e., eos-alpha) client during commit\n+        //         * the eos-beta client should not pickup the pending offsets\n+        //         * verify uncommitted and committed result\n+        // 7.  only for crash case:\n+        //     7a. restart the second client in eos-alpha mode and wait until rebalance stabilizes\n+        //     7b. write 10 records per input topic partition\n+        //         * fail the first (i.e., eos-beta) client during commit\n+        //         * the eos-alpha client should not pickup the pending offsets\n+        //         * verify uncommitted and committed result\n+        //     7c. restart the first client in eos-beta mode and wait until rebalance stabilizes\n+        // 8.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - 2 transaction are base on a task producer; one transaction is based on a thread producer\n+        //      - we will get 4 pending writes for the crash case as we crash processing the 5th record\n+        // 9.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the first client will still have one pending transaction\n+        //      - crash case:\n+        //        * the pending transactions of the crashed client got aborted\n+        //        * the first client will have one pending transactions\n+        // 10. restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - the rebalance should result in a commit of all tasks\n+        // 11. write 5 record per input topic partition and verify that the result was committed\n+\n+        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+        KafkaStreams streams1Alpha = null;\n+        KafkaStreams streams1Beta = null;\n+        KafkaStreams streams1BetaTwo = null;\n+\n+        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+        KafkaStreams streams2Alpha = null;\n+        KafkaStreams streams2AlphaTwo = null;\n+        KafkaStreams streams2Beta = null;\n+//        streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA);\n+//        streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+        try {\n+            // phase 1: start both clients\n+            streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            streams1Alpha.setStateListener(\n+                (newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState))\n+            );\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForStateTransition(\n+                stateTransitions1,\n+                Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )\n+            );\n+\n+            stateTransitions1.clear();\n+            streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            streams2Alpha.setStateListener(\n+                (newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState))\n+            );\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForStateTransition(stateTransitions1, TWO_REBALANCES_RUNNING, stateTransitions2, TWO_REBALANCES_STARTUP);\n+\n+            // in all phases, we write comments that assume that p-0/p-1 are assigned to the first client\n+            // and p-2/p-3 are assigned to the second client (in reality the assignment might be different though)\n+\n+            // phase 2: (write first batch of data)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // p-0: ---> 10 rec + C\n+            // p-1: ---> 10 rec + C\n+            // p-2: ---> 10 rec + C\n+            // p-3: ---> 10 rec + C\n+            final List<KeyValue<Long, Long>> committedInputDataBeforeUpgrade =\n+                prepareData(0L, 10L, 0L, 1L, 2L, 3L);\n+            writeInputData(committedInputDataBeforeUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 4,\n+                MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\"\n+            );\n+\n+            final Map<Long, Long> committedState = new HashMap<>();\n+            final List<KeyValue<Long, Long>> expectedUncommittedResult =\n+                computeExpectedResult(committedInputDataBeforeUpgrade, committedState);\n+            verifyCommitted(expectedUncommittedResult);\n+\n+            // phase 3: (write partial second batch of data)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C ---> 5 rec (pending)\n+            //   p-1: 10 rec + C ---> 5 rec (pending)\n+            //   p-2: 10 rec + C ---> 5 rec (pending)\n+            //   p-3: 10 rec + C ---> 5 rec (pending)\n+            // crash case: (we just assumes that we inject the error for p-0; in reality it might be a different partition)\n+            //   p-0: 10 rec + C ---> 4 rec (pending)\n+            //   p-1: 10 rec + C ---> 5 rec (pending)\n+            //   p-2: 10 rec + C ---> 5 rec (pending)\n+            //   p-3: 10 rec + C ---> 5 rec (pending)\n+            final Set<Long> cleanKeys = mkSet(0L, 1L, 2L, 3L);\n+            final Set<Long> keyFilterFirstClient = keysFromInstance(streams1Alpha);\n+            final long potentiallyFirstFailingKey = keyFilterFirstClient.iterator().next();\n+            cleanKeys.remove(potentiallyFirstFailingKey);\n+\n+            final List<KeyValue<Long, Long>> uncommittedInputDataBeforeFirstUpgrade = new LinkedList<>();\n+            if (!injectError) {\n+                uncommittedInputDataBeforeFirstUpgrade.addAll(\n+                    prepareData(10L, 15L, 0L, 1L, 2L, 3L)\n+                );\n+                writeInputData(uncommittedInputDataBeforeFirstUpgrade);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataBeforeFirstUpgrade, new HashMap<>(committedState))\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+            } else {\n+                final List<KeyValue<Long, Long>> uncommittedInputDataWithoutFailingKey = new LinkedList<>();\n+                for (final long key : cleanKeys) {\n+                    uncommittedInputDataWithoutFailingKey.addAll(prepareData(10L, 15L, key));\n+                }\n+                uncommittedInputDataWithoutFailingKey.addAll(\n+                    prepareData(10L, 14L, potentiallyFirstFailingKey)\n+                );\n+                uncommittedInputDataBeforeFirstUpgrade.addAll(uncommittedInputDataWithoutFailingKey);\n+                writeInputData(uncommittedInputDataWithoutFailingKey);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataWithoutFailingKey, new HashMap<>(committedState))\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+            }\n+\n+            // phase 4: (stop/crash first client)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec ---> C\n+            //   p-1: 10 rec + C + 5 rec ---> C\n+            //   p-2: 10 rec + C + 5 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec (pending)\n+            // crash case:\n+            //   p-0: 10 rec + C + 4 rec ---> A + 5 rec (pending)\n+            //   p-1: 10 rec + C + 5 rec ---> A + 5 rec (pending)\n+            //   p-2: 10 rec + C + 5 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec (pending)\n+            stateTransitions2.clear();\n+            if (!injectError) {\n+                stateTransitions1.clear();\n+                streams1Alpha.close();\n+                waitForStateTransition(stateTransitions1, CLOSE);\n+            } else {\n+                errorInjectedClient1.set(true);\n+\n+                final List<KeyValue<Long, Long>> dataPotentiallyFirstFailingKey =\n+                    prepareData(14L, 15L, potentiallyFirstFailingKey);\n+                uncommittedInputDataBeforeFirstUpgrade.addAll(dataPotentiallyFirstFailingKey);\n+                writeInputData(dataPotentiallyFirstFailingKey);\n+            }\n+            waitForStateTransition(stateTransitions2, SINGLE_REBALANCE);\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataDuringFirstUpgrade =\n+                    uncommittedInputDataBeforeFirstUpgrade\n+                        .stream()\n+                        .filter(pair -> keyFilterFirstClient.contains(pair.key))\n+                        .collect(Collectors.toList());\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResult =\n+                    computeExpectedResult(committedInputDataDuringFirstUpgrade, committedState);\n+                // TODO: if we don't use the custom partitioner, the test hangs here until TX times out and is aborted\n+                verifyCommitted(expectedCommittedResult);\n+            } else {\n+                // retrying TX\n+                expectedUncommittedResult.addAll(computeExpectedResult(\n+                    uncommittedInputDataBeforeFirstUpgrade\n+                        .stream()\n+                        .filter(pair -> keyFilterFirstClient.contains(pair.key))\n+                        .collect(Collectors.toList()),\n+                    new HashMap<>(committedState)\n+                ));\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                errorInjectedClient1.set(false);\n+                stateTransitions1.clear();\n+                streams1Alpha.close();\n+                waitForStateTransition(stateTransitions1, CLOSE_CRASHED);\n+            }\n+\n+            // phase 5: (restart first client)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec ---> C\n+            //   p-3: 10 rec + C + 5 rec ---> C\n+            // crash case:\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec ---> C\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec ---> C\n+            //   p-2: 10 rec + C + 5 rec ---> C\n+            //   p-3: 10 rec + C + 5 rec ---> C\n+            commitRequested.set(0);\n+            stateTransitions1.clear();\n+            stateTransitions2.clear();\n+            streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.start();\n+            waitForStateTransition(stateTransitions1, TWO_REBALANCES_STARTUP, stateTransitions2, TWO_REBALANCES_RUNNING);\n+\n+            final Set<Long> committedKeys = mkSet(0L, 1L, 2L, 3L);\n+            if (!injectError) {\n+                committedKeys.removeAll(keyFilterFirstClient);\n+            }\n+\n+            final List<KeyValue<Long, Long>> expectedCommittedResultAfterRestartFirstClient = computeExpectedResult(\n+                uncommittedInputDataBeforeFirstUpgrade\n+                    .stream()\n+                    .filter(pair -> committedKeys.contains(pair.key))\n+                    .collect(Collectors.toList()),\n+                committedState\n+            );\n+            verifyCommitted(expectedCommittedResultAfterRestartFirstClient);\n+\n+            // phase 6: (complete second batch of data; crash: let second client fail on commit)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczOTQ0Mw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r417739443", "bodyText": "This is the second part of the \"mixed mode\" test: client1 is on eos-beta and client2 is on eoa-alpha. In the first part above, we crashed the second client and restarted it in eos-alpha mode. In this second part, we crash client1 and restart it in eos-beta mode.\nThe actual upgrade continues in the next phase.", "author": "mjsax", "createdAt": "2020-04-30T03:56:49Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,1123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Partitioner;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 100 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> TWO_REBALANCES_STARTUP =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> TWO_REBALANCES_RUNNING =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> SINGLE_REBALANCE =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CLOSE =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.PENDING_SHUTDOWN),\n+                KeyValue.pair(KafkaStreams.State.PENDING_SHUTDOWN, KafkaStreams.State.NOT_RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CRASH =\n+        Collections.unmodifiableList(\n+            Collections.singletonList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.ERROR)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CLOSE_CRASHED =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.ERROR, KafkaStreams.State.PENDING_SHUTDOWN),\n+                KeyValue.pair(KafkaStreams.State.PENDING_SHUTDOWN, KafkaStreams.State.NOT_RUNNING)\n+            )\n+        );\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 4;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+    private final AtomicBoolean errorInjectedClient2 = new AtomicBoolean(false);\n+    private final AtomicBoolean commitErrorInjectedClient1 = new AtomicBoolean(false);\n+    private final AtomicBoolean commitErrorInjectedClient2 = new AtomicBoolean(false);\n+    private final AtomicInteger commitCounterClient1 = new AtomicInteger(-1);\n+    private final AtomicInteger commitCounterClient2 = new AtomicInteger(-1);\n+    private final AtomicInteger commitRequested = new AtomicInteger(0);\n+\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // Additionally, we inject errors while one client is on eos-alpha while the other client is on eos-beta:\n+        // For this case, we inject the error during task commit phase, i.e., after offsets are appended to a TX,\n+        // and before the TX is committed. The goal is to verify that the written but uncommitted offsets are not\n+        // picked up, i.e., GroupCoordinator fencing works correctly.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 4 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - all 4 pending transactions are based on task producers\n+        //      - we will get only 4 pending writes for one partition for the crash case as we crash processing the 5th record\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have two pending transaction\n+        //      - crash case:\n+        //        * the pending transactions of the crashed client got aborted\n+        //        * the second client will have four pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - the rebalance should result in a commit of all tasks\n+        // 6.  write 5 record per input topic partition\n+        //       - stop case:\n+        //         * verify that the result was committed\n+        //       - crash case:\n+        //         * fail the second (i.e., eos-alpha) client during commit\n+        //         * the eos-beta client should not pickup the pending offsets\n+        //         * verify uncommitted and committed result\n+        // 7.  only for crash case:\n+        //     7a. restart the second client in eos-alpha mode and wait until rebalance stabilizes\n+        //     7b. write 10 records per input topic partition\n+        //         * fail the first (i.e., eos-beta) client during commit\n+        //         * the eos-alpha client should not pickup the pending offsets\n+        //         * verify uncommitted and committed result\n+        //     7c. restart the first client in eos-beta mode and wait until rebalance stabilizes\n+        // 8.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - 2 transaction are base on a task producer; one transaction is based on a thread producer\n+        //      - we will get 4 pending writes for the crash case as we crash processing the 5th record\n+        // 9.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the first client will still have one pending transaction\n+        //      - crash case:\n+        //        * the pending transactions of the crashed client got aborted\n+        //        * the first client will have one pending transactions\n+        // 10. restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - the rebalance should result in a commit of all tasks\n+        // 11. write 5 record per input topic partition and verify that the result was committed\n+\n+        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+        KafkaStreams streams1Alpha = null;\n+        KafkaStreams streams1Beta = null;\n+        KafkaStreams streams1BetaTwo = null;\n+\n+        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+        KafkaStreams streams2Alpha = null;\n+        KafkaStreams streams2AlphaTwo = null;\n+        KafkaStreams streams2Beta = null;\n+//        streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA);\n+//        streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+        try {\n+            // phase 1: start both clients\n+            streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            streams1Alpha.setStateListener(\n+                (newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState))\n+            );\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForStateTransition(\n+                stateTransitions1,\n+                Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )\n+            );\n+\n+            stateTransitions1.clear();\n+            streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            streams2Alpha.setStateListener(\n+                (newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState))\n+            );\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForStateTransition(stateTransitions1, TWO_REBALANCES_RUNNING, stateTransitions2, TWO_REBALANCES_STARTUP);\n+\n+            // in all phases, we write comments that assume that p-0/p-1 are assigned to the first client\n+            // and p-2/p-3 are assigned to the second client (in reality the assignment might be different though)\n+\n+            // phase 2: (write first batch of data)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // p-0: ---> 10 rec + C\n+            // p-1: ---> 10 rec + C\n+            // p-2: ---> 10 rec + C\n+            // p-3: ---> 10 rec + C\n+            final List<KeyValue<Long, Long>> committedInputDataBeforeUpgrade =\n+                prepareData(0L, 10L, 0L, 1L, 2L, 3L);\n+            writeInputData(committedInputDataBeforeUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 4,\n+                MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\"\n+            );\n+\n+            final Map<Long, Long> committedState = new HashMap<>();\n+            final List<KeyValue<Long, Long>> expectedUncommittedResult =\n+                computeExpectedResult(committedInputDataBeforeUpgrade, committedState);\n+            verifyCommitted(expectedUncommittedResult);\n+\n+            // phase 3: (write partial second batch of data)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C ---> 5 rec (pending)\n+            //   p-1: 10 rec + C ---> 5 rec (pending)\n+            //   p-2: 10 rec + C ---> 5 rec (pending)\n+            //   p-3: 10 rec + C ---> 5 rec (pending)\n+            // crash case: (we just assumes that we inject the error for p-0; in reality it might be a different partition)\n+            //   p-0: 10 rec + C ---> 4 rec (pending)\n+            //   p-1: 10 rec + C ---> 5 rec (pending)\n+            //   p-2: 10 rec + C ---> 5 rec (pending)\n+            //   p-3: 10 rec + C ---> 5 rec (pending)\n+            final Set<Long> cleanKeys = mkSet(0L, 1L, 2L, 3L);\n+            final Set<Long> keyFilterFirstClient = keysFromInstance(streams1Alpha);\n+            final long potentiallyFirstFailingKey = keyFilterFirstClient.iterator().next();\n+            cleanKeys.remove(potentiallyFirstFailingKey);\n+\n+            final List<KeyValue<Long, Long>> uncommittedInputDataBeforeFirstUpgrade = new LinkedList<>();\n+            if (!injectError) {\n+                uncommittedInputDataBeforeFirstUpgrade.addAll(\n+                    prepareData(10L, 15L, 0L, 1L, 2L, 3L)\n+                );\n+                writeInputData(uncommittedInputDataBeforeFirstUpgrade);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataBeforeFirstUpgrade, new HashMap<>(committedState))\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+            } else {\n+                final List<KeyValue<Long, Long>> uncommittedInputDataWithoutFailingKey = new LinkedList<>();\n+                for (final long key : cleanKeys) {\n+                    uncommittedInputDataWithoutFailingKey.addAll(prepareData(10L, 15L, key));\n+                }\n+                uncommittedInputDataWithoutFailingKey.addAll(\n+                    prepareData(10L, 14L, potentiallyFirstFailingKey)\n+                );\n+                uncommittedInputDataBeforeFirstUpgrade.addAll(uncommittedInputDataWithoutFailingKey);\n+                writeInputData(uncommittedInputDataWithoutFailingKey);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataWithoutFailingKey, new HashMap<>(committedState))\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+            }\n+\n+            // phase 4: (stop/crash first client)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec ---> C\n+            //   p-1: 10 rec + C + 5 rec ---> C\n+            //   p-2: 10 rec + C + 5 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec (pending)\n+            // crash case:\n+            //   p-0: 10 rec + C + 4 rec ---> A + 5 rec (pending)\n+            //   p-1: 10 rec + C + 5 rec ---> A + 5 rec (pending)\n+            //   p-2: 10 rec + C + 5 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec (pending)\n+            stateTransitions2.clear();\n+            if (!injectError) {\n+                stateTransitions1.clear();\n+                streams1Alpha.close();\n+                waitForStateTransition(stateTransitions1, CLOSE);\n+            } else {\n+                errorInjectedClient1.set(true);\n+\n+                final List<KeyValue<Long, Long>> dataPotentiallyFirstFailingKey =\n+                    prepareData(14L, 15L, potentiallyFirstFailingKey);\n+                uncommittedInputDataBeforeFirstUpgrade.addAll(dataPotentiallyFirstFailingKey);\n+                writeInputData(dataPotentiallyFirstFailingKey);\n+            }\n+            waitForStateTransition(stateTransitions2, SINGLE_REBALANCE);\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataDuringFirstUpgrade =\n+                    uncommittedInputDataBeforeFirstUpgrade\n+                        .stream()\n+                        .filter(pair -> keyFilterFirstClient.contains(pair.key))\n+                        .collect(Collectors.toList());\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResult =\n+                    computeExpectedResult(committedInputDataDuringFirstUpgrade, committedState);\n+                // TODO: if we don't use the custom partitioner, the test hangs here until TX times out and is aborted\n+                verifyCommitted(expectedCommittedResult);\n+            } else {\n+                // retrying TX\n+                expectedUncommittedResult.addAll(computeExpectedResult(\n+                    uncommittedInputDataBeforeFirstUpgrade\n+                        .stream()\n+                        .filter(pair -> keyFilterFirstClient.contains(pair.key))\n+                        .collect(Collectors.toList()),\n+                    new HashMap<>(committedState)\n+                ));\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                errorInjectedClient1.set(false);\n+                stateTransitions1.clear();\n+                streams1Alpha.close();\n+                waitForStateTransition(stateTransitions1, CLOSE_CRASHED);\n+            }\n+\n+            // phase 5: (restart first client)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec ---> C\n+            //   p-3: 10 rec + C + 5 rec ---> C\n+            // crash case:\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec ---> C\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec ---> C\n+            //   p-2: 10 rec + C + 5 rec ---> C\n+            //   p-3: 10 rec + C + 5 rec ---> C\n+            commitRequested.set(0);\n+            stateTransitions1.clear();\n+            stateTransitions2.clear();\n+            streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.start();\n+            waitForStateTransition(stateTransitions1, TWO_REBALANCES_STARTUP, stateTransitions2, TWO_REBALANCES_RUNNING);\n+\n+            final Set<Long> committedKeys = mkSet(0L, 1L, 2L, 3L);\n+            if (!injectError) {\n+                committedKeys.removeAll(keyFilterFirstClient);\n+            }\n+\n+            final List<KeyValue<Long, Long>> expectedCommittedResultAfterRestartFirstClient = computeExpectedResult(\n+                uncommittedInputDataBeforeFirstUpgrade\n+                    .stream()\n+                    .filter(pair -> committedKeys.contains(pair.key))\n+                    .collect(Collectors.toList()),\n+                committedState\n+            );\n+            verifyCommitted(expectedCommittedResultAfterRestartFirstClient);\n+\n+            // phase 6: (complete second batch of data; crash: let second client fail on commit)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C ---> 5 rec + C\n+            // crash case:\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec + C ---> 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec + C ---> 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec + C ---> 5 rec + A + 5 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C ---> 5 rec + A + 5 rec + C\n+            commitCounterClient1.set(0);\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataDuringUpgrade =\n+                    prepareData(15L, 20L, 0L, 1L, 2L, 3L);\n+                writeInputData(committedInputDataDuringUpgrade);\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResult =\n+                    computeExpectedResult(committedInputDataDuringUpgrade, committedState);\n+                verifyCommitted(expectedCommittedResult);\n+                expectedUncommittedResult.addAll(expectedCommittedResult);\n+            } else {\n+                final Set<Long> keysFirstClient = keysFromInstance(streams1Beta);\n+                final Set<Long> keysSecondClient = keysFromInstance(streams2Alpha);\n+\n+                final List<KeyValue<Long, Long>> committedInputDataAfterFirstUpgrade =\n+                    prepareData(15L, 20L, keysFirstClient.toArray(new Long[0]));\n+                writeInputData(committedInputDataAfterFirstUpgrade);\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResultBeforeFailure =\n+                    computeExpectedResult(committedInputDataAfterFirstUpgrade, committedState);\n+                verifyCommitted(expectedCommittedResultBeforeFailure);\n+                expectedUncommittedResult.addAll(expectedCommittedResultBeforeFailure);\n+\n+                commitCounterClient2.set(0);\n+\n+                final Iterator<Long> it = keysSecondClient.iterator();\n+                final Long otherKey = it.next();\n+                final Long failingKey = it.next();\n+\n+                final List<KeyValue<Long, Long>> uncommittedInputDataAfterFirstUpgrade =\n+                    prepareData(15L, 19L, keysSecondClient.toArray(new Long[0]));\n+                uncommittedInputDataAfterFirstUpgrade.addAll(prepareData(19L, 20L, otherKey));\n+                writeInputData(uncommittedInputDataAfterFirstUpgrade);\n+\n+                final Map<Long, Long> uncommittedState = new HashMap<>(committedState);\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataAfterFirstUpgrade, uncommittedState)\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                stateTransitions1.clear();\n+                stateTransitions2.clear();\n+                commitCounterClient1.set(0);\n+                commitErrorInjectedClient2.set(true);\n+\n+                final List<KeyValue<Long, Long>> dataFailingKey = prepareData(19L, 20L, failingKey);\n+                uncommittedInputDataAfterFirstUpgrade.addAll(dataFailingKey);\n+                writeInputData(dataFailingKey);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(dataFailingKey, uncommittedState)\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                waitForStateTransition(stateTransitions1, SINGLE_REBALANCE, stateTransitions2, CRASH);\n+\n+                commitErrorInjectedClient2.set(false);\n+                stateTransitions2.clear();\n+                streams2Alpha.close();\n+                waitForStateTransition(stateTransitions2, CLOSE_CRASHED);\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResultAfterFailure =\n+                    computeExpectedResult(uncommittedInputDataAfterFirstUpgrade, committedState);\n+                verifyCommitted(expectedCommittedResultAfterFailure);\n+                expectedUncommittedResult.addAll(expectedCommittedResultAfterFailure);\n+            }\n+\n+            // 7. only for crash case:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczOTg0Mw==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r417739843", "bodyText": "To get the correct observed state transition, we need to add them after waitForCondition failed... Originally, we just called:\nwaitForCondition(\n                () -> observed.equals(expected),\n                MAX_WAIT_TIME_MS,\n                \"Client did not startup on time.\" + observed\n            );\n\nbut this creates the error string with observed when started and thus the error message is miss-leading and useless", "author": "mjsax", "createdAt": "2020-04-30T03:58:54Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,1123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Partitioner;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 100 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> TWO_REBALANCES_STARTUP =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> TWO_REBALANCES_RUNNING =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> SINGLE_REBALANCE =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CLOSE =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.PENDING_SHUTDOWN),\n+                KeyValue.pair(KafkaStreams.State.PENDING_SHUTDOWN, KafkaStreams.State.NOT_RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CRASH =\n+        Collections.unmodifiableList(\n+            Collections.singletonList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.ERROR)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CLOSE_CRASHED =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.ERROR, KafkaStreams.State.PENDING_SHUTDOWN),\n+                KeyValue.pair(KafkaStreams.State.PENDING_SHUTDOWN, KafkaStreams.State.NOT_RUNNING)\n+            )\n+        );\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 4;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+    private final AtomicBoolean errorInjectedClient2 = new AtomicBoolean(false);\n+    private final AtomicBoolean commitErrorInjectedClient1 = new AtomicBoolean(false);\n+    private final AtomicBoolean commitErrorInjectedClient2 = new AtomicBoolean(false);\n+    private final AtomicInteger commitCounterClient1 = new AtomicInteger(-1);\n+    private final AtomicInteger commitCounterClient2 = new AtomicInteger(-1);\n+    private final AtomicInteger commitRequested = new AtomicInteger(0);\n+\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // Additionally, we inject errors while one client is on eos-alpha while the other client is on eos-beta:\n+        // For this case, we inject the error during task commit phase, i.e., after offsets are appended to a TX,\n+        // and before the TX is committed. The goal is to verify that the written but uncommitted offsets are not\n+        // picked up, i.e., GroupCoordinator fencing works correctly.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 4 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - all 4 pending transactions are based on task producers\n+        //      - we will get only 4 pending writes for one partition for the crash case as we crash processing the 5th record\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have two pending transaction\n+        //      - crash case:\n+        //        * the pending transactions of the crashed client got aborted\n+        //        * the second client will have four pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - the rebalance should result in a commit of all tasks\n+        // 6.  write 5 record per input topic partition\n+        //       - stop case:\n+        //         * verify that the result was committed\n+        //       - crash case:\n+        //         * fail the second (i.e., eos-alpha) client during commit\n+        //         * the eos-beta client should not pickup the pending offsets\n+        //         * verify uncommitted and committed result\n+        // 7.  only for crash case:\n+        //     7a. restart the second client in eos-alpha mode and wait until rebalance stabilizes\n+        //     7b. write 10 records per input topic partition\n+        //         * fail the first (i.e., eos-beta) client during commit\n+        //         * the eos-alpha client should not pickup the pending offsets\n+        //         * verify uncommitted and committed result\n+        //     7c. restart the first client in eos-beta mode and wait until rebalance stabilizes\n+        // 8.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - 2 transaction are base on a task producer; one transaction is based on a thread producer\n+        //      - we will get 4 pending writes for the crash case as we crash processing the 5th record\n+        // 9.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the first client will still have one pending transaction\n+        //      - crash case:\n+        //        * the pending transactions of the crashed client got aborted\n+        //        * the first client will have one pending transactions\n+        // 10. restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - the rebalance should result in a commit of all tasks\n+        // 11. write 5 record per input topic partition and verify that the result was committed\n+\n+        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+        KafkaStreams streams1Alpha = null;\n+        KafkaStreams streams1Beta = null;\n+        KafkaStreams streams1BetaTwo = null;\n+\n+        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+        KafkaStreams streams2Alpha = null;\n+        KafkaStreams streams2AlphaTwo = null;\n+        KafkaStreams streams2Beta = null;\n+//        streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA);\n+//        streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+        try {\n+            // phase 1: start both clients\n+            streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            streams1Alpha.setStateListener(\n+                (newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState))\n+            );\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForStateTransition(\n+                stateTransitions1,\n+                Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )\n+            );\n+\n+            stateTransitions1.clear();\n+            streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            streams2Alpha.setStateListener(\n+                (newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState))\n+            );\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForStateTransition(stateTransitions1, TWO_REBALANCES_RUNNING, stateTransitions2, TWO_REBALANCES_STARTUP);\n+\n+            // in all phases, we write comments that assume that p-0/p-1 are assigned to the first client\n+            // and p-2/p-3 are assigned to the second client (in reality the assignment might be different though)\n+\n+            // phase 2: (write first batch of data)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // p-0: ---> 10 rec + C\n+            // p-1: ---> 10 rec + C\n+            // p-2: ---> 10 rec + C\n+            // p-3: ---> 10 rec + C\n+            final List<KeyValue<Long, Long>> committedInputDataBeforeUpgrade =\n+                prepareData(0L, 10L, 0L, 1L, 2L, 3L);\n+            writeInputData(committedInputDataBeforeUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 4,\n+                MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\"\n+            );\n+\n+            final Map<Long, Long> committedState = new HashMap<>();\n+            final List<KeyValue<Long, Long>> expectedUncommittedResult =\n+                computeExpectedResult(committedInputDataBeforeUpgrade, committedState);\n+            verifyCommitted(expectedUncommittedResult);\n+\n+            // phase 3: (write partial second batch of data)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C ---> 5 rec (pending)\n+            //   p-1: 10 rec + C ---> 5 rec (pending)\n+            //   p-2: 10 rec + C ---> 5 rec (pending)\n+            //   p-3: 10 rec + C ---> 5 rec (pending)\n+            // crash case: (we just assumes that we inject the error for p-0; in reality it might be a different partition)\n+            //   p-0: 10 rec + C ---> 4 rec (pending)\n+            //   p-1: 10 rec + C ---> 5 rec (pending)\n+            //   p-2: 10 rec + C ---> 5 rec (pending)\n+            //   p-3: 10 rec + C ---> 5 rec (pending)\n+            final Set<Long> cleanKeys = mkSet(0L, 1L, 2L, 3L);\n+            final Set<Long> keyFilterFirstClient = keysFromInstance(streams1Alpha);\n+            final long potentiallyFirstFailingKey = keyFilterFirstClient.iterator().next();\n+            cleanKeys.remove(potentiallyFirstFailingKey);\n+\n+            final List<KeyValue<Long, Long>> uncommittedInputDataBeforeFirstUpgrade = new LinkedList<>();\n+            if (!injectError) {\n+                uncommittedInputDataBeforeFirstUpgrade.addAll(\n+                    prepareData(10L, 15L, 0L, 1L, 2L, 3L)\n+                );\n+                writeInputData(uncommittedInputDataBeforeFirstUpgrade);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataBeforeFirstUpgrade, new HashMap<>(committedState))\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+            } else {\n+                final List<KeyValue<Long, Long>> uncommittedInputDataWithoutFailingKey = new LinkedList<>();\n+                for (final long key : cleanKeys) {\n+                    uncommittedInputDataWithoutFailingKey.addAll(prepareData(10L, 15L, key));\n+                }\n+                uncommittedInputDataWithoutFailingKey.addAll(\n+                    prepareData(10L, 14L, potentiallyFirstFailingKey)\n+                );\n+                uncommittedInputDataBeforeFirstUpgrade.addAll(uncommittedInputDataWithoutFailingKey);\n+                writeInputData(uncommittedInputDataWithoutFailingKey);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataWithoutFailingKey, new HashMap<>(committedState))\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+            }\n+\n+            // phase 4: (stop/crash first client)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec ---> C\n+            //   p-1: 10 rec + C + 5 rec ---> C\n+            //   p-2: 10 rec + C + 5 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec (pending)\n+            // crash case:\n+            //   p-0: 10 rec + C + 4 rec ---> A + 5 rec (pending)\n+            //   p-1: 10 rec + C + 5 rec ---> A + 5 rec (pending)\n+            //   p-2: 10 rec + C + 5 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec (pending)\n+            stateTransitions2.clear();\n+            if (!injectError) {\n+                stateTransitions1.clear();\n+                streams1Alpha.close();\n+                waitForStateTransition(stateTransitions1, CLOSE);\n+            } else {\n+                errorInjectedClient1.set(true);\n+\n+                final List<KeyValue<Long, Long>> dataPotentiallyFirstFailingKey =\n+                    prepareData(14L, 15L, potentiallyFirstFailingKey);\n+                uncommittedInputDataBeforeFirstUpgrade.addAll(dataPotentiallyFirstFailingKey);\n+                writeInputData(dataPotentiallyFirstFailingKey);\n+            }\n+            waitForStateTransition(stateTransitions2, SINGLE_REBALANCE);\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataDuringFirstUpgrade =\n+                    uncommittedInputDataBeforeFirstUpgrade\n+                        .stream()\n+                        .filter(pair -> keyFilterFirstClient.contains(pair.key))\n+                        .collect(Collectors.toList());\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResult =\n+                    computeExpectedResult(committedInputDataDuringFirstUpgrade, committedState);\n+                // TODO: if we don't use the custom partitioner, the test hangs here until TX times out and is aborted\n+                verifyCommitted(expectedCommittedResult);\n+            } else {\n+                // retrying TX\n+                expectedUncommittedResult.addAll(computeExpectedResult(\n+                    uncommittedInputDataBeforeFirstUpgrade\n+                        .stream()\n+                        .filter(pair -> keyFilterFirstClient.contains(pair.key))\n+                        .collect(Collectors.toList()),\n+                    new HashMap<>(committedState)\n+                ));\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                errorInjectedClient1.set(false);\n+                stateTransitions1.clear();\n+                streams1Alpha.close();\n+                waitForStateTransition(stateTransitions1, CLOSE_CRASHED);\n+            }\n+\n+            // phase 5: (restart first client)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec ---> C\n+            //   p-3: 10 rec + C + 5 rec ---> C\n+            // crash case:\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec ---> C\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec ---> C\n+            //   p-2: 10 rec + C + 5 rec ---> C\n+            //   p-3: 10 rec + C + 5 rec ---> C\n+            commitRequested.set(0);\n+            stateTransitions1.clear();\n+            stateTransitions2.clear();\n+            streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.start();\n+            waitForStateTransition(stateTransitions1, TWO_REBALANCES_STARTUP, stateTransitions2, TWO_REBALANCES_RUNNING);\n+\n+            final Set<Long> committedKeys = mkSet(0L, 1L, 2L, 3L);\n+            if (!injectError) {\n+                committedKeys.removeAll(keyFilterFirstClient);\n+            }\n+\n+            final List<KeyValue<Long, Long>> expectedCommittedResultAfterRestartFirstClient = computeExpectedResult(\n+                uncommittedInputDataBeforeFirstUpgrade\n+                    .stream()\n+                    .filter(pair -> committedKeys.contains(pair.key))\n+                    .collect(Collectors.toList()),\n+                committedState\n+            );\n+            verifyCommitted(expectedCommittedResultAfterRestartFirstClient);\n+\n+            // phase 6: (complete second batch of data; crash: let second client fail on commit)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C ---> 5 rec + C\n+            // crash case:\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec + C ---> 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec + C ---> 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec + C ---> 5 rec + A + 5 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C ---> 5 rec + A + 5 rec + C\n+            commitCounterClient1.set(0);\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataDuringUpgrade =\n+                    prepareData(15L, 20L, 0L, 1L, 2L, 3L);\n+                writeInputData(committedInputDataDuringUpgrade);\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResult =\n+                    computeExpectedResult(committedInputDataDuringUpgrade, committedState);\n+                verifyCommitted(expectedCommittedResult);\n+                expectedUncommittedResult.addAll(expectedCommittedResult);\n+            } else {\n+                final Set<Long> keysFirstClient = keysFromInstance(streams1Beta);\n+                final Set<Long> keysSecondClient = keysFromInstance(streams2Alpha);\n+\n+                final List<KeyValue<Long, Long>> committedInputDataAfterFirstUpgrade =\n+                    prepareData(15L, 20L, keysFirstClient.toArray(new Long[0]));\n+                writeInputData(committedInputDataAfterFirstUpgrade);\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResultBeforeFailure =\n+                    computeExpectedResult(committedInputDataAfterFirstUpgrade, committedState);\n+                verifyCommitted(expectedCommittedResultBeforeFailure);\n+                expectedUncommittedResult.addAll(expectedCommittedResultBeforeFailure);\n+\n+                commitCounterClient2.set(0);\n+\n+                final Iterator<Long> it = keysSecondClient.iterator();\n+                final Long otherKey = it.next();\n+                final Long failingKey = it.next();\n+\n+                final List<KeyValue<Long, Long>> uncommittedInputDataAfterFirstUpgrade =\n+                    prepareData(15L, 19L, keysSecondClient.toArray(new Long[0]));\n+                uncommittedInputDataAfterFirstUpgrade.addAll(prepareData(19L, 20L, otherKey));\n+                writeInputData(uncommittedInputDataAfterFirstUpgrade);\n+\n+                final Map<Long, Long> uncommittedState = new HashMap<>(committedState);\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataAfterFirstUpgrade, uncommittedState)\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                stateTransitions1.clear();\n+                stateTransitions2.clear();\n+                commitCounterClient1.set(0);\n+                commitErrorInjectedClient2.set(true);\n+\n+                final List<KeyValue<Long, Long>> dataFailingKey = prepareData(19L, 20L, failingKey);\n+                uncommittedInputDataAfterFirstUpgrade.addAll(dataFailingKey);\n+                writeInputData(dataFailingKey);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(dataFailingKey, uncommittedState)\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                waitForStateTransition(stateTransitions1, SINGLE_REBALANCE, stateTransitions2, CRASH);\n+\n+                commitErrorInjectedClient2.set(false);\n+                stateTransitions2.clear();\n+                streams2Alpha.close();\n+                waitForStateTransition(stateTransitions2, CLOSE_CRASHED);\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResultAfterFailure =\n+                    computeExpectedResult(uncommittedInputDataAfterFirstUpgrade, committedState);\n+                verifyCommitted(expectedCommittedResultAfterFailure);\n+                expectedUncommittedResult.addAll(expectedCommittedResultAfterFailure);\n+            }\n+\n+            // 7. only for crash case:\n+            //     7a. restart the second client in eos-alpha mode and wait until rebalance stabilizes\n+            //     7b. write third batch of input data\n+            //         * fail the first (i.e., eos-beta) client during commit\n+            //         * the eos-alpha client should not pickup the pending offsets\n+            //         * verify uncommitted and committed result\n+            //     7c. restart the first client in eos-beta mode and wait until rebalance stabilizes\n+            //\n+            // crash case:\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec + C + 5 rec + C ---> 10 rec + A + 10 rec + C\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec + C + 5 rec + C ---> 10 rec + A + 10 rec + C\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C ---> 10 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C ---> 10 rec + C\n+            if (!injectError) {\n+                streams2AlphaTwo = streams2Alpha;\n+            } else {\n+                // 7a restart the second client in eos-alpha mode and wait until rebalance stabilizes\n+                commitCounterClient1.set(0);\n+                commitCounterClient2.set(-1);\n+                stateTransitions1.clear();\n+                stateTransitions2.clear();\n+                streams2AlphaTwo = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+                streams2AlphaTwo.setStateListener(\n+                    (newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState))\n+                );\n+                streams2AlphaTwo.start();\n+                waitForStateTransition(stateTransitions1, TWO_REBALANCES_RUNNING, stateTransitions2, TWO_REBALANCES_STARTUP);\n+\n+                // 7b. write third batch of input data\n+                final Set<Long> keysFirstClient = keysFromInstance(streams1Beta);\n+                final Set<Long> keysSecondClient = keysFromInstance(streams2AlphaTwo);\n+\n+                final List<KeyValue<Long, Long>> committedInputDataBetweenUpgrades =\n+                    prepareData(20L, 30L, keysSecondClient.toArray(new Long[0]));\n+                writeInputData(committedInputDataBetweenUpgrades);\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResultBeforeFailure =\n+                    computeExpectedResult(committedInputDataBetweenUpgrades, committedState);\n+                verifyCommitted(expectedCommittedResultBeforeFailure);\n+                expectedUncommittedResult.addAll(expectedCommittedResultBeforeFailure);\n+\n+                commitCounterClient2.set(0);\n+\n+                final Iterator<Long> it = keysFirstClient.iterator();\n+                final Long otherKey = it.next();\n+                final Long failingKey = it.next();\n+\n+                final List<KeyValue<Long, Long>> uncommittedInputDataBetweenUpgrade =\n+                    prepareData(20L, 29L, keysFirstClient.toArray(new Long[0]));\n+                uncommittedInputDataBetweenUpgrade.addAll(prepareData(29L, 30L, otherKey));\n+                writeInputData(uncommittedInputDataBetweenUpgrade);\n+\n+                final Map<Long, Long> uncommittedState = new HashMap<>(committedState);\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataBetweenUpgrade, uncommittedState)\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                stateTransitions1.clear();\n+                stateTransitions2.clear();\n+                commitCounterClient2.set(0);\n+                commitErrorInjectedClient1.set(true);\n+\n+                final List<KeyValue<Long, Long>> dataFailingKey = prepareData(29L, 30L, failingKey);\n+                uncommittedInputDataBetweenUpgrade.addAll(dataFailingKey);\n+                writeInputData(dataFailingKey);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(dataFailingKey, uncommittedState)\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                waitForStateTransition(stateTransitions1, CRASH, stateTransitions2, SINGLE_REBALANCE);\n+\n+                commitErrorInjectedClient1.set(false);\n+                stateTransitions1.clear();\n+                streams1Beta.close();\n+                waitForStateTransition(stateTransitions1, CLOSE_CRASHED);\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResultAfterFailure =\n+                    computeExpectedResult(uncommittedInputDataBetweenUpgrade, committedState);\n+                verifyCommitted(expectedCommittedResultAfterFailure);\n+                expectedUncommittedResult.addAll(expectedCommittedResultAfterFailure);\n+\n+                // 7c. restart the first client in eos-beta mode and wait until rebalance stabilizes\n+                stateTransitions1.clear();\n+                stateTransitions2.clear();\n+                streams1BetaTwo = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+                streams1BetaTwo.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+                streams1BetaTwo.start();\n+                waitForStateTransition(stateTransitions1, TWO_REBALANCES_STARTUP, stateTransitions2, TWO_REBALANCES_RUNNING);\n+            }\n+\n+            // phase 8: (write partial fourth batch of data)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C + 5 rec + C ---> 5 rec (pending)\n+            //   p-1: 10 rec + C + 5 rec + C + 5 rec + C ---> 5 rec (pending)\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + C ---> 5 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + C ---> 5 rec (pending)\n+            // crash case:  (we just assumes that we inject the error for p-2; in reality it might be a different partition)\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C ---> 5 rec (pending)\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C ---> 5 rec (pending)\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C ---> 4 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C ---> 5 rec (pending)\n+            cleanKeys.addAll(mkSet(0L, 1L, 2L, 3L));\n+            final Set<Long> keyFilterSecondClient = keysFromInstance(streams2AlphaTwo);\n+            final long potentiallySecondFailingKey = keyFilterSecondClient.iterator().next();\n+            cleanKeys.remove(potentiallySecondFailingKey);\n+\n+            final List<KeyValue<Long, Long>> uncommittedInputDataBeforeSecondUpgrade = new LinkedList<>();\n+            if (!injectError) {\n+                uncommittedInputDataBeforeSecondUpgrade.addAll(\n+                    prepareData(30L, 35L, 0L, 1L, 2L, 3L)\n+                );\n+                writeInputData(uncommittedInputDataBeforeSecondUpgrade);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataBeforeSecondUpgrade, new HashMap<>(committedState))\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+            } else {\n+                final List<KeyValue<Long, Long>> uncommittedInputDataWithoutFailingKey = new LinkedList<>();\n+                for (final long key : cleanKeys) {\n+                    uncommittedInputDataWithoutFailingKey.addAll(prepareData(30L, 35L, key));\n+                }\n+                uncommittedInputDataWithoutFailingKey.addAll(\n+                    prepareData(30L, 34L, potentiallySecondFailingKey)\n+                );\n+                uncommittedInputDataBeforeSecondUpgrade.addAll(uncommittedInputDataWithoutFailingKey);\n+                writeInputData(uncommittedInputDataWithoutFailingKey);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataWithoutFailingKey, new HashMap<>(committedState))\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+            }\n+\n+            // phase 9: (stop/crash second client)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec (pending)\n+            //   p-1: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec (pending)\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec ---> C\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec ---> C\n+            // crash case:  (we just assumes that we inject the error for p-2; in reality it might be a different partition)\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C + 5 rec (pending)\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C + 5 rec (pending)\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C + 4 rec ---> A + 5 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C + 5 rec ---> A + 5 rec (pending)\n+            stateTransitions1.clear();\n+            if (!injectError) {\n+                stateTransitions2.clear();\n+                streams2AlphaTwo.close();\n+                waitForStateTransition(stateTransitions2, CLOSE);\n+            } else {\n+                errorInjectedClient2.set(true);\n+\n+                final List<KeyValue<Long, Long>> dataPotentiallySecondFailingKey =\n+                    prepareData(34L, 35L, potentiallySecondFailingKey);\n+                uncommittedInputDataBeforeSecondUpgrade.addAll(dataPotentiallySecondFailingKey);\n+                writeInputData(dataPotentiallySecondFailingKey);\n+            }\n+            waitForStateTransition(stateTransitions1, SINGLE_REBALANCE);\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataDuringSecondUpgrade =\n+                    uncommittedInputDataBeforeSecondUpgrade\n+                        .stream()\n+                        .filter(pair -> keyFilterSecondClient.contains(pair.key))\n+                        .collect(Collectors.toList());\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResult =\n+                    computeExpectedResult(committedInputDataDuringSecondUpgrade, committedState);\n+                verifyCommitted(expectedCommittedResult);\n+            } else {\n+                // retrying TX\n+                expectedUncommittedResult.addAll(computeExpectedResult(\n+                    uncommittedInputDataBeforeSecondUpgrade\n+                        .stream()\n+                        .filter(pair -> keyFilterSecondClient.contains(pair.key))\n+                        .collect(Collectors.toList()),\n+                    new HashMap<>(committedState)\n+                ));\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                errorInjectedClient2.set(false);\n+                stateTransitions2.clear();\n+                streams2AlphaTwo.close();\n+                waitForStateTransition(stateTransitions2, CLOSE_CRASHED);\n+            }\n+\n+            // phase 10: (restart second client)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // the state below indicate the case for which the \"original\" tasks of client2 are migrated back to client2\n+            // if a task \"switch\" happens, we might get additional commits (omitted in the comment for brevity)\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec ---> C\n+            //   p-1: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec ---> C\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec + C\n+            // crash case:  (we just assumes that we inject the error for p-2; in reality it might be a different partition)\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C + 5 rec ---> C\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C + 5 rec ---> C\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C + 4 rec + A + 5 rec ---> C\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C + 5 rec + A + 5 rec ---> C\n+            commitRequested.set(0);\n+            stateTransitions1.clear();\n+            stateTransitions2.clear();\n+            streams2Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            streams2Beta.setStateListener(\n+                (newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState))\n+            );\n+            streams2Beta.start();\n+            waitForStateTransition(stateTransitions1, TWO_REBALANCES_RUNNING, stateTransitions2, TWO_REBALANCES_STARTUP);\n+\n+            committedKeys.addAll(mkSet(0L, 1L, 2L, 3L));\n+            if (!injectError) {\n+                committedKeys.removeAll(keyFilterSecondClient);\n+            }\n+\n+            final List<KeyValue<Long, Long>> expectedCommittedResultAfterRestartSecondClient = computeExpectedResult(\n+                uncommittedInputDataBeforeSecondUpgrade\n+                    .stream()\n+                    .filter(pair -> committedKeys.contains(pair.key))\n+                    .collect(Collectors.toList()),\n+                committedState\n+            );\n+            verifyCommitted(expectedCommittedResultAfterRestartSecondClient);\n+\n+            // phase 11: (complete fourth batch of data)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec + C ---> 5 rec + C\n+            // crash case:  (we just assumes that we inject the error for p-2; in reality it might be a different partition)\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C + 4 rec + A + 5 rec + C ---> 5 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C + 5 rec + A + 5 rec + C ---> 5 rec + C\n+            commitCounterClient1.set(-1);\n+            commitCounterClient2.set(-1);\n+\n+            final List<KeyValue<Long, Long>> committedInputDataAfterUpgrade =\n+                prepareData(35L, 40L, 0L, 1L, 2L, 3L);\n+            writeInputData(committedInputDataAfterUpgrade);\n+\n+            final List<KeyValue<Long, Long>> expectedCommittedResult =\n+                computeExpectedResult(committedInputDataAfterUpgrade, committedState);\n+            verifyCommitted(expectedCommittedResult);\n+        } finally {\n+            if (streams1Alpha != null) {\n+                streams1Alpha.close();\n+            }\n+            if (streams1Beta != null) {\n+                streams1Beta.close();\n+            }\n+            if (streams1BetaTwo != null) {\n+                streams1BetaTwo.close();\n+            }\n+            if (streams2Alpha != null) {\n+                streams2Alpha.close();\n+            }\n+            if (streams2AlphaTwo != null) {\n+                streams2AlphaTwo.close();\n+            }\n+            if (streams2Beta != null) {\n+                streams2Beta.close();\n+            }\n+        }\n+    }\n+\n+    private KafkaStreams getKafkaStreams(final String appDir,\n+                                         final String processingGuarantee) {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+\n+        final String[] storeNames = new String[] {storeName};\n+        final StoreBuilder<KeyValueStore<Long, Long>> storeBuilder = Stores\n+            .keyValueStoreBuilder(Stores.persistentKeyValueStore(storeName), Serdes.Long(), Serdes.Long())\n+            .withCachingEnabled();\n+\n+        builder.addStateStore(storeBuilder);\n+\n+        final KStream<Long, Long> input = builder.stream(MULTI_PARTITION_INPUT_TOPIC);\n+        input.transform(new TransformerSupplier<Long, Long, KeyValue<Long, Long>>() {\n+            @SuppressWarnings(\"unchecked\")\n+            @Override\n+            public Transformer<Long, Long, KeyValue<Long, Long>> get() {\n+                return new Transformer<Long, Long, KeyValue<Long, Long>>() {\n+                    ProcessorContext<Object, Object> context;\n+                    KeyValueStore<Long, Long> state = null;\n+                    AtomicBoolean crash;\n+                    AtomicInteger sharedCommit;\n+\n+                    @Override\n+                    public void init(final ProcessorContext<Object, Object> context) {\n+                        this.context = context;\n+                        state = (KeyValueStore<Long, Long>) context.getStateStore(storeName);\n+                        final String clientId = context.appConfigs().get(StreamsConfig.CLIENT_ID_CONFIG).toString();\n+                        if (\"appDir1\".equals(clientId)) {\n+                            crash = errorInjectedClient1;\n+                            sharedCommit = commitCounterClient1;\n+                        } else {\n+                            crash = errorInjectedClient2;\n+                            sharedCommit = commitCounterClient2;\n+                        }\n+                    }\n+\n+                    @Override\n+                    public KeyValue<Long, Long> transform(final Long key, final Long value) {\n+                        if ((value + 1) % 10 == 0) {\n+                            if (sharedCommit.get() < 0 ||\n+                                sharedCommit.incrementAndGet() == 2) {\n+\n+                                context.commit();\n+                            }\n+                            commitRequested.incrementAndGet();\n+                        }\n+\n+                        Long sum = state.get(key);\n+                        if (sum == null) {\n+                            sum = value;\n+                        } else {\n+                            sum += value;\n+                        }\n+                        state.put(key, sum);\n+                        state.flush();\n+\n+                        if (value % 10 == 4 && // potentially crash when processing 5th, 15th, or 25th record (etc.)\n+                            crash != null && crash.compareAndSet(true, false)) {\n+                            // only crash a single task\n+                            throw new RuntimeException(\"Injected test exception.\");\n+                        }\n+\n+                        return new KeyValue<>(key, state.get(key));\n+                    }\n+\n+                    @Override\n+                    public void close() { }\n+                };\n+            } }, storeNames)\n+            .to(MULTI_PARTITION_OUTPUT_TOPIC);\n+\n+        final Properties properties = new Properties();\n+        properties.put(StreamsConfig.CLIENT_ID_CONFIG, appDir);\n+        properties.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, processingGuarantee);\n+        properties.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, Long.MAX_VALUE);\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.METADATA_MAX_AGE_CONFIG), \"1000\");\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), \"earliest\");\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG), 5 * 1000);\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG), 5 * 1000 - 1);\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG), MAX_POLL_INTERVAL_MS);\n+        // TODO\n+        //   if we don't use this custom partitioner the test fails for the non-error case\n+        //   unclear why -- see other TODO\n+        properties.put(StreamsConfig.producerPrefix(ProducerConfig.PARTITIONER_CLASS_CONFIG), KeyPartitioner.class);\n+        properties.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+        properties.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath() + File.separator + appDir);\n+\n+        final Properties config = StreamsTestUtils.getStreamsConfig(\n+            applicationId,\n+            CLUSTER.bootstrapServers(),\n+            Serdes.LongSerde.class.getName(),\n+            Serdes.LongSerde.class.getName(),\n+            properties\n+        );\n+\n+        final KafkaStreams streams = new KafkaStreams(builder.build(), config, new TestKafkaClientSupplier());\n+\n+        streams.setUncaughtExceptionHandler((t, e) -> {\n+            if (uncaughtException != null) {\n+                e.printStackTrace(System.err);\n+                fail(\"Should only get one uncaught exception from Streams.\");\n+            }\n+            uncaughtException = e;\n+        });\n+\n+        return streams;\n+    }\n+\n+    private void waitForStateTransition(final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> observed,\n+                                        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> expected)\n+        throws Exception {\n+\n+        try {\n+            waitForCondition(\n+                () -> observed.equals(expected),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not startup on time.\"\n+            );\n+        } catch (final AssertionError error) {\n+            final AssertionError newError = new AssertionError(\"Client transitions: \" + observed);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzc0MDI5MA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r417740290", "bodyText": "With 4 partitions, we need a custom partitioner to make sure we write the 4 different keys into 4 different partitions -- the default partitioner would only write data to 2 partitions (this behavior, ie, empty partitions vs. non-empty partitions, seems to be related to the bug when the test fails -- ensuring that data is written into all partitions avoids the issue).", "author": "mjsax", "createdAt": "2020-04-30T04:00:48Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosBetaUpgradeIntegrationTest.java", "diffHunk": "@@ -0,0 +1,1123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Partitioner;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(Parameterized.class)\n+@Category({IntegrationTest.class})\n+public class EosBetaUpgradeIntegrationTest {\n+\n+    @Parameterized.Parameters(name = \"{0}\")\n+    public static Collection<Boolean[]> data() {\n+        return Arrays.asList(new Boolean[][] {\n+            {false},\n+            {true}\n+        });\n+    }\n+\n+    @Parameterized.Parameter\n+    public boolean injectError;\n+\n+    private static final int NUM_BROKERS = 3;\n+    private static final int MAX_POLL_INTERVAL_MS = 100 * 1000;\n+    private static final int MAX_WAIT_TIME_MS = 60 * 1000;\n+\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> TWO_REBALANCES_STARTUP =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> TWO_REBALANCES_RUNNING =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING),\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> SINGLE_REBALANCE =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.REBALANCING),\n+                KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CLOSE =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.PENDING_SHUTDOWN),\n+                KeyValue.pair(KafkaStreams.State.PENDING_SHUTDOWN, KafkaStreams.State.NOT_RUNNING)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CRASH =\n+        Collections.unmodifiableList(\n+            Collections.singletonList(\n+                KeyValue.pair(KafkaStreams.State.RUNNING, KafkaStreams.State.ERROR)\n+            )\n+        );\n+    private static final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> CLOSE_CRASHED =\n+        Collections.unmodifiableList(\n+            Arrays.asList(\n+                KeyValue.pair(KafkaStreams.State.ERROR, KafkaStreams.State.PENDING_SHUTDOWN),\n+                KeyValue.pair(KafkaStreams.State.PENDING_SHUTDOWN, KafkaStreams.State.NOT_RUNNING)\n+            )\n+        );\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(\n+        NUM_BROKERS,\n+        Utils.mkProperties(Collections.singletonMap(\"auto.create.topics.enable\", \"false\"))\n+    );\n+\n+    private static String applicationId;\n+    private final static int NUM_TOPIC_PARTITIONS = 4;\n+    private final static String CONSUMER_GROUP_ID = \"readCommitted\";\n+    private final static String MULTI_PARTITION_INPUT_TOPIC = \"multiPartitionInputTopic\";\n+    private final static String MULTI_PARTITION_OUTPUT_TOPIC = \"multiPartitionOutputTopic\";\n+    private final String storeName = \"store\";\n+\n+    private final AtomicBoolean errorInjectedClient1 = new AtomicBoolean(false);\n+    private final AtomicBoolean errorInjectedClient2 = new AtomicBoolean(false);\n+    private final AtomicBoolean commitErrorInjectedClient1 = new AtomicBoolean(false);\n+    private final AtomicBoolean commitErrorInjectedClient2 = new AtomicBoolean(false);\n+    private final AtomicInteger commitCounterClient1 = new AtomicInteger(-1);\n+    private final AtomicInteger commitCounterClient2 = new AtomicInteger(-1);\n+    private final AtomicInteger commitRequested = new AtomicInteger(0);\n+\n+    private Throwable uncaughtException;\n+\n+    private int testNumber = 0;\n+\n+    @Before\n+    public void createTopics() throws Exception {\n+        applicationId = \"appId-\" + ++testNumber;\n+        CLUSTER.deleteTopicsAndWait(\n+            MULTI_PARTITION_INPUT_TOPIC,\n+            MULTI_PARTITION_OUTPUT_TOPIC,\n+            applicationId + \"-\" + storeName + \"-changelog\"\n+        );\n+\n+        CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+        CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC, NUM_TOPIC_PARTITIONS, 1);\n+    }\n+\n+    @Test\n+    public void shouldUpgradeFromEosAlphaToEosBeta() throws Exception {\n+        // We use two KafkaStreams clients that we upgrade from eos-alpha to eos-beta. During the upgrade,\n+        // we ensure that there are pending transaction and verify that data is processed correctly.\n+        //\n+        // We either close clients cleanly (`injectError = false`) or let them crash (`injectError = true`) during\n+        // the upgrade. For both cases, EOS should not be violated.\n+        //\n+        // Additionally, we inject errors while one client is on eos-alpha while the other client is on eos-beta:\n+        // For this case, we inject the error during task commit phase, i.e., after offsets are appended to a TX,\n+        // and before the TX is committed. The goal is to verify that the written but uncommitted offsets are not\n+        // picked up, i.e., GroupCoordinator fencing works correctly.\n+        //\n+        // The commit interval is set to MAX_VALUE and the used `Processor` request commits manually so we have full\n+        // control when a commit actually happens. We use an input topic with 4 partitions and each task will request\n+        // a commit after processing 10 records.\n+        //\n+        // 1.  start both clients and wait until rebalance stabilizes\n+        // 2.  write 10 records per input topic partition and verify that the result was committed\n+        // 3.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - all 4 pending transactions are based on task producers\n+        //      - we will get only 4 pending writes for one partition for the crash case as we crash processing the 5th record\n+        // 4.  stop/crash the first client, wait until rebalance stabilizes:\n+        //      - stop case:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the second client will still have two pending transaction\n+        //      - crash case:\n+        //        * the pending transactions of the crashed client got aborted\n+        //        * the second client will have four pending transactions\n+        // 5.  restart the first client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - the rebalance should result in a commit of all tasks\n+        // 6.  write 5 record per input topic partition\n+        //       - stop case:\n+        //         * verify that the result was committed\n+        //       - crash case:\n+        //         * fail the second (i.e., eos-alpha) client during commit\n+        //         * the eos-beta client should not pickup the pending offsets\n+        //         * verify uncommitted and committed result\n+        // 7.  only for crash case:\n+        //     7a. restart the second client in eos-alpha mode and wait until rebalance stabilizes\n+        //     7b. write 10 records per input topic partition\n+        //         * fail the first (i.e., eos-beta) client during commit\n+        //         * the eos-alpha client should not pickup the pending offsets\n+        //         * verify uncommitted and committed result\n+        //     7c. restart the first client in eos-beta mode and wait until rebalance stabilizes\n+        // 8.  write 5 records per input topic partition to get pending transactions (verified via \"read_uncommitted\" mode)\n+        //      - 2 transaction are base on a task producer; one transaction is based on a thread producer\n+        //      - we will get 4 pending writes for the crash case as we crash processing the 5th record\n+        // 9.  stop/crash the second client and wait until rebalance stabilizes:\n+        //      - stop only:\n+        //        * verify that the stopped client did commit its pending transaction during shutdown\n+        //        * the first client will still have one pending transaction\n+        //      - crash case:\n+        //        * the pending transactions of the crashed client got aborted\n+        //        * the first client will have one pending transactions\n+        // 10. restart the second client with eos-beta enabled and wait until rebalance stabilizes\n+        //       - the rebalance should result in a commit of all tasks\n+        // 11. write 5 record per input topic partition and verify that the result was committed\n+\n+        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions1 = new LinkedList<>();\n+        KafkaStreams streams1Alpha = null;\n+        KafkaStreams streams1Beta = null;\n+        KafkaStreams streams1BetaTwo = null;\n+\n+        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> stateTransitions2 = new LinkedList<>();\n+        KafkaStreams streams2Alpha = null;\n+        KafkaStreams streams2AlphaTwo = null;\n+        KafkaStreams streams2Beta = null;\n+//        streams2Beta = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE_BETA);\n+//        streams2Beta.setStateListener((newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState)));\n+\n+        try {\n+            // phase 1: start both clients\n+            streams1Alpha = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE);\n+            streams1Alpha.setStateListener(\n+                (newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState))\n+            );\n+            streams1Alpha.cleanUp();\n+            streams1Alpha.start();\n+            waitForStateTransition(\n+                stateTransitions1,\n+                Arrays.asList(\n+                    KeyValue.pair(KafkaStreams.State.CREATED, KafkaStreams.State.REBALANCING),\n+                    KeyValue.pair(KafkaStreams.State.REBALANCING, KafkaStreams.State.RUNNING)\n+                )\n+            );\n+\n+            stateTransitions1.clear();\n+            streams2Alpha = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+            streams2Alpha.setStateListener(\n+                (newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState))\n+            );\n+            streams2Alpha.cleanUp();\n+            streams2Alpha.start();\n+            waitForStateTransition(stateTransitions1, TWO_REBALANCES_RUNNING, stateTransitions2, TWO_REBALANCES_STARTUP);\n+\n+            // in all phases, we write comments that assume that p-0/p-1 are assigned to the first client\n+            // and p-2/p-3 are assigned to the second client (in reality the assignment might be different though)\n+\n+            // phase 2: (write first batch of data)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // p-0: ---> 10 rec + C\n+            // p-1: ---> 10 rec + C\n+            // p-2: ---> 10 rec + C\n+            // p-3: ---> 10 rec + C\n+            final List<KeyValue<Long, Long>> committedInputDataBeforeUpgrade =\n+                prepareData(0L, 10L, 0L, 1L, 2L, 3L);\n+            writeInputData(committedInputDataBeforeUpgrade);\n+\n+            waitForCondition(\n+                () -> commitRequested.get() == 4,\n+                MAX_WAIT_TIME_MS,\n+                \"SteamsTasks did not request commit.\"\n+            );\n+\n+            final Map<Long, Long> committedState = new HashMap<>();\n+            final List<KeyValue<Long, Long>> expectedUncommittedResult =\n+                computeExpectedResult(committedInputDataBeforeUpgrade, committedState);\n+            verifyCommitted(expectedUncommittedResult);\n+\n+            // phase 3: (write partial second batch of data)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C ---> 5 rec (pending)\n+            //   p-1: 10 rec + C ---> 5 rec (pending)\n+            //   p-2: 10 rec + C ---> 5 rec (pending)\n+            //   p-3: 10 rec + C ---> 5 rec (pending)\n+            // crash case: (we just assumes that we inject the error for p-0; in reality it might be a different partition)\n+            //   p-0: 10 rec + C ---> 4 rec (pending)\n+            //   p-1: 10 rec + C ---> 5 rec (pending)\n+            //   p-2: 10 rec + C ---> 5 rec (pending)\n+            //   p-3: 10 rec + C ---> 5 rec (pending)\n+            final Set<Long> cleanKeys = mkSet(0L, 1L, 2L, 3L);\n+            final Set<Long> keyFilterFirstClient = keysFromInstance(streams1Alpha);\n+            final long potentiallyFirstFailingKey = keyFilterFirstClient.iterator().next();\n+            cleanKeys.remove(potentiallyFirstFailingKey);\n+\n+            final List<KeyValue<Long, Long>> uncommittedInputDataBeforeFirstUpgrade = new LinkedList<>();\n+            if (!injectError) {\n+                uncommittedInputDataBeforeFirstUpgrade.addAll(\n+                    prepareData(10L, 15L, 0L, 1L, 2L, 3L)\n+                );\n+                writeInputData(uncommittedInputDataBeforeFirstUpgrade);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataBeforeFirstUpgrade, new HashMap<>(committedState))\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+            } else {\n+                final List<KeyValue<Long, Long>> uncommittedInputDataWithoutFailingKey = new LinkedList<>();\n+                for (final long key : cleanKeys) {\n+                    uncommittedInputDataWithoutFailingKey.addAll(prepareData(10L, 15L, key));\n+                }\n+                uncommittedInputDataWithoutFailingKey.addAll(\n+                    prepareData(10L, 14L, potentiallyFirstFailingKey)\n+                );\n+                uncommittedInputDataBeforeFirstUpgrade.addAll(uncommittedInputDataWithoutFailingKey);\n+                writeInputData(uncommittedInputDataWithoutFailingKey);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataWithoutFailingKey, new HashMap<>(committedState))\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+            }\n+\n+            // phase 4: (stop/crash first client)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec ---> C\n+            //   p-1: 10 rec + C + 5 rec ---> C\n+            //   p-2: 10 rec + C + 5 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec (pending)\n+            // crash case:\n+            //   p-0: 10 rec + C + 4 rec ---> A + 5 rec (pending)\n+            //   p-1: 10 rec + C + 5 rec ---> A + 5 rec (pending)\n+            //   p-2: 10 rec + C + 5 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec (pending)\n+            stateTransitions2.clear();\n+            if (!injectError) {\n+                stateTransitions1.clear();\n+                streams1Alpha.close();\n+                waitForStateTransition(stateTransitions1, CLOSE);\n+            } else {\n+                errorInjectedClient1.set(true);\n+\n+                final List<KeyValue<Long, Long>> dataPotentiallyFirstFailingKey =\n+                    prepareData(14L, 15L, potentiallyFirstFailingKey);\n+                uncommittedInputDataBeforeFirstUpgrade.addAll(dataPotentiallyFirstFailingKey);\n+                writeInputData(dataPotentiallyFirstFailingKey);\n+            }\n+            waitForStateTransition(stateTransitions2, SINGLE_REBALANCE);\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataDuringFirstUpgrade =\n+                    uncommittedInputDataBeforeFirstUpgrade\n+                        .stream()\n+                        .filter(pair -> keyFilterFirstClient.contains(pair.key))\n+                        .collect(Collectors.toList());\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResult =\n+                    computeExpectedResult(committedInputDataDuringFirstUpgrade, committedState);\n+                // TODO: if we don't use the custom partitioner, the test hangs here until TX times out and is aborted\n+                verifyCommitted(expectedCommittedResult);\n+            } else {\n+                // retrying TX\n+                expectedUncommittedResult.addAll(computeExpectedResult(\n+                    uncommittedInputDataBeforeFirstUpgrade\n+                        .stream()\n+                        .filter(pair -> keyFilterFirstClient.contains(pair.key))\n+                        .collect(Collectors.toList()),\n+                    new HashMap<>(committedState)\n+                ));\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                errorInjectedClient1.set(false);\n+                stateTransitions1.clear();\n+                streams1Alpha.close();\n+                waitForStateTransition(stateTransitions1, CLOSE_CRASHED);\n+            }\n+\n+            // phase 5: (restart first client)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec ---> C\n+            //   p-3: 10 rec + C + 5 rec ---> C\n+            // crash case:\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec ---> C\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec ---> C\n+            //   p-2: 10 rec + C + 5 rec ---> C\n+            //   p-3: 10 rec + C + 5 rec ---> C\n+            commitRequested.set(0);\n+            stateTransitions1.clear();\n+            stateTransitions2.clear();\n+            streams1Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            streams1Beta.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+            streams1Beta.start();\n+            waitForStateTransition(stateTransitions1, TWO_REBALANCES_STARTUP, stateTransitions2, TWO_REBALANCES_RUNNING);\n+\n+            final Set<Long> committedKeys = mkSet(0L, 1L, 2L, 3L);\n+            if (!injectError) {\n+                committedKeys.removeAll(keyFilterFirstClient);\n+            }\n+\n+            final List<KeyValue<Long, Long>> expectedCommittedResultAfterRestartFirstClient = computeExpectedResult(\n+                uncommittedInputDataBeforeFirstUpgrade\n+                    .stream()\n+                    .filter(pair -> committedKeys.contains(pair.key))\n+                    .collect(Collectors.toList()),\n+                committedState\n+            );\n+            verifyCommitted(expectedCommittedResultAfterRestartFirstClient);\n+\n+            // phase 6: (complete second batch of data; crash: let second client fail on commit)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C ---> 5 rec + C\n+            // crash case:\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec + C ---> 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec + C ---> 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec + C ---> 5 rec + A + 5 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C ---> 5 rec + A + 5 rec + C\n+            commitCounterClient1.set(0);\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataDuringUpgrade =\n+                    prepareData(15L, 20L, 0L, 1L, 2L, 3L);\n+                writeInputData(committedInputDataDuringUpgrade);\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResult =\n+                    computeExpectedResult(committedInputDataDuringUpgrade, committedState);\n+                verifyCommitted(expectedCommittedResult);\n+                expectedUncommittedResult.addAll(expectedCommittedResult);\n+            } else {\n+                final Set<Long> keysFirstClient = keysFromInstance(streams1Beta);\n+                final Set<Long> keysSecondClient = keysFromInstance(streams2Alpha);\n+\n+                final List<KeyValue<Long, Long>> committedInputDataAfterFirstUpgrade =\n+                    prepareData(15L, 20L, keysFirstClient.toArray(new Long[0]));\n+                writeInputData(committedInputDataAfterFirstUpgrade);\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResultBeforeFailure =\n+                    computeExpectedResult(committedInputDataAfterFirstUpgrade, committedState);\n+                verifyCommitted(expectedCommittedResultBeforeFailure);\n+                expectedUncommittedResult.addAll(expectedCommittedResultBeforeFailure);\n+\n+                commitCounterClient2.set(0);\n+\n+                final Iterator<Long> it = keysSecondClient.iterator();\n+                final Long otherKey = it.next();\n+                final Long failingKey = it.next();\n+\n+                final List<KeyValue<Long, Long>> uncommittedInputDataAfterFirstUpgrade =\n+                    prepareData(15L, 19L, keysSecondClient.toArray(new Long[0]));\n+                uncommittedInputDataAfterFirstUpgrade.addAll(prepareData(19L, 20L, otherKey));\n+                writeInputData(uncommittedInputDataAfterFirstUpgrade);\n+\n+                final Map<Long, Long> uncommittedState = new HashMap<>(committedState);\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataAfterFirstUpgrade, uncommittedState)\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                stateTransitions1.clear();\n+                stateTransitions2.clear();\n+                commitCounterClient1.set(0);\n+                commitErrorInjectedClient2.set(true);\n+\n+                final List<KeyValue<Long, Long>> dataFailingKey = prepareData(19L, 20L, failingKey);\n+                uncommittedInputDataAfterFirstUpgrade.addAll(dataFailingKey);\n+                writeInputData(dataFailingKey);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(dataFailingKey, uncommittedState)\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                waitForStateTransition(stateTransitions1, SINGLE_REBALANCE, stateTransitions2, CRASH);\n+\n+                commitErrorInjectedClient2.set(false);\n+                stateTransitions2.clear();\n+                streams2Alpha.close();\n+                waitForStateTransition(stateTransitions2, CLOSE_CRASHED);\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResultAfterFailure =\n+                    computeExpectedResult(uncommittedInputDataAfterFirstUpgrade, committedState);\n+                verifyCommitted(expectedCommittedResultAfterFailure);\n+                expectedUncommittedResult.addAll(expectedCommittedResultAfterFailure);\n+            }\n+\n+            // 7. only for crash case:\n+            //     7a. restart the second client in eos-alpha mode and wait until rebalance stabilizes\n+            //     7b. write third batch of input data\n+            //         * fail the first (i.e., eos-beta) client during commit\n+            //         * the eos-alpha client should not pickup the pending offsets\n+            //         * verify uncommitted and committed result\n+            //     7c. restart the first client in eos-beta mode and wait until rebalance stabilizes\n+            //\n+            // crash case:\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec + C + 5 rec + C ---> 10 rec + A + 10 rec + C\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec + C + 5 rec + C ---> 10 rec + A + 10 rec + C\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C ---> 10 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C ---> 10 rec + C\n+            if (!injectError) {\n+                streams2AlphaTwo = streams2Alpha;\n+            } else {\n+                // 7a restart the second client in eos-alpha mode and wait until rebalance stabilizes\n+                commitCounterClient1.set(0);\n+                commitCounterClient2.set(-1);\n+                stateTransitions1.clear();\n+                stateTransitions2.clear();\n+                streams2AlphaTwo = getKafkaStreams(\"appDir2\", StreamsConfig.EXACTLY_ONCE);\n+                streams2AlphaTwo.setStateListener(\n+                    (newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState))\n+                );\n+                streams2AlphaTwo.start();\n+                waitForStateTransition(stateTransitions1, TWO_REBALANCES_RUNNING, stateTransitions2, TWO_REBALANCES_STARTUP);\n+\n+                // 7b. write third batch of input data\n+                final Set<Long> keysFirstClient = keysFromInstance(streams1Beta);\n+                final Set<Long> keysSecondClient = keysFromInstance(streams2AlphaTwo);\n+\n+                final List<KeyValue<Long, Long>> committedInputDataBetweenUpgrades =\n+                    prepareData(20L, 30L, keysSecondClient.toArray(new Long[0]));\n+                writeInputData(committedInputDataBetweenUpgrades);\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResultBeforeFailure =\n+                    computeExpectedResult(committedInputDataBetweenUpgrades, committedState);\n+                verifyCommitted(expectedCommittedResultBeforeFailure);\n+                expectedUncommittedResult.addAll(expectedCommittedResultBeforeFailure);\n+\n+                commitCounterClient2.set(0);\n+\n+                final Iterator<Long> it = keysFirstClient.iterator();\n+                final Long otherKey = it.next();\n+                final Long failingKey = it.next();\n+\n+                final List<KeyValue<Long, Long>> uncommittedInputDataBetweenUpgrade =\n+                    prepareData(20L, 29L, keysFirstClient.toArray(new Long[0]));\n+                uncommittedInputDataBetweenUpgrade.addAll(prepareData(29L, 30L, otherKey));\n+                writeInputData(uncommittedInputDataBetweenUpgrade);\n+\n+                final Map<Long, Long> uncommittedState = new HashMap<>(committedState);\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataBetweenUpgrade, uncommittedState)\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                stateTransitions1.clear();\n+                stateTransitions2.clear();\n+                commitCounterClient2.set(0);\n+                commitErrorInjectedClient1.set(true);\n+\n+                final List<KeyValue<Long, Long>> dataFailingKey = prepareData(29L, 30L, failingKey);\n+                uncommittedInputDataBetweenUpgrade.addAll(dataFailingKey);\n+                writeInputData(dataFailingKey);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(dataFailingKey, uncommittedState)\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                waitForStateTransition(stateTransitions1, CRASH, stateTransitions2, SINGLE_REBALANCE);\n+\n+                commitErrorInjectedClient1.set(false);\n+                stateTransitions1.clear();\n+                streams1Beta.close();\n+                waitForStateTransition(stateTransitions1, CLOSE_CRASHED);\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResultAfterFailure =\n+                    computeExpectedResult(uncommittedInputDataBetweenUpgrade, committedState);\n+                verifyCommitted(expectedCommittedResultAfterFailure);\n+                expectedUncommittedResult.addAll(expectedCommittedResultAfterFailure);\n+\n+                // 7c. restart the first client in eos-beta mode and wait until rebalance stabilizes\n+                stateTransitions1.clear();\n+                stateTransitions2.clear();\n+                streams1BetaTwo = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+                streams1BetaTwo.setStateListener((newState, oldState) -> stateTransitions1.add(KeyValue.pair(oldState, newState)));\n+                streams1BetaTwo.start();\n+                waitForStateTransition(stateTransitions1, TWO_REBALANCES_STARTUP, stateTransitions2, TWO_REBALANCES_RUNNING);\n+            }\n+\n+            // phase 8: (write partial fourth batch of data)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C + 5 rec + C ---> 5 rec (pending)\n+            //   p-1: 10 rec + C + 5 rec + C + 5 rec + C ---> 5 rec (pending)\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + C ---> 5 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + C ---> 5 rec (pending)\n+            // crash case:  (we just assumes that we inject the error for p-2; in reality it might be a different partition)\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C ---> 5 rec (pending)\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C ---> 5 rec (pending)\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C ---> 4 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C ---> 5 rec (pending)\n+            cleanKeys.addAll(mkSet(0L, 1L, 2L, 3L));\n+            final Set<Long> keyFilterSecondClient = keysFromInstance(streams2AlphaTwo);\n+            final long potentiallySecondFailingKey = keyFilterSecondClient.iterator().next();\n+            cleanKeys.remove(potentiallySecondFailingKey);\n+\n+            final List<KeyValue<Long, Long>> uncommittedInputDataBeforeSecondUpgrade = new LinkedList<>();\n+            if (!injectError) {\n+                uncommittedInputDataBeforeSecondUpgrade.addAll(\n+                    prepareData(30L, 35L, 0L, 1L, 2L, 3L)\n+                );\n+                writeInputData(uncommittedInputDataBeforeSecondUpgrade);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataBeforeSecondUpgrade, new HashMap<>(committedState))\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+            } else {\n+                final List<KeyValue<Long, Long>> uncommittedInputDataWithoutFailingKey = new LinkedList<>();\n+                for (final long key : cleanKeys) {\n+                    uncommittedInputDataWithoutFailingKey.addAll(prepareData(30L, 35L, key));\n+                }\n+                uncommittedInputDataWithoutFailingKey.addAll(\n+                    prepareData(30L, 34L, potentiallySecondFailingKey)\n+                );\n+                uncommittedInputDataBeforeSecondUpgrade.addAll(uncommittedInputDataWithoutFailingKey);\n+                writeInputData(uncommittedInputDataWithoutFailingKey);\n+\n+                expectedUncommittedResult.addAll(\n+                    computeExpectedResult(uncommittedInputDataWithoutFailingKey, new HashMap<>(committedState))\n+                );\n+                verifyUncommitted(expectedUncommittedResult);\n+            }\n+\n+            // phase 9: (stop/crash second client)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec (pending)\n+            //   p-1: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec (pending)\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec ---> C\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec ---> C\n+            // crash case:  (we just assumes that we inject the error for p-2; in reality it might be a different partition)\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C + 5 rec (pending)\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C + 5 rec (pending)\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C + 4 rec ---> A + 5 rec (pending)\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C + 5 rec ---> A + 5 rec (pending)\n+            stateTransitions1.clear();\n+            if (!injectError) {\n+                stateTransitions2.clear();\n+                streams2AlphaTwo.close();\n+                waitForStateTransition(stateTransitions2, CLOSE);\n+            } else {\n+                errorInjectedClient2.set(true);\n+\n+                final List<KeyValue<Long, Long>> dataPotentiallySecondFailingKey =\n+                    prepareData(34L, 35L, potentiallySecondFailingKey);\n+                uncommittedInputDataBeforeSecondUpgrade.addAll(dataPotentiallySecondFailingKey);\n+                writeInputData(dataPotentiallySecondFailingKey);\n+            }\n+            waitForStateTransition(stateTransitions1, SINGLE_REBALANCE);\n+\n+            if (!injectError) {\n+                final List<KeyValue<Long, Long>> committedInputDataDuringSecondUpgrade =\n+                    uncommittedInputDataBeforeSecondUpgrade\n+                        .stream()\n+                        .filter(pair -> keyFilterSecondClient.contains(pair.key))\n+                        .collect(Collectors.toList());\n+\n+                final List<KeyValue<Long, Long>> expectedCommittedResult =\n+                    computeExpectedResult(committedInputDataDuringSecondUpgrade, committedState);\n+                verifyCommitted(expectedCommittedResult);\n+            } else {\n+                // retrying TX\n+                expectedUncommittedResult.addAll(computeExpectedResult(\n+                    uncommittedInputDataBeforeSecondUpgrade\n+                        .stream()\n+                        .filter(pair -> keyFilterSecondClient.contains(pair.key))\n+                        .collect(Collectors.toList()),\n+                    new HashMap<>(committedState)\n+                ));\n+                verifyUncommitted(expectedUncommittedResult);\n+\n+                errorInjectedClient2.set(false);\n+                stateTransitions2.clear();\n+                streams2AlphaTwo.close();\n+                waitForStateTransition(stateTransitions2, CLOSE_CRASHED);\n+            }\n+\n+            // phase 10: (restart second client)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // the state below indicate the case for which the \"original\" tasks of client2 are migrated back to client2\n+            // if a task \"switch\" happens, we might get additional commits (omitted in the comment for brevity)\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec ---> C\n+            //   p-1: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec ---> C\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec + C\n+            // crash case:  (we just assumes that we inject the error for p-2; in reality it might be a different partition)\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C + 5 rec ---> C\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C + 5 rec ---> C\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C + 4 rec + A + 5 rec ---> C\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C + 5 rec + A + 5 rec ---> C\n+            commitRequested.set(0);\n+            stateTransitions1.clear();\n+            stateTransitions2.clear();\n+            streams2Beta = getKafkaStreams(\"appDir1\", StreamsConfig.EXACTLY_ONCE_BETA);\n+            streams2Beta.setStateListener(\n+                (newState, oldState) -> stateTransitions2.add(KeyValue.pair(oldState, newState))\n+            );\n+            streams2Beta.start();\n+            waitForStateTransition(stateTransitions1, TWO_REBALANCES_RUNNING, stateTransitions2, TWO_REBALANCES_STARTUP);\n+\n+            committedKeys.addAll(mkSet(0L, 1L, 2L, 3L));\n+            if (!injectError) {\n+                committedKeys.removeAll(keyFilterSecondClient);\n+            }\n+\n+            final List<KeyValue<Long, Long>> expectedCommittedResultAfterRestartSecondClient = computeExpectedResult(\n+                uncommittedInputDataBeforeSecondUpgrade\n+                    .stream()\n+                    .filter(pair -> committedKeys.contains(pair.key))\n+                    .collect(Collectors.toList()),\n+                committedState\n+            );\n+            verifyCommitted(expectedCommittedResultAfterRestartSecondClient);\n+\n+            // phase 11: (complete fourth batch of data)\n+            // expected end state per output partition (C == COMMIT; A == ABORT; ---> indicate the changes):\n+            //\n+            // stop case:\n+            //   p-0: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + C + 5 rec + C ---> 5 rec + C\n+            // crash case:  (we just assumes that we inject the error for p-2; in reality it might be a different partition)\n+            //   p-0: 10 rec + C + 4 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-1: 10 rec + C + 5 rec + A + 5 rec + C + 5 rec + C + 10 rec + A + 10 rec + C + 5 rec + C ---> 5 rec + C\n+            //   p-2: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C + 4 rec + A + 5 rec + C ---> 5 rec + C\n+            //   p-3: 10 rec + C + 5 rec + C + 5 rec + A + 5 rec + C + 10 rec + C + 5 rec + A + 5 rec + C ---> 5 rec + C\n+            commitCounterClient1.set(-1);\n+            commitCounterClient2.set(-1);\n+\n+            final List<KeyValue<Long, Long>> committedInputDataAfterUpgrade =\n+                prepareData(35L, 40L, 0L, 1L, 2L, 3L);\n+            writeInputData(committedInputDataAfterUpgrade);\n+\n+            final List<KeyValue<Long, Long>> expectedCommittedResult =\n+                computeExpectedResult(committedInputDataAfterUpgrade, committedState);\n+            verifyCommitted(expectedCommittedResult);\n+        } finally {\n+            if (streams1Alpha != null) {\n+                streams1Alpha.close();\n+            }\n+            if (streams1Beta != null) {\n+                streams1Beta.close();\n+            }\n+            if (streams1BetaTwo != null) {\n+                streams1BetaTwo.close();\n+            }\n+            if (streams2Alpha != null) {\n+                streams2Alpha.close();\n+            }\n+            if (streams2AlphaTwo != null) {\n+                streams2AlphaTwo.close();\n+            }\n+            if (streams2Beta != null) {\n+                streams2Beta.close();\n+            }\n+        }\n+    }\n+\n+    private KafkaStreams getKafkaStreams(final String appDir,\n+                                         final String processingGuarantee) {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+\n+        final String[] storeNames = new String[] {storeName};\n+        final StoreBuilder<KeyValueStore<Long, Long>> storeBuilder = Stores\n+            .keyValueStoreBuilder(Stores.persistentKeyValueStore(storeName), Serdes.Long(), Serdes.Long())\n+            .withCachingEnabled();\n+\n+        builder.addStateStore(storeBuilder);\n+\n+        final KStream<Long, Long> input = builder.stream(MULTI_PARTITION_INPUT_TOPIC);\n+        input.transform(new TransformerSupplier<Long, Long, KeyValue<Long, Long>>() {\n+            @SuppressWarnings(\"unchecked\")\n+            @Override\n+            public Transformer<Long, Long, KeyValue<Long, Long>> get() {\n+                return new Transformer<Long, Long, KeyValue<Long, Long>>() {\n+                    ProcessorContext<Object, Object> context;\n+                    KeyValueStore<Long, Long> state = null;\n+                    AtomicBoolean crash;\n+                    AtomicInteger sharedCommit;\n+\n+                    @Override\n+                    public void init(final ProcessorContext<Object, Object> context) {\n+                        this.context = context;\n+                        state = (KeyValueStore<Long, Long>) context.getStateStore(storeName);\n+                        final String clientId = context.appConfigs().get(StreamsConfig.CLIENT_ID_CONFIG).toString();\n+                        if (\"appDir1\".equals(clientId)) {\n+                            crash = errorInjectedClient1;\n+                            sharedCommit = commitCounterClient1;\n+                        } else {\n+                            crash = errorInjectedClient2;\n+                            sharedCommit = commitCounterClient2;\n+                        }\n+                    }\n+\n+                    @Override\n+                    public KeyValue<Long, Long> transform(final Long key, final Long value) {\n+                        if ((value + 1) % 10 == 0) {\n+                            if (sharedCommit.get() < 0 ||\n+                                sharedCommit.incrementAndGet() == 2) {\n+\n+                                context.commit();\n+                            }\n+                            commitRequested.incrementAndGet();\n+                        }\n+\n+                        Long sum = state.get(key);\n+                        if (sum == null) {\n+                            sum = value;\n+                        } else {\n+                            sum += value;\n+                        }\n+                        state.put(key, sum);\n+                        state.flush();\n+\n+                        if (value % 10 == 4 && // potentially crash when processing 5th, 15th, or 25th record (etc.)\n+                            crash != null && crash.compareAndSet(true, false)) {\n+                            // only crash a single task\n+                            throw new RuntimeException(\"Injected test exception.\");\n+                        }\n+\n+                        return new KeyValue<>(key, state.get(key));\n+                    }\n+\n+                    @Override\n+                    public void close() { }\n+                };\n+            } }, storeNames)\n+            .to(MULTI_PARTITION_OUTPUT_TOPIC);\n+\n+        final Properties properties = new Properties();\n+        properties.put(StreamsConfig.CLIENT_ID_CONFIG, appDir);\n+        properties.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, processingGuarantee);\n+        properties.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, Long.MAX_VALUE);\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.METADATA_MAX_AGE_CONFIG), \"1000\");\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), \"earliest\");\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG), 5 * 1000);\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG), 5 * 1000 - 1);\n+        properties.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG), MAX_POLL_INTERVAL_MS);\n+        // TODO\n+        //   if we don't use this custom partitioner the test fails for the non-error case\n+        //   unclear why -- see other TODO\n+        properties.put(StreamsConfig.producerPrefix(ProducerConfig.PARTITIONER_CLASS_CONFIG), KeyPartitioner.class);\n+        properties.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+        properties.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath() + File.separator + appDir);\n+\n+        final Properties config = StreamsTestUtils.getStreamsConfig(\n+            applicationId,\n+            CLUSTER.bootstrapServers(),\n+            Serdes.LongSerde.class.getName(),\n+            Serdes.LongSerde.class.getName(),\n+            properties\n+        );\n+\n+        final KafkaStreams streams = new KafkaStreams(builder.build(), config, new TestKafkaClientSupplier());\n+\n+        streams.setUncaughtExceptionHandler((t, e) -> {\n+            if (uncaughtException != null) {\n+                e.printStackTrace(System.err);\n+                fail(\"Should only get one uncaught exception from Streams.\");\n+            }\n+            uncaughtException = e;\n+        });\n+\n+        return streams;\n+    }\n+\n+    private void waitForStateTransition(final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> observed,\n+                                        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> expected)\n+        throws Exception {\n+\n+        try {\n+            waitForCondition(\n+                () -> observed.equals(expected),\n+                MAX_WAIT_TIME_MS,\n+                \"Client did not startup on time.\"\n+            );\n+        } catch (final AssertionError error) {\n+            final AssertionError newError = new AssertionError(\"Client transitions: \" + observed);\n+            newError.addSuppressed(error);\n+            throw newError;\n+        }\n+    }\n+\n+    private void waitForStateTransition(final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> observed1,\n+                                        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> expected1,\n+                                        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> observed2,\n+                                        final List<KeyValue<KafkaStreams.State, KafkaStreams.State>> expected2)\n+        throws Exception {\n+\n+        try {\n+            waitForCondition(\n+                () -> observed1.equals(expected1) && observed2.equals(expected2),\n+                MAX_WAIT_TIME_MS,\n+                \"Clients did not startup and stabilize on time.\"\n+            );\n+        } catch (final AssertionError error) {\n+            final AssertionError newError = new AssertionError(\"Client transitions: \" +\n+                \"\\n  client-1 transitions: \" + observed1 +\n+                \"\\n  client-2 transitions: \" + observed2);\n+            newError.addSuppressed(error);\n+            throw newError;\n+        }\n+    }\n+\n+    private List<KeyValue<Long, Long>> prepareData(final long fromInclusive,\n+                                                   final long toExclusive,\n+                                                   final Long... keys) {\n+        final List<KeyValue<Long, Long>> data = new ArrayList<>();\n+\n+        for (final Long k : keys) {\n+            for (long v = fromInclusive; v < toExclusive; ++v) {\n+                data.add(new KeyValue<>(k, v));\n+            }\n+        }\n+\n+        return data;\n+    }\n+\n+    private void writeInputData(final List<KeyValue<Long, Long>> records) {\n+        final Properties config = TestUtils.producerConfig(\n+            CLUSTER.bootstrapServers(),\n+            LongSerializer.class,\n+            LongSerializer.class\n+        );\n+        config.setProperty(ProducerConfig.PARTITIONER_CLASS_CONFIG, KeyPartitioner.class.getName());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODIwMTM5OA==", "url": "https://github.com/apache/kafka/pull/8496#discussion_r418201398", "bodyText": "nit: Could you add the original comment explaining why we set it to smaller value too?", "author": "guozhangwang", "createdAt": "2020-04-30T18:18:04Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -838,6 +838,7 @@\n     static {\n         final Map<String, Object> tempProducerDefaultOverrides = new HashMap<>();\n         tempProducerDefaultOverrides.put(ProducerConfig.LINGER_MS_CONFIG, \"100\");\n+        tempProducerDefaultOverrides.put(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, 10000);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "969512eae43ef042f7d1ebc7c2b2562e5e9ed145", "url": "https://github.com/apache/kafka/commit/969512eae43ef042f7d1ebc7c2b2562e5e9ed145", "message": "KAFKA-9748: Add Streams eos-beta integration test", "committedDate": "2020-05-01T21:35:25Z", "type": "commit"}, {"oid": "0acaa580b03b48de1466668f7fd14647b5e11961", "url": "https://github.com/apache/kafka/commit/0acaa580b03b48de1466668f7fd14647b5e11961", "message": "Fix for unclear issue...", "committedDate": "2020-05-01T21:35:25Z", "type": "commit"}, {"oid": "603f4f44ac6b5c29f3f6326c67c9b0c1e5c31946", "url": "https://github.com/apache/kafka/commit/603f4f44ac6b5c29f3f6326c67c9b0c1e5c31946", "message": "cleanup", "committedDate": "2020-05-01T21:35:25Z", "type": "commit"}, {"oid": "9ee531024576de2f499479a86c88ef5553cab986", "url": "https://github.com/apache/kafka/commit/9ee531024576de2f499479a86c88ef5553cab986", "message": "Github comments", "committedDate": "2020-05-01T21:35:25Z", "type": "commit"}, {"oid": "346c7b4fe34cf6870cd6f6d28365d10ef39a141d", "url": "https://github.com/apache/kafka/commit/346c7b4fe34cf6870cd6f6d28365d10ef39a141d", "message": "simplify error handling", "committedDate": "2020-05-01T21:39:33Z", "type": "commit"}, {"oid": "346c7b4fe34cf6870cd6f6d28365d10ef39a141d", "url": "https://github.com/apache/kafka/commit/346c7b4fe34cf6870cd6f6d28365d10ef39a141d", "message": "simplify error handling", "committedDate": "2020-05-01T21:39:33Z", "type": "forcePushed"}]}