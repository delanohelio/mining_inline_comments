{"pr_number": 8699, "pr_title": "KAFKA-9673: Filter and Conditional SMTs", "pr_createdAt": "2020-05-20T18:33:59Z", "pr_url": "https://github.com/apache/kafka/pull/8699", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODI5NDk2OA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r428294968", "bodyText": "I think we want to check the value instead of the key here?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return record.key() == null;\n          \n          \n            \n                    return record.value() == null;", "author": "C0urante", "createdAt": "2020-05-20T20:41:35Z", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/RecordIsTombstone.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms.predicates;\n+\n+import java.util.Map;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+\n+/**\n+ * A predicate which is true for records which are tombstones (i.e. have null key).\n+ * @param <R> The type of connect record.\n+ */\n+public class RecordIsTombstone<R extends ConnectRecord<R>> implements Predicate<R> {\n+    @Override\n+    public ConfigDef config() {\n+        return new ConfigDef();\n+    }\n+\n+    @Override\n+    public boolean test(R record) {\n+        return record.key() == null;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODI5NTI1MQ==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r428295251", "bodyText": "I think a tombstone is defined as a record with a null value, not a null key:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * A predicate which is true for records which are tombstones (i.e. have null key).\n          \n          \n            \n             * A predicate which is true for records which are tombstones (i.e. have null values).", "author": "C0urante", "createdAt": "2020-05-20T20:42:06Z", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/RecordIsTombstone.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms.predicates;\n+\n+import java.util.Map;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+\n+/**\n+ * A predicate which is true for records which are tombstones (i.e. have null key).", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODI5NzExOA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r428297118", "bodyText": "I think this will fail during validation since transformations must provide non-null ConfigDefs. Might want to instantiate a single empty static ConfigDef object for the class and just return that?", "author": "C0urante", "createdAt": "2020-05-20T20:45:50Z", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Filter.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms;\n+\n+import java.util.Map;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+\n+/**\n+ * Drops all records, filtering them from subsequent transformations in the chain.\n+ * This is intended to be used conditionally to filter out records matching (or not matching)\n+ * a particular {@link org.apache.kafka.connect.transforms.predicates.Predicate}.\n+ * @param <R> The type of record.\n+ */\n+public class Filter<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    @Override\n+    public R apply(R record) {\n+        return null;\n+    }\n+\n+    @Override\n+    public ConfigDef config() {\n+        return null;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODI5OTc3Nw==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r428299777", "bodyText": "Same comment here as with Filter; probably want to return a non-null ConfigDef here.", "author": "C0urante", "createdAt": "2020-05-20T20:51:08Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/PredicatedTransformation.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import java.util.Map;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.transforms.Transformation;\n+import org.apache.kafka.connect.transforms.predicates.Predicate;\n+\n+/**\n+ * Decorator for a {@link Transformation} which applies the delegate only when a\n+ * {@link Predicate} is true (or false, according to {@code negate}).\n+ * @param <R>\n+ */\n+class PredicatedTransformation<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    /*test*/ final Predicate<R> predicate;\n+    /*test*/ final Transformation<R> delegate;\n+    /*test*/ final boolean negate;\n+\n+    PredicatedTransformation(Predicate<R> predicate, boolean negate, Transformation<R> delegate) {\n+        this.predicate = predicate;\n+        this.negate = negate;\n+        this.delegate = delegate;\n+    }\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+\n+    }\n+\n+    @Override\n+    public R apply(R record) {\n+        if (negate ^ predicate.test(record)) {\n+            return delegate.apply(record);\n+        }\n+        return record;\n+    }\n+\n+    @Override\n+    public ConfigDef config() {\n+        return null;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA4MTgzOA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r429081838", "bodyText": "This is related to the issue discussed about configure().\nWe could return an empty ConfigDef here, but that would be a lie which could ultimately lead some other error if someone tried to use it with configure().\nWe can't invent a ConfigDef schema for this because the PredicatedTransformer would need to know about the Transformer is was going to be wrapping, but it can't know that before it's been configured with at least the Transformer's ConfigDef and it can't be configured before config() has been called. So we have a chicken and egg problem. Something (ConnectorConfig) must have some a priori knowledge of either PredicatedTransformer's ConfigDef, or know how to configure it without needing to call config() at all.\nSince PredicatedTransformer is a purely internal class which will never be directly exposed to Connect users, we're not obliged to stick to the contract of config() and configure(). i.e. So  both PredicatedTransformer.config and PredicatedTransformer.configure can throw when called, since we know no one else can call them and we know ConnectorConfig never will.", "author": "tombentley", "createdAt": "2020-05-22T07:16:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODI5OTc3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDA1ODEwMg==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r430058102", "bodyText": "Hmmm... I think there might be some awkwardness here with trying to make PredicatedTransformer implement the Transformation interface. Could we replace every Transformation in the TransformationChain's transformation list with a PredicatedTransformer and, if there are no predicates configured for a transform by the user, make the default behavior for the PredicatedTransformer class to blindly apply its transformation?\nThis would solve a few problems:\n\nNo risk of users trying to actually use a PredicatedTransformer in a connector config, which they may try to do if we don't add logic to prevent it from being picked up during plugin path scanning on startup and logged as an SMT plugin\nNo need to implement methods that aren't used\nOne code path instead of two for application of transformations\nMore flexibility in instantiation and, possibly, the ability to encapsulate some of the ConfigDef generation logic in a separate class from ConnectorConfig (haven't looked into the specifics of this yet so may not actually be feasible or that elegant)", "author": "C0urante", "createdAt": "2020-05-25T19:45:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODI5OTc3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMyNjM4Mw==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r428326383", "bodyText": "Do we need to remove these properties here, or can we just read them? Removing might cause issues with SMTs that have config properties with these names; would leaving them in be likely to cause issues as well?", "author": "C0urante", "createdAt": "2020-05-20T21:46:56Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java", "diffHunk": "@@ -257,12 +273,25 @@ public boolean includeRecordDetailsInErrorLog() {\n         final List<Transformation<R>> transformations = new ArrayList<>(transformAliases.size());\n         for (String alias : transformAliases) {\n             final String prefix = TRANSFORMS_CONFIG + \".\" + alias + \".\";\n+\n             try {\n                 @SuppressWarnings(\"unchecked\")\n                 final Transformation<R> transformation = getClass(prefix + \"type\").asSubclass(Transformation.class)\n                         .getDeclaredConstructor().newInstance();\n-                transformation.configure(originalsWithPrefix(prefix));\n-                transformations.add(transformation);\n+                Map<String, Object> configs = originalsWithPrefix(prefix);\n+                Object predicateAlias = configs.remove(\"predicate\");\n+                Object negate = configs.remove(\"negate\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA2NDQ5MQ==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r429064491", "bodyText": "The compatibility section of the KIP says that if a connector already has these configs then they'll be masked by the new implicit configs. If we don't remove them here then we'd be passing the KIP-585 configs to a connector which had it's own semantics for those config keys, which would be incorrect.", "author": "tombentley", "createdAt": "2020-05-22T06:25:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMyNjM4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDA1ODM1Mw==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r430058353", "bodyText": "Hmmm... wish we'd caught that earlier. Seems safer to just leave the properties in, but unless we want to call for a re-vote and an extension on the KIP deadline guess we'll have to keep this as-is.", "author": "C0urante", "createdAt": "2020-05-25T19:46:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMyNjM4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMzMjcwNA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r428332704", "bodyText": "Just curious, why directly parse the negate property here instead of doing that in PredicatedTransformation::configure?", "author": "C0urante", "createdAt": "2020-05-20T22:00:45Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java", "diffHunk": "@@ -257,12 +273,25 @@ public boolean includeRecordDetailsInErrorLog() {\n         final List<Transformation<R>> transformations = new ArrayList<>(transformAliases.size());\n         for (String alias : transformAliases) {\n             final String prefix = TRANSFORMS_CONFIG + \".\" + alias + \".\";\n+\n             try {\n                 @SuppressWarnings(\"unchecked\")\n                 final Transformation<R> transformation = getClass(prefix + \"type\").asSubclass(Transformation.class)\n                         .getDeclaredConstructor().newInstance();\n-                transformation.configure(originalsWithPrefix(prefix));\n-                transformations.add(transformation);\n+                Map<String, Object> configs = originalsWithPrefix(prefix);\n+                Object predicateAlias = configs.remove(\"predicate\");\n+                Object negate = configs.remove(\"negate\");\n+                transformation.configure(configs);\n+                if (predicateAlias != null) {\n+                    String predicatePrefix = \"predicates.\" + predicateAlias + \".\";\n+                    @SuppressWarnings(\"unchecked\")\n+                    Predicate<R> predicate = getClass(predicatePrefix + \"type\").asSubclass(Predicate.class)\n+                            .getDeclaredConstructor().newInstance();\n+                    predicate.configure(originalsWithPrefix(predicatePrefix));\n+                    transformations.add(new PredicatedTransformation<>(predicate, negate == null ? false : Boolean.parseBoolean(negate.toString()), transformation));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA2Njc3Mg==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r429066772", "bodyText": "A more general question is \"Why does PredicatedTransformation have a special constructor rather than use configure(Map<String, ?>)?\" This arises because PredicatedTransformer is a bit special. In particular is has to be passed an already configured Predicate and a Transformation, which you couldn't do with normal Transformation (which can only be configured with the types supported by ConfigDef).\nI guess we could use configure(Map<String, ?>), but then we have to instantiate a Map at this call site only to unpick it again in PredicatedTransformation, which feels like more work than just having a constructor. But if you prefer the consistency of using configure() I'm happy to do it.", "author": "tombentley", "createdAt": "2020-05-22T06:32:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMzMjcwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA2NzQ4Mw==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r429067483", "bodyText": "Actually, maybe I can make this nicer by changing how ConfigDef instantiates the PredicateTransformer, let me see...", "author": "tombentley", "createdAt": "2020-05-22T06:35:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMzMjcwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA3MjA0Nw==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r429072047", "bodyText": "My idea was to let the PredicatedTransformer instantiate its delegate. That could be done, I think but PredicatedTransformer cannot instantiate its Predicate because the configs for that are under the predicates.<key> prefix rather than transformers.<key>, so there's not a single Map which you could pass to PredicatedTransformer.configure() to let it instantiate both transformer and predicate. We could instantiate the predicate in ConnectorConfig, but pass a Map and let the PredicatedTransformer instantiate the Transformer, but then transformer instantiation happens in different places depending on whether it's its predicated or not, so it doesn't seem worth it. I'm back to \"if you prefer the consistency of using configure() I'm happy to do it.\", so just let me know.", "author": "tombentley", "createdAt": "2020-05-22T06:48:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMzMjcwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMzMzc0Mw==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r428333743", "bodyText": "Probably won't impact performance too much but we could technically use a single ConfigDef instance for the entire class instead of creating a new one every time this method is called.", "author": "C0urante", "createdAt": "2020-05-20T22:03:22Z", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/RecordIsTombstone.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms.predicates;\n+\n+import java.util.Map;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+\n+/**\n+ * A predicate which is true for records which are tombstones (i.e. have null key).\n+ * @param <R> The type of connect record.\n+ */\n+public class RecordIsTombstone<R extends ConnectRecord<R>> implements Predicate<R> {\n+    @Override\n+    public ConfigDef config() {\n+        return new ConfigDef();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMzNDY1OQ==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r428334659", "bodyText": "Might consider using a SimpleConfig here. Won't make a huge difference with the class as-is, but will make it easier to make changes in the future if we ever want to expand on the configurability of this predicate.", "author": "C0urante", "createdAt": "2020-05-20T22:05:44Z", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/HasHeaderKey.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms.predicates;\n+\n+import java.util.Map;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+\n+/**\n+ * A predicate which is true for records with at least one header with the configured name.\n+ * @param <R> The type of connect record.\n+ */\n+public class HasHeaderKey<R extends ConnectRecord<R>> implements Predicate<R> {\n+\n+    private static final String NAME_CONFIG_KEY = \"name\";\n+    private String name;\n+\n+    @Override\n+    public ConfigDef config() {\n+        return new ConfigDef().define(NAME_CONFIG_KEY, ConfigDef.Type.STRING, null,\n+                new ConfigDef.NonEmptyString(), ConfigDef.Importance.MEDIUM,\n+                \"The header name.\");\n+    }\n+\n+    @Override\n+    public boolean test(R record) {\n+        return record.headers().allWithName(name).hasNext();\n+    }\n+\n+    @Override\n+    public void close() {\n+\n+    }\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        this.name = (String) configs.get(NAME_CONFIG_KEY);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMzNjQyNw==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r428336427", "bodyText": "Might want to break this up to avoid an NPE since Headers::allWithName is technically allowed to return null in some situations:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return record.headers().allWithName(name).hasNext();\n          \n          \n            \n                    Iterator<Header> headersWithName = record.headers().allWithName(name);\n          \n          \n            \n                    return headersWithName != null ? headersWithName.hasNext() : false;", "author": "C0urante", "createdAt": "2020-05-20T22:10:04Z", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/HasHeaderKey.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms.predicates;\n+\n+import java.util.Map;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+\n+/**\n+ * A predicate which is true for records with at least one header with the configured name.\n+ * @param <R> The type of connect record.\n+ */\n+public class HasHeaderKey<R extends ConnectRecord<R>> implements Predicate<R> {\n+\n+    private static final String NAME_CONFIG_KEY = \"name\";\n+    private String name;\n+\n+    @Override\n+    public ConfigDef config() {\n+        return new ConfigDef().define(NAME_CONFIG_KEY, ConfigDef.Type.STRING, null,\n+                new ConfigDef.NonEmptyString(), ConfigDef.Importance.MEDIUM,\n+                \"The header name.\");\n+    }\n+\n+    @Override\n+    public boolean test(R record) {\n+        return record.headers().allWithName(name).hasNext();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM0MjE4Nw==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r428342187", "bodyText": "Blegh, was hoping we might be able to use Utils::newInstance or AbstractConfig::newConfiguredInstance but it looks like neither quite does what we need; the former doesn't do casting to a subclass unless you give it the FQCN of a class instead of an already-loaded Class<?> object, and the latter doesn't give enough control over exactly which properties the new instance is configured with.\nSince we're using this same logic in several different places in this file alone, we might consider expanding the Utils class with a new utility method that does this for us. Maybe something like:\npublic static Class<T> newInstance(Class<?> klass, Class<T> baseClass) {\n    // Return an instance of klass that has been automatically cast to the type of baseClass\n}", "author": "C0urante", "createdAt": "2020-05-20T22:24:40Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java", "diffHunk": "@@ -257,12 +273,25 @@ public boolean includeRecordDetailsInErrorLog() {\n         final List<Transformation<R>> transformations = new ArrayList<>(transformAliases.size());\n         for (String alias : transformAliases) {\n             final String prefix = TRANSFORMS_CONFIG + \".\" + alias + \".\";\n+\n             try {\n                 @SuppressWarnings(\"unchecked\")\n                 final Transformation<R> transformation = getClass(prefix + \"type\").asSubclass(Transformation.class)\n                         .getDeclaredConstructor().newInstance();\n-                transformation.configure(originalsWithPrefix(prefix));\n-                transformations.add(transformation);\n+                Map<String, Object> configs = originalsWithPrefix(prefix);\n+                Object predicateAlias = configs.remove(\"predicate\");\n+                Object negate = configs.remove(\"negate\");\n+                transformation.configure(configs);\n+                if (predicateAlias != null) {\n+                    String predicatePrefix = \"predicates.\" + predicateAlias + \".\";\n+                    @SuppressWarnings(\"unchecked\")\n+                    Predicate<R> predicate = getClass(predicatePrefix + \"type\").asSubclass(Predicate.class)\n+                            .getDeclaredConstructor().newInstance();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM0NDE4NA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r428344184", "bodyText": "Same comment as elsewhere: might want to use a SimpleConfig in this class.", "author": "C0urante", "createdAt": "2020-05-20T22:29:25Z", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/TopicNameMatches.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms.predicates;\n+\n+import java.util.Map;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+\n+/**\n+ * A predicate which is true for records with a topic name that matches the configured regular expression.\n+ * @param <R> The type of connect record.\n+ */\n+public class TopicNameMatches<R extends ConnectRecord<R>> implements Predicate<R> {\n+\n+    public static final String PATTERN_CONFIG_KEY = \"pattern\";\n+    private Pattern pattern;\n+\n+    @Override\n+    public ConfigDef config() {\n+        return new ConfigDef().define(PATTERN_CONFIG_KEY, ConfigDef.Type.STRING, null,\n+                new ConfigDef.Validator() {\n+                    @Override\n+                    public void ensureValid(String name, Object value) {\n+                        if (value != null) {\n+                            compile(name, value);\n+                        }\n+                    }\n+                }, ConfigDef.Importance.MEDIUM,\n+                \"A Java regular expression for matching against the name of a record's topic.\");\n+    }\n+\n+    private Pattern compile(String name, Object value) {\n+        try {\n+            return Pattern.compile((String) value);\n+        } catch (PatternSyntaxException e) {\n+            throw new ConfigException(name, value, \"entry must be a Java-compatible regular expression: \" + e.getMessage());\n+        }\n+    }\n+\n+    @Override\n+    public boolean test(R record) {\n+        return record.topic() != null && pattern.matcher(record.topic()).matches();\n+    }\n+\n+    @Override\n+    public void close() {\n+\n+    }\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        this.pattern = compile(PATTERN_CONFIG_KEY, configs.get(PATTERN_CONFIG_KEY));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg2NzM3MA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r430867370", "bodyText": "I'd suggest declaring a PREDICATES_PREFIX variable for higher visibility.", "author": "kkonstantine", "createdAt": "2020-05-27T05:37:13Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java", "diffHunk": "@@ -257,12 +274,23 @@ public boolean includeRecordDetailsInErrorLog() {\n         final List<Transformation<R>> transformations = new ArrayList<>(transformAliases.size());\n         for (String alias : transformAliases) {\n             final String prefix = TRANSFORMS_CONFIG + \".\" + alias + \".\";\n+\n             try {\n                 @SuppressWarnings(\"unchecked\")\n-                final Transformation<R> transformation = getClass(prefix + \"type\").asSubclass(Transformation.class)\n-                        .getDeclaredConstructor().newInstance();\n-                transformation.configure(originalsWithPrefix(prefix));\n-                transformations.add(transformation);\n+                final Transformation<R> transformation = Utils.newInstance(getClass(prefix + \"type\"), Transformation.class);\n+                Map<String, Object> configs = originalsWithPrefix(prefix);\n+                Object predicateAlias = configs.remove(PredicatedTransformation.PREDICATE_CONFIG);\n+                Object negate = configs.remove(PredicatedTransformation.NEGATE_CONFIG);\n+                transformation.configure(configs);\n+                if (predicateAlias != null) {\n+                    String predicatePrefix = \"predicates.\" + predicateAlias + \".\";", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg3ODM3Ng==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r430878376", "bodyText": "Consider using LambdaValidator to be able to add a lambda for the toString method of this validator.", "author": "kkonstantine", "createdAt": "2020-05-27T06:11:31Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java", "diffHunk": "@@ -276,116 +304,251 @@ public boolean includeRecordDetailsInErrorLog() {\n      * <p>\n      * {@code requireFullConfig} specifies whether required config values that are missing should cause an exception to be thrown.\n      */\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n     public static ConfigDef enrich(Plugins plugins, ConfigDef baseConfigDef, Map<String, String> props, boolean requireFullConfig) {\n-        Object transformAliases = ConfigDef.parseType(TRANSFORMS_CONFIG, props.get(TRANSFORMS_CONFIG), Type.LIST);\n-        if (!(transformAliases instanceof List)) {\n-            return baseConfigDef;\n-        }\n-\n         ConfigDef newDef = new ConfigDef(baseConfigDef);\n-        LinkedHashSet<?> uniqueTransformAliases = new LinkedHashSet<>((List<?>) transformAliases);\n-        for (Object o : uniqueTransformAliases) {\n-            if (!(o instanceof String)) {\n-                throw new ConfigException(\"Item in \" + TRANSFORMS_CONFIG + \" property is not of \"\n-                        + \"type String\");\n+        new EnrichablePlugin<Transformation<?>>(\"Transformation\", TRANSFORMS_CONFIG, TRANSFORMS_GROUP, (Class) Transformation.class,\n+                props, requireFullConfig) {\n+            @SuppressWarnings(\"rawtypes\")\n+            @Override\n+            protected Set<PluginDesc<Transformation<?>>> plugins() {\n+                return (Set) plugins.transformations();\n             }\n-            String alias = (String) o;\n-            final String prefix = TRANSFORMS_CONFIG + \".\" + alias + \".\";\n-            final String group = TRANSFORMS_GROUP + \": \" + alias;\n-            int orderInGroup = 0;\n-\n-            final String transformationTypeConfig = prefix + \"type\";\n-            final ConfigDef.Validator typeValidator = new ConfigDef.Validator() {\n-                @Override\n-                public void ensureValid(String name, Object value) {\n-                    getConfigDefFromTransformation(transformationTypeConfig, (Class) value);\n-                }\n-            };\n-            newDef.define(transformationTypeConfig, Type.CLASS, ConfigDef.NO_DEFAULT_VALUE, typeValidator, Importance.HIGH,\n-                    \"Class for the '\" + alias + \"' transformation.\", group, orderInGroup++, Width.LONG, \"Transformation type for \" + alias,\n-                    Collections.<String>emptyList(), new TransformationClassRecommender(plugins));\n \n-            final ConfigDef transformationConfigDef;\n-            try {\n-                final String className = props.get(transformationTypeConfig);\n-                final Class<?> cls = (Class<?>) ConfigDef.parseType(transformationTypeConfig, className, Type.CLASS);\n-                transformationConfigDef = getConfigDefFromTransformation(transformationTypeConfig, cls);\n-            } catch (ConfigException e) {\n-                if (requireFullConfig) {\n-                    throw e;\n-                } else {\n-                    continue;\n+            @Override\n+            protected ConfigDef initialConfigDef() {\n+                // All Transformations get these config parameters implicitly\n+                return super.initialConfigDef()\n+                        .define(PredicatedTransformation.PREDICATE_CONFIG, Type.STRING, \"\", Importance.MEDIUM,\n+                                \"The alias of a predicate used to determine whether to apply this transformation.\")\n+                        .define(PredicatedTransformation.NEGATE_CONFIG, Type.BOOLEAN, false, Importance.MEDIUM,\n+                                \"Whether the configured predicate should be negated.\");\n+            }\n+\n+            @Override\n+            protected Stream<Map.Entry<String, ConfigDef.ConfigKey>> configDefsForClass(String typeConfig) {\n+                return super.configDefsForClass(typeConfig)\n+                    .filter(entry -> {\n+                        // The implicit parameters mask any from the transformer with the same name\n+                        if (PredicatedTransformation.PREDICATE_CONFIG.equals(entry.getValue())\n+                                || PredicatedTransformation.NEGATE_CONFIG.equals(entry.getValue())) {\n+                            log.warn(\"Transformer config {} is masked by implicit config of that name\",\n+                                    entry.getValue());\n+                            return false;\n+                        } else {\n+                            return true;\n+                        }\n+                    });\n+            }\n+\n+            @Override\n+            protected ConfigDef config(Transformation<?> transformation) {\n+                return transformation.config();\n+            }\n+\n+            @Override\n+            protected void validateProps(String prefix) {\n+                String prefixedNegate = prefix + PredicatedTransformation.NEGATE_CONFIG;\n+                String prefixedPredicate = prefix + PredicatedTransformation.PREDICATE_CONFIG;\n+                if (props.containsKey(prefixedNegate) &&\n+                        !props.containsKey(prefixedPredicate)) {\n+                    throw new ConfigException(\"Config '\" + prefixedNegate + \"' was provided \" +\n+                            \"but there is no config '\" + prefixedPredicate + \"' defining a predicate to be negated.\");\n                 }\n             }\n+        }.enrich(newDef);\n \n-            newDef.embed(prefix, group, orderInGroup, transformationConfigDef);\n-        }\n+        new EnrichablePlugin<Predicate<?>>(\"Predicate\", PREDICATES_CONFIG, TRANSFORMS_GROUP,\n+                (Class) Predicate.class, props, requireFullConfig) {\n+            @Override\n+            protected Set<PluginDesc<Predicate<?>>> plugins() {\n+                return (Set) plugins.predicates();\n+            }\n \n+            @Override\n+            protected ConfigDef config(Predicate<?> predicate) {\n+                return predicate.config();\n+            }\n+        }.enrich(newDef);\n         return newDef;\n     }\n \n     /**\n-     * Return {@link ConfigDef} from {@code transformationCls}, which is expected to be a non-null {@code Class<Transformation>},\n-     * by instantiating it and invoking {@link Transformation#config()}.\n+     * An abstraction over \"enrichable plugins\" ({@link Transformation}s and {@link Predicate}s) used for computing the\n+     * contribution to a Connectors ConfigDef.\n+     *\n+     * This is not entirely elegant because\n+     * although they basically use the same \"alias prefix\" configuration idiom there are some differences.\n+     * The abstract method pattern is used to cope with this.\n+     * @param <T> The type of plugin (either {@code Transformation} or {@code Predicate}).\n      */\n-    static ConfigDef getConfigDefFromTransformation(String key, Class<?> transformationCls) {\n-        if (transformationCls == null || !Transformation.class.isAssignableFrom(transformationCls)) {\n-            throw new ConfigException(key, String.valueOf(transformationCls), \"Not a Transformation\");\n-        }\n-        if (Modifier.isAbstract(transformationCls.getModifiers())) {\n-            String childClassNames = Stream.of(transformationCls.getClasses())\n-                .filter(transformationCls::isAssignableFrom)\n-                .filter(c -> !Modifier.isAbstract(c.getModifiers()))\n-                .filter(c -> Modifier.isPublic(c.getModifiers()))\n-                .map(Class::getName)\n-                .collect(Collectors.joining(\", \"));\n-            String message = childClassNames.trim().isEmpty() ?\n-                \"Transformation is abstract and cannot be created.\" :\n-                \"Transformation is abstract and cannot be created. Did you mean \" + childClassNames + \"?\";\n-            throw new ConfigException(key, String.valueOf(transformationCls), message);\n+    static abstract class EnrichablePlugin<T> {\n+\n+        private final String aliasKind;\n+        private final String aliasConfig;\n+        private final String aliasGroup;\n+        private final Class<T> baseClass;\n+        private final Map<String, String> props;\n+        private final boolean requireFullConfig;\n+\n+        public EnrichablePlugin(\n+                String aliasKind,\n+                String aliasConfig, String aliasGroup, Class<T> baseClass,\n+                Map<String, String> props, boolean requireFullConfig) {\n+            this.aliasKind = aliasKind;\n+            this.aliasConfig = aliasConfig;\n+            this.aliasGroup = aliasGroup;\n+            this.baseClass = baseClass;\n+            this.props = props;\n+            this.requireFullConfig = requireFullConfig;\n         }\n-        Transformation transformation;\n-        try {\n-            transformation = transformationCls.asSubclass(Transformation.class).getConstructor().newInstance();\n-        } catch (Exception e) {\n-            ConfigException exception = new ConfigException(key, String.valueOf(transformationCls), \"Error getting config definition from Transformation: \" + e.getMessage());\n-            exception.initCause(e);\n-            throw exception;\n+\n+        /** Add the configs for this alias to the given {@code ConfigDef}. */\n+        void enrich(ConfigDef newDef) {\n+            Object aliases = ConfigDef.parseType(aliasConfig, props.get(aliasConfig), Type.LIST);\n+            if (!(aliases instanceof List)) {\n+                return;\n+            }\n+\n+            LinkedHashSet<?> uniqueAliases = new LinkedHashSet<>((List<?>) aliases);\n+            for (Object o : uniqueAliases) {\n+                if (!(o instanceof String)) {\n+                    throw new ConfigException(\"Item in \" + aliasConfig + \" property is not of \"\n+                            + \"type String\");\n+                }\n+                String alias = (String) o;\n+                final String prefix = aliasConfig + \".\" + alias + \".\";\n+                final String group = aliasGroup + \": \" + alias;\n+                int orderInGroup = 0;\n+\n+                final String typeConfig = prefix + \"type\";\n+                final ConfigDef.Validator typeValidator = new ConfigDef.Validator() {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4MTE4Ng==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r430881186", "bodyText": "this type of comment is not something we use elsewhere and is not immediately obvious what it means. I'd suggest removing instead.", "author": "kkonstantine", "createdAt": "2020-05-27T06:19:23Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/PredicatedTransformation.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import java.util.Map;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.transforms.Transformation;\n+import org.apache.kafka.connect.transforms.predicates.Predicate;\n+\n+/**\n+ * Decorator for a {@link Transformation} which applies the delegate only when a\n+ * {@link Predicate} is true (or false, according to {@code negate}).\n+ * @param <R>\n+ */\n+class PredicatedTransformation<R extends ConnectRecord<R>> implements Transformation<R> {\n+\n+    static final String PREDICATE_CONFIG = \"predicate\";\n+    static final String NEGATE_CONFIG = \"negate\";\n+    /*test*/ Predicate<R> predicate;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4MjE0OA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r430882148", "bodyText": "nit: Our current import style says that these imports stay above the java ones. That's why these show up as changes here.", "author": "kkonstantine", "createdAt": "2020-05-27T06:21:50Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorHandle.java", "diffHunk": "@@ -16,19 +16,21 @@\n  */\n package org.apache.kafka.connect.integration;\n \n-import org.apache.kafka.connect.errors.DataException;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n \n+import org.apache.kafka.connect.errors.DataException;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4Mjg2MQ==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r430882861", "bodyText": "nit: same comment as above", "author": "kkonstantine", "createdAt": "2020-05-27T06:23:50Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/TaskHandle.java", "diffHunk": "@@ -16,15 +16,17 @@\n  */\n package org.apache.kafka.connect.integration;\n \n-import org.apache.kafka.connect.errors.DataException;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Consumer;\n import java.util.stream.IntStream;\n \n+import org.apache.kafka.connect.errors.DataException;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4MzY5OA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r430883698", "bodyText": "This might break existing tests that depend on Connect's integration tests framework. Probably good idea to keep it, in which case the consumer should be ignored.", "author": "kkonstantine", "createdAt": "2020-05-27T06:26:03Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/TaskHandle.java", "diffHunk": "@@ -37,22 +39,26 @@\n     private final ConnectorHandle connectorHandle;\n     private final AtomicInteger partitionsAssigned = new AtomicInteger(0);\n     private final StartAndStopCounter startAndStopCounter = new StartAndStopCounter();\n+    private final Consumer<SinkRecord> consumer;\n \n     private CountDownLatch recordsRemainingLatch;\n     private CountDownLatch recordsToCommitLatch;\n     private int expectedRecords = -1;\n     private int expectedCommits = -1;\n \n-    public TaskHandle(ConnectorHandle connectorHandle, String taskId) {\n-        log.info(\"Created task {} for connector {}\", taskId, connectorHandle);\n+    public TaskHandle(ConnectorHandle connectorHandle, String taskId, Consumer<SinkRecord> consumer) {\n         this.taskId = taskId;\n         this.connectorHandle = connectorHandle;\n+        this.consumer = consumer;\n     }\n \n     /**\n      * Record a message arrival at the task and the connector overall.\n      */\n-    public void record() {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4Mzk5MA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r430883990", "bodyText": "nit: same as above", "author": "kkonstantine", "createdAt": "2020-05-27T06:26:47Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ConnectorConfigTest.java", "diffHunk": "@@ -16,21 +16,23 @@\n  */\n package org.apache.kafka.connect.runtime;\n \n+import java.util.Collections;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDg4NDE3MQ==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r430884171", "bodyText": "nit: 2 extra lines", "author": "kkonstantine", "createdAt": "2020-05-27T06:27:13Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ConnectorConfigTest.java", "diffHunk": "@@ -214,6 +216,187 @@ public void abstractKeyValueTransform() {\n         }\n     }\n \n+", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "643e74e730ab86f724ac1bead403c22d6fc478a2", "url": "https://github.com/apache/kafka/commit/643e74e730ab86f724ac1bead403c22d6fc478a2", "message": "KIP-585/KAFKA-9673: Filter and Conditional SMTs\n\n* Add Predicate interface\n* Add Filter SMT\n* Add the predicate implementations defined in the KIP.\n* Create abstraction in ConnectorConfig for configuring Transformations and Connectors with the \"alias prefix\" mechanism\n* Add tests and fix existing tests.", "committedDate": "2020-05-27T10:47:01Z", "type": "commit"}, {"oid": "2cfe4e1d3a92afc4c5188b8b5f8ac3732c24b527", "url": "https://github.com/apache/kafka/commit/2cfe4e1d3a92afc4c5188b8b5f8ac3732c24b527", "message": "Use Predicates config group", "committedDate": "2020-05-27T10:48:19Z", "type": "commit"}, {"oid": "0f0e1383f29d293441b70e69a28e077f0c343ce6", "url": "https://github.com/apache/kafka/commit/0f0e1383f29d293441b70e69a28e077f0c343ce6", "message": "Some review comments", "committedDate": "2020-05-27T10:48:20Z", "type": "commit"}, {"oid": "6477fe9cd5b537f515695a0099cd1df49bf711b1", "url": "https://github.com/apache/kafka/commit/6477fe9cd5b537f515695a0099cd1df49bf711b1", "message": "fixup", "committedDate": "2020-05-27T10:48:20Z", "type": "commit"}, {"oid": "460c5f826a8272d6fba06ec1926ef3c8e5fc3e2a", "url": "https://github.com/apache/kafka/commit/460c5f826a8272d6fba06ec1926ef3c8e5fc3e2a", "message": "Comment", "committedDate": "2020-05-27T10:48:20Z", "type": "commit"}, {"oid": "fba43c8fef6a14c7a73639bf3358aa1c52673c8e", "url": "https://github.com/apache/kafka/commit/fba43c8fef6a14c7a73639bf3358aa1c52673c8e", "message": "tidy and fix a couple of tests", "committedDate": "2020-05-27T10:49:13Z", "type": "commit"}, {"oid": "f55e14616ee9698d24d0d1e6534cbe477333d7f0", "url": "https://github.com/apache/kafka/commit/f55e14616ee9698d24d0d1e6534cbe477333d7f0", "message": "Numerous fixes and an integration test.", "committedDate": "2020-05-27T10:49:13Z", "type": "commit"}, {"oid": "49b362cd4144fbe2dc10114d3c1a4095c4c30bdb", "url": "https://github.com/apache/kafka/commit/49b362cd4144fbe2dc10114d3c1a4095c4c30bdb", "message": "Review comments", "committedDate": "2020-05-27T10:49:13Z", "type": "commit"}, {"oid": "e76a0b4d4601310bd080e812668466997ac61268", "url": "https://github.com/apache/kafka/commit/e76a0b4d4601310bd080e812668466997ac61268", "message": "Review comments", "committedDate": "2020-05-27T10:49:13Z", "type": "commit"}, {"oid": "e76a0b4d4601310bd080e812668466997ac61268", "url": "https://github.com/apache/kafka/commit/e76a0b4d4601310bd080e812668466997ac61268", "message": "Review comments", "committedDate": "2020-05-27T10:49:13Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI2MzM1Mg==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431263352", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                /**\n          \n          \n            \n                 * Configuration specification for this predicate.\n          \n          \n            \n                 */\n          \n          \n            \n                /**\n          \n          \n            \n                 * Configuration specification for this predicate.\n          \n          \n            \n                 *\n          \n          \n            \n                 * @return the configuration definition for this predicate; never null\n          \n          \n            \n                 */", "author": "rhauch", "createdAt": "2020-05-27T16:06:34Z", "path": "connect/api/src/main/java/org/apache/kafka/connect/transforms/predicates/Predicate.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms.predicates;\n+\n+import org.apache.kafka.common.Configurable;\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+\n+/**\n+ * <p>A predicate on records.\n+ * Predicates can be used to conditionally apply a {@link org.apache.kafka.connect.transforms.Transformation}\n+ * by configuring the transformation's {@code predicate} (and {@code negate}) configuration parameters.\n+ * In particular, the {@code Filter} transformation can be conditionally applied in order to filter\n+ * certain records from further processing.\n+ *\n+ * <p>Implementations of this interface must be public and have a public constructor with no parameters.\n+ *\n+ * @param <R> The type of record.\n+ */\n+public interface Predicate<R extends ConnectRecord<R>> extends Configurable, AutoCloseable {\n+\n+    /**\n+     * Configuration specification for this predicate.\n+     */", "originalCommit": "e76a0b4d4601310bd080e812668466997ac61268", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI2NTUzMw==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431265533", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                /**\n          \n          \n            \n                 * Returns whether the given record satisfies this predicate.\n          \n          \n            \n                 */\n          \n          \n            \n                /**\n          \n          \n            \n                 * Returns whether the given record satisfies this predicate.\n          \n          \n            \n                 *\n          \n          \n            \n                 * @param record the record to evaluate; may not be null\n          \n          \n            \n                 * @return true if the predicate matches, or false otherwise\n          \n          \n            \n                 */", "author": "rhauch", "createdAt": "2020-05-27T16:09:41Z", "path": "connect/api/src/main/java/org/apache/kafka/connect/transforms/predicates/Predicate.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms.predicates;\n+\n+import org.apache.kafka.common.Configurable;\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+\n+/**\n+ * <p>A predicate on records.\n+ * Predicates can be used to conditionally apply a {@link org.apache.kafka.connect.transforms.Transformation}\n+ * by configuring the transformation's {@code predicate} (and {@code negate}) configuration parameters.\n+ * In particular, the {@code Filter} transformation can be conditionally applied in order to filter\n+ * certain records from further processing.\n+ *\n+ * <p>Implementations of this interface must be public and have a public constructor with no parameters.\n+ *\n+ * @param <R> The type of record.\n+ */\n+public interface Predicate<R extends ConnectRecord<R>> extends Configurable, AutoCloseable {\n+\n+    /**\n+     * Configuration specification for this predicate.\n+     */\n+    ConfigDef config();\n+\n+    /**\n+     * Returns whether the given record satisfies this predicate.\n+     */", "originalCommit": "e76a0b4d4601310bd080e812668466997ac61268", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI4MTE1OQ==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431281159", "bodyText": "Should we wait until all brokers and Connect workers are available, via something like:\n        connect.assertions().assertExactlyNumBrokersAreUp(numBrokers, \"Brokers did not start in time.\");\n        connect.assertions().assertExactlyNumWorkersAreUp(numWorkers, \"Worker did not start in time.\");", "author": "rhauch", "createdAt": "2020-05-27T16:33:26Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/TransformationIntegrationTest.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.connect.storage.StringConverter;\n+import org.apache.kafka.connect.transforms.Filter;\n+import org.apache.kafka.connect.transforms.predicates.HasHeaderKey;\n+import org.apache.kafka.connect.transforms.predicates.RecordIsTombstone;\n+import org.apache.kafka.connect.transforms.predicates.TopicNameMatches;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.KEY_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.PREDICATES_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.TASKS_MAX_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.TRANSFORMS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.VALUE_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.SinkConnectorConfig.TOPICS_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.OFFSET_COMMIT_INTERVAL_MS_CONFIG;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+\n+/**\n+ * An integration test for connectors with transformations\n+ */\n+@Category(IntegrationTest.class)\n+public class TransformationIntegrationTest {\n+\n+    private static final int NUM_RECORDS_PRODUCED = 2000;\n+    private static final int NUM_TOPIC_PARTITIONS = 3;\n+    private static final long RECORD_TRANSFER_DURATION_MS = TimeUnit.SECONDS.toMillis(30);\n+    private static final long OBSERVED_RECORDS_DURATION_MS = TimeUnit.SECONDS.toMillis(60);\n+    private static final int NUM_TASKS = 3;\n+    private static final int NUM_WORKERS = 3;\n+    private static final String CONNECTOR_NAME = \"simple-conn\";\n+    private static final String SINK_CONNECTOR_CLASS_NAME = MonitorableSinkConnector.class.getSimpleName();\n+    private static final String SOURCE_CONNECTOR_CLASS_NAME = MonitorableSourceConnector.class.getSimpleName();\n+\n+    private EmbeddedConnectCluster connect;\n+    private ConnectorHandle connectorHandle;\n+\n+    @Before\n+    public void setup() {\n+        // setup Connect worker properties\n+        Map<String, String> exampleWorkerProps = new HashMap<>();\n+        exampleWorkerProps.put(OFFSET_COMMIT_INTERVAL_MS_CONFIG, String.valueOf(5_000));\n+\n+        // setup Kafka broker properties\n+        Properties exampleBrokerProps = new Properties();\n+        exampleBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connect = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(NUM_WORKERS)\n+                .numBrokers(1)\n+                .workerProps(exampleWorkerProps)\n+                .brokerProps(exampleBrokerProps)\n+                .build();\n+\n+        // start the clusters\n+        connect.start();", "originalCommit": "e76a0b4d4601310bd080e812668466997ac61268", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI4MjcxMg==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431282712", "bodyText": "This is an asynchronous method, and it's likely the connector will not be started and running before the test proceeds to the next statements. This can lead to very flaky tests.\nWe could instead wait until the connector is actually running, using something like:\n        connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(CONNECTOR_NAME, NUM_TASKS,\n                \"Connector tasks did not start in time.\");", "author": "rhauch", "createdAt": "2020-05-27T16:35:32Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/TransformationIntegrationTest.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.connect.storage.StringConverter;\n+import org.apache.kafka.connect.transforms.Filter;\n+import org.apache.kafka.connect.transforms.predicates.HasHeaderKey;\n+import org.apache.kafka.connect.transforms.predicates.RecordIsTombstone;\n+import org.apache.kafka.connect.transforms.predicates.TopicNameMatches;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.KEY_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.PREDICATES_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.TASKS_MAX_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.TRANSFORMS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.VALUE_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.SinkConnectorConfig.TOPICS_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.OFFSET_COMMIT_INTERVAL_MS_CONFIG;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+\n+/**\n+ * An integration test for connectors with transformations\n+ */\n+@Category(IntegrationTest.class)\n+public class TransformationIntegrationTest {\n+\n+    private static final int NUM_RECORDS_PRODUCED = 2000;\n+    private static final int NUM_TOPIC_PARTITIONS = 3;\n+    private static final long RECORD_TRANSFER_DURATION_MS = TimeUnit.SECONDS.toMillis(30);\n+    private static final long OBSERVED_RECORDS_DURATION_MS = TimeUnit.SECONDS.toMillis(60);\n+    private static final int NUM_TASKS = 3;\n+    private static final int NUM_WORKERS = 3;\n+    private static final String CONNECTOR_NAME = \"simple-conn\";\n+    private static final String SINK_CONNECTOR_CLASS_NAME = MonitorableSinkConnector.class.getSimpleName();\n+    private static final String SOURCE_CONNECTOR_CLASS_NAME = MonitorableSourceConnector.class.getSimpleName();\n+\n+    private EmbeddedConnectCluster connect;\n+    private ConnectorHandle connectorHandle;\n+\n+    @Before\n+    public void setup() {\n+        // setup Connect worker properties\n+        Map<String, String> exampleWorkerProps = new HashMap<>();\n+        exampleWorkerProps.put(OFFSET_COMMIT_INTERVAL_MS_CONFIG, String.valueOf(5_000));\n+\n+        // setup Kafka broker properties\n+        Properties exampleBrokerProps = new Properties();\n+        exampleBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connect = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(NUM_WORKERS)\n+                .numBrokers(1)\n+                .workerProps(exampleWorkerProps)\n+                .brokerProps(exampleBrokerProps)\n+                .build();\n+\n+        // start the clusters\n+        connect.start();\n+\n+        // get a handle to the connector\n+        connectorHandle = RuntimeHandles.get().connectorHandle(CONNECTOR_NAME);\n+    }\n+\n+    @After\n+    public void close() {\n+        // delete connector handle\n+        RuntimeHandles.get().deleteConnector(CONNECTOR_NAME);\n+\n+        // stop all Connect, Kafka and Zk threads.\n+        connect.stop();\n+    }\n+\n+    /**\n+     * Test the {@link Filter} transformer with a\n+     * {@link TopicNameMatches} predicate on a sink connector.\n+     */\n+    @Test\n+    public void testFilterOnTopicNameWithSinkConnector() throws Exception {\n+        Map<String, Long> observedRecords = observeRecords();\n+\n+        // create test topics\n+        String fooTopic = \"foo-topic\";\n+        String barTopic = \"bar-topic\";\n+        int numFooRecords = NUM_RECORDS_PRODUCED;\n+        int numBarRecords = NUM_RECORDS_PRODUCED;\n+        connect.kafka().createTopic(fooTopic, NUM_TOPIC_PARTITIONS);\n+        connect.kafka().createTopic(barTopic, NUM_TOPIC_PARTITIONS);\n+\n+        // setup up props for the sink connector\n+        Map<String, String> props = new HashMap<>();\n+        props.put(\"name\", CONNECTOR_NAME);\n+        props.put(CONNECTOR_CLASS_CONFIG, SINK_CONNECTOR_CLASS_NAME);\n+        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));\n+        props.put(TOPICS_CONFIG, String.join(\",\", fooTopic, barTopic));\n+        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(TRANSFORMS_CONFIG, \"filter\");\n+        props.put(TRANSFORMS_CONFIG + \".filter.type\", Filter.class.getSimpleName());\n+        props.put(TRANSFORMS_CONFIG + \".filter.predicate\", \"barPredicate\");\n+        props.put(PREDICATES_CONFIG, \"barPredicate\");\n+        props.put(PREDICATES_CONFIG + \".barPredicate.type\", TopicNameMatches.class.getSimpleName());\n+        props.put(PREDICATES_CONFIG + \".barPredicate.pattern\", \"bar-.*\");\n+\n+        // expect all records to be consumed by the connector\n+        connectorHandle.expectedRecords(numFooRecords);\n+\n+        // expect all records to be consumed by the connector\n+        connectorHandle.expectedCommits(numFooRecords);\n+\n+        // start a sink connector\n+        connect.configureConnector(CONNECTOR_NAME, props);", "originalCommit": "e76a0b4d4601310bd080e812668466997ac61268", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI4NjE2MA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431286160", "bodyText": "Here the default is null, which means that the configuration validation allows the name field to not be set. Per the KIP, we want to require that name is set. To do that, we should use ConfigDef.NO_DEFAULT_VALUE:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static final ConfigDef CONFIG_DEF = new ConfigDef().define(NAME_CONFIG, ConfigDef.Type.STRING, null,\n          \n          \n            \n                private static final ConfigDef CONFIG_DEF = new ConfigDef().define(NAME_CONFIG, ConfigDef.Type.STRING, ConfigDef.NO_DEFAULT_VALUE,", "author": "rhauch", "createdAt": "2020-05-27T16:39:23Z", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/HasHeaderKey.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms.predicates;\n+\n+import java.util.Iterator;\n+import java.util.Map;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+/**\n+ * A predicate which is true for records with at least one header with the configured name.\n+ * @param <R> The type of connect record.\n+ */\n+public class HasHeaderKey<R extends ConnectRecord<R>> implements Predicate<R> {\n+\n+    private static final String NAME_CONFIG = \"name\";\n+    private static final ConfigDef CONFIG_DEF = new ConfigDef().define(NAME_CONFIG, ConfigDef.Type.STRING, null,", "originalCommit": "e76a0b4d4601310bd080e812668466997ac61268", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMwNTI3Ng==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431305276", "bodyText": "Ah, thanks, I'd not realised that was the point of ConfigDef.NO_DEFAULT_VALUE.", "author": "tombentley", "createdAt": "2020-05-27T17:08:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI4NjE2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI4NzYwOA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431287608", "bodyText": "The default doesn't match the KIP. Either we should update the KIP to accept .* as the default pattern, or we should use ConfigDef.NO_DEFAULT_VALUE as the default to require the pattern to be set.", "author": "rhauch", "createdAt": "2020-05-27T16:41:55Z", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/TopicNameMatches.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms.predicates;\n+\n+import java.util.Map;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.transforms.util.RegexValidator;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+/**\n+ * A predicate which is true for records with a topic name that matches the configured regular expression.\n+ * @param <R> The type of connect record.\n+ */\n+public class TopicNameMatches<R extends ConnectRecord<R>> implements Predicate<R> {\n+\n+    private static final String PATTERN_CONFIG = \"pattern\";\n+    private static final ConfigDef CONFIG_DEF = new ConfigDef().define(PATTERN_CONFIG, ConfigDef.Type.STRING, \".*\",\n+            new RegexValidator(), ConfigDef.Importance.MEDIUM,\n+            \"A Java regular expression for matching against the name of a record's topic.\");", "originalCommit": "e76a0b4d4601310bd080e812668466997ac61268", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMwODcyNA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431308724", "bodyText": "I changed it to .* only when I realised that the default had to be valid and before I knew about NO_DEFAULT_VALUE, so using NO_DEFAULT_VALUE is good. Thanks!", "author": "tombentley", "createdAt": "2020-05-27T17:13:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI4NzYwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI4ODkxMw==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431288913", "bodyText": "Can we ever get to line 64? The constructor of the config (line 58) should fail if the pattern validator fails to ensure the pattern is a valid regex, which means that if we make it past 58 then line 62 will never fail.\nAm I missing something?", "author": "rhauch", "createdAt": "2020-05-27T16:43:59Z", "path": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/TopicNameMatches.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms.predicates;\n+\n+import java.util.Map;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.transforms.util.RegexValidator;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+\n+/**\n+ * A predicate which is true for records with a topic name that matches the configured regular expression.\n+ * @param <R> The type of connect record.\n+ */\n+public class TopicNameMatches<R extends ConnectRecord<R>> implements Predicate<R> {\n+\n+    private static final String PATTERN_CONFIG = \"pattern\";\n+    private static final ConfigDef CONFIG_DEF = new ConfigDef().define(PATTERN_CONFIG, ConfigDef.Type.STRING, \".*\",\n+            new RegexValidator(), ConfigDef.Importance.MEDIUM,\n+            \"A Java regular expression for matching against the name of a record's topic.\");\n+    private Pattern pattern;\n+\n+    @Override\n+    public ConfigDef config() {\n+        return CONFIG_DEF;\n+    }\n+\n+    @Override\n+    public boolean test(R record) {\n+        return record.topic() != null && pattern.matcher(record.topic()).matches();\n+    }\n+\n+    @Override\n+    public void close() {\n+\n+    }\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+        SimpleConfig simpleConfig = new SimpleConfig(config(), configs);\n+        Pattern result;\n+        String value = simpleConfig.getString(PATTERN_CONFIG);\n+        try {\n+            result = Pattern.compile(value);\n+        } catch (PatternSyntaxException e) {\n+            throw new ConfigException(PATTERN_CONFIG, value, \"entry must be a Java-compatible regular expression: \" + e.getMessage());\n+        }", "originalCommit": "e76a0b4d4601310bd080e812668466997ac61268", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "602a89dad35f2340c48f7c0d0336382c7e94cfb0", "url": "https://github.com/apache/kafka/commit/602a89dad35f2340c48f7c0d0336382c7e94cfb0", "message": "Apply suggestions from code review\n\nCo-authored-by: Randall Hauch <rhauch@gmail.com>", "committedDate": "2020-05-27T17:09:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMyMTAxNA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431321014", "bodyText": "Maybe a few more tests that test the ConfigDef and AbstractConfig validation:\n    @Test\n    public void testNameRequiredInConfig() {\n        Map<String, String> props = new HashMap<>();\n        ConfigException e = assertThrows(ConfigException.class, () -> config(props));\n        assertTrue(e.getMessage().contains(\"Missing required configuration \\\"name\\\"\"));\n    }\n\n    @Test\n    public void testNameMayNotBeEmptyInConfig() {\n        Map<String, String> props = new HashMap<>();\n        props.put(\"name\", \"\");\n        ConfigException e = assertThrows(ConfigException.class, () -> config(props));\n        assertTrue(e.getMessage().contains(\"String must be non-empty\"));\n    }\n\n    protected SimpleConfig config(Map<String, String> props) {\n        return new SimpleConfig(new HasHeaderKey().config(), props);\n    }\n\nBTW, note that the new HasHeaderKey().config() is required because there is no accessible static ConfigDef. Might want to just make the static field package protected.", "author": "rhauch", "createdAt": "2020-05-27T17:34:30Z", "path": "connect/transforms/src/test/java/org/apache/kafka/connect/transforms/predicates/HasHeaderKeyTest.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms.predicates;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import org.apache.kafka.common.config.ConfigValue;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.source.SourceRecord;\n+import org.junit.Test;\n+\n+import static java.util.Collections.singletonList;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+public class HasHeaderKeyTest {\n+", "originalCommit": "602a89dad35f2340c48f7c0d0336382c7e94cfb0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTY1MjM0OA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431652348", "bodyText": "The empty string is a valid regex, so I used CompositeValidator.of(new NonEmptyString(), new RegexValidator()) in TopicNameMatches, rather than just new RegexValidator().", "author": "tombentley", "createdAt": "2020-05-28T08:01:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMyMTAxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTY1MzMxOQ==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431653319", "bodyText": "Oops, replied to wrong comment, but I'm sure you guessed what I mean.", "author": "tombentley", "createdAt": "2020-05-28T08:03:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMyMTAxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMyODQ2NA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431328464", "bodyText": "Maybe a few more tests that test the ConfigDef and AbstractConfig validation:\n    @Test\n    public void testPatternRequiredInConfig() {\n        Map<String, String> props = new HashMap<>();\n        ConfigException e = assertThrows(ConfigException.class, () -> config(props));\n        assertTrue(e.getMessage().contains(\"Missing required configuration \\\"pattern\\\"\"));\n    }\n\n    @Test\n    public void testPatternMayNotBeEmptyInConfig() {\n        Map<String, String> props = new HashMap<>();\n        props.put(\"pattern\", \"\");\n        ConfigException e = assertThrows(ConfigException.class, () -> config(props));\n        System.out.println(e.getMessage());\n        assertTrue(e.getMessage().contains(\"String must be non-empty\"));\n    }\n\n    @Test\n    public void testPatternIsValidRegexInConfig() {\n        Map<String, String> props = new HashMap<>();\n        props.put(\"pattern\", \"[\");\n        ConfigException e = assertThrows(ConfigException.class, () -> config(props));\n        System.out.println(e.getMessage());\n        assertTrue(e.getMessage().contains(\"Invalid regex\"));\n    }\n\n    protected SimpleConfig config(Map<String, String> props) {\n        return new SimpleConfig(new TopicNameMatches().config(), props);\n    }", "author": "rhauch", "createdAt": "2020-05-27T17:47:12Z", "path": "connect/transforms/src/test/java/org/apache/kafka/connect/transforms/predicates/TopicNameMatchesTest.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.transforms.predicates;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.kafka.common.config.ConfigValue;\n+import org.apache.kafka.connect.source.SourceRecord;\n+import org.junit.Test;\n+\n+import static java.util.Collections.singletonList;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TopicNameMatchesTest {\n+", "originalCommit": "602a89dad35f2340c48f7c0d0336382c7e94cfb0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMzMzQ4NA==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431333484", "bodyText": "Should we also check the other two predicate implementations, too? I know it's not strictly required, but it would help to better ensure the pattern matches more than just one implementation.", "author": "rhauch", "createdAt": "2020-05-27T17:52:56Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginUtilsTest.java", "diffHunk": "@@ -128,6 +131,10 @@ public void testAllowedConnectFrameworkClasses() {\n         assertTrue(PluginUtils.shouldLoadInIsolation(\n                 \"org.apache.kafka.connect.transforms.ExtractField$Key\")\n         );\n+        assertTrue(PluginUtils.shouldLoadInIsolation(\"org.apache.kafka.connect.transforms.predicates.\"));\n+        assertTrue(PluginUtils.shouldLoadInIsolation(\n+                \"org.apache.kafka.connect.transforms.predicates.TopicNameMatches\")\n+        );", "originalCommit": "602a89dad35f2340c48f7c0d0336382c7e94cfb0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MjMyMQ==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431542321", "bodyText": "Given the small number of predicates, I agree with @rhauch", "author": "kkonstantine", "createdAt": "2020-05-28T02:07:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMzMzQ4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMzOTg1Nw==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431339857", "bodyText": "It'd be good to have the test reflect the current behavior.", "author": "rhauch", "createdAt": "2020-05-27T18:00:39Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ConnectorConfigTest.java", "diffHunk": "@@ -214,6 +216,185 @@ public void abstractKeyValueTransform() {\n         }\n     }\n \n+    @Test(expected = ConfigException.class)\n+    public void wrongPredicateType() {\n+        Map<String, String> props = new HashMap<>();\n+        props.put(\"name\", \"test\");\n+        props.put(\"connector.class\", TestConnector.class.getName());\n+        props.put(\"transforms\", \"a\");\n+        props.put(\"transforms.a.type\", SimpleTransformation.class.getName());\n+        props.put(\"transforms.a.magic.number\", \"42\");\n+        props.put(\"transforms.a.predicate\", \"my-pred\");\n+        props.put(\"predicates\", \"my-pred\");\n+        props.put(\"predicates.my-pred.type\", TestConnector.class.getName());\n+        new ConnectorConfig(MOCK_PLUGINS, props);\n+    }\n+\n+    @Test\n+    public void singleConditionalTransform() {\n+        Map<String, String> props = new HashMap<>();\n+        props.put(\"name\", \"test\");\n+        props.put(\"connector.class\", TestConnector.class.getName());\n+        props.put(\"transforms\", \"a\");\n+        props.put(\"transforms.a.type\", SimpleTransformation.class.getName());\n+        props.put(\"transforms.a.magic.number\", \"42\");\n+        props.put(\"transforms.a.predicate\", \"my-pred\");\n+        props.put(\"transforms.a.negate\", \"true\");\n+        props.put(\"predicates\", \"my-pred\");\n+        props.put(\"predicates.my-pred.type\", TestPredicate.class.getName());\n+        props.put(\"predicates.my-pred.int\", \"84\");\n+        assertPredicatedTransform(props, true);\n+    }\n+\n+    @Test\n+    public void predicateNegationDefaultsToFalse() {\n+        Map<String, String> props = new HashMap<>();\n+        props.put(\"name\", \"test\");\n+        props.put(\"connector.class\", TestConnector.class.getName());\n+        props.put(\"transforms\", \"a\");\n+        props.put(\"transforms.a.type\", SimpleTransformation.class.getName());\n+        props.put(\"transforms.a.magic.number\", \"42\");\n+        props.put(\"transforms.a.predicate\", \"my-pred\");\n+        props.put(\"predicates\", \"my-pred\");\n+        props.put(\"predicates.my-pred.type\", TestPredicate.class.getName());\n+        props.put(\"predicates.my-pred.int\", \"84\");\n+        assertPredicatedTransform(props, false);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void abstractPredicate() {\n+        Map<String, String> props = new HashMap<>();\n+        props.put(\"name\", \"test\");\n+        props.put(\"connector.class\", TestConnector.class.getName());\n+        props.put(\"transforms\", \"a\");\n+        props.put(\"transforms.a.type\", SimpleTransformation.class.getName());\n+        props.put(\"transforms.a.magic.number\", \"42\");\n+        props.put(\"transforms.a.predicate\", \"my-pred\");\n+        props.put(\"predicates\", \"my-pred\");\n+        props.put(\"predicates.my-pred.type\", AbstractTestPredicate.class.getName());\n+        props.put(\"predicates.my-pred.int\", \"84\");\n+        assertPredicatedTransform(props, false);\n+    }\n+\n+    private void assertPredicatedTransform(Map<String, String> props, boolean expectedNegated) {\n+        final ConnectorConfig config = new ConnectorConfig(MOCK_PLUGINS, props);\n+        final List<Transformation<R>> transformations = config.transformations();\n+        assertEquals(1, transformations.size());\n+        assertTrue(transformations.get(0) instanceof PredicatedTransformation);\n+        PredicatedTransformation<?> predicated = (PredicatedTransformation<?>) transformations.get(0);\n+\n+        assertEquals(expectedNegated, predicated.negate);\n+\n+        assertTrue(predicated.delegate instanceof ConnectorConfigTest.SimpleTransformation);\n+        assertEquals(42, ((SimpleTransformation<?>) predicated.delegate).magicNumber);\n+\n+        assertTrue(predicated.predicate instanceof ConnectorConfigTest.TestPredicate);\n+        assertEquals(84, ((TestPredicate<?>) predicated.predicate).param);\n+\n+        predicated.close();\n+\n+        assertEquals(0, ((SimpleTransformation<?>) predicated.delegate).magicNumber);\n+        assertEquals(0, ((TestPredicate<?>) predicated.predicate).param);\n+    }\n+\n+    @Test\n+    public void misconfiguredPredicate() {\n+        Map<String, String> props = new HashMap<>();\n+        props.put(\"name\", \"test\");\n+        props.put(\"connector.class\", TestConnector.class.getName());\n+        props.put(\"transforms\", \"a\");\n+        props.put(\"transforms.a.type\", SimpleTransformation.class.getName());\n+        props.put(\"transforms.a.magic.number\", \"42\");\n+        props.put(\"transforms.a.predicate\", \"my-pred\");\n+        props.put(\"transforms.a.negate\", \"true\");\n+        props.put(\"predicates\", \"my-pred\");\n+        props.put(\"predicates.my-pred.type\", TestPredicate.class.getName());\n+        props.put(\"predicates.my-pred.int\", \"79\");\n+        try {\n+            new ConnectorConfig(MOCK_PLUGINS, props);\n+            fail();\n+        } catch (ConfigException e) {\n+            assertTrue(e.getMessage().contains(\"Value must be at least 80\"));\n+        }\n+    }\n+\n+    @Ignore(\"Is this really an error. There's no actual need for the predicates config (unlike transforms where it defines the order).\")", "originalCommit": "602a89dad35f2340c48f7c0d0336382c7e94cfb0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM0MTA3Mg==", "url": "https://github.com/apache/kafka/pull/8699#discussion_r431341072", "bodyText": "We've moved to using assertThrows here, which would look something like:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    try {\n          \n          \n            \n                        new ConnectorConfig(MOCK_PLUGINS, props);\n          \n          \n            \n                        fail();\n          \n          \n            \n                    } catch (ConfigException e) {\n          \n          \n            \n                        assertTrue(e.getMessage().contains(\"Value must be at least 80\"));\n          \n          \n            \n                    }\n          \n          \n            \n                    ConfigException e = assertThrows(ConfigException.class, () -> new ConnectorConfig(MOCK_PLUGINS, props));\n          \n          \n            \n                    assertTrue(e.getMessage().contains(\"Value must be at least 42\"));", "author": "rhauch", "createdAt": "2020-05-27T18:02:56Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ConnectorConfigTest.java", "diffHunk": "@@ -214,6 +216,185 @@ public void abstractKeyValueTransform() {\n         }\n     }\n \n+    @Test(expected = ConfigException.class)\n+    public void wrongPredicateType() {\n+        Map<String, String> props = new HashMap<>();\n+        props.put(\"name\", \"test\");\n+        props.put(\"connector.class\", TestConnector.class.getName());\n+        props.put(\"transforms\", \"a\");\n+        props.put(\"transforms.a.type\", SimpleTransformation.class.getName());\n+        props.put(\"transforms.a.magic.number\", \"42\");\n+        props.put(\"transforms.a.predicate\", \"my-pred\");\n+        props.put(\"predicates\", \"my-pred\");\n+        props.put(\"predicates.my-pred.type\", TestConnector.class.getName());\n+        new ConnectorConfig(MOCK_PLUGINS, props);\n+    }\n+\n+    @Test\n+    public void singleConditionalTransform() {\n+        Map<String, String> props = new HashMap<>();\n+        props.put(\"name\", \"test\");\n+        props.put(\"connector.class\", TestConnector.class.getName());\n+        props.put(\"transforms\", \"a\");\n+        props.put(\"transforms.a.type\", SimpleTransformation.class.getName());\n+        props.put(\"transforms.a.magic.number\", \"42\");\n+        props.put(\"transforms.a.predicate\", \"my-pred\");\n+        props.put(\"transforms.a.negate\", \"true\");\n+        props.put(\"predicates\", \"my-pred\");\n+        props.put(\"predicates.my-pred.type\", TestPredicate.class.getName());\n+        props.put(\"predicates.my-pred.int\", \"84\");\n+        assertPredicatedTransform(props, true);\n+    }\n+\n+    @Test\n+    public void predicateNegationDefaultsToFalse() {\n+        Map<String, String> props = new HashMap<>();\n+        props.put(\"name\", \"test\");\n+        props.put(\"connector.class\", TestConnector.class.getName());\n+        props.put(\"transforms\", \"a\");\n+        props.put(\"transforms.a.type\", SimpleTransformation.class.getName());\n+        props.put(\"transforms.a.magic.number\", \"42\");\n+        props.put(\"transforms.a.predicate\", \"my-pred\");\n+        props.put(\"predicates\", \"my-pred\");\n+        props.put(\"predicates.my-pred.type\", TestPredicate.class.getName());\n+        props.put(\"predicates.my-pred.int\", \"84\");\n+        assertPredicatedTransform(props, false);\n+    }\n+\n+    @Test(expected = ConfigException.class)\n+    public void abstractPredicate() {\n+        Map<String, String> props = new HashMap<>();\n+        props.put(\"name\", \"test\");\n+        props.put(\"connector.class\", TestConnector.class.getName());\n+        props.put(\"transforms\", \"a\");\n+        props.put(\"transforms.a.type\", SimpleTransformation.class.getName());\n+        props.put(\"transforms.a.magic.number\", \"42\");\n+        props.put(\"transforms.a.predicate\", \"my-pred\");\n+        props.put(\"predicates\", \"my-pred\");\n+        props.put(\"predicates.my-pred.type\", AbstractTestPredicate.class.getName());\n+        props.put(\"predicates.my-pred.int\", \"84\");\n+        assertPredicatedTransform(props, false);\n+    }\n+\n+    private void assertPredicatedTransform(Map<String, String> props, boolean expectedNegated) {\n+        final ConnectorConfig config = new ConnectorConfig(MOCK_PLUGINS, props);\n+        final List<Transformation<R>> transformations = config.transformations();\n+        assertEquals(1, transformations.size());\n+        assertTrue(transformations.get(0) instanceof PredicatedTransformation);\n+        PredicatedTransformation<?> predicated = (PredicatedTransformation<?>) transformations.get(0);\n+\n+        assertEquals(expectedNegated, predicated.negate);\n+\n+        assertTrue(predicated.delegate instanceof ConnectorConfigTest.SimpleTransformation);\n+        assertEquals(42, ((SimpleTransformation<?>) predicated.delegate).magicNumber);\n+\n+        assertTrue(predicated.predicate instanceof ConnectorConfigTest.TestPredicate);\n+        assertEquals(84, ((TestPredicate<?>) predicated.predicate).param);\n+\n+        predicated.close();\n+\n+        assertEquals(0, ((SimpleTransformation<?>) predicated.delegate).magicNumber);\n+        assertEquals(0, ((TestPredicate<?>) predicated.predicate).param);\n+    }\n+\n+    @Test\n+    public void misconfiguredPredicate() {\n+        Map<String, String> props = new HashMap<>();\n+        props.put(\"name\", \"test\");\n+        props.put(\"connector.class\", TestConnector.class.getName());\n+        props.put(\"transforms\", \"a\");\n+        props.put(\"transforms.a.type\", SimpleTransformation.class.getName());\n+        props.put(\"transforms.a.magic.number\", \"42\");\n+        props.put(\"transforms.a.predicate\", \"my-pred\");\n+        props.put(\"transforms.a.negate\", \"true\");\n+        props.put(\"predicates\", \"my-pred\");\n+        props.put(\"predicates.my-pred.type\", TestPredicate.class.getName());\n+        props.put(\"predicates.my-pred.int\", \"79\");\n+        try {\n+            new ConnectorConfig(MOCK_PLUGINS, props);\n+            fail();\n+        } catch (ConfigException e) {\n+            assertTrue(e.getMessage().contains(\"Value must be at least 80\"));\n+        }", "originalCommit": "602a89dad35f2340c48f7c0d0336382c7e94cfb0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "03ce156772cc890cf0abf6529e960d0c5809dfbe", "url": "https://github.com/apache/kafka/commit/03ce156772cc890cf0abf6529e960d0c5809dfbe", "message": "KAFKA-9673: Fix a few unit and integration tests", "committedDate": "2020-05-27T22:39:53Z", "type": "commit"}, {"oid": "60cd39eb0e17cdf7e3fa52886391e774460123c7", "url": "https://github.com/apache/kafka/commit/60cd39eb0e17cdf7e3fa52886391e774460123c7", "message": "KAFKA-9673: Added some tests, made others a bit more robust, and added more validation to TopicNameMatches", "committedDate": "2020-05-27T23:43:07Z", "type": "commit"}, {"oid": "cbc89818ce2a9ac9dae9ca02288f9d71b1a851da", "url": "https://github.com/apache/kafka/commit/cbc89818ce2a9ac9dae9ca02288f9d71b1a851da", "message": "Trivial correction", "committedDate": "2020-05-28T08:23:37Z", "type": "commit"}]}