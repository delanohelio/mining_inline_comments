{"pr_number": 8657, "pr_title": "KAFKA-8334 Make sure the thread which tries to complete delayed reque\u2026", "pr_createdAt": "2020-05-12T17:23:47Z", "pr_url": "https://github.com/apache/kafka/pull/8657", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwNjc2MA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r423906760", "bodyText": "It collects the \"key\" used to complete delayed requests. The completion is execute out of group lock.", "author": "chia7712", "createdAt": "2020-05-12T17:25:21Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -377,6 +402,7 @@ class GroupCoordinator(val brokerId: Int,\n                           groupInstanceId: Option[String],\n                           groupAssignment: Map[String, Array[Byte]],\n                           responseCallback: SyncCallback): Unit = {\n+    val partitionsToComplete = mutable.Map[TopicPartition, Boolean]()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwNzYzOA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r423907638", "bodyText": "this is the main change of this PR (address #6915 (comment)).", "author": "chia7712", "createdAt": "2020-05-12T17:26:53Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,20 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\". There was a lot of cases that hold a lock and then try to\n+   * hold more locks to complete delayed requests. Unfortunately, that scenario causes deadlock and we had introduced\n+   * the \"tryLock\" to avoid deadlock. However, the \"tryLock\" causes another issue that thread_A holds a lock but it does\n+   * not complete the delayed requests and there are no threads can complete request as the lock is not free.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * We introduces a flag, called completeDelayedRequests, to prevent the method from automatically completing delayed\n+   * request.\n    */\n   private[server] def maybeTryComplete(): Boolean = {\n-    var retry = false\n-    var done = false\n-    do {\n-      if (lock.tryLock()) {\n-        try {\n-          tryCompletePending.set(false)\n-          done = tryComplete()\n-        } finally {\n-          lock.unlock()\n-        }\n-        // While we were holding the lock, another thread may have invoked `maybeTryComplete` and set\n-        // `tryCompletePending`. In this case we should retry.\n-        retry = tryCompletePending.get()\n-      } else {\n-        // Another thread is holding the lock. If `tryCompletePending` is already set and this thread failed to\n-        // acquire the lock, then the thread that is holding the lock is guaranteed to see the flag and retry.\n-        // Otherwise, we should set the flag and retry on this thread since the thread holding the lock may have\n-        // released the lock and returned by the time the flag is set.\n-        retry = !tryCompletePending.getAndSet(true)\n-      }\n-    } while (!isCompleted && retry)\n-    done\n+    lock.lock()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwODA4NA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r423908084", "bodyText": "new check for this PR. Make sure it does not hold group lock", "author": "chia7712", "createdAt": "2020-05-12T17:27:34Z", "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala", "diffHunk": "@@ -3750,20 +3758,25 @@ class GroupCoordinatorTest {\n     val (responseFuture, responseCallback) = setupSyncGroupCallback\n \n     val capturedArgument: Capture[scala.collection.Map[TopicPartition, PartitionResponse] => Unit] = EasyMock.newCapture()\n-\n-    EasyMock.expect(replicaManager.appendRecords(EasyMock.anyLong(),\n-      EasyMock.anyShort(),\n+    EasyMock.expect(replicaManager.completeDelayedRequests(EasyMock.anyObject()))\n+      // No lock is held when completing delayed requests\n+      .andAnswer(() => assertFalse(groupCoordinator.groupManager.getGroup(groupId).get.lock.isLocked))", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwODQ2OA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r423908468", "bodyText": "This test case is for \"tryLock\" so I just remove it.", "author": "chia7712", "createdAt": "2020-05-12T17:28:09Z", "path": "core/src/test/scala/unit/kafka/server/DelayedOperationTest.scala", "diffHunk": "@@ -192,43 +191,6 @@ class DelayedOperationTest {\n     assertEquals(Nil, cancelledOperations)\n   }\n \n-  /**\n-    * Verify that if there is lock contention between two threads attempting to complete,\n-    * completion is performed without any blocking in either thread.\n-    */\n-  @Test\n-  def testTryCompleteLockContention(): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxMzI0Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r423913246", "bodyText": "This is another case of deadlock. DelayedJoin#tryComplete is in a group lock and it tries to complete other delayed joins which related to same __consumer_offsets partition.\nHence, this PR make it control the group lock manually in order to make sure it does not hold group lock when calling GroupCoordinator#onCompleteJoin", "author": "chia7712", "createdAt": "2020-05-12T17:35:42Z", "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +33,15 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since GroupCoordinator#onCompleteJoin() tries to complete delayed requests.\n+   */\n+  override def tryComplete(): Boolean = if (group.inLock(group.hasAllMembersJoined)) forceComplete() else false\n+  override def onExpiration(): Unit = coordinator.onExpireJoin()\n+  override def onComplete(): Unit = coordinator.onCompleteJoin(group)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MzYyNw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r426943627", "bodyText": "Currently we have a somewhat convoluted model where ReplicaManager creates delayed operations, but we depend on lower level components like Partition to be aware of them and complete them. This breaks encapsulation.\nNot something we should try to complete in this PR, but as an eventual goal, I think we can consider trying to factor delayed operations out of Partition so that they can be managed by ReplicaManager exclusively. If you assume that is the end state, then we could drop completeDelayedRequests and let ReplicaManager always be responsible for checking delayed operations after appending to the log.\nOther than ReplicaManager, the only caller of this method is GroupMetadataManager which uses it during offset expiration. I think the only reason we do this is because we didn't want to waste purgatory space. I don't think that's a good enough reason to go outside the normal flow. It would be simpler to follow the same path. Potentially we could make the callback an Option so that we still have a way to avoid polluting the purgatory.", "author": "hachikuji", "createdAt": "2020-05-18T23:19:57Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -970,7 +970,16 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  def appendRecordsToLeader(records: MemoryRecords, origin: AppendOrigin, requiredAcks: Int): LogAppendInfo = {\n+  /**\n+   * @param completeDelayedRequests It may requires a bunch of group locks when completing delayed requests so it may\n+   *                                produce deadlock if caller already holds a group lock. Hence, caller should pass\n+   *                                false to disable completion and then complete the delayed requests after releasing\n+   *                                held group lock\n+   */\n+  def appendRecordsToLeader(records: MemoryRecords,\n+                            origin: AppendOrigin,\n+                            requiredAcks: Int,\n+                            completeDelayedRequests: Boolean): LogAppendResult = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzExNDA4Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r427114083", "bodyText": "Potentially we could make the callback an Option so that we still have a way to avoid polluting the purgatory.\n\npardon me. I fail to catch your point.", "author": "chia7712", "createdAt": "2020-05-19T08:16:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MzYyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzMjE0MA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r427432140", "bodyText": "Sorry, let me be clearer:\n\nMainly I'm suggesting moving the delayed operation checking into ReplicaManager instead of Partition.\nWe can change the call to appendRecordsToLeader in GroupMetadataManager to go through ReplicaManager as well.\nWe could make the callback optional in ReplicaManager.appendRecords so that we do not have to add a callback (which appears to be the only reason we write directly to Partition from GroupMetadataManager).\n\nAnyway, just a suggestion. I thought it might let us keep the completion logic encapsulated in ReplicaManager.", "author": "hachikuji", "createdAt": "2020-05-19T16:22:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MzYyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1NzI4Nw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430757287", "bodyText": "I agree that it's simpler to let the caller in ReplicaManager to complete the delayed requests. This way, we don't need to pass completeDelayedRequests in here.", "author": "junrao", "createdAt": "2020-05-26T23:13:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MzYyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0NDY3OQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r426944679", "bodyText": "Hmm.. Does the group purgatory suffer from the same deadlock potential? If we call checkAndComplete for a group \"foo,\" I don't think we would attempt completion for any other group.", "author": "hachikuji", "createdAt": "2020-05-18T23:23:37Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -369,6 +369,31 @@ class GroupCoordinator(val brokerId: Int,\n     }\n   }\n \n+  /**\n+   * try to complete produce, fetch and delete requests if the HW of partition is incremented. Otherwise, we try to complete\n+   * only delayed fetch requests.\n+   *\n+   * Noted that this method may hold a lot of group lock so the caller should NOT hold any group lock\n+   * in order to avoid deadlock\n+   * @param topicPartitions topic partition and leaderHWIncremented\n+   */\n+  private[this] def completeDelayedRequests(topicPartitions: Map[TopicPartition, Boolean]): Unit =\n+    topicPartitions.foreach {\n+      case (tp, leaderHWIncremented) =>\n+        if (leaderHWIncremented) groupManager.replicaManager.completeDelayedRequests(tp)\n+        else groupManager.replicaManager.completeDelayedFetchRequests(tp)\n+    }\n+\n+  /**\n+   * complete the delayed join requests associated to input group keys.\n+   *\n+   * Noted that this method may hold a lot of group lock so the caller should NOT hold any group lock\n+   * in order to avoid deadlock\n+   * @param groupKeys group keys to complete\n+   */\n+  private[this] def completeDelayedJoinRequests(groupKeys: Set[GroupKey]): Unit =\n+    groupKeys.foreach(joinPurgatory.checkAndComplete)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzEwMjcyOQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r427102729", "bodyText": "Does the group purgatory suffer from the same deadlock potential? If we call checkAndComplete for a group \"foo,\" I don't think we would attempt completion for any other group.\n\nyou are right. It requires lock for group \"foo\" only. But the potential deadlock I tried to avoid/describe is that the caller has held a lock of group_a and then it tried to complete delayed request for group_b. It is possible to cause deadlock as it requires the lock of group_b to completing delayed request for group_b.\nThat way the comment says the caller should NOT hold any group lock.", "author": "chia7712", "createdAt": "2020-05-19T07:57:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0NDY3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzMDAzMg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r427430032", "bodyText": "The join/heartbeat purgatories are a little different from the produce purgatory. The key is based on the groupId, so when we complete an operation for group \"foo,\" we won't complete for group \"bar\" incidentally. At least that is my understanding. If you look at DelayedOperationPurgatory.checkAndComplete, we only complete watchers for the passed key.", "author": "hachikuji", "createdAt": "2020-05-19T16:19:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0NDY3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQzNTc4MA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r427435780", "bodyText": "so when we complete an operation for group \"foo,\" we won't complete for group \"bar\" incidentally. At least that is my understanding. If you look at DelayedOperationPurgatory.checkAndComplete, we only complete watchers for the passed key.\n\nYou are totally right. My above comment is not clear. I added the comment to remind developers following code is dangerous.\nlock group_a\ncompleteDelayedJoinRequests for group_b (this line may produce deadlock if there is another thread which is holding lock of group_b and prepare to complete requests for group_a)\nrelease group_a\nOf course, the above code is nonexistent currently. I'm just worry that the deadlock is easy to produce in the future if we don't notice the lock issue.", "author": "chia7712", "createdAt": "2020-05-19T16:27:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0NDY3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzcyMzgwMg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r427723802", "bodyText": "so when we complete an operation for group \"foo,\" we won't complete for group \"bar\"\n\nIn fact, there is a example of deadlock in join purgatories.\nGroupCoordinator#onCompleteJoin called by DelayedJoin#onComplete is possible to call storeGroup to append records to __consumer_offsets-{partitionFor(group.groupId)}. If there are groups related to same partition, GroupCoordinator#onCompleteJoin which is holding a group lock will require locks for other groups.", "author": "chia7712", "createdAt": "2020-05-20T03:40:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0NDY3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk2MDYzNw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r426960637", "bodyText": "For reference, here are links to two alternative approaches that I considered earlier this year:\n\nAsync completion: hachikuji@3bee4ac\nLock-safe offset cache: hachikuji@3705f33\n\nI think Jun was not satisfied with the first approach because it called for another thread pool. Its advantage though was a simpler and more intuitive API than what we have here. An idea which I never implemented was to let the request handlers also handle delayed operation completion so that we did not need another thread pool. Basically rather than calling the callback in DelayedProduce directly, we add a new operation to the request queues. Obviously this has its own tradeoffs.\nThe second commit tries to use lock-free data structures so that we do not need the lock when completing the callback. This was only a partial solution which handled offset commit appends, but not group metadata appends. I am not sure how to handle join group completion asynchronously, so I gave up on this idea.\nOnly posting in case it's useful to see how some of these alternatives might have looked. I'm ok with the approach here, but I do wish we could come up with a simpler API. One thought I had is whether we could make the need for external completion more explicit. For example, maybe appendRecords could return some kind of object which encapsulates purgatory completion.\nval completion = inLock(lock) {\n  replicaManager.appendRecords(...)\n}\ncompletion.run()\nJust a thought.", "author": "hachikuji", "createdAt": "2020-05-19T00:20:05Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -769,20 +813,25 @@ class GroupCoordinator(val brokerId: Int,\n             // on heartbeat response to eventually notify the rebalance in progress signal to the consumer\n             val member = group.get(memberId)\n             completeAndScheduleNextHeartbeatExpiration(group, member)\n-            groupManager.storeOffsets(group, memberId, offsetMetadata, responseCallback)\n+            partitionsToComplete ++= groupManager.storeOffsets(\n+              group = group,\n+              consumerId = memberId,\n+              offsetMetadata = offsetMetadata,\n+              responseCallback = responseCallback,\n+              completeDelayedRequests = false)\n \n           case CompletingRebalance =>\n             // We should not receive a commit request if the group has not completed rebalance;\n             // but since the consumer's member.id and generation is valid, it means it has received\n             // the latest group generation information from the JoinResponse.\n             // So let's return a REBALANCE_IN_PROGRESS to let consumer handle it gracefully.\n             responseCallback(offsetMetadata.map { case (k, _) => k -> Errors.REBALANCE_IN_PROGRESS })\n-\n           case _ =>\n             throw new RuntimeException(s\"Logic error: unexpected group state ${group.currentState}\")\n         }\n       }\n     }\n+    completeDelayedRequests(partitionsToComplete)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA5NTYxNA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r427095614", "bodyText": "@hachikuji  thanks for reviews!\n\nAsync completion: hachikuji/kafka@3bee4ac\n\nwe are on the same page :) (my previous comment #6915 (comment))\n\nOnly posting in case it's useful to see how some of these alternatives might have looked. I'm ok with the approach here, but I do wish we could come up with a simpler API. One thought I had is whether we could make the need for external completion more explicit. For example, maybe appendRecords could return some kind of object which encapsulates purgatory completion.\n\nthis style LGTM :)", "author": "chia7712", "createdAt": "2020-05-19T07:45:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk2MDYzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDI1MzkyMQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430253921", "bodyText": "@junrao Could you take a look? I'd like to address @hachikuji comment but it produces a big change to this PR. Hence, it would be better to have more reviews/suggestions before kicking off.", "author": "chia7712", "createdAt": "2020-05-26T08:47:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk2MDYzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk0OTc2OQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r428949769", "bodyText": "Perhaps reword like the following?\nReturning a map of successfully appended topic partitions and a flag indicting whether the HWM has been incremented. If the caller passes in completeDelayedRequests as false, the caller is expected to complete delayed requests for those returned partitions.", "author": "junrao", "createdAt": "2020-05-21T22:28:41Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -550,19 +584,22 @@ class ReplicaManager(val config: KafkaConfig,\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   * @return the topic partitions we succeed to append data. It is useful to caller which tries to complete delayed requests.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk1MDM2MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r428950361", "bodyText": "Could we do localProduceResults.filter{ case (tp, logAppendResult) => ... } to avoid unnamed references?", "author": "junrao", "createdAt": "2020-05-21T22:30:37Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -593,6 +630,7 @@ class ReplicaManager(val config: KafkaConfig,\n         val produceResponseStatus = produceStatus.map { case (k, status) => k -> status.responseStatus }\n         responseCallback(produceResponseStatus)\n       }\n+      localProduceResults.filter(_._2.exception.isEmpty).map(e => e._1 -> e._2.leaderHWIncremented)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk1NjA3OA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r428956078", "bodyText": "Could we add a comment for the return value?", "author": "junrao", "createdAt": "2020-05-21T22:48:37Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1103,7 +1153,7 @@ class GroupCoordinator(val brokerId: Int,\n     joinPurgatory.tryCompleteElseWatch(delayedRebalance, Seq(groupKey))\n   }\n \n-  private def removeMemberAndUpdateGroup(group: GroupMetadata, member: MemberMetadata, reason: String): Unit = {\n+  private def removeMemberAndUpdateGroup(group: GroupMetadata, member: MemberMetadata, reason: String): Option[GroupKey] = {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk1ODUzMA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r428958530", "bodyText": "Could we add a comment for the return value?", "author": "junrao", "createdAt": "2020-05-21T22:56:39Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1113,33 +1163,27 @@ class GroupCoordinator(val brokerId: Int,\n     group.removeStaticMember(member.groupInstanceId)\n \n     group.currentState match {\n-      case Dead | Empty =>\n-      case Stable | CompletingRebalance => maybePrepareRebalance(group, reason)\n-      case PreparingRebalance => joinPurgatory.checkAndComplete(GroupKey(group.groupId))\n+      case Dead | Empty => None\n+      case Stable | CompletingRebalance =>\n+        maybePrepareRebalance(group, reason)\n+        None\n+      case PreparingRebalance => Some(GroupKey(group.groupId))\n     }\n   }\n \n-  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Unit = {\n+  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Option[GroupKey] = {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk3MjUzOA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r428972538", "bodyText": "This is bit tricky to untangle. It seems the original code holds the group lock for both the group.hasAllMembersJoined check and the call to forceComplete(). So, we probably want to keep doing that.\nI am thinking that we could do the following.\n\nChange GroupCoordinator.onCompleteJoin() so that (1) it checks group.hasAllMembersJoined inside the group lock and returns whether hasAllMembersJoined is true.\nIn DelayedJoin.tryComplete() , we do\n\n        if (GroupCoordinator.onCompleteJoin()) \n               forceComplete() \n}\n\nIn onComplete(), we do nothing.", "author": "junrao", "createdAt": "2020-05-21T23:47:16Z", "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +33,15 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since GroupCoordinator#onCompleteJoin() tries to complete delayed requests.\n+   */\n+  override def tryComplete(): Boolean = if (group.inLock(group.hasAllMembersJoined)) forceComplete() else false", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTAwMTg2OQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r431001869", "bodyText": "So, we probably want to keep doing that.\n\nIt may produce deadlock if we hold the group lock for GroupCoordinator.onCompleteJoin.\nGroupCoordinator.onCompleteJoin is possible to append record to __consumer_offsets (see https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala#L1168), and hence it will try to complete other delayed joins which have groups belonging to same partition of __consumer_offsets.\nThat is why I make DelayedJoin control the group lock manually.  For another,  GroupCoordinator.onCompleteJoin is used by DelayedJoin only so it should be fine to change behavior of group lock in this case.", "author": "chia7712", "createdAt": "2020-05-27T10:01:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk3MjUzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTAwMjQ5NA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r431002494", "bodyText": "I am thinking that we could do the following.\n\nCopy that.", "author": "chia7712", "createdAt": "2020-05-27T10:02:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk3MjUzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTAwMzM3NA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r431003374", "bodyText": "So, we probably want to keep doing that.\n\nhmmm, I misunderstand your point. Please ignore my first comment :)", "author": "chia7712", "createdAt": "2020-05-27T10:04:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk3MjUzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0ODU1Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430748553", "bodyText": "I was trying to check if it's safe to do this. The intention for this is probably to avoid the deadlock between the group lock and the lock in DelayedOperation. None of the caller of joinPurgatory.checkAndComplete holds a group lock now. The only other caller that can first hold a group lock and then the lock in DelayedOperation is joinPurgatory.tryCompleteElseWatch(). However, that's not an issue since that's when the DelayedJoin operation is first added. So, this changes seems ok.", "author": "junrao", "createdAt": "2020-05-26T22:47:10Z", "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +33,15 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1MDAyMA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430750020", "bodyText": "\"as the lock is not free\" : Do you mean \"when the lock is free\"?", "author": "junrao", "createdAt": "2020-05-26T22:51:20Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,20 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\". There was a lot of cases that hold a lock and then try to\n+   * hold more locks to complete delayed requests. Unfortunately, that scenario causes deadlock and we had introduced\n+   * the \"tryLock\" to avoid deadlock. However, the \"tryLock\" causes another issue that thread_A holds a lock but it does\n+   * not complete the delayed requests and there are no threads can complete request as the lock is not free.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1MDcyMw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430750723", "bodyText": "a flag => a flag in ReplicaManager.appendRecords().", "author": "junrao", "createdAt": "2020-05-26T22:53:22Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,20 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\". There was a lot of cases that hold a lock and then try to\n+   * hold more locks to complete delayed requests. Unfortunately, that scenario causes deadlock and we had introduced\n+   * the \"tryLock\" to avoid deadlock. However, the \"tryLock\" causes another issue that thread_A holds a lock but it does\n+   * not complete the delayed requests and there are no threads can complete request as the lock is not free.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * We introduces a flag, called completeDelayedRequests, to prevent the method from automatically completing delayed", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1NTY1Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430755656", "bodyText": "Could we add a comment to explain the return value?", "author": "junrao", "createdAt": "2020-05-26T23:08:08Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -311,37 +312,40 @@ class GroupMetadataManager(brokerId: Int,\n \n           responseCallback(responseError)\n         }\n-        appendForGroup(group, groupMetadataRecords, putCacheCallback)\n-\n+        appendForGroup(group, groupMetadataRecords, putCacheCallback, completeDelayedRequests)\n       case None =>\n         responseCallback(Errors.NOT_COORDINATOR)\n-        None\n+        Map.empty\n     }\n   }\n \n   private def appendForGroup(group: GroupMetadata,\n                              records: Map[TopicPartition, MemoryRecords],\n-                             callback: Map[TopicPartition, PartitionResponse] => Unit): Unit = {\n+                             callback: Map[TopicPartition, PartitionResponse] => Unit,\n+                             completeDelayedRequests: Boolean): Map[TopicPartition, Boolean] = {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1NjE4Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430756183", "bodyText": "A map containing the topic partitions having new records and a flag indicating whether the HWM has been incremented.", "author": "junrao", "createdAt": "2020-05-26T23:09:43Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -311,37 +312,40 @@ class GroupMetadataManager(brokerId: Int,\n \n           responseCallback(responseError)\n         }\n-        appendForGroup(group, groupMetadataRecords, putCacheCallback)\n-\n+        appendForGroup(group, groupMetadataRecords, putCacheCallback, completeDelayedRequests)\n       case None =>\n         responseCallback(Errors.NOT_COORDINATOR)\n-        None\n+        Map.empty\n     }\n   }\n \n   private def appendForGroup(group: GroupMetadata,\n                              records: Map[TopicPartition, MemoryRecords],\n-                             callback: Map[TopicPartition, PartitionResponse] => Unit): Unit = {\n+                             callback: Map[TopicPartition, PartitionResponse] => Unit,\n+                             completeDelayedRequests: Boolean): Map[TopicPartition, Boolean] = {\n     // call replica manager to append the group message\n     replicaManager.appendRecords(\n       timeout = config.offsetCommitTimeoutMs.toLong,\n       requiredAcks = config.offsetCommitRequiredAcks,\n       internalTopicsAllowed = true,\n       origin = AppendOrigin.Coordinator,\n+      completeDelayedRequests = completeDelayedRequests,\n       entriesPerPartition = records,\n       delayedProduceLock = Some(group.lock),\n       responseCallback = callback)\n   }\n \n   /**\n    * Store offsets by appending it to the replicated log and then inserting to cache\n+   * @return the topic partitions having new records", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2MTE4Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r430761183", "bodyText": "Could we add a comment to explain the return value?", "author": "junrao", "createdAt": "2020-05-26T23:26:08Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -241,7 +241,8 @@ class GroupMetadataManager(brokerId: Int,\n \n   def storeGroup(group: GroupMetadata,\n                  groupAssignment: Map[String, Array[Byte]],\n-                 responseCallback: Errors => Unit): Unit = {\n+                 responseCallback: Errors => Unit,\n+                 completeDelayedRequests: Boolean): Map[TopicPartition, Boolean] = {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIwNjE0Nw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r431206147", "bodyText": "make sure DelayedJoinTest does not cause deadlock", "author": "chia7712", "createdAt": "2020-05-27T14:58:55Z", "path": "core/src/test/scala/unit/kafka/coordinator/group/DelayedJoinTest.scala", "diffHunk": "@@ -0,0 +1,73 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.coordinator.group\n+\n+import java.util.concurrent.{CountDownLatch, Executors, TimeUnit}\n+\n+import kafka.utils.MockTime\n+import org.easymock.EasyMock\n+import org.junit.{Assert, Test}\n+\n+class DelayedJoinTest {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIwODU0OQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r431208549", "bodyText": "@junrao both methods are executed with lock", "author": "chia7712", "createdAt": "2020-05-27T15:00:48Z", "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +34,40 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * The delayed requests should be completed without holding group lock so we keep those partitions and then\n+   * complete them after releasing lock.\n+   */\n+  private[group] var partitionsToComplete: scala.collection.Map[TopicPartition, LeaderHWChange] = Map.empty\n+\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since GroupCoordinator#onCompleteJoin() tries to complete delayed requests.\n+   *\n+   */\n+  override def tryComplete(): Boolean = try group.inLock {\n+    /**\n+     * holds the group lock for both the \"group.hasAllMembersJoined\" check and the call to forceComplete()\n+     */\n+    if (group.hasAllMembersJoined) forceComplete()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIwOTM2OA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r431209368", "bodyText": "this is a workaround to deal with deadlock caused by taking multiples group locks", "author": "chia7712", "createdAt": "2020-05-27T15:01:31Z", "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +34,40 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * The delayed requests should be completed without holding group lock so we keep those partitions and then\n+   * complete them after releasing lock.\n+   */\n+  private[group] var partitionsToComplete: scala.collection.Map[TopicPartition, LeaderHWChange] = Map.empty\n+\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since GroupCoordinator#onCompleteJoin() tries to complete delayed requests.\n+   *\n+   */\n+  override def tryComplete(): Boolean = try group.inLock {\n+    /**\n+     * holds the group lock for both the \"group.hasAllMembersJoined\" check and the call to forceComplete()\n+     */\n+    if (group.hasAllMembersJoined) forceComplete()\n+    else false\n+  } finally completeDelayedRequests()\n+  override def onExpiration(): Unit = coordinator.onExpireJoin()\n+  override def onComplete(): Unit = try partitionsToComplete = coordinator.onCompleteJoin(group)\n+  finally completeDelayedRequests()\n+\n+  /**\n+   * try to complete delayed requests only if the caller does not hold the group lock.\n+   * This method is called by following cases:\n+   * 1) tryComplete -> hold lock -> onComplete -> release lock -> completeDelayedRequests\n+   * 2) onComplete -> completeDelayedRequests\n+   */\n+  private[group] def completeDelayedRequests(): Unit = if (!group.lock.isHeldByCurrentThread) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3Mjc0Nw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439872747", "bodyText": "Another way that doesn't require checking lock.isHeldByCurrentThread is the following. But your approach seems simpler.\nOverride forceComplete() to\noverride def forceComplete() {\n    if (completed.compareAndSet(false, true)) {\n      // cancel the timeout timer\n      cancel()\n      partitionsToComplete  = coordinator.onCompleteJoin(group)\n      onComplete()\n      true\n    } else {\n      false\n    }\n}\n\nIn onComplete(), do nothing.\nIn tryComplete(), do\noverride def tryComplete() {\n  group.inLock {\n    if (group.hasAllMembersJoined) \n      isForceComplete = forceComplete()\n  }\n  completeDelayedRequests(partitionsToComplete)\n  isForceComplete\n}\n\nIn onExpiration(),\noverride def onExpiration() {\n  completeDelayedRequests(partitionsToComplete)\n}", "author": "junrao", "createdAt": "2020-06-14T22:15:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIwOTM2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIxMDQ5Nw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r431210497", "bodyText": "this enum type is more readable than boolean", "author": "chia7712", "createdAt": "2020-05-27T15:02:28Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -65,13 +65,24 @@ import scala.compat.java8.OptionConverters._\n /*\n  * Result metadata of a log append operation on the log\n  */\n-case class LogAppendResult(info: LogAppendInfo, exception: Option[Throwable] = None) {\n+case class LogAppendResult(info: LogAppendInfo,\n+                           exception: Option[Throwable] = None,\n+                           leaderHWChange: LeaderHWChange = LeaderHWChange.None) {\n   def error: Errors = exception match {\n     case None => Errors.NONE\n     case Some(e) => Errors.forException(e)\n   }\n }\n \n+/**\n+ * a flag indicting whether the HWM has been changed.\n+ */\n+sealed trait LeaderHWChange", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0MzczNw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439543737", "bodyText": "It's cleaner to not pass in completeDelayedRequests here and let the caller (ReplicaManager.appendRecords()) check and complete purgatory instead.", "author": "junrao", "createdAt": "2020-06-12T17:12:08Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -967,7 +967,16 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  def appendRecordsToLeader(records: MemoryRecords, origin: AppendOrigin, requiredAcks: Int): LogAppendInfo = {\n+  /**\n+   * @param completeDelayedRequests It may requires a bunch of group locks when completing delayed requests so it may", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTkxNTQzOA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439915438", "bodyText": "the caller of ReplicaManager#appendRecords may hold the group lock so it could produce deadlock if ReplicaManager#appendRecords tries to complete  purgatory.", "author": "chia7712", "createdAt": "2020-06-15T03:39:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0MzczNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTkyMzcyMA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439923720", "bodyText": "read @hachikuji #8657 (comment) again. It is a nice idea to refactor ReplicaManager and Partition to simplify the behavior of checking delayed operations.\nCould I address the refactor in another PR to avoid bigger patch?", "author": "chia7712", "createdAt": "2020-06-15T04:23:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0MzczNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUxNTA0Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r440515043", "bodyText": "Yes, we can refactor that in a separate PR. Could you file a followup jira for that?", "author": "junrao", "createdAt": "2020-06-16T00:15:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0MzczNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU3ODIwMg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r440578202", "bodyText": "https://issues.apache.org/jira/browse/KAFKA-10170", "author": "chia7712", "createdAt": "2020-06-16T04:24:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0MzczNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0OTEzNg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439549136", "bodyText": "Could we use map {case (tp, appendResult) => ...} here to avoid using unamed references?", "author": "junrao", "createdAt": "2020-06-12T17:23:34Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -603,6 +650,9 @@ class ReplicaManager(val config: KafkaConfig,\n         val produceResponseStatus = produceStatus.map { case (k, status) => k -> status.responseStatus }\n         responseCallback(produceResponseStatus)\n       }\n+      localProduceResults\n+        .filter { case (_, logAppendResult) => logAppendResult.exception.isEmpty}\n+        .map(e => e._1 -> e._2.leaderHWChange)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3Mjg0Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439872846", "bodyText": "expire ->  onComplete -> completeDelayedRequests", "author": "junrao", "createdAt": "2020-06-14T22:16:58Z", "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +34,40 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * The delayed requests should be completed without holding group lock so we keep those partitions and then\n+   * complete them after releasing lock.\n+   */\n+  private[group] var partitionsToComplete: scala.collection.Map[TopicPartition, LeaderHWChange] = Map.empty\n+\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since GroupCoordinator#onCompleteJoin() tries to complete delayed requests.\n+   *\n+   */\n+  override def tryComplete(): Boolean = try group.inLock {\n+    /**\n+     * holds the group lock for both the \"group.hasAllMembersJoined\" check and the call to forceComplete()\n+     */\n+    if (group.hasAllMembersJoined) forceComplete()\n+    else false\n+  } finally completeDelayedRequests()\n+  override def onExpiration(): Unit = coordinator.onExpireJoin()\n+  override def onComplete(): Unit = try partitionsToComplete = coordinator.onCompleteJoin(group)\n+  finally completeDelayedRequests()\n+\n+  /**\n+   * try to complete delayed requests only if the caller does not hold the group lock.\n+   * This method is called by following cases:\n+   * 1) tryComplete -> hold lock -> onComplete -> release lock -> completeDelayedRequests\n+   * 2) onComplete -> completeDelayedRequests", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3MzE3NQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439873175", "bodyText": "DelyaedOperation.lockOpt defaults to None. So, we don't have to specify it explicitly.", "author": "junrao", "createdAt": "2020-06-14T22:21:43Z", "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +34,40 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3MzQ0MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439873441", "bodyText": "\"GroupCoordinator#onCompleteJoin() tries to complete delayed requests\" => since the completion of the delayed request for partitions returned from GroupCoordinator#onCompleteJoin() need to be done outside of the group lock.", "author": "junrao", "createdAt": "2020-06-14T22:25:23Z", "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +34,40 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, None) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * The delayed requests should be completed without holding group lock so we keep those partitions and then\n+   * complete them after releasing lock.\n+   */\n+  private[group] var partitionsToComplete: scala.collection.Map[TopicPartition, LeaderHWChange] = Map.empty\n+\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since GroupCoordinator#onCompleteJoin() tries to complete delayed requests.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3NTgwMQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439875801", "bodyText": "typo whihc", "author": "junrao", "createdAt": "2020-06-14T22:54:00Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1118,33 +1170,38 @@ class GroupCoordinator(val brokerId: Int,\n     group.removeStaticMember(member.groupInstanceId)\n \n     group.currentState match {\n-      case Dead | Empty =>\n-      case Stable | CompletingRebalance => maybePrepareRebalance(group, reason)\n-      case PreparingRebalance => joinPurgatory.checkAndComplete(GroupKey(group.groupId))\n+      case Dead | Empty => None\n+      case Stable | CompletingRebalance =>\n+        maybePrepareRebalance(group, reason)\n+        None\n+      case PreparingRebalance => Some(GroupKey(group.groupId))\n     }\n   }\n \n-  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Unit = {\n+  /**\n+   * remove the pending member and then return the group key whihc is in PreparingRebalance,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3NTg5Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439875892", "bodyText": "The caller no longer passed in completeDelayedRequests.", "author": "junrao", "createdAt": "2020-06-14T22:55:07Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1118,33 +1170,38 @@ class GroupCoordinator(val brokerId: Int,\n     group.removeStaticMember(member.groupInstanceId)\n \n     group.currentState match {\n-      case Dead | Empty =>\n-      case Stable | CompletingRebalance => maybePrepareRebalance(group, reason)\n-      case PreparingRebalance => joinPurgatory.checkAndComplete(GroupKey(group.groupId))\n+      case Dead | Empty => None\n+      case Stable | CompletingRebalance =>\n+        maybePrepareRebalance(group, reason)\n+        None\n+      case PreparingRebalance => Some(GroupKey(group.groupId))\n     }\n   }\n \n-  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Unit = {\n+  /**\n+   * remove the pending member and then return the group key whihc is in PreparingRebalance,\n+   * @param group group\n+   * @param memberId member id\n+   * @return group key if it is in PreparingRebalance. Otherwise, None\n+   */\n+  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Option[GroupKey] = {\n     group.removePendingMember(memberId)\n \n-    if (group.is(PreparingRebalance)) {\n-      joinPurgatory.checkAndComplete(GroupKey(group.groupId))\n-    }\n-  }\n-\n-  def tryCompleteJoin(group: GroupMetadata, forceComplete: () => Boolean) = {\n-    group.inLock {\n-      if (group.hasAllMembersJoined)\n-        forceComplete()\n-      else false\n-    }\n+    if (group.is(PreparingRebalance)) Some(GroupKey(group.groupId))\n+    else None\n   }\n \n   def onExpireJoin(): Unit = {\n     // TODO: add metrics for restabilize timeouts\n   }\n \n-  def onCompleteJoin(group: GroupMetadata): Unit = {\n+  /**\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. If the caller passes in completeDelayedRequests as false, the caller is expected to complete", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3NjIxNQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439876215", "bodyText": "All callers pass in completeDelayedRequests as false. Could we remove this param?", "author": "junrao", "createdAt": "2020-06-14T22:59:03Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -311,37 +317,47 @@ class GroupMetadataManager(brokerId: Int,\n \n           responseCallback(responseError)\n         }\n-        appendForGroup(group, groupMetadataRecords, putCacheCallback)\n-\n+        appendForGroup(group, groupMetadataRecords, putCacheCallback, completeDelayedRequests)\n       case None =>\n         responseCallback(Errors.NOT_COORDINATOR)\n-        None\n+        Map.empty\n     }\n   }\n \n+  /**\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. If the caller passes in completeDelayedRequests as false, the caller is expected to complete\n+   *         delayed requests for those returned partitions.\n+   */\n   private def appendForGroup(group: GroupMetadata,\n                              records: Map[TopicPartition, MemoryRecords],\n-                             callback: Map[TopicPartition, PartitionResponse] => Unit): Unit = {\n+                             callback: Map[TopicPartition, PartitionResponse] => Unit,\n+                             completeDelayedRequests: Boolean): Map[TopicPartition, LeaderHWChange] = {\n     // call replica manager to append the group message\n     replicaManager.appendRecords(\n       timeout = config.offsetCommitTimeoutMs.toLong,\n       requiredAcks = config.offsetCommitRequiredAcks,\n       internalTopicsAllowed = true,\n       origin = AppendOrigin.Coordinator,\n+      completeDelayedRequests = completeDelayedRequests,\n       entriesPerPartition = records,\n       delayedProduceLock = Some(group.lock),\n       responseCallback = callback)\n   }\n \n   /**\n    * Store offsets by appending it to the replicated log and then inserting to cache\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. If the caller passes in completeDelayedRequests as false, the caller is expected to complete\n+   *         delayed requests for those returned partitions.\n    */\n   def storeOffsets(group: GroupMetadata,\n                    consumerId: String,\n                    offsetMetadata: immutable.Map[TopicPartition, OffsetAndMetadata],\n                    responseCallback: immutable.Map[TopicPartition, Errors] => Unit,\n+                   completeDelayedRequests: Boolean,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU4NTMxNA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r440585314", "bodyText": "nice caching. Most methods don't need this flag. Let me revert them :)", "author": "chia7712", "createdAt": "2020-06-16T04:53:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3NjIxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3NjM5Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439876392", "bodyText": "There was => There were", "author": "junrao", "createdAt": "2020-06-14T23:01:18Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,24 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3NjQ5Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439876493", "bodyText": "ReplicaManager.appendRecords()., => ReplicaManager.appendRecords(),", "author": "junrao", "createdAt": "2020-06-14T23:02:54Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,24 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * We introduces a flag in ReplicaManager.appendRecords()., called completeDelayedRequests, to prevent the method", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3NjU1Nw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439876557", "bodyText": "may requires => may require", "author": "junrao", "createdAt": "2020-06-14T23:04:08Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -967,7 +967,16 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  def appendRecordsToLeader(records: MemoryRecords, origin: AppendOrigin, requiredAcks: Int): LogAppendInfo = {\n+  /**\n+   * @param completeDelayedRequests It may requires a bunch of group locks when completing delayed requests so it may", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODM0MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439878341", "bodyText": "Hmm, why do we need this logic now?", "author": "junrao", "createdAt": "2020-06-14T23:27:03Z", "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -307,8 +307,14 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n     override def runWithCallback(member: GroupMember, responseCallback: CompleteTxnCallback): Unit = {\n       val producerId = 1000L\n       val offsetsPartitions = (0 to numPartitions).map(new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, _))\n-      groupCoordinator.groupManager.handleTxnCompletion(producerId,\n-        offsetsPartitions.map(_.partition).toSet, isCommit = random.nextBoolean)\n+      val isCommit = random.nextBoolean\n+      try groupCoordinator.groupManager.handleTxnCompletion(producerId,\n+        offsetsPartitions.map(_.partition).toSet, isCommit = isCommit)\n+      catch {\n+        case e: IllegalStateException if isCommit\n+          && e.getMessage.contains(\"though the offset commit record itself hasn't been appended to the log\")=>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTkxNDY4MA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439914680", "bodyText": "TestReplicaManager#appendRecords (https://github.com/apache/kafka/blob/trunk/core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala#L207) always complete the delayedProduce immediately so the txn offset is append also. This PR tries to complete the delayedProduce after releasing the group lock so it is possible to cause following execution order.\n\ntxn prepare\ntxn completion (fail)\ntxn append (this is executed by delayedProduce)", "author": "chia7712", "createdAt": "2020-06-15T03:35:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODM0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUxNDc0MA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r440514740", "bodyText": "Thanks. I am still not sure that I fully understand this. It seems that by not completing the delayedProduce within the group lock, we are hitting IllegalStateException. That seems a bug. Do you know which code depends on that? It seems that we do hold a group lock when updating the txnOffset.\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L462", "author": "junrao", "createdAt": "2020-06-16T00:14:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODM0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU3NjA3MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r440576071", "bodyText": "That seems a bug.\n\nThe root cause (changed by this PR) is that the \"txn initialization\" and \"txn append\" are not executed within same lock.\nThe test story is shown below.\nCommitTxnOffsetsOperation calls GroupMetadata.prepareTxnOffsetCommit to add CommitRecordMetadataAndOffset(None, offsetAndMetadata) to pendingTransactionalOffsetCommits (this is the link you attached).\nGroupMetadata.completePendingTxnOffsetCommit called by CompleteTxnOperation throws IllegalStateException if CommitRecordMetadataAndOffset.appendedBatchOffset is None (https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala#L664).\nWhy it does not cause error before?\nCommitRecordMetadataAndOffset.appendedBatchOffset is updated by the callback putCacheCallback (https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L407). TestReplicManager always create delayedProduce do handle the putCacheCallback (https://github.com/apache/kafka/blob/trunk/core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala#L188). The condition to complete the delayedProduce is completeAttempts.incrementAndGet() >= 3. And the condition gets true when call both producePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys) and tryCompleteDelayedRequests() since the former calls tryComplete two times and another calls tryComplete once. It means putCacheCallback is always executed by TestReplicManager.appendRecords and noted that TestReplicManager.appendRecords is executed within a group lock (https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala#L738) . In short, txn initialization (https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L464) and txn append (https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L407) are executed with same group lock. Hence, the following execution order is impossible.\n\ntxn initialization\ntxn completion\ntxn append\n\nHowever, this PR disable to complete delayed requests within group lock held by caller. The putCacheCallback which used to append txn needs to require group lock again.", "author": "chia7712", "createdAt": "2020-06-16T04:15:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODM0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE4NjQ2Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441186463", "bodyText": "Thanks for the great explanation. I understand the issue now. Essentially, this exposed a limitation of the existing test. The existing test happens to work because the producer callbacks are always completed in the same ReplicaManager.appendRecords() call under the group lock. However, this is not necessarily the general case.\nYour fix works, but may hide other real problems. I was thinking that another way to fix this is to change the test a bit. For example, we expect CompleteTxnOperation to happen after CommitTxnOffsetsOperation. So, instead of letting them run in parallel, we can change the test to make sure that CompleteTxnOperation only runs after CommitTxnOffsetsOperation completes successfully. JoinGroupOperation and SyncGroupOperation might need a similar consideration.", "author": "junrao", "createdAt": "2020-06-16T22:54:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODM0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI4OTY4OQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441289689", "bodyText": "we expect CompleteTxnOperation to happen after CommitTxnOffsetsOperation. So, instead of letting them run in parallel, we can change the test to make sure that CompleteTxnOperation only runs after CommitTxnOffsetsOperation completes successfully.\n\nwill roger that !\n\nJoinGroupOperation and SyncGroupOperation might need a similar consideration.\n\nI didn't notice something interesting. Could you share it with me?", "author": "chia7712", "createdAt": "2020-06-17T05:32:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODM0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODczNg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439878736", "bodyText": "Hmm, why do we need to mock this since replicaManager.getMagic() is only called through replicaManager.handleWriteTxnMarkersRequest()?", "author": "junrao", "createdAt": "2020-06-14T23:31:46Z", "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala", "diffHunk": "@@ -536,6 +537,11 @@ class GroupCoordinatorTest {\n     // Make sure the NewMemberTimeout is not still in effect, and the member is not kicked\n     assertEquals(1, group.size)\n \n+    // prepare the mock replica manager again since the delayed join is going to complete\n+    EasyMock.reset(replicaManager)\n+    EasyMock.expect(replicaManager.getMagic(EasyMock.anyObject())).andReturn(Some(RecordBatch.MAGIC_VALUE_V1)).anyTimes()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTkyMDAxOQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439920019", "bodyText": "GroupMetadataManager#storeGroup (https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L245) also call ReplicaManager.getMagic.\nThere are delayed ops are completed by timer.advanceClock so we have to mock the replicaManager.getMagic. the mock is same to https://github.com/apache/kafka/blob/trunk/core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala#L3823.", "author": "chia7712", "createdAt": "2020-06-15T04:03:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODczNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODk2OQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r439878969", "bodyText": "Hmm, this should only be called with LeaderHWChange.LeaderHWIncremented, but the mock later returns LeaderHWChange.None? Ditto below.", "author": "junrao", "createdAt": "2020-06-14T23:34:51Z", "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala", "diffHunk": "@@ -3921,22 +3934,26 @@ class GroupCoordinatorTest {\n     val (responseFuture, responseCallback) = setupCommitOffsetsCallback\n \n     val capturedArgument: Capture[scala.collection.Map[TopicPartition, PartitionResponse] => Unit] = EasyMock.newCapture()\n-\n-    EasyMock.expect(replicaManager.appendRecords(EasyMock.anyLong(),\n-      EasyMock.anyShort(),\n+    EasyMock.expect(replicaManager.completeDelayedRequests(EasyMock.anyObject()))", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE3NTgwNg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441175806", "bodyText": "typo rebalacne", "author": "junrao", "createdAt": "2020-06-16T22:23:40Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1108,7 +1157,11 @@ class GroupCoordinator(val brokerId: Int,\n     joinPurgatory.tryCompleteElseWatch(delayedRebalance, Seq(groupKey))\n   }\n \n-  private def removeMemberAndUpdateGroup(group: GroupMetadata, member: MemberMetadata, reason: String): Unit = {\n+  /**\n+   * @return the group which is preparing to rebalacne. Callers should use this group key to complete delayed requests", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE3NzA1NQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441177055", "bodyText": "the delayed requests may be completed as much as possible =>  the delayed requests may be completed inside the call with the expectation that no conflicting locks are held by the caller", "author": "junrao", "createdAt": "2020-06-16T22:27:14Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,27 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There were a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * We introduces a flag \"completeDelayedRequests\" to ReplicaManager.appendRecords() (and related methods).\n+   *\n+   * 1) If \"completeDelayedRequests\" is true, the delayed requests may be completed as much as possible.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE3NzM1MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441177351", "bodyText": "Callers can complete the delayed requests manually => Callers can complete the delayed requests after releasing any conflicting lock.", "author": "junrao", "createdAt": "2020-06-16T22:28:07Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,40 +99,27 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n+   * Thread-safe variant of tryComplete() that attempts completion after it succeed to hold the lock.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There were a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * We introduces a flag \"completeDelayedRequests\" to ReplicaManager.appendRecords() (and related methods).\n+   *\n+   * 1) If \"completeDelayedRequests\" is true, the delayed requests may be completed as much as possible.\n+   * 2) If \"completeDelayedRequests\" is false, the delayed requests are NOT completed and the watch key (for example,\n+   *    group key) are returned. Callers can complete the delayed requests manually.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE3ODQxOA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441178418", "bodyText": "\"If the caller no longer passed in completeDelayedRequests\" The caller still passes this in, just as false.", "author": "junrao", "createdAt": "2020-06-16T22:31:00Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -560,19 +603,23 @@ class ReplicaManager(val config: KafkaConfig,\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. If the caller no longer passed in completeDelayedRequests, the caller is expected to complete", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE4MDY2Nw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441180667", "bodyText": "\" If the caller no longer passed in completeDelayedRequests\" => There is no completeDelayedRequests passed in.", "author": "junrao", "createdAt": "2020-06-16T22:37:19Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1118,33 +1171,38 @@ class GroupCoordinator(val brokerId: Int,\n     group.removeStaticMember(member.groupInstanceId)\n \n     group.currentState match {\n-      case Dead | Empty =>\n-      case Stable | CompletingRebalance => maybePrepareRebalance(group, reason)\n-      case PreparingRebalance => joinPurgatory.checkAndComplete(GroupKey(group.groupId))\n+      case Dead | Empty => None\n+      case Stable | CompletingRebalance =>\n+        maybePrepareRebalance(group, reason)\n+        None\n+      case PreparingRebalance => Some(GroupKey(group.groupId))\n     }\n   }\n \n-  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Unit = {\n+  /**\n+   * remove the pending member and then return the group key which is in PreparingRebalance,\n+   * @param group group\n+   * @param memberId member id\n+   * @return group key if it is in PreparingRebalance. Otherwise, None\n+   */\n+  private def removePendingMemberAndUpdateGroup(group: GroupMetadata, memberId: String): Option[GroupKey] = {\n     group.removePendingMember(memberId)\n \n-    if (group.is(PreparingRebalance)) {\n-      joinPurgatory.checkAndComplete(GroupKey(group.groupId))\n-    }\n-  }\n-\n-  def tryCompleteJoin(group: GroupMetadata, forceComplete: () => Boolean) = {\n-    group.inLock {\n-      if (group.hasAllMembersJoined)\n-        forceComplete()\n-      else false\n-    }\n+    if (group.is(PreparingRebalance)) Some(GroupKey(group.groupId))\n+    else None\n   }\n \n   def onExpireJoin(): Unit = {\n     // TODO: add metrics for restabilize timeouts\n   }\n \n-  def onCompleteJoin(group: GroupMetadata): Unit = {\n+  /**\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. If the caller no longer passed in completeDelayedRequests, the caller is expected to complete", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE4MjQ1MA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441182450", "bodyText": "to to  => to", "author": "junrao", "createdAt": "2020-06-16T22:42:33Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -239,9 +239,13 @@ class GroupMetadataManager(brokerId: Int,\n     }\n   }\n \n+  /**\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. The caller ought to to complete delayed requests for those returned partitions.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg0NjAwNQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441846005", "bodyText": "if there is no completeDelayedRequests passed in => if completeDelayedRequests is false", "author": "junrao", "createdAt": "2020-06-17T21:31:23Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -967,7 +967,16 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  def appendRecordsToLeader(records: MemoryRecords, origin: AppendOrigin, requiredAcks: Int): LogAppendInfo = {\n+  /**\n+   * @param completeDelayedRequests true: the delayed requests may be completed inside the call with the expectation\n+   *                                that no conflicting locks are held by the caller. Otherwise, the caller is expected\n+   *                                to complete delayed requests for those returned partitions if there is no\n+   *                                completeDelayedRequests passed in.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg0NjgxOQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441846819", "bodyText": "if there is no completeDelayedRequests passed in => if completeDelayedRequests is false", "author": "junrao", "createdAt": "2020-06-17T21:33:16Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -560,19 +603,27 @@ class ReplicaManager(val config: KafkaConfig,\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   *\n+   * @param completeDelayedRequests true: the delayed requests may be completed inside the call with the expectation\n+   *                                that no conflicting locks are held by the caller. Otherwise, the caller is expected\n+   *                                to complete delayed requests for those returned partitions if there is no\n+   *                                completeDelayedRequests passed in.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg1OTExMA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441859110", "bodyText": "I think the intention for the test is probably to use the same producerId since it tests more on transactional conflicts.", "author": "junrao", "createdAt": "2020-06-17T22:02:38Z", "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -106,25 +97,56 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n     }\n   }\n \n-  def createGroupMembers(groupPrefix: String): Set[GroupMember] = {\n-    (0 until nGroups).flatMap { i =>\n-      new Group(s\"$groupPrefix$i\", nMembersPerGroup, groupCoordinator, replicaManager).members\n-    }.toSet\n+  private var producerIdCount = 0L\n+  private val allGroupMembers = mutable.ArrayBuffer[GroupMember]()\n+\n+  def groupMembers(groupId: String, nMembers: Int, groupCoordinator: GroupCoordinator): Seq[GroupMember] = {\n+    val groupPartitionId = groupCoordinator.partitionFor(groupId)\n+    groupCoordinator.groupManager.addPartitionOwnership(groupPartitionId)\n+    val lock = new ReentrantLock()\n+    val producerId = producerIdCount\n+    producerIdCount += 1", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkzODE3NA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441938174", "bodyText": "Got it. However, the same producerId means the group completed by CompleteTxnOperation is possible to be impacted by any CommitTxnOffsetsOperation (since the partitions are same also). Hence, the side-effect is that we need a single lock to control the happen-before of txn completion and commit so the test will get slower.", "author": "chia7712", "createdAt": "2020-06-18T02:48:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg1OTExMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg1OTgyNA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441859824", "bodyText": "Hmm, why don't we need the lock here since CommitTxnOffsetsOperation and CompleteTxnOperation could still run in parallel?", "author": "junrao", "createdAt": "2020-06-17T22:04:30Z", "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -106,25 +97,56 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n     }\n   }\n \n-  def createGroupMembers(groupPrefix: String): Set[GroupMember] = {\n-    (0 until nGroups).flatMap { i =>\n-      new Group(s\"$groupPrefix$i\", nMembersPerGroup, groupCoordinator, replicaManager).members\n-    }.toSet\n+  private var producerIdCount = 0L\n+  private val allGroupMembers = mutable.ArrayBuffer[GroupMember]()\n+\n+  def groupMembers(groupId: String, nMembers: Int, groupCoordinator: GroupCoordinator): Seq[GroupMember] = {\n+    val groupPartitionId = groupCoordinator.partitionFor(groupId)\n+    groupCoordinator.groupManager.addPartitionOwnership(groupPartitionId)\n+    val lock = new ReentrantLock()\n+    val producerId = producerIdCount\n+    producerIdCount += 1\n+    val members = (0 until nMembers).map(i => new GroupMember(groupId = groupId,\n+      groupPartitionId = groupPartitionId,\n+      leader = i == 0,\n+      producerId = producerId,\n+      txnLock = lock))\n+    allGroupMembers ++= members\n+    members\n   }\n \n+  def createGroupMembers(groupPrefix: String): Set[GroupMember] =\n+    (0 until nGroups).flatMap(i => groupMembers(s\"$groupPrefix$i\", nMembersPerGroup, groupCoordinator)).toSet\n+\n   @Test\n   def testConcurrentGoodPathSequence(): Unit = {\n     verifyConcurrentOperations(createGroupMembers, allOperations)\n   }\n \n   @Test\n   def testConcurrentTxnGoodPathSequence(): Unit = {\n-    verifyConcurrentOperations(createGroupMembers, allOperationsWithTxn)\n+    verifyConcurrentOperations(createGroupMembers, Seq(\n+      new JoinGroupOperation,\n+      new SyncGroupOperation,\n+      new OffsetFetchOperation,\n+      new CommitTxnOffsetsOperation(needLock = false),\n+      new CompleteTxnOperation(needLock = false),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkzODM5Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441938393", "bodyText": "you are right", "author": "chia7712", "createdAt": "2020-06-18T02:49:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg1OTgyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg2MjAxMw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r441862013", "bodyText": "Perhaps change the comment to sth like the following?\n\"Setting to true to make CompleteTxnOperation and CommitTxnOffsetsOperation complete atomically since they don't typically overlap. Otherwise CompleteTxnOperation may see a pending offsetAndMetadata without an appendedBatchOffset.\"", "author": "junrao", "createdAt": "2020-06-17T22:10:14Z", "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -278,37 +302,48 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n     }\n   }\n \n-  class CommitTxnOffsetsOperation extends CommitOffsetsOperation {\n+  /**\n+   * @param needLock true to make CompleteTxnOperation happen after CommitTxnOffsetsOperation. It presents error when\n+   *                 both CommitTxnOffsetsOperation and CompleteTxnOperation are executed on parallel.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMzMDYwNg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r442330606", "bodyText": "Since createGroupMembers() is called in multiple tests, it seems we will be accumulating allGroupMembers across tests. That seems unexpected?", "author": "junrao", "createdAt": "2020-06-18T15:53:30Z", "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -106,12 +107,29 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n     }\n   }\n \n-  def createGroupMembers(groupPrefix: String): Set[GroupMember] = {\n-    (0 until nGroups).flatMap { i =>\n-      new Group(s\"$groupPrefix$i\", nMembersPerGroup, groupCoordinator, replicaManager).members\n-    }.toSet\n+  /**\n+   * make CompleteTxnOperation and CommitTxnOffsetsOperation complete atomically since they don't typically overlap.\n+   * Otherwise CompleteTxnOperation may see a pending offsetAndMetadata without an appendedBatchOffset.\n+   */\n+  private val txnLock = new ReentrantLock\n+  private val allGroupMembers = mutable.ArrayBuffer[GroupMember]()\n+\n+  def groupMembers(groupId: String, nMembers: Int, groupCoordinator: GroupCoordinator): Seq[GroupMember] = {\n+    val groupPartitionId = groupCoordinator.partitionFor(groupId)\n+    groupCoordinator.groupManager.addPartitionOwnership(groupPartitionId)\n+    val members = (0 until nMembers).map(i => new GroupMember(groupId = groupId,\n+      groupPartitionId = groupPartitionId,\n+      leader = i == 0,\n+      // same producerId to tests more on transactional conflicts.\n+      producerId = 1000,\n+      txnLock = txnLock))\n+    allGroupMembers ++= members", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMzNzE4NA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r442337184", "bodyText": "Junit, by default, creates a new instance for each test case so allGroupMembers is always new one for each test case.", "author": "chia7712", "createdAt": "2020-06-18T16:03:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMzMDYwNg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYzNDUxNg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r452634516", "bodyText": "It's weird to have a method that invokes a callback and returns a result. Do we need both? We have a number of other methods that do something similar. It would be good to reconsider that as it's difficult to reason about usage in such cases.", "author": "ijuma", "createdAt": "2020-07-10T05:48:24Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -239,9 +239,13 @@ class GroupMetadataManager(brokerId: Int,\n     }\n   }\n \n+  /**\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. The caller ought to complete delayed requests for those returned partitions.\n+   */\n   def storeGroup(group: GroupMetadata,\n                  groupAssignment: Map[String, Array[Byte]],\n-                 responseCallback: Errors => Unit): Unit = {\n+                 responseCallback: Errors => Unit): Map[TopicPartition, LeaderHWChange] = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjc1NDE2NQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r452754165", "bodyText": "@ijuma thanks for your reviews.\n\nIt would be good to reconsider that as it's difficult to reason about usage in such cases.\n\nYou are totally right and @hachikuji had given a great refactor idea (#8657 (comment)). Given that refactor will bring a lot of changes to this PR, I had filed a ticket to refactor related code (see https://issues.apache.org/jira/browse/KAFKA-10170) in order to make this PR focus on bug fix.", "author": "chia7712", "createdAt": "2020-07-10T10:12:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYzNDUxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYzNTExMQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r452635111", "bodyText": "The usual naming convention is to only capitalize the first letter, eg LeaderHwChange.", "author": "ijuma", "createdAt": "2020-07-10T05:50:38Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -65,13 +65,24 @@ import scala.compat.java8.OptionConverters._\n /*\n  * Result metadata of a log append operation on the log\n  */\n-case class LogAppendResult(info: LogAppendInfo, exception: Option[Throwable] = None) {\n+case class LogAppendResult(info: LogAppendInfo,\n+                           exception: Option[Throwable] = None,\n+                           leaderHWChange: LeaderHWChange = LeaderHWChange.None) {\n   def error: Errors = exception match {\n     case None => Errors.NONE\n     case Some(e) => Errors.forException(e)\n   }\n }\n \n+/**\n+ * a flag indicting whether the HWM has been changed.\n+ */\n+sealed trait LeaderHWChange\n+object LeaderHWChange {\n+  case object LeaderHWIncremented extends LeaderHWChange", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjc1NTIzNg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r452755236", "bodyText": "will roger that!", "author": "chia7712", "createdAt": "2020-07-10T10:15:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYzNTExMQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0MzA0MA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r463043040", "bodyText": "I notice that we are including the $ here and in a few other places, we should not do that.", "author": "ijuma", "createdAt": "2020-07-30T14:35:33Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -369,6 +370,32 @@ class GroupCoordinator(val brokerId: Int,\n     }\n   }\n \n+  /**\n+   * try to complete produce, fetch and delete requests if the HW of partition is incremented. Otherwise, we try to complete\n+   * only delayed fetch requests.\n+   *\n+   * Noted that this method may hold a lot of group lock so the caller should NOT hold any group lock\n+   * in order to avoid deadlock\n+   * @param topicPartitions a map contains the partition and a flag indicting whether the HWM has been changed\n+   */\n+  private[group] def completeDelayedRequests(topicPartitions: Map[TopicPartition, LeaderHwChange]): Unit =\n+    topicPartitions.foreach {\n+      case (tp, leaderHWIncremented) => leaderHWIncremented match {\n+        case LeaderHwIncremented$ => groupManager.replicaManager.completeDelayedRequests(tp)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2NjMyMw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r463066323", "bodyText": "will copy that!", "author": "chia7712", "createdAt": "2020-07-30T15:07:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0MzA0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NDM3MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r463044371", "bodyText": "I raised the point before that it's a bit unusual and unintuitive to have both a callback and a return value. Any thoughts on this?", "author": "ijuma", "createdAt": "2020-07-30T14:37:19Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -239,9 +239,13 @@ class GroupMetadataManager(brokerId: Int,\n     }\n   }\n \n+  /**\n+   * @return Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been\n+   *         incremented. The caller ought to complete delayed requests for those returned partitions.\n+   */\n   def storeGroup(group: GroupMetadata,\n                  groupAssignment: Map[String, Array[Byte]],\n-                 responseCallback: Errors => Unit): Unit = {\n+                 responseCallback: Errors => Unit): Map[TopicPartition, LeaderHwChange] = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2NjE5MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r463066191", "bodyText": "The response was #8657 (comment)\nIn short, we should have a way of fetching delayed request from partition instead of using return value to carry them.", "author": "chia7712", "createdAt": "2020-07-30T15:07:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NDM3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxODM0Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r463318346", "bodyText": "Thanks, I had missed that. Will respond in that thread.", "author": "ijuma", "createdAt": "2020-07-30T23:03:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NDM3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2Mjk3Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464562972", "bodyText": "Could we change the explanation to sth like the following?\nThis method may trigger the completeness check for delayed requests in a few purgatories. Occasionally, for serialization in the log, a caller may need to hold a lock while calling this method. To avoid deadlock, if the caller holds a conflicting lock while calling this method, the caller is expected to set completeDelayedRequests to false to avoid checking the delayed operations during this call. The caller will then explicitly complete those delayed operations based on the return value, without holding the conflicting lock.", "author": "junrao", "createdAt": "2020-08-03T17:39:30Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -554,19 +596,27 @@ class ReplicaManager(val config: KafkaConfig,\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   *\n+   * @param completeDelayedRequests true: the delayed requests may be completed inside the call with the expectation\n+   *                                that no conflicting locks are held by the caller. Otherwise, the caller is expected\n+   *                                to complete delayed requests for those returned partitions if completeDelayedRequests\n+   *                                is false", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NzIxOQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464567219", "bodyText": "group lock => conflicting lock", "author": "junrao", "createdAt": "2020-08-03T17:45:18Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -334,6 +345,37 @@ class ReplicaManager(val config: KafkaConfig,\n       brokerTopicStats.removeMetrics(topic)\n   }\n \n+  /**\n+   * try to complete delayed requests in following purgatories.\n+   * 1) delayedFetchPurgatory\n+   * 2) delayedProducePurgatory\n+   * 3) delayedDeleteRecordsPurgatory\n+   *\n+   * Noted that caller should NOT hold any group lock in order to avoid deadlock.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NzI2Nw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464567267", "bodyText": "group lock => conflicting lock", "author": "junrao", "createdAt": "2020-08-03T17:45:24Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -334,6 +345,37 @@ class ReplicaManager(val config: KafkaConfig,\n       brokerTopicStats.removeMetrics(topic)\n   }\n \n+  /**\n+   * try to complete delayed requests in following purgatories.\n+   * 1) delayedFetchPurgatory\n+   * 2) delayedProducePurgatory\n+   * 3) delayedDeleteRecordsPurgatory\n+   *\n+   * Noted that caller should NOT hold any group lock in order to avoid deadlock.\n+   *\n+   * this method is visible for testing.\n+   */\n+  def completeDelayedRequests(topicPartition: TopicPartition): Unit = {\n+    val topicPartitionOperationKey = TopicPartitionOperationKey(topicPartition)\n+    delayedFetchPurgatory.checkAndComplete(topicPartitionOperationKey)\n+    delayedProducePurgatory.checkAndComplete(topicPartitionOperationKey)\n+    delayedDeleteRecordsPurgatory.checkAndComplete(topicPartitionOperationKey)\n+  }\n+\n+  /**\n+   * try to complete delayed requests in delayedFetchPurgatory.\n+   *\n+   * Noted that caller should NOT hold any group lock in order to avoid deadlock.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3NDgzNw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464574837", "bodyText": "Perhaps add \"but completes the delayed requests without holding the group lock\".", "author": "junrao", "createdAt": "2020-08-03T18:00:13Z", "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -33,11 +34,40 @@ import scala.math.{max, min}\n  */\n private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  group: GroupMetadata,\n-                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n+                                 rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout) {\n \n-  override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  /**\n+   * The delayed requests should be completed without holding group lock so we keep those partitions and then\n+   * complete them after releasing lock.\n+   */\n+  private[group] var partitionsToComplete: scala.collection.Map[TopicPartition, LeaderHwChange] = Map.empty\n+\n+  /**\n+   * It controls the lock manually since GroupCoordinator#onCompleteJoin() invoked by onComplete() can't be within a\n+   * group lock since the completion of the delayed request for partitions returned from GroupCoordinator#onCompleteJoin()\n+   * need to be done outside of the group lock.\n+   */\n+  override def tryComplete(): Boolean = try group.inLock {\n+    /**\n+     * holds the group lock for both the \"group.hasAllMembersJoined\" check and the call to forceComplete()\n+     */", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4MDM4OA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464580388", "bodyText": "a lot of group lock => multiple group locks", "author": "junrao", "createdAt": "2020-08-03T18:11:18Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -369,6 +370,32 @@ class GroupCoordinator(val brokerId: Int,\n     }\n   }\n \n+  /**\n+   * try to complete produce, fetch and delete requests if the HW of partition is incremented. Otherwise, we try to complete\n+   * only delayed fetch requests.\n+   *\n+   * Noted that this method may hold a lot of group lock so the caller should NOT hold any group lock", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4MDg5MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464580891", "bodyText": "\"this method may hold a lot of group lock\" : This is actually not true. Unlike producer/fetch purgatory, which is keyed on partition, joinPurgatory is keyed on the group. So, when we complete a key, only a single group's lock will be held.\nThe reason that we don't want the caller to hold a group lock is that DelayedJoin itself uses a lock other than the group lock for DelayedOperation.maybeTryComplete() and we want to avoid the deadlock between that lock and the group lock.", "author": "junrao", "createdAt": "2020-08-03T18:12:15Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -369,6 +370,32 @@ class GroupCoordinator(val brokerId: Int,\n     }\n   }\n \n+  /**\n+   * try to complete produce, fetch and delete requests if the HW of partition is incremented. Otherwise, we try to complete\n+   * only delayed fetch requests.\n+   *\n+   * Noted that this method may hold a lot of group lock so the caller should NOT hold any group lock\n+   * in order to avoid deadlock\n+   * @param topicPartitions a map contains the partition and a flag indicting whether the HWM has been changed\n+   */\n+  private[group] def completeDelayedRequests(topicPartitions: Map[TopicPartition, LeaderHwChange]): Unit =\n+    topicPartitions.foreach {\n+      case (tp, leaderHWIncremented) => leaderHWIncremented match {\n+        case LeaderHwIncremented => groupManager.replicaManager.completeDelayedRequests(tp)\n+        case _ => groupManager.replicaManager.completeDelayedFetchRequests(tp)\n+      }\n+    }\n+\n+  /**\n+   * complete the delayed join requests associated to input group keys.\n+   *\n+   * Noted that this method may hold a lot of group lock so the caller should NOT hold any group lock", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc5MTQxNA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464791414", "bodyText": "thanks for explanation. I will revise the comment according to your comment.", "author": "chia7712", "createdAt": "2020-08-04T04:25:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4MDg5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5Nzg0MA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464697840", "bodyText": "This could be completeDelayedJoinRequests(groupsToComplete) ?", "author": "junrao", "createdAt": "2020-08-03T22:36:59Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1240,26 +1298,30 @@ class GroupCoordinator(val brokerId: Int,\n   }\n \n   def onExpireHeartbeat(group: GroupMetadata, memberId: String, isPending: Boolean): Unit = {\n+    val groupsToComplete = scala.collection.mutable.Set[GroupKey]()\n     group.inLock {\n       if (group.is(Dead)) {\n         info(s\"Received notification of heartbeat expiration for member $memberId after group ${group.groupId} had already been unloaded or deleted.\")\n       } else if (isPending) {\n         info(s\"Pending member $memberId in group ${group.groupId} has been removed after session timeout expiration.\")\n-        removePendingMemberAndUpdateGroup(group, memberId)\n+        groupsToComplete ++= removePendingMemberAndUpdateGroup(group, memberId)\n       } else if (!group.has(memberId)) {\n         debug(s\"Member $memberId has already been removed from the group.\")\n       } else {\n         val member = group.get(memberId)\n         if (!member.hasSatisfiedHeartbeat) {\n           info(s\"Member ${member.memberId} in group ${group.groupId} has failed, removing it from the group\")\n-          removeMemberAndUpdateGroup(group, member, s\"removing member ${member.memberId} on heartbeat expiration\")\n+          groupsToComplete ++= removeMemberAndUpdateGroup(group, member, s\"removing member ${member.memberId} on heartbeat expiration\")\n         }\n       }\n     }\n+    groupsToComplete.foreach(joinPurgatory.checkAndComplete)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxNTM3Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r464715376", "bodyText": "Perhaps we could add a comment on what this method is intended to test?", "author": "junrao", "createdAt": "2020-08-03T23:33:17Z", "path": "core/src/test/scala/unit/kafka/coordinator/group/DelayedJoinTest.scala", "diffHunk": "@@ -0,0 +1,73 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.coordinator.group\n+\n+import java.util.concurrent.{CountDownLatch, Executors, TimeUnit}\n+\n+import kafka.utils.MockTime\n+import org.easymock.EasyMock\n+import org.junit.{Assert, Test}\n+\n+class DelayedJoinTest {\n+\n+  @Test\n+  def testCompleteDelayedRequests(): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE3OTMzNQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r465179335", "bodyText": "Hmm, it seems that we are now introducing a new potential deadlock. The conflicting paths are the following.\npath 1\nhold group lock -> joinPurgatory.tryCompleteElseWatch(delayedJoin) -> watchForOperation (now delayedJoin visible through other threads) -> operation.maybeTryComplete() -> hold delayedJoin.lock\npath 2\ndelayedJoin.maybeTryComplete -> hold hold delayedJoin.lock -> tryComplete() -> hold group lock", "author": "junrao", "createdAt": "2020-08-04T16:31:50Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -369,6 +370,32 @@ class GroupCoordinator(val brokerId: Int,\n     }\n   }\n \n+  /**\n+   * try to complete produce, fetch and delete requests if the HW of partition is incremented. Otherwise, we try to complete\n+   * only delayed fetch requests.\n+   *\n+   * Noted that this method may hold multiple group locks so the caller should NOT hold any group lock\n+   * in order to avoid deadlock\n+   * @param topicPartitions a map contains the partition and a flag indicting whether the HWM has been changed\n+   */\n+  private[group] def completeDelayedRequests(topicPartitions: Map[TopicPartition, LeaderHwChange]): Unit =\n+    topicPartitions.foreach {\n+      case (tp, leaderHWIncremented) => leaderHWIncremented match {\n+        case LeaderHwIncremented => groupManager.replicaManager.completeDelayedRequests(tp)\n+        case _ => groupManager.replicaManager.completeDelayedFetchRequests(tp)\n+      }\n+    }\n+\n+  /**\n+   * complete the delayed join requests associated to input group keys.\n+   *\n+   * Noted that delayedJoin itself uses a lock other than the group lock for DelayedOperation.maybeTryComplete() and", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIwMDUwOA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r465200508", "bodyText": "How about removing inner lock (ReentrantLock) from DelayedJoin ? It seems to me DelayedJoin does not need the inner lock.", "author": "chia7712", "createdAt": "2020-08-04T17:07:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE3OTMzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIwNjI1Nw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r465206257", "bodyText": "another approach is that - we introduce an new method \"afterTryComplete\" to DelayedOperation. the new method is invoked by maybeTryComplete after lock is released. DelayedJoin still pass group lock to DelayedOperation and use \"afterTryComplete\" to complete delayed requests", "author": "chia7712", "createdAt": "2020-08-04T17:17:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE3OTMzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTg2NzQwNg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r465867406", "bodyText": "@chia7712 : Yes, that's a possibility. It adds some complexity to DelayedOperation. Another possibility is to have a special case to complete the delayed requests from groupManager.storeGroup() in GroupCoordinator.onCompleteJoin() in a separate thread.", "author": "junrao", "createdAt": "2020-08-05T16:53:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE3OTMzNQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODU5OTk3OQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r478599979", "bodyText": "Unneeded new line.", "author": "junrao", "createdAt": "2020-08-27T18:01:35Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -585,6 +597,27 @@ class ReplicaManager(val config: KafkaConfig,\n                     result.info.logStartOffset, result.info.recordErrors.asJava, result.info.errorMessage)) // response status\n       }\n \n+      delayedActions.put {\n+        () =>\n+          localProduceResults.foreach {\n+            case (topicPartition, result) =>\n+              result.info.leaderHWIncremented.foreach {\n+                incremented =>\n+                  val requestKey = TopicPartitionOperationKey(topicPartition)\n+                  if (incremented) {\n+                    // some delayed operations may be unblocked after HW changed\n+                    delayedProducePurgatory.checkAndComplete(requestKey)\n+                    delayedFetchPurgatory.checkAndComplete(requestKey)\n+                    delayedDeleteRecordsPurgatory.checkAndComplete(requestKey)\n+                  } else {\n+                    // probably unblock some follower fetch requests since log end offset has been updated\n+                    delayedFetchPurgatory.checkAndComplete(requestKey)\n+                  }\n+              }\n+          }\n+      }\n+\n+", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYwMDUwNw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r478600507", "bodyText": "We probably want to add a comment why this is needed.", "author": "junrao", "createdAt": "2020-08-27T18:02:36Z", "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -36,8 +36,13 @@ private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n \n   override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n+  override def onExpiration() = {\n+    coordinator.onExpireJoin()\n+    tryToCompleteDelayedAction()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYwMzI0Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r478603243", "bodyText": "Hmm, why do we need to override this instead of using the one defined in DelayedJoin?", "author": "junrao", "createdAt": "2020-08-27T18:07:40Z", "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -57,6 +62,8 @@ private[group] class InitialDelayedJoin(coordinator: GroupCoordinator,\n \n   override def tryComplete(): Boolean = false\n \n+  override def onExpiration(): Unit = tryToCompleteDelayedAction()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYwMzc3Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r478603776", "bodyText": "This probably should be included in the local time as before.", "author": "junrao", "createdAt": "2020-08-27T18:08:37Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -187,6 +187,8 @@ class KafkaApis(val requestChannel: RequestChannel,\n       // The local completion time may be set while processing the request. Only record it if it's unset.\n       if (request.apiLocalCompleteTimeNanos < 0)\n         request.apiLocalCompleteTimeNanos = time.nanoseconds\n+\n+      replicaManager.tryCompleteDelayedAction()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYwNDI5Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r478604292", "bodyText": "Could we add the new param to the javadoc?", "author": "junrao", "createdAt": "2020-08-27T18:09:33Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -100,7 +100,8 @@ case class LogAppendInfo(var firstOffset: Option[Long],\n                          offsetsMonotonic: Boolean,\n                          lastOffsetOfFirstBatch: Long,\n                          recordErrors: Seq[RecordError] = List(),\n-                         errorMessage: String = null) {\n+                         errorMessage: String = null,\n+                         leaderHWIncremented: Option[Boolean] = None) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM3OTcwNQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479379705", "bodyText": "This can just be private.", "author": "junrao", "createdAt": "2020-08-28T15:31:08Z", "path": "core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala", "diffHunk": "@@ -36,8 +36,14 @@ private[group] class DelayedJoin(coordinator: GroupCoordinator,\n                                  rebalanceTimeout: Long) extends DelayedOperation(rebalanceTimeout, Some(group.lock)) {\n \n   override def tryComplete(): Boolean = coordinator.tryCompleteJoin(group, forceComplete _)\n-  override def onExpiration() = coordinator.onExpireJoin()\n-  override def onComplete() = coordinator.onCompleteJoin(group)\n+  override def onExpiration(): Unit = {\n+    coordinator.onExpireJoin()\n+    // try to complete delayed actions introduced by coordinator.onCompleteJoin\n+    tryToCompleteDelayedAction()\n+  }\n+  override def onComplete(): Unit = coordinator.onCompleteJoin(group)\n+\n+  protected def tryToCompleteDelayedAction(): Unit = coordinator.groupManager.replicaManager.tryCompleteDelayedAction()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM4MTk2Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479381963", "bodyText": "KafkaApis.handle() => KafkaApis.handle() and the expiration thread for certain delayed operations (e.g. DelayedJoin)", "author": "junrao", "createdAt": "2020-08-28T15:34:58Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -110,31 +109,21 @@ abstract class DelayedOperation(override val delayMs: Long,\n    * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n    * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n    * the operation is actually completed.\n+   *\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * The approach is that ReplicaManager collects all actions, which are used to complete delayed requests, in a queue.\n+   * KafkaApis.handle() picks up and then execute an action when no lock is held.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM4MjE2Nw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479382167", "bodyText": "The above comment is outdated now.", "author": "junrao", "createdAt": "2020-08-28T15:35:19Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -110,31 +109,21 @@ abstract class DelayedOperation(override val delayMs: Long,\n    * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n    * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n    * the operation is actually completed.\n+   *", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM4Mjk2Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479382962", "bodyText": "We probably should rename this to sth like safeTryComplete().", "author": "junrao", "createdAt": "2020-08-28T15:36:41Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -110,31 +109,21 @@ abstract class DelayedOperation(override val delayMs: Long,\n    * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n    * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n    * the operation is actually completed.\n+   *\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * The approach is that ReplicaManager collects all actions, which are used to complete delayed requests, in a queue.\n+   * KafkaApis.handle() picks up and then execute an action when no lock is held.\n    */\n-  private[server] def maybeTryComplete(): Boolean = {\n-    var retry = false\n-    var done = false\n-    do {\n-      if (lock.tryLock()) {\n-        try {\n-          tryCompletePending.set(false)\n-          done = tryComplete()\n-        } finally {\n-          lock.unlock()\n-        }\n-        // While we were holding the lock, another thread may have invoked `maybeTryComplete` and set\n-        // `tryCompletePending`. In this case we should retry.\n-        retry = tryCompletePending.get()\n-      } else {\n-        // Another thread is holding the lock. If `tryCompletePending` is already set and this thread failed to\n-        // acquire the lock, then the thread that is holding the lock is guaranteed to see the flag and retry.\n-        // Otherwise, we should set the flag and retry on this thread since the thread holding the lock may have\n-        // released the lock and returned by the time the flag is set.\n-        retry = !tryCompletePending.getAndSet(true)\n-      }\n-    } while (!isCompleted && retry)\n-    done\n-  }\n+  private[server] def maybeTryComplete(): Boolean = inLock(lock)(tryComplete())", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM4MzgwNw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479383807", "bodyText": "The default value is None.", "author": "junrao", "createdAt": "2020-08-28T15:38:11Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -85,6 +85,8 @@ object LogAppendInfo {\n  * @param validBytes The number of valid bytes\n  * @param offsetsMonotonic Are the offsets in this message set monotonically increasing\n  * @param lastOffsetOfFirstBatch The last offset of the first batch\n+ * @param leaderHWIncremented true if the high watermark is increased when appending record. Otherwise, false.\n+ *                            this field is updated after appending record so it has default value option.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM4OTYxNw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479389617", "bodyText": "at the end of KafkaApis.handle() => at the end of KafkaApis.handle() and the expiration thread for certain delayed operations (e.g. DelayedJoin)", "author": "junrao", "createdAt": "2020-08-28T15:49:04Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -558,10 +558,27 @@ class ReplicaManager(val config: KafkaConfig,\n     localLog(topicPartition).map(_.parentDir)\n   }\n \n+  // visible for testing\n+  val delayedActions = new LinkedBlockingQueue[() => Unit]()\n+\n+  /**\n+   * try to complete delayed action. In order to avoid conflicting locking, the actions to complete delayed requests\n+   * are kept in a queue. We add the logic to check the ReplicaManager queue at the end of KafkaApis.handle(),", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM5MDA1Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479390056", "bodyText": "in a queue => are stored in a queue", "author": "junrao", "createdAt": "2020-08-28T15:49:47Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -558,10 +558,27 @@ class ReplicaManager(val config: KafkaConfig,\n     localLog(topicPartition).map(_.parentDir)\n   }\n \n+  // visible for testing\n+  val delayedActions = new LinkedBlockingQueue[() => Unit]()\n+\n+  /**\n+   * try to complete delayed action. In order to avoid conflicting locking, the actions to complete delayed requests\n+   * are kept in a queue. We add the logic to check the ReplicaManager queue at the end of KafkaApis.handle(),\n+   * at which point, no conflicting locks will be held.\n+   */\n+  def tryCompleteDelayedAction(): Unit = {\n+    val action = delayedActions.poll()\n+    if (action != null) action()\n+  }\n+\n   /**\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   *\n+   * Noted that all pending delayed check operations in a queue. All callers to ReplicaManager.appendRecords() are", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ2NzU0OA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479467548", "bodyText": "The last sentence doesn't complete.", "author": "junrao", "createdAt": "2020-08-28T18:24:30Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,41 +99,21 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * The approach is that ReplicaManager collects all actions, which are used to complete delayed requests, in a queue.\n+   * KafkaApis.handle() and the expiration thread for certain delayed operations (e.g. DelayedJoin)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3NDkwNA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479474904", "bodyText": "Can we not use a boolean here? false until it's been incremented and then true. Is there value in having the third state?", "author": "ijuma", "createdAt": "2020-08-28T18:41:05Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -100,7 +102,8 @@ case class LogAppendInfo(var firstOffset: Option[Long],\n                          offsetsMonotonic: Boolean,\n                          lastOffsetOfFirstBatch: Long,\n                          recordErrors: Seq[RecordError] = List(),\n-                         errorMessage: String = null) {\n+                         errorMessage: String = null,\n+                         leaderHWIncremented: Option[Boolean] = None) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3NTY4MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479475681", "bodyText": "Should we be guarding against exceptions here?", "author": "ijuma", "createdAt": "2020-08-28T18:42:47Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -184,6 +184,7 @@ class KafkaApis(val requestChannel: RequestChannel,\n       case e: FatalExitError => throw e\n       case e: Throwable => handleError(request, e)\n     } finally {\n+      replicaManager.tryCompleteDelayedAction()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY2MTM4Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479661386", "bodyText": "a action => an action", "author": "junrao", "createdAt": "2020-08-29T15:40:22Z", "path": "core/src/main/scala/kafka/server/ActionQueue.scala", "diffHunk": "@@ -0,0 +1,48 @@\n+/**\n+  * Licensed to the Apache Software Foundation (ASF) under one or more\n+  * contributor license agreements.  See the NOTICE file distributed with\n+  * this work for additional information regarding copyright ownership.\n+  * The ASF licenses this file to You under the Apache License, Version 2.0\n+  * (the \"License\"); you may not use this file except in compliance with\n+  * the License.  You may obtain a copy of the License at\n+  *\n+  *    http://www.apache.org/licenses/LICENSE-2.0\n+  *\n+  * Unless required by applicable law or agreed to in writing, software\n+  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  * See the License for the specific language governing permissions and\n+  * limitations under the License.\n+  */\n+\n+package kafka.server\n+\n+import java.util.concurrent.LinkedBlockingDeque\n+\n+/**\n+ * This queue is used to collect actions which need to be executed later. One use case is that ReplicaManager#appendRecords\n+ * produces record changes so we need to check and complete delayed requests. In order to avoid conflicting locking,\n+ * we add those actions to this queue and then complete them at the end of KafkaApis.handle() or DelayedJoin.onExpiration.\n+ */\n+class ActionQueue {\n+  private val queue = new LinkedBlockingDeque[() => Unit]()\n+\n+  /**\n+   * add action to this queue.\n+   * @param action action\n+   */\n+  def add(action: () => Unit): Unit = queue.put(action)\n+\n+  /**\n+   * picks up a action to complete.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY2OTM1NA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479669354", "bodyText": "Even if we hit an exception in handleXXX(), it would still be useful to complete the actionQueue.", "author": "junrao", "createdAt": "2020-08-29T17:05:19Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -180,6 +181,11 @@ class KafkaApis(val requestChannel: RequestChannel,\n         case ApiKeys.DESCRIBE_CLIENT_QUOTAS => handleDescribeClientQuotasRequest(request)\n         case ApiKeys.ALTER_CLIENT_QUOTAS => handleAlterClientQuotasRequest(request)\n       }\n+\n+      // try to complete delayed action. In order to avoid conflicting locking, the actions to complete delayed requests\n+      // are kept in a queue. We add the logic to check the ReplicaManager queue at the end of KafkaApis.handle() and the\n+      // expiration thread for certain delayed operations (e.g. DelayedJoin)\n+      actionQueue.tryCompleteAction()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3Nzg4OQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479677889", "bodyText": "Completing delayed actions may cause exception. Should exception be swallowed and log if we move the completion to the final block?", "author": "chia7712", "createdAt": "2020-08-29T18:40:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY2OTM1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4MzIxMg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479683212", "bodyText": "Yes, if actionQueue.tryCompleteAction() throws an exception, we can just catch it and log a warning in finally since the response has been sent by then.", "author": "junrao", "createdAt": "2020-08-29T19:42:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY2OTM1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MDA1Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479670056", "bodyText": "It seems that we have to distinguish 3 states here: (1) records not appended due to an error; (2) records appended successfully and HWM advanced; (3) records appended successfully and HWM not advanced. In case (1), no purgatory needs to be checked.", "author": "junrao", "createdAt": "2020-08-29T17:13:30Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -585,6 +591,23 @@ class ReplicaManager(val config: KafkaConfig,\n                     result.info.logStartOffset, result.info.recordErrors.asJava, result.info.errorMessage)) // response status\n       }\n \n+      actionQueue.add {\n+        () =>\n+          localProduceResults.foreach {\n+            case (topicPartition, result) =>\n+              val requestKey = TopicPartitionOperationKey(topicPartition)\n+              if (result.info.leaderHWIncremented) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3ODY1Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479678653", "bodyText": "you are right. I miss the exception in appendToLocalLog. Let me revert this change", "author": "chia7712", "createdAt": "2020-08-29T18:50:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MDA1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MDM1MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479670351", "bodyText": "Is this used?", "author": "junrao", "createdAt": "2020-08-29T17:16:24Z", "path": "core/src/main/scala/kafka/server/ActionQueue.scala", "diffHunk": "@@ -0,0 +1,48 @@\n+/**\n+  * Licensed to the Apache Software Foundation (ASF) under one or more\n+  * contributor license agreements.  See the NOTICE file distributed with\n+  * this work for additional information regarding copyright ownership.\n+  * The ASF licenses this file to You under the Apache License, Version 2.0\n+  * (the \"License\"); you may not use this file except in compliance with\n+  * the License.  You may obtain a copy of the License at\n+  *\n+  *    http://www.apache.org/licenses/LICENSE-2.0\n+  *\n+  * Unless required by applicable law or agreed to in writing, software\n+  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  * See the License for the specific language governing permissions and\n+  * limitations under the License.\n+  */\n+\n+package kafka.server\n+\n+import java.util.concurrent.LinkedBlockingDeque\n+\n+/**\n+ * This queue is used to collect actions which need to be executed later. One use case is that ReplicaManager#appendRecords\n+ * produces record changes so we need to check and complete delayed requests. In order to avoid conflicting locking,\n+ * we add those actions to this queue and then complete them at the end of KafkaApis.handle() or DelayedJoin.onExpiration.\n+ */\n+class ActionQueue {\n+  private val queue = new LinkedBlockingDeque[() => Unit]()\n+\n+  /**\n+   * add action to this queue.\n+   * @param action action\n+   */\n+  def add(action: () => Unit): Unit = queue.put(action)\n+\n+  /**\n+   * picks up a action to complete.\n+   */\n+  def tryCompleteAction(): Unit = {\n+    val action = queue.poll()\n+    if (action != null) action()\n+  }\n+\n+  /**\n+   * @return number of actions kept by this queue\n+   */\n+  def size: Int = queue.size()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MDcyNw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479670727", "bodyText": "Perhaps we could add a note at the top of DelayedOperation so that people are aware of the need to complete actions for new DelayedOperations in the future.", "author": "junrao", "createdAt": "2020-08-29T17:20:46Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,41 +99,22 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * The approach is that ReplicaManager collects all actions, which are used to complete delayed requests, in a queue.\n+   * KafkaApis.handle() and the expiration thread for certain delayed operations (e.g. DelayedJoin) pick up and then\n+   * execute an action when no lock is held.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3OTkzMA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479679930", "bodyText": "It is dangerous to complete delayed actions by DelayedOperation as most methods of DelayedOperation are executed with locking. We, now, depend on KafkaApi. handler to complete the delayed action produced by someone who can't complete delayed action due to locking. For example, DelayedJoin.onComplete can produce an new action and the new action can't be completed by DelayedJoin itself due to group locking.", "author": "chia7712", "createdAt": "2020-08-29T19:05:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MDcyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4MzU0NA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479683544", "bodyText": "I was thinking to add a comment so that if someone adds a future delayed operation that calls ReplicaManager.appendRecords() in onComplete() like DelayedJoin, he/she is aware that this operation's onExpiration() needs to call actionQueue.tryCompleteAction().", "author": "junrao", "createdAt": "2020-08-29T19:45:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MDcyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3ODc2MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479678761", "bodyText": "What's the reasoning for taking just 1 item @junrao? Could this cause the queue to grow over time?", "author": "ijuma", "createdAt": "2020-08-29T18:51:18Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -562,6 +564,10 @@ class ReplicaManager(val config: KafkaConfig,\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   *\n+   * Noted that all pending delayed check operations are stored in a queue. All callers to ReplicaManager.appendRecords()\n+   * are expected to take up to 1 item from that queue and check the completeness for all affected partitions, without", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4Mjg0OQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479682849", "bodyText": "Good question. It's based on the assumption that each KafkaApis.handle() call only calls ReplicaManager. appendRecords() once. Not sure if this is always true in the future. Perhaps a safer approach is to have Action.tryCompleteAction() get the current size of the queue and complete all those actions.", "author": "junrao", "createdAt": "2020-08-29T19:37:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3ODc2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4MzAzNg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479683036", "bodyText": "It should be fine to let handler complete actions as much as possible since the response is created before handling delayed actions.", "author": "chia7712", "createdAt": "2020-08-29T19:40:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3ODc2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3ODgxMg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479678812", "bodyText": "Another approach would be for this queue to be per request thread instead of per server. That would simplify concurrency handling.", "author": "ijuma", "createdAt": "2020-08-29T18:52:04Z", "path": "core/src/main/scala/kafka/server/KafkaServer.scala", "diffHunk": "@@ -134,6 +134,8 @@ class KafkaServer(val config: KafkaConfig, time: Time = Time.SYSTEM, threadNameP\n   private val KAFKA_CLUSTER_ID: String = \"kafka.cluster.id\"\n   private val KAFKA_BROKER_ID: String = \"kafka.broker.id\"\n \n+  private val actionQueue = new ActionQueue", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4MDYzNw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479680637", "bodyText": "The thread has to pass queue to method of ReplicManager/GroupCoordinator if queue is kept by thread. A lot of methods are included so that is a big patch. I prefer to keep small patch though it gets bigger now :(", "author": "chia7712", "createdAt": "2020-08-29T19:13:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3ODgxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4NDM1Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479684353", "bodyText": "Note that the action queue is not only called by requests threads, but also by the expiration thread for certain delayed operations.", "author": "junrao", "createdAt": "2020-08-29T19:55:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3ODgxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4NjI5Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479686292", "bodyText": "Good point @junrao.", "author": "ijuma", "createdAt": "2020-08-29T20:17:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3ODgxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4NDE2Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479684163", "bodyText": "Perhaps we can make this a bit clearer. Sth like the following.\nleaderHWIncremented has 3 possible values: (1) If records are not appended due to an error, the value will be None; (2) if records are appended successfully and HWM is advanced, the value is Some(true); (3) if records are appended successfully and HWM is not advanced, the value is Some(false).", "author": "junrao", "createdAt": "2020-08-29T19:53:51Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -85,6 +85,8 @@ object LogAppendInfo {\n  * @param validBytes The number of valid bytes\n  * @param offsetsMonotonic Are the offsets in this message set monotonically increasing\n  * @param lastOffsetOfFirstBatch The last offset of the first batch\n+ * @param leaderHWIncremented true if the high watermark is increased when appending record. Otherwise, false.\n+ *                            this field is updated after appending record so the default value is None.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4NjM5OA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479686398", "bodyText": "@chia7712 I suggest using a sealed trait with 3 case objects to make this clearer. Using Option[Boolean] as a tristate value is generally best avoided.", "author": "ijuma", "createdAt": "2020-08-29T20:19:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4NDE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4NjUxNw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479686517", "bodyText": "Will copy that", "author": "chia7712", "createdAt": "2020-08-29T20:20:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4NDE2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTM4MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479689381", "bodyText": "@junrao @ijuma please take a look at this method", "author": "chia7712", "createdAt": "2020-08-29T20:55:45Z", "path": "core/src/main/scala/kafka/server/ActionQueue.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/**\n+  * Licensed to the Apache Software Foundation (ASF) under one or more\n+  * contributor license agreements.  See the NOTICE file distributed with\n+  * this work for additional information regarding copyright ownership.\n+  * The ASF licenses this file to You under the Apache License, Version 2.0\n+  * (the \"License\"); you may not use this file except in compliance with\n+  * the License.  You may obtain a copy of the License at\n+  *\n+  *    http://www.apache.org/licenses/LICENSE-2.0\n+  *\n+  * Unless required by applicable law or agreed to in writing, software\n+  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  * See the License for the specific language governing permissions and\n+  * limitations under the License.\n+  */\n+\n+package kafka.server\n+\n+import java.util.concurrent.LinkedBlockingDeque\n+\n+/**\n+ * This queue is used to collect actions which need to be executed later. One use case is that ReplicaManager#appendRecords\n+ * produces record changes so we need to check and complete delayed requests. In order to avoid conflicting locking,\n+ * we add those actions to this queue and then complete them at the end of KafkaApis.handle() or DelayedJoin.onExpiration.\n+ */\n+class ActionQueue {\n+  private val queue = new LinkedBlockingDeque[() => Unit]()\n+\n+  /**\n+   * add action to this queue.\n+   * @param action action\n+   */\n+  def add(action: () => Unit): Unit = queue.put(action)\n+\n+  /**\n+   * picks up an action to complete.\n+   */\n+  def tryCompleteAction(): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTQ1Nw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479689457", "bodyText": "Main thing to decide is what to do in case of exception, do we stop processing or do we continue?", "author": "ijuma", "createdAt": "2020-08-29T20:56:53Z", "path": "core/src/main/scala/kafka/server/ActionQueue.scala", "diffHunk": "@@ -37,7 +37,10 @@ class ActionQueue {\n    * picks up an action to complete.\n    */\n   def tryCompleteAction(): Unit = {\n-    val action = queue.poll()\n-    if (action != null) action()\n+    var action = queue.poll()\n+    while (action != null) {\n+      action()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTY5NA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479689694", "bodyText": "I prefer to just catch it and log a warning as the response has been processed. WDYT?", "author": "chia7712", "createdAt": "2020-08-29T20:59:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTQ1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcwNjMwMw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479706303", "bodyText": "Perhaps we could do the try/catch of each action here instead of KafkaApis. This way, we are guaranteed that all pending actions are processed in time.", "author": "junrao", "createdAt": "2020-08-30T01:01:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTQ1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTY1Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479689653", "bodyText": "Why are we using a blocking queue? It doesn't seem like we ever need the blocking functionality. Am I missing something?", "author": "ijuma", "createdAt": "2020-08-29T20:59:38Z", "path": "core/src/main/scala/kafka/server/ActionQueue.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/**\n+  * Licensed to the Apache Software Foundation (ASF) under one or more\n+  * contributor license agreements.  See the NOTICE file distributed with\n+  * this work for additional information regarding copyright ownership.\n+  * The ASF licenses this file to You under the Apache License, Version 2.0\n+  * (the \"License\"); you may not use this file except in compliance with\n+  * the License.  You may obtain a copy of the License at\n+  *\n+  *    http://www.apache.org/licenses/LICENSE-2.0\n+  *\n+  * Unless required by applicable law or agreed to in writing, software\n+  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  * See the License for the specific language governing permissions and\n+  * limitations under the License.\n+  */\n+\n+package kafka.server\n+\n+import java.util.concurrent.LinkedBlockingDeque\n+\n+/**\n+ * This queue is used to collect actions which need to be executed later. One use case is that ReplicaManager#appendRecords\n+ * produces record changes so we need to check and complete delayed requests. In order to avoid conflicting locking,\n+ * we add those actions to this queue and then complete them at the end of KafkaApis.handle() or DelayedJoin.onExpiration.\n+ */\n+class ActionQueue {\n+  private val queue = new LinkedBlockingDeque[() => Unit]()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTgzOA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479689838", "bodyText": "you are right. How about using ConcurrentLinkedQueue instead?", "author": "chia7712", "createdAt": "2020-08-29T21:01:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY5MTE4MA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479691180", "bodyText": "Yes, I think that's better. One thing I was wondering about is whether contention is going to be an issue for this ActionQueue. Multiple threads are adding items to it and then trying to consume from it. I haven't thought about all the details, but would a thread local work better? In that case, each thread would add and then drain. This works fine for the request threads, but I wasn't sure about the other case that @junrao pointed out.", "author": "ijuma", "createdAt": "2020-08-29T21:20:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY5Mjg5Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479692892", "bodyText": "Handler (thread) can have local ActionQueue and it is passed to each method to collect delayed actions. DelayedJoin is specific case since the delayed actions are possible to be access by two thread (timeout thread and handler thread). A simple way to address thread local is that DelayedJoin owns an ActionQueue and the queue is passed to coordinator.onCompleteJoin and then the queue is consumed by DelayedJoin.onExpiration. Both onComplete and onExpiration are thread-safe so use a thread local queue is safe.", "author": "chia7712", "createdAt": "2020-08-29T21:44:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxNjg4Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479716882", "bodyText": "Maybe we can go with a single ActionQueue in this PR. We can submit a separate PR for reducing the contention by having one per thread.", "author": "ijuma", "createdAt": "2020-08-30T03:41:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTczNzM1OA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479737358", "bodyText": "Sure. I will file a PR to follow the pattern in #9229\nIn order to simplify code base, I will revert the action queue in \"per server\" :)", "author": "chia7712", "createdAt": "2020-08-30T08:05:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY4OTY1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcwNjE4Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479706182", "bodyText": "If we are unlucky, a single thread could be held up in this loop for a long time. Perhaps we could let each thread only complete the number of actions that it sees when entering tryCompleteActions().", "author": "junrao", "createdAt": "2020-08-30T00:59:37Z", "path": "core/src/main/scala/kafka/server/ActionQueue.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/**\n+  * Licensed to the Apache Software Foundation (ASF) under one or more\n+  * contributor license agreements.  See the NOTICE file distributed with\n+  * this work for additional information regarding copyright ownership.\n+  * The ASF licenses this file to You under the Apache License, Version 2.0\n+  * (the \"License\"); you may not use this file except in compliance with\n+  * the License.  You may obtain a copy of the License at\n+  *\n+  *    http://www.apache.org/licenses/LICENSE-2.0\n+  *\n+  * Unless required by applicable law or agreed to in writing, software\n+  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  * See the License for the specific language governing permissions and\n+  * limitations under the License.\n+  */\n+\n+package kafka.server\n+\n+import java.util.concurrent.LinkedBlockingDeque\n+\n+/**\n+ * This queue is used to collect actions which need to be executed later. One use case is that ReplicaManager#appendRecords\n+ * produces record changes so we need to check and complete delayed requests. In order to avoid conflicting locking,\n+ * we add those actions to this queue and then complete them at the end of KafkaApis.handle() or DelayedJoin.onExpiration.\n+ */\n+class ActionQueue {\n+  private val queue = new LinkedBlockingDeque[() => Unit]()\n+\n+  /**\n+   * add action to this queue.\n+   * @param action action\n+   */\n+  def add(action: () => Unit): Unit = queue.put(action)\n+\n+  /**\n+   * try to complete all delayed actions\n+   */\n+  def tryCompleteActions(): Unit = {\n+    var action = queue.poll()\n+    while (action != null) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcwNjQwNw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479706407", "bodyText": "Since we are draining more than 1 item now, this comment is no longer accurate.", "author": "junrao", "createdAt": "2020-08-30T01:02:56Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -562,6 +564,10 @@ class ReplicaManager(val config: KafkaConfig,\n    * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas;\n    * the callback function will be triggered either when timeout or the required acks are satisfied;\n    * if the callback function itself is already synchronized on some object then pass this object to avoid deadlock.\n+   *\n+   * Noted that all pending delayed check operations are stored in a queue. All callers to ReplicaManager.appendRecords()\n+   * are expected to take up to 1 item from that queue and check the completeness for all affected partitions, without", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgwMjg2MA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479802860", "bodyText": "need => needs", "author": "junrao", "createdAt": "2020-08-30T18:47:31Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -85,6 +92,9 @@ object LogAppendInfo {\n  * @param validBytes The number of valid bytes\n  * @param offsetsMonotonic Are the offsets in this message set monotonically increasing\n  * @param lastOffsetOfFirstBatch The last offset of the first batch\n+ * @param leaderHWChange Incremental if the high watermark need to be increased after appending record.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgwMjkxMg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479802912", "bodyText": "is failed => failed", "author": "junrao", "createdAt": "2020-08-30T18:48:05Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -85,6 +92,9 @@ object LogAppendInfo {\n  * @param validBytes The number of valid bytes\n  * @param offsetsMonotonic Are the offsets in this message set monotonically increasing\n  * @param lastOffsetOfFirstBatch The last offset of the first batch\n+ * @param leaderHWChange Incremental if the high watermark need to be increased after appending record.\n+ *                       Same if high watermark is not changed. None is the default value and it means append is failed", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgwMzE2OA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479803168", "bodyText": "Probably Increased is clearer than Incremental.", "author": "junrao", "createdAt": "2020-08-30T18:50:50Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -68,6 +68,13 @@ object LogAppendInfo {\n       offsetsMonotonic = false, -1L, recordErrors, errorMessage)\n }\n \n+sealed trait LeaderHWChange\n+object LeaderHWChange {\n+  case object Incremental extends LeaderHWChange", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgyOTI3NQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r479829275", "bodyText": "Another nit: LeaderHwChange adheres to the coding convention better.", "author": "ijuma", "createdAt": "2020-08-30T23:26:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgwMzE2OA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r482743072", "bodyText": "@junrao This change avoids deadlock in TransactionCoordinatorConcurrencyTest.\nIf we update watchKeys before tryCompleteElseWatch, the other threads can take the same key to complete delayed request.  Hence the deadlock happens due to following conditions.\nthread_1  holds stateLock of TransactionStateManager to call appendRecords and it requires lock of delayed request to call tryCompleteElseWatch.\nthread_2 holds lock of delayed request to call onComplete (updateCacheCallback) and updateCacheCallback requires stateLock of TransactionStateManager.", "author": "chia7712", "createdAt": "2020-09-03T06:50:50Z", "path": "core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -201,8 +201,8 @@ object AbstractCoordinatorConcurrencyTest {\n         }\n       }\n       val producerRequestKeys = entriesPerPartition.keys.map(TopicPartitionOperationKey(_)).toSeq\n-      watchKeys ++= producerRequestKeys\n       producePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)\n+      watchKeys ++= producerRequestKeys", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc1MzUyOQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r482753529", "bodyText": "According to above case, there is a potential deadlock.\n    var watchCreated = false\n    for(key <- watchKeys) {\n      // If the operation is already completed, stop adding it to the rest of the watcher list.\n      if (operation.isCompleted)\n        return false\n      watchForOperation(key, operation)\n\n      if (!watchCreated) {\n        watchCreated = true\n        estimatedTotalOperations.incrementAndGet()\n      }\n    }\n\n    isCompletedByMe = operation.safeTryComplete()\n    if (isCompletedByMe)\n      return true\n\n\nsafeTryComplete() is executed after updating watchKey. Hence, it is possible that the lock of this request is held by another thread. The deadlock happens if this tryCompleteElseWatch is holding the lock required by another thread.\nIt seems to me the simple approach is to remove operation.safeTryComplete. That should be fine since we have called tryComplete before.", "author": "chia7712", "createdAt": "2020-09-03T07:06:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI3NjA3NQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r483276075", "bodyText": "@chia7712 :\n\n\nI think we still need operation.safeTryComplete in DelayedOperation.tryCompleteElseWatch(). The reason is that after the operation.tryComplete() call, but before we add the key to watch, the operation could have been completed by another thread. Since that thread doesn't see the registered key, it won't complete the request. If we don't call operation.safeTryComplete after adding the key for watch, we could have missed the only chance for completing this operation.\n\n\nI am not sure if there is a deadlock caused by TransactionStateManager. I don't see updateCacheCallback hold any lock on stateLock. The following locking sequence is possible through TransactionStateManager.\n\n\nthread 1 : hold readLock of stateLock, call ReplicaManager.appendRecords, call tryCompleteElseWatch, hold lock on delayedOperation\nthread 2: hold lock on delayedOperation, call delayedOperation.onComplete, call removeFromCacheCallback(), hold readLock of stateLock.\nHowever, since both threads hold readLock of stateLock, there shouldn't be a conflict.\nDo you see the test fail due to a deadlock?", "author": "junrao", "createdAt": "2020-09-03T21:58:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzU1MjI3NQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r483552275", "bodyText": "Do you see the test fail due to a deadlock?\n\nthe following read/write lock is fromstateLock of TransactionStateManager\n\nThread_1: holding readlock and waiting for lock of delayed op (TransactionStateManager#appendTransactionToLog)\nThread_2: waiting for writelock (TransactionCoordinatorConcurrencyTest#testConcurrentGoodPathWithConcurrentPartitionLoading)\nval t = new Thread() {\n  override def run(): Unit = {\n    while (keepRunning.get()) {\n      txnStateManager.addLoadingPartition(numPartitions + 1, coordinatorEpoch)\n    }\n  }\n}\nprivate[transaction] def addLoadingPartition(partitionId: Int, coordinatorEpoch: Int): Unit = {\n  val partitionAndLeaderEpoch = TransactionPartitionAndLeaderEpoch(partitionId, coordinatorEpoch)\n  inWriteLock(stateLock) {\n    loadingPartitions.add(partitionAndLeaderEpoch)\n  }\n}\n\n\nThread_3: holding lock of delayed op and waiting for readlock (another thread is trying to complete delayed op)\n\ndeadlock\n\nThread_1 is waiting for thread_3\nThread_3 is waiting for Thread_2\nThread_2 is waiting for thread_1", "author": "chia7712", "createdAt": "2020-09-04T11:15:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzgzMjk4MA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r483832980", "bodyText": "@chia7712 : Thanks for the explanation. stateLock is created as an unfair ReentrantReadWriteLock. So, in that case, will thread_3's attempt for getting the readLock blocked after thread_2? Did the test actually failed because of this?", "author": "junrao", "createdAt": "2020-09-04T20:33:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzg5NzI0NQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r483897245", "bodyText": "will thread_3's attempt for getting the readLock blocked after thread_2?\n\nTo the best of my knowledge, writers have preference over readers in order to avoid starvation. That behavior is not public and we can get some evidence from source code. for example:\n        final boolean readerShouldBlock() {\n            /* As a heuristic to avoid indefinite writer starvation,\n             * block if the thread that momentarily appears to be head\n             * of queue, if one exists, is a waiting writer.  This is\n             * only a probabilistic effect since a new reader will not\n             * block if there is a waiting writer behind other enabled\n             * readers that have not yet drained from the queue.\n             */\n            return apparentlyFirstQueuedIsExclusive();\n        }\nAt any rate, the non-fair mode does not guarantee above situation does not happen. Hence, it would be better to avoid potential deadlock caused by tryCompleteElseWatch.\n\nI think we still need operation.safeTryComplete in DelayedOperation.tryCompleteElseWatch()\n\nHow about using tryLock in tryCompleteElseWatch? It avoids conflicting locking and still check completion of delayed operations after adding watches?", "author": "chia7712", "createdAt": "2020-09-05T01:53:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzk2NjEyOA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r483966128", "bodyText": "@chia7712 : Thanks for the explanation. I agree that it's a potential problem.\nDoes using tryLock in tryCompleteElseWatch() lead us back to the previous issue that we could miss the opportunity to to complete an operation (fixed with KAFKA-6653)?\nAnother possibly is that we hold the lock in delayed operation while adding the operation to watch list and do the final safeTryComplete() check. This way, when the delayed operation is exposed to another thread, every thread, including the caller, always first acquires the lock in delayed operation. This should avoid all potential deadlocks between tryCompleteElseWatch() and checkAndComplete(). What do you think?", "author": "junrao", "createdAt": "2020-09-05T16:31:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDAzOTUyOA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484039528", "bodyText": "Another possibly is that we hold the lock in delayed operation while adding the operation to watch list and do the final safeTryComplete() check. This way, when the delayed operation is exposed to another thread, every thread, including the caller, always first acquires the lock in delayed operation. This should avoid all potential deadlocks between tryCompleteElseWatch() and checkAndComplete()\n\nnice idea. I have addressed this approach.", "author": "chia7712", "createdAt": "2020-09-06T08:22:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MjQ5Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484092496", "bodyText": "Do we still need this change to avoid deadlocks?", "author": "junrao", "createdAt": "2020-09-06T17:02:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5Nzk0OA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484097948", "bodyText": "Yep. if we add it to watch list too early, the concurrent issue happens as tryCompleteElseWatch calls tryComplete without locking.", "author": "chia7712", "createdAt": "2020-09-06T18:01:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDEzODY0OQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484138649", "bodyText": "@junrao This approach can not resolve all potential deadlock. For example:\n\nthread_a gets lock of op\nthread_a adds op to watch list\nthread_a calls op#tryComplete (and it requires lock_b)\nthread_b holds lock_b\nthread_b sees op from watch list\nthread_b needs lock of op\n\nHence, we are facing the following issues.\n\nthe last completion check can cause deadlock after the op is exposed to other threads (by watch list).\nthe last completion check can not be removed due to KAFKA-6653.\n\nHow about using ActionQueue to resolve it? We add completion check to ActionQueue after adding op to watch list. All handlers are able to complete it even if they don't have same key.", "author": "chia7712", "createdAt": "2020-09-07T00:59:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDE4MTkzMA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484181930", "bodyText": "@chia7712 : What you described is possible, but probably not an issue in practice. Since tryComplete() always completes a delayed operation asynchronously, there is no reason for the caller of a delayed operation to hold any lock while calling tryComplete. Therefore, in step 4 above, the first lock that thread_b (assuming it's well designed) can acquire should be the lock in delayed operation.", "author": "junrao", "createdAt": "2020-09-07T04:52:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDIwOTA2OA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484209068", "bodyText": "there is no reason for the caller of a delayed operation to hold any lock while calling tryComplete.\n\nYou are right. However, not sure how to maintain that \"well designed\" code in the future as the deadlock is implicit. It seems to me avoiding multiples exclusive lockings can avoid the deadlock.\nI will keep current approach since the story I described may be overkill in practice.", "author": "chia7712", "createdAt": "2020-09-07T06:32:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyNDU0NQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484524545", "bodyText": "Is this change still necessary now that we always call tryComplete() with lock in tryCompleteElseWatch?", "author": "junrao", "createdAt": "2020-09-07T17:04:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjc0MzA3Mg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MDI0NQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484090245", "bodyText": "requiring => requires", "author": "junrao", "createdAt": "2020-09-06T16:38:01Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -231,26 +214,27 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // At this point the only thread that can attempt this operation is this current thread\n     // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    if (operation.tryComplete()) return true\n+\n+    // There is a potential deadlock if we don't hold the lock while adding the operation to watch list and do the\n+    // final tryComplete() check. For example,\n+    // 1) thread_a holds lock_a\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds the op to watch list\n+    // 4) thread_b holds lock of op to complete op\n+    // 5) thread_b calls op's onComplete which requiring lock_a", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MDU2Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484090563", "bodyText": "It seems that we don't need the if here?", "author": "junrao", "createdAt": "2020-09-06T16:41:37Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -231,26 +214,27 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // At this point the only thread that can attempt this operation is this current thread\n     // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    if (operation.tryComplete()) return true\n+\n+    // There is a potential deadlock if we don't hold the lock while adding the operation to watch list and do the\n+    // final tryComplete() check. For example,\n+    // 1) thread_a holds lock_a\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds the op to watch list\n+    // 4) thread_b holds lock of op to complete op\n+    // 5) thread_b calls op's onComplete which requiring lock_a\n+    // 6) thread_a requires lock of op to call safeTryComplete\n+    if (inLock(operation.lock) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MDY5NA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484090694", "bodyText": "We should return if tryComplete() returns true.", "author": "junrao", "createdAt": "2020-09-06T16:43:11Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -231,26 +214,27 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // At this point the only thread that can attempt this operation is this current thread\n     // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    if (operation.tryComplete()) return true\n+\n+    // There is a potential deadlock if we don't hold the lock while adding the operation to watch list and do the\n+    // final tryComplete() check. For example,\n+    // 1) thread_a holds lock_a\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds the op to watch list\n+    // 4) thread_b holds lock of op to complete op\n+    // 5) thread_b calls op's onComplete which requiring lock_a\n+    // 6) thread_a requires lock of op to call safeTryComplete\n+    if (inLock(operation.lock) {\n+      var watchCreated = false\n+      watchKeys.foreach { key =>\n+        watchForOperation(key, operation)\n+        if (!watchCreated) {\n+          watchCreated = true\n+          estimatedTotalOperations.incrementAndGet()\n+        }\n       }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+      operation.tryComplete()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5NzE2MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484097161", "bodyText": "Pardon me, the latest commit does it. If the tryComplete return true, this method return true also.\nOr you mean we can do return in the lambda function directly? If so, the reason I don\u2019t address it is the impl of return in lambda is to throws and then catch exception.That is anti-pattern in scala and we should avoid it from our hot methods.", "author": "chia7712", "createdAt": "2020-09-06T17:52:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MDY5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDEwMTUzNQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484101535", "bodyText": "Yes, you are right. I missed the bracket alignment.", "author": "junrao", "createdAt": "2020-09-06T18:40:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MDY5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MTQ2Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484091466", "bodyText": "This is an existing issue. I am not sure if calling tryComplete() without holding the operation's lock guarantees visibility to another thread. For example, thread 1 changes the state in the operation in tryComplete(). It then calls tryComplete() holding the operations's lock but doesn't change the state in the operation. thread 2 calls tryComplete() holding the operations's lock. Is thread 2 guaranteed to see the changes made by thread 1 since the update was made without crossing the memory boundary by subsequent readers?\nIf this is an issue, we could extend to lock to do the first tryComplete() check.", "author": "junrao", "createdAt": "2020-09-06T16:51:33Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -231,26 +214,27 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // At this point the only thread that can attempt this operation is this current thread\n     // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    if (operation.tryComplete()) return true", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5OTcwOQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484099709", "bodyText": "It depends :)\nHowever, it should be fine to extend the lock to do it in this PR. Will address it in next commit", "author": "chia7712", "createdAt": "2020-09-06T18:21:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MTQ2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDE2Mzg2Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484163863", "bodyText": "As the potential deadlock caused by holding lock in tryCompleteElseWatch, I DON\"T change the tryComplete to safeTryComplete. Maybe we can deal with this issue when we meet such issue.", "author": "chia7712", "createdAt": "2020-09-07T03:20:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MTQ2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MjI1Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484092252", "bodyText": "This approach is fine but leaks operation.lock beyond tests. Another way to package this is to add a new method in DelayedOperation like tryCompleteAndMaybeWatch(). If that's not very clean, we can keep the current approach.", "author": "junrao", "createdAt": "2020-09-06T16:59:20Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -231,26 +214,27 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // At this point the only thread that can attempt this operation is this current thread\n     // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    if (operation.tryComplete()) return true\n+\n+    // There is a potential deadlock if we don't hold the lock while adding the operation to watch list and do the\n+    // final tryComplete() check. For example,\n+    // 1) thread_a holds lock_a\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds the op to watch list\n+    // 4) thread_b holds lock of op to complete op\n+    // 5) thread_b calls op's onComplete which requiring lock_a\n+    // 6) thread_a requires lock of op to call safeTryComplete\n+    if (inLock(operation.lock) {\n+      var watchCreated = false\n+      watchKeys.foreach { key =>\n+        watchForOperation(key, operation)\n+        if (!watchCreated) {\n+          watchCreated = true\n+          estimatedTotalOperations.incrementAndGet()\n+        }\n       }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+      operation.tryComplete()\n+    }) return true", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5NzYwMQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484097601", "bodyText": "There is an new test for this new behavior. In the DelayedOperationTest.", "author": "chia7712", "createdAt": "2020-09-06T17:57:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MjI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDEwMTc2OQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484101769", "bodyText": "Yes, it's just that if all non-testing usage of DelayedOperation.lock is within DelayedOperation itself, it makes it a bit easier to trace down all usage of lock.", "author": "junrao", "createdAt": "2020-09-06T18:43:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MjI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU0MzgzNQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484543835", "bodyText": "Will copy that", "author": "chia7712", "createdAt": "2020-09-07T18:44:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MjI1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5Mjk0NQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484092945", "bodyText": "With this change, DelayedOperations.checkAndCompleteFetch() is only used in tests. I am wondering if it can be removed. It's fine if we want to do this in a followup PR.\nUnrelated to this PR, DelayedOperations.checkAndCompleteProduce and DelayedOperations.checkAndCompleteDeleteRecords seem unused. We can probably remove them in a separate PR.", "author": "junrao", "createdAt": "2020-09-06T17:06:29Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1010,15 +1010,7 @@ class Partition(val topicPartition: TopicPartition,\n       }\n     }\n \n-    // some delayed operations may be unblocked after HW changed\n-    if (leaderHWIncremented)\n-      tryCompleteDelayedRequests()\n-    else {\n-      // probably unblock some follower fetch requests since log end offset has been updated\n-      delayedOperations.checkAndCompleteFetch()\n-    }\n-\n-    info\n+    info.copy(leaderHwChange = if (leaderHWIncremented) LeaderHwChange.Increased else LeaderHwChange.Same)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDE2NDYyNw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484164627", "bodyText": "With this change, DelayedOperations.checkAndCompleteFetch() is only used in tests. I am wondering if it can be removed. It's fine if we want to do this in a followup PR.\n\nI will file a ticket after this PR is merged.\n\nUnrelated to this PR, DelayedOperations.checkAndCompleteProduce and DelayedOperations.checkAndCompleteDeleteRecords seem unused. We can probably remove them in a separate PR.\n\nthis is small change. I will remove them in this PR", "author": "chia7712", "createdAt": "2020-09-07T03:24:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5Mjk0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxOTY1MQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484519651", "bodyText": "Since safeTryComplete() is no longer used in tryCompleteElseWatch(), the above comment is not completely relevant. Perhaps we could just explain what this method does \"Thread-safe variant of tryComplete().\"", "author": "junrao", "createdAt": "2020-09-07T16:45:35Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,41 +102,22 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n    *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * There is a long story about using \"lock\" or \"tryLock\".\n+   *\n+   * 1) using lock - There was a lot of cases that a thread holds a group lock and then it tries to hold more group\n+   * locks to complete delayed requests. Unfortunately, the scenario causes deadlock and so we had introduced the\n+   * \"tryLock\" to avoid deadlock.\n+   *\n+   * 2) using tryLock -  However, the \"tryLock\" causes another issue that the delayed requests may be into\n+   * oblivion if the thread, which should complete the delayed requests, fails to get the lock.\n+   *\n+   * Now, we go back to use \"lock\" and make sure the thread which tries to complete delayed requests does NOT hold lock.\n+   * The approach is that ReplicaManager collects all actions, which are used to complete delayed requests, in a queue.\n+   * KafkaApis.handle() and the expiration thread for certain delayed operations (e.g. DelayedJoin) pick up and then\n+   * execute delayed actions when no lock is held.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMTYyOA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484521628", "bodyText": "it requires lock_b => it tries to require lock_b", "author": "junrao", "createdAt": "2020-09-07T16:52:44Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +211,37 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    //\n+    // ==============[story about lock]==============\n+    // There is a potential deadlock in practice if we don't hold the lock while adding the operation to watch\n+    // list and do the final tryComplete() check. For example,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c holds lock of op (from watch list)\n+    // 6) thread_c is waiting readlock of stateLock to complete op (blocked by thread_b)\n+    // 7) thread_a is waiting lock of op to call safeTryComplete (blocked by thread_c)\n+    //\n+    // Noted that current approach can't prevent all deadlock. For example,\n+    // 1) thread_a gets lock of op\n+    // 2) thread_a adds op to watch list\n+    // 3) thread_a calls op#tryComplete (and it requires lock_b)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMjgxNg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484522816", "bodyText": "Perhaps we could change this comment to sth like the following.\nTo avoid the above scenario, we recommend DelayedOperationPurgatory.checkAndComplete() be called without holding any lock. Since DelayedOperationPurgatory.checkAndComplete() completes delayed operations asynchronously, holding a lock to make the call is often unnecessary.", "author": "junrao", "createdAt": "2020-09-07T16:57:32Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +211,37 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    //\n+    // ==============[story about lock]==============\n+    // There is a potential deadlock in practice if we don't hold the lock while adding the operation to watch\n+    // list and do the final tryComplete() check. For example,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c holds lock of op (from watch list)\n+    // 6) thread_c is waiting readlock of stateLock to complete op (blocked by thread_b)\n+    // 7) thread_a is waiting lock of op to call safeTryComplete (blocked by thread_c)\n+    //\n+    // Noted that current approach can't prevent all deadlock. For example,\n+    // 1) thread_a gets lock of op\n+    // 2) thread_a adds op to watch list\n+    // 3) thread_a calls op#tryComplete (and it requires lock_b)\n+    // 4) thread_b holds lock_b\n+    // 5) thread_b sees op from watch list\n+    // 6) thread_b needs lock of op\n+    // The above story produces a deadlock but it is not an issue in production yet since there is no reason for the", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMzI1Nw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484523257", "bodyText": "There is a potential deadlock => There is a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete()", "author": "junrao", "createdAt": "2020-09-07T16:59:24Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +211,37 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    //\n+    // ==============[story about lock]==============\n+    // There is a potential deadlock in practice if we don't hold the lock while adding the operation to watch", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMzUzNA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484523534", "bodyText": "thread_c holds lock of op => thread_c calls checkAndComplete () and holds lock of op", "author": "junrao", "createdAt": "2020-09-07T17:00:21Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +211,37 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n+    //\n+    // ==============[story about lock]==============\n+    // There is a potential deadlock in practice if we don't hold the lock while adding the operation to watch\n+    // list and do the final tryComplete() check. For example,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c holds lock of op (from watch list)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNTU3Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484605576", "bodyText": "safeTryCompleteAndElse => safeTryCompleteOrElse ?", "author": "junrao", "createdAt": "2020-09-08T01:37:51Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -100,42 +102,24 @@ abstract class DelayedOperation(override val delayMs: Long,\n   def tryComplete(): Boolean\n \n   /**\n-   * Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired\n-   * without blocking.\n-   *\n-   * If threadA acquires the lock and performs the check for completion before completion criteria is met\n-   * and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not\n-   * yet released the lock, we need to ensure that completion is attempted again without blocking threadA\n-   * or threadB. `tryCompletePending` is set by threadB when it fails to acquire the lock and at least one\n-   * of threadA or threadB will attempt completion of the operation if this flag is set. This ensures that\n-   * every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until\n-   * the operation is actually completed.\n+   * Thread-safe variant of tryComplete() and call extra function if first tryComplete returns false\n+   * @param f else function to be executed after first tryComplete returns false\n+   * @return result of tryComplete\n    */\n-  private[server] def maybeTryComplete(): Boolean = {\n-    var retry = false\n-    var done = false\n-    do {\n-      if (lock.tryLock()) {\n-        try {\n-          tryCompletePending.set(false)\n-          done = tryComplete()\n-        } finally {\n-          lock.unlock()\n-        }\n-        // While we were holding the lock, another thread may have invoked `maybeTryComplete` and set\n-        // `tryCompletePending`. In this case we should retry.\n-        retry = tryCompletePending.get()\n-      } else {\n-        // Another thread is holding the lock. If `tryCompletePending` is already set and this thread failed to\n-        // acquire the lock, then the thread that is holding the lock is guaranteed to see the flag and retry.\n-        // Otherwise, we should set the flag and retry on this thread since the thread holding the lock may have\n-        // released the lock and returned by the time the flag is set.\n-        retry = !tryCompletePending.getAndSet(true)\n-      }\n-    } while (!isCompleted && retry)\n-    done\n+  private[server] def safeTryCompleteAndElse(f: => Unit): Boolean = inLock(lock) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNjgxMQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484606811", "bodyText": "The above comment is a bit out of context now. Perhaps we could change \"we do the check in the following way\" to \"we do the check in the following way through safeTryCompleteAndElse()\".", "author": "junrao", "createdAt": "2020-09-08T01:43:41Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +212,33 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    //", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNzg4Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484607882", "bodyText": "Do we still need to change the ordering now that we always call tryComplete() with lock in tryCompleteElseWatch?", "author": "junrao", "createdAt": "2020-09-08T01:48:33Z", "path": "core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -202,9 +201,8 @@ object AbstractCoordinatorConcurrencyTest {\n         }\n       }\n       val producerRequestKeys = entriesPerPartition.keys.map(TopicPartitionOperationKey(_)).toSeq\n-      watchKeys ++= producerRequestKeys\n       producePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)\n-      tryCompleteDelayedRequests()\n+      watchKeys ++= producerRequestKeys", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYyMjI5MA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484622290", "bodyText": "I will revert this change", "author": "chia7712", "createdAt": "2020-09-08T02:52:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNzg4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwOTA3Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484609072", "bodyText": "checkAndComplete () => checkAndComplete()", "author": "junrao", "createdAt": "2020-09-08T01:53:44Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +212,33 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    //\n+    // ==============[story about lock]==============\n+    // There is a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete() in practice\n+    // if we don't hold the lock while adding the operation to watch\n+    // list and do the final tryComplete() check. For example,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c calls checkAndComplete () and holds lock of op", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYxMDg2Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484610866", "bodyText": "Perhaps change the above to the following?\nWe hold the operation's lock while adding the operation to watch list and doing the tryComplete() check. This is to avoid a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete(). For example, the following deadlock can happen if the lock is only held for the final tryComplete() check.", "author": "junrao", "createdAt": "2020-09-08T02:01:32Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -228,29 +212,33 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n     // if the operation is completed (by another thread) between the two tryComplete() calls, the\n     // operation is unnecessarily added for watch. However, this is a less severe issue since the\n     // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    //\n+    // ==============[story about lock]==============\n+    // There is a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete() in practice\n+    // if we don't hold the lock while adding the operation to watch\n+    // list and do the final tryComplete() check. For example,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYzODkzOA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484638938", "bodyText": "I think we still want to keep the rest of the paragraph starting from \"Call tryComplete().\".", "author": "junrao", "createdAt": "2020-09-08T04:08:51Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -221,36 +205,34 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // The cost of tryComplete() is typically proportional to the number of keys. Calling\n     // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // we do the check in the following way through safeTryCompleteOrElse().", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYzOTIzNg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484639236", "bodyText": "We hold => Through safeTryCompleteOrElse(), we hold", "author": "junrao", "createdAt": "2020-09-08T04:10:06Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -221,36 +205,34 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // The cost of tryComplete() is typically proportional to the number of keys. Calling\n     // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // we do the check in the following way through safeTryCompleteOrElse().\n+    //\n+    // ==============[story about lock]==============\n+    // We hold the operation's lock while adding the operation to watch list and doing the tryComplete() check. This is", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY0MDE2OA==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484640168", "bodyText": "thread_b holds lock_b => thread_b holds lock_b and calls checkAndComplete()", "author": "junrao", "createdAt": "2020-09-08T04:14:27Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -221,36 +205,34 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // The cost of tryComplete() is typically proportional to the number of keys. Calling\n     // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // we do the check in the following way through safeTryCompleteOrElse().\n+    //\n+    // ==============[story about lock]==============\n+    // We hold the operation's lock while adding the operation to watch list and doing the tryComplete() check. This is\n+    // to avoid a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete(). For example,\n+    // the following deadlock can happen if the lock is only held for the final tryComplete() check,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c calls checkAndComplete() and holds lock of op\n+    // 6) thread_c is waiting readlock of stateLock to complete op (blocked by thread_b)\n+    // 7) thread_a is waiting lock of op to call safeTryComplete (blocked by thread_c)\n+    //\n+    // Noted that current approach can't prevent all deadlock. For example,\n+    // 1) thread_a gets lock of op\n+    // 2) thread_a adds op to watch list\n+    // 3) thread_a calls op#tryComplete (and it tries to require lock_b)\n+    // 4) thread_b holds lock_b", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY0MDg2Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484640862", "bodyText": "Noted that current approach can't prevent all deadlock. => Note that even with the current approach, deadlocks could still be introduced.", "author": "junrao", "createdAt": "2020-09-08T04:17:41Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -221,36 +205,34 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // The cost of tryComplete() is typically proportional to the number of keys. Calling\n     // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // we do the check in the following way through safeTryCompleteOrElse().\n+    //\n+    // ==============[story about lock]==============\n+    // We hold the operation's lock while adding the operation to watch list and doing the tryComplete() check. This is\n+    // to avoid a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete(). For example,\n+    // the following deadlock can happen if the lock is only held for the final tryComplete() check,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c calls checkAndComplete() and holds lock of op\n+    // 6) thread_c is waiting readlock of stateLock to complete op (blocked by thread_b)\n+    // 7) thread_a is waiting lock of op to call safeTryComplete (blocked by thread_c)\n+    //\n+    // Noted that current approach can't prevent all deadlock. For example,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY0NDgwMg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484644802", "bodyText": "Instead of introducing a global var, could we add a new param when constructing CommitTxnOffsetsOperation and CompleteTxnOperation?", "author": "junrao", "createdAt": "2020-09-08T04:35:49Z", "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -112,6 +113,13 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n     }.toSet\n   }\n \n+  /**\n+   * handleTxnCommitOffsets does not complete delayed requests now so it causes error if handleTxnCompletion is executed\n+   * before completing delayed request. In random mode, we use this global lock to prevent such error.\n+   */\n+  private var isRandomOrder = false", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY0NTg0OQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r484645849", "bodyText": "thread_a gets lock of op => thread_a calls tryCompleteElseWatch() and gets lock of op", "author": "junrao", "createdAt": "2020-09-08T04:40:17Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -221,36 +205,34 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n \n     // The cost of tryComplete() is typically proportional to the number of keys. Calling\n     // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // we do the check in the following way through safeTryCompleteOrElse().\n+    //\n+    // ==============[story about lock]==============\n+    // We hold the operation's lock while adding the operation to watch list and doing the tryComplete() check. This is\n+    // to avoid a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete(). For example,\n+    // the following deadlock can happen if the lock is only held for the final tryComplete() check,\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c calls checkAndComplete() and holds lock of op\n+    // 6) thread_c is waiting readlock of stateLock to complete op (blocked by thread_b)\n+    // 7) thread_a is waiting lock of op to call safeTryComplete (blocked by thread_c)\n+    //\n+    // Noted that current approach can't prevent all deadlock. For example,\n+    // 1) thread_a gets lock of op", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAyNjU4Mw==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r485026583", "bodyText": "to call safeTryComplete => to call the final tryComplete()", "author": "junrao", "createdAt": "2020-09-08T15:52:56Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -219,38 +203,38 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n   def tryCompleteElseWatch(operation: T, watchKeys: Seq[Any]): Boolean = {\n     assert(watchKeys.nonEmpty, \"The watch key list can't be empty\")\n \n-    // The cost of tryComplete() is typically proportional to the number of keys. Calling\n-    // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // The cost of tryComplete() is typically proportional to the number of keys. Calling tryComplete() for each key is\n+    // going to be expensive if there are many keys. Instead, we do the check in the following way through safeTryCompleteOrElse().\n+    // If the operation is not completed, we just add the operation to all keys. Then we call tryComplete() again. At\n+    // this time, if the operation is still not completed, we are guaranteed that it won't miss any future triggering\n+    // event since the operation is already on the watcher list for all keys.\n+    //\n+    // ==============[story about lock]==============\n+    // Through safeTryCompleteOrElse(), we hold the operation's lock while adding the operation to watch list and doing\n+    // the tryComplete() check. This is to avoid a potential deadlock between the callers to tryCompleteElseWatch() and\n+    // checkAndComplete(). For example, the following deadlock can happen if the lock is only held for the final tryComplete()\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch\n+    // 3) thread_a adds op to watch list\n+    // 4) thread_b requires writelock of stateLock from TransactionStateManager (blocked by thread_a)\n+    // 5) thread_c calls checkAndComplete() and holds lock of op\n+    // 6) thread_c is waiting readlock of stateLock to complete op (blocked by thread_b)\n+    // 7) thread_a is waiting lock of op to call safeTryComplete (blocked by thread_c)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAyNjc5OQ==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r485026799", "bodyText": "tryCompleteElseWatch => tryCompleteElseWatch()", "author": "junrao", "createdAt": "2020-09-08T15:53:16Z", "path": "core/src/main/scala/kafka/server/DelayedOperation.scala", "diffHunk": "@@ -219,38 +203,38 @@ final class DelayedOperationPurgatory[T <: DelayedOperation](purgatoryName: Stri\n   def tryCompleteElseWatch(operation: T, watchKeys: Seq[Any]): Boolean = {\n     assert(watchKeys.nonEmpty, \"The watch key list can't be empty\")\n \n-    // The cost of tryComplete() is typically proportional to the number of keys. Calling\n-    // tryComplete() for each key is going to be expensive if there are many keys. Instead,\n-    // we do the check in the following way. Call tryComplete(). If the operation is not completed,\n-    // we just add the operation to all keys. Then we call tryComplete() again. At this time, if\n-    // the operation is still not completed, we are guaranteed that it won't miss any future triggering\n-    // event since the operation is already on the watcher list for all keys. This does mean that\n-    // if the operation is completed (by another thread) between the two tryComplete() calls, the\n-    // operation is unnecessarily added for watch. However, this is a less severe issue since the\n-    // expire reaper will clean it up periodically.\n-\n-    // At this point the only thread that can attempt this operation is this current thread\n-    // Hence it is safe to tryComplete() without a lock\n-    var isCompletedByMe = operation.tryComplete()\n-    if (isCompletedByMe)\n-      return true\n-\n-    var watchCreated = false\n-    for(key <- watchKeys) {\n-      // If the operation is already completed, stop adding it to the rest of the watcher list.\n-      if (operation.isCompleted)\n-        return false\n-      watchForOperation(key, operation)\n-\n-      if (!watchCreated) {\n-        watchCreated = true\n-        estimatedTotalOperations.incrementAndGet()\n-      }\n-    }\n-\n-    isCompletedByMe = operation.maybeTryComplete()\n-    if (isCompletedByMe)\n-      return true\n+    // The cost of tryComplete() is typically proportional to the number of keys. Calling tryComplete() for each key is\n+    // going to be expensive if there are many keys. Instead, we do the check in the following way through safeTryCompleteOrElse().\n+    // If the operation is not completed, we just add the operation to all keys. Then we call tryComplete() again. At\n+    // this time, if the operation is still not completed, we are guaranteed that it won't miss any future triggering\n+    // event since the operation is already on the watcher list for all keys.\n+    //\n+    // ==============[story about lock]==============\n+    // Through safeTryCompleteOrElse(), we hold the operation's lock while adding the operation to watch list and doing\n+    // the tryComplete() check. This is to avoid a potential deadlock between the callers to tryCompleteElseWatch() and\n+    // checkAndComplete(). For example, the following deadlock can happen if the lock is only held for the final tryComplete()\n+    // 1) thread_a holds readlock of stateLock from TransactionStateManager\n+    // 2) thread_a is executing tryCompleteElseWatch", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNDI2Mg==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r485034262", "bodyText": "causes error => causes an error", "author": "junrao", "createdAt": "2020-09-08T16:04:21Z", "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -119,12 +110,33 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n \n   @Test\n   def testConcurrentTxnGoodPathSequence(): Unit = {\n-    verifyConcurrentOperations(createGroupMembers, allOperationsWithTxn)\n+    verifyConcurrentOperations(createGroupMembers, Seq(\n+      new JoinGroupOperation,\n+      new SyncGroupOperation,\n+      new OffsetFetchOperation,\n+      new CommitTxnOffsetsOperation,\n+      new CompleteTxnOperation,\n+      new HeartbeatOperation,\n+      new LeaveGroupOperation\n+    ))\n   }\n \n   @Test\n   def testConcurrentRandomSequence(): Unit = {\n-    verifyConcurrentRandomSequences(createGroupMembers, allOperationsWithTxn)\n+    /**\n+     * handleTxnCommitOffsets does not complete delayed requests now so it causes error if handleTxnCompletion is executed", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNDI5Ng==", "url": "https://github.com/apache/kafka/pull/8657#discussion_r485034296", "bodyText": "such error => such an error", "author": "junrao", "createdAt": "2020-09-08T16:04:23Z", "path": "core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala", "diffHunk": "@@ -119,12 +110,33 @@ class GroupCoordinatorConcurrencyTest extends AbstractCoordinatorConcurrencyTest\n \n   @Test\n   def testConcurrentTxnGoodPathSequence(): Unit = {\n-    verifyConcurrentOperations(createGroupMembers, allOperationsWithTxn)\n+    verifyConcurrentOperations(createGroupMembers, Seq(\n+      new JoinGroupOperation,\n+      new SyncGroupOperation,\n+      new OffsetFetchOperation,\n+      new CommitTxnOffsetsOperation,\n+      new CompleteTxnOperation,\n+      new HeartbeatOperation,\n+      new LeaveGroupOperation\n+    ))\n   }\n \n   @Test\n   def testConcurrentRandomSequence(): Unit = {\n-    verifyConcurrentRandomSequences(createGroupMembers, allOperationsWithTxn)\n+    /**\n+     * handleTxnCommitOffsets does not complete delayed requests now so it causes error if handleTxnCompletion is executed\n+     * before completing delayed request. In random mode, we use this global lock to prevent such error.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c086bd2e8b9fecb10df5ab74837c5ef6c0ac29ef", "url": "https://github.com/apache/kafka/commit/c086bd2e8b9fecb10df5ab74837c5ef6c0ac29ef", "message": "KAFKA-8334 Make sure the thread which tries to complete delayed requests does NOT hold any group lock", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "c8f54add2b01625d3d990e44538c5e92c7c12d0e", "url": "https://github.com/apache/kafka/commit/c8f54add2b01625d3d990e44538c5e92c7c12d0e", "message": "address review comment", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "dfb9b4b6ca5633951fddc3ba708b5d0e8066c040", "url": "https://github.com/apache/kafka/commit/dfb9b4b6ca5633951fddc3ba708b5d0e8066c040", "message": "add more comment", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "bb7a9e53aa7d66585c71bb190197ba51e5406943", "url": "https://github.com/apache/kafka/commit/bb7a9e53aa7d66585c71bb190197ba51e5406943", "message": "address review comments", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "3e89824c711064acdf5e72954906c4090b7488eb", "url": "https://github.com/apache/kafka/commit/3e89824c711064acdf5e72954906c4090b7488eb", "message": "address review comments", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "9c981c56cb189f45741da7911ff2a77b9cfca97a", "url": "https://github.com/apache/kafka/commit/9c981c56cb189f45741da7911ff2a77b9cfca97a", "message": "revert leaderHWIncremented to option", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "eba6df82ea1480c91107c9fee45c290a53462855", "url": "https://github.com/apache/kafka/commit/eba6df82ea1480c91107c9fee45c290a53462855", "message": "introduce leaderHWChange and consume all delayed actions", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "4a9e49fc8a2213cb57a782f4a15bae352a4c7d43", "url": "https://github.com/apache/kafka/commit/4a9e49fc8a2213cb57a782f4a15bae352a4c7d43", "message": "revise comment", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "c42f46426f4cdb2ed441d7bf99fbc887861dff93", "url": "https://github.com/apache/kafka/commit/c42f46426f4cdb2ed441d7bf99fbc887861dff93", "message": "revert action queue in per server", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "e1a6044b54fa50541955761c3e83e8247eb70026", "url": "https://github.com/apache/kafka/commit/e1a6044b54fa50541955761c3e83e8247eb70026", "message": "Incremental -> Increased; LeaderHWChange -> LeaderHwChange; and other grammar fix", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "5da2fab1f02ebf3965a6385a20cf078ba692e541", "url": "https://github.com/apache/kafka/commit/5da2fab1f02ebf3965a6385a20cf078ba692e541", "message": "fix deadlock in TransactionCoordinatorConcurrencyTest", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "4e66db09b6a8fb2f93fd097fbbd327cab98b5107", "url": "https://github.com/apache/kafka/commit/4e66db09b6a8fb2f93fd097fbbd327cab98b5107", "message": "fix potential deadlock in tryCompleteElseWatch", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "b3cd7add61a91949611c39ceb7d37ada41614cf8", "url": "https://github.com/apache/kafka/commit/b3cd7add61a91949611c39ceb7d37ada41614cf8", "message": "a bit tweak", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "50032c16eda4c5845dea94b7076a4b6f432d9550", "url": "https://github.com/apache/kafka/commit/50032c16eda4c5845dea94b7076a4b6f432d9550", "message": "remove unused methods from DelayedOperations; add safeTryCompleteAndElse to DelayedOperation", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "28b68545183b13f39861ee45812042f5464777b2", "url": "https://github.com/apache/kafka/commit/28b68545183b13f39861ee45812042f5464777b2", "message": "rename safeTryCompleteOrElse to safeTryCompleteAndElse; revise comment", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "9a49f046f96fe082de7bddb539dd68589e8853db", "url": "https://github.com/apache/kafka/commit/9a49f046f96fe082de7bddb539dd68589e8853db", "message": "remove global variable from GroupCoordinatorConcurrencyTest; revise comment", "committedDate": "2020-09-08T16:10:57Z", "type": "commit"}, {"oid": "fbd46565aa5e03f4fd9c857a184b7c2371ca5932", "url": "https://github.com/apache/kafka/commit/fbd46565aa5e03f4fd9c857a184b7c2371ca5932", "message": "tweak comment", "committedDate": "2020-09-08T16:13:09Z", "type": "commit"}, {"oid": "fbd46565aa5e03f4fd9c857a184b7c2371ca5932", "url": "https://github.com/apache/kafka/commit/fbd46565aa5e03f4fd9c857a184b7c2371ca5932", "message": "tweak comment", "committedDate": "2020-09-08T16:13:09Z", "type": "forcePushed"}]}