{"pr_number": 8040, "pr_title": "KAFKA-6607: Commit correct offsets for transactional input data", "pr_createdAt": "2020-02-05T01:16:19Z", "pr_url": "https://github.com/apache/kafka/pull/8040", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxMzI1MA==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375013250", "bodyText": "Some additional side cleanup", "author": "mjsax", "createdAt": "2020-02-05T01:17:59Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java", "diffHunk": "@@ -58,15 +58,14 @@\n     private int totalBuffered;\n     private boolean allBuffered;\n \n-\n-    public static class RecordInfo {\n+    static class RecordInfo {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxMzM5NQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375013395", "bodyText": "This new method is add for the fix", "author": "mjsax", "createdAt": "2020-02-05T01:18:28Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java", "diffHunk": "@@ -172,25 +178,35 @@ int addRawRecords(final TopicPartition partition, final Iterable<ConsumerRecord<\n         return newSize;\n     }\n \n-    public Set<TopicPartition> partitions() {\n+    Set<TopicPartition> partitions() {\n         return Collections.unmodifiableSet(partitionQueues.keySet());\n     }\n \n     /**\n      * Return the stream-time of this partition group defined as the largest timestamp seen across all partitions\n      */\n-    public long streamTime() {\n+    long streamTime() {\n         return streamTime;\n     }\n \n+    Long headRecordOffset(final TopicPartition partition) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxMzYzOQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375013639", "bodyText": "To reuse this condition, we move it to out test utils class", "author": "mjsax", "createdAt": "2020-02-05T01:19:10Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/AbstractResetIntegrationTest.java", "diffHunk": "@@ -170,25 +168,11 @@ private void prepareConfigs() {\n     private static final long CLEANUP_CONSUMER_TIMEOUT = 2000L;\n     private static final int TIMEOUT_MULTIPLIER = 15;\n \n-    private class ConsumerGroupInactiveCondition implements TestCondition {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxMzcyNA==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375013724", "bodyText": "new method create in test utils class to make is reusable", "author": "mjsax", "createdAt": "2020-02-05T01:19:31Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/AbstractResetIntegrationTest.java", "diffHunk": "@@ -170,25 +168,11 @@ private void prepareConfigs() {\n     private static final long CLEANUP_CONSUMER_TIMEOUT = 2000L;\n     private static final int TIMEOUT_MULTIPLIER = 15;\n \n-    private class ConsumerGroupInactiveCondition implements TestCondition {\n-        @Override\n-        public boolean conditionMet() {\n-            try {\n-                final ConsumerGroupDescription groupDescription = adminClient.describeConsumerGroups(Collections.singletonList(appID)).describedGroups().get(appID).get();\n-                return groupDescription.members().isEmpty();\n-            } catch (final ExecutionException | InterruptedException e) {\n-                return false;\n-            }\n-        }\n-    }\n-\n     void prepareTest() throws Exception {\n         prepareConfigs();\n         prepareEnvironment();\n \n-        // busy wait until cluster (ie, ConsumerGroupCoordinator) is available\n-        TestUtils.waitForCondition(new ConsumerGroupInactiveCondition(), TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT,\n-                \"Test consumer group \" + appID + \" still active even after waiting \" + (TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT) + \" ms.\");\n+        waitForEmptyConsumerGroup(adminClient, appID, TIMEOUT_MULTIPLIER * CLEANUP_CONSUMER_TIMEOUT);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNDE5NA==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375014194", "bodyText": "Need to set retries no a not-zero value for transactions...", "author": "mjsax", "createdAt": "2020-02-05T01:21:10Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -176,12 +214,20 @@ private void runSimpleCopyTest(final int numberOfRestarts,\n                 startKafkaStreamsAndWaitForRunningState(streams, MAX_WAIT_TIME_MS);\n \n                 final List<KeyValue<Long, Long>> inputData = prepareData(i * 100, i * 100 + 10L, 0L, 1L);\n+                System.out.println(\"mjsax: \" + inputData.size());\n+\n+                final Properties producerConfigs = new Properties();\n+                if (inputTopicTransactional) {\n+                    producerConfigs.setProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-input-producer\");\n+                    producerConfigs.setProperty(ProducerConfig.RETRIES_CONFIG, \"\" + Integer.MAX_VALUE);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU4MTA5NQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375581095", "bodyText": "The default value for retries are Integer.MAX_VALUE anyways right?", "author": "guozhangwang", "createdAt": "2020-02-06T00:19:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNDE5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU4NTQxMA==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375585410", "bodyText": "Yes, but we set it to zero within TestUtils.producerConfig() (not sure why) -- should we remove it there instead?", "author": "mjsax", "createdAt": "2020-02-06T00:35:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNDE5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU4OTEyMA==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375589120", "bodyText": "Hmm.. I am not sure either but we can check if removing that breaks any other tests --- with the new deliver timeout value we should no longer rely on that config values, --- if we want, we should change the deliver.timeout not this one.\nI guess it's up to you :) if it is too much we can just keep it as is.", "author": "guozhangwang", "createdAt": "2020-02-06T00:49:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNDE5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU5NTYxMg==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375595612", "bodyText": "I removed the retries overwrite and left the delivery timeout default. Locally all integration tests passed. Hence, I hope it's fine that way.", "author": "mjsax", "createdAt": "2020-02-06T01:13:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNDE5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNDY5OQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375014699", "bodyText": "The original method was too long and checkstyle failed -- we could also add a checkstyle exception... This was just a quick fix -- let me know what you think", "author": "mjsax", "createdAt": "2020-02-05T01:23:08Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java", "diffHunk": "@@ -88,6 +91,14 @@ private static Sensor getValueSensor(final Metrics metrics, final MetricName met\n \n     @Test\n     public void testTimeTracking() {\n+        testFirstBatch();\n+        testSecondBatch();\n+    }", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU4MTU1Nw==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375581557", "bodyText": "Sounds good to me -- better not add more checkstyle exceptions :)", "author": "guozhangwang", "createdAt": "2020-02-06T00:20:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNDY5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNDc2OQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375014769", "bodyText": "add verification for the \"head record offset\"", "author": "mjsax", "createdAt": "2020-02-05T01:23:25Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java", "diffHunk": "@@ -109,12 +120,13 @@ public void testTimeTracking() {\n         // st: -1 since no records was being processed yet\n \n         verifyBuffered(6, 3, 3);\n+        assertEquals(1L, group.partitionTimestamp(partition1));\n+        assertEquals(2L, group.partitionTimestamp(partition2));\n+        assertEquals(1L, group.headRecordOffset(partition1).longValue());\n+        assertEquals(2L, group.headRecordOffset(partition2).longValue());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNDk0Nw==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375014947", "bodyText": "Improve some existing tests, and add couple of more that are missing.", "author": "mjsax", "createdAt": "2020-02-05T01:24:04Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java", "diffHunk": "@@ -305,16 +340,79 @@ public void shouldSetPartitionTimestampAndStreamTime() {\n     }\n \n     @Test\n-    public void shouldThrowNullpointerUponSetPartitionTimestampFailure() {\n-        assertThrows(errMessage, NullPointerException.class, () -> {\n-            group.setPartitionTime(randomPartition, 0L);\n-        });\n+    public void shouldThrowIllegalStateExceptionUponAddRecordsIfPartitionUnknown() {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNTAyNQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375015025", "bodyText": "Side cleanup", "author": "mjsax", "createdAt": "2020-02-05T01:24:20Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java", "diffHunk": "@@ -47,14 +48,18 @@\n import java.util.Collections;\n import java.util.List;\n \n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.MatcherAssert.assertThat;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertThrows;\n import static org.junit.Assert.assertTrue;\n \n public class RecordQueueTest {\n     private final Serializer<Integer> intSerializer = new IntegerSerializer();\n     private final Deserializer<Integer> intDeserializer = new IntegerDeserializer();\n     private final TimestampExtractor timestampExtractor = new MockTimestampExtractor();\n-    private final String[] topics = {\"topic\"};", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNTA4MQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375015081", "bodyText": "add \"head record offset\" verification", "author": "mjsax", "createdAt": "2020-02-05T01:24:34Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java", "diffHunk": "@@ -98,10 +103,10 @@ public void after() {\n \n     @Test\n     public void testTimeTracking() {\n-\n         assertTrue(queue.isEmpty());\n         assertEquals(0, queue.size());\n         assertEquals(RecordQueue.UNKNOWN, queue.headRecordTimestamp());\n+        assertNull(queue.headRecordOffset());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNTE0OQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375015149", "bodyText": "just some test improvments", "author": "mjsax", "createdAt": "2020-02-05T01:24:49Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java", "diffHunk": "@@ -245,13 +262,17 @@ public void shouldThrowStreamsExceptionWhenKeyDeserializationFails() {\n         queue.addRawRecords(records);\n     }\n \n-    @Test(expected = StreamsException.class)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNTM5MQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375015391", "bodyText": "Because we call consumer.position()  now, we need to fix the mock consumer setup in the TTD", "author": "mjsax", "createdAt": "2020-02-05T01:25:46Z", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -333,22 +333,28 @@ public void onRestoreEnd(final TopicPartition topicPartition, final String store\n             offsetsByTopicPartition.put(tp, new AtomicLong());\n         }\n         consumer.assign(partitionsByTopic.values());\n+        final Map<TopicPartition, Long> startOffsets = new HashMap<>();\n+        for (final TopicPartition topicPartition : partitionsByTopic.values()) {\n+            startOffsets.put(topicPartition, 0L);\n+        }\n+        consumer.updateBeginningOffsets(startOffsets);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNTYwNw==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375015607", "bodyText": "We cannot share the consumer any longer, because the global task calls unassign() that nukes our setup from above.", "author": "mjsax", "createdAt": "2020-02-05T01:26:37Z", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -333,22 +333,28 @@ public void onRestoreEnd(final TopicPartition topicPartition, final String store\n             offsetsByTopicPartition.put(tp, new AtomicLong());\n         }\n         consumer.assign(partitionsByTopic.values());\n+        final Map<TopicPartition, Long> startOffsets = new HashMap<>();\n+        for (final TopicPartition topicPartition : partitionsByTopic.values()) {\n+            startOffsets.put(topicPartition, 0L);\n+        }\n+        consumer.updateBeginningOffsets(startOffsets);\n \n         if (globalTopology != null) {\n+            final MockConsumer<byte[], byte[]> globalConsumer = new MockConsumer<>(OffsetResetStrategy.NONE);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU4MzM2MQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375583361", "bodyText": "Not clear why? In MockConsumer we only do the following:\npublic synchronized void unsubscribe() {\n        ensureNotClosed();\n        committed.clear();\n        subscriptions.unsubscribe();\n    }\n\nAnd the beginningOffsets map are not nuked.", "author": "guozhangwang", "createdAt": "2020-02-06T00:27:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNTYwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU4Nzc4Mw==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375587783", "bodyText": "The problem is that the subscription is nuked and when we call position() the MockConsumer checks if the passed in partition is in its subscription and fails before it tries to access the beginningOffsets map.", "author": "mjsax", "createdAt": "2020-02-06T00:44:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNTYwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU4OTg2Nw==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375589867", "bodyText": "Ah okay, got it.", "author": "guozhangwang", "createdAt": "2020-02-06T00:52:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNTYwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAxNTc2MA==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375015760", "bodyText": "This was actually detected by the improved tests... Minor side fix.", "author": "mjsax", "createdAt": "2020-02-05T01:27:15Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java", "diffHunk": "@@ -204,14 +220,15 @@ boolean allPartitionsBuffered() {\n         return allBuffered;\n     }\n \n-    public void close() {\n+    void close() {\n         clear();\n         partitionQueues.clear();\n     }\n \n-    public void clear() {\n+    void clear() {\n         nonEmptyQueuesByTime.clear();\n         streamTime = RecordQueue.UNKNOWN;\n+        totalBuffered = 0;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU3OTkzNA==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375579934", "bodyText": "We need to consider handling two exceptions that consumer.position may throw: KafkaException -> should be a fatal one; TimeoutException -> in this case we cannot commit, probably have to treat as fatal..", "author": "guozhangwang", "createdAt": "2020-02-06T00:14:47Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -496,9 +496,15 @@ void commit(final boolean startNewTransaction, final Map<TopicPartition, Long> p\n         }\n \n         final Map<TopicPartition, OffsetAndMetadata> consumedOffsetsAndMetadata = new HashMap<>(consumedOffsets.size());\n+\n         for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {\n             final TopicPartition partition = entry.getKey();\n-            final long offset = entry.getValue() + 1;\n+            Long offset = partitionGroup.headRecordOffset(partition);\n+            if (offset == null) {\n+                // this call should never block, because we know that we did process data for this partition\n+                // and thus the consumer should have a valid local position that it can return immediately\n+                offset = consumer.position(partition);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU4ODU5MQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375588591", "bodyText": "Ack. We can wrap KafkaException as StreamsException. A TimeoutException should never happen (compare my comment -- let me know if you think the comment is incorrect) -- hence, we can rethrow TimeoutException as IllegalStateException to flag potential bugs.", "author": "mjsax", "createdAt": "2020-02-06T00:47:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU3OTkzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTU4MTI0Nw==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r375581247", "bodyText": "nit: unkonwn-partition.", "author": "guozhangwang", "createdAt": "2020-02-06T00:19:44Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java", "diffHunk": "@@ -39,16 +39,19 @@\n \n import static org.apache.kafka.common.utils.Utils.mkEntry;\n import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.core.IsEqual.equalTo;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertThrows;\n \n public class PartitionGroupTest {\n     private final LogContext logContext = new LogContext();\n     private final Serializer<Integer> intSerializer = new IntegerSerializer();\n     private final Deserializer<Integer> intDeserializer = new IntegerDeserializer();\n     private final TimestampExtractor timestampExtractor = new MockTimestampExtractor();\n-    private final TopicPartition randomPartition = new TopicPartition(\"random-partition\", 0);\n-    private final String errMessage = \"Partition \" + randomPartition + \" not found.\";\n+    private final TopicPartition unknownPartition = new TopicPartition(\"unknonw-partition\", 0);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MzEyOQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r376683129", "bodyText": "Even if records might not be empty, we need to filter out the dummy records we added to indicate tx-markers", "author": "mjsax", "createdAt": "2020-02-08T03:21:23Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -636,6 +636,8 @@ private ListOffsetResult fetchOffsetsByTimes(Map<TopicPartition, Long> timestamp\n                     List<ConsumerRecord<K, V>> records = fetchRecords(nextInLineFetch, recordsRemaining);\n \n                     if (!records.isEmpty()) {\n+                        records = records.stream().filter(r -> !(r instanceof NoConsumerRecord)).collect(Collectors.toList());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MzE2NA==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r376683164", "bodyText": "We add a dummy record is we stepped over an tx-marker", "author": "mjsax", "createdAt": "2020-02-08T03:21:57Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -1539,10 +1543,14 @@ private Record nextFetchedRecord() {\n                     }\n                     if (lastRecord == null)\n                         break;\n-                    records.add(parseRecord(partition, currentBatch, lastRecord));\n-                    recordsRead++;\n-                    bytesRead += lastRecord.sizeInBytes();\n-                    nextFetchOffset = lastRecord.offset() + 1;\n+                    if (lastRecord instanceof NoRecord) {\n+                        records.add(new NoConsumerRecord<>());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MzM1OQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r376683359", "bodyText": "Instead of nothing, we return an empty list if we step over a tx-marker. After a second fetchRecords() we return nothing.", "author": "mjsax", "createdAt": "2020-02-08T03:25:37Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2602,6 +2602,9 @@ public void testSkippingAbortedTransactions() {\n         assertTrue(fetcher.hasCompletedFetches());\n \n         Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();\n+        assertTrue(fetchedRecords.get(tp0).isEmpty());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MzM5NA==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r376683394", "bodyText": "We know pass in the current consumerPosition to track it correctly.", "author": "mjsax", "createdAt": "2020-02-08T03:26:02Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java", "diffHunk": "@@ -149,11 +151,17 @@ StampedRecord nextRecord(final RecordInfo info) {\n      * @param rawRecords  the raw records\n      * @return the queue size for the partition\n      */\n-    int addRawRecords(final TopicPartition partition, final Iterable<ConsumerRecord<byte[], byte[]>> rawRecords) {\n+    int addRawRecords(final TopicPartition partition,\n+                      final Iterable<ConsumerRecord<byte[], byte[]>> rawRecords,\n+                      final long consumerPosition) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MzQxNw==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r376683417", "bodyText": "We track the consumer position expliclity now", "author": "mjsax", "createdAt": "2020-02-08T03:26:29Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java", "diffHunk": "@@ -48,6 +48,7 @@\n \n     private StampedRecord headRecord = null;\n     private long partitionTime = RecordQueue.UNKNOWN;\n+    private long consumerPosition = -1L;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MzQ1NA==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r376683454", "bodyText": "This could happen if the buffer is empty and if the consumer only stepped over a commit marker passing in empty list of records.", "author": "mjsax", "createdAt": "2020-02-08T03:27:08Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java", "diffHunk": "@@ -104,10 +105,27 @@ public TopicPartition partition() {\n      * @param rawRecords the raw records\n      * @return the size of this queue\n      */\n-    int addRawRecords(final Iterable<ConsumerRecord<byte[], byte[]>> rawRecords) {\n+    int addRawRecords(final Iterable<ConsumerRecord<byte[], byte[]>> rawRecords,\n+                      final long consumerPosition) {\n         for (final ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n             fifoQueue.addLast(rawRecord);\n         }\n+        if (consumerPosition <= this.consumerPosition) {\n+            throw new IllegalArgumentException(String.format(\n+                \"Consumer position must go forward; current %d, new %d\",\n+                this.consumerPosition,\n+                consumerPosition));\n+        }\n+        if (!fifoQueue.isEmpty()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MzU4Mg==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r376683582", "bodyText": "Previously, we clear the partitionGroup within closeTopology() that we call above -- however, because of the consumer position tracking, we need to delay it after the commit.", "author": "mjsax", "createdAt": "2020-02-08T03:30:01Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -621,6 +622,7 @@ void suspend(final boolean clean,\n             try {\n                 commit(false, partitionTimes);\n             } finally {\n+                partitionGroup.clear();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MzYyMg==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r376683622", "bodyText": "We add the current consumer position when we add records to the queue now.", "author": "mjsax", "createdAt": "2020-02-08T03:30:42Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -929,7 +930,21 @@ private void addRecordsToTasks(final ConsumerRecords<byte[], byte[]> records) {\n                 throw new TaskMigratedException(task);\n             }\n \n-            task.addRecords(partition, records.records(partition));\n+            final long consumerPosition;\n+            try {\n+                consumerPosition = consumer.position(partition);\n+            } catch (final TimeoutException error) {\n+                // the `consumer.position()` call should never block, because we know that we did process data\n+                // for the requested partition and thus the consumer should have a valid local position\n+                // that it can return immediately\n+\n+                // hence, a `TimeoutException` indicates a bug and thus we rethrow it as fatal `IllegalStateException`\n+                throw new IllegalStateException(error);\n+            } catch (final KafkaException fatal) {\n+                throw new StreamsException(fatal);\n+            }\n+\n+            task.addRecords(partition, records.records(partition), consumerPosition);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MzY1OQ==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r376683659", "bodyText": "I improve this test a little bit, adding more conditions.", "author": "mjsax", "createdAt": "2020-02-08T03:31:16Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -116,38 +127,66 @@ public void createTopics() throws Exception {\n \n     @Test\n     public void shouldBeAbleToRunWithEosEnabled() throws Exception {\n-        runSimpleCopyTest(1, SINGLE_PARTITION_INPUT_TOPIC, null, SINGLE_PARTITION_OUTPUT_TOPIC);\n+        runSimpleCopyTest(1, SINGLE_PARTITION_INPUT_TOPIC, null, SINGLE_PARTITION_OUTPUT_TOPIC, false);\n+    }\n+\n+    @Test\n+    public void shouldCommitCorrectOffsetIfInputTopicIsTransactional() throws Exception {\n+        runSimpleCopyTest(1, SINGLE_PARTITION_INPUT_TOPIC, null, SINGLE_PARTITION_OUTPUT_TOPIC, true);\n+\n+        try (final Admin adminClient = Admin.create(mkMap(mkEntry(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers())));\n+            final Consumer<byte[], byte[]> consumer = new KafkaConsumer<>(mkMap(\n+                mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ConsumerConfig.GROUP_ID_CONFIG, applicationId),\n+                mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class),\n+                mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class)))) {\n+\n+            waitForEmptyConsumerGroup(adminClient, applicationId, 5 * MAX_POLL_INTERVAL_MS);\n+\n+            final TopicPartition topicPartition = new TopicPartition(SINGLE_PARTITION_INPUT_TOPIC, 0);\n+            final Collection<TopicPartition> topicPartitions = Collections.singleton(topicPartition);\n+\n+            final long committedOffset = adminClient.listConsumerGroupOffsets(applicationId).partitionsToOffsetAndMetadata().get().get(topicPartition).offset();\n+\n+            consumer.assign(topicPartitions);\n+            final long consumerPosition = consumer.position(topicPartition);\n+            final long endOffset = consumer.endOffsets(topicPartitions).get(topicPartition);\n+\n+            assertThat(committedOffset, equalTo(consumerPosition));\n+            assertThat(committedOffset, equalTo(endOffset));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "8a564d863a6fab1f46f84746e458b98bb3c17208", "url": "https://github.com/apache/kafka/commit/8a564d863a6fab1f46f84746e458b98bb3c17208", "message": "KAFKA-6607: Commit correct offsets for transactional input data", "committedDate": "2020-02-11T01:09:44Z", "type": "commit"}, {"oid": "d73faa32e31ca49c6bd13953d4621975db40d146", "url": "https://github.com/apache/kafka/commit/d73faa32e31ca49c6bd13953d4621975db40d146", "message": "Remove debug stuff", "committedDate": "2020-02-11T01:09:44Z", "type": "commit"}, {"oid": "96f63c37ec0135d1c579c36a1c4200fe56d3ea07", "url": "https://github.com/apache/kafka/commit/96f63c37ec0135d1c579c36a1c4200fe56d3ea07", "message": "Github comments", "committedDate": "2020-02-11T01:09:44Z", "type": "commit"}, {"oid": "be4557192c09d5b2473a49cfb8c5707e71999da7", "url": "https://github.com/apache/kafka/commit/be4557192c09d5b2473a49cfb8c5707e71999da7", "message": "Fix test", "committedDate": "2020-02-11T01:09:44Z", "type": "commit"}, {"oid": "7f3c62b60fc1a4a73a3770838ebbc83dcafe0a8d", "url": "https://github.com/apache/kafka/commit/7f3c62b60fc1a4a73a3770838ebbc83dcafe0a8d", "message": "Revert consumer change and fix StreamThread to not drop records", "committedDate": "2020-02-11T01:09:44Z", "type": "commit"}, {"oid": "7f3c62b60fc1a4a73a3770838ebbc83dcafe0a8d", "url": "https://github.com/apache/kafka/commit/7f3c62b60fc1a4a73a3770838ebbc83dcafe0a8d", "message": "Revert consumer change and fix StreamThread to not drop records", "committedDate": "2020-02-11T01:09:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQyNTE0Mw==", "url": "https://github.com/apache/kafka/pull/8040#discussion_r377425143", "bodyText": "maybe we can also log this as INFO for debugging purposes?", "author": "guozhangwang", "createdAt": "2020-02-11T02:41:54Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -917,6 +917,13 @@ private void addRecordsToTasks(final ConsumerRecords<byte[], byte[]> records) {\n             final StreamTask task = taskManager.activeTask(partition);\n \n             if (task == null) {\n+                if (!isRunning()) {\n+                    // if we are in PENDING_SHUTDOWN and don't find the task it implies that it was a newly assigned\n+                    // task that we just skipped to create;\n+                    // hence, we just skip adding the corresponding records\n+                    continue;", "originalCommit": "7f3c62b60fc1a4a73a3770838ebbc83dcafe0a8d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "944fe8ec3720a43d895669f340184d025c880708", "url": "https://github.com/apache/kafka/commit/944fe8ec3720a43d895669f340184d025c880708", "message": "Github comment", "committedDate": "2020-02-11T03:10:03Z", "type": "commit"}]}