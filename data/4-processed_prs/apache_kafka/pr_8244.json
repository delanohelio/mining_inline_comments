{"pr_number": 8244, "pr_title": "KAFKA-8820: kafka-reassign-partitions.sh should support the KIP-455 API", "pr_createdAt": "2020-03-06T19:18:06Z", "pr_url": "https://github.com/apache/kafka/pull/8244", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTYxNzQ5NA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389617494", "bodyText": "nit: space", "author": "stanislavkozlovski", "createdAt": "2020-03-09T12:03:23Z", "path": "core/src/test/scala/integration/kafka/admin/ReassignPartitionsIntegrationTest.scala", "diffHunk": "@@ -1,175 +1,485 @@\n-/**\n-  * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE\n-  * file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file\n-  * to You under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\n-  * License. You may obtain a copy of the License at\n-  *\n-  * http://www.apache.org/licenses/LICENSE-2.0\n-  *\n-  * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-  * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-  * specific language governing permissions and limitations under the License.\n-  */\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n package kafka.admin\n \n-import java.util.Optional\n+import java.io.Closeable\n+import java.util.{Collections, HashMap, List}\n \n-import kafka.admin.TopicCommand.ZookeeperTopicService\n+import kafka.admin.ReassignPartitionsCommand._\n import kafka.server.{KafkaConfig, KafkaServer}\n import kafka.utils.TestUtils\n-import kafka.zk.ZooKeeperTestHarness\n-import org.apache.kafka.clients.admin.{AdminClientConfig, NewPartitionReassignment, NewTopic, AdminClient => JAdminClient}\n+import kafka.zk.{KafkaZkClient, ZooKeeperTestHarness}\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, DescribeLogDirsResult, NewTopic}\n+import org.apache.kafka.clients.producer.ProducerRecord\n import org.apache.kafka.common.TopicPartition\n-import org.apache.kafka.common.errors.InvalidReplicaAssignmentException\n+import org.apache.kafka.common.config.ConfigResource\n import org.apache.kafka.common.network.ListenerName\n-import org.apache.kafka.test.{TestUtils => JTestUtils}\n-import org.junit.{After, Before, Test}\n+import org.apache.kafka.common.security.auth.SecurityProtocol\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.rules.Timeout\n+import org.junit.Assert.{assertEquals, assertFalse, assertTrue}\n+import org.junit.{After, Rule, Test}\n \n+import scala.collection.Map\n import scala.collection.JavaConverters._\n-import scala.collection.Seq\n-\n-class ReassignPartitionsIntegrationTest extends ZooKeeperTestHarness with RackAwareTest {\n-  import ReassignPartitionsIntegrationTest._\n-\n-  var servers: Seq[KafkaServer] = Seq()\n-  val broker1 = 0\n-  val broker2 = 1\n-  val broker3 = 2\n-  val broker4 = 3\n-  val broker5 = 4\n-  val broker6 = 5\n-  val rack = Map(\n-    broker1 -> \"rack1\",\n-    broker2 -> \"rack2\",\n-    broker3 -> \"rack2\",\n-    broker4 -> \"rack1\",\n-    broker5 -> \"rack3\",\n-    broker6 -> \"rack3\"\n-  )\n-\n-  @Before\n-  override def setUp(): Unit = {\n-    super.setUp()\n-\n-    val brokerConfigs = TestUtils.createBrokerConfigs(6, zkConnect, enableControlledShutdown = true)\n-    servers = brokerConfigs.map { config =>\n-      config.setProperty(KafkaConfig.RackProp, rack(config.getProperty(KafkaConfig.BrokerIdProp).toInt))\n-      config.setProperty(KafkaConfig.AutoLeaderRebalanceEnableProp, \"false\")\n-      config.setProperty(KafkaConfig.ControlledShutdownMaxRetriesProp, \"1\")\n-      config.setProperty(KafkaConfig.ControlledShutdownRetryBackoffMsProp, \"1000\")\n-      config.setProperty(KafkaConfig.ReplicaLagTimeMaxMsProp, \"1000\")\n-      TestUtils.createServer(KafkaConfig.fromProps(config))\n-    }\n+import scala.collection.{Seq, mutable}\n+\n+class ReassignPartitionsIntegrationTest extends ZooKeeperTestHarness {\n+  @Rule\n+  def globalTimeout: Timeout = Timeout.millis(300000)\n+\n+  var cluster: ReassignPartitionsTestCluster = null\n+\n+  def generateConfigs: Seq[KafkaConfig] = {\n+    TestUtils.createBrokerConfigs(5, zkConnect).map(KafkaConfig.fromProps)\n   }\n \n   @After\n   override def tearDown(): Unit = {\n-    TestUtils.shutdownServers(servers)\n+    Utils.closeQuietly(cluster , \"ReassignPartitionsTestCluster\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTYxOTc0Mg==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389619742", "bodyText": "Should we ensure the createTopics call completes without an error here?", "author": "stanislavkozlovski", "createdAt": "2020-03-09T12:08:36Z", "path": "core/src/test/scala/integration/kafka/admin/ReassignPartitionsIntegrationTest.scala", "diffHunk": "@@ -1,175 +1,485 @@\n-/**\n-  * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE\n-  * file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file\n-  * to You under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\n-  * License. You may obtain a copy of the License at\n-  *\n-  * http://www.apache.org/licenses/LICENSE-2.0\n-  *\n-  * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-  * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-  * specific language governing permissions and limitations under the License.\n-  */\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n package kafka.admin\n \n-import java.util.Optional\n+import java.io.Closeable\n+import java.util.{Collections, HashMap, List}\n \n-import kafka.admin.TopicCommand.ZookeeperTopicService\n+import kafka.admin.ReassignPartitionsCommand._\n import kafka.server.{KafkaConfig, KafkaServer}\n import kafka.utils.TestUtils\n-import kafka.zk.ZooKeeperTestHarness\n-import org.apache.kafka.clients.admin.{AdminClientConfig, NewPartitionReassignment, NewTopic, AdminClient => JAdminClient}\n+import kafka.zk.{KafkaZkClient, ZooKeeperTestHarness}\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, DescribeLogDirsResult, NewTopic}\n+import org.apache.kafka.clients.producer.ProducerRecord\n import org.apache.kafka.common.TopicPartition\n-import org.apache.kafka.common.errors.InvalidReplicaAssignmentException\n+import org.apache.kafka.common.config.ConfigResource\n import org.apache.kafka.common.network.ListenerName\n-import org.apache.kafka.test.{TestUtils => JTestUtils}\n-import org.junit.{After, Before, Test}\n+import org.apache.kafka.common.security.auth.SecurityProtocol\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.rules.Timeout\n+import org.junit.Assert.{assertEquals, assertFalse, assertTrue}\n+import org.junit.{After, Rule, Test}\n \n+import scala.collection.Map\n import scala.collection.JavaConverters._\n-import scala.collection.Seq\n-\n-class ReassignPartitionsIntegrationTest extends ZooKeeperTestHarness with RackAwareTest {\n-  import ReassignPartitionsIntegrationTest._\n-\n-  var servers: Seq[KafkaServer] = Seq()\n-  val broker1 = 0\n-  val broker2 = 1\n-  val broker3 = 2\n-  val broker4 = 3\n-  val broker5 = 4\n-  val broker6 = 5\n-  val rack = Map(\n-    broker1 -> \"rack1\",\n-    broker2 -> \"rack2\",\n-    broker3 -> \"rack2\",\n-    broker4 -> \"rack1\",\n-    broker5 -> \"rack3\",\n-    broker6 -> \"rack3\"\n-  )\n-\n-  @Before\n-  override def setUp(): Unit = {\n-    super.setUp()\n-\n-    val brokerConfigs = TestUtils.createBrokerConfigs(6, zkConnect, enableControlledShutdown = true)\n-    servers = brokerConfigs.map { config =>\n-      config.setProperty(KafkaConfig.RackProp, rack(config.getProperty(KafkaConfig.BrokerIdProp).toInt))\n-      config.setProperty(KafkaConfig.AutoLeaderRebalanceEnableProp, \"false\")\n-      config.setProperty(KafkaConfig.ControlledShutdownMaxRetriesProp, \"1\")\n-      config.setProperty(KafkaConfig.ControlledShutdownRetryBackoffMsProp, \"1000\")\n-      config.setProperty(KafkaConfig.ReplicaLagTimeMaxMsProp, \"1000\")\n-      TestUtils.createServer(KafkaConfig.fromProps(config))\n-    }\n+import scala.collection.{Seq, mutable}\n+\n+class ReassignPartitionsIntegrationTest extends ZooKeeperTestHarness {\n+  @Rule\n+  def globalTimeout: Timeout = Timeout.millis(300000)\n+\n+  var cluster: ReassignPartitionsTestCluster = null\n+\n+  def generateConfigs: Seq[KafkaConfig] = {\n+    TestUtils.createBrokerConfigs(5, zkConnect).map(KafkaConfig.fromProps)\n   }\n \n   @After\n   override def tearDown(): Unit = {\n-    TestUtils.shutdownServers(servers)\n+    Utils.closeQuietly(cluster , \"ReassignPartitionsTestCluster\")\n     super.tearDown()\n   }\n \n+  val unthrottledBrokerConfigs =\n+    0.to(4).map {\n+      case brokerId => (brokerId, brokerLevelThrottles.map {\n+        case throttleName => (throttleName, -1L)\n+      }.toMap)\n+    }.toMap\n+\n+  /**\n+   * Test running a quick reassignment.\n+   */\n   @Test\n-  def testRackAwareReassign(): Unit = {\n-    val numPartitions = 18\n-    val replicationFactor = 3\n-\n-    // create a non rack aware assignment topic first\n-    val createOpts = new kafka.admin.TopicCommand.TopicCommandOptions(Array(\n-      \"--partitions\", numPartitions.toString,\n-      \"--replication-factor\", replicationFactor.toString,\n-      \"--disable-rack-aware\",\n-      \"--topic\", \"foo\"))\n-    new ZookeeperTopicService(zkClient).createTopic(createOpts)\n-\n-    val topicJson = \"\"\"{\"topics\": [{\"topic\": \"foo\"}], \"version\":1}\"\"\"\n-    val (proposedAssignment, currentAssignment) = ReassignPartitionsCommand.generateAssignment(zkClient,\n-      rack.keys.toSeq.sorted, topicJson, disableRackAware = false)\n-\n-    val assignment = proposedAssignment map { case (topicPartition, replicas) =>\n-      (topicPartition.partition, replicas)\n-    }\n-    checkReplicaDistribution(assignment, rack, rack.size, numPartitions, replicationFactor)\n+  def testReassignment(): Unit = {\n+    cluster = new ReassignPartitionsTestCluster(zkConnect)\n+    cluster.setup()\n+    val assignment = \"\"\"{\"version\":1,\"partitions\":\"\"\" +\n+      \"\"\"[{\"topic\":\"foo\",\"partition\":0,\"replicas\":[0,1,3],\"log_dirs\":[\"any\",\"any\",\"any\"]},\"\"\" +\n+      \"\"\"{\"topic\":\"bar\",\"partition\":0,\"replicas\":[3,2,0],\"log_dirs\":[\"any\",\"any\",\"any\"]}\"\"\" +\n+      \"\"\"]}\"\"\"\n+\n+    // Check that the assignment has not yet been started yet.\n+    val initialAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 2), Seq(0, 1, 3), true),\n+      new TopicPartition(\"bar\", 0) ->\n+        PartitionReassignmentState(Seq(3, 2, 1), Seq(3, 2, 0), true)\n+    )\n+    assertEquals((initialAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+    assertEquals((initialAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, false))\n+\n+    // Execute the assignment\n+    executeAssignment(cluster.adminClient, false, assignment)\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n+    val finalAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 3), Seq(0, 1, 3), true),\n+      new TopicPartition(\"bar\", 0) ->\n+        PartitionReassignmentState(Seq(3, 2, 0), Seq(3, 2, 0), true)\n+    )\n+\n+    // Wait for the assignment to complete\n+    TestUtils.waitUntilTrue(\n+      () => {\n+        // When using --zookeeper, we aren't able to see the new-style assignment\n+        assertFalse(runVerifyAssignment(zkClient, assignment, false)._2)\n+        finalAssignment.equals(runVerifyAssignment(cluster.adminClient, assignment, false)._1)\n+      }, \"Expected reassignment to complete\")\n+    assertEquals((finalAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, false))\n+    assertEquals((finalAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n   }\n \n+  /**\n+   * Test running a reassignment with the interBrokerThrottle set.\n+   */\n   @Test\n-  def testReassignPartition(): Unit = {\n-    TestUtils.resource(JAdminClient.create(createConfig(servers).asJava)) { client =>\n-      val topic = \"test-topic\"\n-      val partition = 0: Integer\n+  def testThrottledReassignment(): Unit = {\n+    cluster = new ReassignPartitionsTestCluster(zkConnect)\n+    cluster.setup()\n+    cluster.produceMessages(\"foo\", 0, 50)\n+    cluster.produceMessages(\"baz\", 2, 60)\n+    val assignment = \"\"\"{\"version\":1,\"partitions\":\"\"\" +\n+      \"\"\"[{\"topic\":\"foo\",\"partition\":0,\"replicas\":[0,3,2],\"log_dirs\":[\"any\",\"any\",\"any\"]},\"\"\" +\n+      \"\"\"{\"topic\":\"baz\",\"partition\":2,\"replicas\":[3,2,1],\"log_dirs\":[\"any\",\"any\",\"any\"]}\"\"\" +\n+      \"\"\"]}\"\"\"\n \n-      val partitionAssignment = Map(partition -> Seq(broker1: Integer, broker2:Integer).asJava).asJava\n-      val newTopic = new NewTopic(topic, partitionAssignment)\n-      client.createTopics(Seq(newTopic).asJava).all().get()\n+    // Check that the assignment has not yet been started yet.\n+    val initialAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 2), Seq(0, 3, 2), true),\n+      new TopicPartition(\"baz\", 2) ->\n+        PartitionReassignmentState(Seq(0, 2, 1), Seq(3, 2, 1), true)\n+    )\n+    assertEquals((initialAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+    assertEquals((initialAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, false))\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n \n-      val topicPartition = new TopicPartition(topic, partition)\n+    // Execute the assignment\n+    val interBrokerThrottle = 300000L\n+    executeAssignment(cluster.adminClient, false, assignment, interBrokerThrottle)\n \n-      // All sync replicas are in the ISR\n-      TestUtils.waitForBrokersInIsr(client, topicPartition, Set(broker1, broker2))\n+    val throttledConfigMap = Map[String, Long](\n+      brokerLevelLeaderThrottle -> interBrokerThrottle,\n+      brokerLevelFollowerThrottle -> interBrokerThrottle,\n+      brokerLevelReplicaThrottle -> -1L\n+    )\n+    val throttledBrokerConfigs = Map[Int, Map[String, Long]](\n+      0 -> throttledConfigMap,\n+      1 -> throttledConfigMap,\n+      2 -> throttledConfigMap,\n+      3 -> throttledConfigMap,\n+      4 -> unthrottledBrokerConfigs(4)\n+    )\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+    val finalAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 3, 2), Seq(0, 3, 2), true),\n+      new TopicPartition(\"baz\", 2) ->\n+        PartitionReassignmentState(Seq(3, 2, 1), Seq(3, 2, 1), true)\n+    )\n+\n+    // Wait for the assignment to complete\n+    TestUtils.waitUntilTrue(\n+      () => {\n+        // Check the reassignment status.\n+        val (partStates, partsOngoing, _, _) =\n+          runVerifyAssignment(cluster.adminClient, assignment, true)\n+        if (!partsOngoing) {\n+          assertEquals(finalAssignment, partStates)\n+          true\n+        } else {\n+          assertTrue(\"Expected at least one partition reassignment to be ongoing when \" +\n+            \"partsOngoing = true.  partStates = \" + partStates,\n+            !partStates.forall(_._2.done))\n+          assertEquals(Seq(0, 3, 2),\n+            partStates.get(new TopicPartition(\"foo\", 0)).get.targetReplicas)\n+          assertEquals(Seq(3, 2, 1),\n+            partStates.get(new TopicPartition(\"baz\", 2)).get.targetReplicas)\n+          logger.info(s\"Current partition states: ${partStates}\")\n+          waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+          false\n+        }\n+      }, \"Expected reassignment to complete.\")\n+    assertEquals((finalAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, true))\n+    assertEquals((finalAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, true))\n+    // The throttles should still have been preserved, since we ran with --preserve-throttles\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+    // Now remove the throttles.\n+    assertEquals((finalAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+    waitForBrokerLevelThrottles(unthrottledBrokerConfigs)\n+  }\n \n-      // Reassign replicas to different brokers\n-      client.alterPartitionReassignments(\n-        Map(topicPartition -> reassignmentEntry(Seq(broker3, broker4))).asJava\n-      ).all().get()\n+  /**\n+   * Test running a reassignment and then cancelling it.\n+   */\n+  @Test\n+  def testCancellation(): Unit = {\n+    cluster = new ReassignPartitionsTestCluster(zkConnect)\n+    cluster.setup()\n+    cluster.produceMessages(\"foo\", 0, 60)\n+    cluster.produceMessages(\"baz\", 1, 80)\n+    val assignment = \"\"\"{\"version\":1,\"partitions\":\"\"\" +\n+      \"\"\"[{\"topic\":\"foo\",\"partition\":0,\"replicas\":[0,1,3],\"log_dirs\":[\"any\",\"any\",\"any\"]},\"\"\" +\n+      \"\"\"{\"topic\":\"baz\",\"partition\":1,\"replicas\":[0,2,3],\"log_dirs\":[\"any\",\"any\",\"any\"]}\"\"\" +\n+      \"\"\"]}\"\"\"\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n+    val interBrokerThrottle = 100L\n+    executeAssignment(cluster.adminClient, false, assignment, interBrokerThrottle)\n+    val throttledConfigMap = Map[String, Long](\n+      brokerLevelLeaderThrottle -> interBrokerThrottle,\n+      brokerLevelFollowerThrottle -> interBrokerThrottle,\n+      brokerLevelReplicaThrottle -> -1L\n+    )\n+    val throttledBrokerConfigs = Map[Int, Map[String, Long]](\n+      0 -> throttledConfigMap,\n+      1 -> throttledConfigMap,\n+      2 -> throttledConfigMap,\n+      3 -> throttledConfigMap,\n+      4 -> unthrottledBrokerConfigs(4)\n+    )\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+    // Verify that the reassignment is running.  The very low throttle should keep it\n+    // from completing before this runs.\n+    assertEquals((Map(new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 3, 2), Seq(0, 1, 3), false),\n+        new TopicPartition(\"baz\", 1) ->\n+          PartitionReassignmentState(Seq(0, 2, 3, 1), Seq(0, 2, 3), false)),\n+      true, Map(), false),\n+      runVerifyAssignment(cluster.adminClient, assignment, true))\n+    // Cancel the reassignment.\n+    assertEquals((Set(\n+        new TopicPartition(\"foo\", 0),\n+        new TopicPartition(\"baz\", 1)\n+      ), Set()), runCancelAssignment(cluster.adminClient, assignment, true))\n+    // Broker throttles are still active because we passed --preserve-throttles\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+    // Cancelling the reassignment again should reveal nothing to cancel.\n+    assertEquals((Set(), Set()), runCancelAssignment(cluster.adminClient, assignment, false))\n+    // This time, the broker throttles were removed.\n+    waitForBrokerLevelThrottles(unthrottledBrokerConfigs)\n+  }\n \n-      waitForAllReassignmentsToComplete(client)\n+  private def waitForBrokerLevelThrottles(targetThrottles: Map[Int, Map[String, Long]]): Unit = {\n+    var curThrottles: Map[Int, Map[String, Long]] = Map.empty\n+    TestUtils.waitUntilTrue(() => {\n+      curThrottles = describeBrokerLevelThrottles(targetThrottles.keySet.toSeq)\n+      targetThrottles.equals(curThrottles)\n+    }, s\"timed out waiting for broker throttle to become ${targetThrottles}.  \" +\n+      s\"Latest throttles were ${curThrottles}\", pause = 25)\n+  }\n \n-      // Metadata info is eventually consistent wait for update\n-      TestUtils.waitForReplicasAssigned(client, topicPartition, Seq(broker3, broker4))\n-      // All sync replicas are in the ISR\n-      TestUtils.waitForBrokersInIsr(client, topicPartition, Set(broker3, broker4))\n-    }\n+  /**\n+   * Describe the broker-level throttles in the cluster.\n+   *\n+   * @return                A map whose keys are broker IDs and whose values are throttle\n+   *                        information.  The nested maps are keyed on throttle name.\n+   */\n+  private def describeBrokerLevelThrottles(brokerIds: Seq[Int]): Map[Int, Map[String, Long]] = {\n+    brokerIds.map {\n+      case brokerId =>\n+        val props = zkClient.getEntityConfigs(\"brokers\", brokerId.toString)\n+        (brokerId, brokerLevelThrottles.map {\n+          case throttleName => (throttleName,\n+            props.getOrDefault(throttleName, \"-1\").asInstanceOf[String].toLong)\n+        }.toMap)\n+    }.toMap\n   }\n \n+  /**\n+   * Test moving partitions between directories.\n+   */\n   @Test\n-  def testInvalidReplicaIds(): Unit = {\n-    TestUtils.resource(JAdminClient.create(createConfig(servers).asJava)) { client =>\n-      val topic = \"test-topic\"\n-      val partition = 0: Integer\n-\n-      val partitionAssignment = Map(partition -> Seq(broker1: Integer, broker2: Integer).asJava).asJava\n-      val newTopic = new NewTopic(topic, partitionAssignment)\n-      client.createTopics(Seq(newTopic).asJava).all().get()\n-\n-      val topicPartition = new TopicPartition(topic, partition)\n-\n-      // All sync replicas are in the ISR\n-      TestUtils.waitForBrokersInIsr(client, topicPartition, Set(broker1, broker2))\n-\n-      // Test reassignment with duplicate broker ids\n-      var future = client.alterPartitionReassignments(\n-        Map(topicPartition -> reassignmentEntry(Seq(broker4, broker5, broker5))).asJava\n-      ).all()\n-      JTestUtils.assertFutureThrows(future, classOf[InvalidReplicaAssignmentException])\n-\n-      // Test reassignment with invalid broker ids\n-      future = client.alterPartitionReassignments(\n-        Map(topicPartition -> reassignmentEntry(Seq(-1, broker3))).asJava\n-      ).all()\n-      JTestUtils.assertFutureThrows(future, classOf[InvalidReplicaAssignmentException])\n-\n-      // Test reassignment with extra broker ids\n-      future = client.alterPartitionReassignments(\n-        Map(topicPartition -> reassignmentEntry(Seq(6, broker2, broker3))).asJava\n-      ).all()\n-      JTestUtils.assertFutureThrows(future, classOf[InvalidReplicaAssignmentException])\n+  def testReplicaMoves(): Unit = {\n+    cluster = new ReassignPartitionsTestCluster(zkConnect)\n+    cluster.setup()\n+    cluster.produceMessages(\"foo\", 0, 7000)\n+    cluster.produceMessages(\"baz\", 1, 6000)\n+\n+    val result0 = cluster.adminClient.describeLogDirs(\n+        0.to(4).map(_.asInstanceOf[Integer]).asJavaCollection)\n+    val info0 = new BrokerDirs(result0, 0)\n+    assertTrue(info0.futureLogDirs.isEmpty)\n+    assertEquals(Set(new TopicPartition(\"foo\", 0),\n+        new TopicPartition(\"baz\", 0),\n+        new TopicPartition(\"baz\", 1),\n+        new TopicPartition(\"baz\", 2)),\n+      info0.curLogDirs.keySet)\n+    val curFoo1Dir = info0.curLogDirs.get(new TopicPartition(\"foo\", 0)).getOrElse(\"\")\n+    assertFalse(curFoo1Dir.equals(\"\"))\n+    val newFoo1Dir = info0.logDirs.find(!_.equals(curFoo1Dir)).get\n+    val assignment = \"\"\"{\"version\":1,\"partitions\":\"\"\" +\n+        \"\"\"[{\"topic\":\"foo\",\"partition\":0,\"replicas\":[0,1,2],\"\"\" +\n+          \"\\\"log_dirs\\\":[\\\"%s\\\",\\\"any\\\",\\\"any\\\"]}\".format(newFoo1Dir) +\n+            \"]}\"\n+    // Start the replica move, but throttle it to be very slow so that it can't complete\n+    // before our next checks happen.\n+    executeAssignment(cluster.adminClient, false, assignment, -1L, 1L)\n+\n+    // Check the output of --verify\n+    val (_, _, replicaMoveStates, movesOngoing) =\n+      runVerifyAssignment(cluster.adminClient, assignment, true)\n+    assertEquals(1, replicaMoveStates.size)\n+    replicaMoveStates.foreach {\n+      case (_, state) => assertEquals(ActiveMoveState(curFoo1Dir, newFoo1Dir, newFoo1Dir), state)\n     }\n-  }\n-}\n+    assertTrue(movesOngoing)\n \n-object ReassignPartitionsIntegrationTest {\n-  def createConfig(servers: Seq[KafkaServer]): Map[String, Object] = {\n-    Map(\n-      AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG -> TestUtils.bootstrapServers(servers, new ListenerName(\"PLAINTEXT\")),\n-      AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG -> \"20000\"\n+    // Check that the appropriate broker throttle is in place.\n+    val throttledConfigMap = Map[String, Long](\n+      brokerLevelLeaderThrottle -> -1,\n+      brokerLevelFollowerThrottle -> -1,\n+      brokerLevelReplicaThrottle -> 1L\n     )\n+    val throttledBrokerConfigs = Map[Int, Map[String, Long]](\n+      0 -> throttledConfigMap,\n+      1 -> unthrottledBrokerConfigs(1),\n+      2 -> unthrottledBrokerConfigs(2),\n+      3 -> unthrottledBrokerConfigs(3),\n+      4 -> unthrottledBrokerConfigs(4)\n+    )\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+\n+    // Remove the throttle\n+    cluster.adminClient.incrementalAlterConfigs(Collections.singletonMap(\n+      new ConfigResource(ConfigResource.Type.BROKER, \"0\"),\n+      Collections.singletonList(new AlterConfigOp(\n+        new ConfigEntry(brokerLevelReplicaThrottle, \"\"), AlterConfigOp.OpType.DELETE)))).\n+          all().get()\n+    waitForBrokerLevelThrottles(unthrottledBrokerConfigs)\n+\n+    // Wait for the assignment to complete\n+    TestUtils.waitUntilTrue(\n+      () => {\n+        // Check the reassignment status.\n+        val (_, partsOngoing, _, movesOngoing) =\n+          runVerifyAssignment(cluster.adminClient, assignment, true)\n+        assertFalse(partsOngoing)\n+        !movesOngoing\n+      }, \"Expected directory movement to complete.\")\n+\n+    val info1 = new BrokerDirs(cluster.adminClient.describeLogDirs(0.to(4).\n+        map(_.asInstanceOf[Integer]).asJavaCollection), 0)\n+    assertEquals(newFoo1Dir,\n+      info1.curLogDirs.getOrElse(new TopicPartition(\"foo\", 0), \"\"))\n+  }\n+\n+  private def runVerifyAssignment(adminClient: Admin, jsonString: String,\n+                                  preserveThrottles: Boolean) = {\n+    println(s\"==> verifyAssignment(adminClient, jsonString = ${jsonString})\")\n+    verifyAssignment(adminClient, jsonString, preserveThrottles)\n+  }\n+\n+  private def runVerifyAssignment(zkClient: KafkaZkClient, jsonString: String,\n+                                  preserveThrottles: Boolean) = {\n+    println(s\"==> verifyAssignment(zkClient, jsonString = ${jsonString})\")\n+    verifyAssignment(zkClient, jsonString, preserveThrottles)\n+  }\n+\n+  private def runCancelAssignment(adminClient: Admin, jsonString: String,\n+                                  preserveThrottles: Boolean) = {\n+    println(s\"==> cancelAssignment(adminClient, jsonString = ${jsonString})\")\n+    cancelAssignment(adminClient, jsonString, preserveThrottles)\n   }\n \n-  def reassignmentEntry(replicas: Seq[Int]): Optional[NewPartitionReassignment] = {\n-    Optional.of(new NewPartitionReassignment(replicas.map(r => r: Integer).asJava))\n+  class BrokerDirs(result: DescribeLogDirsResult, val brokerId: Int) {\n+    val logDirs = new mutable.HashSet[String]\n+    val curLogDirs = new mutable.HashMap[TopicPartition, String]\n+    val futureLogDirs = new mutable.HashMap[TopicPartition, String]\n+    result.values().get(brokerId).get().asScala.foreach {\n+      case (logDirName, logDirInfo) => {\n+        logDirs.add(logDirName)\n+        logDirInfo.replicaInfos.asScala.foreach {\n+          case (part, info) =>\n+            println(s\"part = ${part}\")\n+            if (info.isFuture) {\n+              futureLogDirs.put(part, logDirName)\n+            } else {\n+              curLogDirs.put(part, logDirName)\n+            }\n+        }\n+      }\n+    }\n   }\n \n-  def waitForAllReassignmentsToComplete(client: JAdminClient): Unit = {\n-    TestUtils.waitUntilTrue(() => client.listPartitionReassignments().reassignments().get().isEmpty,\n-      s\"There still are ongoing reassignments\", pause = 100L)\n+  class ReassignPartitionsTestCluster(val zkConnect: String) extends Closeable {\n+    val brokers = Map(\n+      0 -> \"rack0\",\n+      1 -> \"rack0\",\n+      2 -> \"rack1\",\n+      3 -> \"rack1\",\n+      4 -> \"rack1\"\n+    )\n+\n+    val topics = Map(\n+      \"foo\" -> Seq(Seq(0, 1, 2), Seq(1, 2, 3)),\n+      \"bar\" -> Seq(Seq(3, 2, 1)),\n+      \"baz\" -> Seq(Seq(1, 0, 2), Seq(2, 0, 1), Seq(0, 2, 1))\n+    )\n+\n+    val brokerConfigs = brokers.map {\n+      case (brokerId, rack) =>\n+        val config = TestUtils.createBrokerConfig(\n+          nodeId = brokerId,\n+          zkConnect = zkConnect,\n+          rack = Some(rack),\n+          enableControlledShutdown = false, // shorten test time\n+          logDirCount = 3)\n+        // shorter backoff to reduce test durations when no active partitions are eligible for fetching due to throttling\n+        config.setProperty(KafkaConfig.ReplicaFetchBackoffMsProp, \"100\")\n+        // Don't move partition leaders automatically.\n+        config.setProperty(KafkaConfig.AutoLeaderRebalanceEnableProp, \"false\")\n+        config.setProperty(KafkaConfig.ReplicaLagTimeMaxMsProp, \"1000\")\n+        config\n+    }.toBuffer\n+\n+    var servers = new mutable.ArrayBuffer[KafkaServer]\n+\n+    var brokerList: String = null\n+\n+    var adminClient: Admin = null\n+\n+    def setup(): Unit = {\n+      createServers()\n+      createTopics()\n+    }\n+\n+    def createServers(): Unit = {\n+      brokers.keySet.foreach {\n+        case brokerId =>\n+          servers += TestUtils.createServer(KafkaConfig(brokerConfigs(brokerId)))\n+      }\n+    }\n+\n+    def createTopics(): Unit = {\n+      TestUtils.waitUntilBrokerMetadataIsPropagated(servers)\n+      brokerList = TestUtils.bootstrapServers(servers,\n+        ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT))\n+      adminClient = Admin.create(Map[String, Object](\n+        AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG -> brokerList\n+      ).asJava)\n+      adminClient.createTopics(topics.map {\n+        case (topicName, parts) =>\n+          val partMap = new HashMap[Integer, List[Integer]]()\n+          parts.zipWithIndex.foreach {\n+            case (part, index) => partMap.put(index, part.map(Integer.valueOf(_)).asJava)\n+          }\n+          new NewTopic(topicName, partMap)\n+      }.toList.asJava)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTYyMTM2Mw==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389621363", "bodyText": "nit: Should we include the fact that replicas are moving in between directories in the test name? This would make the test failure output clearer on first read", "author": "stanislavkozlovski", "createdAt": "2020-03-09T12:12:21Z", "path": "core/src/test/scala/integration/kafka/admin/ReassignPartitionsIntegrationTest.scala", "diffHunk": "@@ -1,175 +1,485 @@\n-/**\n-  * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE\n-  * file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file\n-  * to You under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\n-  * License. You may obtain a copy of the License at\n-  *\n-  * http://www.apache.org/licenses/LICENSE-2.0\n-  *\n-  * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-  * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-  * specific language governing permissions and limitations under the License.\n-  */\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n package kafka.admin\n \n-import java.util.Optional\n+import java.io.Closeable\n+import java.util.{Collections, HashMap, List}\n \n-import kafka.admin.TopicCommand.ZookeeperTopicService\n+import kafka.admin.ReassignPartitionsCommand._\n import kafka.server.{KafkaConfig, KafkaServer}\n import kafka.utils.TestUtils\n-import kafka.zk.ZooKeeperTestHarness\n-import org.apache.kafka.clients.admin.{AdminClientConfig, NewPartitionReassignment, NewTopic, AdminClient => JAdminClient}\n+import kafka.zk.{KafkaZkClient, ZooKeeperTestHarness}\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, DescribeLogDirsResult, NewTopic}\n+import org.apache.kafka.clients.producer.ProducerRecord\n import org.apache.kafka.common.TopicPartition\n-import org.apache.kafka.common.errors.InvalidReplicaAssignmentException\n+import org.apache.kafka.common.config.ConfigResource\n import org.apache.kafka.common.network.ListenerName\n-import org.apache.kafka.test.{TestUtils => JTestUtils}\n-import org.junit.{After, Before, Test}\n+import org.apache.kafka.common.security.auth.SecurityProtocol\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.rules.Timeout\n+import org.junit.Assert.{assertEquals, assertFalse, assertTrue}\n+import org.junit.{After, Rule, Test}\n \n+import scala.collection.Map\n import scala.collection.JavaConverters._\n-import scala.collection.Seq\n-\n-class ReassignPartitionsIntegrationTest extends ZooKeeperTestHarness with RackAwareTest {\n-  import ReassignPartitionsIntegrationTest._\n-\n-  var servers: Seq[KafkaServer] = Seq()\n-  val broker1 = 0\n-  val broker2 = 1\n-  val broker3 = 2\n-  val broker4 = 3\n-  val broker5 = 4\n-  val broker6 = 5\n-  val rack = Map(\n-    broker1 -> \"rack1\",\n-    broker2 -> \"rack2\",\n-    broker3 -> \"rack2\",\n-    broker4 -> \"rack1\",\n-    broker5 -> \"rack3\",\n-    broker6 -> \"rack3\"\n-  )\n-\n-  @Before\n-  override def setUp(): Unit = {\n-    super.setUp()\n-\n-    val brokerConfigs = TestUtils.createBrokerConfigs(6, zkConnect, enableControlledShutdown = true)\n-    servers = brokerConfigs.map { config =>\n-      config.setProperty(KafkaConfig.RackProp, rack(config.getProperty(KafkaConfig.BrokerIdProp).toInt))\n-      config.setProperty(KafkaConfig.AutoLeaderRebalanceEnableProp, \"false\")\n-      config.setProperty(KafkaConfig.ControlledShutdownMaxRetriesProp, \"1\")\n-      config.setProperty(KafkaConfig.ControlledShutdownRetryBackoffMsProp, \"1000\")\n-      config.setProperty(KafkaConfig.ReplicaLagTimeMaxMsProp, \"1000\")\n-      TestUtils.createServer(KafkaConfig.fromProps(config))\n-    }\n+import scala.collection.{Seq, mutable}\n+\n+class ReassignPartitionsIntegrationTest extends ZooKeeperTestHarness {\n+  @Rule\n+  def globalTimeout: Timeout = Timeout.millis(300000)\n+\n+  var cluster: ReassignPartitionsTestCluster = null\n+\n+  def generateConfigs: Seq[KafkaConfig] = {\n+    TestUtils.createBrokerConfigs(5, zkConnect).map(KafkaConfig.fromProps)\n   }\n \n   @After\n   override def tearDown(): Unit = {\n-    TestUtils.shutdownServers(servers)\n+    Utils.closeQuietly(cluster , \"ReassignPartitionsTestCluster\")\n     super.tearDown()\n   }\n \n+  val unthrottledBrokerConfigs =\n+    0.to(4).map {\n+      case brokerId => (brokerId, brokerLevelThrottles.map {\n+        case throttleName => (throttleName, -1L)\n+      }.toMap)\n+    }.toMap\n+\n+  /**\n+   * Test running a quick reassignment.\n+   */\n   @Test\n-  def testRackAwareReassign(): Unit = {\n-    val numPartitions = 18\n-    val replicationFactor = 3\n-\n-    // create a non rack aware assignment topic first\n-    val createOpts = new kafka.admin.TopicCommand.TopicCommandOptions(Array(\n-      \"--partitions\", numPartitions.toString,\n-      \"--replication-factor\", replicationFactor.toString,\n-      \"--disable-rack-aware\",\n-      \"--topic\", \"foo\"))\n-    new ZookeeperTopicService(zkClient).createTopic(createOpts)\n-\n-    val topicJson = \"\"\"{\"topics\": [{\"topic\": \"foo\"}], \"version\":1}\"\"\"\n-    val (proposedAssignment, currentAssignment) = ReassignPartitionsCommand.generateAssignment(zkClient,\n-      rack.keys.toSeq.sorted, topicJson, disableRackAware = false)\n-\n-    val assignment = proposedAssignment map { case (topicPartition, replicas) =>\n-      (topicPartition.partition, replicas)\n-    }\n-    checkReplicaDistribution(assignment, rack, rack.size, numPartitions, replicationFactor)\n+  def testReassignment(): Unit = {\n+    cluster = new ReassignPartitionsTestCluster(zkConnect)\n+    cluster.setup()\n+    val assignment = \"\"\"{\"version\":1,\"partitions\":\"\"\" +\n+      \"\"\"[{\"topic\":\"foo\",\"partition\":0,\"replicas\":[0,1,3],\"log_dirs\":[\"any\",\"any\",\"any\"]},\"\"\" +\n+      \"\"\"{\"topic\":\"bar\",\"partition\":0,\"replicas\":[3,2,0],\"log_dirs\":[\"any\",\"any\",\"any\"]}\"\"\" +\n+      \"\"\"]}\"\"\"\n+\n+    // Check that the assignment has not yet been started yet.\n+    val initialAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 2), Seq(0, 1, 3), true),\n+      new TopicPartition(\"bar\", 0) ->\n+        PartitionReassignmentState(Seq(3, 2, 1), Seq(3, 2, 0), true)\n+    )\n+    assertEquals((initialAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+    assertEquals((initialAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, false))\n+\n+    // Execute the assignment\n+    executeAssignment(cluster.adminClient, false, assignment)\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n+    val finalAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 3), Seq(0, 1, 3), true),\n+      new TopicPartition(\"bar\", 0) ->\n+        PartitionReassignmentState(Seq(3, 2, 0), Seq(3, 2, 0), true)\n+    )\n+\n+    // Wait for the assignment to complete\n+    TestUtils.waitUntilTrue(\n+      () => {\n+        // When using --zookeeper, we aren't able to see the new-style assignment\n+        assertFalse(runVerifyAssignment(zkClient, assignment, false)._2)\n+        finalAssignment.equals(runVerifyAssignment(cluster.adminClient, assignment, false)._1)\n+      }, \"Expected reassignment to complete\")\n+    assertEquals((finalAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, false))\n+    assertEquals((finalAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n   }\n \n+  /**\n+   * Test running a reassignment with the interBrokerThrottle set.\n+   */\n   @Test\n-  def testReassignPartition(): Unit = {\n-    TestUtils.resource(JAdminClient.create(createConfig(servers).asJava)) { client =>\n-      val topic = \"test-topic\"\n-      val partition = 0: Integer\n+  def testThrottledReassignment(): Unit = {\n+    cluster = new ReassignPartitionsTestCluster(zkConnect)\n+    cluster.setup()\n+    cluster.produceMessages(\"foo\", 0, 50)\n+    cluster.produceMessages(\"baz\", 2, 60)\n+    val assignment = \"\"\"{\"version\":1,\"partitions\":\"\"\" +\n+      \"\"\"[{\"topic\":\"foo\",\"partition\":0,\"replicas\":[0,3,2],\"log_dirs\":[\"any\",\"any\",\"any\"]},\"\"\" +\n+      \"\"\"{\"topic\":\"baz\",\"partition\":2,\"replicas\":[3,2,1],\"log_dirs\":[\"any\",\"any\",\"any\"]}\"\"\" +\n+      \"\"\"]}\"\"\"\n \n-      val partitionAssignment = Map(partition -> Seq(broker1: Integer, broker2:Integer).asJava).asJava\n-      val newTopic = new NewTopic(topic, partitionAssignment)\n-      client.createTopics(Seq(newTopic).asJava).all().get()\n+    // Check that the assignment has not yet been started yet.\n+    val initialAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 2), Seq(0, 3, 2), true),\n+      new TopicPartition(\"baz\", 2) ->\n+        PartitionReassignmentState(Seq(0, 2, 1), Seq(3, 2, 1), true)\n+    )\n+    assertEquals((initialAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+    assertEquals((initialAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, false))\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n \n-      val topicPartition = new TopicPartition(topic, partition)\n+    // Execute the assignment\n+    val interBrokerThrottle = 300000L\n+    executeAssignment(cluster.adminClient, false, assignment, interBrokerThrottle)\n \n-      // All sync replicas are in the ISR\n-      TestUtils.waitForBrokersInIsr(client, topicPartition, Set(broker1, broker2))\n+    val throttledConfigMap = Map[String, Long](\n+      brokerLevelLeaderThrottle -> interBrokerThrottle,\n+      brokerLevelFollowerThrottle -> interBrokerThrottle,\n+      brokerLevelReplicaThrottle -> -1L\n+    )\n+    val throttledBrokerConfigs = Map[Int, Map[String, Long]](\n+      0 -> throttledConfigMap,\n+      1 -> throttledConfigMap,\n+      2 -> throttledConfigMap,\n+      3 -> throttledConfigMap,\n+      4 -> unthrottledBrokerConfigs(4)\n+    )\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+    val finalAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 3, 2), Seq(0, 3, 2), true),\n+      new TopicPartition(\"baz\", 2) ->\n+        PartitionReassignmentState(Seq(3, 2, 1), Seq(3, 2, 1), true)\n+    )\n+\n+    // Wait for the assignment to complete\n+    TestUtils.waitUntilTrue(\n+      () => {\n+        // Check the reassignment status.\n+        val (partStates, partsOngoing, _, _) =\n+          runVerifyAssignment(cluster.adminClient, assignment, true)\n+        if (!partsOngoing) {\n+          assertEquals(finalAssignment, partStates)\n+          true\n+        } else {\n+          assertTrue(\"Expected at least one partition reassignment to be ongoing when \" +\n+            \"partsOngoing = true.  partStates = \" + partStates,\n+            !partStates.forall(_._2.done))\n+          assertEquals(Seq(0, 3, 2),\n+            partStates.get(new TopicPartition(\"foo\", 0)).get.targetReplicas)\n+          assertEquals(Seq(3, 2, 1),\n+            partStates.get(new TopicPartition(\"baz\", 2)).get.targetReplicas)\n+          logger.info(s\"Current partition states: ${partStates}\")\n+          waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+          false\n+        }\n+      }, \"Expected reassignment to complete.\")\n+    assertEquals((finalAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, true))\n+    assertEquals((finalAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, true))\n+    // The throttles should still have been preserved, since we ran with --preserve-throttles\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+    // Now remove the throttles.\n+    assertEquals((finalAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+    waitForBrokerLevelThrottles(unthrottledBrokerConfigs)\n+  }\n \n-      // Reassign replicas to different brokers\n-      client.alterPartitionReassignments(\n-        Map(topicPartition -> reassignmentEntry(Seq(broker3, broker4))).asJava\n-      ).all().get()\n+  /**\n+   * Test running a reassignment and then cancelling it.\n+   */\n+  @Test\n+  def testCancellation(): Unit = {\n+    cluster = new ReassignPartitionsTestCluster(zkConnect)\n+    cluster.setup()\n+    cluster.produceMessages(\"foo\", 0, 60)\n+    cluster.produceMessages(\"baz\", 1, 80)\n+    val assignment = \"\"\"{\"version\":1,\"partitions\":\"\"\" +\n+      \"\"\"[{\"topic\":\"foo\",\"partition\":0,\"replicas\":[0,1,3],\"log_dirs\":[\"any\",\"any\",\"any\"]},\"\"\" +\n+      \"\"\"{\"topic\":\"baz\",\"partition\":1,\"replicas\":[0,2,3],\"log_dirs\":[\"any\",\"any\",\"any\"]}\"\"\" +\n+      \"\"\"]}\"\"\"\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n+    val interBrokerThrottle = 100L\n+    executeAssignment(cluster.adminClient, false, assignment, interBrokerThrottle)\n+    val throttledConfigMap = Map[String, Long](\n+      brokerLevelLeaderThrottle -> interBrokerThrottle,\n+      brokerLevelFollowerThrottle -> interBrokerThrottle,\n+      brokerLevelReplicaThrottle -> -1L\n+    )\n+    val throttledBrokerConfigs = Map[Int, Map[String, Long]](\n+      0 -> throttledConfigMap,\n+      1 -> throttledConfigMap,\n+      2 -> throttledConfigMap,\n+      3 -> throttledConfigMap,\n+      4 -> unthrottledBrokerConfigs(4)\n+    )\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+    // Verify that the reassignment is running.  The very low throttle should keep it\n+    // from completing before this runs.\n+    assertEquals((Map(new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 3, 2), Seq(0, 1, 3), false),\n+        new TopicPartition(\"baz\", 1) ->\n+          PartitionReassignmentState(Seq(0, 2, 3, 1), Seq(0, 2, 3), false)),\n+      true, Map(), false),\n+      runVerifyAssignment(cluster.adminClient, assignment, true))\n+    // Cancel the reassignment.\n+    assertEquals((Set(\n+        new TopicPartition(\"foo\", 0),\n+        new TopicPartition(\"baz\", 1)\n+      ), Set()), runCancelAssignment(cluster.adminClient, assignment, true))\n+    // Broker throttles are still active because we passed --preserve-throttles\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+    // Cancelling the reassignment again should reveal nothing to cancel.\n+    assertEquals((Set(), Set()), runCancelAssignment(cluster.adminClient, assignment, false))\n+    // This time, the broker throttles were removed.\n+    waitForBrokerLevelThrottles(unthrottledBrokerConfigs)\n+  }\n \n-      waitForAllReassignmentsToComplete(client)\n+  private def waitForBrokerLevelThrottles(targetThrottles: Map[Int, Map[String, Long]]): Unit = {\n+    var curThrottles: Map[Int, Map[String, Long]] = Map.empty\n+    TestUtils.waitUntilTrue(() => {\n+      curThrottles = describeBrokerLevelThrottles(targetThrottles.keySet.toSeq)\n+      targetThrottles.equals(curThrottles)\n+    }, s\"timed out waiting for broker throttle to become ${targetThrottles}.  \" +\n+      s\"Latest throttles were ${curThrottles}\", pause = 25)\n+  }\n \n-      // Metadata info is eventually consistent wait for update\n-      TestUtils.waitForReplicasAssigned(client, topicPartition, Seq(broker3, broker4))\n-      // All sync replicas are in the ISR\n-      TestUtils.waitForBrokersInIsr(client, topicPartition, Set(broker3, broker4))\n-    }\n+  /**\n+   * Describe the broker-level throttles in the cluster.\n+   *\n+   * @return                A map whose keys are broker IDs and whose values are throttle\n+   *                        information.  The nested maps are keyed on throttle name.\n+   */\n+  private def describeBrokerLevelThrottles(brokerIds: Seq[Int]): Map[Int, Map[String, Long]] = {\n+    brokerIds.map {\n+      case brokerId =>\n+        val props = zkClient.getEntityConfigs(\"brokers\", brokerId.toString)\n+        (brokerId, brokerLevelThrottles.map {\n+          case throttleName => (throttleName,\n+            props.getOrDefault(throttleName, \"-1\").asInstanceOf[String].toLong)\n+        }.toMap)\n+    }.toMap\n   }\n \n+  /**\n+   * Test moving partitions between directories.\n+   */\n   @Test\n-  def testInvalidReplicaIds(): Unit = {\n-    TestUtils.resource(JAdminClient.create(createConfig(servers).asJava)) { client =>\n-      val topic = \"test-topic\"\n-      val partition = 0: Integer\n-\n-      val partitionAssignment = Map(partition -> Seq(broker1: Integer, broker2: Integer).asJava).asJava\n-      val newTopic = new NewTopic(topic, partitionAssignment)\n-      client.createTopics(Seq(newTopic).asJava).all().get()\n-\n-      val topicPartition = new TopicPartition(topic, partition)\n-\n-      // All sync replicas are in the ISR\n-      TestUtils.waitForBrokersInIsr(client, topicPartition, Set(broker1, broker2))\n-\n-      // Test reassignment with duplicate broker ids\n-      var future = client.alterPartitionReassignments(\n-        Map(topicPartition -> reassignmentEntry(Seq(broker4, broker5, broker5))).asJava\n-      ).all()\n-      JTestUtils.assertFutureThrows(future, classOf[InvalidReplicaAssignmentException])\n-\n-      // Test reassignment with invalid broker ids\n-      future = client.alterPartitionReassignments(\n-        Map(topicPartition -> reassignmentEntry(Seq(-1, broker3))).asJava\n-      ).all()\n-      JTestUtils.assertFutureThrows(future, classOf[InvalidReplicaAssignmentException])\n-\n-      // Test reassignment with extra broker ids\n-      future = client.alterPartitionReassignments(\n-        Map(topicPartition -> reassignmentEntry(Seq(6, broker2, broker3))).asJava\n-      ).all()\n-      JTestUtils.assertFutureThrows(future, classOf[InvalidReplicaAssignmentException])\n+  def testReplicaMoves(): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg5MjM0MA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389892340", "bodyText": "I changed the name to testRplicaDirectoryMoves", "author": "cmccabe", "createdAt": "2020-03-09T18:50:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTYyMTM2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTYzMDMzNg==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389630336", "bodyText": "Should we leave one integration test that still uses the ZK API?", "author": "stanislavkozlovski", "createdAt": "2020-03-09T12:31:52Z", "path": "core/src/test/scala/integration/kafka/admin/ReassignPartitionsIntegrationTest.scala", "diffHunk": "@@ -1,175 +1,485 @@\n-/**\n-  * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE\n-  * file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file\n-  * to You under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\n-  * License. You may obtain a copy of the License at\n-  *\n-  * http://www.apache.org/licenses/LICENSE-2.0\n-  *\n-  * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-  * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-  * specific language governing permissions and limitations under the License.\n-  */\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n package kafka.admin\n \n-import java.util.Optional\n+import java.io.Closeable\n+import java.util.{Collections, HashMap, List}\n \n-import kafka.admin.TopicCommand.ZookeeperTopicService\n+import kafka.admin.ReassignPartitionsCommand._\n import kafka.server.{KafkaConfig, KafkaServer}\n import kafka.utils.TestUtils\n-import kafka.zk.ZooKeeperTestHarness\n-import org.apache.kafka.clients.admin.{AdminClientConfig, NewPartitionReassignment, NewTopic, AdminClient => JAdminClient}\n+import kafka.zk.{KafkaZkClient, ZooKeeperTestHarness}\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, DescribeLogDirsResult, NewTopic}\n+import org.apache.kafka.clients.producer.ProducerRecord\n import org.apache.kafka.common.TopicPartition\n-import org.apache.kafka.common.errors.InvalidReplicaAssignmentException\n+import org.apache.kafka.common.config.ConfigResource\n import org.apache.kafka.common.network.ListenerName\n-import org.apache.kafka.test.{TestUtils => JTestUtils}\n-import org.junit.{After, Before, Test}\n+import org.apache.kafka.common.security.auth.SecurityProtocol\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.rules.Timeout\n+import org.junit.Assert.{assertEquals, assertFalse, assertTrue}\n+import org.junit.{After, Rule, Test}\n \n+import scala.collection.Map\n import scala.collection.JavaConverters._\n-import scala.collection.Seq\n-\n-class ReassignPartitionsIntegrationTest extends ZooKeeperTestHarness with RackAwareTest {\n-  import ReassignPartitionsIntegrationTest._\n-\n-  var servers: Seq[KafkaServer] = Seq()\n-  val broker1 = 0\n-  val broker2 = 1\n-  val broker3 = 2\n-  val broker4 = 3\n-  val broker5 = 4\n-  val broker6 = 5\n-  val rack = Map(\n-    broker1 -> \"rack1\",\n-    broker2 -> \"rack2\",\n-    broker3 -> \"rack2\",\n-    broker4 -> \"rack1\",\n-    broker5 -> \"rack3\",\n-    broker6 -> \"rack3\"\n-  )\n-\n-  @Before\n-  override def setUp(): Unit = {\n-    super.setUp()\n-\n-    val brokerConfigs = TestUtils.createBrokerConfigs(6, zkConnect, enableControlledShutdown = true)\n-    servers = brokerConfigs.map { config =>\n-      config.setProperty(KafkaConfig.RackProp, rack(config.getProperty(KafkaConfig.BrokerIdProp).toInt))\n-      config.setProperty(KafkaConfig.AutoLeaderRebalanceEnableProp, \"false\")\n-      config.setProperty(KafkaConfig.ControlledShutdownMaxRetriesProp, \"1\")\n-      config.setProperty(KafkaConfig.ControlledShutdownRetryBackoffMsProp, \"1000\")\n-      config.setProperty(KafkaConfig.ReplicaLagTimeMaxMsProp, \"1000\")\n-      TestUtils.createServer(KafkaConfig.fromProps(config))\n-    }\n+import scala.collection.{Seq, mutable}\n+\n+class ReassignPartitionsIntegrationTest extends ZooKeeperTestHarness {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQzOTYyMg==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r390439622", "bodyText": "added", "author": "cmccabe", "createdAt": "2020-03-10T16:20:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTYzMDMzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTYzNjc4NA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389636784", "bodyText": "nit: IntelliJ recommends Replace with getOrElse(key, defaultValue)", "author": "stanislavkozlovski", "createdAt": "2020-03-09T12:45:34Z", "path": "core/src/test/scala/integration/kafka/admin/ReassignPartitionsIntegrationTest.scala", "diffHunk": "@@ -1,175 +1,485 @@\n-/**\n-  * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE\n-  * file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file\n-  * to You under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\n-  * License. You may obtain a copy of the License at\n-  *\n-  * http://www.apache.org/licenses/LICENSE-2.0\n-  *\n-  * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-  * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-  * specific language governing permissions and limitations under the License.\n-  */\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n package kafka.admin\n \n-import java.util.Optional\n+import java.io.Closeable\n+import java.util.{Collections, HashMap, List}\n \n-import kafka.admin.TopicCommand.ZookeeperTopicService\n+import kafka.admin.ReassignPartitionsCommand._\n import kafka.server.{KafkaConfig, KafkaServer}\n import kafka.utils.TestUtils\n-import kafka.zk.ZooKeeperTestHarness\n-import org.apache.kafka.clients.admin.{AdminClientConfig, NewPartitionReassignment, NewTopic, AdminClient => JAdminClient}\n+import kafka.zk.{KafkaZkClient, ZooKeeperTestHarness}\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, DescribeLogDirsResult, NewTopic}\n+import org.apache.kafka.clients.producer.ProducerRecord\n import org.apache.kafka.common.TopicPartition\n-import org.apache.kafka.common.errors.InvalidReplicaAssignmentException\n+import org.apache.kafka.common.config.ConfigResource\n import org.apache.kafka.common.network.ListenerName\n-import org.apache.kafka.test.{TestUtils => JTestUtils}\n-import org.junit.{After, Before, Test}\n+import org.apache.kafka.common.security.auth.SecurityProtocol\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.rules.Timeout\n+import org.junit.Assert.{assertEquals, assertFalse, assertTrue}\n+import org.junit.{After, Rule, Test}\n \n+import scala.collection.Map\n import scala.collection.JavaConverters._\n-import scala.collection.Seq\n-\n-class ReassignPartitionsIntegrationTest extends ZooKeeperTestHarness with RackAwareTest {\n-  import ReassignPartitionsIntegrationTest._\n-\n-  var servers: Seq[KafkaServer] = Seq()\n-  val broker1 = 0\n-  val broker2 = 1\n-  val broker3 = 2\n-  val broker4 = 3\n-  val broker5 = 4\n-  val broker6 = 5\n-  val rack = Map(\n-    broker1 -> \"rack1\",\n-    broker2 -> \"rack2\",\n-    broker3 -> \"rack2\",\n-    broker4 -> \"rack1\",\n-    broker5 -> \"rack3\",\n-    broker6 -> \"rack3\"\n-  )\n-\n-  @Before\n-  override def setUp(): Unit = {\n-    super.setUp()\n-\n-    val brokerConfigs = TestUtils.createBrokerConfigs(6, zkConnect, enableControlledShutdown = true)\n-    servers = brokerConfigs.map { config =>\n-      config.setProperty(KafkaConfig.RackProp, rack(config.getProperty(KafkaConfig.BrokerIdProp).toInt))\n-      config.setProperty(KafkaConfig.AutoLeaderRebalanceEnableProp, \"false\")\n-      config.setProperty(KafkaConfig.ControlledShutdownMaxRetriesProp, \"1\")\n-      config.setProperty(KafkaConfig.ControlledShutdownRetryBackoffMsProp, \"1000\")\n-      config.setProperty(KafkaConfig.ReplicaLagTimeMaxMsProp, \"1000\")\n-      TestUtils.createServer(KafkaConfig.fromProps(config))\n-    }\n+import scala.collection.{Seq, mutable}\n+\n+class ReassignPartitionsIntegrationTest extends ZooKeeperTestHarness {\n+  @Rule\n+  def globalTimeout: Timeout = Timeout.millis(300000)\n+\n+  var cluster: ReassignPartitionsTestCluster = null\n+\n+  def generateConfigs: Seq[KafkaConfig] = {\n+    TestUtils.createBrokerConfigs(5, zkConnect).map(KafkaConfig.fromProps)\n   }\n \n   @After\n   override def tearDown(): Unit = {\n-    TestUtils.shutdownServers(servers)\n+    Utils.closeQuietly(cluster , \"ReassignPartitionsTestCluster\")\n     super.tearDown()\n   }\n \n+  val unthrottledBrokerConfigs =\n+    0.to(4).map {\n+      case brokerId => (brokerId, brokerLevelThrottles.map {\n+        case throttleName => (throttleName, -1L)\n+      }.toMap)\n+    }.toMap\n+\n+  /**\n+   * Test running a quick reassignment.\n+   */\n   @Test\n-  def testRackAwareReassign(): Unit = {\n-    val numPartitions = 18\n-    val replicationFactor = 3\n-\n-    // create a non rack aware assignment topic first\n-    val createOpts = new kafka.admin.TopicCommand.TopicCommandOptions(Array(\n-      \"--partitions\", numPartitions.toString,\n-      \"--replication-factor\", replicationFactor.toString,\n-      \"--disable-rack-aware\",\n-      \"--topic\", \"foo\"))\n-    new ZookeeperTopicService(zkClient).createTopic(createOpts)\n-\n-    val topicJson = \"\"\"{\"topics\": [{\"topic\": \"foo\"}], \"version\":1}\"\"\"\n-    val (proposedAssignment, currentAssignment) = ReassignPartitionsCommand.generateAssignment(zkClient,\n-      rack.keys.toSeq.sorted, topicJson, disableRackAware = false)\n-\n-    val assignment = proposedAssignment map { case (topicPartition, replicas) =>\n-      (topicPartition.partition, replicas)\n-    }\n-    checkReplicaDistribution(assignment, rack, rack.size, numPartitions, replicationFactor)\n+  def testReassignment(): Unit = {\n+    cluster = new ReassignPartitionsTestCluster(zkConnect)\n+    cluster.setup()\n+    val assignment = \"\"\"{\"version\":1,\"partitions\":\"\"\" +\n+      \"\"\"[{\"topic\":\"foo\",\"partition\":0,\"replicas\":[0,1,3],\"log_dirs\":[\"any\",\"any\",\"any\"]},\"\"\" +\n+      \"\"\"{\"topic\":\"bar\",\"partition\":0,\"replicas\":[3,2,0],\"log_dirs\":[\"any\",\"any\",\"any\"]}\"\"\" +\n+      \"\"\"]}\"\"\"\n+\n+    // Check that the assignment has not yet been started yet.\n+    val initialAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 2), Seq(0, 1, 3), true),\n+      new TopicPartition(\"bar\", 0) ->\n+        PartitionReassignmentState(Seq(3, 2, 1), Seq(3, 2, 0), true)\n+    )\n+    assertEquals((initialAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+    assertEquals((initialAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, false))\n+\n+    // Execute the assignment\n+    executeAssignment(cluster.adminClient, false, assignment)\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n+    val finalAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 3), Seq(0, 1, 3), true),\n+      new TopicPartition(\"bar\", 0) ->\n+        PartitionReassignmentState(Seq(3, 2, 0), Seq(3, 2, 0), true)\n+    )\n+\n+    // Wait for the assignment to complete\n+    TestUtils.waitUntilTrue(\n+      () => {\n+        // When using --zookeeper, we aren't able to see the new-style assignment\n+        assertFalse(runVerifyAssignment(zkClient, assignment, false)._2)\n+        finalAssignment.equals(runVerifyAssignment(cluster.adminClient, assignment, false)._1)\n+      }, \"Expected reassignment to complete\")\n+    assertEquals((finalAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, false))\n+    assertEquals((finalAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n   }\n \n+  /**\n+   * Test running a reassignment with the interBrokerThrottle set.\n+   */\n   @Test\n-  def testReassignPartition(): Unit = {\n-    TestUtils.resource(JAdminClient.create(createConfig(servers).asJava)) { client =>\n-      val topic = \"test-topic\"\n-      val partition = 0: Integer\n+  def testThrottledReassignment(): Unit = {\n+    cluster = new ReassignPartitionsTestCluster(zkConnect)\n+    cluster.setup()\n+    cluster.produceMessages(\"foo\", 0, 50)\n+    cluster.produceMessages(\"baz\", 2, 60)\n+    val assignment = \"\"\"{\"version\":1,\"partitions\":\"\"\" +\n+      \"\"\"[{\"topic\":\"foo\",\"partition\":0,\"replicas\":[0,3,2],\"log_dirs\":[\"any\",\"any\",\"any\"]},\"\"\" +\n+      \"\"\"{\"topic\":\"baz\",\"partition\":2,\"replicas\":[3,2,1],\"log_dirs\":[\"any\",\"any\",\"any\"]}\"\"\" +\n+      \"\"\"]}\"\"\"\n \n-      val partitionAssignment = Map(partition -> Seq(broker1: Integer, broker2:Integer).asJava).asJava\n-      val newTopic = new NewTopic(topic, partitionAssignment)\n-      client.createTopics(Seq(newTopic).asJava).all().get()\n+    // Check that the assignment has not yet been started yet.\n+    val initialAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 2), Seq(0, 3, 2), true),\n+      new TopicPartition(\"baz\", 2) ->\n+        PartitionReassignmentState(Seq(0, 2, 1), Seq(3, 2, 1), true)\n+    )\n+    assertEquals((initialAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+    assertEquals((initialAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, false))\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n \n-      val topicPartition = new TopicPartition(topic, partition)\n+    // Execute the assignment\n+    val interBrokerThrottle = 300000L\n+    executeAssignment(cluster.adminClient, false, assignment, interBrokerThrottle)\n \n-      // All sync replicas are in the ISR\n-      TestUtils.waitForBrokersInIsr(client, topicPartition, Set(broker1, broker2))\n+    val throttledConfigMap = Map[String, Long](\n+      brokerLevelLeaderThrottle -> interBrokerThrottle,\n+      brokerLevelFollowerThrottle -> interBrokerThrottle,\n+      brokerLevelReplicaThrottle -> -1L\n+    )\n+    val throttledBrokerConfigs = Map[Int, Map[String, Long]](\n+      0 -> throttledConfigMap,\n+      1 -> throttledConfigMap,\n+      2 -> throttledConfigMap,\n+      3 -> throttledConfigMap,\n+      4 -> unthrottledBrokerConfigs(4)\n+    )\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+    val finalAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 3, 2), Seq(0, 3, 2), true),\n+      new TopicPartition(\"baz\", 2) ->\n+        PartitionReassignmentState(Seq(3, 2, 1), Seq(3, 2, 1), true)\n+    )\n+\n+    // Wait for the assignment to complete\n+    TestUtils.waitUntilTrue(\n+      () => {\n+        // Check the reassignment status.\n+        val (partStates, partsOngoing, _, _) =\n+          runVerifyAssignment(cluster.adminClient, assignment, true)\n+        if (!partsOngoing) {\n+          assertEquals(finalAssignment, partStates)\n+          true\n+        } else {\n+          assertTrue(\"Expected at least one partition reassignment to be ongoing when \" +\n+            \"partsOngoing = true.  partStates = \" + partStates,\n+            !partStates.forall(_._2.done))\n+          assertEquals(Seq(0, 3, 2),\n+            partStates.get(new TopicPartition(\"foo\", 0)).get.targetReplicas)\n+          assertEquals(Seq(3, 2, 1),\n+            partStates.get(new TopicPartition(\"baz\", 2)).get.targetReplicas)\n+          logger.info(s\"Current partition states: ${partStates}\")\n+          waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+          false\n+        }\n+      }, \"Expected reassignment to complete.\")\n+    assertEquals((finalAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, true))\n+    assertEquals((finalAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, true))\n+    // The throttles should still have been preserved, since we ran with --preserve-throttles\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+    // Now remove the throttles.\n+    assertEquals((finalAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+    waitForBrokerLevelThrottles(unthrottledBrokerConfigs)\n+  }\n \n-      // Reassign replicas to different brokers\n-      client.alterPartitionReassignments(\n-        Map(topicPartition -> reassignmentEntry(Seq(broker3, broker4))).asJava\n-      ).all().get()\n+  /**\n+   * Test running a reassignment and then cancelling it.\n+   */\n+  @Test\n+  def testCancellation(): Unit = {\n+    cluster = new ReassignPartitionsTestCluster(zkConnect)\n+    cluster.setup()\n+    cluster.produceMessages(\"foo\", 0, 60)\n+    cluster.produceMessages(\"baz\", 1, 80)\n+    val assignment = \"\"\"{\"version\":1,\"partitions\":\"\"\" +\n+      \"\"\"[{\"topic\":\"foo\",\"partition\":0,\"replicas\":[0,1,3],\"log_dirs\":[\"any\",\"any\",\"any\"]},\"\"\" +\n+      \"\"\"{\"topic\":\"baz\",\"partition\":1,\"replicas\":[0,2,3],\"log_dirs\":[\"any\",\"any\",\"any\"]}\"\"\" +\n+      \"\"\"]}\"\"\"\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n+    val interBrokerThrottle = 100L\n+    executeAssignment(cluster.adminClient, false, assignment, interBrokerThrottle)\n+    val throttledConfigMap = Map[String, Long](\n+      brokerLevelLeaderThrottle -> interBrokerThrottle,\n+      brokerLevelFollowerThrottle -> interBrokerThrottle,\n+      brokerLevelReplicaThrottle -> -1L\n+    )\n+    val throttledBrokerConfigs = Map[Int, Map[String, Long]](\n+      0 -> throttledConfigMap,\n+      1 -> throttledConfigMap,\n+      2 -> throttledConfigMap,\n+      3 -> throttledConfigMap,\n+      4 -> unthrottledBrokerConfigs(4)\n+    )\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+    // Verify that the reassignment is running.  The very low throttle should keep it\n+    // from completing before this runs.\n+    assertEquals((Map(new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 3, 2), Seq(0, 1, 3), false),\n+        new TopicPartition(\"baz\", 1) ->\n+          PartitionReassignmentState(Seq(0, 2, 3, 1), Seq(0, 2, 3), false)),\n+      true, Map(), false),\n+      runVerifyAssignment(cluster.adminClient, assignment, true))\n+    // Cancel the reassignment.\n+    assertEquals((Set(\n+        new TopicPartition(\"foo\", 0),\n+        new TopicPartition(\"baz\", 1)\n+      ), Set()), runCancelAssignment(cluster.adminClient, assignment, true))\n+    // Broker throttles are still active because we passed --preserve-throttles\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+    // Cancelling the reassignment again should reveal nothing to cancel.\n+    assertEquals((Set(), Set()), runCancelAssignment(cluster.adminClient, assignment, false))\n+    // This time, the broker throttles were removed.\n+    waitForBrokerLevelThrottles(unthrottledBrokerConfigs)\n+  }\n \n-      waitForAllReassignmentsToComplete(client)\n+  private def waitForBrokerLevelThrottles(targetThrottles: Map[Int, Map[String, Long]]): Unit = {\n+    var curThrottles: Map[Int, Map[String, Long]] = Map.empty\n+    TestUtils.waitUntilTrue(() => {\n+      curThrottles = describeBrokerLevelThrottles(targetThrottles.keySet.toSeq)\n+      targetThrottles.equals(curThrottles)\n+    }, s\"timed out waiting for broker throttle to become ${targetThrottles}.  \" +\n+      s\"Latest throttles were ${curThrottles}\", pause = 25)\n+  }\n \n-      // Metadata info is eventually consistent wait for update\n-      TestUtils.waitForReplicasAssigned(client, topicPartition, Seq(broker3, broker4))\n-      // All sync replicas are in the ISR\n-      TestUtils.waitForBrokersInIsr(client, topicPartition, Set(broker3, broker4))\n-    }\n+  /**\n+   * Describe the broker-level throttles in the cluster.\n+   *\n+   * @return                A map whose keys are broker IDs and whose values are throttle\n+   *                        information.  The nested maps are keyed on throttle name.\n+   */\n+  private def describeBrokerLevelThrottles(brokerIds: Seq[Int]): Map[Int, Map[String, Long]] = {\n+    brokerIds.map {\n+      case brokerId =>\n+        val props = zkClient.getEntityConfigs(\"brokers\", brokerId.toString)\n+        (brokerId, brokerLevelThrottles.map {\n+          case throttleName => (throttleName,\n+            props.getOrDefault(throttleName, \"-1\").asInstanceOf[String].toLong)\n+        }.toMap)\n+    }.toMap\n   }\n \n+  /**\n+   * Test moving partitions between directories.\n+   */\n   @Test\n-  def testInvalidReplicaIds(): Unit = {\n-    TestUtils.resource(JAdminClient.create(createConfig(servers).asJava)) { client =>\n-      val topic = \"test-topic\"\n-      val partition = 0: Integer\n-\n-      val partitionAssignment = Map(partition -> Seq(broker1: Integer, broker2: Integer).asJava).asJava\n-      val newTopic = new NewTopic(topic, partitionAssignment)\n-      client.createTopics(Seq(newTopic).asJava).all().get()\n-\n-      val topicPartition = new TopicPartition(topic, partition)\n-\n-      // All sync replicas are in the ISR\n-      TestUtils.waitForBrokersInIsr(client, topicPartition, Set(broker1, broker2))\n-\n-      // Test reassignment with duplicate broker ids\n-      var future = client.alterPartitionReassignments(\n-        Map(topicPartition -> reassignmentEntry(Seq(broker4, broker5, broker5))).asJava\n-      ).all()\n-      JTestUtils.assertFutureThrows(future, classOf[InvalidReplicaAssignmentException])\n-\n-      // Test reassignment with invalid broker ids\n-      future = client.alterPartitionReassignments(\n-        Map(topicPartition -> reassignmentEntry(Seq(-1, broker3))).asJava\n-      ).all()\n-      JTestUtils.assertFutureThrows(future, classOf[InvalidReplicaAssignmentException])\n-\n-      // Test reassignment with extra broker ids\n-      future = client.alterPartitionReassignments(\n-        Map(topicPartition -> reassignmentEntry(Seq(6, broker2, broker3))).asJava\n-      ).all()\n-      JTestUtils.assertFutureThrows(future, classOf[InvalidReplicaAssignmentException])\n+  def testReplicaMoves(): Unit = {\n+    cluster = new ReassignPartitionsTestCluster(zkConnect)\n+    cluster.setup()\n+    cluster.produceMessages(\"foo\", 0, 7000)\n+    cluster.produceMessages(\"baz\", 1, 6000)\n+\n+    val result0 = cluster.adminClient.describeLogDirs(\n+        0.to(4).map(_.asInstanceOf[Integer]).asJavaCollection)\n+    val info0 = new BrokerDirs(result0, 0)\n+    assertTrue(info0.futureLogDirs.isEmpty)\n+    assertEquals(Set(new TopicPartition(\"foo\", 0),\n+        new TopicPartition(\"baz\", 0),\n+        new TopicPartition(\"baz\", 1),\n+        new TopicPartition(\"baz\", 2)),\n+      info0.curLogDirs.keySet)\n+    val curFoo1Dir = info0.curLogDirs.get(new TopicPartition(\"foo\", 0)).getOrElse(\"\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTYzNzA5Nw==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389637097", "bodyText": "nit: IntelliJ recommends Replace with .(key)", "author": "stanislavkozlovski", "createdAt": "2020-03-09T12:46:13Z", "path": "core/src/test/scala/integration/kafka/admin/ReassignPartitionsIntegrationTest.scala", "diffHunk": "@@ -1,175 +1,485 @@\n-/**\n-  * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE\n-  * file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file\n-  * to You under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\n-  * License. You may obtain a copy of the License at\n-  *\n-  * http://www.apache.org/licenses/LICENSE-2.0\n-  *\n-  * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-  * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-  * specific language governing permissions and limitations under the License.\n-  */\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n package kafka.admin\n \n-import java.util.Optional\n+import java.io.Closeable\n+import java.util.{Collections, HashMap, List}\n \n-import kafka.admin.TopicCommand.ZookeeperTopicService\n+import kafka.admin.ReassignPartitionsCommand._\n import kafka.server.{KafkaConfig, KafkaServer}\n import kafka.utils.TestUtils\n-import kafka.zk.ZooKeeperTestHarness\n-import org.apache.kafka.clients.admin.{AdminClientConfig, NewPartitionReassignment, NewTopic, AdminClient => JAdminClient}\n+import kafka.zk.{KafkaZkClient, ZooKeeperTestHarness}\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, DescribeLogDirsResult, NewTopic}\n+import org.apache.kafka.clients.producer.ProducerRecord\n import org.apache.kafka.common.TopicPartition\n-import org.apache.kafka.common.errors.InvalidReplicaAssignmentException\n+import org.apache.kafka.common.config.ConfigResource\n import org.apache.kafka.common.network.ListenerName\n-import org.apache.kafka.test.{TestUtils => JTestUtils}\n-import org.junit.{After, Before, Test}\n+import org.apache.kafka.common.security.auth.SecurityProtocol\n+import org.apache.kafka.common.utils.Utils\n+import org.junit.rules.Timeout\n+import org.junit.Assert.{assertEquals, assertFalse, assertTrue}\n+import org.junit.{After, Rule, Test}\n \n+import scala.collection.Map\n import scala.collection.JavaConverters._\n-import scala.collection.Seq\n-\n-class ReassignPartitionsIntegrationTest extends ZooKeeperTestHarness with RackAwareTest {\n-  import ReassignPartitionsIntegrationTest._\n-\n-  var servers: Seq[KafkaServer] = Seq()\n-  val broker1 = 0\n-  val broker2 = 1\n-  val broker3 = 2\n-  val broker4 = 3\n-  val broker5 = 4\n-  val broker6 = 5\n-  val rack = Map(\n-    broker1 -> \"rack1\",\n-    broker2 -> \"rack2\",\n-    broker3 -> \"rack2\",\n-    broker4 -> \"rack1\",\n-    broker5 -> \"rack3\",\n-    broker6 -> \"rack3\"\n-  )\n-\n-  @Before\n-  override def setUp(): Unit = {\n-    super.setUp()\n-\n-    val brokerConfigs = TestUtils.createBrokerConfigs(6, zkConnect, enableControlledShutdown = true)\n-    servers = brokerConfigs.map { config =>\n-      config.setProperty(KafkaConfig.RackProp, rack(config.getProperty(KafkaConfig.BrokerIdProp).toInt))\n-      config.setProperty(KafkaConfig.AutoLeaderRebalanceEnableProp, \"false\")\n-      config.setProperty(KafkaConfig.ControlledShutdownMaxRetriesProp, \"1\")\n-      config.setProperty(KafkaConfig.ControlledShutdownRetryBackoffMsProp, \"1000\")\n-      config.setProperty(KafkaConfig.ReplicaLagTimeMaxMsProp, \"1000\")\n-      TestUtils.createServer(KafkaConfig.fromProps(config))\n-    }\n+import scala.collection.{Seq, mutable}\n+\n+class ReassignPartitionsIntegrationTest extends ZooKeeperTestHarness {\n+  @Rule\n+  def globalTimeout: Timeout = Timeout.millis(300000)\n+\n+  var cluster: ReassignPartitionsTestCluster = null\n+\n+  def generateConfigs: Seq[KafkaConfig] = {\n+    TestUtils.createBrokerConfigs(5, zkConnect).map(KafkaConfig.fromProps)\n   }\n \n   @After\n   override def tearDown(): Unit = {\n-    TestUtils.shutdownServers(servers)\n+    Utils.closeQuietly(cluster , \"ReassignPartitionsTestCluster\")\n     super.tearDown()\n   }\n \n+  val unthrottledBrokerConfigs =\n+    0.to(4).map {\n+      case brokerId => (brokerId, brokerLevelThrottles.map {\n+        case throttleName => (throttleName, -1L)\n+      }.toMap)\n+    }.toMap\n+\n+  /**\n+   * Test running a quick reassignment.\n+   */\n   @Test\n-  def testRackAwareReassign(): Unit = {\n-    val numPartitions = 18\n-    val replicationFactor = 3\n-\n-    // create a non rack aware assignment topic first\n-    val createOpts = new kafka.admin.TopicCommand.TopicCommandOptions(Array(\n-      \"--partitions\", numPartitions.toString,\n-      \"--replication-factor\", replicationFactor.toString,\n-      \"--disable-rack-aware\",\n-      \"--topic\", \"foo\"))\n-    new ZookeeperTopicService(zkClient).createTopic(createOpts)\n-\n-    val topicJson = \"\"\"{\"topics\": [{\"topic\": \"foo\"}], \"version\":1}\"\"\"\n-    val (proposedAssignment, currentAssignment) = ReassignPartitionsCommand.generateAssignment(zkClient,\n-      rack.keys.toSeq.sorted, topicJson, disableRackAware = false)\n-\n-    val assignment = proposedAssignment map { case (topicPartition, replicas) =>\n-      (topicPartition.partition, replicas)\n-    }\n-    checkReplicaDistribution(assignment, rack, rack.size, numPartitions, replicationFactor)\n+  def testReassignment(): Unit = {\n+    cluster = new ReassignPartitionsTestCluster(zkConnect)\n+    cluster.setup()\n+    val assignment = \"\"\"{\"version\":1,\"partitions\":\"\"\" +\n+      \"\"\"[{\"topic\":\"foo\",\"partition\":0,\"replicas\":[0,1,3],\"log_dirs\":[\"any\",\"any\",\"any\"]},\"\"\" +\n+      \"\"\"{\"topic\":\"bar\",\"partition\":0,\"replicas\":[3,2,0],\"log_dirs\":[\"any\",\"any\",\"any\"]}\"\"\" +\n+      \"\"\"]}\"\"\"\n+\n+    // Check that the assignment has not yet been started yet.\n+    val initialAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 2), Seq(0, 1, 3), true),\n+      new TopicPartition(\"bar\", 0) ->\n+        PartitionReassignmentState(Seq(3, 2, 1), Seq(3, 2, 0), true)\n+    )\n+    assertEquals((initialAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+    assertEquals((initialAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, false))\n+\n+    // Execute the assignment\n+    executeAssignment(cluster.adminClient, false, assignment)\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n+    val finalAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 3), Seq(0, 1, 3), true),\n+      new TopicPartition(\"bar\", 0) ->\n+        PartitionReassignmentState(Seq(3, 2, 0), Seq(3, 2, 0), true)\n+    )\n+\n+    // Wait for the assignment to complete\n+    TestUtils.waitUntilTrue(\n+      () => {\n+        // When using --zookeeper, we aren't able to see the new-style assignment\n+        assertFalse(runVerifyAssignment(zkClient, assignment, false)._2)\n+        finalAssignment.equals(runVerifyAssignment(cluster.adminClient, assignment, false)._1)\n+      }, \"Expected reassignment to complete\")\n+    assertEquals((finalAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, false))\n+    assertEquals((finalAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n   }\n \n+  /**\n+   * Test running a reassignment with the interBrokerThrottle set.\n+   */\n   @Test\n-  def testReassignPartition(): Unit = {\n-    TestUtils.resource(JAdminClient.create(createConfig(servers).asJava)) { client =>\n-      val topic = \"test-topic\"\n-      val partition = 0: Integer\n+  def testThrottledReassignment(): Unit = {\n+    cluster = new ReassignPartitionsTestCluster(zkConnect)\n+    cluster.setup()\n+    cluster.produceMessages(\"foo\", 0, 50)\n+    cluster.produceMessages(\"baz\", 2, 60)\n+    val assignment = \"\"\"{\"version\":1,\"partitions\":\"\"\" +\n+      \"\"\"[{\"topic\":\"foo\",\"partition\":0,\"replicas\":[0,3,2],\"log_dirs\":[\"any\",\"any\",\"any\"]},\"\"\" +\n+      \"\"\"{\"topic\":\"baz\",\"partition\":2,\"replicas\":[3,2,1],\"log_dirs\":[\"any\",\"any\",\"any\"]}\"\"\" +\n+      \"\"\"]}\"\"\"\n \n-      val partitionAssignment = Map(partition -> Seq(broker1: Integer, broker2:Integer).asJava).asJava\n-      val newTopic = new NewTopic(topic, partitionAssignment)\n-      client.createTopics(Seq(newTopic).asJava).all().get()\n+    // Check that the assignment has not yet been started yet.\n+    val initialAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 1, 2), Seq(0, 3, 2), true),\n+      new TopicPartition(\"baz\", 2) ->\n+        PartitionReassignmentState(Seq(0, 2, 1), Seq(3, 2, 1), true)\n+    )\n+    assertEquals((initialAssignment, false, Map.empty, false),\n+      runVerifyAssignment(cluster.adminClient, assignment, false))\n+    assertEquals((initialAssignment, false),\n+      runVerifyAssignment(zkClient, assignment, false))\n+    assertEquals(unthrottledBrokerConfigs,\n+      describeBrokerLevelThrottles(unthrottledBrokerConfigs.keySet.toSeq))\n \n-      val topicPartition = new TopicPartition(topic, partition)\n+    // Execute the assignment\n+    val interBrokerThrottle = 300000L\n+    executeAssignment(cluster.adminClient, false, assignment, interBrokerThrottle)\n \n-      // All sync replicas are in the ISR\n-      TestUtils.waitForBrokersInIsr(client, topicPartition, Set(broker1, broker2))\n+    val throttledConfigMap = Map[String, Long](\n+      brokerLevelLeaderThrottle -> interBrokerThrottle,\n+      brokerLevelFollowerThrottle -> interBrokerThrottle,\n+      brokerLevelReplicaThrottle -> -1L\n+    )\n+    val throttledBrokerConfigs = Map[Int, Map[String, Long]](\n+      0 -> throttledConfigMap,\n+      1 -> throttledConfigMap,\n+      2 -> throttledConfigMap,\n+      3 -> throttledConfigMap,\n+      4 -> unthrottledBrokerConfigs(4)\n+    )\n+    waitForBrokerLevelThrottles(throttledBrokerConfigs)\n+    val finalAssignment = Map(\n+      new TopicPartition(\"foo\", 0) ->\n+        PartitionReassignmentState(Seq(0, 3, 2), Seq(0, 3, 2), true),\n+      new TopicPartition(\"baz\", 2) ->\n+        PartitionReassignmentState(Seq(3, 2, 1), Seq(3, 2, 1), true)\n+    )\n+\n+    // Wait for the assignment to complete\n+    TestUtils.waitUntilTrue(\n+      () => {\n+        // Check the reassignment status.\n+        val (partStates, partsOngoing, _, _) =\n+          runVerifyAssignment(cluster.adminClient, assignment, true)\n+        if (!partsOngoing) {\n+          assertEquals(finalAssignment, partStates)\n+          true\n+        } else {\n+          assertTrue(\"Expected at least one partition reassignment to be ongoing when \" +\n+            \"partsOngoing = true.  partStates = \" + partStates,\n+            !partStates.forall(_._2.done))\n+          assertEquals(Seq(0, 3, 2),\n+            partStates.get(new TopicPartition(\"foo\", 0)).get.targetReplicas)\n+          assertEquals(Seq(3, 2, 1),\n+            partStates.get(new TopicPartition(\"baz\", 2)).get.targetReplicas)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY2Njk1Mw==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389666953", "bodyText": "nit: val is redundant for case classes as their values are immutable by default", "author": "stanislavkozlovski", "createdAt": "2020-03-09T13:31:38Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1521 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(val sources: mutable.Set[Int],", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY2ODQwNw==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389668407", "bodyText": "Should this be False if... ?", "author": "stanislavkozlovski", "createdAt": "2020-03-09T13:32:55Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1521 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(val sources: mutable.Set[Int],\n+                                  val destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is still progressing.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg5NDk0OQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389894949", "bodyText": "Fixed", "author": "cmccabe", "createdAt": "2020-03-09T18:54:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY2ODQwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY2OTI0OQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389669249", "bodyText": "Should this be False if... ?", "author": "stanislavkozlovski", "createdAt": "2020-03-09T13:33:40Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1521 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(val sources: mutable.Set[Int],\n+                                  val destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is still progressing.\n+   */\n+  sealed case class PartitionReassignmentState(val currentReplicas: Seq[Int],\n+                                               val targetReplicas: Seq[Int],\n+                                               val done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {\n+    /**\n+     * True if the move is still progressing.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg5NTA3Nw==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389895077", "bodyText": "Fixed", "author": "cmccabe", "createdAt": "2020-03-09T18:55:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY2OTI0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY3OTExMw==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389679113", "bodyText": "nit: typo missng", "author": "stanislavkozlovski", "createdAt": "2020-03-09T13:42:14Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1521 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(val sources: mutable.Set[Int],\n+                                  val destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is still progressing.\n+   */\n+  sealed case class PartitionReassignmentState(val currentReplicas: Seq[Int],\n+                                               val targetReplicas: Seq[Int],\n+                                               val done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {\n+    /**\n+     * True if the move is still progressing.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica move state where the source log directory is missng.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY4MDc2Ng==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389680766", "bodyText": "What do you think we call this LogDirMoveState or something similar? ReplicaMove can be confused with a normal replica reassignment", "author": "stanislavkozlovski", "createdAt": "2020-03-09T13:43:37Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1521 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(val sources: mutable.Set[Int],\n+                                  val destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is still progressing.\n+   */\n+  sealed case class PartitionReassignmentState(val currentReplicas: Seq[Int],\n+                                               val targetReplicas: Seq[Int],\n+                                               val done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY4MjQyMA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389682420", "bodyText": "Should this be true given that no move is in progress, or should we update the docstring for the done variable to mention that error states are false", "author": "stanislavkozlovski", "createdAt": "2020-03-09T13:45:08Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1521 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(val sources: mutable.Set[Int],\n+                                  val destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is still progressing.\n+   */\n+  sealed case class PartitionReassignmentState(val currentReplicas: Seq[Int],\n+                                               val targetReplicas: Seq[Int],\n+                                               val done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {\n+    /**\n+     * True if the move is still progressing.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica move state where the source log directory is missng.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg5NjQxNg==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389896416", "bodyText": "As a practical matter, we probably don't want to remove the throttles in --verify if the log directory move is screwed up (because of an offline log directory or similar).  I will update the docstring.", "author": "cmccabe", "createdAt": "2020-03-09T18:57:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY4MjQyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY4Nzc4MQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389687781", "bodyText": "Are we going to add --show or will we keep using--list? In any case, this comment seems leftover", "author": "stanislavkozlovski", "createdAt": "2020-03-09T13:49:45Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1521 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(val sources: mutable.Set[Int],\n+                                  val destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is still progressing.\n+   */\n+  sealed case class PartitionReassignmentState(val currentReplicas: Seq[Int],\n+                                               val targetReplicas: Seq[Int],\n+                                               val done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {\n+    /**\n+     * True if the move is still progressing.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica move state where the source log directory is missng.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(val currentLogDir: String,\n+                                    val targetLogDir: String,\n+                                    val futureLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(val currentLogDir: String,\n+                                       val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        // Use AdminClient to manage reassignments.\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        // TODO: add --show and --cancel", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg5NjcxOQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389896719", "bodyText": "yes, this is left over.  removed", "author": "cmccabe", "createdAt": "2020-03-09T18:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY4Nzc4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg5NjkxOA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389896918", "bodyText": "I went with --list since it's more consistent with our other commands", "author": "cmccabe", "createdAt": "2020-03-09T18:58:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY4Nzc4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY5Mjg0NA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389692844", "bodyText": "nit: .nonEmpty ?", "author": "stanislavkozlovski", "createdAt": "2020-03-09T13:54:13Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1521 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(val sources: mutable.Set[Int],\n+                                  val destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is still progressing.\n+   */\n+  sealed case class PartitionReassignmentState(val currentReplicas: Seq[Int],\n+                                               val targetReplicas: Seq[Int],\n+                                               val done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {\n+    /**\n+     * True if the move is still progressing.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica move state where the source log directory is missng.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(val currentLogDir: String,\n+                                    val targetLogDir: String,\n+                                    val futureLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(val currentLogDir: String,\n+                                       val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        // Use AdminClient to manage reassignments.\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        // TODO: add --show and --cancel\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(adminClient,\n+            opts.options.has(opts.additionalOpt),\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt),\n+            opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.cancelOpt)) {\n+          cancelAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.listOpt)) {\n+          showReassignments(adminClient)\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      } else {\n+        // Use the legacy ZooKeeper client to manage reassignments.\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt))\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n-    } else {\n-      None\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states, whether there are ongoing\n+   *                              reassignments, replica move states, and whether there are\n+   *                              ongoing replica moves.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : (Map[TopicPartition, PartitionReassignmentState], Boolean,\n+                         Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetReplicas)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    (partStates, partsOngoing, moveStates, movesOngoing)\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states and whether there are ongoing\n+   *                              legacy reassignments.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    if (!targetReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    (partStates, partsOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      case topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n+        }\n       }\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    bld.mkString(System.lineSeparator())\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n-      }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n         }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val moveStates = findReplicaMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findReplicaMoveStates(adminClient: Admin,\n+                            targetMoves: Map[TopicPartitionReplica, String])\n+                            : Map[TopicPartitionReplica, ReplicaMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: ReplicaMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingLogDirMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingReplicaMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, ReplicaMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    allBrokerIds.foreach {\n+      case brokerId => configOps.put(\n+        new ConfigResource(ConfigResource.Type.BROKER, brokerId.toString),\n+        brokerLevelThrottles.map(throttle => new AlterConfigOp(\n+          new ConfigEntry(throttle, null), OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   */\n+  def clearBrokerLevelThrottles(zkClient: KafkaZkClient): Unit = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val brokerMetadatas = adminZkClient.getBrokerMetadatas(rackAwareMode, Some(brokerListToReassign))\n+    for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n+      if (!brokerLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n+      }\n+    }\n+  }\n \n-    val partitionsToBeReassigned = mutable.Map[TopicPartition, Seq[Int]]()\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    topics.foreach {\n+      topicName => configOps.put(\n+        new ConfigResource(ConfigResource.Type.TOPIC, topicName),\n+        topicLevelThrottles.map(throttle => new AlterConfigOp(new ConfigEntry(throttle, null),\n+          OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(zkClient: KafkaZkClient, topics: Set[String]): Unit = {\n+    val adminZkClient = new AdminZkClient(zkClient)\n+    for (topic <- topics) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n+      if (!topicLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeTopicConfig(topic, configs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --generate command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(adminClient: Admin,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = getReplicaAssignmentForTopics(adminClient, topicsToReassign)\n+    val brokerMetadatas = if (enableRackAwareness) {\n+      getBrokerRackInformation(adminClient, brokersToReassign)\n+    } else {\n+      brokersToReassign.map(new BrokerMetadata(_, None))\n+    }\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * The legacy entry point for the --generate command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(zkClient: KafkaZkClient,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+    val brokerMetadatas = if (enableRackAwareness) {\n+      getBrokerRackInformation(zkClient, brokersToReassign)\n+    } else {\n+      brokersToReassign.map(new BrokerMetadata(_, None))\n+    }\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * Calculate the new partition assignments to suggest in --generate.\n+   *\n+   * @param currentAssignment  The current partition assignments.\n+   * @param brokerMetadatas    The rack information for each broker.\n+   *\n+   * @return                   A map from partitions to the proposed assignments for each.\n+   */\n+  def calculateAssignment(currentAssignment: Map[TopicPartition, Seq[Int]],\n+                          brokerMetadatas: Seq[BrokerMetadata])\n+                          : Map[TopicPartition, Seq[Int]] = {\n+    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n+    val proposedAssignments = mutable.Map[TopicPartition, Seq[Int]]()\n     groupedByTopic.foreach { case (topic, assignment) =>\n       val (_, replicas) = assignment.head\n-      val assignedReplicas = AdminUtils.assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n-      partitionsToBeReassigned ++= assignedReplicas.map { case (partition, replicas) =>\n+      val assignedReplicas = AdminUtils.\n+        assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n+      proposedAssignments ++= assignedReplicas.map { case (partition, replicas) =>\n         new TopicPartition(topic, partition) -> replicas\n       }\n     }\n-    (partitionsToBeReassigned, currentAssignment)\n+    proposedAssignments\n+  }\n+\n+  /**\n+   * Get the current replica assignments for some topics.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics          The topics to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForTopics(adminClient: Admin,\n+                                    topics: Seq[String])\n+                                    : Map[TopicPartition, Seq[Int]] = {\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.map {\n+          info => (new TopicPartition(topicName, info.partition()),\n+            info.replicas().asScala.map(_.id()))\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val reassignmentJsonFile =  opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val reassignmentJsonString = Utils.readFileAsString(reassignmentJsonFile)\n-    val interBrokerThrottle = opts.options.valueOf(opts.interBrokerThrottleOpt)\n-    val replicaAlterLogDirsThrottle = opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt)\n-    val timeoutMs = opts.options.valueOf(opts.timeoutOpt)\n-    executeAssignment(zkClient, adminClientOpt, reassignmentJsonString, Throttle(interBrokerThrottle, replicaAlterLogDirsThrottle), timeoutMs)\n+  /**\n+   * Get the current replica assignments for some partitions.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param partitions      The partitions to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForPartitions(adminClient: Admin,\n+                                        partitions: Set[TopicPartition])\n+                                        : Map[TopicPartition, Seq[Int]] = {\n+    val topics = new mutable.HashSet[String]()\n+    partitions.foreach(topics += _.topic)\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.flatMap {\n+          info => if (partitions.contains(new TopicPartition(topicName, info.partition()))) {\n+            Some(new TopicPartition(topicName, info.partition()),\n+                info.replicas().asScala.map(_.id()))\n+          } else {\n+            None\n+          }\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], reassignmentJsonString: String, throttle: Throttle, timeoutMs: Long = 10000L): Unit = {\n-    val (partitionAssignment, replicaAssignment) = parseAndValidate(zkClient, reassignmentJsonString)\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param adminClient    The AdminClient object.\n+   * @param brokers        The brokers to gather metadata about.\n+   *\n+   * @return               The metadata for each broker.\n+   */\n+  def getBrokerRackInformation(adminClient: Admin,\n+                               brokers: Seq[Int]): Seq[BrokerMetadata] = {\n+    val brokerSet = brokers.toSet\n+    val results = adminClient.describeCluster().nodes().get().asScala.flatMap(\n+      node => if (!brokerSet.contains(node.id())) {\n+        None\n+      } else {\n+        if (node.rack() != null) {\n+          Some(new BrokerMetadata(node.id(), Some(node.rack())))\n+        } else {\n+          Some(new BrokerMetadata(node.id(), None))\n+        }\n+      }\n+    ).toSeq\n+    val numRackless = results.count(_.rack.isEmpty)\n+    if ((numRackless != 0) && (numRackless != results.size)) {\n+      throw new AdminOperationException(\"Not all brokers have rack information. Add \" +\n+        \"--disable-rack-aware in command line to make replica assignment without rack \" +\n+        \"information.\")\n+    }\n+    results\n+  }\n+\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param zkClient       The ZooKeeper client to use.\n+   * @param brokers        The brokers to gather metadata about.\n+   *\n+   * @return               The metadata for each broker.\n+   */\n+  def getBrokerRackInformation(zkClient: KafkaZkClient,\n+                               brokers: Seq[Int]): Seq[BrokerMetadata] = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val reassignPartitionsCommand = new ReassignPartitionsCommand(zkClient, adminClientOpt, partitionAssignment.toMap, replicaAssignment, adminZkClient)\n+    adminZkClient.getBrokerMetadatas(RackAwareMode.Enforced, Some(brokers))\n+  }\n+\n+  /**\n+   * Parse and validate data gathered from the command-line for --generate\n+   * In particular, we parse the JSON and validate that duplicate brokers and\n+   * topics don't appear.\n+   *\n+   * @param reassignmentJson       The JSON passed to --generate .\n+   * @param brokerList             A list of brokers passed to --generate.\n+   *\n+   * @return                       A tuple of brokers to reassign, topics to reassign\n+   */\n+  def parseGenerateAssignmentArgs(reassignmentJson: String,\n+                                  brokerList: String): (Seq[Int], Seq[String]) = {\n+    val brokerListToReassign = brokerList.split(',').map(_.toInt)\n+    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n+    if (duplicateReassignments.nonEmpty)\n+      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".\n+        format(duplicateReassignments.mkString(\",\")))\n+    val topicsToReassign = parseTopicsData(reassignmentJson)\n+    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n+    if (duplicateTopicsToReassign.nonEmpty)\n+      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".\n+        format(duplicateTopicsToReassign.mkString(\",\")))\n+    (brokerListToReassign, topicsToReassign)\n+  }\n+\n+  /**\n+   * The entry point for the --execute and --execute-additional commands.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param additional                  Whether --additional was passed.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   * @param replicaAlterLogDirsThrottle The replica throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeAssignment(adminClient: Admin,\n+                        additional: Boolean,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long = -1L,\n+                        replicaAlterLogDirsThrottle: Long = -1L,\n+                        timeoutMs: Long = 10000L,\n+                        time: Time = Time.SYSTEM): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    // If there is an existing assignment, check for --additional before proceeding.\n+    // This helps avoid surprising users.\n+    if (!additional && !currentReassignments.isEmpty) {\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n+    }\n+    verifyBrokerIds(adminClient, proposedParts.values.flatten.toSet)\n+    val currentParts = getReplicaAssignmentForPartitions(adminClient, proposedParts.keySet.toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+    if (interBrokerThrottle >= 0 || replicaAlterLogDirsThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(currentReassignments, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(adminClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      val movingBrokers = calculateMovingBrokers(proposedReplicas.keySet.toSet)\n+      modifyBrokerThrottles(adminClient,\n+        reassigningBrokers, interBrokerThrottle,\n+        movingBrokers, replicaAlterLogDirsThrottle)\n+      if (interBrokerThrottle >= 0) {\n+        println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+      }\n+      if (replicaAlterLogDirsThrottle >= 0) {\n+        println(s\"The replica-alter-dir throttle limit was set to ${replicaAlterLogDirsThrottle} B/s\")\n+      }\n+    }\n+\n+    // Execute the partition reassignments.\n+    val errors = alterPartitionReassignments(adminClient, proposedParts)\n+    if (!errors.isEmpty) {\n+      throw new TerseReassignmentFailureException(\n+        \"Error reassigning partition(s):%n%s\".format(\n+          errors.keySet.toBuffer.sortWith(compareTopicPartitions).map {\n+            case part => s\"${part}: ${errors(part).getMessage}\"\n+          }.mkString(System.lineSeparator())))\n+    }\n+    println(s\"Started ${proposedParts.size} partition reassignment(s)\")\n \n-    // If there is an existing rebalance running, attempt to change its throttle\n-    if (zkClient.reassignPartitionsInProgress()) {\n-      println(\"There is an existing assignment running.\")\n-      reassignPartitionsCommand.maybeLimit(throttle)\n+    if (!proposedReplicas.isEmpty) {\n+      executeMoves(adminClient, proposedReplicas, timeoutMs, time)\n+    }\n+  }\n+\n+  /**\n+   * Execute some partition log directory movements.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param proposedReplicas            A map from TopicPartitionReplicas to the\n+   *                                    directories to move them to.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeMoves(adminClient: Admin,\n+                   proposedReplicas: Map[TopicPartitionReplica, String],\n+                   timeoutMs: Long,\n+                   time: Time): Unit = {\n+    val startTimeMs = time.milliseconds()\n+    val pendingReplicas = new mutable.HashMap[TopicPartitionReplica, String]()\n+    pendingReplicas ++= proposedReplicas\n+    var done = false\n+    do {\n+      pendingReplicas --= alterReplicaLogDirs(adminClient, pendingReplicas)\n+      if (pendingReplicas.isEmpty) {\n+        println(s\"Started ${proposedReplicas.size} replica log directory reassignment(s)\")\n+        done = true\n+      } else if (time.milliseconds() >= startTimeMs + timeoutMs) {\n+        throw new TerseReassignmentFailureException(\n+          \"Timed out before replica log dir(s) could be reassigned:%n%s\".format(\n+            pendingReplicas.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).map {\n+              case replica => s\"${replica.toString}\"\n+            }.mkString(System.lineSeparator())))\n+      } else {\n+        // If a replica has been moved to a new host and we also specified a particular\n+        // log directory, we will have to keep retrying the alterReplicaLogDirs\n+        // call.  It can't take effect until the replica is moved to that host.\n+        time.sleep(100)\n+      }\n+    } while (!done)\n+  }\n+\n+  /**\n+   * Entry point for the --show-reassignments command.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   */\n+  def showReassignments(adminClient: Admin): Unit = {\n+    println(curReassignmentsToString(adminClient))\n+  }\n+\n+  /**\n+   * Convert the current partition reassignments to text.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   * @return              A string describing the current partition reassignments.\n+   */\n+  def curReassignmentsToString(adminClient: Admin): String = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val text = currentReassignments.keySet.toList.sortWith(compareTopicPartitions).map {\n+      case part =>\n+        val reassignment = currentReassignments(part)\n+        val replicas = reassignment.replicas().asScala\n+        val addingReplicas = reassignment.addingReplicas().asScala\n+        val removingReplicas = reassignment.removingReplicas().asScala\n+        \"%s: replicas: %s.%s%s\".format(part, replicas.mkString(\",\"),\n+          if (addingReplicas.isEmpty) \"\" else\n+            \" adding: %s.\".format(addingReplicas.mkString(\",\")),\n+          if (removingReplicas.isEmpty) \"\" else\n+            \" removing: %s.\".format(removingReplicas.mkString(\",\")))\n+    }.mkString(System.lineSeparator())\n+    if (text.isEmpty) {\n+      \"No partition reassignments found.\"\n     } else {\n-      printCurrentAssignment(zkClient, partitionAssignment.map(_._1.topic))\n-      if (throttle.interBrokerLimit >= 0 || throttle.replicaAlterLogDirsLimit >= 0)\n-        println(String.format(\"Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.\"))\n-      if (reassignPartitionsCommand.reassignPartitions(throttle, timeoutMs)) {\n-        println(\"Successfully started reassignment of partitions.\")\n-      } else\n-        println(\"Failed to reassign partitions %s\".format(partitionAssignment))\n+      \"Current partition reassignments:%n%s\".format(text)\n+    }\n+  }\n+\n+  /**\n+   * Verify that all the brokers in an assignment exist.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param brokers                     The broker IDs to verify.\n+   */\n+  def verifyBrokerIds(adminClient: Admin, brokers: Set[Int]): Unit = {\n+    val allNodeIds = adminClient.describeCluster().nodes().get().asScala.map(_.id).toSet\n+    brokers.find(!allNodeIds.contains(_)).map {\n+      case id => throw new AdminCommandFailedException(s\"Unknown broker id ${id}\")\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --execute command.\n+   *\n+   * @param zkClient                    The ZooKeeper client to use.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   */\n+  def executeAssignment(zkClient: KafkaZkClient,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    if (!proposedReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    verifyReplicasAndBrokersInAssignment(zkClient, proposedParts)\n+\n+    // Check for the presence of the legacy partition reassignment ZNode.  This actually\n+    // won't detect all rebalances... only ones initiated by the legacy method.\n+    // This is a limitation of the legacy ZK API.\n+    val reassignPartitionsInProgress = zkClient.reassignPartitionsInProgress()\n+    if (reassignPartitionsInProgress) {\n+      // Note: older versions of this tool would modify the broker quotas here (but not\n+      // topic quotas, for some reason).  This behavior wasn't documented in the --execute\n+      // command line help.  Since it might interfere with other ongoing reassignments,\n+      // this behavior was dropped as part of the KIP-455 changes.\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n+    }\n+    val currentParts = zkClient.getReplicaAssignmentForTopics(\n+      proposedParts.map(_._1.topic()).toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+\n+    if (interBrokerThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(Map.empty, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(zkClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      modifyBrokerThrottles(zkClient, reassigningBrokers, interBrokerThrottle)\n+      println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+    }\n+    zkClient.createPartitionReassignment(proposedParts)\n+    println(s\"Started ${proposedParts.size} partition reassignment(s)\")\n+  }\n+\n+  /**\n+   * Return the string which we want to print to describe the current partition assignment.\n+   *\n+   * @param proposedParts               The proposed partition assignment.\n+   * @param currentParts                The current partition assignment.\n+   *\n+   * @return                            The string to print.  We will only print information about\n+   *                                    partitions that appear in the proposed partition assignment.\n+   */\n+  def currentPartitionReplicaAssignmentToString(proposedParts: Map[TopicPartition, Seq[Int]],\n+                                                currentParts: Map[TopicPartition, Seq[Int]]): String = {\n+    \"Current partition replica assignment%n%n%s%n%nSave this to use as the %s\".\n+        format(formatAsReassignmentJson(currentParts.filterKeys(proposedParts.contains(_)).toMap, Map.empty),\n+              \"--reassignment-json-file option during rollback\")\n+  }\n+\n+  /**\n+   * Verify that the replicas and brokers referenced in the given partition assignment actually\n+   * exist.  This is necessary when using the deprecated ZK API, since ZooKeeper itself can't\n+   * validate what we're applying.\n+   *\n+   * @param zkClient                    The ZooKeeper client to use.\n+   * @param proposedParts               The partition assignment.\n+   */\n+  def verifyReplicasAndBrokersInAssignment(zkClient: KafkaZkClient,\n+                                           proposedParts: Map[TopicPartition, Seq[Int]]): Unit = {\n+    // check that all partitions in the proposed assignment exist in the cluster\n+    val proposedTopics = proposedParts.map { case (tp, _) => tp.topic }\n+    val existingAssignment = zkClient.getReplicaAssignmentForTopics(proposedTopics.toSet)\n+    val nonExistentPartitions = proposedParts.map { case (tp, _) => tp }.filterNot(existingAssignment.contains)\n+    if (nonExistentPartitions.nonEmpty)\n+      throw new AdminCommandFailedException(\"The proposed assignment contains non-existent partitions: \" +\n+        nonExistentPartitions)\n+\n+    // check that all brokers in the proposed assignment exist in the cluster\n+    val existingBrokerIDs = zkClient.getSortedBrokerList\n+    val nonExistingBrokerIDs = proposedParts.toMap.values.flatten.filterNot(existingBrokerIDs.contains).toSet\n+    if (nonExistingBrokerIDs.nonEmpty)\n+      throw new AdminCommandFailedException(\"The proposed assignment contains non-existent brokerIDs: \" + nonExistingBrokerIDs.mkString(\",\"))\n+  }\n+\n+  /**\n+   * Execute the given partition reassignments.\n+   *\n+   * @param adminClient       The admin client object to use.\n+   * @param reassignments     A map from topic names to target replica assignments.\n+   * @return                  A map from partition objects to error strings.\n+   */\n+  def alterPartitionReassignments(adminClient: Admin,\n+                                  reassignments: Map[TopicPartition, Seq[Int]])\n+                                  : Map[TopicPartition, Throwable] = {\n+    val results: Map[TopicPartition, KafkaFuture[Void]] =\n+      adminClient.alterPartitionReassignments(reassignments.map {\n+        case (part, replicas) => {\n+          (part, Optional.of(new NewPartitionReassignment(replicas.map(Integer.valueOf(_)).asJava)))\n+        }\n+      }.asJava).values().asScala\n+    results.flatMap {\n+      case (part, future) => {\n+        try {\n+          future.get()\n+          None\n+        } catch {\n+          case t: ExecutionException => Some(part, t.getCause())\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Cancel the given partition reassignments.\n+   *\n+   * @param adminClient       The admin client object to use.\n+   * @param reassignments     The partition reassignments to cancel.\n+   * @return                  A map from partition objects to error strings.\n+   */\n+  def cancelPartitionReassignments(adminClient: Admin,\n+                                  reassignments: Set[TopicPartition])\n+  : Map[TopicPartition, Throwable] = {\n+    val results: Map[TopicPartition, KafkaFuture[Void]] =\n+      adminClient.alterPartitionReassignments(reassignments.map {\n+          (_, (None: Option[NewPartitionReassignment]).asJava)\n+        }.toMap.asJava).values().asScala\n+    results.flatMap {\n+      case (part, future) => {\n+        try {\n+          future.get()\n+          None\n+        } catch {\n+          case t: ExecutionException => Some(part, t.getCause())\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Calculate the global map of all partitions that are moving.\n+   *\n+   * @param currentReassignments    The currently active reassignments.\n+   * @param proposedReassignments   The proposed reassignments (destinations replicas only).\n+   * @param currentParts            The current location of the partitions that we are\n+   *                                proposing to move.\n+   * @return                        A map from topic name to partition map.\n+   *                                The partition map is keyed on partition index and contains\n+   *                                the movements for that partition.\n+   */\n+  def calculateMoveMap(currentReassignments: Map[TopicPa", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg5NzQ0OA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389897448", "bodyText": "hmm, I'm not sure I follow.  Where do you think we should add .nonEmpty?", "author": "cmccabe", "createdAt": "2020-03-09T18:59:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY5Mjg0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEyMTc3MA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r391121770", "bodyText": "Ugh, I don't remember. GitHub's UI broke. This was likely in one fo the places where we do !condition.isEmpty - IntelliJ glows and suggests doing .nonEmpty. I don't feel strongly about this", "author": "stanislavkozlovski", "createdAt": "2020-03-11T16:59:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY5Mjg0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcxNzM5Nw==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389717397", "bodyText": "Do we allow cancellation only of partition reassignments whose addingReplicas isn't empty or whose removingReplicas isn't empty? If so, the latter check needs a !\nWhat about the alternative - allow cancellation of whatever listPartitionReassignments() returns? The API should return reassignments that are in progress only anyway", "author": "stanislavkozlovski", "createdAt": "2020-03-09T14:17:16Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1521 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(val sources: mutable.Set[Int],\n+                                  val destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is still progressing.\n+   */\n+  sealed case class PartitionReassignmentState(val currentReplicas: Seq[Int],\n+                                               val targetReplicas: Seq[Int],\n+                                               val done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {\n+    /**\n+     * True if the move is still progressing.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica move state where the source log directory is missng.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(val currentLogDir: String,\n+                                    val targetLogDir: String,\n+                                    val futureLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(val currentLogDir: String,\n+                                       val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        // Use AdminClient to manage reassignments.\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        // TODO: add --show and --cancel\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(adminClient,\n+            opts.options.has(opts.additionalOpt),\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt),\n+            opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.cancelOpt)) {\n+          cancelAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.listOpt)) {\n+          showReassignments(adminClient)\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      } else {\n+        // Use the legacy ZooKeeper client to manage reassignments.\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt))\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n-    } else {\n-      None\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states, whether there are ongoing\n+   *                              reassignments, replica move states, and whether there are\n+   *                              ongoing replica moves.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : (Map[TopicPartition, PartitionReassignmentState], Boolean,\n+                         Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetReplicas)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    (partStates, partsOngoing, moveStates, movesOngoing)\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states and whether there are ongoing\n+   *                              legacy reassignments.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    if (!targetReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    (partStates, partsOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      case topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n+        }\n       }\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    bld.mkString(System.lineSeparator())\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n-      }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n         }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val moveStates = findReplicaMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findReplicaMoveStates(adminClient: Admin,\n+                            targetMoves: Map[TopicPartitionReplica, String])\n+                            : Map[TopicPartitionReplica, ReplicaMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: ReplicaMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingLogDirMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingReplicaMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, ReplicaMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    allBrokerIds.foreach {\n+      case brokerId => configOps.put(\n+        new ConfigResource(ConfigResource.Type.BROKER, brokerId.toString),\n+        brokerLevelThrottles.map(throttle => new AlterConfigOp(\n+          new ConfigEntry(throttle, null), OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   */\n+  def clearBrokerLevelThrottles(zkClient: KafkaZkClient): Unit = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val brokerMetadatas = adminZkClient.getBrokerMetadatas(rackAwareMode, Some(brokerListToReassign))\n+    for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n+      if (!brokerLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n+      }\n+    }\n+  }\n \n-    val partitionsToBeReassigned = mutable.Map[TopicPartition, Seq[Int]]()\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    topics.foreach {\n+      topicName => configOps.put(\n+        new ConfigResource(ConfigResource.Type.TOPIC, topicName),\n+        topicLevelThrottles.map(throttle => new AlterConfigOp(new ConfigEntry(throttle, null),\n+          OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(zkClient: KafkaZkClient, topics: Set[String]): Unit = {\n+    val adminZkClient = new AdminZkClient(zkClient)\n+    for (topic <- topics) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n+      if (!topicLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeTopicConfig(topic, configs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --generate command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(adminClient: Admin,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = getReplicaAssignmentForTopics(adminClient, topicsToReassign)\n+    val brokerMetadatas = if (enableRackAwareness) {\n+      getBrokerRackInformation(adminClient, brokersToReassign)\n+    } else {\n+      brokersToReassign.map(new BrokerMetadata(_, None))\n+    }\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * The legacy entry point for the --generate command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(zkClient: KafkaZkClient,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+    val brokerMetadatas = if (enableRackAwareness) {\n+      getBrokerRackInformation(zkClient, brokersToReassign)\n+    } else {\n+      brokersToReassign.map(new BrokerMetadata(_, None))\n+    }\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * Calculate the new partition assignments to suggest in --generate.\n+   *\n+   * @param currentAssignment  The current partition assignments.\n+   * @param brokerMetadatas    The rack information for each broker.\n+   *\n+   * @return                   A map from partitions to the proposed assignments for each.\n+   */\n+  def calculateAssignment(currentAssignment: Map[TopicPartition, Seq[Int]],\n+                          brokerMetadatas: Seq[BrokerMetadata])\n+                          : Map[TopicPartition, Seq[Int]] = {\n+    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n+    val proposedAssignments = mutable.Map[TopicPartition, Seq[Int]]()\n     groupedByTopic.foreach { case (topic, assignment) =>\n       val (_, replicas) = assignment.head\n-      val assignedReplicas = AdminUtils.assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n-      partitionsToBeReassigned ++= assignedReplicas.map { case (partition, replicas) =>\n+      val assignedReplicas = AdminUtils.\n+        assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n+      proposedAssignments ++= assignedReplicas.map { case (partition, replicas) =>\n         new TopicPartition(topic, partition) -> replicas\n       }\n     }\n-    (partitionsToBeReassigned, currentAssignment)\n+    proposedAssignments\n+  }\n+\n+  /**\n+   * Get the current replica assignments for some topics.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics          The topics to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForTopics(adminClient: Admin,\n+                                    topics: Seq[String])\n+                                    : Map[TopicPartition, Seq[Int]] = {\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.map {\n+          info => (new TopicPartition(topicName, info.partition()),\n+            info.replicas().asScala.map(_.id()))\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val reassignmentJsonFile =  opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val reassignmentJsonString = Utils.readFileAsString(reassignmentJsonFile)\n-    val interBrokerThrottle = opts.options.valueOf(opts.interBrokerThrottleOpt)\n-    val replicaAlterLogDirsThrottle = opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt)\n-    val timeoutMs = opts.options.valueOf(opts.timeoutOpt)\n-    executeAssignment(zkClient, adminClientOpt, reassignmentJsonString, Throttle(interBrokerThrottle, replicaAlterLogDirsThrottle), timeoutMs)\n+  /**\n+   * Get the current replica assignments for some partitions.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param partitions      The partitions to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForPartitions(adminClient: Admin,\n+                                        partitions: Set[TopicPartition])\n+                                        : Map[TopicPartition, Seq[Int]] = {\n+    val topics = new mutable.HashSet[String]()\n+    partitions.foreach(topics += _.topic)\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.flatMap {\n+          info => if (partitions.contains(new TopicPartition(topicName, info.partition()))) {\n+            Some(new TopicPartition(topicName, info.partition()),\n+                info.replicas().asScala.map(_.id()))\n+          } else {\n+            None\n+          }\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], reassignmentJsonString: String, throttle: Throttle, timeoutMs: Long = 10000L): Unit = {\n-    val (partitionAssignment, replicaAssignment) = parseAndValidate(zkClient, reassignmentJsonString)\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param adminClient    The AdminClient object.\n+   * @param brokers        The brokers to gather metadata about.\n+   *\n+   * @return               The metadata for each broker.\n+   */\n+  def getBrokerRackInformation(adminClient: Admin,\n+                               brokers: Seq[Int]): Seq[BrokerMetadata] = {\n+    val brokerSet = brokers.toSet\n+    val results = adminClient.describeCluster().nodes().get().asScala.flatMap(\n+      node => if (!brokerSet.contains(node.id())) {\n+        None\n+      } else {\n+        if (node.rack() != null) {\n+          Some(new BrokerMetadata(node.id(), Some(node.rack())))\n+        } else {\n+          Some(new BrokerMetadata(node.id(), None))\n+        }\n+      }\n+    ).toSeq\n+    val numRackless = results.count(_.rack.isEmpty)\n+    if ((numRackless != 0) && (numRackless != results.size)) {\n+      throw new AdminOperationException(\"Not all brokers have rack information. Add \" +\n+        \"--disable-rack-aware in command line to make replica assignment without rack \" +\n+        \"information.\")\n+    }\n+    results\n+  }\n+\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param zkClient       The ZooKeeper client to use.\n+   * @param brokers        The brokers to gather metadata about.\n+   *\n+   * @return               The metadata for each broker.\n+   */\n+  def getBrokerRackInformation(zkClient: KafkaZkClient,\n+                               brokers: Seq[Int]): Seq[BrokerMetadata] = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val reassignPartitionsCommand = new ReassignPartitionsCommand(zkClient, adminClientOpt, partitionAssignment.toMap, replicaAssignment, adminZkClient)\n+    adminZkClient.getBrokerMetadatas(RackAwareMode.Enforced, Some(brokers))\n+  }\n+\n+  /**\n+   * Parse and validate data gathered from the command-line for --generate\n+   * In particular, we parse the JSON and validate that duplicate brokers and\n+   * topics don't appear.\n+   *\n+   * @param reassignmentJson       The JSON passed to --generate .\n+   * @param brokerList             A list of brokers passed to --generate.\n+   *\n+   * @return                       A tuple of brokers to reassign, topics to reassign\n+   */\n+  def parseGenerateAssignmentArgs(reassignmentJson: String,\n+                                  brokerList: String): (Seq[Int], Seq[String]) = {\n+    val brokerListToReassign = brokerList.split(',').map(_.toInt)\n+    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n+    if (duplicateReassignments.nonEmpty)\n+      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".\n+        format(duplicateReassignments.mkString(\",\")))\n+    val topicsToReassign = parseTopicsData(reassignmentJson)\n+    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n+    if (duplicateTopicsToReassign.nonEmpty)\n+      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".\n+        format(duplicateTopicsToReassign.mkString(\",\")))\n+    (brokerListToReassign, topicsToReassign)\n+  }\n+\n+  /**\n+   * The entry point for the --execute and --execute-additional commands.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param additional                  Whether --additional was passed.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   * @param replicaAlterLogDirsThrottle The replica throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeAssignment(adminClient: Admin,\n+                        additional: Boolean,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long = -1L,\n+                        replicaAlterLogDirsThrottle: Long = -1L,\n+                        timeoutMs: Long = 10000L,\n+                        time: Time = Time.SYSTEM): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    // If there is an existing assignment, check for --additional before proceeding.\n+    // This helps avoid surprising users.\n+    if (!additional && !currentReassignments.isEmpty) {\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n+    }\n+    verifyBrokerIds(adminClient, proposedParts.values.flatten.toSet)\n+    val currentParts = getReplicaAssignmentForPartitions(adminClient, proposedParts.keySet.toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+    if (interBrokerThrottle >= 0 || replicaAlterLogDirsThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(currentReassignments, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(adminClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      val movingBrokers = calculateMovingBrokers(proposedReplicas.keySet.toSet)\n+      modifyBrokerThrottles(adminClient,\n+        reassigningBrokers, interBrokerThrottle,\n+        movingBrokers, replicaAlterLogDirsThrottle)\n+      if (interBrokerThrottle >= 0) {\n+        println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+      }\n+      if (replicaAlterLogDirsThrottle >= 0) {\n+        println(s\"The replica-alter-dir throttle limit was set to ${replicaAlterLogDirsThrottle} B/s\")\n+      }\n+    }\n+\n+    // Execute the partition reassignments.\n+    val errors = alterPartitionReassignments(adminClient, proposedParts)\n+    if (!errors.isEmpty) {\n+      throw new TerseReassignmentFailureException(\n+        \"Error reassigning partition(s):%n%s\".format(\n+          errors.keySet.toBuffer.sortWith(compareTopicPartitions).map {\n+            case part => s\"${part}: ${errors(part).getMessage}\"\n+          }.mkString(System.lineSeparator())))\n+    }\n+    println(s\"Started ${proposedParts.size} partition reassignment(s)\")\n \n-    // If there is an existing rebalance running, attempt to change its throttle\n-    if (zkClient.reassignPartitionsInProgress()) {\n-      println(\"There is an existing assignment running.\")\n-      reassignPartitionsCommand.maybeLimit(throttle)\n+    if (!proposedReplicas.isEmpty) {\n+      executeMoves(adminClient, proposedReplicas, timeoutMs, time)\n+    }\n+  }\n+\n+  /**\n+   * Execute some partition log directory movements.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param proposedReplicas            A map from TopicPartitionReplicas to the\n+   *                                    directories to move them to.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeMoves(adminClient: Admin,\n+                   proposedReplicas: Map[TopicPartitionReplica, String],\n+                   timeoutMs: Long,\n+                   time: Time): Unit = {\n+    val startTimeMs = time.milliseconds()\n+    val pendingReplicas = new mutable.HashMap[TopicPartitionReplica, String]()\n+    pendingReplicas ++= proposedReplicas\n+    var done = false\n+    do {\n+      pendingReplicas --= alterReplicaLogDirs(adminClient, pendingReplicas)\n+      if (pendingReplicas.isEmpty) {\n+        println(s\"Started ${proposedReplicas.size} replica log directory reassignment(s)\")\n+        done = true\n+      } else if (time.milliseconds() >= startTimeMs + timeoutMs) {\n+        throw new TerseReassignmentFailureException(\n+          \"Timed out before replica log dir(s) could be reassigned:%n%s\".format(\n+            pendingReplicas.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).map {\n+              case replica => s\"${replica.toString}\"\n+            }.mkString(System.lineSeparator())))\n+      } else {\n+        // If a replica has been moved to a new host and we also specified a particular\n+        // log directory, we will have to keep retrying the alterReplicaLogDirs\n+        // call.  It can't take effect until the replica is moved to that host.\n+        time.sleep(100)\n+      }\n+    } while (!done)\n+  }\n+\n+  /**\n+   * Entry point for the --show-reassignments command.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   */\n+  def showReassignments(adminClient: Admin): Unit = {\n+    println(curReassignmentsToString(adminClient))\n+  }\n+\n+  /**\n+   * Convert the current partition reassignments to text.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   * @return              A string describing the current partition reassignments.\n+   */\n+  def curReassignmentsToString(adminClient: Admin): String = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val text = currentReassignments.keySet.toList.sortWith(compareTopicPartitions).map {\n+      case part =>\n+        val reassignment = currentReassignments(part)\n+        val replicas = reassignment.replicas().asScala\n+        val addingReplicas = reassignment.addingReplicas().asScala\n+        val removingReplicas = reassignment.removingReplicas().asScala\n+        \"%s: replicas: %s.%s%s\".format(part, replicas.mkString(\",\"),\n+          if (addingReplicas.isEmpty) \"\" else\n+            \" adding: %s.\".format(addingReplicas.mkString(\",\")),\n+          if (removingReplicas.isEmpty) \"\" else\n+            \" removing: %s.\".format(removingReplicas.mkString(\",\")))\n+    }.mkString(System.lineSeparator())\n+    if (text.isEmpty) {\n+      \"No partition reassignments found.\"\n     } else {\n-      printCurrentAssignment(zkClient, partitionAssignment.map(_._1.topic))\n-      if (throttle.interBrokerLimit >= 0 || throttle.replicaAlterLogDirsLimit >= 0)\n-        println(String.format(\"Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.\"))\n-      if (reassignPartitionsCommand.reassignPartitions(throttle, timeoutMs)) {\n-        println(\"Successfully started reassignment of partitions.\")\n-      } else\n-        println(\"Failed to reassign partitions %s\".format(partitionAssignment))\n+      \"Current partition reassignments:%n%s\".format(text)\n+    }\n+  }\n+\n+  /**\n+   * Verify that all the brokers in an assignment exist.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param brokers                     The broker IDs to verify.\n+   */\n+  def verifyBrokerIds(adminClient: Admin, brokers: Set[Int]): Unit = {\n+    val allNodeIds = adminClient.describeCluster().nodes().get().asScala.map(_.id).toSet\n+    brokers.find(!allNodeIds.contains(_)).map {\n+      case id => throw new AdminCommandFailedException(s\"Unknown broker id ${id}\")\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --execute command.\n+   *\n+   * @param zkClient                    The ZooKeeper client to use.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   */\n+  def executeAssignment(zkClient: KafkaZkClient,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    if (!proposedReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    verifyReplicasAndBrokersInAssignment(zkClient, proposedParts)\n+\n+    // Check for the presence of the legacy partition reassignment ZNode.  This actually\n+    // won't detect all rebalances... only ones initiated by the legacy method.\n+    // This is a limitation of the legacy ZK API.\n+    val reassignPartitionsInProgress = zkClient.reassignPartitionsInProgress()\n+    if (reassignPartitionsInProgress) {\n+      // Note: older versions of this tool would modify the broker quotas here (but not\n+      // topic quotas, for some reason).  This behavior wasn't documented in the --execute\n+      // command line help.  Since it might interfere with other ongoing reassignments,\n+      // this behavior was dropped as part of the KIP-455 changes.\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n+    }\n+    val currentParts = zkClient.getReplicaAssignmentForTopics(\n+      proposedParts.map(_._1.topic()).toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+\n+    if (interBrokerThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(Map.empty, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(zkClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      modifyBrokerThrottles(zkClient, reassigningBrokers, interBrokerThrottle)\n+      println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+    }\n+    zkClient.createPartitionReassignment(proposedParts)\n+    println(s\"Started ${proposedParts.size} partition reassignment(s)\")\n+  }\n+\n+  /**\n+   * Return the string which we want to print to describe the current partition assignment.\n+   *\n+   * @param proposedParts               The proposed partition assignment.\n+   * @param currentParts                The current partition assignment.\n+   *\n+   * @return                            The string to print.  We will only print information about\n+   *                                    partitions that appear in the proposed partition assignment.\n+   */\n+  def currentPartitionReplicaAssignmentToString(proposedParts: Map[TopicPartition, Seq[Int]],\n+                                                currentParts: Map[TopicPartition, Seq[Int]]): String = {\n+    \"Current partition replica assignment%n%n%s%n%nSave this to use as the %s\".\n+        format(formatAsReassignmentJson(currentParts.filterKeys(proposedParts.contains(_)).toMap, Map.empty),\n+              \"--reassignment-json-file option during rollback\")\n+  }\n+\n+  /**\n+   * Verify that the replicas and brokers referenced in the given partition assignment actually\n+   * exist.  This is necessary when using the deprecated ZK API, since ZooKeeper itself can't\n+   * validate what we're applying.\n+   *\n+   * @param zkClient                    The ZooKeeper client to use.\n+   * @param proposedParts               The partition assignment.\n+   */\n+  def verifyReplicasAndBrokersInAssignment(zkClient: KafkaZkClient,\n+                                           proposedParts: Map[TopicPartition, Seq[Int]]): Unit = {\n+    // check that all partitions in the proposed assignment exist in the cluster\n+    val proposedTopics = proposedParts.map { case (tp, _) => tp.topic }\n+    val existingAssignment = zkClient.getReplicaAssignmentForTopics(proposedTopics.toSet)\n+    val nonExistentPartitions = proposedParts.map { case (tp, _) => tp }.filterNot(existingAssignment.contains)\n+    if (nonExistentPartitions.nonEmpty)\n+      throw new AdminCommandFailedException(\"The proposed assignment contains non-existent partitions: \" +\n+        nonExistentPartitions)\n+\n+    // check that all brokers in the proposed assignment exist in the cluster\n+    val existingBrokerIDs = zkClient.getSortedBrokerList\n+    val nonExistingBrokerIDs = proposedParts.toMap.values.flatten.filterNot(existingBrokerIDs.contains).toSet\n+    if (nonExistingBrokerIDs.nonEmpty)\n+      throw new AdminCommandFailedException(\"The proposed assignment contains non-existent brokerIDs: \" + nonExistingBrokerIDs.mkString(\",\"))\n+  }\n+\n+  /**\n+   * Execute the given partition reassignments.\n+   *\n+   * @param adminClient       The admin client object to use.\n+   * @param reassignments     A map from topic names to target replica assignments.\n+   * @return                  A map from partition objects to error strings.\n+   */\n+  def alterPartitionReassignments(adminClient: Admin,\n+                                  reassignments: Map[TopicPartition, Seq[Int]])\n+                                  : Map[TopicPartition, Throwable] = {\n+    val results: Map[TopicPartition, KafkaFuture[Void]] =\n+      adminClient.alterPartitionReassignments(reassignments.map {\n+        case (part, replicas) => {\n+          (part, Optional.of(new NewPartitionReassignment(replicas.map(Integer.valueOf(_)).asJava)))\n+        }\n+      }.asJava).values().asScala\n+    results.flatMap {\n+      case (part, future) => {\n+        try {\n+          future.get()\n+          None\n+        } catch {\n+          case t: ExecutionException => Some(part, t.getCause())\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Cancel the given partition reassignments.\n+   *\n+   * @param adminClient       The admin client object to use.\n+   * @param reassignments     The partition reassignments to cancel.\n+   * @return                  A map from partition objects to error strings.\n+   */\n+  def cancelPartitionReassignments(adminClient: Admin,\n+                                  reassignments: Set[TopicPartition])\n+  : Map[TopicPartition, Throwable] = {\n+    val results: Map[TopicPartition, KafkaFuture[Void]] =\n+      adminClient.alterPartitionReassignments(reassignments.map {\n+          (_, (None: Option[NewPartitionReassignment]).asJava)\n+        }.toMap.asJava).values().asScala\n+    results.flatMap {\n+      case (part, future) => {\n+        try {\n+          future.get()\n+          None\n+        } catch {\n+          case t: ExecutionException => Some(part, t.getCause())\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Calculate the global map of all partitions that are moving.\n+   *\n+   * @param currentReassignments    The currently active reassignments.\n+   * @param proposedReassignments   The proposed reassignments (destinations replicas only).\n+   * @param currentParts            The current location of the partitions that we are\n+   *                                proposing to move.\n+   * @return                        A map from topic name to partition map.\n+   *                                The partition map is keyed on partition index and contains\n+   *                                the movements for that partition.\n+   */\n+  def calculateMoveMap(currentReassignments: Map[TopicPa", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg5OTY3NQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389899675", "bodyText": "Hmm, github is showing this comment in a weird place-- on the calculateMoveMap function rather than in the cancelAssignment function as you probably intended.\nGood find on the missing negation.  That was definitely not intended.  Fixed.\nIf users want to cancel arbitrary partition moves, they can create a JSON file and use --cancel on it, right?", "author": "cmccabe", "createdAt": "2020-03-09T19:04:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcxNzM5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcxODQ1MQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389718451", "bodyText": "Should we print the ones that were successfully cancelled?", "author": "stanislavkozlovski", "createdAt": "2020-03-09T14:18:47Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1521 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(val sources: mutable.Set[Int],\n+                                  val destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is still progressing.\n+   */\n+  sealed case class PartitionReassignmentState(val currentReplicas: Seq[Int],\n+                                               val targetReplicas: Seq[Int],\n+                                               val done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {\n+    /**\n+     * True if the move is still progressing.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica move state where the source log directory is missng.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(val currentLogDir: String,\n+                                    val targetLogDir: String,\n+                                    val futureLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(val currentLogDir: String,\n+                                       val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        // Use AdminClient to manage reassignments.\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        // TODO: add --show and --cancel\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(adminClient,\n+            opts.options.has(opts.additionalOpt),\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt),\n+            opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.cancelOpt)) {\n+          cancelAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.listOpt)) {\n+          showReassignments(adminClient)\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      } else {\n+        // Use the legacy ZooKeeper client to manage reassignments.\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt))\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n-    } else {\n-      None\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states, whether there are ongoing\n+   *                              reassignments, replica move states, and whether there are\n+   *                              ongoing replica moves.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : (Map[TopicPartition, PartitionReassignmentState], Boolean,\n+                         Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetReplicas)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    (partStates, partsOngoing, moveStates, movesOngoing)\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states and whether there are ongoing\n+   *                              legacy reassignments.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    if (!targetReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    (partStates, partsOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      case topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n+        }\n       }\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    bld.mkString(System.lineSeparator())\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n-      }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n         }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val moveStates = findReplicaMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findReplicaMoveStates(adminClient: Admin,\n+                            targetMoves: Map[TopicPartitionReplica, String])\n+                            : Map[TopicPartitionReplica, ReplicaMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: ReplicaMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingLogDirMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingReplicaMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, ReplicaMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    allBrokerIds.foreach {\n+      case brokerId => configOps.put(\n+        new ConfigResource(ConfigResource.Type.BROKER, brokerId.toString),\n+        brokerLevelThrottles.map(throttle => new AlterConfigOp(\n+          new ConfigEntry(throttle, null), OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   */\n+  def clearBrokerLevelThrottles(zkClient: KafkaZkClient): Unit = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val brokerMetadatas = adminZkClient.getBrokerMetadatas(rackAwareMode, Some(brokerListToReassign))\n+    for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n+      if (!brokerLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n+      }\n+    }\n+  }\n \n-    val partitionsToBeReassigned = mutable.Map[TopicPartition, Seq[Int]]()\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    topics.foreach {\n+      topicName => configOps.put(\n+        new ConfigResource(ConfigResource.Type.TOPIC, topicName),\n+        topicLevelThrottles.map(throttle => new AlterConfigOp(new ConfigEntry(throttle, null),\n+          OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(zkClient: KafkaZkClient, topics: Set[String]): Unit = {\n+    val adminZkClient = new AdminZkClient(zkClient)\n+    for (topic <- topics) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n+      if (!topicLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeTopicConfig(topic, configs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --generate command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(adminClient: Admin,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = getReplicaAssignmentForTopics(adminClient, topicsToReassign)\n+    val brokerMetadatas = if (enableRackAwareness) {\n+      getBrokerRackInformation(adminClient, brokersToReassign)\n+    } else {\n+      brokersToReassign.map(new BrokerMetadata(_, None))\n+    }\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * The legacy entry point for the --generate command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(zkClient: KafkaZkClient,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+    val brokerMetadatas = if (enableRackAwareness) {\n+      getBrokerRackInformation(zkClient, brokersToReassign)\n+    } else {\n+      brokersToReassign.map(new BrokerMetadata(_, None))\n+    }\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * Calculate the new partition assignments to suggest in --generate.\n+   *\n+   * @param currentAssignment  The current partition assignments.\n+   * @param brokerMetadatas    The rack information for each broker.\n+   *\n+   * @return                   A map from partitions to the proposed assignments for each.\n+   */\n+  def calculateAssignment(currentAssignment: Map[TopicPartition, Seq[Int]],\n+                          brokerMetadatas: Seq[BrokerMetadata])\n+                          : Map[TopicPartition, Seq[Int]] = {\n+    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n+    val proposedAssignments = mutable.Map[TopicPartition, Seq[Int]]()\n     groupedByTopic.foreach { case (topic, assignment) =>\n       val (_, replicas) = assignment.head\n-      val assignedReplicas = AdminUtils.assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n-      partitionsToBeReassigned ++= assignedReplicas.map { case (partition, replicas) =>\n+      val assignedReplicas = AdminUtils.\n+        assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n+      proposedAssignments ++= assignedReplicas.map { case (partition, replicas) =>\n         new TopicPartition(topic, partition) -> replicas\n       }\n     }\n-    (partitionsToBeReassigned, currentAssignment)\n+    proposedAssignments\n+  }\n+\n+  /**\n+   * Get the current replica assignments for some topics.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics          The topics to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForTopics(adminClient: Admin,\n+                                    topics: Seq[String])\n+                                    : Map[TopicPartition, Seq[Int]] = {\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.map {\n+          info => (new TopicPartition(topicName, info.partition()),\n+            info.replicas().asScala.map(_.id()))\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val reassignmentJsonFile =  opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val reassignmentJsonString = Utils.readFileAsString(reassignmentJsonFile)\n-    val interBrokerThrottle = opts.options.valueOf(opts.interBrokerThrottleOpt)\n-    val replicaAlterLogDirsThrottle = opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt)\n-    val timeoutMs = opts.options.valueOf(opts.timeoutOpt)\n-    executeAssignment(zkClient, adminClientOpt, reassignmentJsonString, Throttle(interBrokerThrottle, replicaAlterLogDirsThrottle), timeoutMs)\n+  /**\n+   * Get the current replica assignments for some partitions.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param partitions      The partitions to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForPartitions(adminClient: Admin,\n+                                        partitions: Set[TopicPartition])\n+                                        : Map[TopicPartition, Seq[Int]] = {\n+    val topics = new mutable.HashSet[String]()\n+    partitions.foreach(topics += _.topic)\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.flatMap {\n+          info => if (partitions.contains(new TopicPartition(topicName, info.partition()))) {\n+            Some(new TopicPartition(topicName, info.partition()),\n+                info.replicas().asScala.map(_.id()))\n+          } else {\n+            None\n+          }\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], reassignmentJsonString: String, throttle: Throttle, timeoutMs: Long = 10000L): Unit = {\n-    val (partitionAssignment, replicaAssignment) = parseAndValidate(zkClient, reassignmentJsonString)\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param adminClient    The AdminClient object.\n+   * @param brokers        The brokers to gather metadata about.\n+   *\n+   * @return               The metadata for each broker.\n+   */\n+  def getBrokerRackInformation(adminClient: Admin,\n+                               brokers: Seq[Int]): Seq[BrokerMetadata] = {\n+    val brokerSet = brokers.toSet\n+    val results = adminClient.describeCluster().nodes().get().asScala.flatMap(\n+      node => if (!brokerSet.contains(node.id())) {\n+        None\n+      } else {\n+        if (node.rack() != null) {\n+          Some(new BrokerMetadata(node.id(), Some(node.rack())))\n+        } else {\n+          Some(new BrokerMetadata(node.id(), None))\n+        }\n+      }\n+    ).toSeq\n+    val numRackless = results.count(_.rack.isEmpty)\n+    if ((numRackless != 0) && (numRackless != results.size)) {\n+      throw new AdminOperationException(\"Not all brokers have rack information. Add \" +\n+        \"--disable-rack-aware in command line to make replica assignment without rack \" +\n+        \"information.\")\n+    }\n+    results\n+  }\n+\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param zkClient       The ZooKeeper client to use.\n+   * @param brokers        The brokers to gather metadata about.\n+   *\n+   * @return               The metadata for each broker.\n+   */\n+  def getBrokerRackInformation(zkClient: KafkaZkClient,\n+                               brokers: Seq[Int]): Seq[BrokerMetadata] = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val reassignPartitionsCommand = new ReassignPartitionsCommand(zkClient, adminClientOpt, partitionAssignment.toMap, replicaAssignment, adminZkClient)\n+    adminZkClient.getBrokerMetadatas(RackAwareMode.Enforced, Some(brokers))\n+  }\n+\n+  /**\n+   * Parse and validate data gathered from the command-line for --generate\n+   * In particular, we parse the JSON and validate that duplicate brokers and\n+   * topics don't appear.\n+   *\n+   * @param reassignmentJson       The JSON passed to --generate .\n+   * @param brokerList             A list of brokers passed to --generate.\n+   *\n+   * @return                       A tuple of brokers to reassign, topics to reassign\n+   */\n+  def parseGenerateAssignmentArgs(reassignmentJson: String,\n+                                  brokerList: String): (Seq[Int], Seq[String]) = {\n+    val brokerListToReassign = brokerList.split(',').map(_.toInt)\n+    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n+    if (duplicateReassignments.nonEmpty)\n+      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".\n+        format(duplicateReassignments.mkString(\",\")))\n+    val topicsToReassign = parseTopicsData(reassignmentJson)\n+    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n+    if (duplicateTopicsToReassign.nonEmpty)\n+      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".\n+        format(duplicateTopicsToReassign.mkString(\",\")))\n+    (brokerListToReassign, topicsToReassign)\n+  }\n+\n+  /**\n+   * The entry point for the --execute and --execute-additional commands.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param additional                  Whether --additional was passed.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   * @param replicaAlterLogDirsThrottle The replica throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeAssignment(adminClient: Admin,\n+                        additional: Boolean,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long = -1L,\n+                        replicaAlterLogDirsThrottle: Long = -1L,\n+                        timeoutMs: Long = 10000L,\n+                        time: Time = Time.SYSTEM): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    // If there is an existing assignment, check for --additional before proceeding.\n+    // This helps avoid surprising users.\n+    if (!additional && !currentReassignments.isEmpty) {\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n+    }\n+    verifyBrokerIds(adminClient, proposedParts.values.flatten.toSet)\n+    val currentParts = getReplicaAssignmentForPartitions(adminClient, proposedParts.keySet.toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+    if (interBrokerThrottle >= 0 || replicaAlterLogDirsThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(currentReassignments, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(adminClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      val movingBrokers = calculateMovingBrokers(proposedReplicas.keySet.toSet)\n+      modifyBrokerThrottles(adminClient,\n+        reassigningBrokers, interBrokerThrottle,\n+        movingBrokers, replicaAlterLogDirsThrottle)\n+      if (interBrokerThrottle >= 0) {\n+        println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+      }\n+      if (replicaAlterLogDirsThrottle >= 0) {\n+        println(s\"The replica-alter-dir throttle limit was set to ${replicaAlterLogDirsThrottle} B/s\")\n+      }\n+    }\n+\n+    // Execute the partition reassignments.\n+    val errors = alterPartitionReassignments(adminClient, proposedParts)\n+    if (!errors.isEmpty) {\n+      throw new TerseReassignmentFailureException(\n+        \"Error reassigning partition(s):%n%s\".format(\n+          errors.keySet.toBuffer.sortWith(compareTopicPartitions).map {\n+            case part => s\"${part}: ${errors(part).getMessage}\"\n+          }.mkString(System.lineSeparator())))\n+    }\n+    println(s\"Started ${proposedParts.size} partition reassignment(s)\")\n \n-    // If there is an existing rebalance running, attempt to change its throttle\n-    if (zkClient.reassignPartitionsInProgress()) {\n-      println(\"There is an existing assignment running.\")\n-      reassignPartitionsCommand.maybeLimit(throttle)\n+    if (!proposedReplicas.isEmpty) {\n+      executeMoves(adminClient, proposedReplicas, timeoutMs, time)\n+    }\n+  }\n+\n+  /**\n+   * Execute some partition log directory movements.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param proposedReplicas            A map from TopicPartitionReplicas to the\n+   *                                    directories to move them to.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeMoves(adminClient: Admin,\n+                   proposedReplicas: Map[TopicPartitionReplica, String],\n+                   timeoutMs: Long,\n+                   time: Time): Unit = {\n+    val startTimeMs = time.milliseconds()\n+    val pendingReplicas = new mutable.HashMap[TopicPartitionReplica, String]()\n+    pendingReplicas ++= proposedReplicas\n+    var done = false\n+    do {\n+      pendingReplicas --= alterReplicaLogDirs(adminClient, pendingReplicas)\n+      if (pendingReplicas.isEmpty) {\n+        println(s\"Started ${proposedReplicas.size} replica log directory reassignment(s)\")\n+        done = true\n+      } else if (time.milliseconds() >= startTimeMs + timeoutMs) {\n+        throw new TerseReassignmentFailureException(\n+          \"Timed out before replica log dir(s) could be reassigned:%n%s\".format(\n+            pendingReplicas.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).map {\n+              case replica => s\"${replica.toString}\"\n+            }.mkString(System.lineSeparator())))\n+      } else {\n+        // If a replica has been moved to a new host and we also specified a particular\n+        // log directory, we will have to keep retrying the alterReplicaLogDirs\n+        // call.  It can't take effect until the replica is moved to that host.\n+        time.sleep(100)\n+      }\n+    } while (!done)\n+  }\n+\n+  /**\n+   * Entry point for the --show-reassignments command.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   */\n+  def showReassignments(adminClient: Admin): Unit = {\n+    println(curReassignmentsToString(adminClient))\n+  }\n+\n+  /**\n+   * Convert the current partition reassignments to text.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   * @return              A string describing the current partition reassignments.\n+   */\n+  def curReassignmentsToString(adminClient: Admin): String = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val text = currentReassignments.keySet.toList.sortWith(compareTopicPartitions).map {\n+      case part =>\n+        val reassignment = currentReassignments(part)\n+        val replicas = reassignment.replicas().asScala\n+        val addingReplicas = reassignment.addingReplicas().asScala\n+        val removingReplicas = reassignment.removingReplicas().asScala\n+        \"%s: replicas: %s.%s%s\".format(part, replicas.mkString(\",\"),\n+          if (addingReplicas.isEmpty) \"\" else\n+            \" adding: %s.\".format(addingReplicas.mkString(\",\")),\n+          if (removingReplicas.isEmpty) \"\" else\n+            \" removing: %s.\".format(removingReplicas.mkString(\",\")))\n+    }.mkString(System.lineSeparator())\n+    if (text.isEmpty) {\n+      \"No partition reassignments found.\"\n     } else {\n-      printCurrentAssignment(zkClient, partitionAssignment.map(_._1.topic))\n-      if (throttle.interBrokerLimit >= 0 || throttle.replicaAlterLogDirsLimit >= 0)\n-        println(String.format(\"Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.\"))\n-      if (reassignPartitionsCommand.reassignPartitions(throttle, timeoutMs)) {\n-        println(\"Successfully started reassignment of partitions.\")\n-      } else\n-        println(\"Failed to reassign partitions %s\".format(partitionAssignment))\n+      \"Current partition reassignments:%n%s\".format(text)\n+    }\n+  }\n+\n+  /**\n+   * Verify that all the brokers in an assignment exist.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param brokers                     The broker IDs to verify.\n+   */\n+  def verifyBrokerIds(adminClient: Admin, brokers: Set[Int]): Unit = {\n+    val allNodeIds = adminClient.describeCluster().nodes().get().asScala.map(_.id).toSet\n+    brokers.find(!allNodeIds.contains(_)).map {\n+      case id => throw new AdminCommandFailedException(s\"Unknown broker id ${id}\")\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --execute command.\n+   *\n+   * @param zkClient                    The ZooKeeper client to use.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   */\n+  def executeAssignment(zkClient: KafkaZkClient,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    if (!proposedReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    verifyReplicasAndBrokersInAssignment(zkClient, proposedParts)\n+\n+    // Check for the presence of the legacy partition reassignment ZNode.  This actually\n+    // won't detect all rebalances... only ones initiated by the legacy method.\n+    // This is a limitation of the legacy ZK API.\n+    val reassignPartitionsInProgress = zkClient.reassignPartitionsInProgress()\n+    if (reassignPartitionsInProgress) {\n+      // Note: older versions of this tool would modify the broker quotas here (but not\n+      // topic quotas, for some reason).  This behavior wasn't documented in the --execute\n+      // command line help.  Since it might interfere with other ongoing reassignments,\n+      // this behavior was dropped as part of the KIP-455 changes.\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n+    }\n+    val currentParts = zkClient.getReplicaAssignmentForTopics(\n+      proposedParts.map(_._1.topic()).toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+\n+    if (interBrokerThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(Map.empty, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(zkClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      modifyBrokerThrottles(zkClient, reassigningBrokers, interBrokerThrottle)\n+      println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+    }\n+    zkClient.createPartitionReassignment(proposedParts)\n+    println(s\"Started ${proposedParts.size} partition reassignment(s)\")\n+  }\n+\n+  /**\n+   * Return the string which we want to print to describe the current partition assignment.\n+   *\n+   * @param proposedParts               The proposed partition assignment.\n+   * @param currentParts                The current partition assignment.\n+   *\n+   * @return                            The string to print.  We will only print information about\n+   *                                    partitions that appear in the proposed partition assignment.\n+   */\n+  def currentPartitionReplicaAssignmentToString(proposedParts: Map[TopicPartition, Seq[Int]],\n+                                                currentParts: Map[TopicPartition, Seq[Int]]): String = {\n+    \"Current partition replica assignment%n%n%s%n%nSave this to use as the %s\".\n+        format(formatAsReassignmentJson(currentParts.filterKeys(proposedParts.contains(_)).toMap, Map.empty),\n+              \"--reassignment-json-file option during rollback\")\n+  }\n+\n+  /**\n+   * Verify that the replicas and brokers referenced in the given partition assignment actually\n+   * exist.  This is necessary when using the deprecated ZK API, since ZooKeeper itself can't\n+   * validate what we're applying.\n+   *\n+   * @param zkClient                    The ZooKeeper client to use.\n+   * @param proposedParts               The partition assignment.\n+   */\n+  def verifyReplicasAndBrokersInAssignment(zkClient: KafkaZkClient,\n+                                           proposedParts: Map[TopicPartition, Seq[Int]]): Unit = {\n+    // check that all partitions in the proposed assignment exist in the cluster\n+    val proposedTopics = proposedParts.map { case (tp, _) => tp.topic }\n+    val existingAssignment = zkClient.getReplicaAssignmentForTopics(proposedTopics.toSet)\n+    val nonExistentPartitions = proposedParts.map { case (tp, _) => tp }.filterNot(existingAssignment.contains)\n+    if (nonExistentPartitions.nonEmpty)\n+      throw new AdminCommandFailedException(\"The proposed assignment contains non-existent partitions: \" +\n+        nonExistentPartitions)\n+\n+    // check that all brokers in the proposed assignment exist in the cluster\n+    val existingBrokerIDs = zkClient.getSortedBrokerList\n+    val nonExistingBrokerIDs = proposedParts.toMap.values.flatten.filterNot(existingBrokerIDs.contains).toSet\n+    if (nonExistingBrokerIDs.nonEmpty)\n+      throw new AdminCommandFailedException(\"The proposed assignment contains non-existent brokerIDs: \" + nonExistingBrokerIDs.mkString(\",\"))\n+  }\n+\n+  /**\n+   * Execute the given partition reassignments.\n+   *\n+   * @param adminClient       The admin client object to use.\n+   * @param reassignments     A map from topic names to target replica assignments.\n+   * @return                  A map from partition objects to error strings.\n+   */\n+  def alterPartitionReassignments(adminClient: Admin,\n+                                  reassignments: Map[TopicPartition, Seq[Int]])\n+                                  : Map[TopicPartition, Throwable] = {\n+    val results: Map[TopicPartition, KafkaFuture[Void]] =\n+      adminClient.alterPartitionReassignments(reassignments.map {\n+        case (part, replicas) => {\n+          (part, Optional.of(new NewPartitionReassignment(replicas.map(Integer.valueOf(_)).asJava)))\n+        }\n+      }.asJava).values().asScala\n+    results.flatMap {\n+      case (part, future) => {\n+        try {\n+          future.get()\n+          None\n+        } catch {\n+          case t: ExecutionException => Some(part, t.getCause())\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Cancel the given partition reassignments.\n+   *\n+   * @param adminClient       The admin client object to use.\n+   * @param reassignments     The partition reassignments to cancel.\n+   * @return                  A map from partition objects to error strings.\n+   */\n+  def cancelPartitionReassignments(adminClient: Admin,\n+                                  reassignments: Set[TopicPartition])\n+  : Map[TopicPartition, Throwable] = {\n+    val results: Map[TopicPartition, KafkaFuture[Void]] =\n+      adminClient.alterPartitionReassignments(reassignments.map {\n+          (_, (None: Option[NewPartitionReassignment]).asJava)\n+        }.toMap.asJava).values().asScala\n+    results.flatMap {\n+      case (part, future) => {\n+        try {\n+          future.get()\n+          None\n+        } catch {\n+          case t: ExecutionException => Some(part, t.getCause())\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Calculate the global map of all partitions that are moving.\n+   *\n+   * @param currentReassignments    The currently active reassignments.\n+   * @param proposedReassignments   The proposed reassignments (destinations replicas only).\n+   * @param currentParts            The current location of the partitions that we are\n+   *                                proposing to move.\n+   * @return                        A map from topic name to partition map.\n+   *                                The partition map is keyed on partition index and contains\n+   *                                the movements for that partition.\n+   */\n+  def calculateMoveMap(currentReassignments: Map[TopicPa", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyMjY2OQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389722669", "bodyText": "If we get a ReplicaNotAvailableException for one replica throughout the whole timeoutMs duration , we'd print out that all movements failed. Shall we print pendingReplicas?", "author": "stanislavkozlovski", "createdAt": "2020-03-09T14:24:52Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1521 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(val sources: mutable.Set[Int],\n+                                  val destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is still progressing.\n+   */\n+  sealed case class PartitionReassignmentState(val currentReplicas: Seq[Int],\n+                                               val targetReplicas: Seq[Int],\n+                                               val done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {\n+    /**\n+     * True if the move is still progressing.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica move state where the source log directory is missng.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(val currentLogDir: String,\n+                                    val targetLogDir: String,\n+                                    val futureLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(val currentLogDir: String,\n+                                       val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        // Use AdminClient to manage reassignments.\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        // TODO: add --show and --cancel\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(adminClient,\n+            opts.options.has(opts.additionalOpt),\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt),\n+            opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.cancelOpt)) {\n+          cancelAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.listOpt)) {\n+          showReassignments(adminClient)\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      } else {\n+        // Use the legacy ZooKeeper client to manage reassignments.\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt))\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n-    } else {\n-      None\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states, whether there are ongoing\n+   *                              reassignments, replica move states, and whether there are\n+   *                              ongoing replica moves.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : (Map[TopicPartition, PartitionReassignmentState], Boolean,\n+                         Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetReplicas)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    (partStates, partsOngoing, moveStates, movesOngoing)\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states and whether there are ongoing\n+   *                              legacy reassignments.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    if (!targetReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    (partStates, partsOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      case topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n+        }\n       }\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    bld.mkString(System.lineSeparator())\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n-      }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n         }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val moveStates = findReplicaMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findReplicaMoveStates(adminClient: Admin,\n+                            targetMoves: Map[TopicPartitionReplica, String])\n+                            : Map[TopicPartitionReplica, ReplicaMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: ReplicaMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingLogDirMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingReplicaMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, ReplicaMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    allBrokerIds.foreach {\n+      case brokerId => configOps.put(\n+        new ConfigResource(ConfigResource.Type.BROKER, brokerId.toString),\n+        brokerLevelThrottles.map(throttle => new AlterConfigOp(\n+          new ConfigEntry(throttle, null), OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   */\n+  def clearBrokerLevelThrottles(zkClient: KafkaZkClient): Unit = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val brokerMetadatas = adminZkClient.getBrokerMetadatas(rackAwareMode, Some(brokerListToReassign))\n+    for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n+      if (!brokerLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n+      }\n+    }\n+  }\n \n-    val partitionsToBeReassigned = mutable.Map[TopicPartition, Seq[Int]]()\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    topics.foreach {\n+      topicName => configOps.put(\n+        new ConfigResource(ConfigResource.Type.TOPIC, topicName),\n+        topicLevelThrottles.map(throttle => new AlterConfigOp(new ConfigEntry(throttle, null),\n+          OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(zkClient: KafkaZkClient, topics: Set[String]): Unit = {\n+    val adminZkClient = new AdminZkClient(zkClient)\n+    for (topic <- topics) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n+      if (!topicLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeTopicConfig(topic, configs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --generate command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(adminClient: Admin,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = getReplicaAssignmentForTopics(adminClient, topicsToReassign)\n+    val brokerMetadatas = if (enableRackAwareness) {\n+      getBrokerRackInformation(adminClient, brokersToReassign)\n+    } else {\n+      brokersToReassign.map(new BrokerMetadata(_, None))\n+    }\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * The legacy entry point for the --generate command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(zkClient: KafkaZkClient,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+    val brokerMetadatas = if (enableRackAwareness) {\n+      getBrokerRackInformation(zkClient, brokersToReassign)\n+    } else {\n+      brokersToReassign.map(new BrokerMetadata(_, None))\n+    }\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * Calculate the new partition assignments to suggest in --generate.\n+   *\n+   * @param currentAssignment  The current partition assignments.\n+   * @param brokerMetadatas    The rack information for each broker.\n+   *\n+   * @return                   A map from partitions to the proposed assignments for each.\n+   */\n+  def calculateAssignment(currentAssignment: Map[TopicPartition, Seq[Int]],\n+                          brokerMetadatas: Seq[BrokerMetadata])\n+                          : Map[TopicPartition, Seq[Int]] = {\n+    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n+    val proposedAssignments = mutable.Map[TopicPartition, Seq[Int]]()\n     groupedByTopic.foreach { case (topic, assignment) =>\n       val (_, replicas) = assignment.head\n-      val assignedReplicas = AdminUtils.assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n-      partitionsToBeReassigned ++= assignedReplicas.map { case (partition, replicas) =>\n+      val assignedReplicas = AdminUtils.\n+        assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n+      proposedAssignments ++= assignedReplicas.map { case (partition, replicas) =>\n         new TopicPartition(topic, partition) -> replicas\n       }\n     }\n-    (partitionsToBeReassigned, currentAssignment)\n+    proposedAssignments\n+  }\n+\n+  /**\n+   * Get the current replica assignments for some topics.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics          The topics to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForTopics(adminClient: Admin,\n+                                    topics: Seq[String])\n+                                    : Map[TopicPartition, Seq[Int]] = {\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.map {\n+          info => (new TopicPartition(topicName, info.partition()),\n+            info.replicas().asScala.map(_.id()))\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val reassignmentJsonFile =  opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val reassignmentJsonString = Utils.readFileAsString(reassignmentJsonFile)\n-    val interBrokerThrottle = opts.options.valueOf(opts.interBrokerThrottleOpt)\n-    val replicaAlterLogDirsThrottle = opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt)\n-    val timeoutMs = opts.options.valueOf(opts.timeoutOpt)\n-    executeAssignment(zkClient, adminClientOpt, reassignmentJsonString, Throttle(interBrokerThrottle, replicaAlterLogDirsThrottle), timeoutMs)\n+  /**\n+   * Get the current replica assignments for some partitions.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param partitions      The partitions to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForPartitions(adminClient: Admin,\n+                                        partitions: Set[TopicPartition])\n+                                        : Map[TopicPartition, Seq[Int]] = {\n+    val topics = new mutable.HashSet[String]()\n+    partitions.foreach(topics += _.topic)\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.flatMap {\n+          info => if (partitions.contains(new TopicPartition(topicName, info.partition()))) {\n+            Some(new TopicPartition(topicName, info.partition()),\n+                info.replicas().asScala.map(_.id()))\n+          } else {\n+            None\n+          }\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], reassignmentJsonString: String, throttle: Throttle, timeoutMs: Long = 10000L): Unit = {\n-    val (partitionAssignment, replicaAssignment) = parseAndValidate(zkClient, reassignmentJsonString)\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param adminClient    The AdminClient object.\n+   * @param brokers        The brokers to gather metadata about.\n+   *\n+   * @return               The metadata for each broker.\n+   */\n+  def getBrokerRackInformation(adminClient: Admin,\n+                               brokers: Seq[Int]): Seq[BrokerMetadata] = {\n+    val brokerSet = brokers.toSet\n+    val results = adminClient.describeCluster().nodes().get().asScala.flatMap(\n+      node => if (!brokerSet.contains(node.id())) {\n+        None\n+      } else {\n+        if (node.rack() != null) {\n+          Some(new BrokerMetadata(node.id(), Some(node.rack())))\n+        } else {\n+          Some(new BrokerMetadata(node.id(), None))\n+        }\n+      }\n+    ).toSeq\n+    val numRackless = results.count(_.rack.isEmpty)\n+    if ((numRackless != 0) && (numRackless != results.size)) {\n+      throw new AdminOperationException(\"Not all brokers have rack information. Add \" +\n+        \"--disable-rack-aware in command line to make replica assignment without rack \" +\n+        \"information.\")\n+    }\n+    results\n+  }\n+\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param zkClient       The ZooKeeper client to use.\n+   * @param brokers        The brokers to gather metadata about.\n+   *\n+   * @return               The metadata for each broker.\n+   */\n+  def getBrokerRackInformation(zkClient: KafkaZkClient,\n+                               brokers: Seq[Int]): Seq[BrokerMetadata] = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val reassignPartitionsCommand = new ReassignPartitionsCommand(zkClient, adminClientOpt, partitionAssignment.toMap, replicaAssignment, adminZkClient)\n+    adminZkClient.getBrokerMetadatas(RackAwareMode.Enforced, Some(brokers))\n+  }\n+\n+  /**\n+   * Parse and validate data gathered from the command-line for --generate\n+   * In particular, we parse the JSON and validate that duplicate brokers and\n+   * topics don't appear.\n+   *\n+   * @param reassignmentJson       The JSON passed to --generate .\n+   * @param brokerList             A list of brokers passed to --generate.\n+   *\n+   * @return                       A tuple of brokers to reassign, topics to reassign\n+   */\n+  def parseGenerateAssignmentArgs(reassignmentJson: String,\n+                                  brokerList: String): (Seq[Int], Seq[String]) = {\n+    val brokerListToReassign = brokerList.split(',').map(_.toInt)\n+    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n+    if (duplicateReassignments.nonEmpty)\n+      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".\n+        format(duplicateReassignments.mkString(\",\")))\n+    val topicsToReassign = parseTopicsData(reassignmentJson)\n+    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n+    if (duplicateTopicsToReassign.nonEmpty)\n+      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".\n+        format(duplicateTopicsToReassign.mkString(\",\")))\n+    (brokerListToReassign, topicsToReassign)\n+  }\n+\n+  /**\n+   * The entry point for the --execute and --execute-additional commands.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param additional                  Whether --additional was passed.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   * @param replicaAlterLogDirsThrottle The replica throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeAssignment(adminClient: Admin,\n+                        additional: Boolean,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long = -1L,\n+                        replicaAlterLogDirsThrottle: Long = -1L,\n+                        timeoutMs: Long = 10000L,\n+                        time: Time = Time.SYSTEM): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    // If there is an existing assignment, check for --additional before proceeding.\n+    // This helps avoid surprising users.\n+    if (!additional && !currentReassignments.isEmpty) {\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n+    }\n+    verifyBrokerIds(adminClient, proposedParts.values.flatten.toSet)\n+    val currentParts = getReplicaAssignmentForPartitions(adminClient, proposedParts.keySet.toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+    if (interBrokerThrottle >= 0 || replicaAlterLogDirsThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(currentReassignments, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(adminClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      val movingBrokers = calculateMovingBrokers(proposedReplicas.keySet.toSet)\n+      modifyBrokerThrottles(adminClient,\n+        reassigningBrokers, interBrokerThrottle,\n+        movingBrokers, replicaAlterLogDirsThrottle)\n+      if (interBrokerThrottle >= 0) {\n+        println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+      }\n+      if (replicaAlterLogDirsThrottle >= 0) {\n+        println(s\"The replica-alter-dir throttle limit was set to ${replicaAlterLogDirsThrottle} B/s\")\n+      }\n+    }\n+\n+    // Execute the partition reassignments.\n+    val errors = alterPartitionReassignments(adminClient, proposedParts)\n+    if (!errors.isEmpty) {\n+      throw new TerseReassignmentFailureException(\n+        \"Error reassigning partition(s):%n%s\".format(\n+          errors.keySet.toBuffer.sortWith(compareTopicPartitions).map {\n+            case part => s\"${part}: ${errors(part).getMessage}\"\n+          }.mkString(System.lineSeparator())))\n+    }\n+    println(s\"Started ${proposedParts.size} partition reassignment(s)\")\n \n-    // If there is an existing rebalance running, attempt to change its throttle\n-    if (zkClient.reassignPartitionsInProgress()) {\n-      println(\"There is an existing assignment running.\")\n-      reassignPartitionsCommand.maybeLimit(throttle)\n+    if (!proposedReplicas.isEmpty) {\n+      executeMoves(adminClient, proposedReplicas, timeoutMs, time)\n+    }\n+  }\n+\n+  /**\n+   * Execute some partition log directory movements.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param proposedReplicas            A map from TopicPartitionReplicas to the\n+   *                                    directories to move them to.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeMoves(adminClient: Admin,\n+                   proposedReplicas: Map[TopicPartitionReplica, String],\n+                   timeoutMs: Long,\n+                   time: Time): Unit = {\n+    val startTimeMs = time.milliseconds()\n+    val pendingReplicas = new mutable.HashMap[TopicPartitionReplica, String]()\n+    pendingReplicas ++= proposedReplicas\n+    var done = false\n+    do {\n+      pendingReplicas --= alterReplicaLogDirs(adminClient, pendingReplicas)\n+      if (pendingReplicas.isEmpty) {\n+        println(s\"Started ${proposedReplicas.size} replica log directory reassignment(s)\")\n+        done = true\n+      } else if (time.milliseconds() >= startTimeMs + timeoutMs) {\n+        throw new TerseReassignmentFailureException(\n+          \"Timed out before replica log dir(s) could be reassigned:%n%s\".format(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTk1Nzc1MQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389957751", "bodyText": "That's a good idea.  In general, we should just print out all the partitions or log directories that were affected for each operation.", "author": "cmccabe", "createdAt": "2020-03-09T20:58:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyMjY2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTczNDQxOA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389734418", "bodyText": "Do we have a test to ensure that the old way to run replica log dir reassignments with ZK works? i.e ./kafka-reassign-partitions --zookeeper ZK --bootstrap-server server --reassignment-json-file file_with_dir_reassignments.json\nFrom reading the code, I think so - but always worth making sure", "author": "stanislavkozlovski", "createdAt": "2020-03-09T14:41:58Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1521 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(val sources: mutable.Set[Int],\n+                                  val destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is still progressing.\n+   */\n+  sealed case class PartitionReassignmentState(val currentReplicas: Seq[Int],\n+                                               val targetReplicas: Seq[Int],\n+                                               val done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {\n+    /**\n+     * True if the move is still progressing.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica move state where the source log directory is missng.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(val currentLogDir: String,\n+                                    val targetLogDir: String,\n+                                    val futureLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(val currentLogDir: String,\n+                                       val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(val targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        // Use AdminClient to manage reassignments.\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        // TODO: add --show and --cancel\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(adminClient,\n+            opts.options.has(opts.additionalOpt),\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt),\n+            opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.cancelOpt)) {\n+          cancelAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.listOpt)) {\n+          showReassignments(adminClient)\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      } else {\n+        // Use the legacy ZooKeeper client to manage reassignments.\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt))\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n-    } else {\n-      None\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states, whether there are ongoing\n+   *                              reassignments, replica move states, and whether there are\n+   *                              ongoing replica moves.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : (Map[TopicPartition, PartitionReassignmentState], Boolean,\n+                         Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetReplicas)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    (partStates, partsOngoing, moveStates, movesOngoing)\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states and whether there are ongoing\n+   *                              legacy reassignments.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    if (!targetReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    (partStates, partsOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      case topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n+        }\n       }\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    bld.mkString(System.lineSeparator())\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n-      }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n         }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val moveStates = findReplicaMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findReplicaMoveStates(adminClient: Admin,\n+                            targetMoves: Map[TopicPartitionReplica, String])\n+                            : Map[TopicPartitionReplica, ReplicaMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: ReplicaMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingLogDirMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingReplicaMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, ReplicaMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    allBrokerIds.foreach {\n+      case brokerId => configOps.put(\n+        new ConfigResource(ConfigResource.Type.BROKER, brokerId.toString),\n+        brokerLevelThrottles.map(throttle => new AlterConfigOp(\n+          new ConfigEntry(throttle, null), OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   */\n+  def clearBrokerLevelThrottles(zkClient: KafkaZkClient): Unit = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val brokerMetadatas = adminZkClient.getBrokerMetadatas(rackAwareMode, Some(brokerListToReassign))\n+    for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n+      if (!brokerLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n+      }\n+    }\n+  }\n \n-    val partitionsToBeReassigned = mutable.Map[TopicPartition, Seq[Int]]()\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    topics.foreach {\n+      topicName => configOps.put(\n+        new ConfigResource(ConfigResource.Type.TOPIC, topicName),\n+        topicLevelThrottles.map(throttle => new AlterConfigOp(new ConfigEntry(throttle, null),\n+          OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(zkClient: KafkaZkClient, topics: Set[String]): Unit = {\n+    val adminZkClient = new AdminZkClient(zkClient)\n+    for (topic <- topics) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n+      if (!topicLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeTopicConfig(topic, configs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --generate command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(adminClient: Admin,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = getReplicaAssignmentForTopics(adminClient, topicsToReassign)\n+    val brokerMetadatas = if (enableRackAwareness) {\n+      getBrokerRackInformation(adminClient, brokersToReassign)\n+    } else {\n+      brokersToReassign.map(new BrokerMetadata(_, None))\n+    }\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * The legacy entry point for the --generate command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(zkClient: KafkaZkClient,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+    val brokerMetadatas = if (enableRackAwareness) {\n+      getBrokerRackInformation(zkClient, brokersToReassign)\n+    } else {\n+      brokersToReassign.map(new BrokerMetadata(_, None))\n+    }\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * Calculate the new partition assignments to suggest in --generate.\n+   *\n+   * @param currentAssignment  The current partition assignments.\n+   * @param brokerMetadatas    The rack information for each broker.\n+   *\n+   * @return                   A map from partitions to the proposed assignments for each.\n+   */\n+  def calculateAssignment(currentAssignment: Map[TopicPartition, Seq[Int]],\n+                          brokerMetadatas: Seq[BrokerMetadata])\n+                          : Map[TopicPartition, Seq[Int]] = {\n+    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n+    val proposedAssignments = mutable.Map[TopicPartition, Seq[Int]]()\n     groupedByTopic.foreach { case (topic, assignment) =>\n       val (_, replicas) = assignment.head\n-      val assignedReplicas = AdminUtils.assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n-      partitionsToBeReassigned ++= assignedReplicas.map { case (partition, replicas) =>\n+      val assignedReplicas = AdminUtils.\n+        assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n+      proposedAssignments ++= assignedReplicas.map { case (partition, replicas) =>\n         new TopicPartition(topic, partition) -> replicas\n       }\n     }\n-    (partitionsToBeReassigned, currentAssignment)\n+    proposedAssignments\n+  }\n+\n+  /**\n+   * Get the current replica assignments for some topics.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics          The topics to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForTopics(adminClient: Admin,\n+                                    topics: Seq[String])\n+                                    : Map[TopicPartition, Seq[Int]] = {\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.map {\n+          info => (new TopicPartition(topicName, info.partition()),\n+            info.replicas().asScala.map(_.id()))\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val reassignmentJsonFile =  opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val reassignmentJsonString = Utils.readFileAsString(reassignmentJsonFile)\n-    val interBrokerThrottle = opts.options.valueOf(opts.interBrokerThrottleOpt)\n-    val replicaAlterLogDirsThrottle = opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt)\n-    val timeoutMs = opts.options.valueOf(opts.timeoutOpt)\n-    executeAssignment(zkClient, adminClientOpt, reassignmentJsonString, Throttle(interBrokerThrottle, replicaAlterLogDirsThrottle), timeoutMs)\n+  /**\n+   * Get the current replica assignments for some partitions.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param partitions      The partitions to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForPartitions(adminClient: Admin,\n+                                        partitions: Set[TopicPartition])\n+                                        : Map[TopicPartition, Seq[Int]] = {\n+    val topics = new mutable.HashSet[String]()\n+    partitions.foreach(topics += _.topic)\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.flatMap {\n+          info => if (partitions.contains(new TopicPartition(topicName, info.partition()))) {\n+            Some(new TopicPartition(topicName, info.partition()),\n+                info.replicas().asScala.map(_.id()))\n+          } else {\n+            None\n+          }\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], reassignmentJsonString: String, throttle: Throttle, timeoutMs: Long = 10000L): Unit = {\n-    val (partitionAssignment, replicaAssignment) = parseAndValidate(zkClient, reassignmentJsonString)\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param adminClient    The AdminClient object.\n+   * @param brokers        The brokers to gather metadata about.\n+   *\n+   * @return               The metadata for each broker.\n+   */\n+  def getBrokerRackInformation(adminClient: Admin,\n+                               brokers: Seq[Int]): Seq[BrokerMetadata] = {\n+    val brokerSet = brokers.toSet\n+    val results = adminClient.describeCluster().nodes().get().asScala.flatMap(\n+      node => if (!brokerSet.contains(node.id())) {\n+        None\n+      } else {\n+        if (node.rack() != null) {\n+          Some(new BrokerMetadata(node.id(), Some(node.rack())))\n+        } else {\n+          Some(new BrokerMetadata(node.id(), None))\n+        }\n+      }\n+    ).toSeq\n+    val numRackless = results.count(_.rack.isEmpty)\n+    if ((numRackless != 0) && (numRackless != results.size)) {\n+      throw new AdminOperationException(\"Not all brokers have rack information. Add \" +\n+        \"--disable-rack-aware in command line to make replica assignment without rack \" +\n+        \"information.\")\n+    }\n+    results\n+  }\n+\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param zkClient       The ZooKeeper client to use.\n+   * @param brokers        The brokers to gather metadata about.\n+   *\n+   * @return               The metadata for each broker.\n+   */\n+  def getBrokerRackInformation(zkClient: KafkaZkClient,\n+                               brokers: Seq[Int]): Seq[BrokerMetadata] = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val reassignPartitionsCommand = new ReassignPartitionsCommand(zkClient, adminClientOpt, partitionAssignment.toMap, replicaAssignment, adminZkClient)\n+    adminZkClient.getBrokerMetadatas(RackAwareMode.Enforced, Some(brokers))\n+  }\n+\n+  /**\n+   * Parse and validate data gathered from the command-line for --generate\n+   * In particular, we parse the JSON and validate that duplicate brokers and\n+   * topics don't appear.\n+   *\n+   * @param reassignmentJson       The JSON passed to --generate .\n+   * @param brokerList             A list of brokers passed to --generate.\n+   *\n+   * @return                       A tuple of brokers to reassign, topics to reassign\n+   */\n+  def parseGenerateAssignmentArgs(reassignmentJson: String,\n+                                  brokerList: String): (Seq[Int], Seq[String]) = {\n+    val brokerListToReassign = brokerList.split(',').map(_.toInt)\n+    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n+    if (duplicateReassignments.nonEmpty)\n+      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".\n+        format(duplicateReassignments.mkString(\",\")))\n+    val topicsToReassign = parseTopicsData(reassignmentJson)\n+    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n+    if (duplicateTopicsToReassign.nonEmpty)\n+      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".\n+        format(duplicateTopicsToReassign.mkString(\",\")))\n+    (brokerListToReassign, topicsToReassign)\n+  }\n+\n+  /**\n+   * The entry point for the --execute and --execute-additional commands.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param additional                  Whether --additional was passed.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   * @param replicaAlterLogDirsThrottle The replica throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeAssignment(adminClient: Admin,\n+                        additional: Boolean,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long = -1L,\n+                        replicaAlterLogDirsThrottle: Long = -1L,\n+                        timeoutMs: Long = 10000L,\n+                        time: Time = Time.SYSTEM): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    // If there is an existing assignment, check for --additional before proceeding.\n+    // This helps avoid surprising users.\n+    if (!additional && !currentReassignments.isEmpty) {\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n+    }\n+    verifyBrokerIds(adminClient, proposedParts.values.flatten.toSet)\n+    val currentParts = getReplicaAssignmentForPartitions(adminClient, proposedParts.keySet.toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+    if (interBrokerThrottle >= 0 || replicaAlterLogDirsThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(currentReassignments, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(adminClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      val movingBrokers = calculateMovingBrokers(proposedReplicas.keySet.toSet)\n+      modifyBrokerThrottles(adminClient,\n+        reassigningBrokers, interBrokerThrottle,\n+        movingBrokers, replicaAlterLogDirsThrottle)\n+      if (interBrokerThrottle >= 0) {\n+        println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+      }\n+      if (replicaAlterLogDirsThrottle >= 0) {\n+        println(s\"The replica-alter-dir throttle limit was set to ${replicaAlterLogDirsThrottle} B/s\")\n+      }\n+    }\n+\n+    // Execute the partition reassignments.\n+    val errors = alterPartitionReassignments(adminClient, proposedParts)\n+    if (!errors.isEmpty) {\n+      throw new TerseReassignmentFailureException(\n+        \"Error reassigning partition(s):%n%s\".format(\n+          errors.keySet.toBuffer.sortWith(compareTopicPartitions).map {\n+            case part => s\"${part}: ${errors(part).getMessage}\"\n+          }.mkString(System.lineSeparator())))\n+    }\n+    println(s\"Started ${proposedParts.size} partition reassignment(s)\")\n \n-    // If there is an existing rebalance running, attempt to change its throttle\n-    if (zkClient.reassignPartitionsInProgress()) {\n-      println(\"There is an existing assignment running.\")\n-      reassignPartitionsCommand.maybeLimit(throttle)\n+    if (!proposedReplicas.isEmpty) {\n+      executeMoves(adminClient, proposedReplicas, timeoutMs, time)\n+    }\n+  }\n+\n+  /**\n+   * Execute some partition log directory movements.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param proposedReplicas            A map from TopicPartitionReplicas to the\n+   *                                    directories to move them to.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeMoves(adminClient: Admin,\n+                   proposedReplicas: Map[TopicPartitionReplica, String],\n+                   timeoutMs: Long,\n+                   time: Time): Unit = {\n+    val startTimeMs = time.milliseconds()\n+    val pendingReplicas = new mutable.HashMap[TopicPartitionReplica, String]()\n+    pendingReplicas ++= proposedReplicas\n+    var done = false\n+    do {\n+      pendingReplicas --= alterReplicaLogDirs(adminClient, pendingReplicas)\n+      if (pendingReplicas.isEmpty) {\n+        println(s\"Started ${proposedReplicas.size} replica log directory reassignment(s)\")\n+        done = true\n+      } else if (time.milliseconds() >= startTimeMs + timeoutMs) {\n+        throw new TerseReassignmentFailureException(\n+          \"Timed out before replica log dir(s) could be reassigned:%n%s\".format(\n+            pendingReplicas.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).map {\n+              case replica => s\"${replica.toString}\"\n+            }.mkString(System.lineSeparator())))\n+      } else {\n+        // If a replica has been moved to a new host and we also specified a particular\n+        // log directory, we will have to keep retrying the alterReplicaLogDirs\n+        // call.  It can't take effect until the replica is moved to that host.\n+        time.sleep(100)\n+      }\n+    } while (!done)\n+  }\n+\n+  /**\n+   * Entry point for the --show-reassignments command.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   */\n+  def showReassignments(adminClient: Admin): Unit = {\n+    println(curReassignmentsToString(adminClient))\n+  }\n+\n+  /**\n+   * Convert the current partition reassignments to text.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   * @return              A string describing the current partition reassignments.\n+   */\n+  def curReassignmentsToString(adminClient: Admin): String = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val text = currentReassignments.keySet.toList.sortWith(compareTopicPartitions).map {\n+      case part =>\n+        val reassignment = currentReassignments(part)\n+        val replicas = reassignment.replicas().asScala\n+        val addingReplicas = reassignment.addingReplicas().asScala\n+        val removingReplicas = reassignment.removingReplicas().asScala\n+        \"%s: replicas: %s.%s%s\".format(part, replicas.mkString(\",\"),\n+          if (addingReplicas.isEmpty) \"\" else\n+            \" adding: %s.\".format(addingReplicas.mkString(\",\")),\n+          if (removingReplicas.isEmpty) \"\" else\n+            \" removing: %s.\".format(removingReplicas.mkString(\",\")))\n+    }.mkString(System.lineSeparator())\n+    if (text.isEmpty) {\n+      \"No partition reassignments found.\"\n     } else {\n-      printCurrentAssignment(zkClient, partitionAssignment.map(_._1.topic))\n-      if (throttle.interBrokerLimit >= 0 || throttle.replicaAlterLogDirsLimit >= 0)\n-        println(String.format(\"Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.\"))\n-      if (reassignPartitionsCommand.reassignPartitions(throttle, timeoutMs)) {\n-        println(\"Successfully started reassignment of partitions.\")\n-      } else\n-        println(\"Failed to reassign partitions %s\".format(partitionAssignment))\n+      \"Current partition reassignments:%n%s\".format(text)\n+    }\n+  }\n+\n+  /**\n+   * Verify that all the brokers in an assignment exist.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param brokers                     The broker IDs to verify.\n+   */\n+  def verifyBrokerIds(adminClient: Admin, brokers: Set[Int]): Unit = {\n+    val allNodeIds = adminClient.describeCluster().nodes().get().asScala.map(_.id).toSet\n+    brokers.find(!allNodeIds.contains(_)).map {\n+      case id => throw new AdminCommandFailedException(s\"Unknown broker id ${id}\")\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --execute command.\n+   *\n+   * @param zkClient                    The ZooKeeper client to use.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   */\n+  def executeAssignment(zkClient: KafkaZkClient,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    if (!proposedReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg5MDgzNw==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r389890837", "bodyText": "If you supply both --zookeeper and --bootstrap-server, then --zookeeper is ignored.  So there's no need for separate tests for the combination.", "author": "cmccabe", "createdAt": "2020-03-09T18:47:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTczNDQxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA3NTgyOA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r391075828", "bodyText": "It would be nice if we gave a warning that the --zookeeper flag was being ignore in this case", "author": "mumrah", "createdAt": "2020-03-11T15:54:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTczNDQxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAzMjM0Ng==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r391032346", "bodyText": "Is this case necessary? Since there's no type binding and no \"null\" matches, wouldn't it just match everything?", "author": "mumrah", "createdAt": "2020-03-11T14:56:41Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1531 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(sources: mutable.Set[Int],\n+                                  destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is done.\n+   */\n+  sealed case class PartitionReassignmentState(currentReplicas: Seq[Int],\n+                                               targetReplicas: Seq[Int],\n+                                               done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {\n+    /**\n+     * True if the move is done without errors.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica move state where the source log directory is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(currentLogDir: String,\n+                                    targetLogDir: String,\n+                                    futureLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(currentLogDir: String,\n+                                       targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        // Use AdminClient to manage reassignments.\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(adminClient,\n+            opts.options.has(opts.additionalOpt),\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt),\n+            opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.cancelOpt)) {\n+          cancelAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.listOpt)) {\n+          listReassignments(adminClient)\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      } else {\n+        // Use the legacy ZooKeeper client to manage reassignments.\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt))\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n-    } else {\n-      None\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states, whether there are ongoing\n+   *                              reassignments, replica move states, and whether there are\n+   *                              ongoing replica moves.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : (Map[TopicPartition, PartitionReassignmentState], Boolean,\n+                         Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetReplicas)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    (partStates, partsOngoing, moveStates, movesOngoing)\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states and whether there are ongoing\n+   *                              legacy reassignments.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    if (!targetReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    (partStates, partsOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      case topicPartition => {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAzNzgwMg==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r391037802", "bodyText": "Maybe we could split this into two \"main\" methods: one for AdminClient and one for ZK? Might make this a bit more readable.", "author": "mumrah", "createdAt": "2020-03-11T15:03:49Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1531 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(sources: mutable.Set[Int],\n+                                  destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is done.\n+   */\n+  sealed case class PartitionReassignmentState(currentReplicas: Seq[Int],\n+                                               targetReplicas: Seq[Int],\n+                                               done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {\n+    /**\n+     * True if the move is done without errors.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica move state where the source log directory is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(currentLogDir: String,\n+                                    targetLogDir: String,\n+                                    futureLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(currentLogDir: String,\n+                                       targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI5MTk1OQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r391291959", "bodyText": "ok", "author": "cmccabe", "createdAt": "2020-03-11T21:59:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAzNzgwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA1NTM5Mw==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r391055393", "bodyText": "Since these two generateAssignment methods are nearly identical, do you think it's worth trying to factor out the common parts? Maybe we could have a single generateAssignment that accepts an Either[Admin, ZkClient]?", "author": "mumrah", "createdAt": "2020-03-11T15:27:30Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1531 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(sources: mutable.Set[Int],\n+                                  destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is done.\n+   */\n+  sealed case class PartitionReassignmentState(currentReplicas: Seq[Int],\n+                                               targetReplicas: Seq[Int],\n+                                               done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {\n+    /**\n+     * True if the move is done without errors.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica move state where the source log directory is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(currentLogDir: String,\n+                                    targetLogDir: String,\n+                                    futureLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(currentLogDir: String,\n+                                       targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        // Use AdminClient to manage reassignments.\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(adminClient,\n+            opts.options.has(opts.additionalOpt),\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt),\n+            opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.cancelOpt)) {\n+          cancelAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.listOpt)) {\n+          listReassignments(adminClient)\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      } else {\n+        // Use the legacy ZooKeeper client to manage reassignments.\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt))\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n-    } else {\n-      None\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states, whether there are ongoing\n+   *                              reassignments, replica move states, and whether there are\n+   *                              ongoing replica moves.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : (Map[TopicPartition, PartitionReassignmentState], Boolean,\n+                         Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetReplicas)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    (partStates, partsOngoing, moveStates, movesOngoing)\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states and whether there are ongoing\n+   *                              legacy reassignments.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    if (!targetReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    (partStates, partsOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      case topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n+        }\n       }\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    bld.mkString(System.lineSeparator())\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n-      }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n         }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val moveStates = findReplicaMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findReplicaMoveStates(adminClient: Admin,\n+                            targetMoves: Map[TopicPartitionReplica, String])\n+                            : Map[TopicPartitionReplica, ReplicaMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: ReplicaMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingLogDirMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingReplicaMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, ReplicaMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    allBrokerIds.foreach {\n+      case brokerId => configOps.put(\n+        new ConfigResource(ConfigResource.Type.BROKER, brokerId.toString),\n+        brokerLevelThrottles.map(throttle => new AlterConfigOp(\n+          new ConfigEntry(throttle, null), OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   */\n+  def clearBrokerLevelThrottles(zkClient: KafkaZkClient): Unit = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val brokerMetadatas = adminZkClient.getBrokerMetadatas(rackAwareMode, Some(brokerListToReassign))\n+    for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n+      if (!brokerLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n+      }\n+    }\n+  }\n \n-    val partitionsToBeReassigned = mutable.Map[TopicPartition, Seq[Int]]()\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    topics.foreach {\n+      topicName => configOps.put(\n+        new ConfigResource(ConfigResource.Type.TOPIC, topicName),\n+        topicLevelThrottles.map(throttle => new AlterConfigOp(new ConfigEntry(throttle, null),\n+          OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(zkClient: KafkaZkClient, topics: Set[String]): Unit = {\n+    val adminZkClient = new AdminZkClient(zkClient)\n+    for (topic <- topics) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n+      if (!topicLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeTopicConfig(topic, configs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --generate command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(adminClient: Admin,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = getReplicaAssignmentForTopics(adminClient, topicsToReassign)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI5MzMwNg==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r391293306", "bodyText": "Good question.  The zookeeper code is going away soon, so I don't think it's worth trying to combine the functions.\nI could write a function to factor out the \"print\" statements, but I'm not sure it's worth it since the main work is done by calculateAssignment and formatAsReassignmentJson, which are already shared code.", "author": "cmccabe", "createdAt": "2020-03-11T22:03:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA1NTM5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzEzMjY3Mg==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r393132672", "bodyText": "Yea, probably not worth the effort in this case.", "author": "mumrah", "createdAt": "2020-03-16T16:00:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA1NTM5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA3MTI0Mg==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r391071242", "bodyText": "In the old code, we always called AdminZkClient#getBrokerMetadatas regardless of the rack-aware mode. The only difference I can see for the RackAwareMode.Disabled case is that the old code would always filter down to the set of brokers register in ZK at the time of the call\ndef getBrokerMetadatas(rackAwareMode: RackAwareMode = RackAwareMode.Enforced,\n                         brokerList: Option[Seq[Int]] = None): Seq[BrokerMetadata] = {\n    val allBrokers = zkClient.getAllBrokersInCluster\n    val brokers = brokerList.map(brokerIds => allBrokers.filter(b => brokerIds.contains(b.id))).getOrElse(allBrokers)\n    ...\n    case RackAwareMode.Disabled => brokers.map(broker => BrokerMetadata(broker.id, None))\nDoes this mean that the new code will generate an assignment for a broker that is currently offline, whereas before it would not?", "author": "mumrah", "createdAt": "2020-03-11T15:48:11Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1531 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelReplicaThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelReplicaThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(sources: mutable.Set[Int],\n+                                  destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is done.\n+   */\n+  sealed case class PartitionReassignmentState(currentReplicas: Seq[Int],\n+                                               targetReplicas: Seq[Int],\n+                                               done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait ReplicaMoveState {\n+    /**\n+     * True if the move is done without errors.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica move state where the source log directory is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(currentLogDir: String,\n+                                    targetLogDir: String,\n+                                    futureLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(currentLogDir: String,\n+                                       targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(targetLogDir: String)\n+      extends ReplicaMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        // Use AdminClient to manage reassignments.\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(adminClient,\n+            opts.options.has(opts.additionalOpt),\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt),\n+            opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.cancelOpt)) {\n+          cancelAssignment(adminClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt),\n+            opts.options.valueOf(opts.timeoutOpt))\n+        } else if (opts.options.has(opts.listOpt)) {\n+          listReassignments(adminClient)\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      } else {\n+        // Use the legacy ZooKeeper client to manage reassignments.\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        if (opts.options.has(opts.verifyOpt)) {\n+          verifyAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.has(opts.preserveThrottlesOpt))\n+        } else if (opts.options.has(opts.generateOpt)) {\n+          generateAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+            opts.options.valueOf(opts.brokerListOpt),\n+            !opts.options.has(opts.disableRackAware))\n+        } else if (opts.options.has(opts.executeOpt)) {\n+          executeAssignment(zkClient,\n+            Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+            opts.options.valueOf(opts.interBrokerThrottleOpt))\n+        } else {\n+          throw new RuntimeException(\"Unsupported action.\")\n+        }\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n-    } else {\n-      None\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states, whether there are ongoing\n+   *                              reassignments, replica move states, and whether there are\n+   *                              ongoing replica moves.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : (Map[TopicPartition, PartitionReassignmentState], Boolean,\n+                         Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetReplicas)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    (partStates, partsOngoing, moveStates, movesOngoing)\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A tuple of partition states and whether there are ongoing\n+   *                              legacy reassignments.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (targetParts, targetReplicas) = parsePartitionReassignmentData(jsonString)\n+    if (!targetReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    (partStates, partsOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      case topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n+        }\n       }\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    bld.mkString(System.lineSeparator())\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n-      }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n         }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, ReplicaMoveState], Boolean) = {\n+    val moveStates = findReplicaMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findReplicaMoveStates(adminClient: Admin,\n+                            targetMoves: Map[TopicPartitionReplica, String])\n+                            : Map[TopicPartitionReplica, ReplicaMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: ReplicaMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingLogDirMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingReplicaMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, ReplicaMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    allBrokerIds.foreach {\n+      case brokerId => configOps.put(\n+        new ConfigResource(ConfigResource.Type.BROKER, brokerId.toString),\n+        brokerLevelThrottles.map(throttle => new AlterConfigOp(\n+          new ConfigEntry(throttle, null), OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   */\n+  def clearBrokerLevelThrottles(zkClient: KafkaZkClient): Unit = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val brokerMetadatas = adminZkClient.getBrokerMetadatas(rackAwareMode, Some(brokerListToReassign))\n+    for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n+      if (!brokerLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n+      }\n+    }\n+  }\n \n-    val partitionsToBeReassigned = mutable.Map[TopicPartition, Seq[Int]]()\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    topics.foreach {\n+      topicName => configOps.put(\n+        new ConfigResource(ConfigResource.Type.TOPIC, topicName),\n+        topicLevelThrottles.map(throttle => new AlterConfigOp(new ConfigEntry(throttle, null),\n+          OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(zkClient: KafkaZkClient, topics: Set[String]): Unit = {\n+    val adminZkClient = new AdminZkClient(zkClient)\n+    for (topic <- topics) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n+      if (!topicLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeTopicConfig(topic, configs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --generate command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(adminClient: Admin,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = getReplicaAssignmentForTopics(adminClient, topicsToReassign)\n+    val brokerMetadatas = if (enableRackAwareness) {\n+      getBrokerRackInformation(adminClient, brokersToReassign)\n+    } else {\n+      brokersToReassign.map(new BrokerMetadata(_, None))\n+    }\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * The legacy entry point for the --generate command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(zkClient: KafkaZkClient,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+    val brokerMetadatas = if (enableRackAwareness) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI5ODUwNQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r391298505", "bodyText": "Good call.  Yeah, we should probably continue the old behavior of ignoring brokers that aren't up at the time of generating the plan, since there's no specific reason to change it.  Fixed.", "author": "cmccabe", "createdAt": "2020-03-11T22:13:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA3MTI0Mg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzEyNjUxMg==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r393126512", "bodyText": "The complexity gets a bit deep here. Maybe we could move this try/catch to a separate method", "author": "mumrah", "createdAt": "2020-03-16T15:51:55Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1555 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelLogDirThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelLogDirThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(sources: mutable.Set[Int],\n+                                  destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is done.\n+   */\n+  sealed case class PartitionReassignmentState(currentReplicas: Seq[Int],\n+                                               targetReplicas: Seq[Int],\n+                                               done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait LogDirMoveState {\n+    /**\n+     * True if the move is done without errors.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source log directory is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(currentLogDir: String,\n+                                    targetLogDir: String,\n+                                    futureLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(currentLogDir: String,\n+                                       targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica log directory move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        if (opts.options.has(opts.zkConnectOpt)) {\n+          println(\"Warning: ignoring deprecated --zookeeper option because \" +\n+            \"--bootstrap-server was specified.  The --zookeeper option will \" +\n+            \"be removed in a future version of Kafka.\")\n+        }\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        handleAction(adminClient, opts)\n+      } else {\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        handleAction(zkClient, opts)\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n+    }\n+  }\n+\n+  private def handleAction(adminClient: Admin,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(adminClient,\n+        opts.options.has(opts.additionalOpt),\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt),\n+        opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.cancelOpt)) {\n+      cancelAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.listOpt)) {\n+      listReassignments(adminClient)\n     } else {\n-      None\n+      throw new RuntimeException(\"Unsupported action.\")\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  private def handleAction(zkClient: KafkaZkClient,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt))\n+    } else {\n+      throw new RuntimeException(\"Unsupported action.\")\n+    }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * A result returned from verifyAssignment.\n+   *\n+   * @param partStates    A map from partitions to reassignment states.\n+   * @param partsOngoing  True if there are any ongoing partition reassignments.\n+   * @param moveStates    A map from log directories to movement states.\n+   * @param movesOngoing  True if there are any ongoing moves that we know about.\n+   */\n+  case class VerifyAssignmentResult(partStates: Map[TopicPartition, PartitionReassignmentState],\n+                                    partsOngoing: Boolean = false,\n+                                    moveStates: Map[TopicPartitionReplica, LogDirMoveState] = Map.empty,\n+                                    movesOngoing: Boolean = false)\n+\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetLogDirs)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    VerifyAssignmentResult(partStates, partsOngoing, moveStates, movesOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.  Note that anything that\n+   *                              would require AdminClient to see will be left out of this result.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    if (!targetLogDirs.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    VerifyAssignmentResult(partStates, partsOngoing, Map.empty, false)\n+  }\n+\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n         }\n       }\n+    }\n+    bld.mkString(System.lineSeparator())\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzEzMDMzNQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r393130335", "bodyText": "Will we miss brokers that are currently offline here?", "author": "mumrah", "createdAt": "2020-03-16T15:57:18Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1555 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelLogDirThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelLogDirThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(sources: mutable.Set[Int],\n+                                  destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is done.\n+   */\n+  sealed case class PartitionReassignmentState(currentReplicas: Seq[Int],\n+                                               targetReplicas: Seq[Int],\n+                                               done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait LogDirMoveState {\n+    /**\n+     * True if the move is done without errors.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source log directory is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(currentLogDir: String,\n+                                    targetLogDir: String,\n+                                    futureLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(currentLogDir: String,\n+                                       targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica log directory move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        if (opts.options.has(opts.zkConnectOpt)) {\n+          println(\"Warning: ignoring deprecated --zookeeper option because \" +\n+            \"--bootstrap-server was specified.  The --zookeeper option will \" +\n+            \"be removed in a future version of Kafka.\")\n+        }\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        handleAction(adminClient, opts)\n+      } else {\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        handleAction(zkClient, opts)\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n+    }\n+  }\n+\n+  private def handleAction(adminClient: Admin,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(adminClient,\n+        opts.options.has(opts.additionalOpt),\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt),\n+        opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.cancelOpt)) {\n+      cancelAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.listOpt)) {\n+      listReassignments(adminClient)\n     } else {\n-      None\n+      throw new RuntimeException(\"Unsupported action.\")\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  private def handleAction(zkClient: KafkaZkClient,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt))\n+    } else {\n+      throw new RuntimeException(\"Unsupported action.\")\n+    }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * A result returned from verifyAssignment.\n+   *\n+   * @param partStates    A map from partitions to reassignment states.\n+   * @param partsOngoing  True if there are any ongoing partition reassignments.\n+   * @param moveStates    A map from log directories to movement states.\n+   * @param movesOngoing  True if there are any ongoing moves that we know about.\n+   */\n+  case class VerifyAssignmentResult(partStates: Map[TopicPartition, PartitionReassignmentState],\n+                                    partsOngoing: Boolean = false,\n+                                    moveStates: Map[TopicPartitionReplica, LogDirMoveState] = Map.empty,\n+                                    movesOngoing: Boolean = false)\n+\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetLogDirs)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    VerifyAssignmentResult(partStates, partsOngoing, moveStates, movesOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.  Note that anything that\n+   *                              would require AdminClient to see will be left out of this result.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    if (!targetLogDirs.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    VerifyAssignmentResult(partStates, partsOngoing, Map.empty, false)\n+  }\n+\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n         }\n       }\n+    }\n+    bld.mkString(System.lineSeparator())\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n+        }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, LogDirMoveState], Boolean) = {\n+    val moveStates = findLogDirMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findLogDirMoveStates(adminClient: Admin,\n+                           targetMoves: Map[TopicPartitionReplica, String])\n+                           : Map[TopicPartitionReplica, LogDirMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: LogDirMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingReplicaMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingLogDirMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, LogDirMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg1NDcwOQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r393854709", "bodyText": "Yes, we do miss brokers that are currently offline.  This is an existing design flaw with how the reassignment tool works.  I think we can improve it somewhat in this change by at least including the brokers that we know about from the reassignment JSON file.\nWe can't fix it completely, though, since some brokers might have had quotas set, but not be in the JSON file.  If those brokers are down, they will be missed.  For example, if a broker appeared only as a source broker, it won't be in the JSON file.", "author": "cmccabe", "createdAt": "2020-03-17T17:37:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzEzMDMzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzEzMzY5Nw==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r393133697", "bodyText": "Could this just be partitions.map(_.topic)?", "author": "mumrah", "createdAt": "2020-03-16T16:01:57Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1555 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelLogDirThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelLogDirThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(sources: mutable.Set[Int],\n+                                  destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is done.\n+   */\n+  sealed case class PartitionReassignmentState(currentReplicas: Seq[Int],\n+                                               targetReplicas: Seq[Int],\n+                                               done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait LogDirMoveState {\n+    /**\n+     * True if the move is done without errors.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source log directory is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(currentLogDir: String,\n+                                    targetLogDir: String,\n+                                    futureLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(currentLogDir: String,\n+                                       targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica log directory move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        if (opts.options.has(opts.zkConnectOpt)) {\n+          println(\"Warning: ignoring deprecated --zookeeper option because \" +\n+            \"--bootstrap-server was specified.  The --zookeeper option will \" +\n+            \"be removed in a future version of Kafka.\")\n+        }\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        handleAction(adminClient, opts)\n+      } else {\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        handleAction(zkClient, opts)\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n+    }\n+  }\n+\n+  private def handleAction(adminClient: Admin,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(adminClient,\n+        opts.options.has(opts.additionalOpt),\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt),\n+        opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.cancelOpt)) {\n+      cancelAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.listOpt)) {\n+      listReassignments(adminClient)\n     } else {\n-      None\n+      throw new RuntimeException(\"Unsupported action.\")\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  private def handleAction(zkClient: KafkaZkClient,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt))\n+    } else {\n+      throw new RuntimeException(\"Unsupported action.\")\n+    }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * A result returned from verifyAssignment.\n+   *\n+   * @param partStates    A map from partitions to reassignment states.\n+   * @param partsOngoing  True if there are any ongoing partition reassignments.\n+   * @param moveStates    A map from log directories to movement states.\n+   * @param movesOngoing  True if there are any ongoing moves that we know about.\n+   */\n+  case class VerifyAssignmentResult(partStates: Map[TopicPartition, PartitionReassignmentState],\n+                                    partsOngoing: Boolean = false,\n+                                    moveStates: Map[TopicPartitionReplica, LogDirMoveState] = Map.empty,\n+                                    movesOngoing: Boolean = false)\n+\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetLogDirs)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    VerifyAssignmentResult(partStates, partsOngoing, moveStates, movesOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.  Note that anything that\n+   *                              would require AdminClient to see will be left out of this result.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    if (!targetLogDirs.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    VerifyAssignmentResult(partStates, partsOngoing, Map.empty, false)\n+  }\n+\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n         }\n       }\n+    }\n+    bld.mkString(System.lineSeparator())\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n+        }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, LogDirMoveState], Boolean) = {\n+    val moveStates = findLogDirMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findLogDirMoveStates(adminClient: Admin,\n+                           targetMoves: Map[TopicPartitionReplica, String])\n+                           : Map[TopicPartitionReplica, LogDirMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: LogDirMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingReplicaMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingLogDirMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, LogDirMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    allBrokerIds.foreach {\n+      case brokerId => configOps.put(\n+        new ConfigResource(ConfigResource.Type.BROKER, brokerId.toString),\n+        brokerLevelThrottles.map(throttle => new AlterConfigOp(\n+          new ConfigEntry(throttle, null), OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   */\n+  def clearBrokerLevelThrottles(zkClient: KafkaZkClient): Unit = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val brokerMetadatas = adminZkClient.getBrokerMetadatas(rackAwareMode, Some(brokerListToReassign))\n+    for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n+      if (!brokerLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    topics.foreach {\n+      topicName => configOps.put(\n+        new ConfigResource(ConfigResource.Type.TOPIC, topicName),\n+        topicLevelThrottles.map(throttle => new AlterConfigOp(new ConfigEntry(throttle, null),\n+          OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(zkClient: KafkaZkClient, topics: Set[String]): Unit = {\n+    val adminZkClient = new AdminZkClient(zkClient)\n+    for (topic <- topics) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n+      if (!topicLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeTopicConfig(topic, configs)\n+      }\n+    }\n+  }\n \n-    val partitionsToBeReassigned = mutable.Map[TopicPartition, Seq[Int]]()\n+  /**\n+   * The entry point for the --generate command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(adminClient: Admin,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = getReplicaAssignmentForTopics(adminClient, topicsToReassign)\n+    val brokerMetadatas = getBrokerMetadata(adminClient, brokersToReassign, enableRackAwareness)\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * The legacy entry point for the --generate command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(zkClient: KafkaZkClient,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+    val brokerMetadatas = getBrokerMetadata(zkClient, brokersToReassign, enableRackAwareness)\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * Calculate the new partition assignments to suggest in --generate.\n+   *\n+   * @param currentAssignment  The current partition assignments.\n+   * @param brokerMetadatas    The rack information for each broker.\n+   *\n+   * @return                   A map from partitions to the proposed assignments for each.\n+   */\n+  def calculateAssignment(currentAssignment: Map[TopicPartition, Seq[Int]],\n+                          brokerMetadatas: Seq[BrokerMetadata])\n+                          : Map[TopicPartition, Seq[Int]] = {\n+    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n+    val proposedAssignments = mutable.Map[TopicPartition, Seq[Int]]()\n     groupedByTopic.foreach { case (topic, assignment) =>\n       val (_, replicas) = assignment.head\n-      val assignedReplicas = AdminUtils.assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n-      partitionsToBeReassigned ++= assignedReplicas.map { case (partition, replicas) =>\n+      val assignedReplicas = AdminUtils.\n+        assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n+      proposedAssignments ++= assignedReplicas.map { case (partition, replicas) =>\n         new TopicPartition(topic, partition) -> replicas\n       }\n     }\n-    (partitionsToBeReassigned, currentAssignment)\n+    proposedAssignments\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val reassignmentJsonFile =  opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val reassignmentJsonString = Utils.readFileAsString(reassignmentJsonFile)\n-    val interBrokerThrottle = opts.options.valueOf(opts.interBrokerThrottleOpt)\n-    val replicaAlterLogDirsThrottle = opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt)\n-    val timeoutMs = opts.options.valueOf(opts.timeoutOpt)\n-    executeAssignment(zkClient, adminClientOpt, reassignmentJsonString, Throttle(interBrokerThrottle, replicaAlterLogDirsThrottle), timeoutMs)\n+  /**\n+   * Get the current replica assignments for some topics.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics          The topics to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForTopics(adminClient: Admin,\n+                                    topics: Seq[String])\n+                                    : Map[TopicPartition, Seq[Int]] = {\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.map {\n+          info => (new TopicPartition(topicName, info.partition()),\n+            info.replicas().asScala.map(_.id()))\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], reassignmentJsonString: String, throttle: Throttle, timeoutMs: Long = 10000L): Unit = {\n-    val (partitionAssignment, replicaAssignment) = parseAndValidate(zkClient, reassignmentJsonString)\n+  /**\n+   * Get the current replica assignments for some partitions.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param partitions      The partitions to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForPartitions(adminClient: Admin,\n+                                        partitions: Set[TopicPartition])\n+                                        : Map[TopicPartition, Seq[Int]] = {\n+    val topics = new mutable.HashSet[String]()\n+    partitions.foreach(topics += _.topic)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzEzOTgyOA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r393139828", "bodyText": "nit: using a filter followed with a map in might be a little more readable here", "author": "mumrah", "createdAt": "2020-03-16T16:10:53Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1555 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelLogDirThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelLogDirThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(sources: mutable.Set[Int],\n+                                  destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is done.\n+   */\n+  sealed case class PartitionReassignmentState(currentReplicas: Seq[Int],\n+                                               targetReplicas: Seq[Int],\n+                                               done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait LogDirMoveState {\n+    /**\n+     * True if the move is done without errors.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source log directory is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(currentLogDir: String,\n+                                    targetLogDir: String,\n+                                    futureLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(currentLogDir: String,\n+                                       targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica log directory move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        if (opts.options.has(opts.zkConnectOpt)) {\n+          println(\"Warning: ignoring deprecated --zookeeper option because \" +\n+            \"--bootstrap-server was specified.  The --zookeeper option will \" +\n+            \"be removed in a future version of Kafka.\")\n+        }\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        handleAction(adminClient, opts)\n+      } else {\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        handleAction(zkClient, opts)\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n+    }\n+  }\n+\n+  private def handleAction(adminClient: Admin,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(adminClient,\n+        opts.options.has(opts.additionalOpt),\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt),\n+        opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.cancelOpt)) {\n+      cancelAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.listOpt)) {\n+      listReassignments(adminClient)\n     } else {\n-      None\n+      throw new RuntimeException(\"Unsupported action.\")\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  private def handleAction(zkClient: KafkaZkClient,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt))\n+    } else {\n+      throw new RuntimeException(\"Unsupported action.\")\n+    }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * A result returned from verifyAssignment.\n+   *\n+   * @param partStates    A map from partitions to reassignment states.\n+   * @param partsOngoing  True if there are any ongoing partition reassignments.\n+   * @param moveStates    A map from log directories to movement states.\n+   * @param movesOngoing  True if there are any ongoing moves that we know about.\n+   */\n+  case class VerifyAssignmentResult(partStates: Map[TopicPartition, PartitionReassignmentState],\n+                                    partsOngoing: Boolean = false,\n+                                    moveStates: Map[TopicPartitionReplica, LogDirMoveState] = Map.empty,\n+                                    movesOngoing: Boolean = false)\n+\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetLogDirs)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    VerifyAssignmentResult(partStates, partsOngoing, moveStates, movesOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.  Note that anything that\n+   *                              would require AdminClient to see will be left out of this result.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    if (!targetLogDirs.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    VerifyAssignmentResult(partStates, partsOngoing, Map.empty, false)\n+  }\n+\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n         }\n       }\n+    }\n+    bld.mkString(System.lineSeparator())\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n+        }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, LogDirMoveState], Boolean) = {\n+    val moveStates = findLogDirMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findLogDirMoveStates(adminClient: Admin,\n+                           targetMoves: Map[TopicPartitionReplica, String])\n+                           : Map[TopicPartitionReplica, LogDirMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: LogDirMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingReplicaMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingLogDirMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, LogDirMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    allBrokerIds.foreach {\n+      case brokerId => configOps.put(\n+        new ConfigResource(ConfigResource.Type.BROKER, brokerId.toString),\n+        brokerLevelThrottles.map(throttle => new AlterConfigOp(\n+          new ConfigEntry(throttle, null), OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   */\n+  def clearBrokerLevelThrottles(zkClient: KafkaZkClient): Unit = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val brokerMetadatas = adminZkClient.getBrokerMetadatas(rackAwareMode, Some(brokerListToReassign))\n+    for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n+      if (!brokerLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    topics.foreach {\n+      topicName => configOps.put(\n+        new ConfigResource(ConfigResource.Type.TOPIC, topicName),\n+        topicLevelThrottles.map(throttle => new AlterConfigOp(new ConfigEntry(throttle, null),\n+          OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(zkClient: KafkaZkClient, topics: Set[String]): Unit = {\n+    val adminZkClient = new AdminZkClient(zkClient)\n+    for (topic <- topics) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n+      if (!topicLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeTopicConfig(topic, configs)\n+      }\n+    }\n+  }\n \n-    val partitionsToBeReassigned = mutable.Map[TopicPartition, Seq[Int]]()\n+  /**\n+   * The entry point for the --generate command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(adminClient: Admin,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = getReplicaAssignmentForTopics(adminClient, topicsToReassign)\n+    val brokerMetadatas = getBrokerMetadata(adminClient, brokersToReassign, enableRackAwareness)\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * The legacy entry point for the --generate command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(zkClient: KafkaZkClient,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+    val brokerMetadatas = getBrokerMetadata(zkClient, brokersToReassign, enableRackAwareness)\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * Calculate the new partition assignments to suggest in --generate.\n+   *\n+   * @param currentAssignment  The current partition assignments.\n+   * @param brokerMetadatas    The rack information for each broker.\n+   *\n+   * @return                   A map from partitions to the proposed assignments for each.\n+   */\n+  def calculateAssignment(currentAssignment: Map[TopicPartition, Seq[Int]],\n+                          brokerMetadatas: Seq[BrokerMetadata])\n+                          : Map[TopicPartition, Seq[Int]] = {\n+    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n+    val proposedAssignments = mutable.Map[TopicPartition, Seq[Int]]()\n     groupedByTopic.foreach { case (topic, assignment) =>\n       val (_, replicas) = assignment.head\n-      val assignedReplicas = AdminUtils.assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n-      partitionsToBeReassigned ++= assignedReplicas.map { case (partition, replicas) =>\n+      val assignedReplicas = AdminUtils.\n+        assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n+      proposedAssignments ++= assignedReplicas.map { case (partition, replicas) =>\n         new TopicPartition(topic, partition) -> replicas\n       }\n     }\n-    (partitionsToBeReassigned, currentAssignment)\n+    proposedAssignments\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val reassignmentJsonFile =  opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val reassignmentJsonString = Utils.readFileAsString(reassignmentJsonFile)\n-    val interBrokerThrottle = opts.options.valueOf(opts.interBrokerThrottleOpt)\n-    val replicaAlterLogDirsThrottle = opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt)\n-    val timeoutMs = opts.options.valueOf(opts.timeoutOpt)\n-    executeAssignment(zkClient, adminClientOpt, reassignmentJsonString, Throttle(interBrokerThrottle, replicaAlterLogDirsThrottle), timeoutMs)\n+  /**\n+   * Get the current replica assignments for some topics.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics          The topics to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForTopics(adminClient: Admin,\n+                                    topics: Seq[String])\n+                                    : Map[TopicPartition, Seq[Int]] = {\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.map {\n+          info => (new TopicPartition(topicName, info.partition()),\n+            info.replicas().asScala.map(_.id()))\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], reassignmentJsonString: String, throttle: Throttle, timeoutMs: Long = 10000L): Unit = {\n-    val (partitionAssignment, replicaAssignment) = parseAndValidate(zkClient, reassignmentJsonString)\n+  /**\n+   * Get the current replica assignments for some partitions.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param partitions      The partitions to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForPartitions(adminClient: Admin,\n+                                        partitions: Set[TopicPartition])\n+                                        : Map[TopicPartition, Seq[Int]] = {\n+    val topics = new mutable.HashSet[String]()\n+    partitions.foreach(topics += _.topic)\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.flatMap {\n+          info => if (partitions.contains(new TopicPartition(topicName, info.partition()))) {\n+            Some(new TopicPartition(topicName, info.partition()),\n+                info.replicas().asScala.map(_.id()))\n+          } else {\n+            None\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param adminClient         The AdminClient object.\n+   * @param brokers             The brokers to gather metadata about.\n+   * @param enableRackAwareness True if we should return rack information, and throw an\n+   *                            exception if it is inconsistent.\n+   *\n+   * @return                    The metadata for each broker that was found.\n+   *                            Brokers that were not found will be omitted.\n+   */\n+  def getBrokerMetadata(adminClient: Admin,\n+                        brokers: Seq[Int],\n+                        enableRackAwareness: Boolean): Seq[BrokerMetadata] = {\n+    val brokerSet = brokers.toSet\n+    val results = adminClient.describeCluster().nodes().get().asScala.flatMap(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzE0NjM3Mw==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r393146373", "bodyText": "Since id is not typed, won't this always match? I think the case can be removed", "author": "mumrah", "createdAt": "2020-03-16T16:20:31Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1555 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelLogDirThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelLogDirThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(sources: mutable.Set[Int],\n+                                  destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is done.\n+   */\n+  sealed case class PartitionReassignmentState(currentReplicas: Seq[Int],\n+                                               targetReplicas: Seq[Int],\n+                                               done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait LogDirMoveState {\n+    /**\n+     * True if the move is done without errors.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source log directory is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(currentLogDir: String,\n+                                    targetLogDir: String,\n+                                    futureLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(currentLogDir: String,\n+                                       targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica log directory move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        if (opts.options.has(opts.zkConnectOpt)) {\n+          println(\"Warning: ignoring deprecated --zookeeper option because \" +\n+            \"--bootstrap-server was specified.  The --zookeeper option will \" +\n+            \"be removed in a future version of Kafka.\")\n+        }\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        handleAction(adminClient, opts)\n+      } else {\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        handleAction(zkClient, opts)\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n+    }\n+  }\n+\n+  private def handleAction(adminClient: Admin,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(adminClient,\n+        opts.options.has(opts.additionalOpt),\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt),\n+        opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.cancelOpt)) {\n+      cancelAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.listOpt)) {\n+      listReassignments(adminClient)\n     } else {\n-      None\n+      throw new RuntimeException(\"Unsupported action.\")\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  private def handleAction(zkClient: KafkaZkClient,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt))\n+    } else {\n+      throw new RuntimeException(\"Unsupported action.\")\n+    }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * A result returned from verifyAssignment.\n+   *\n+   * @param partStates    A map from partitions to reassignment states.\n+   * @param partsOngoing  True if there are any ongoing partition reassignments.\n+   * @param moveStates    A map from log directories to movement states.\n+   * @param movesOngoing  True if there are any ongoing moves that we know about.\n+   */\n+  case class VerifyAssignmentResult(partStates: Map[TopicPartition, PartitionReassignmentState],\n+                                    partsOngoing: Boolean = false,\n+                                    moveStates: Map[TopicPartitionReplica, LogDirMoveState] = Map.empty,\n+                                    movesOngoing: Boolean = false)\n+\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetLogDirs)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    VerifyAssignmentResult(partStates, partsOngoing, moveStates, movesOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.  Note that anything that\n+   *                              would require AdminClient to see will be left out of this result.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    if (!targetLogDirs.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    VerifyAssignmentResult(partStates, partsOngoing, Map.empty, false)\n+  }\n+\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n         }\n       }\n+    }\n+    bld.mkString(System.lineSeparator())\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n+        }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, LogDirMoveState], Boolean) = {\n+    val moveStates = findLogDirMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findLogDirMoveStates(adminClient: Admin,\n+                           targetMoves: Map[TopicPartitionReplica, String])\n+                           : Map[TopicPartitionReplica, LogDirMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: LogDirMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingReplicaMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingLogDirMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, LogDirMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    allBrokerIds.foreach {\n+      case brokerId => configOps.put(\n+        new ConfigResource(ConfigResource.Type.BROKER, brokerId.toString),\n+        brokerLevelThrottles.map(throttle => new AlterConfigOp(\n+          new ConfigEntry(throttle, null), OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   */\n+  def clearBrokerLevelThrottles(zkClient: KafkaZkClient): Unit = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val brokerMetadatas = adminZkClient.getBrokerMetadatas(rackAwareMode, Some(brokerListToReassign))\n+    for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n+      if (!brokerLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    topics.foreach {\n+      topicName => configOps.put(\n+        new ConfigResource(ConfigResource.Type.TOPIC, topicName),\n+        topicLevelThrottles.map(throttle => new AlterConfigOp(new ConfigEntry(throttle, null),\n+          OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(zkClient: KafkaZkClient, topics: Set[String]): Unit = {\n+    val adminZkClient = new AdminZkClient(zkClient)\n+    for (topic <- topics) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n+      if (!topicLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeTopicConfig(topic, configs)\n+      }\n+    }\n+  }\n \n-    val partitionsToBeReassigned = mutable.Map[TopicPartition, Seq[Int]]()\n+  /**\n+   * The entry point for the --generate command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(adminClient: Admin,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = getReplicaAssignmentForTopics(adminClient, topicsToReassign)\n+    val brokerMetadatas = getBrokerMetadata(adminClient, brokersToReassign, enableRackAwareness)\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * The legacy entry point for the --generate command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(zkClient: KafkaZkClient,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+    val brokerMetadatas = getBrokerMetadata(zkClient, brokersToReassign, enableRackAwareness)\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * Calculate the new partition assignments to suggest in --generate.\n+   *\n+   * @param currentAssignment  The current partition assignments.\n+   * @param brokerMetadatas    The rack information for each broker.\n+   *\n+   * @return                   A map from partitions to the proposed assignments for each.\n+   */\n+  def calculateAssignment(currentAssignment: Map[TopicPartition, Seq[Int]],\n+                          brokerMetadatas: Seq[BrokerMetadata])\n+                          : Map[TopicPartition, Seq[Int]] = {\n+    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n+    val proposedAssignments = mutable.Map[TopicPartition, Seq[Int]]()\n     groupedByTopic.foreach { case (topic, assignment) =>\n       val (_, replicas) = assignment.head\n-      val assignedReplicas = AdminUtils.assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n-      partitionsToBeReassigned ++= assignedReplicas.map { case (partition, replicas) =>\n+      val assignedReplicas = AdminUtils.\n+        assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n+      proposedAssignments ++= assignedReplicas.map { case (partition, replicas) =>\n         new TopicPartition(topic, partition) -> replicas\n       }\n     }\n-    (partitionsToBeReassigned, currentAssignment)\n+    proposedAssignments\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val reassignmentJsonFile =  opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val reassignmentJsonString = Utils.readFileAsString(reassignmentJsonFile)\n-    val interBrokerThrottle = opts.options.valueOf(opts.interBrokerThrottleOpt)\n-    val replicaAlterLogDirsThrottle = opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt)\n-    val timeoutMs = opts.options.valueOf(opts.timeoutOpt)\n-    executeAssignment(zkClient, adminClientOpt, reassignmentJsonString, Throttle(interBrokerThrottle, replicaAlterLogDirsThrottle), timeoutMs)\n+  /**\n+   * Get the current replica assignments for some topics.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics          The topics to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForTopics(adminClient: Admin,\n+                                    topics: Seq[String])\n+                                    : Map[TopicPartition, Seq[Int]] = {\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.map {\n+          info => (new TopicPartition(topicName, info.partition()),\n+            info.replicas().asScala.map(_.id()))\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], reassignmentJsonString: String, throttle: Throttle, timeoutMs: Long = 10000L): Unit = {\n-    val (partitionAssignment, replicaAssignment) = parseAndValidate(zkClient, reassignmentJsonString)\n+  /**\n+   * Get the current replica assignments for some partitions.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param partitions      The partitions to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForPartitions(adminClient: Admin,\n+                                        partitions: Set[TopicPartition])\n+                                        : Map[TopicPartition, Seq[Int]] = {\n+    val topics = new mutable.HashSet[String]()\n+    partitions.foreach(topics += _.topic)\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.flatMap {\n+          info => if (partitions.contains(new TopicPartition(topicName, info.partition()))) {\n+            Some(new TopicPartition(topicName, info.partition()),\n+                info.replicas().asScala.map(_.id()))\n+          } else {\n+            None\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param adminClient         The AdminClient object.\n+   * @param brokers             The brokers to gather metadata about.\n+   * @param enableRackAwareness True if we should return rack information, and throw an\n+   *                            exception if it is inconsistent.\n+   *\n+   * @return                    The metadata for each broker that was found.\n+   *                            Brokers that were not found will be omitted.\n+   */\n+  def getBrokerMetadata(adminClient: Admin,\n+                        brokers: Seq[Int],\n+                        enableRackAwareness: Boolean): Seq[BrokerMetadata] = {\n+    val brokerSet = brokers.toSet\n+    val results = adminClient.describeCluster().nodes().get().asScala.flatMap(\n+      node => if (!brokerSet.contains(node.id())) {\n+        None\n+      } else {\n+        if (enableRackAwareness && node.rack() != null) {\n+          Some(new BrokerMetadata(node.id(), Some(node.rack())))\n+        } else {\n+          Some(new BrokerMetadata(node.id(), None))\n+        }\n+      }\n+    ).toSeq\n+    val numRackless = results.count(_.rack.isEmpty)\n+    if (enableRackAwareness && numRackless != 0 && numRackless != results.size) {\n+      throw new AdminOperationException(\"Not all brokers have rack information. Add \" +\n+        \"--disable-rack-aware in command line to make replica assignment without rack \" +\n+        \"information.\")\n+    }\n+    results\n+  }\n+\n+  /**\n+   * Find the metadata for some brokers.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param brokers               The brokers to gather metadata about.\n+   * @param enableRackAwareness   True if we should return rack information, and throw an\n+   *                              exception if it is inconsistent.\n+   *\n+   * @return                      The metadata for each broker that was found.\n+   *                              Brokers that were not found will be omitted.\n+   */\n+  def getBrokerMetadata(zkClient: KafkaZkClient,\n+                        brokers: Seq[Int],\n+                        enableRackAwareness: Boolean): Seq[BrokerMetadata] = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val reassignPartitionsCommand = new ReassignPartitionsCommand(zkClient, adminClientOpt, partitionAssignment.toMap, replicaAssignment, adminZkClient)\n+    adminZkClient.getBrokerMetadatas(if (enableRackAwareness)\n+      RackAwareMode.Enforced else RackAwareMode.Disabled, Some(brokers))\n+  }\n+\n+  /**\n+   * Parse and validate data gathered from the command-line for --generate\n+   * In particular, we parse the JSON and validate that duplicate brokers and\n+   * topics don't appear.\n+   *\n+   * @param reassignmentJson       The JSON passed to --generate .\n+   * @param brokerList             A list of brokers passed to --generate.\n+   *\n+   * @return                       A tuple of brokers to reassign, topics to reassign\n+   */\n+  def parseGenerateAssignmentArgs(reassignmentJson: String,\n+                                  brokerList: String): (Seq[Int], Seq[String]) = {\n+    val brokerListToReassign = brokerList.split(',').map(_.toInt)\n+    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n+    if (duplicateReassignments.nonEmpty)\n+      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".\n+        format(duplicateReassignments.mkString(\",\")))\n+    val topicsToReassign = parseTopicsData(reassignmentJson)\n+    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n+    if (duplicateTopicsToReassign.nonEmpty)\n+      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".\n+        format(duplicateTopicsToReassign.mkString(\",\")))\n+    (brokerListToReassign, topicsToReassign)\n+  }\n \n-    // If there is an existing rebalance running, attempt to change its throttle\n-    if (zkClient.reassignPartitionsInProgress()) {\n-      println(\"There is an existing assignment running.\")\n-      reassignPartitionsCommand.maybeLimit(throttle)\n+  /**\n+   * The entry point for the --execute and --execute-additional commands.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param additional                  Whether --additional was passed.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative\n+   *                                    number to skip using a throttle.\n+   * @param logDirThrottle              The replica log directory throttle to use, or a\n+   *                                    negative number to skip using a throttle.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeAssignment(adminClient: Admin,\n+                        additional: Boolean,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long = -1L,\n+                        logDirThrottle: Long = -1L,\n+                        timeoutMs: Long = 10000L,\n+                        time: Time = Time.SYSTEM): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    // If there is an existing assignment, check for --additional before proceeding.\n+    // This helps avoid surprising users.\n+    if (!additional && !currentReassignments.isEmpty) {\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n+    }\n+    verifyBrokerIds(adminClient, proposedParts.values.flatten.toSet)\n+    val currentParts = getReplicaAssignmentForPartitions(adminClient, proposedParts.keySet.toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+    if (interBrokerThrottle >= 0 || logDirThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(currentReassignments, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(adminClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      val movingBrokers = calculateMovingBrokers(proposedReplicas.keySet.toSet)\n+      modifyBrokerThrottles(adminClient,\n+        reassigningBrokers, interBrokerThrottle,\n+        movingBrokers, logDirThrottle)\n+      if (interBrokerThrottle >= 0) {\n+        println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+      }\n+      if (logDirThrottle >= 0) {\n+        println(s\"The replica-alter-dir throttle limit was set to ${logDirThrottle} B/s\")\n+      }\n+    }\n+\n+    // Execute the partition reassignments.\n+    val errors = alterPartitionReassignments(adminClient, proposedParts)\n+    if (!errors.isEmpty) {\n+      throw new TerseReassignmentFailureException(\n+        \"Error reassigning partition(s):%n%s\".format(\n+          errors.keySet.toBuffer.sortWith(compareTopicPartitions).map {\n+            case part => s\"${part}: ${errors(part).getMessage}\"\n+          }.mkString(System.lineSeparator())))\n+    }\n+    println(\"Successfully started partition reassignment%s for %s\".format(\n+      if (proposedParts.size == 1) \"\" else \"s\",\n+      proposedParts.keySet.toBuffer.sortWith(compareTopicPartitions).mkString(\",\")))\n+    if (!proposedReplicas.isEmpty) {\n+      executeMoves(adminClient, proposedReplicas, timeoutMs, time)\n+    }\n+  }\n+\n+  /**\n+   * Execute some partition log directory movements.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param proposedReplicas            A map from TopicPartitionReplicas to the\n+   *                                    directories to move them to.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeMoves(adminClient: Admin,\n+                   proposedReplicas: Map[TopicPartitionReplica, String],\n+                   timeoutMs: Long,\n+                   time: Time): Unit = {\n+    val startTimeMs = time.milliseconds()\n+    val pendingReplicas = new mutable.HashMap[TopicPartitionReplica, String]()\n+    pendingReplicas ++= proposedReplicas\n+    var done = false\n+    do {\n+      val completed = alterReplicaLogDirs(adminClient, pendingReplicas)\n+      if (!completed.isEmpty) {\n+        println(\"Successfully started log directory move%s for: %s\".format(\n+          if (completed.size == 1) \"\" else \"s\",\n+          completed.toBuffer.sortWith(compareTopicPartitionReplicas).mkString(\",\")))\n+      }\n+      pendingReplicas --= completed\n+      if (pendingReplicas.isEmpty) {\n+        done = true\n+      } else if (time.milliseconds() >= startTimeMs + timeoutMs) {\n+        throw new TerseReassignmentFailureException(\n+          \"Timed out before log directory move%s could be started for: %s\".format(\n+            if (pendingReplicas.size == 1) \"\" else \"s\",\n+            pendingReplicas.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).\n+              mkString(\",\")))\n+      } else {\n+        // If a replica has been moved to a new host and we also specified a particular\n+        // log directory, we will have to keep retrying the alterReplicaLogDirs\n+        // call.  It can't take effect until the replica is moved to that host.\n+        time.sleep(100)\n+      }\n+    } while (!done)\n+  }\n+\n+  /**\n+   * Entry point for the --list command.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   */\n+  def listReassignments(adminClient: Admin): Unit = {\n+    println(curReassignmentsToString(adminClient))\n+  }\n+\n+  /**\n+   * Convert the current partition reassignments to text.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   * @return              A string describing the current partition reassignments.\n+   */\n+  def curReassignmentsToString(adminClient: Admin): String = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val text = currentReassignments.keySet.toBuffer.sortWith(compareTopicPartitions).map {\n+      case part =>\n+        val reassignment = currentReassignments(part)\n+        val replicas = reassignment.replicas().asScala\n+        val addingReplicas = reassignment.addingReplicas().asScala\n+        val removingReplicas = reassignment.removingReplicas().asScala\n+        \"%s: replicas: %s.%s%s\".format(part, replicas.mkString(\",\"),\n+          if (addingReplicas.isEmpty) \"\" else\n+            \" adding: %s.\".format(addingReplicas.mkString(\",\")),\n+          if (removingReplicas.isEmpty) \"\" else\n+            \" removing: %s.\".format(removingReplicas.mkString(\",\")))\n+    }.mkString(System.lineSeparator())\n+    if (text.isEmpty) {\n+      \"No partition reassignments found.\"\n     } else {\n-      printCurrentAssignment(zkClient, partitionAssignment.map(_._1.topic))\n-      if (throttle.interBrokerLimit >= 0 || throttle.replicaAlterLogDirsLimit >= 0)\n-        println(String.format(\"Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.\"))\n-      if (reassignPartitionsCommand.reassignPartitions(throttle, timeoutMs)) {\n-        println(\"Successfully started reassignment of partitions.\")\n-      } else\n-        println(\"Failed to reassign partitions %s\".format(partitionAssignment))\n+      \"Current partition reassignments:%n%s\".format(text)\n+    }\n+  }\n+\n+  /**\n+   * Verify that all the brokers in an assignment exist.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param brokers                     The broker IDs to verify.\n+   */\n+  def verifyBrokerIds(adminClient: Admin, brokers: Set[Int]): Unit = {\n+    val allNodeIds = adminClient.describeCluster().nodes().get().asScala.map(_.id).toSet\n+    brokers.find(!allNodeIds.contains(_)).map {\n+      case id => throw new AdminCommandFailedException(s\"Unknown broker id ${id}\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzE1MDQ5OA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r393150498", "bodyText": "Same as question above -- do we need this case?", "author": "mumrah", "createdAt": "2020-03-16T16:26:44Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1555 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelLogDirThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelLogDirThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(sources: mutable.Set[Int],\n+                                  destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is done.\n+   */\n+  sealed case class PartitionReassignmentState(currentReplicas: Seq[Int],\n+                                               targetReplicas: Seq[Int],\n+                                               done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait LogDirMoveState {\n+    /**\n+     * True if the move is done without errors.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source log directory is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(currentLogDir: String,\n+                                    targetLogDir: String,\n+                                    futureLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(currentLogDir: String,\n+                                       targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica log directory move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        if (opts.options.has(opts.zkConnectOpt)) {\n+          println(\"Warning: ignoring deprecated --zookeeper option because \" +\n+            \"--bootstrap-server was specified.  The --zookeeper option will \" +\n+            \"be removed in a future version of Kafka.\")\n+        }\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        handleAction(adminClient, opts)\n+      } else {\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        handleAction(zkClient, opts)\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n+    }\n+  }\n+\n+  private def handleAction(adminClient: Admin,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(adminClient,\n+        opts.options.has(opts.additionalOpt),\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt),\n+        opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.cancelOpt)) {\n+      cancelAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.listOpt)) {\n+      listReassignments(adminClient)\n     } else {\n-      None\n+      throw new RuntimeException(\"Unsupported action.\")\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  private def handleAction(zkClient: KafkaZkClient,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt))\n+    } else {\n+      throw new RuntimeException(\"Unsupported action.\")\n+    }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * A result returned from verifyAssignment.\n+   *\n+   * @param partStates    A map from partitions to reassignment states.\n+   * @param partsOngoing  True if there are any ongoing partition reassignments.\n+   * @param moveStates    A map from log directories to movement states.\n+   * @param movesOngoing  True if there are any ongoing moves that we know about.\n+   */\n+  case class VerifyAssignmentResult(partStates: Map[TopicPartition, PartitionReassignmentState],\n+                                    partsOngoing: Boolean = false,\n+                                    moveStates: Map[TopicPartitionReplica, LogDirMoveState] = Map.empty,\n+                                    movesOngoing: Boolean = false)\n+\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetLogDirs)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    VerifyAssignmentResult(partStates, partsOngoing, moveStates, movesOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.  Note that anything that\n+   *                              would require AdminClient to see will be left out of this result.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    if (!targetLogDirs.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    VerifyAssignmentResult(partStates, partsOngoing, Map.empty, false)\n+  }\n+\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n         }\n       }\n+    }\n+    bld.mkString(System.lineSeparator())\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n+        }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, LogDirMoveState], Boolean) = {\n+    val moveStates = findLogDirMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findLogDirMoveStates(adminClient: Admin,\n+                           targetMoves: Map[TopicPartitionReplica, String])\n+                           : Map[TopicPartitionReplica, LogDirMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: LogDirMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingReplicaMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingLogDirMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, LogDirMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    allBrokerIds.foreach {\n+      case brokerId => configOps.put(\n+        new ConfigResource(ConfigResource.Type.BROKER, brokerId.toString),\n+        brokerLevelThrottles.map(throttle => new AlterConfigOp(\n+          new ConfigEntry(throttle, null), OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   */\n+  def clearBrokerLevelThrottles(zkClient: KafkaZkClient): Unit = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val brokerMetadatas = adminZkClient.getBrokerMetadatas(rackAwareMode, Some(brokerListToReassign))\n+    for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n+      if (!brokerLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    topics.foreach {\n+      topicName => configOps.put(\n+        new ConfigResource(ConfigResource.Type.TOPIC, topicName),\n+        topicLevelThrottles.map(throttle => new AlterConfigOp(new ConfigEntry(throttle, null),\n+          OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(zkClient: KafkaZkClient, topics: Set[String]): Unit = {\n+    val adminZkClient = new AdminZkClient(zkClient)\n+    for (topic <- topics) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n+      if (!topicLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeTopicConfig(topic, configs)\n+      }\n+    }\n+  }\n \n-    val partitionsToBeReassigned = mutable.Map[TopicPartition, Seq[Int]]()\n+  /**\n+   * The entry point for the --generate command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(adminClient: Admin,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = getReplicaAssignmentForTopics(adminClient, topicsToReassign)\n+    val brokerMetadatas = getBrokerMetadata(adminClient, brokersToReassign, enableRackAwareness)\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * The legacy entry point for the --generate command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(zkClient: KafkaZkClient,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+    val brokerMetadatas = getBrokerMetadata(zkClient, brokersToReassign, enableRackAwareness)\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * Calculate the new partition assignments to suggest in --generate.\n+   *\n+   * @param currentAssignment  The current partition assignments.\n+   * @param brokerMetadatas    The rack information for each broker.\n+   *\n+   * @return                   A map from partitions to the proposed assignments for each.\n+   */\n+  def calculateAssignment(currentAssignment: Map[TopicPartition, Seq[Int]],\n+                          brokerMetadatas: Seq[BrokerMetadata])\n+                          : Map[TopicPartition, Seq[Int]] = {\n+    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n+    val proposedAssignments = mutable.Map[TopicPartition, Seq[Int]]()\n     groupedByTopic.foreach { case (topic, assignment) =>\n       val (_, replicas) = assignment.head\n-      val assignedReplicas = AdminUtils.assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n-      partitionsToBeReassigned ++= assignedReplicas.map { case (partition, replicas) =>\n+      val assignedReplicas = AdminUtils.\n+        assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n+      proposedAssignments ++= assignedReplicas.map { case (partition, replicas) =>\n         new TopicPartition(topic, partition) -> replicas\n       }\n     }\n-    (partitionsToBeReassigned, currentAssignment)\n+    proposedAssignments\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val reassignmentJsonFile =  opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val reassignmentJsonString = Utils.readFileAsString(reassignmentJsonFile)\n-    val interBrokerThrottle = opts.options.valueOf(opts.interBrokerThrottleOpt)\n-    val replicaAlterLogDirsThrottle = opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt)\n-    val timeoutMs = opts.options.valueOf(opts.timeoutOpt)\n-    executeAssignment(zkClient, adminClientOpt, reassignmentJsonString, Throttle(interBrokerThrottle, replicaAlterLogDirsThrottle), timeoutMs)\n+  /**\n+   * Get the current replica assignments for some topics.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics          The topics to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForTopics(adminClient: Admin,\n+                                    topics: Seq[String])\n+                                    : Map[TopicPartition, Seq[Int]] = {\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.map {\n+          info => (new TopicPartition(topicName, info.partition()),\n+            info.replicas().asScala.map(_.id()))\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], reassignmentJsonString: String, throttle: Throttle, timeoutMs: Long = 10000L): Unit = {\n-    val (partitionAssignment, replicaAssignment) = parseAndValidate(zkClient, reassignmentJsonString)\n+  /**\n+   * Get the current replica assignments for some partitions.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param partitions      The partitions to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForPartitions(adminClient: Admin,\n+                                        partitions: Set[TopicPartition])\n+                                        : Map[TopicPartition, Seq[Int]] = {\n+    val topics = new mutable.HashSet[String]()\n+    partitions.foreach(topics += _.topic)\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.flatMap {\n+          info => if (partitions.contains(new TopicPartition(topicName, info.partition()))) {\n+            Some(new TopicPartition(topicName, info.partition()),\n+                info.replicas().asScala.map(_.id()))\n+          } else {\n+            None\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param adminClient         The AdminClient object.\n+   * @param brokers             The brokers to gather metadata about.\n+   * @param enableRackAwareness True if we should return rack information, and throw an\n+   *                            exception if it is inconsistent.\n+   *\n+   * @return                    The metadata for each broker that was found.\n+   *                            Brokers that were not found will be omitted.\n+   */\n+  def getBrokerMetadata(adminClient: Admin,\n+                        brokers: Seq[Int],\n+                        enableRackAwareness: Boolean): Seq[BrokerMetadata] = {\n+    val brokerSet = brokers.toSet\n+    val results = adminClient.describeCluster().nodes().get().asScala.flatMap(\n+      node => if (!brokerSet.contains(node.id())) {\n+        None\n+      } else {\n+        if (enableRackAwareness && node.rack() != null) {\n+          Some(new BrokerMetadata(node.id(), Some(node.rack())))\n+        } else {\n+          Some(new BrokerMetadata(node.id(), None))\n+        }\n+      }\n+    ).toSeq\n+    val numRackless = results.count(_.rack.isEmpty)\n+    if (enableRackAwareness && numRackless != 0 && numRackless != results.size) {\n+      throw new AdminOperationException(\"Not all brokers have rack information. Add \" +\n+        \"--disable-rack-aware in command line to make replica assignment without rack \" +\n+        \"information.\")\n+    }\n+    results\n+  }\n+\n+  /**\n+   * Find the metadata for some brokers.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param brokers               The brokers to gather metadata about.\n+   * @param enableRackAwareness   True if we should return rack information, and throw an\n+   *                              exception if it is inconsistent.\n+   *\n+   * @return                      The metadata for each broker that was found.\n+   *                              Brokers that were not found will be omitted.\n+   */\n+  def getBrokerMetadata(zkClient: KafkaZkClient,\n+                        brokers: Seq[Int],\n+                        enableRackAwareness: Boolean): Seq[BrokerMetadata] = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val reassignPartitionsCommand = new ReassignPartitionsCommand(zkClient, adminClientOpt, partitionAssignment.toMap, replicaAssignment, adminZkClient)\n+    adminZkClient.getBrokerMetadatas(if (enableRackAwareness)\n+      RackAwareMode.Enforced else RackAwareMode.Disabled, Some(brokers))\n+  }\n+\n+  /**\n+   * Parse and validate data gathered from the command-line for --generate\n+   * In particular, we parse the JSON and validate that duplicate brokers and\n+   * topics don't appear.\n+   *\n+   * @param reassignmentJson       The JSON passed to --generate .\n+   * @param brokerList             A list of brokers passed to --generate.\n+   *\n+   * @return                       A tuple of brokers to reassign, topics to reassign\n+   */\n+  def parseGenerateAssignmentArgs(reassignmentJson: String,\n+                                  brokerList: String): (Seq[Int], Seq[String]) = {\n+    val brokerListToReassign = brokerList.split(',').map(_.toInt)\n+    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n+    if (duplicateReassignments.nonEmpty)\n+      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".\n+        format(duplicateReassignments.mkString(\",\")))\n+    val topicsToReassign = parseTopicsData(reassignmentJson)\n+    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n+    if (duplicateTopicsToReassign.nonEmpty)\n+      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".\n+        format(duplicateTopicsToReassign.mkString(\",\")))\n+    (brokerListToReassign, topicsToReassign)\n+  }\n \n-    // If there is an existing rebalance running, attempt to change its throttle\n-    if (zkClient.reassignPartitionsInProgress()) {\n-      println(\"There is an existing assignment running.\")\n-      reassignPartitionsCommand.maybeLimit(throttle)\n+  /**\n+   * The entry point for the --execute and --execute-additional commands.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param additional                  Whether --additional was passed.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative\n+   *                                    number to skip using a throttle.\n+   * @param logDirThrottle              The replica log directory throttle to use, or a\n+   *                                    negative number to skip using a throttle.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeAssignment(adminClient: Admin,\n+                        additional: Boolean,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long = -1L,\n+                        logDirThrottle: Long = -1L,\n+                        timeoutMs: Long = 10000L,\n+                        time: Time = Time.SYSTEM): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    // If there is an existing assignment, check for --additional before proceeding.\n+    // This helps avoid surprising users.\n+    if (!additional && !currentReassignments.isEmpty) {\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n+    }\n+    verifyBrokerIds(adminClient, proposedParts.values.flatten.toSet)\n+    val currentParts = getReplicaAssignmentForPartitions(adminClient, proposedParts.keySet.toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+    if (interBrokerThrottle >= 0 || logDirThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(currentReassignments, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(adminClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      val movingBrokers = calculateMovingBrokers(proposedReplicas.keySet.toSet)\n+      modifyBrokerThrottles(adminClient,\n+        reassigningBrokers, interBrokerThrottle,\n+        movingBrokers, logDirThrottle)\n+      if (interBrokerThrottle >= 0) {\n+        println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+      }\n+      if (logDirThrottle >= 0) {\n+        println(s\"The replica-alter-dir throttle limit was set to ${logDirThrottle} B/s\")\n+      }\n+    }\n+\n+    // Execute the partition reassignments.\n+    val errors = alterPartitionReassignments(adminClient, proposedParts)\n+    if (!errors.isEmpty) {\n+      throw new TerseReassignmentFailureException(\n+        \"Error reassigning partition(s):%n%s\".format(\n+          errors.keySet.toBuffer.sortWith(compareTopicPartitions).map {\n+            case part => s\"${part}: ${errors(part).getMessage}\"\n+          }.mkString(System.lineSeparator())))\n+    }\n+    println(\"Successfully started partition reassignment%s for %s\".format(\n+      if (proposedParts.size == 1) \"\" else \"s\",\n+      proposedParts.keySet.toBuffer.sortWith(compareTopicPartitions).mkString(\",\")))\n+    if (!proposedReplicas.isEmpty) {\n+      executeMoves(adminClient, proposedReplicas, timeoutMs, time)\n+    }\n+  }\n+\n+  /**\n+   * Execute some partition log directory movements.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param proposedReplicas            A map from TopicPartitionReplicas to the\n+   *                                    directories to move them to.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeMoves(adminClient: Admin,\n+                   proposedReplicas: Map[TopicPartitionReplica, String],\n+                   timeoutMs: Long,\n+                   time: Time): Unit = {\n+    val startTimeMs = time.milliseconds()\n+    val pendingReplicas = new mutable.HashMap[TopicPartitionReplica, String]()\n+    pendingReplicas ++= proposedReplicas\n+    var done = false\n+    do {\n+      val completed = alterReplicaLogDirs(adminClient, pendingReplicas)\n+      if (!completed.isEmpty) {\n+        println(\"Successfully started log directory move%s for: %s\".format(\n+          if (completed.size == 1) \"\" else \"s\",\n+          completed.toBuffer.sortWith(compareTopicPartitionReplicas).mkString(\",\")))\n+      }\n+      pendingReplicas --= completed\n+      if (pendingReplicas.isEmpty) {\n+        done = true\n+      } else if (time.milliseconds() >= startTimeMs + timeoutMs) {\n+        throw new TerseReassignmentFailureException(\n+          \"Timed out before log directory move%s could be started for: %s\".format(\n+            if (pendingReplicas.size == 1) \"\" else \"s\",\n+            pendingReplicas.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).\n+              mkString(\",\")))\n+      } else {\n+        // If a replica has been moved to a new host and we also specified a particular\n+        // log directory, we will have to keep retrying the alterReplicaLogDirs\n+        // call.  It can't take effect until the replica is moved to that host.\n+        time.sleep(100)\n+      }\n+    } while (!done)\n+  }\n+\n+  /**\n+   * Entry point for the --list command.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   */\n+  def listReassignments(adminClient: Admin): Unit = {\n+    println(curReassignmentsToString(adminClient))\n+  }\n+\n+  /**\n+   * Convert the current partition reassignments to text.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   * @return              A string describing the current partition reassignments.\n+   */\n+  def curReassignmentsToString(adminClient: Admin): String = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val text = currentReassignments.keySet.toBuffer.sortWith(compareTopicPartitions).map {\n+      case part =>\n+        val reassignment = currentReassignments(part)\n+        val replicas = reassignment.replicas().asScala\n+        val addingReplicas = reassignment.addingReplicas().asScala\n+        val removingReplicas = reassignment.removingReplicas().asScala\n+        \"%s: replicas: %s.%s%s\".format(part, replicas.mkString(\",\"),\n+          if (addingReplicas.isEmpty) \"\" else\n+            \" adding: %s.\".format(addingReplicas.mkString(\",\")),\n+          if (removingReplicas.isEmpty) \"\" else\n+            \" removing: %s.\".format(removingReplicas.mkString(\",\")))\n+    }.mkString(System.lineSeparator())\n+    if (text.isEmpty) {\n+      \"No partition reassignments found.\"\n     } else {\n-      printCurrentAssignment(zkClient, partitionAssignment.map(_._1.topic))\n-      if (throttle.interBrokerLimit >= 0 || throttle.replicaAlterLogDirsLimit >= 0)\n-        println(String.format(\"Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.\"))\n-      if (reassignPartitionsCommand.reassignPartitions(throttle, timeoutMs)) {\n-        println(\"Successfully started reassignment of partitions.\")\n-      } else\n-        println(\"Failed to reassign partitions %s\".format(partitionAssignment))\n+      \"Current partition reassignments:%n%s\".format(text)\n+    }\n+  }\n+\n+  /**\n+   * Verify that all the brokers in an assignment exist.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param brokers                     The broker IDs to verify.\n+   */\n+  def verifyBrokerIds(adminClient: Admin, brokers: Set[Int]): Unit = {\n+    val allNodeIds = adminClient.describeCluster().nodes().get().asScala.map(_.id).toSet\n+    brokers.find(!allNodeIds.contains(_)).map {\n+      case id => throw new AdminCommandFailedException(s\"Unknown broker id ${id}\")\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --execute command.\n+   *\n+   * @param zkClient                    The ZooKeeper client to use.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   */\n+  def executeAssignment(zkClient: KafkaZkClient,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    if (!proposedReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    verifyReplicasAndBrokersInAssignment(zkClient, proposedParts)\n+\n+    // Check for the presence of the legacy partition reassignment ZNode.  This actually\n+    // won't detect all rebalances... only ones initiated by the legacy method.\n+    // This is a limitation of the legacy ZK API.\n+    val reassignPartitionsInProgress = zkClient.reassignPartitionsInProgress()\n+    if (reassignPartitionsInProgress) {\n+      // Note: older versions of this tool would modify the broker quotas here (but not\n+      // topic quotas, for some reason).  This behavior wasn't documented in the --execute\n+      // command line help.  Since it might interfere with other ongoing reassignments,\n+      // this behavior was dropped as part of the KIP-455 changes.\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n     }\n+    val currentParts = zkClient.getReplicaAssignmentForTopics(\n+      proposedParts.map(_._1.topic()).toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+\n+    if (interBrokerThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(Map.empty, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(zkClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      modifyBrokerThrottles(zkClient, reassigningBrokers, interBrokerThrottle)\n+      println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+    }\n+    zkClient.createPartitionReassignment(proposedParts)\n+    println(\"Successfully started partition reassignment%s for %s\".format(\n+      if (proposedParts.size == 1) \"\" else \"s\",\n+      proposedParts.keySet.toBuffer.sortWith(compareTopicPartitions).mkString(\",\")))\n   }\n \n-  def printCurrentAssignment(zkClient: KafkaZkClient, topics: Seq[String]): Unit = {\n-    // before starting assignment, output the current replica assignment to facilitate rollback\n-    val currentPartitionReplicaAssignment = zkClient.getReplicaAssignmentForTopics(topics.toSet)\n-    println(\"Current partition replica assignment\\n\\n%s\\n\\nSave this to use as the --reassignment-json-file option during rollback\"\n-      .format(formatAsReassignmentJson(currentPartitionReplicaAssignment, Map.empty)))\n+  /**\n+   * Return the string which we want to print to describe the current partition assignment.\n+   *\n+   * @param proposedParts               The proposed partition assignment.\n+   * @param currentParts                The current partition assignment.\n+   *\n+   * @return                            The string to print.  We will only print information about\n+   *                                    partitions that appear in the proposed partition assignment.\n+   */\n+  def currentPartitionReplicaAssignmentToString(proposedParts: Map[TopicPartition, Seq[Int]],\n+                                                currentParts: Map[TopicPartition, Seq[Int]]): String = {\n+    \"Current partition replica assignment%n%n%s%n%nSave this to use as the %s\".\n+        format(formatAsReassignmentJson(currentParts.filterKeys(proposedParts.contains(_)).toMap, Map.empty),\n+              \"--reassignment-json-file option during rollback\")\n+  }\n+\n+  /**\n+   * Verify that the replicas and brokers referenced in the given partition assignment actually\n+   * exist.  This is necessary when using the deprecated ZK API, since ZooKeeper itself can't\n+   * validate what we're applying.\n+   *\n+   * @param zkClient                    The ZooKeeper client to use.\n+   * @param proposedParts               The partition assignment.\n+   */\n+  def verifyReplicasAndBrokersInAssignment(zkClient: KafkaZkClient,\n+                                           proposedParts: Map[TopicPartition, Seq[Int]]): Unit = {\n+    // check that all partitions in the proposed assignment exist in the cluster\n+    val proposedTopics = proposedParts.map { case (tp, _) => tp.topic }\n+    val existingAssignment = zkClient.getReplicaAssignmentForTopics(proposedTopics.toSet)\n+    val nonExistentPartitions = proposedParts.map { case (tp, _) => tp }.filterNot(existingAssignment.contains)\n+    if (nonExistentPartitions.nonEmpty)\n+      throw new AdminCommandFailedException(\"The proposed assignment contains non-existent partitions: \" +\n+        nonExistentPartitions)\n+\n+    // check that all brokers in the proposed assignment exist in the cluster\n+    val existingBrokerIDs = zkClient.getSortedBrokerList\n+    val nonExistingBrokerIDs = proposedParts.toMap.values.flatten.filterNot(existingBrokerIDs.contains).toSet\n+    if (nonExistingBrokerIDs.nonEmpty)\n+      throw new AdminCommandFailedException(\"The proposed assignment contains non-existent brokerIDs: \" + nonExistingBrokerIDs.mkString(\",\"))\n+  }\n+\n+  /**\n+   * Execute the given partition reassignments.\n+   *\n+   * @param adminClient       The admin client object to use.\n+   * @param reassignments     A map from topic names to target replica assignments.\n+   * @return                  A map from partition objects to error strings.\n+   */\n+  def alterPartitionReassignments(adminClient: Admin,\n+                                  reassignments: Map[TopicPartition, Seq[Int]])\n+                                ", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzE1MTUxMA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r393151510", "bodyText": "Same case question", "author": "mumrah", "createdAt": "2020-03-16T16:28:15Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1555 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelLogDirThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelLogDirThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(sources: mutable.Set[Int],\n+                                  destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is done.\n+   */\n+  sealed case class PartitionReassignmentState(currentReplicas: Seq[Int],\n+                                               targetReplicas: Seq[Int],\n+                                               done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait LogDirMoveState {\n+    /**\n+     * True if the move is done without errors.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source log directory is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(currentLogDir: String,\n+                                    targetLogDir: String,\n+                                    futureLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(currentLogDir: String,\n+                                       targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica log directory move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        if (opts.options.has(opts.zkConnectOpt)) {\n+          println(\"Warning: ignoring deprecated --zookeeper option because \" +\n+            \"--bootstrap-server was specified.  The --zookeeper option will \" +\n+            \"be removed in a future version of Kafka.\")\n+        }\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        handleAction(adminClient, opts)\n+      } else {\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        handleAction(zkClient, opts)\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n+    }\n+  }\n+\n+  private def handleAction(adminClient: Admin,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(adminClient,\n+        opts.options.has(opts.additionalOpt),\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt),\n+        opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.cancelOpt)) {\n+      cancelAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.listOpt)) {\n+      listReassignments(adminClient)\n     } else {\n-      None\n+      throw new RuntimeException(\"Unsupported action.\")\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  private def handleAction(zkClient: KafkaZkClient,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt))\n+    } else {\n+      throw new RuntimeException(\"Unsupported action.\")\n+    }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * A result returned from verifyAssignment.\n+   *\n+   * @param partStates    A map from partitions to reassignment states.\n+   * @param partsOngoing  True if there are any ongoing partition reassignments.\n+   * @param moveStates    A map from log directories to movement states.\n+   * @param movesOngoing  True if there are any ongoing moves that we know about.\n+   */\n+  case class VerifyAssignmentResult(partStates: Map[TopicPartition, PartitionReassignmentState],\n+                                    partsOngoing: Boolean = false,\n+                                    moveStates: Map[TopicPartitionReplica, LogDirMoveState] = Map.empty,\n+                                    movesOngoing: Boolean = false)\n+\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetLogDirs)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    VerifyAssignmentResult(partStates, partsOngoing, moveStates, movesOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.  Note that anything that\n+   *                              would require AdminClient to see will be left out of this result.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    if (!targetLogDirs.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    VerifyAssignmentResult(partStates, partsOngoing, Map.empty, false)\n+  }\n+\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n         }\n       }\n+    }\n+    bld.mkString(System.lineSeparator())\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n+        }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, LogDirMoveState], Boolean) = {\n+    val moveStates = findLogDirMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findLogDirMoveStates(adminClient: Admin,\n+                           targetMoves: Map[TopicPartitionReplica, String])\n+                           : Map[TopicPartitionReplica, LogDirMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: LogDirMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingReplicaMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingLogDirMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, LogDirMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    allBrokerIds.foreach {\n+      case brokerId => configOps.put(\n+        new ConfigResource(ConfigResource.Type.BROKER, brokerId.toString),\n+        brokerLevelThrottles.map(throttle => new AlterConfigOp(\n+          new ConfigEntry(throttle, null), OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   */\n+  def clearBrokerLevelThrottles(zkClient: KafkaZkClient): Unit = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val brokerMetadatas = adminZkClient.getBrokerMetadatas(rackAwareMode, Some(brokerListToReassign))\n+    for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n+      if (!brokerLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    topics.foreach {\n+      topicName => configOps.put(\n+        new ConfigResource(ConfigResource.Type.TOPIC, topicName),\n+        topicLevelThrottles.map(throttle => new AlterConfigOp(new ConfigEntry(throttle, null),\n+          OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(zkClient: KafkaZkClient, topics: Set[String]): Unit = {\n+    val adminZkClient = new AdminZkClient(zkClient)\n+    for (topic <- topics) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n+      if (!topicLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeTopicConfig(topic, configs)\n+      }\n+    }\n+  }\n \n-    val partitionsToBeReassigned = mutable.Map[TopicPartition, Seq[Int]]()\n+  /**\n+   * The entry point for the --generate command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(adminClient: Admin,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = getReplicaAssignmentForTopics(adminClient, topicsToReassign)\n+    val brokerMetadatas = getBrokerMetadata(adminClient, brokersToReassign, enableRackAwareness)\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * The legacy entry point for the --generate command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(zkClient: KafkaZkClient,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+    val brokerMetadatas = getBrokerMetadata(zkClient, brokersToReassign, enableRackAwareness)\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * Calculate the new partition assignments to suggest in --generate.\n+   *\n+   * @param currentAssignment  The current partition assignments.\n+   * @param brokerMetadatas    The rack information for each broker.\n+   *\n+   * @return                   A map from partitions to the proposed assignments for each.\n+   */\n+  def calculateAssignment(currentAssignment: Map[TopicPartition, Seq[Int]],\n+                          brokerMetadatas: Seq[BrokerMetadata])\n+                          : Map[TopicPartition, Seq[Int]] = {\n+    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n+    val proposedAssignments = mutable.Map[TopicPartition, Seq[Int]]()\n     groupedByTopic.foreach { case (topic, assignment) =>\n       val (_, replicas) = assignment.head\n-      val assignedReplicas = AdminUtils.assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n-      partitionsToBeReassigned ++= assignedReplicas.map { case (partition, replicas) =>\n+      val assignedReplicas = AdminUtils.\n+        assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n+      proposedAssignments ++= assignedReplicas.map { case (partition, replicas) =>\n         new TopicPartition(topic, partition) -> replicas\n       }\n     }\n-    (partitionsToBeReassigned, currentAssignment)\n+    proposedAssignments\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val reassignmentJsonFile =  opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val reassignmentJsonString = Utils.readFileAsString(reassignmentJsonFile)\n-    val interBrokerThrottle = opts.options.valueOf(opts.interBrokerThrottleOpt)\n-    val replicaAlterLogDirsThrottle = opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt)\n-    val timeoutMs = opts.options.valueOf(opts.timeoutOpt)\n-    executeAssignment(zkClient, adminClientOpt, reassignmentJsonString, Throttle(interBrokerThrottle, replicaAlterLogDirsThrottle), timeoutMs)\n+  /**\n+   * Get the current replica assignments for some topics.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics          The topics to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForTopics(adminClient: Admin,\n+                                    topics: Seq[String])\n+                                    : Map[TopicPartition, Seq[Int]] = {\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.map {\n+          info => (new TopicPartition(topicName, info.partition()),\n+            info.replicas().asScala.map(_.id()))\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], reassignmentJsonString: String, throttle: Throttle, timeoutMs: Long = 10000L): Unit = {\n-    val (partitionAssignment, replicaAssignment) = parseAndValidate(zkClient, reassignmentJsonString)\n+  /**\n+   * Get the current replica assignments for some partitions.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param partitions      The partitions to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForPartitions(adminClient: Admin,\n+                                        partitions: Set[TopicPartition])\n+                                        : Map[TopicPartition, Seq[Int]] = {\n+    val topics = new mutable.HashSet[String]()\n+    partitions.foreach(topics += _.topic)\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.flatMap {\n+          info => if (partitions.contains(new TopicPartition(topicName, info.partition()))) {\n+            Some(new TopicPartition(topicName, info.partition()),\n+                info.replicas().asScala.map(_.id()))\n+          } else {\n+            None\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param adminClient         The AdminClient object.\n+   * @param brokers             The brokers to gather metadata about.\n+   * @param enableRackAwareness True if we should return rack information, and throw an\n+   *                            exception if it is inconsistent.\n+   *\n+   * @return                    The metadata for each broker that was found.\n+   *                            Brokers that were not found will be omitted.\n+   */\n+  def getBrokerMetadata(adminClient: Admin,\n+                        brokers: Seq[Int],\n+                        enableRackAwareness: Boolean): Seq[BrokerMetadata] = {\n+    val brokerSet = brokers.toSet\n+    val results = adminClient.describeCluster().nodes().get().asScala.flatMap(\n+      node => if (!brokerSet.contains(node.id())) {\n+        None\n+      } else {\n+        if (enableRackAwareness && node.rack() != null) {\n+          Some(new BrokerMetadata(node.id(), Some(node.rack())))\n+        } else {\n+          Some(new BrokerMetadata(node.id(), None))\n+        }\n+      }\n+    ).toSeq\n+    val numRackless = results.count(_.rack.isEmpty)\n+    if (enableRackAwareness && numRackless != 0 && numRackless != results.size) {\n+      throw new AdminOperationException(\"Not all brokers have rack information. Add \" +\n+        \"--disable-rack-aware in command line to make replica assignment without rack \" +\n+        \"information.\")\n+    }\n+    results\n+  }\n+\n+  /**\n+   * Find the metadata for some brokers.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param brokers               The brokers to gather metadata about.\n+   * @param enableRackAwareness   True if we should return rack information, and throw an\n+   *                              exception if it is inconsistent.\n+   *\n+   * @return                      The metadata for each broker that was found.\n+   *                              Brokers that were not found will be omitted.\n+   */\n+  def getBrokerMetadata(zkClient: KafkaZkClient,\n+                        brokers: Seq[Int],\n+                        enableRackAwareness: Boolean): Seq[BrokerMetadata] = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val reassignPartitionsCommand = new ReassignPartitionsCommand(zkClient, adminClientOpt, partitionAssignment.toMap, replicaAssignment, adminZkClient)\n+    adminZkClient.getBrokerMetadatas(if (enableRackAwareness)\n+      RackAwareMode.Enforced else RackAwareMode.Disabled, Some(brokers))\n+  }\n+\n+  /**\n+   * Parse and validate data gathered from the command-line for --generate\n+   * In particular, we parse the JSON and validate that duplicate brokers and\n+   * topics don't appear.\n+   *\n+   * @param reassignmentJson       The JSON passed to --generate .\n+   * @param brokerList             A list of brokers passed to --generate.\n+   *\n+   * @return                       A tuple of brokers to reassign, topics to reassign\n+   */\n+  def parseGenerateAssignmentArgs(reassignmentJson: String,\n+                                  brokerList: String): (Seq[Int], Seq[String]) = {\n+    val brokerListToReassign = brokerList.split(',').map(_.toInt)\n+    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n+    if (duplicateReassignments.nonEmpty)\n+      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".\n+        format(duplicateReassignments.mkString(\",\")))\n+    val topicsToReassign = parseTopicsData(reassignmentJson)\n+    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n+    if (duplicateTopicsToReassign.nonEmpty)\n+      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".\n+        format(duplicateTopicsToReassign.mkString(\",\")))\n+    (brokerListToReassign, topicsToReassign)\n+  }\n \n-    // If there is an existing rebalance running, attempt to change its throttle\n-    if (zkClient.reassignPartitionsInProgress()) {\n-      println(\"There is an existing assignment running.\")\n-      reassignPartitionsCommand.maybeLimit(throttle)\n+  /**\n+   * The entry point for the --execute and --execute-additional commands.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param additional                  Whether --additional was passed.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative\n+   *                                    number to skip using a throttle.\n+   * @param logDirThrottle              The replica log directory throttle to use, or a\n+   *                                    negative number to skip using a throttle.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeAssignment(adminClient: Admin,\n+                        additional: Boolean,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long = -1L,\n+                        logDirThrottle: Long = -1L,\n+                        timeoutMs: Long = 10000L,\n+                        time: Time = Time.SYSTEM): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    // If there is an existing assignment, check for --additional before proceeding.\n+    // This helps avoid surprising users.\n+    if (!additional && !currentReassignments.isEmpty) {\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n+    }\n+    verifyBrokerIds(adminClient, proposedParts.values.flatten.toSet)\n+    val currentParts = getReplicaAssignmentForPartitions(adminClient, proposedParts.keySet.toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+    if (interBrokerThrottle >= 0 || logDirThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(currentReassignments, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(adminClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      val movingBrokers = calculateMovingBrokers(proposedReplicas.keySet.toSet)\n+      modifyBrokerThrottles(adminClient,\n+        reassigningBrokers, interBrokerThrottle,\n+        movingBrokers, logDirThrottle)\n+      if (interBrokerThrottle >= 0) {\n+        println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+      }\n+      if (logDirThrottle >= 0) {\n+        println(s\"The replica-alter-dir throttle limit was set to ${logDirThrottle} B/s\")\n+      }\n+    }\n+\n+    // Execute the partition reassignments.\n+    val errors = alterPartitionReassignments(adminClient, proposedParts)\n+    if (!errors.isEmpty) {\n+      throw new TerseReassignmentFailureException(\n+        \"Error reassigning partition(s):%n%s\".format(\n+          errors.keySet.toBuffer.sortWith(compareTopicPartitions).map {\n+            case part => s\"${part}: ${errors(part).getMessage}\"\n+          }.mkString(System.lineSeparator())))\n+    }\n+    println(\"Successfully started partition reassignment%s for %s\".format(\n+      if (proposedParts.size == 1) \"\" else \"s\",\n+      proposedParts.keySet.toBuffer.sortWith(compareTopicPartitions).mkString(\",\")))\n+    if (!proposedReplicas.isEmpty) {\n+      executeMoves(adminClient, proposedReplicas, timeoutMs, time)\n+    }\n+  }\n+\n+  /**\n+   * Execute some partition log directory movements.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param proposedReplicas            A map from TopicPartitionReplicas to the\n+   *                                    directories to move them to.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeMoves(adminClient: Admin,\n+                   proposedReplicas: Map[TopicPartitionReplica, String],\n+                   timeoutMs: Long,\n+                   time: Time): Unit = {\n+    val startTimeMs = time.milliseconds()\n+    val pendingReplicas = new mutable.HashMap[TopicPartitionReplica, String]()\n+    pendingReplicas ++= proposedReplicas\n+    var done = false\n+    do {\n+      val completed = alterReplicaLogDirs(adminClient, pendingReplicas)\n+      if (!completed.isEmpty) {\n+        println(\"Successfully started log directory move%s for: %s\".format(\n+          if (completed.size == 1) \"\" else \"s\",\n+          completed.toBuffer.sortWith(compareTopicPartitionReplicas).mkString(\",\")))\n+      }\n+      pendingReplicas --= completed\n+      if (pendingReplicas.isEmpty) {\n+        done = true\n+      } else if (time.milliseconds() >= startTimeMs + timeoutMs) {\n+        throw new TerseReassignmentFailureException(\n+          \"Timed out before log directory move%s could be started for: %s\".format(\n+            if (pendingReplicas.size == 1) \"\" else \"s\",\n+            pendingReplicas.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).\n+              mkString(\",\")))\n+      } else {\n+        // If a replica has been moved to a new host and we also specified a particular\n+        // log directory, we will have to keep retrying the alterReplicaLogDirs\n+        // call.  It can't take effect until the replica is moved to that host.\n+        time.sleep(100)\n+      }\n+    } while (!done)\n+  }\n+\n+  /**\n+   * Entry point for the --list command.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   */\n+  def listReassignments(adminClient: Admin): Unit = {\n+    println(curReassignmentsToString(adminClient))\n+  }\n+\n+  /**\n+   * Convert the current partition reassignments to text.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   * @return              A string describing the current partition reassignments.\n+   */\n+  def curReassignmentsToString(adminClient: Admin): String = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val text = currentReassignments.keySet.toBuffer.sortWith(compareTopicPartitions).map {\n+      case part =>\n+        val reassignment = currentReassignments(part)\n+        val replicas = reassignment.replicas().asScala\n+        val addingReplicas = reassignment.addingReplicas().asScala\n+        val removingReplicas = reassignment.removingReplicas().asScala\n+        \"%s: replicas: %s.%s%s\".format(part, replicas.mkString(\",\"),\n+          if (addingReplicas.isEmpty) \"\" else\n+            \" adding: %s.\".format(addingReplicas.mkString(\",\")),\n+          if (removingReplicas.isEmpty) \"\" else\n+            \" removing: %s.\".format(removingReplicas.mkString(\",\")))\n+    }.mkString(System.lineSeparator())\n+    if (text.isEmpty) {\n+      \"No partition reassignments found.\"\n     } else {\n-      printCurrentAssignment(zkClient, partitionAssignment.map(_._1.topic))\n-      if (throttle.interBrokerLimit >= 0 || throttle.replicaAlterLogDirsLimit >= 0)\n-        println(String.format(\"Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.\"))\n-      if (reassignPartitionsCommand.reassignPartitions(throttle, timeoutMs)) {\n-        println(\"Successfully started reassignment of partitions.\")\n-      } else\n-        println(\"Failed to reassign partitions %s\".format(partitionAssignment))\n+      \"Current partition reassignments:%n%s\".format(text)\n+    }\n+  }\n+\n+  /**\n+   * Verify that all the brokers in an assignment exist.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param brokers                     The broker IDs to verify.\n+   */\n+  def verifyBrokerIds(adminClient: Admin, brokers: Set[Int]): Unit = {\n+    val allNodeIds = adminClient.describeCluster().nodes().get().asScala.map(_.id).toSet\n+    brokers.find(!allNodeIds.contains(_)).map {\n+      case id => throw new AdminCommandFailedException(s\"Unknown broker id ${id}\")\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --execute command.\n+   *\n+   * @param zkClient                    The ZooKeeper client to use.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   */\n+  def executeAssignment(zkClient: KafkaZkClient,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    if (!proposedReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    verifyReplicasAndBrokersInAssignment(zkClient, proposedParts)\n+\n+    // Check for the presence of the legacy partition reassignment ZNode.  This actually\n+    // won't detect all rebalances... only ones initiated by the legacy method.\n+    // This is a limitation of the legacy ZK API.\n+    val reassignPartitionsInProgress = zkClient.reassignPartitionsInProgress()\n+    if (reassignPartitionsInProgress) {\n+      // Note: older versions of this tool would modify the broker quotas here (but not\n+      // topic quotas, for some reason).  This behavior wasn't documented in the --execute\n+      // command line help.  Since it might interfere with other ongoing reassignments,\n+      // this behavior was dropped as part of the KIP-455 changes.\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n     }\n+    val currentParts = zkClient.getReplicaAssignmentForTopics(\n+      proposedParts.map(_._1.topic()).toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+\n+    if (interBrokerThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(Map.empty, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(zkClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      modifyBrokerThrottles(zkClient, reassigningBrokers, interBrokerThrottle)\n+      println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+    }\n+    zkClient.createPartitionReassignment(proposedParts)\n+    println(\"Successfully started partition reassignment%s for %s\".format(\n+      if (proposedParts.size == 1) \"\" else \"s\",\n+      proposedParts.keySet.toBuffer.sortWith(compareTopicPartitions).mkString(\",\")))\n   }\n \n-  def printCurrentAssignment(zkClient: KafkaZkClient, topics: Seq[String]): Unit = {\n-    // before starting assignment, output the current replica assignment to facilitate rollback\n-    val currentPartitionReplicaAssignment = zkClient.getReplicaAssignmentForTopics(topics.toSet)\n-    println(\"Current partition replica assignment\\n\\n%s\\n\\nSave this to use as the --reassignment-json-file option during rollback\"\n-      .format(formatAsReassignmentJson(currentPartitionReplicaAssignment, Map.empty)))\n+  /**\n+   * Return the string which we want to print to describe the current partition assignment.\n+   *\n+   * @param proposedParts               The proposed partition assignment.\n+   * @param currentParts                The current partition assignment.\n+   *\n+   * @return                            The string to print.  We will only print information about\n+   *                                    partitions that appear in the proposed partition assignment.\n+   */\n+  def currentPartitionReplicaAssignmentToString(proposedParts: Map[TopicPartition, Seq[Int]],\n+                                                currentParts: Map[TopicPartition, Seq[Int]]): String = {\n+    \"Current partition replica assignment%n%n%s%n%nSave this to use as the %s\".\n+        format(formatAsReassignmentJson(currentParts.filterKeys(proposedParts.contains(_)).toMap, Map.empty),\n+              \"--reassignment-json-file option during rollback\")\n+  }\n+\n+  /**\n+   * Verify that the replicas and brokers referenced in the given partition assignment actually\n+   * exist.  This is necessary when using the deprecated ZK API, since ZooKeeper itself can't\n+   * validate what we're applying.\n+   *\n+   * @param zkClient                    The ZooKeeper client to use.\n+   * @param proposedParts               The partition assignment.\n+   */\n+  def verifyReplicasAndBrokersInAssignment(zkClient: KafkaZkClient,\n+                                           proposedParts: Map[TopicPartition, Seq[Int]]): Unit = {\n+    // check that all partitions in the proposed assignment exist in the cluster\n+    val proposedTopics = proposedParts.map { case (tp, _) => tp.topic }\n+    val existingAssignment = zkClient.getReplicaAssignmentForTopics(proposedTopics.toSet)\n+    val nonExistentPartitions = proposedParts.map { case (tp, _) => tp }.filterNot(existingAssignment.contains)\n+    if (nonExistentPartitions.nonEmpty)\n+      throw new AdminCommandFailedException(\"The proposed assignment contains non-existent partitions: \" +\n+        nonExistentPartitions)\n+\n+    // check that all brokers in the proposed assignment exist in the cluster\n+    val existingBrokerIDs = zkClient.getSortedBrokerList\n+    val nonExistingBrokerIDs = proposedParts.toMap.values.flatten.filterNot(existingBrokerIDs.contains).toSet\n+    if (nonExistingBrokerIDs.nonEmpty)\n+      throw new AdminCommandFailedException(\"The proposed assignment contains non-existent brokerIDs: \" + nonExistingBrokerIDs.mkString(\",\"))\n+  }\n+\n+  /**\n+   * Execute the given partition reassignments.\n+   *\n+   * @param adminClient       The admin client object to use.\n+   * @param reassignments     A map from topic names to target replica assignments.\n+   * @return                  A map from partition objects to error strings.\n+   */\n+  def alterPartitionReassignments(adminClient: Admin,\n+                                  reassignments: Map[TopicPartition, Seq[Int]])\n+                                ", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzE1MjEyNQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r393152125", "bodyText": "I'm guessing a None is implied here? If so, could we include it?", "author": "mumrah", "createdAt": "2020-03-16T16:29:11Z", "path": "core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala", "diffHunk": "@@ -16,233 +16,1555 @@\n  */\n package kafka.admin\n \n-import java.util.Properties\n+import java.util\n+import java.util.Optional\n import java.util.concurrent.ExecutionException\n \n import kafka.common.AdminCommandFailedException\n import kafka.log.LogConfig\n-import kafka.log.LogConfig._\n import kafka.server.{ConfigType, DynamicConfig}\n-import kafka.utils._\n+import kafka.utils.{CommandDefaultOptions, CommandLineUtils, CoreUtils, Exit, Json, Logging}\n import kafka.utils.json.JsonValue\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n-import org.apache.kafka.clients.admin.DescribeReplicaLogDirsResult.ReplicaLogDirInfo\n-import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterReplicaLogDirsOptions}\n-import org.apache.kafka.common.errors.ReplicaNotAvailableException\n+import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n+import org.apache.kafka.clients.admin.{Admin, AdminClientConfig, AlterConfigOp, ConfigEntry, NewPartitionReassignment, PartitionReassignment}\n+import org.apache.kafka.common.config.ConfigResource\n+import org.apache.kafka.common.errors.{ReplicaNotAvailableException, UnknownTopicOrPartitionException}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.utils.{Time, Utils}\n-import org.apache.kafka.common.{TopicPartition, TopicPartitionReplica}\n-import org.apache.zookeeper.KeeperException.NodeExistsException\n+import org.apache.kafka.common.{KafkaException, KafkaFuture, TopicPartition, TopicPartitionReplica}\n \n import scala.collection.JavaConverters._\n-import scala.collection._\n+import scala.collection.{Map, Seq, mutable}\n+import scala.compat.java8.OptionConverters._\n+import scala.math.Ordered.orderingToOrdered\n \n object ReassignPartitionsCommand extends Logging {\n-\n-  case class Throttle(interBrokerLimit: Long, replicaAlterLogDirsLimit: Long = -1, postUpdateAction: () => Unit = () => ())\n-\n-  private[admin] val NoThrottle = Throttle(-1, -1)\n   private[admin] val AnyLogDir = \"any\"\n \n+  val helpText = \"This tool helps to move topic partitions between replicas.\"\n+\n+  /**\n+   * The earliest version of the partition reassignment JSON.  We will default to this\n+   * version if no other version number is given.\n+   */\n   private[admin] val EarliestVersion = 1\n \n-  val helpText = \"This tool helps to moves topic partitions between replicas.\"\n+  /**\n+   * The earliest version of the JSON for each partition reassignment topic.  We will\n+   * default to this version if no other version number is given.\n+   */\n+  private[admin] val EarliestTopicsJsonVersion = 1\n+\n+  // Throttles that are set at the level of an individual broker.\n+  private[admin] val brokerLevelLeaderThrottle =\n+    DynamicConfig.Broker.LeaderReplicationThrottledRateProp\n+  private[admin] val brokerLevelFollowerThrottle =\n+    DynamicConfig.Broker.FollowerReplicationThrottledRateProp\n+  private[admin] val brokerLevelLogDirThrottle =\n+    DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp\n+  private[admin] val brokerLevelThrottles = Seq(\n+    brokerLevelLeaderThrottle,\n+    brokerLevelFollowerThrottle,\n+    brokerLevelLogDirThrottle\n+  )\n+\n+  // Throttles that are set at the level of an individual topic.\n+  private[admin] val topicLevelLeaderThrottle =\n+    LogConfig.LeaderReplicationThrottledReplicasProp\n+  private[admin] val topicLevelFollowerThrottle =\n+    LogConfig.FollowerReplicationThrottledReplicasProp\n+  private[admin] val topicLevelThrottles = Seq(\n+    topicLevelLeaderThrottle,\n+    topicLevelFollowerThrottle\n+  )\n+\n+  private[admin] val cannotExecuteBecauseOfExistingMessage = \"Cannot execute because \" +\n+    \"there is an existing partition assignment.  Use --additional to override this and \" +\n+    \"create a new partition assignment in addition to the existing one.\"\n+\n+  private[admin] val youMustRunVerifyPeriodicallyMessage = \"Warning: You must run \" +\n+    \"--verify periodically, until the reassignment completes, to ensure the throttle \" +\n+    \"is removed.\"\n+\n+  /**\n+   * A map from topic names to partition movements.\n+   */\n+  type MoveMap = mutable.Map[String, mutable.Map[Int, PartitionMove]]\n+\n+  /**\n+   * A partition movement.  The source and destination brokers may overlap.\n+   *\n+   * @param sources         The source brokers.\n+   * @param destinations    The destination brokers.\n+   */\n+  sealed case class PartitionMove(sources: mutable.Set[Int],\n+                                  destinations: mutable.Set[Int]) { }\n+\n+  /**\n+   * The state of a partition reassignment.  The current replicas and target replicas\n+   * may overlap.\n+   *\n+   * @param currentReplicas The current replicas.\n+   * @param targetReplicas  The target replicas.\n+   * @param done            True if the reassignment is done.\n+   */\n+  sealed case class PartitionReassignmentState(currentReplicas: Seq[Int],\n+                                               targetReplicas: Seq[Int],\n+                                               done: Boolean) {}\n+\n+  /**\n+   * The state of a replica log directory movement.\n+   */\n+  sealed trait LogDirMoveState {\n+    /**\n+     * True if the move is done without errors.\n+     */\n+    def done: Boolean\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source log directory is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingReplicaMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the source replica is missing.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class MissingLogDirMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where the move is in progress.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param futureLogDir        The log directory that the replica is moving to.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class ActiveMoveState(currentLogDir: String,\n+                                    targetLogDir: String,\n+                                    futureLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = false\n+  }\n+\n+  /**\n+   * A replica log directory move state where there is no move in progress, but we did not\n+   * reach the target log directory.\n+   *\n+   * @param currentLogDir       The current log directory.\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CancelledMoveState(currentLogDir: String,\n+                                       targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * The completed replica log directory move state.\n+   *\n+   * @param targetLogDir        The log directory that we wanted the replica to move to.\n+   */\n+  sealed case class CompletedMoveState(targetLogDir: String)\n+      extends LogDirMoveState {\n+    override def done = true\n+  }\n+\n+  /**\n+   * An exception thrown to indicate that the command has failed, but we don't want to\n+   * print a stack trace.\n+   *\n+   * @param message     The message to print out before exiting.  A stack trace will not\n+   *                    be printed.\n+   */\n+  class TerseReassignmentFailureException(message: String) extends KafkaException(message) {\n+  }\n \n   def main(args: Array[String]): Unit = {\n     val opts = validateAndParseArgs(args)\n-    val zkConnect = opts.options.valueOf(opts.zkConnectOpt)\n-    val time = Time.SYSTEM\n-    val zkClient = KafkaZkClient(zkConnect, JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, time)\n-\n-    val adminClientOpt = createAdminClient(opts)\n+    var toClose: Option[AutoCloseable] = None\n+    var failed = true\n \n     try {\n-      if(opts.options.has(opts.verifyOpt))\n-        verifyAssignment(zkClient, adminClientOpt, opts)\n-      else if(opts.options.has(opts.generateOpt))\n-        generateAssignment(zkClient, opts)\n-      else if (opts.options.has(opts.executeOpt))\n-        executeAssignment(zkClient, adminClientOpt, opts)\n+      if (opts.options.has(opts.bootstrapServerOpt)) {\n+        if (opts.options.has(opts.zkConnectOpt)) {\n+          println(\"Warning: ignoring deprecated --zookeeper option because \" +\n+            \"--bootstrap-server was specified.  The --zookeeper option will \" +\n+            \"be removed in a future version of Kafka.\")\n+        }\n+        val props = if (opts.options.has(opts.commandConfigOpt))\n+          Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n+        else\n+          new util.Properties()\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n+        props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n+        val adminClient = Admin.create(props)\n+        toClose = Some(adminClient)\n+        handleAction(adminClient, opts)\n+      } else {\n+        println(\"Warning: --zookeeper is deprecated, and will be removed in a future \" +\n+          \"version of Kafka.\")\n+        val zkClient = KafkaZkClient(opts.options.valueOf(opts.zkConnectOpt),\n+          JaasUtils.isZkSaslEnabled, 30000, 30000, Int.MaxValue, Time.SYSTEM)\n+        toClose = Some(zkClient)\n+        handleAction(zkClient, opts)\n+      }\n+      failed = false\n     } catch {\n+      case e: TerseReassignmentFailureException =>\n+        println(e.getMessage)\n       case e: Throwable =>\n-        println(\"Partitions reassignment failed due to \" + e.getMessage)\n+        println(\"Error: \" + e.getMessage)\n         println(Utils.stackTrace(e))\n-    } finally zkClient.close()\n-  }\n-\n-  private def createAdminClient(opts: ReassignPartitionsCommandOptions): Option[Admin] = {\n-    if (opts.options.has(opts.bootstrapServerOpt)) {\n-      val props = if (opts.options.has(opts.commandConfigOpt))\n-        Utils.loadProps(opts.options.valueOf(opts.commandConfigOpt))\n-      else\n-        new Properties()\n-      props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, opts.options.valueOf(opts.bootstrapServerOpt))\n-      props.putIfAbsent(AdminClientConfig.CLIENT_ID_CONFIG, \"reassign-partitions-tool\")\n-      Some(Admin.create(props))\n+    } finally {\n+      // Close the AdminClient or ZooKeeper client, as appropriate.\n+      // It's good to do this after printing any error stack trace.\n+      toClose.foreach(_.close())\n+    }\n+    // If the command failed, exit with a non-zero exit code.\n+    if (failed) {\n+      Exit.exit(1)\n+    }\n+  }\n+\n+  private def handleAction(adminClient: Admin,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(adminClient,\n+        opts.options.has(opts.additionalOpt),\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt),\n+        opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.cancelOpt)) {\n+      cancelAssignment(adminClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt),\n+        opts.options.valueOf(opts.timeoutOpt))\n+    } else if (opts.options.has(opts.listOpt)) {\n+      listReassignments(adminClient)\n     } else {\n-      None\n+      throw new RuntimeException(\"Unsupported action.\")\n     }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val jsonFile = opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val jsonString = Utils.readFileAsString(jsonFile)\n-    verifyAssignment(zkClient, adminClientOpt, jsonString)\n+  private def handleAction(zkClient: KafkaZkClient,\n+                           opts: ReassignPartitionsCommandOptions): Unit = {\n+    if (opts.options.has(opts.verifyOpt)) {\n+      verifyAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.has(opts.preserveThrottlesOpt))\n+    } else if (opts.options.has(opts.generateOpt)) {\n+      generateAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.topicsToMoveJsonFileOpt)),\n+        opts.options.valueOf(opts.brokerListOpt),\n+        !opts.options.has(opts.disableRackAware))\n+    } else if (opts.options.has(opts.executeOpt)) {\n+      executeAssignment(zkClient,\n+        Utils.readFileAsString(opts.options.valueOf(opts.reassignmentJsonFileOpt)),\n+        opts.options.valueOf(opts.interBrokerThrottleOpt))\n+    } else {\n+      throw new RuntimeException(\"Unsupported action.\")\n+    }\n   }\n \n-  def verifyAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], jsonString: String): Unit = {\n-    println(\"Status of partition reassignment: \")\n-    val adminZkClient = new AdminZkClient(zkClient)\n-    val (partitionsToBeReassigned, replicaAssignment) = parsePartitionReassignmentData(jsonString)\n-    val reassignedPartitionsStatus = checkIfPartitionReassignmentSucceeded(zkClient, partitionsToBeReassigned.toMap)\n-    val replicasReassignmentStatus = checkIfReplicaReassignmentSucceeded(adminClientOpt, replicaAssignment)\n-\n-    reassignedPartitionsStatus.foreach { case (topicPartition, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of partition %s completed successfully\".format(topicPartition))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of partition %s failed\".format(topicPartition))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of partition %s is still in progress\".format(topicPartition))\n-      }\n+  /**\n+   * A result returned from verifyAssignment.\n+   *\n+   * @param partStates    A map from partitions to reassignment states.\n+   * @param partsOngoing  True if there are any ongoing partition reassignments.\n+   * @param moveStates    A map from log directories to movement states.\n+   * @param movesOngoing  True if there are any ongoing moves that we know about.\n+   */\n+  case class VerifyAssignmentResult(partStates: Map[TopicPartition, PartitionReassignmentState],\n+                                    partsOngoing: Boolean = false,\n+                                    moveStates: Map[TopicPartitionReplica, LogDirMoveState] = Map.empty,\n+                                    movesOngoing: Boolean = false)\n+\n+  /**\n+   * The entry point for the --verify command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.\n+   */\n+  def verifyAssignment(adminClient: Admin, jsonString: String, preserveThrottles: Boolean)\n+                      : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(adminClient, targetParts)\n+    val (moveStates, movesOngoing) = verifyReplicaMoves(adminClient, targetLogDirs)\n+    if (!partsOngoing && !movesOngoing && !preserveThrottles) {\n+      // If the partition assignments and replica assignments are done, clear any throttles\n+      // that were set.  We have to clear all throttles, because we don't have enough\n+      // information to know all of the source brokers that might have been involved in the\n+      // previous reassignments.\n+      clearAllThrottles(adminClient, targetParts.map(_._1.topic()).toSet)\n     }\n+    VerifyAssignmentResult(partStates, partsOngoing, moveStates, movesOngoing)\n+  }\n \n-    replicasReassignmentStatus.foreach { case (replica, status) =>\n-      status match {\n-        case ReassignmentCompleted =>\n-          println(\"Reassignment of replica %s completed successfully\".format(replica))\n-        case ReassignmentFailed =>\n-          println(\"Reassignment of replica %s failed\".format(replica))\n-        case ReassignmentInProgress =>\n-          println(\"Reassignment of replica %s is still in progress\".format(replica))\n-      }\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the partition reassignment states, and a\n+   *                              boolean which is true if there are no ongoing\n+   *                              reassignments (including reassignments not described\n+   *                              in the JSON file.)\n+   */\n+  def verifyPartitionAssignments(adminClient: Admin,\n+                                 targets: Seq[(TopicPartition, Seq[Int])])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(adminClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  /**\n+   * The deprecated entry point for the --verify command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param jsonString            The JSON string to use for the topics and partitions to verify.\n+   * @param preserveThrottles     True if we should avoid changing topic or broker throttles.\n+   *\n+   * @returns                     A result that is useful for testing.  Note that anything that\n+   *                              would require AdminClient to see will be left out of this result.\n+   */\n+  def verifyAssignment(zkClient: KafkaZkClient, jsonString: String, preserveThrottles: Boolean)\n+                       : VerifyAssignmentResult = {\n+    val (targetParts, targetLogDirs) = parsePartitionReassignmentData(jsonString)\n+    if (!targetLogDirs.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n     }\n-    removeThrottle(zkClient, reassignedPartitionsStatus, replicasReassignmentStatus, adminZkClient)\n-  }\n-\n-  private[admin] def removeThrottle(zkClient: KafkaZkClient,\n-                                    reassignedPartitionsStatus: Map[TopicPartition, ReassignmentStatus],\n-                                    replicasReassignmentStatus: Map[TopicPartitionReplica, ReassignmentStatus],\n-                                    adminZkClient: AdminZkClient): Unit = {\n-\n-    //If both partition assignment and replica reassignment have completed remove both the inter-broker and replica-alter-dir throttle\n-    if (reassignedPartitionsStatus.forall { case (_, status) => status == ReassignmentCompleted } &&\n-        replicasReassignmentStatus.forall { case (_, status) => status == ReassignmentCompleted }) {\n-      var changed = false\n-\n-      //Remove the throttle limit from all brokers in the cluster\n-      //(as we no longer know which specific brokers were involved in the move)\n-      for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(DynamicConfig.Broker.LeaderReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.FollowerReplicationThrottledRateProp) != null\n-          | configs.remove(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp) != null){\n-          adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n-          changed = true\n+    println(\"Warning: because you are using the deprecated --zookeeper option, the results \" +\n+      \"may be incomplete.  Use --bootstrap-server instead for more accurate results.\")\n+    val (partStates, partsOngoing) = verifyPartitionAssignments(zkClient, targetParts.toMap)\n+    if (!partsOngoing && !preserveThrottles) {\n+      println(\"Clearing broker-level throttles....\")\n+      clearBrokerLevelThrottles(zkClient)\n+      println(\"Clearing topic-level throttles....\")\n+      clearTopicLevelThrottles(zkClient, targetParts.map(_._1.topic()).toSet)\n+    }\n+    VerifyAssignmentResult(partStates, partsOngoing, Map.empty, false)\n+  }\n+\n+  /**\n+   * Verify the partition reassignments specified by the user.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targets               The partition reassignments specified by the user.\n+   *\n+   * @returns                     A tuple of partition states and whether there are any\n+   *                              ongoing reassignments found in the legacy reassign\n+   *                              partitions ZNode.\n+   */\n+  def verifyPartitionAssignments(zkClient: KafkaZkClient,\n+                                 targets: Map[TopicPartition, Seq[Int]])\n+                                 : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val (partStates, partsOngoing) = findPartitionReassignmentStates(zkClient, targets)\n+    println(partitionReassignmentStatesToString(partStates))\n+    (partStates, partsOngoing)\n+  }\n+\n+  def compareTopicPartitions(a: TopicPartition, b: TopicPartition): Boolean = {\n+    (a.topic(), a.partition()) < (b.topic(), b.partition())\n+  }\n+\n+  def compareTopicPartitionReplicas(a: TopicPartitionReplica, b: TopicPartitionReplica): Boolean = {\n+    (a.brokerId(), a.topic(), a.partition()) < (b.brokerId(), b.topic(), b.partition())\n+  }\n+\n+  /**\n+   * Convert partition reassignment states to a human-readable string.\n+   *\n+   * @param states      A map from topic partitions to states.\n+   * @return            A string summarizing the partition reassignment states.\n+   */\n+  def partitionReassignmentStatesToString(states: Map[TopicPartition, PartitionReassignmentState])\n+                                          : String = {\n+    val bld = new mutable.ArrayBuffer[String]()\n+    bld.append(\"Status of partition reassignment:\")\n+    states.keySet.toBuffer.sortWith(compareTopicPartitions).foreach {\n+      topicPartition => {\n+        val state = states(topicPartition)\n+        if (state.done) {\n+          if (state.currentReplicas.equals(state.targetReplicas)) {\n+            bld.append(\"Reassignment of partition %s is complete.\".\n+              format(topicPartition.toString))\n+          } else {\n+            bld.append(s\"There is no active reassignment of partition ${topicPartition}, \" +\n+              s\"but replica set is ${state.currentReplicas.mkString(\",\")} rather than \" +\n+              s\"${state.targetReplicas.mkString(\",\")}.\")\n+          }\n+        } else {\n+          bld.append(\"Reassignment of partition %s is still in progress.\".format(topicPartition))\n         }\n       }\n+    }\n+    bld.mkString(System.lineSeparator())\n+  }\n \n-      //Remove the list of throttled replicas from all topics with partitions being moved\n-      val topics = (reassignedPartitionsStatus.keySet.map(tp => tp.topic) ++ replicasReassignmentStatus.keySet.map(replica => replica.topic)).toSeq.distinct\n-      for (topic <- topics) {\n-        val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n-        // bitwise OR as we don't want to short-circuit\n-        if (configs.remove(LogConfig.LeaderReplicationThrottledReplicasProp) != null\n-          | configs.remove(LogConfig.FollowerReplicationThrottledReplicasProp) != null) {\n-          adminZkClient.changeTopicConfig(topic, configs)\n-          changed = true\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient          The Admin client to use.\n+   * @param targetReassignments  The reassignments we want to learn about.\n+   *\n+   * @return                     A tuple containing the reassignment states for each topic\n+   *                             partition, plus whether there are any ongoing reassignments.\n+   */\n+  def findPartitionReassignmentStates(adminClient: Admin,\n+                                      targetReassignments: Seq[(TopicPartition, Seq[Int])])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val (foundReassignments, notFoundReassignments) = targetReassignments.partition {\n+      case (topicPartition, _) => currentReassignments.contains(topicPartition)\n+    }\n+    val foundResults: Seq[(TopicPartition, PartitionReassignmentState)] = foundReassignments.map {\n+      case (topicPartition, targetReplicas) => (topicPartition,\n+        new PartitionReassignmentState(\n+          currentReassignments.get(topicPartition).get.replicas().\n+            asScala.map(i => i.asInstanceOf[Int]),\n+          targetReplicas,\n+          false))\n+    }\n+    val topicNamesToLookUp = new mutable.HashSet[String]()\n+    notFoundReassignments.foreach {\n+      case (topicPartition, targetReplicas) =>\n+        if (!currentReassignments.contains(topicPartition))\n+          topicNamesToLookUp.add(topicPartition.topic())\n+    }\n+    val topicDescriptions = adminClient.\n+      describeTopics(topicNamesToLookUp.asJava).values().asScala\n+    val notFoundResults: Seq[(TopicPartition, PartitionReassignmentState)] = notFoundReassignments.map {\n+      case (topicPartition, targetReplicas) =>\n+        currentReassignments.get(topicPartition) match {\n+          case Some(reassignment) => (topicPartition,\n+            new PartitionReassignmentState(\n+              reassignment.replicas().asScala.map(_.asInstanceOf[Int]),\n+              targetReplicas,\n+              false))\n+          case None => {\n+            try {\n+              val topicDescription = topicDescriptions.get(topicPartition.topic()).get.get()\n+              if (topicDescription.partitions().size() < topicPartition.partition())\n+                throw new ExecutionException(\"Too few partitions found\", new UnknownTopicOrPartitionException())\n+              (topicPartition,\n+                new PartitionReassignmentState(\n+                  topicDescription.partitions().get(topicPartition.partition()).replicas().asScala.map(_.id),\n+                  targetReplicas,\n+                  true))\n+            } catch {\n+              case t: ExecutionException =>\n+                t.getCause match {\n+                  case _: UnknownTopicOrPartitionException => (topicPartition,\n+                    new PartitionReassignmentState(\n+                      Seq(),\n+                      targetReplicas,\n+                      true))\n+                }\n+            }\n+          }\n         }\n+    }\n+    val allResults = foundResults ++ notFoundResults\n+    (allResults.toMap, !currentReassignments.isEmpty)\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param targetReassignments   The reassignments we want to learn about.\n+   *\n+   * @return                      A tuple containing the reassignment states for each topic\n+   *                              partition, plus whether there are any ongoing reassignments\n+   *                              found in the legacy reassign partitions znode.\n+   */\n+  def findPartitionReassignmentStates(zkClient: KafkaZkClient,\n+                                      targetReassignments: Map[TopicPartition, Seq[Int]])\n+                                      : (Map[TopicPartition, PartitionReassignmentState], Boolean) = {\n+    val partitionsBeingReassigned = zkClient.getPartitionReassignment\n+    val results = new mutable.HashMap[TopicPartition, PartitionReassignmentState]()\n+    targetReassignments.groupBy(_._1.topic).foreach {\n+      case (topic, partitions) =>\n+        val replicasForTopic = zkClient.getReplicaAssignmentForTopics(Set(topic))\n+        partitions.foreach {\n+          case (partition, targetReplicas) =>\n+            val currentReplicas = replicasForTopic.getOrElse(partition, Seq())\n+            results.put(partition, new PartitionReassignmentState(\n+              currentReplicas, targetReplicas, !partitionsBeingReassigned.contains(partition)))\n+        }\n+    }\n+    (results, !partitionsBeingReassigned.isEmpty)\n+  }\n+\n+  /**\n+   * Verify the replica reassignments specified by the user.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetReassignments   The replica reassignments specified by the user.\n+   *\n+   * @return                      A tuple of the replica states, and a boolean which is true\n+   *                              if there are any ongoing replica moves.\n+   *\n+   *                              Note: Unlike in verifyPartitionAssignments, we will\n+   *                              return false here even if there are unrelated ongoing\n+   *                              reassignments. (We don't have an efficient API that\n+   *                              returns all ongoing replica reassignments.)\n+   */\n+  def verifyReplicaMoves(adminClient: Admin,\n+                         targetReassignments: Map[TopicPartitionReplica, String])\n+                         : (Map[TopicPartitionReplica, LogDirMoveState], Boolean) = {\n+    val moveStates = findLogDirMoveStates(adminClient, targetReassignments)\n+    println(replicaMoveStatesToString(moveStates))\n+    (moveStates, !moveStates.values.forall(_.done))\n+  }\n+\n+  /**\n+   * Find the state of the specified partition reassignments.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param targetMoves           The movements we want to learn about.  The map is keyed\n+   *                              by TopicPartitionReplica, and its values are target log\n+   *                              directories.\n+   *\n+   * @return                      The states for each replica movement.\n+   */\n+  def findLogDirMoveStates(adminClient: Admin,\n+                           targetMoves: Map[TopicPartitionReplica, String])\n+                           : Map[TopicPartitionReplica, LogDirMoveState] = {\n+    val replicaLogDirInfos = adminClient.describeReplicaLogDirs(\n+      targetMoves.keySet.asJava).all().get().asScala\n+    targetMoves.map { case (replica, targetLogDir) =>\n+      val moveState: LogDirMoveState = replicaLogDirInfos.get(replica) match {\n+        case None => MissingReplicaMoveState(targetLogDir)\n+        case Some(info) => if (info.getCurrentReplicaLogDir == null) {\n+            MissingLogDirMoveState(targetLogDir)\n+          } else if (info.getFutureReplicaLogDir == null) {\n+            if (info.getCurrentReplicaLogDir.equals(targetLogDir)) {\n+              CompletedMoveState(targetLogDir)\n+            } else {\n+              CancelledMoveState(info.getCurrentReplicaLogDir, targetLogDir)\n+            }\n+          } else {\n+            ActiveMoveState(info.getCurrentReplicaLogDir(),\n+              targetLogDir,\n+              info.getFutureReplicaLogDir)\n+          }\n       }\n-      if (changed)\n-        println(\"Throttle was removed.\")\n+      (replica, moveState)\n     }\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, opts: ReassignPartitionsCommandOptions): Unit = {\n-    val topicsToMoveJsonFile = opts.options.valueOf(opts.topicsToMoveJsonFileOpt)\n-    val brokerListToReassign = opts.options.valueOf(opts.brokerListOpt).split(',').map(_.toInt)\n-    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n-    if (duplicateReassignments.nonEmpty)\n-      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".format(duplicateReassignments.mkString(\",\")))\n-    val topicsToMoveJsonString = Utils.readFileAsString(topicsToMoveJsonFile)\n-    val disableRackAware = opts.options.has(opts.disableRackAware)\n-    val (proposedAssignments, currentAssignments) = generateAssignment(zkClient, brokerListToReassign, topicsToMoveJsonString, disableRackAware)\n-    println(\"Current partition replica assignment\\n%s\\n\".format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n-    println(\"Proposed partition reassignment configuration\\n%s\".format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+  /**\n+   * Convert replica move states to a human-readable string.\n+   *\n+   * @param states          A map from topic partition replicas to states.\n+   * @return                A tuple of a summary string, and a boolean describing\n+   *                        whether there are any active replica moves.\n+   */\n+  def replicaMoveStatesToString(states: Map[TopicPartitionReplica, LogDirMoveState])\n+                                : String = {\n+    val bld = new mutable.ArrayBuffer[String]\n+    states.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).foreach {\n+      case replica =>\n+        val state = states(replica)\n+        state match {\n+          case MissingLogDirMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} is not found \" +\n+              s\"in any live log dir on broker ${replica.brokerId()}. There is likely an \" +\n+              s\"offline log directory on the broker.\")\n+          case MissingReplicaMoveState(targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} cannot be found \" +\n+              s\"in any live log directory on broker ${replica.brokerId()}.\")\n+          case ActiveMoveState(currentLogDir, targetLogDir, futureLogDir) =>\n+            if (targetLogDir.equals(futureLogDir)) {\n+              bld.append(s\"Reassignment of replica ${replica} is still in progress.\")\n+            } else {\n+              bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+                s\"${replica.brokerId()} is being moved to log dir ${futureLogDir} \" +\n+                s\"instead of ${targetLogDir}.\")\n+            }\n+          case CancelledMoveState(currentLogDir, targetLogDir) =>\n+            bld.append(s\"Partition ${replica.topic()}-${replica.partition()} on broker \" +\n+              s\"${replica.brokerId()} is not being moved from log dir ${currentLogDir} to \" +\n+              s\"${targetLogDir}.\")\n+          case CompletedMoveState(targetLogDir) =>\n+            bld.append(s\"Reassignment of replica ${replica} completed successfully.\")\n+        }\n+    }\n+    bld.mkString(System.lineSeparator())\n   }\n \n-  def generateAssignment(zkClient: KafkaZkClient, brokerListToReassign: Seq[Int], topicsToMoveJsonString: String, disableRackAware: Boolean): (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n-    val topicsToReassign = parseTopicsData(topicsToMoveJsonString)\n-    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n-    if (duplicateTopicsToReassign.nonEmpty)\n-      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".format(duplicateTopicsToReassign.mkString(\",\")))\n-    val currentAssignment = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+  /**\n+   * Clear all topic-level and broker-level throttles.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearAllThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    println(\"Clearing broker-level throttles....\")\n+    clearBrokerLevelThrottles(adminClient)\n+    println(\"Clearing topic-level throttles....\")\n+    clearTopicLevelThrottles(adminClient, topics)\n+  }\n \n-    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n-    val rackAwareMode = if (disableRackAware) RackAwareMode.Disabled else RackAwareMode.Enforced\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   */\n+  def clearBrokerLevelThrottles(adminClient: Admin): Unit = {\n+    val allBrokerIds = adminClient.describeCluster().nodes().get().asScala.map(_.id())\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    allBrokerIds.foreach {\n+      case brokerId => configOps.put(\n+        new ConfigResource(ConfigResource.Type.BROKER, brokerId.toString),\n+        brokerLevelThrottles.map(throttle => new AlterConfigOp(\n+          new ConfigEntry(throttle, null), OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear all throttles which have been set at the broker level.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   */\n+  def clearBrokerLevelThrottles(zkClient: KafkaZkClient): Unit = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val brokerMetadatas = adminZkClient.getBrokerMetadatas(rackAwareMode, Some(brokerListToReassign))\n+    for (brokerId <- zkClient.getAllBrokersInCluster.map(_.id)) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Broker, brokerId.toString)\n+      if (!brokerLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeBrokerConfig(Seq(brokerId), configs)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(adminClient: Admin, topics: Set[String]): Unit = {\n+    val configOps = new util.HashMap[ConfigResource, util.Collection[AlterConfigOp]]()\n+    topics.foreach {\n+      topicName => configOps.put(\n+        new ConfigResource(ConfigResource.Type.TOPIC, topicName),\n+        topicLevelThrottles.map(throttle => new AlterConfigOp(new ConfigEntry(throttle, null),\n+          OpType.DELETE)).asJava)\n+    }\n+    adminClient.incrementalAlterConfigs(configOps).all().get()\n+  }\n+\n+  /**\n+   * Clear the reassignment throttles for the specified topics.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param topics                The topics to clear the throttles for.\n+   */\n+  def clearTopicLevelThrottles(zkClient: KafkaZkClient, topics: Set[String]): Unit = {\n+    val adminZkClient = new AdminZkClient(zkClient)\n+    for (topic <- topics) {\n+      val configs = adminZkClient.fetchEntityConfig(ConfigType.Topic, topic)\n+      if (!topicLevelThrottles.flatMap(throttle => Option(configs.remove(throttle))).isEmpty) {\n+        adminZkClient.changeTopicConfig(topic, configs)\n+      }\n+    }\n+  }\n \n-    val partitionsToBeReassigned = mutable.Map[TopicPartition, Seq[Int]]()\n+  /**\n+   * The entry point for the --generate command.\n+   *\n+   * @param adminClient           The AdminClient to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(adminClient: Admin,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = getReplicaAssignmentForTopics(adminClient, topicsToReassign)\n+    val brokerMetadatas = getBrokerMetadata(adminClient, brokersToReassign, enableRackAwareness)\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * The legacy entry point for the --generate command.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param reassignmentJson      The JSON string to use for the topics to reassign.\n+   * @param brokerListString      The comma-separated string of broker IDs to use.\n+   * @param enableRackAwareness   True if rack-awareness should be enabled.\n+   *\n+   * @return                      A tuple containing the proposed assignment and the\n+   *                              current assignment.\n+   */\n+  def generateAssignment(zkClient: KafkaZkClient,\n+                         reassignmentJson: String,\n+                         brokerListString: String,\n+                         enableRackAwareness: Boolean)\n+                         : (Map[TopicPartition, Seq[Int]], Map[TopicPartition, Seq[Int]]) = {\n+    val (brokersToReassign, topicsToReassign) =\n+      parseGenerateAssignmentArgs(reassignmentJson, brokerListString)\n+    val currentAssignments = zkClient.getReplicaAssignmentForTopics(topicsToReassign.toSet)\n+    val brokerMetadatas = getBrokerMetadata(zkClient, brokersToReassign, enableRackAwareness)\n+    val proposedAssignments = calculateAssignment(currentAssignments, brokerMetadatas)\n+    println(\"Current partition replica assignment\\n%s\\n\".\n+      format(formatAsReassignmentJson(currentAssignments, Map.empty)))\n+    println(\"Proposed partition reassignment configuration\\n%s\".\n+      format(formatAsReassignmentJson(proposedAssignments, Map.empty)))\n+    (proposedAssignments, currentAssignments)\n+  }\n+\n+  /**\n+   * Calculate the new partition assignments to suggest in --generate.\n+   *\n+   * @param currentAssignment  The current partition assignments.\n+   * @param brokerMetadatas    The rack information for each broker.\n+   *\n+   * @return                   A map from partitions to the proposed assignments for each.\n+   */\n+  def calculateAssignment(currentAssignment: Map[TopicPartition, Seq[Int]],\n+                          brokerMetadatas: Seq[BrokerMetadata])\n+                          : Map[TopicPartition, Seq[Int]] = {\n+    val groupedByTopic = currentAssignment.groupBy { case (tp, _) => tp.topic }\n+    val proposedAssignments = mutable.Map[TopicPartition, Seq[Int]]()\n     groupedByTopic.foreach { case (topic, assignment) =>\n       val (_, replicas) = assignment.head\n-      val assignedReplicas = AdminUtils.assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n-      partitionsToBeReassigned ++= assignedReplicas.map { case (partition, replicas) =>\n+      val assignedReplicas = AdminUtils.\n+        assignReplicasToBrokers(brokerMetadatas, assignment.size, replicas.size)\n+      proposedAssignments ++= assignedReplicas.map { case (partition, replicas) =>\n         new TopicPartition(topic, partition) -> replicas\n       }\n     }\n-    (partitionsToBeReassigned, currentAssignment)\n+    proposedAssignments\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], opts: ReassignPartitionsCommandOptions): Unit = {\n-    val reassignmentJsonFile =  opts.options.valueOf(opts.reassignmentJsonFileOpt)\n-    val reassignmentJsonString = Utils.readFileAsString(reassignmentJsonFile)\n-    val interBrokerThrottle = opts.options.valueOf(opts.interBrokerThrottleOpt)\n-    val replicaAlterLogDirsThrottle = opts.options.valueOf(opts.replicaAlterLogDirsThrottleOpt)\n-    val timeoutMs = opts.options.valueOf(opts.timeoutOpt)\n-    executeAssignment(zkClient, adminClientOpt, reassignmentJsonString, Throttle(interBrokerThrottle, replicaAlterLogDirsThrottle), timeoutMs)\n+  /**\n+   * Get the current replica assignments for some topics.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param topics          The topics to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForTopics(adminClient: Admin,\n+                                    topics: Seq[String])\n+                                    : Map[TopicPartition, Seq[Int]] = {\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.map {\n+          info => (new TopicPartition(topicName, info.partition()),\n+            info.replicas().asScala.map(_.id()))\n+        }\n+      }\n+    }\n   }\n \n-  def executeAssignment(zkClient: KafkaZkClient, adminClientOpt: Option[Admin], reassignmentJsonString: String, throttle: Throttle, timeoutMs: Long = 10000L): Unit = {\n-    val (partitionAssignment, replicaAssignment) = parseAndValidate(zkClient, reassignmentJsonString)\n+  /**\n+   * Get the current replica assignments for some partitions.\n+   *\n+   * @param adminClient     The AdminClient to use.\n+   * @param partitions      The partitions to get information about.\n+   * @return                A map from partitions to broker assignments.\n+   *                        If any topic can't be found, an exception will be thrown.\n+   */\n+  def getReplicaAssignmentForPartitions(adminClient: Admin,\n+                                        partitions: Set[TopicPartition])\n+                                        : Map[TopicPartition, Seq[Int]] = {\n+    val topics = new mutable.HashSet[String]()\n+    partitions.foreach(topics += _.topic)\n+    adminClient.describeTopics(topics.asJava).all().get().asScala.flatMap {\n+      case (topicName, topicDescription) => {\n+        topicDescription.partitions().asScala.flatMap {\n+          info => if (partitions.contains(new TopicPartition(topicName, info.partition()))) {\n+            Some(new TopicPartition(topicName, info.partition()),\n+                info.replicas().asScala.map(_.id()))\n+          } else {\n+            None\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Find the rack information for some brokers.\n+   *\n+   * @param adminClient         The AdminClient object.\n+   * @param brokers             The brokers to gather metadata about.\n+   * @param enableRackAwareness True if we should return rack information, and throw an\n+   *                            exception if it is inconsistent.\n+   *\n+   * @return                    The metadata for each broker that was found.\n+   *                            Brokers that were not found will be omitted.\n+   */\n+  def getBrokerMetadata(adminClient: Admin,\n+                        brokers: Seq[Int],\n+                        enableRackAwareness: Boolean): Seq[BrokerMetadata] = {\n+    val brokerSet = brokers.toSet\n+    val results = adminClient.describeCluster().nodes().get().asScala.flatMap(\n+      node => if (!brokerSet.contains(node.id())) {\n+        None\n+      } else {\n+        if (enableRackAwareness && node.rack() != null) {\n+          Some(new BrokerMetadata(node.id(), Some(node.rack())))\n+        } else {\n+          Some(new BrokerMetadata(node.id(), None))\n+        }\n+      }\n+    ).toSeq\n+    val numRackless = results.count(_.rack.isEmpty)\n+    if (enableRackAwareness && numRackless != 0 && numRackless != results.size) {\n+      throw new AdminOperationException(\"Not all brokers have rack information. Add \" +\n+        \"--disable-rack-aware in command line to make replica assignment without rack \" +\n+        \"information.\")\n+    }\n+    results\n+  }\n+\n+  /**\n+   * Find the metadata for some brokers.\n+   *\n+   * @param zkClient              The ZooKeeper client to use.\n+   * @param brokers               The brokers to gather metadata about.\n+   * @param enableRackAwareness   True if we should return rack information, and throw an\n+   *                              exception if it is inconsistent.\n+   *\n+   * @return                      The metadata for each broker that was found.\n+   *                              Brokers that were not found will be omitted.\n+   */\n+  def getBrokerMetadata(zkClient: KafkaZkClient,\n+                        brokers: Seq[Int],\n+                        enableRackAwareness: Boolean): Seq[BrokerMetadata] = {\n     val adminZkClient = new AdminZkClient(zkClient)\n-    val reassignPartitionsCommand = new ReassignPartitionsCommand(zkClient, adminClientOpt, partitionAssignment.toMap, replicaAssignment, adminZkClient)\n+    adminZkClient.getBrokerMetadatas(if (enableRackAwareness)\n+      RackAwareMode.Enforced else RackAwareMode.Disabled, Some(brokers))\n+  }\n+\n+  /**\n+   * Parse and validate data gathered from the command-line for --generate\n+   * In particular, we parse the JSON and validate that duplicate brokers and\n+   * topics don't appear.\n+   *\n+   * @param reassignmentJson       The JSON passed to --generate .\n+   * @param brokerList             A list of brokers passed to --generate.\n+   *\n+   * @return                       A tuple of brokers to reassign, topics to reassign\n+   */\n+  def parseGenerateAssignmentArgs(reassignmentJson: String,\n+                                  brokerList: String): (Seq[Int], Seq[String]) = {\n+    val brokerListToReassign = brokerList.split(',').map(_.toInt)\n+    val duplicateReassignments = CoreUtils.duplicates(brokerListToReassign)\n+    if (duplicateReassignments.nonEmpty)\n+      throw new AdminCommandFailedException(\"Broker list contains duplicate entries: %s\".\n+        format(duplicateReassignments.mkString(\",\")))\n+    val topicsToReassign = parseTopicsData(reassignmentJson)\n+    val duplicateTopicsToReassign = CoreUtils.duplicates(topicsToReassign)\n+    if (duplicateTopicsToReassign.nonEmpty)\n+      throw new AdminCommandFailedException(\"List of topics to reassign contains duplicate entries: %s\".\n+        format(duplicateTopicsToReassign.mkString(\",\")))\n+    (brokerListToReassign, topicsToReassign)\n+  }\n \n-    // If there is an existing rebalance running, attempt to change its throttle\n-    if (zkClient.reassignPartitionsInProgress()) {\n-      println(\"There is an existing assignment running.\")\n-      reassignPartitionsCommand.maybeLimit(throttle)\n+  /**\n+   * The entry point for the --execute and --execute-additional commands.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param additional                  Whether --additional was passed.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative\n+   *                                    number to skip using a throttle.\n+   * @param logDirThrottle              The replica log directory throttle to use, or a\n+   *                                    negative number to skip using a throttle.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeAssignment(adminClient: Admin,\n+                        additional: Boolean,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long = -1L,\n+                        logDirThrottle: Long = -1L,\n+                        timeoutMs: Long = 10000L,\n+                        time: Time = Time.SYSTEM): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    // If there is an existing assignment, check for --additional before proceeding.\n+    // This helps avoid surprising users.\n+    if (!additional && !currentReassignments.isEmpty) {\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n+    }\n+    verifyBrokerIds(adminClient, proposedParts.values.flatten.toSet)\n+    val currentParts = getReplicaAssignmentForPartitions(adminClient, proposedParts.keySet.toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+    if (interBrokerThrottle >= 0 || logDirThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(currentReassignments, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(adminClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      val movingBrokers = calculateMovingBrokers(proposedReplicas.keySet.toSet)\n+      modifyBrokerThrottles(adminClient,\n+        reassigningBrokers, interBrokerThrottle,\n+        movingBrokers, logDirThrottle)\n+      if (interBrokerThrottle >= 0) {\n+        println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+      }\n+      if (logDirThrottle >= 0) {\n+        println(s\"The replica-alter-dir throttle limit was set to ${logDirThrottle} B/s\")\n+      }\n+    }\n+\n+    // Execute the partition reassignments.\n+    val errors = alterPartitionReassignments(adminClient, proposedParts)\n+    if (!errors.isEmpty) {\n+      throw new TerseReassignmentFailureException(\n+        \"Error reassigning partition(s):%n%s\".format(\n+          errors.keySet.toBuffer.sortWith(compareTopicPartitions).map {\n+            case part => s\"${part}: ${errors(part).getMessage}\"\n+          }.mkString(System.lineSeparator())))\n+    }\n+    println(\"Successfully started partition reassignment%s for %s\".format(\n+      if (proposedParts.size == 1) \"\" else \"s\",\n+      proposedParts.keySet.toBuffer.sortWith(compareTopicPartitions).mkString(\",\")))\n+    if (!proposedReplicas.isEmpty) {\n+      executeMoves(adminClient, proposedReplicas, timeoutMs, time)\n+    }\n+  }\n+\n+  /**\n+   * Execute some partition log directory movements.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param proposedReplicas            A map from TopicPartitionReplicas to the\n+   *                                    directories to move them to.\n+   * @param timeoutMs                   The maximum time in ms to wait for log directory\n+   *                                    replica assignment to begin.\n+   * @param time                        The Time object to use.\n+   */\n+  def executeMoves(adminClient: Admin,\n+                   proposedReplicas: Map[TopicPartitionReplica, String],\n+                   timeoutMs: Long,\n+                   time: Time): Unit = {\n+    val startTimeMs = time.milliseconds()\n+    val pendingReplicas = new mutable.HashMap[TopicPartitionReplica, String]()\n+    pendingReplicas ++= proposedReplicas\n+    var done = false\n+    do {\n+      val completed = alterReplicaLogDirs(adminClient, pendingReplicas)\n+      if (!completed.isEmpty) {\n+        println(\"Successfully started log directory move%s for: %s\".format(\n+          if (completed.size == 1) \"\" else \"s\",\n+          completed.toBuffer.sortWith(compareTopicPartitionReplicas).mkString(\",\")))\n+      }\n+      pendingReplicas --= completed\n+      if (pendingReplicas.isEmpty) {\n+        done = true\n+      } else if (time.milliseconds() >= startTimeMs + timeoutMs) {\n+        throw new TerseReassignmentFailureException(\n+          \"Timed out before log directory move%s could be started for: %s\".format(\n+            if (pendingReplicas.size == 1) \"\" else \"s\",\n+            pendingReplicas.keySet.toBuffer.sortWith(compareTopicPartitionReplicas).\n+              mkString(\",\")))\n+      } else {\n+        // If a replica has been moved to a new host and we also specified a particular\n+        // log directory, we will have to keep retrying the alterReplicaLogDirs\n+        // call.  It can't take effect until the replica is moved to that host.\n+        time.sleep(100)\n+      }\n+    } while (!done)\n+  }\n+\n+  /**\n+   * Entry point for the --list command.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   */\n+  def listReassignments(adminClient: Admin): Unit = {\n+    println(curReassignmentsToString(adminClient))\n+  }\n+\n+  /**\n+   * Convert the current partition reassignments to text.\n+   *\n+   * @param adminClient   The AdminClient to use.\n+   * @return              A string describing the current partition reassignments.\n+   */\n+  def curReassignmentsToString(adminClient: Admin): String = {\n+    val currentReassignments = adminClient.\n+      listPartitionReassignments().reassignments().get().asScala\n+    val text = currentReassignments.keySet.toBuffer.sortWith(compareTopicPartitions).map {\n+      case part =>\n+        val reassignment = currentReassignments(part)\n+        val replicas = reassignment.replicas().asScala\n+        val addingReplicas = reassignment.addingReplicas().asScala\n+        val removingReplicas = reassignment.removingReplicas().asScala\n+        \"%s: replicas: %s.%s%s\".format(part, replicas.mkString(\",\"),\n+          if (addingReplicas.isEmpty) \"\" else\n+            \" adding: %s.\".format(addingReplicas.mkString(\",\")),\n+          if (removingReplicas.isEmpty) \"\" else\n+            \" removing: %s.\".format(removingReplicas.mkString(\",\")))\n+    }.mkString(System.lineSeparator())\n+    if (text.isEmpty) {\n+      \"No partition reassignments found.\"\n     } else {\n-      printCurrentAssignment(zkClient, partitionAssignment.map(_._1.topic))\n-      if (throttle.interBrokerLimit >= 0 || throttle.replicaAlterLogDirsLimit >= 0)\n-        println(String.format(\"Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.\"))\n-      if (reassignPartitionsCommand.reassignPartitions(throttle, timeoutMs)) {\n-        println(\"Successfully started reassignment of partitions.\")\n-      } else\n-        println(\"Failed to reassign partitions %s\".format(partitionAssignment))\n+      \"Current partition reassignments:%n%s\".format(text)\n+    }\n+  }\n+\n+  /**\n+   * Verify that all the brokers in an assignment exist.\n+   *\n+   * @param adminClient                 The AdminClient to use.\n+   * @param brokers                     The broker IDs to verify.\n+   */\n+  def verifyBrokerIds(adminClient: Admin, brokers: Set[Int]): Unit = {\n+    val allNodeIds = adminClient.describeCluster().nodes().get().asScala.map(_.id).toSet\n+    brokers.find(!allNodeIds.contains(_)).map {\n+      case id => throw new AdminCommandFailedException(s\"Unknown broker id ${id}\")\n+    }\n+  }\n+\n+  /**\n+   * The entry point for the --execute command.\n+   *\n+   * @param zkClient                    The ZooKeeper client to use.\n+   * @param reassignmentJson            The JSON string to use for the topics to reassign.\n+   * @param interBrokerThrottle         The inter-broker throttle to use, or a negative number\n+   *                                    to skip using a throttle.\n+   */\n+  def executeAssignment(zkClient: KafkaZkClient,\n+                        reassignmentJson: String,\n+                        interBrokerThrottle: Long): Unit = {\n+    val (proposedParts, proposedReplicas) = parseExecuteAssignmentArgs(reassignmentJson)\n+    if (!proposedReplicas.isEmpty) {\n+      throw new AdminCommandFailedException(\"bootstrap-server needs to be provided when \" +\n+        \"replica reassignments are present.\")\n+    }\n+    verifyReplicasAndBrokersInAssignment(zkClient, proposedParts)\n+\n+    // Check for the presence of the legacy partition reassignment ZNode.  This actually\n+    // won't detect all rebalances... only ones initiated by the legacy method.\n+    // This is a limitation of the legacy ZK API.\n+    val reassignPartitionsInProgress = zkClient.reassignPartitionsInProgress()\n+    if (reassignPartitionsInProgress) {\n+      // Note: older versions of this tool would modify the broker quotas here (but not\n+      // topic quotas, for some reason).  This behavior wasn't documented in the --execute\n+      // command line help.  Since it might interfere with other ongoing reassignments,\n+      // this behavior was dropped as part of the KIP-455 changes.\n+      throw new TerseReassignmentFailureException(cannotExecuteBecauseOfExistingMessage)\n     }\n+    val currentParts = zkClient.getReplicaAssignmentForTopics(\n+      proposedParts.map(_._1.topic()).toSet)\n+    println(currentPartitionReplicaAssignmentToString(proposedParts, currentParts))\n+\n+    if (interBrokerThrottle >= 0) {\n+      println(youMustRunVerifyPeriodicallyMessage)\n+      val moveMap = calculateMoveMap(Map.empty, proposedParts, currentParts)\n+      val leaderThrottles = calculateLeaderThrottles(moveMap)\n+      val followerThrottles = calculateFollowerThrottles(moveMap)\n+      modifyTopicThrottles(zkClient, leaderThrottles, followerThrottles)\n+      val reassigningBrokers = calculateReassigningBrokers(moveMap)\n+      modifyBrokerThrottles(zkClient, reassigningBrokers, interBrokerThrottle)\n+      println(s\"The inter-broker throttle limit was set to ${interBrokerThrottle} B/s\")\n+    }\n+    zkClient.createPartitionReassignment(proposedParts)\n+    println(\"Successfully started partition reassignment%s for %s\".format(\n+      if (proposedParts.size == 1) \"\" else \"s\",\n+      proposedParts.keySet.toBuffer.sortWith(compareTopicPartitions).mkString(\",\")))\n   }\n \n-  def printCurrentAssignment(zkClient: KafkaZkClient, topics: Seq[String]): Unit = {\n-    // before starting assignment, output the current replica assignment to facilitate rollback\n-    val currentPartitionReplicaAssignment = zkClient.getReplicaAssignmentForTopics(topics.toSet)\n-    println(\"Current partition replica assignment\\n\\n%s\\n\\nSave this to use as the --reassignment-json-file option during rollback\"\n-      .format(formatAsReassignmentJson(currentPartitionReplicaAssignment, Map.empty)))\n+  /**\n+   * Return the string which we want to print to describe the current partition assignment.\n+   *\n+   * @param proposedParts               The proposed partition assignment.\n+   * @param currentParts                The current partition assignment.\n+   *\n+   * @return                            The string to print.  We will only print information about\n+   *                                    partitions that appear in the proposed partition assignment.\n+   */\n+  def currentPartitionReplicaAssignmentToString(proposedParts: Map[TopicPartition, Seq[Int]],\n+                                                currentParts: Map[TopicPartition, Seq[Int]]): String = {\n+    \"Current partition replica assignment%n%n%s%n%nSave this to use as the %s\".\n+        format(formatAsReassignmentJson(currentParts.filterKeys(proposedParts.contains(_)).toMap, Map.empty),\n+              \"--reassignment-json-file option during rollback\")\n+  }\n+\n+  /**\n+   * Verify that the replicas and brokers referenced in the given partition assignment actually\n+   * exist.  This is necessary when using the deprecated ZK API, since ZooKeeper itself can't\n+   * validate what we're applying.\n+   *\n+   * @param zkClient                    The ZooKeeper client to use.\n+   * @param proposedParts               The partition assignment.\n+   */\n+  def verifyReplicasAndBrokersInAssignment(zkClient: KafkaZkClient,\n+                                           proposedParts: Map[TopicPartition, Seq[Int]]): Unit = {\n+    // check that all partitions in the proposed assignment exist in the cluster\n+    val proposedTopics = proposedParts.map { case (tp, _) => tp.topic }\n+    val existingAssignment = zkClient.getReplicaAssignmentForTopics(proposedTopics.toSet)\n+    val nonExistentPartitions = proposedParts.map { case (tp, _) => tp }.filterNot(existingAssignment.contains)\n+    if (nonExistentPartitions.nonEmpty)\n+      throw new AdminCommandFailedException(\"The proposed assignment contains non-existent partitions: \" +\n+        nonExistentPartitions)\n+\n+    // check that all brokers in the proposed assignment exist in the cluster\n+    val existingBrokerIDs = zkClient.getSortedBrokerList\n+    val nonExistingBrokerIDs = proposedParts.toMap.values.flatten.filterNot(existingBrokerIDs.contains).toSet\n+    if (nonExistingBrokerIDs.nonEmpty)\n+      throw new AdminCommandFailedException(\"The proposed assignment contains non-existent brokerIDs: \" + nonExistingBrokerIDs.mkString(\",\"))\n+  }\n+\n+  /**\n+   * Execute the given partition reassignments.\n+   *\n+   * @param adminClient       The admin client object to use.\n+   * @param reassignments     A map from topic names to target replica assignments.\n+   * @return                  A map from partition objects to error strings.\n+   */\n+  def alterPartitionReassignments(adminClient: Admin,\n+                                  reassignments: Map[TopicPartition, Seq[Int]])\n+                                ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzMxNDE3NA==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r393314174", "bodyText": "Github's broken UI is showing this comment as being on line 1170 for me.  Since the comment doesn't make sense for that line, what line did you intend this for?  I checked the file for case Some without case None, but didn't find anything...", "author": "cmccabe", "createdAt": "2020-03-16T21:14:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzE1MjEyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDM3MjE4Ng==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r394372186", "bodyText": "After your latest changes, this is L1421 inside def modifyTopicThrottles.\nleaderThrottles.get(topicName) match {\n  case None =>\n  case Some(value) => ...\n}", "author": "mumrah", "createdAt": "2020-03-18T14:07:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzE1MjEyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDYwNTQwOQ==", "url": "https://github.com/apache/kafka/pull/8244#discussion_r394605409", "bodyText": "OK, then it's already been addressed.  Sounds good.", "author": "cmccabe", "createdAt": "2020-03-18T20:00:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzE1MjEyNQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "829a0e60ee0d119833156dc1a781c523a6d43232", "url": "https://github.com/apache/kafka/commit/829a0e60ee0d119833156dc1a781c523a6d43232", "message": "KAFKA-8820: kafka-reassign-partitions.sh should support the KIP-455 API\n\nRewrite ReassignPartitionsCommand to use the KIP-455 API when possible, rather\nthan direct communication with ZooKeeper.  Direct ZK access is still supported,\nbut deprecated, as described in KIP-455.\n\nAs specified in KIP-455, the tool has several new flags.  --cancel stops\nan assignment which is in progress.  --preserve-throttle causes the\n--verify and --cancel commands to leave the throttles alone.\n--additional allows users to execute another partition assignment even\nif there is already one in progress.  Finally, --show displays all of\nthe current partition reassignments.\n\nReorganize the reassignment code and tests somewhat to rely more on unit\ntesting using the MockAdminClient and less on integration testing.  Each\nintegration test where we bring up a cluster seems to take about 5 seconds, so\nit's good when we can get similar coverage from unit tests.  To enable this,\nMockAdminClient now supports incrementalAlterConfigs, alterReplicaLogDirs,\ndescribeReplicaLogDirs, and some other APIs.  MockAdminClient is also now\nthread-safe, to match the real AdminClient implementation.\n\nIn DeleteTopicTest, use the KIP-455 API rather than invoking the reassignment\ncommand.", "committedDate": "2020-03-20T01:23:31Z", "type": "commit"}, {"oid": "829a0e60ee0d119833156dc1a781c523a6d43232", "url": "https://github.com/apache/kafka/commit/829a0e60ee0d119833156dc1a781c523a6d43232", "message": "KAFKA-8820: kafka-reassign-partitions.sh should support the KIP-455 API\n\nRewrite ReassignPartitionsCommand to use the KIP-455 API when possible, rather\nthan direct communication with ZooKeeper.  Direct ZK access is still supported,\nbut deprecated, as described in KIP-455.\n\nAs specified in KIP-455, the tool has several new flags.  --cancel stops\nan assignment which is in progress.  --preserve-throttle causes the\n--verify and --cancel commands to leave the throttles alone.\n--additional allows users to execute another partition assignment even\nif there is already one in progress.  Finally, --show displays all of\nthe current partition reassignments.\n\nReorganize the reassignment code and tests somewhat to rely more on unit\ntesting using the MockAdminClient and less on integration testing.  Each\nintegration test where we bring up a cluster seems to take about 5 seconds, so\nit's good when we can get similar coverage from unit tests.  To enable this,\nMockAdminClient now supports incrementalAlterConfigs, alterReplicaLogDirs,\ndescribeReplicaLogDirs, and some other APIs.  MockAdminClient is also now\nthread-safe, to match the real AdminClient implementation.\n\nIn DeleteTopicTest, use the KIP-455 API rather than invoking the reassignment\ncommand.", "committedDate": "2020-03-20T01:23:31Z", "type": "forcePushed"}]}