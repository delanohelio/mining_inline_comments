{"pr_number": 9777, "pr_title": "KAFKA-7940: wait until we've got the expected partition size", "pr_createdAt": "2020-12-22T07:25:38Z", "pr_url": "https://github.com/apache/kafka/pull/9777", "timeline": [{"oid": "15990d7918e0e0e86a2ce5896d268a807d8c8898", "url": "https://github.com/apache/kafka/commit/15990d7918e0e0e86a2ce5896d268a807d8c8898", "message": "KAFKA-7940: wait until we've got the expected partition size before get partition leader", "committedDate": "2020-12-22T06:47:29Z", "type": "commit"}, {"oid": "ac9affb7a2a0400a3b1441cb1d4e927cccc373e0", "url": "https://github.com/apache/kafka/commit/ac9affb7a2a0400a3b1441cb1d4e927cccc373e0", "message": "KAFKA-7940: remove uecessary Class name", "committedDate": "2021-01-27T06:09:54Z", "type": "commit"}, {"oid": "f7b3f528ba538d4e70369409fe26dfae16760f3e", "url": "https://github.com/apache/kafka/commit/f7b3f528ba538d4e70369409fe26dfae16760f3e", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KAFKA-7940", "committedDate": "2021-01-27T06:14:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzExMzM4OQ==", "url": "https://github.com/apache/kafka/pull/9777#discussion_r567113389", "bodyText": "Can we use server.metadataCache?", "author": "hachikuji", "createdAt": "2021-01-29T21:51:22Z", "path": "core/src/test/scala/unit/kafka/utils/TestUtils.scala", "diffHunk": "@@ -885,6 +891,24 @@ object TestUtils extends Logging {\n     ), \"Timed out waiting for broker metadata to propagate to all servers\", timeout)\n   }\n \n+  /**\n+   * Wait until the expected number of partitions is in the metadata cache in each broker.\n+   *\n+   * @param servers The list of servers that the metadata should reach to\n+   * @param topic The topic name\n+   * @param expectedNumPartitions The expected number of partitions\n+   */\n+  def waitUntilMetadataIsPropagatedWithExpectedSize(servers: Seq[KafkaServer], topic: String, expectedNumPartitions: Int): Unit = {\n+    waitUntilTrue(\n+      () => servers.forall { server =>\n+        server.dataPlaneRequestProcessor.metadataCache.numPartitions(topic) match {", "originalCommit": "f7b3f528ba538d4e70369409fe26dfae16760f3e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzY1MDA2NQ==", "url": "https://github.com/apache/kafka/pull/9777#discussion_r567650065", "bodyText": "Good suggestion. Updated.", "author": "showuon", "createdAt": "2021-02-01T08:49:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzExMzM4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzExMzY0NQ==", "url": "https://github.com/apache/kafka/pull/9777#discussion_r567113645", "bodyText": "What do you think about more concise names?\nwaitUntilMetadataIsPropagatedWithExpectedSize -> waitForAllPartitionMetadata\nwaitUntilMetadataIsPropagated -> waitForPartitionMetadata\nI wonder if this would be more useful if we return the partition metadata: Map[TopicPartition, UpdateMetadataPartitionState. Then we could probably skip the calls to waitUntilMetadataIsPropagated and waitUntilLeaderIsElectedOrChanged above.", "author": "hachikuji", "createdAt": "2021-01-29T21:52:06Z", "path": "core/src/test/scala/unit/kafka/utils/TestUtils.scala", "diffHunk": "@@ -885,6 +891,24 @@ object TestUtils extends Logging {\n     ), \"Timed out waiting for broker metadata to propagate to all servers\", timeout)\n   }\n \n+  /**\n+   * Wait until the expected number of partitions is in the metadata cache in each broker.\n+   *\n+   * @param servers The list of servers that the metadata should reach to\n+   * @param topic The topic name\n+   * @param expectedNumPartitions The expected number of partitions\n+   */\n+  def waitUntilMetadataIsPropagatedWithExpectedSize(servers: Seq[KafkaServer], topic: String, expectedNumPartitions: Int): Unit = {", "originalCommit": "f7b3f528ba538d4e70369409fe26dfae16760f3e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzY1MDIyNQ==", "url": "https://github.com/apache/kafka/pull/9777#discussion_r567650225", "bodyText": "Good suggestion. Updated.", "author": "showuon", "createdAt": "2021-02-01T08:49:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzExMzY0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzExNzM5Nw==", "url": "https://github.com/apache/kafka/pull/9777#discussion_r567117397", "bodyText": "As mentioned in the other comment, this is probably good enough. The extra validations below seem like overkill.", "author": "hachikuji", "createdAt": "2021-01-29T22:00:51Z", "path": "core/src/test/scala/unit/kafka/utils/TestUtils.scala", "diffHunk": "@@ -349,10 +349,13 @@ object TestUtils extends Logging {\n       !hasSessionExpirationException},\n       s\"Can't create topic $topic\")\n \n+    // wait until we've got the expected partition size\n+    waitUntilMetadataIsPropagatedWithExpectedSize(servers, topic, numPartitions)", "originalCommit": "f7b3f528ba538d4e70369409fe26dfae16760f3e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "73d9cfc120cc92a25babb62eccd61ac73619dee9", "url": "https://github.com/apache/kafka/commit/73d9cfc120cc92a25babb62eccd61ac73619dee9", "message": "KAFKA-7940: remve unneeded wait and rename methods", "committedDate": "2021-02-01T08:54:37Z", "type": "commit"}, {"oid": "73d9cfc120cc92a25babb62eccd61ac73619dee9", "url": "https://github.com/apache/kafka/commit/73d9cfc120cc92a25babb62eccd61ac73619dee9", "message": "KAFKA-7940: remve unneeded wait and rename methods", "committedDate": "2021-02-01T08:54:37Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzgzNzM0Mw==", "url": "https://github.com/apache/kafka/pull/9777#discussion_r567837343", "bodyText": "Don't replace them with waitForAllPartitionsMetadata because I'm afraid it'll break the original testing purposes. And same as other places.", "author": "showuon", "createdAt": "2021-02-01T13:48:01Z", "path": "core/src/test/scala/unit/kafka/admin/AddPartitionsTest.scala", "diffHunk": "@@ -137,12 +137,12 @@ class AddPartitionsTest extends BaseRequestTest {\n     adminZkClient.addPartitions(topic3, topic3Assignment, adminZkClient.getBrokerMetadatas(), 7)\n \n     // read metadata from a broker and verify the new topic partitions exist\n-    TestUtils.waitUntilMetadataIsPropagated(servers, topic3, 1)\n-    TestUtils.waitUntilMetadataIsPropagated(servers, topic3, 2)\n-    TestUtils.waitUntilMetadataIsPropagated(servers, topic3, 3)\n-    TestUtils.waitUntilMetadataIsPropagated(servers, topic3, 4)\n-    TestUtils.waitUntilMetadataIsPropagated(servers, topic3, 5)\n-    TestUtils.waitUntilMetadataIsPropagated(servers, topic3, 6)\n+    TestUtils.waitForPartitionMetadata(servers, topic3, 1)\n+    TestUtils.waitForPartitionMetadata(servers, topic3, 2)\n+    TestUtils.waitForPartitionMetadata(servers, topic3, 3)\n+    TestUtils.waitForPartitionMetadata(servers, topic3, 4)\n+    TestUtils.waitForPartitionMetadata(servers, topic3, 5)\n+    TestUtils.waitForPartitionMetadata(servers, topic3, 6)", "originalCommit": "73d9cfc120cc92a25babb62eccd61ac73619dee9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}