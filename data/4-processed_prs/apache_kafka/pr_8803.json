{"pr_number": 8803, "pr_title": "KAFKA-10102: update ProcessorTopology instead of rebuilding it", "pr_createdAt": "2020-06-05T04:02:58Z", "pr_url": "https://github.com/apache/kafka/pull/8803", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTk4NDkzMA==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r435984930", "bodyText": "We do not need to pass in the topics here, just make the topic string creation logic inside ProcessorTopology since the data is stored there.", "author": "abbccdda", "createdAt": "2020-06-05T15:08:40Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/SourceNode.java", "diffHunk": "@@ -112,7 +106,7 @@ public String toString() {\n     /**\n      * @return a string representation of this node starting with the given indent, useful for debugging.\n      */\n-    public String toString(final String indent) {\n+    public String toString(final String indent, final List<String> topics) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTk5MDQ5Ng==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r435990496", "bodyText": "Why don't we just verify the name of processorTopology.source(\"topic-2\") to be source-1?", "author": "abbccdda", "createdAt": "2020-06-05T15:17:42Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorTopologyTest.java", "diffHunk": "@@ -149,6 +150,29 @@ public void shouldGetTerminalNodes() {\n         assertThat(processorTopology.terminalNodes(), equalTo(mkSet(\"processor-2\", \"sink-1\")));\n     }\n \n+    @Test\n+    public void shouldUpdateSourceTopicsWithNewMatchingTopic() {\n+        topology.addSource(\"source-1\", \"topic-1\");\n+        final ProcessorTopology processorTopology = topology.getInternalBuilder(\"X\").buildTopology();\n+\n+        assertNull(processorTopology.source(\"topic-2\"));\n+        processorTopology.updateSourceTopics(Collections.singletonMap(\"source-1\", asList(\"topic-1\", \"topic-2\")));\n+\n+        assertThat(processorTopology.source(\"topic-2\"), instanceOf(SourceNode.class));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTk5MzY2Mw==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r435993663", "bodyText": "For this update function, should we also look at the sourceNodesByName and make sure their keysets are matching, otherwise we would throw illegal state here? I think this is legitimate insurance before @cadonna starts the topology revolution work.", "author": "abbccdda", "createdAt": "2020-06-05T15:22:41Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorTopology.java", "diffHunk": "@@ -131,6 +139,16 @@ public boolean hasPersistentGlobalStore() {\n         return false;\n     }\n \n+    public void updateSourceTopics(final Map<String, List<String>> nodeToSourceTopics) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA5MzcxNA==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r436093714", "bodyText": "good call", "author": "ableegoldman", "createdAt": "2020-06-05T18:27:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTk5MzY2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTk5NTg2Mg==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r435995862", "bodyText": "Nice to simplify :)", "author": "abbccdda", "createdAt": "2020-06-05T15:26:18Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "diffHunk": "@@ -270,16 +270,7 @@ private SourceNodeFactory(final String name,\n \n         @Override\n         public ProcessorNode<K, V> build() {\n-            final List<String> sourceTopics = nodeToSourceTopics.get(name);\n-\n-            // if it is subscribed via patterns, it is possible that the topic metadata has not been updated\n-            // yet and hence the map from source node to topics is stale, in this case we put the pattern as a place holder;\n-            // this should only happen for debugging since during runtime this function should always be called after the metadata has updated.\n-            if (sourceTopics == null) {\n-                return new SourceNode<>(name, Collections.singletonList(String.valueOf(pattern)), timestampExtractor, keyDeserializer, valDeserializer);\n-            } else {\n-                return new SourceNode<>(name, maybeDecorateInternalSourceTopics(sourceTopics), timestampExtractor, keyDeserializer, valDeserializer);\n-            }\n+            return new SourceNode<>(name, timestampExtractor, keyDeserializer, valDeserializer);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTk5Nzc1OA==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r435997758", "bodyText": "nit: space", "author": "abbccdda", "createdAt": "2020-06-05T15:29:26Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorTopology.java", "diffHunk": "@@ -167,11 +185,21 @@ public String toString() {\n      * @return A string representation of this instance.\n      */\n     public String toString(final String indent) {\n+        final Map<SourceNode<?, ?>, List<String>> sourceToTopics = new HashMap<>();\n+        for (final Map.Entry<String, SourceNode<?, ?>> sourceNodeEntry : sourcesByTopic.entrySet()) {\n+            final String topic = sourceNodeEntry.getKey();\n+            final SourceNode<?, ?> source = sourceNodeEntry.getValue();\n+            sourceToTopics.computeIfAbsent(source, s -> new ArrayList<>());\n+            sourceToTopics.get(source).add(topic);\n+        }\n+\n         final StringBuilder sb = new StringBuilder(indent + \"ProcessorTopology:\\n\");\n \n         // start from sources\n-        for (final SourceNode<?, ?> source : sourcesByTopic.values()) {\n-            sb.append(source.toString(indent + \"\\t\")).append(childrenToString(indent + \"\\t\", source.children()));\n+        for (final Map.Entry<SourceNode<?, ?>, List<String>> sourceNodeEntry : sourceToTopics.entrySet()) {\n+            final SourceNode<?, ?> source  = sourceNodeEntry.getKey();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTk5ODYzNw==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r435998637", "bodyText": "Does the new map ensure a topological order?", "author": "abbccdda", "createdAt": "2020-06-05T15:30:53Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorTopology.java", "diffHunk": "@@ -167,11 +185,21 @@ public String toString() {\n      * @return A string representation of this instance.\n      */\n     public String toString(final String indent) {\n+        final Map<SourceNode<?, ?>, List<String>> sourceToTopics = new HashMap<>();\n+        for (final Map.Entry<String, SourceNode<?, ?>> sourceNodeEntry : sourcesByTopic.entrySet()) {\n+            final String topic = sourceNodeEntry.getKey();\n+            final SourceNode<?, ?> source = sourceNodeEntry.getValue();\n+            sourceToTopics.computeIfAbsent(source, s -> new ArrayList<>());\n+            sourceToTopics.get(source).add(topic);\n+        }\n+\n         final StringBuilder sb = new StringBuilder(indent + \"ProcessorTopology:\\n\");\n \n         // start from sources\n-        for (final SourceNode<?, ?> source : sourcesByTopic.values()) {\n-            sb.append(source.toString(indent + \"\\t\")).append(childrenToString(indent + \"\\t\", source.children()));\n+        for (final Map.Entry<SourceNode<?, ?>, List<String>> sourceNodeEntry : sourceToTopics.entrySet()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA5NjY2NQ==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r436096665", "bodyText": "Well, it's in topological order in that we start from source nodes and proceed through the children", "author": "ableegoldman", "createdAt": "2020-06-05T18:33:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTk5ODYzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjExNzg1OA==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r436117858", "bodyText": "Oh I see", "author": "abbccdda", "createdAt": "2020-06-05T19:19:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTk5ODYzNw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEyNTc5Mw==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r436125793", "bodyText": "I prefer we get a newbie ticket to test out this logic in a unit test case.", "author": "abbccdda", "createdAt": "2020-06-05T19:38:51Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorTopology.java", "diffHunk": "@@ -167,12 +195,36 @@ public String toString() {\n      * @return A string representation of this instance.\n      */\n     public String toString(final String indent) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3NzAzOA==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r436177038", "bodyText": "SG", "author": "ableegoldman", "createdAt": "2020-06-05T21:42:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEyNTc5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEyNzEzNQ==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r436127135", "bodyText": "Could we add a unit test in StreamTaskTest?", "author": "abbccdda", "createdAt": "2020-06-05T19:42:05Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -495,12 +496,9 @@ public void closeDirty() {\n     }\n \n     @Override\n-    public void update(final Set<TopicPartition> topicPartitions, final ProcessorTopology processorTopology) {\n-        super.update(topicPartitions, processorTopology);\n+    public void update(final Set<TopicPartition> topicPartitions, final Map<String, List<String>> nodeToSourceTopics) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE4NzkyNA==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r436187924", "bodyText": "Ack, done", "author": "ableegoldman", "createdAt": "2020-06-05T22:13:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEyNzEzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE4ODY5OQ==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r436188699", "bodyText": "Should we have a sanity check, that no topic is added twice?", "author": "mjsax", "createdAt": "2020-06-05T22:16:13Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorTopology.java", "diffHunk": "@@ -131,6 +142,23 @@ public boolean hasPersistentGlobalStore() {\n         return false;\n     }\n \n+    public void updateSourceTopics(final Map<String, List<String>> sourceTopicsByName) {\n+        if (!sourceTopicsByName.keySet().equals(sourceNodesByName.keySet())) {\n+            log.error(\"Set of source nodes do not match: \\n\" +\n+                \"sourceNodesByName = {}\\n\" +\n+                \"sourceTopicsByName = {}\",\n+                sourceNodesByName.keySet(), sourceTopicsByName.keySet());\n+            throw new IllegalStateException(\"Tried to update source topics but source nodes did not match\");\n+        }\n+        sourceNodesByTopic.clear();\n+        for (final Map.Entry<String, List<String>> sourceEntry : sourceTopicsByName.entrySet()) {\n+            final String nodeName = sourceEntry.getKey();\n+            for (final String topic : sourceEntry.getValue()) {\n+                sourceNodesByTopic.put(topic, sourceNodesByName.get(nodeName));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE4ODk2Ng==", "url": "https://github.com/apache/kafka/pull/8803#discussion_r436188966", "bodyText": "Thanks for the cleanup!", "author": "mjsax", "createdAt": "2020-06-05T22:17:10Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -512,18 +510,16 @@ public void closeAndRecycleState() {\n         }\n         switch (state()) {\n             case CREATED:\n-            case RUNNING:\n             case RESTORING:\n+            case RUNNING:\n             case SUSPENDED:\n                 stateMgr.recycle();\n                 recordCollector.close();\n                 break;\n-\n             case CLOSED:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while closing active task \" + id);\n-\n+                throw new IllegalStateException(\"Illegal state \" + state() + \" while recycling active task \" + id);\n             default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while closing active task \" + id);\n+                throw new IllegalStateException(\"Unknown state \" + state() + \" while recycling active task \" + id);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "c16a2027b88351e911ece554bead6a2d9105de53", "url": "https://github.com/apache/kafka/commit/c16a2027b88351e911ece554bead6a2d9105de53", "message": "dont rebuild topology, remove topics from SourceNode and fix toString", "committedDate": "2020-06-05T22:30:40Z", "type": "commit"}, {"oid": "d29fed9e7be299c2c2f04597fc024308e1aa8931", "url": "https://github.com/apache/kafka/commit/d29fed9e7be299c2c2f04597fc024308e1aa8931", "message": "get source node from node name", "committedDate": "2020-06-05T22:30:40Z", "type": "commit"}, {"oid": "3f36f51e182f8e617a52c72da92484ac7ab5f730", "url": "https://github.com/apache/kafka/commit/3f36f51e182f8e617a52c72da92484ac7ab5f730", "message": "tests compiling & checkstyl'in", "committedDate": "2020-06-05T22:30:40Z", "type": "commit"}, {"oid": "89abefbacd5137f7b8a80fe1ba7fbd4b69389e3a", "url": "https://github.com/apache/kafka/commit/89abefbacd5137f7b8a80fe1ba7fbd4b69389e3a", "message": "fix TM test", "committedDate": "2020-06-05T22:30:40Z", "type": "commit"}, {"oid": "f0b281391f5a0465cdbb6f490ece04e2336ba447", "url": "https://github.com/apache/kafka/commit/f0b281391f5a0465cdbb6f490ece04e2336ba447", "message": "github review suggestions", "committedDate": "2020-06-05T22:30:40Z", "type": "commit"}, {"oid": "43cd2c54c32092381c04afb27094c7859b3e3b01", "url": "https://github.com/apache/kafka/commit/43cd2c54c32092381c04afb27094c7859b3e3b01", "message": "add StreamTask test", "committedDate": "2020-06-05T22:30:40Z", "type": "commit"}, {"oid": "81deef5a458af200212138e0cdbe3d66cf08a568", "url": "https://github.com/apache/kafka/commit/81deef5a458af200212138e0cdbe3d66cf08a568", "message": "sanity check for unique topics", "committedDate": "2020-06-05T22:30:40Z", "type": "commit"}, {"oid": "81deef5a458af200212138e0cdbe3d66cf08a568", "url": "https://github.com/apache/kafka/commit/81deef5a458af200212138e0cdbe3d66cf08a568", "message": "sanity check for unique topics", "committedDate": "2020-06-05T22:30:40Z", "type": "forcePushed"}, {"oid": "61a200ccbd54035438ae8628a6fadb43d3b174a3", "url": "https://github.com/apache/kafka/commit/61a200ccbd54035438ae8628a6fadb43d3b174a3", "message": "checkstyle", "committedDate": "2020-06-05T22:36:35Z", "type": "commit"}]}