{"pr_number": 8970, "pr_title": "MINOR: Improve logging around initial log loading", "pr_createdAt": "2020-07-01T19:13:11Z", "pr_url": "https://github.com/apache/kafka/pull/8970", "timeline": [{"oid": "74261831b460fc620e0629e81bdbb381282630bb", "url": "https://github.com/apache/kafka/commit/74261831b460fc620e0629e81bdbb381282630bb", "message": "MINOR: Improve logging around initial log loading", "committedDate": "2020-07-01T19:10:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcyMzg5NQ==", "url": "https://github.com/apache/kafka/pull/8970#discussion_r448723895", "bodyText": "Have we checked that the dir.toString does what we want? If the path is relative, I think it will only print that.", "author": "ijuma", "createdAt": "2020-07-02T03:11:17Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -308,11 +312,11 @@ class LogManager(logDirs: Seq[File],\n         threadPools.append(pool)\n \n         val cleanShutdownFile = new File(dir, Log.CleanShutdownFile)\n-\n         if (cleanShutdownFile.exists) {\n-          debug(s\"Found clean shutdown file. Skipping recovery for all logs in data directory: ${dir.getAbsolutePath}\")\n+          info(s\"Skipping recovery for all logs in $dir since clean shutdown file was found\")", "originalCommit": "74261831b460fc620e0629e81bdbb381282630bb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcyNjA4Nw==", "url": "https://github.com/apache/kafka/pull/8970#discussion_r448726087", "bodyText": "We should fix this and related code to use hiResClockMs since it's monotonic.", "author": "ijuma", "createdAt": "2020-07-02T03:20:41Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -321,25 +325,32 @@ class LogManager(logDirs: Seq[File],\n           recoveryPoints = this.recoveryPointCheckpoints(dir).read()\n         } catch {\n           case e: Exception =>\n-            warn(s\"Error occurred while reading recovery-point-offset-checkpoint file of directory $dir\", e)\n-            warn(\"Resetting the recovery checkpoint to 0\")\n+            warn(s\"Error occurred while reading recovery-point-offset-checkpoint file of directory $dir, \" +\n+              \"resetting the recovery checkpoint to 0\", e)\n         }\n \n         var logStartOffsets = Map[TopicPartition, Long]()\n         try {\n           logStartOffsets = this.logStartOffsetCheckpoints(dir).read()\n         } catch {\n           case e: Exception =>\n-            warn(s\"Error occurred while reading log-start-offset-checkpoint file of directory $dir\", e)\n+            warn(s\"Error occurred while reading log-start-offset-checkpoint file of directory $dir, \" +\n+              \"resetting to the base offset of the first segment\", e)\n         }\n \n-        val jobsForDir = for {\n-          dirContent <- Option(dir.listFiles).toList\n-          logDir <- dirContent if logDir.isDirectory\n-        } yield {\n+        val logsToLoad = Option(dir.listFiles).getOrElse(Array.empty).filter(_.isDirectory)\n+        val numRemainingLogsToLoad = new AtomicInteger(logsToLoad.length)\n+\n+        val jobsForDir = logsToLoad.map { logDir =>\n           val runnable: Runnable = () => {\n             try {\n-              loadLog(logDir, recoveryPoints, logStartOffsets)\n+              debug(s\"Loading log $logDir\")\n+              val logLoadStartMs = time.milliseconds()\n+              val log = loadLog(logDir, recoveryPoints, logStartOffsets)\n+              numRemainingLogsToLoad.decrementAndGet()\n+              info(s\"Completed load of $log with ${log.numberOfSegments} segments \" +\n+                s\"in ${time.milliseconds() - logLoadStartMs}ms \" +", "originalCommit": "74261831b460fc620e0629e81bdbb381282630bb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcyNjQ4NQ==", "url": "https://github.com/apache/kafka/pull/8970#discussion_r448726485", "bodyText": "Can we keep the variable here and then use it in the log below? Also, should we decrement the value if there's an exception?", "author": "ijuma", "createdAt": "2020-07-02T03:22:18Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -321,25 +325,32 @@ class LogManager(logDirs: Seq[File],\n           recoveryPoints = this.recoveryPointCheckpoints(dir).read()\n         } catch {\n           case e: Exception =>\n-            warn(s\"Error occurred while reading recovery-point-offset-checkpoint file of directory $dir\", e)\n-            warn(\"Resetting the recovery checkpoint to 0\")\n+            warn(s\"Error occurred while reading recovery-point-offset-checkpoint file of directory $dir, \" +\n+              \"resetting the recovery checkpoint to 0\", e)\n         }\n \n         var logStartOffsets = Map[TopicPartition, Long]()\n         try {\n           logStartOffsets = this.logStartOffsetCheckpoints(dir).read()\n         } catch {\n           case e: Exception =>\n-            warn(s\"Error occurred while reading log-start-offset-checkpoint file of directory $dir\", e)\n+            warn(s\"Error occurred while reading log-start-offset-checkpoint file of directory $dir, \" +\n+              \"resetting to the base offset of the first segment\", e)\n         }\n \n-        val jobsForDir = for {\n-          dirContent <- Option(dir.listFiles).toList\n-          logDir <- dirContent if logDir.isDirectory\n-        } yield {\n+        val logsToLoad = Option(dir.listFiles).getOrElse(Array.empty).filter(_.isDirectory)\n+        val numRemainingLogsToLoad = new AtomicInteger(logsToLoad.length)\n+\n+        val jobsForDir = logsToLoad.map { logDir =>\n           val runnable: Runnable = () => {\n             try {\n-              loadLog(logDir, recoveryPoints, logStartOffsets)\n+              debug(s\"Loading log $logDir\")\n+              val logLoadStartMs = time.milliseconds()\n+              val log = loadLog(logDir, recoveryPoints, logStartOffsets)\n+              numRemainingLogsToLoad.decrementAndGet()", "originalCommit": "74261831b460fc620e0629e81bdbb381282630bb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcyNzE0OA==", "url": "https://github.com/apache/kafka/pull/8970#discussion_r448727148", "bodyText": "Could we maybe say something like Loaded n logs after... where n is the number of logs we loaded?", "author": "ijuma", "createdAt": "2020-07-02T03:25:14Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -379,7 +391,7 @@ class LogManager(logDirs: Seq[File],\n       threadPools.foreach(_.shutdown())\n     }\n \n-    info(s\"Logs loading complete in ${time.milliseconds - startMs} ms.\")\n+    info(s\"Log loading completed after ${time.milliseconds - startMs}ms.\")", "originalCommit": "74261831b460fc620e0629e81bdbb381282630bb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODc0Mzk3Mw==", "url": "https://github.com/apache/kafka/pull/8970#discussion_r448743973", "bodyText": "Nit: an alternative would be to say something like 3 out of 5 logs have been loaded in $dir. It gives a slightly better sense of progression.", "author": "ijuma", "createdAt": "2020-07-02T04:37:57Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -321,25 +325,32 @@ class LogManager(logDirs: Seq[File],\n           recoveryPoints = this.recoveryPointCheckpoints(dir).read()\n         } catch {\n           case e: Exception =>\n-            warn(s\"Error occurred while reading recovery-point-offset-checkpoint file of directory $dir\", e)\n-            warn(\"Resetting the recovery checkpoint to 0\")\n+            warn(s\"Error occurred while reading recovery-point-offset-checkpoint file of directory $dir, \" +\n+              \"resetting the recovery checkpoint to 0\", e)\n         }\n \n         var logStartOffsets = Map[TopicPartition, Long]()\n         try {\n           logStartOffsets = this.logStartOffsetCheckpoints(dir).read()\n         } catch {\n           case e: Exception =>\n-            warn(s\"Error occurred while reading log-start-offset-checkpoint file of directory $dir\", e)\n+            warn(s\"Error occurred while reading log-start-offset-checkpoint file of directory $dir, \" +\n+              \"resetting to the base offset of the first segment\", e)\n         }\n \n-        val jobsForDir = for {\n-          dirContent <- Option(dir.listFiles).toList\n-          logDir <- dirContent if logDir.isDirectory\n-        } yield {\n+        val logsToLoad = Option(dir.listFiles).getOrElse(Array.empty).filter(_.isDirectory)\n+        val numRemainingLogsToLoad = new AtomicInteger(logsToLoad.length)\n+\n+        val jobsForDir = logsToLoad.map { logDir =>\n           val runnable: Runnable = () => {\n             try {\n-              loadLog(logDir, recoveryPoints, logStartOffsets)\n+              debug(s\"Loading log $logDir\")\n+              val logLoadStartMs = time.milliseconds()\n+              val log = loadLog(logDir, recoveryPoints, logStartOffsets)\n+              numRemainingLogsToLoad.decrementAndGet()\n+              info(s\"Completed load of $log with ${log.numberOfSegments} segments \" +\n+                s\"in ${time.milliseconds() - logLoadStartMs}ms \" +\n+                s\"(${numRemainingLogsToLoad.get} logs remaining to load in $dir)\")", "originalCommit": "74261831b460fc620e0629e81bdbb381282630bb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "77bc7335122ead4ee67c5a2c53d24ebffb63336b", "url": "https://github.com/apache/kafka/commit/77bc7335122ead4ee67c5a2c53d24ebffb63336b", "message": "Review comments", "committedDate": "2020-07-02T18:10:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE5Mzc3NQ==", "url": "https://github.com/apache/kafka/pull/8970#discussion_r449193775", "bodyText": "Did you mean to include the word logs somewhere in this sentence?", "author": "ijuma", "createdAt": "2020-07-02T18:19:56Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -379,7 +396,7 @@ class LogManager(logDirs: Seq[File],\n       threadPools.foreach(_.shutdown())\n     }\n \n-    info(s\"Logs loading complete in ${time.milliseconds - startMs} ms.\")\n+    info(s\"Loaded $numTotalLogs in ${time.hiResClockMs() - startMs}ms.\")", "originalCommit": "77bc7335122ead4ee67c5a2c53d24ebffb63336b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2f9e2beaec870d67a1e8cba005038c0820602c54", "url": "https://github.com/apache/kafka/commit/2f9e2beaec870d67a1e8cba005038c0820602c54", "message": "Add missing 'logs' to clarify what was loaded", "committedDate": "2020-07-04T01:42:26Z", "type": "commit"}]}