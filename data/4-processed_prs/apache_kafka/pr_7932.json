{"pr_number": 7932, "pr_title": "KAFKA-8764: LogCleanerManager endless loop while compacting/clea", "pr_createdAt": "2020-01-10T20:14:07Z", "pr_url": "https://github.com/apache/kafka/pull/7932", "timeline": [{"oid": "b2ec1054ed03d946b71d15b32eb050eb47d6fa20", "url": "https://github.com/apache/kafka/commit/b2ec1054ed03d946b71d15b32eb050eb47d6fa20", "message": "fixes KAFKA-8764 LogCleanerManager endless loop while compacting/cleaning segments", "committedDate": "2020-01-10T15:49:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4Mjk2OQ==", "url": "https://github.com/apache/kafka/pull/7932#discussion_r365482969", "bodyText": "I am not sure if this completely fixes the issue. Let's say we have one non active segment with offset range [100, 190) and an active segment of [200, 300). With this change, we will set the latestOffset in the map to 190 (which is an improvement from 0). However, this can still trigger another round of unnecessary cleaning since the first segment will still be treated as cleanable (instead of cleaned).\nOne way to fix this more completely is that when we have finished iterating all batches in a segment, we update the latest offset in the map to the start offset of the next segment. In the above example, this will set the latest offset of the map to 200 and the next round of cleaning won't be triggered until the active segment rolls.", "author": "junrao", "createdAt": "2020-01-11T00:45:49Z", "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "diffHunk": "@@ -931,11 +931,15 @@ private[log] class Cleaner(val id: Int,\n             stats.indexMessagesRead(batch.countOrNull)\n           } else {\n             for (record <- batch.asScala) {\n-              if (record.hasKey && record.offset >= startOffset) {\n-                if (map.size < maxDesiredMapSize)\n-                  map.put(record.key, record.offset)\n-                else\n-                  return true\n+              if (record.hasKey) {\n+                if (record.offset >= startOffset) {\n+                  if (map.size < maxDesiredMapSize)\n+                    map.put(record.key, record.offset)\n+                  else\n+                    return true\n+                } else {\n+                  map.updateLatestOffset(record.offset + 1)", "originalCommit": "b2ec1054ed03d946b71d15b32eb050eb47d6fa20", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTg1NTgzMw==", "url": "https://github.com/apache/kafka/pull/7932#discussion_r365855833", "bodyText": "You were right here. Please, have a look at new fix proposal, and test covering specific issue.", "author": "trajakovic", "createdAt": "2020-01-13T15:11:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4Mjk2OQ=="}], "type": "inlineReview"}, {"oid": "e0c2dcc39fefdabeb67534a93b0bd32d53dbe815", "url": "https://github.com/apache/kafka/commit/e0c2dcc39fefdabeb67534a93b0bd32d53dbe815", "message": "add tests replicating KAFKA-8764 issue\nupdate LogCleaner to fix KAFKA-8764 issue", "committedDate": "2020-01-13T15:01:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTg1NDMwMw==", "url": "https://github.com/apache/kafka/pull/7932#discussion_r365854303", "bodyText": "@junrao\nwell you were right, and after i wrote test case(es), first fix didn't resolve issue, so I've reverted the code, and give 2nd chance to fix it.", "author": "trajakovic", "createdAt": "2020-01-13T15:08:49Z", "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "diffHunk": "@@ -957,6 +961,12 @@ private[log] class Cleaner(val id: Int,\n       if(position == startPosition)\n         growBuffersOrFail(segment.log, position, maxLogMessageSize, records)\n     }\n+\n+    // check for missing offsets at the end of logSegment\n+    if (lastOffsetInSegment < nextSegmentStartOffset - 1L) {", "originalCommit": "e0c2dcc39fefdabeb67534a93b0bd32d53dbe815", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTg1NzY1Mg==", "url": "https://github.com/apache/kafka/pull/7932#discussion_r365857652", "bodyText": "Don't know more idiomatic way to write this in Scala", "author": "trajakovic", "createdAt": "2020-01-13T15:14:50Z", "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "diffHunk": "@@ -867,6 +867,11 @@ private[log] class Cleaner(val id: Int,\n                                   stats: CleanerStats): Unit = {\n     map.clear()\n     val dirty = log.logSegments(start, end).toBuffer\n+    val nextSegmentStartOffsets : mutable.Buffer[Long] = if (dirty.nonEmpty) {", "originalCommit": "e0c2dcc39fefdabeb67534a93b0bd32d53dbe815", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTg3NTk5MA==", "url": "https://github.com/apache/kafka/pull/7932#discussion_r365875990", "bodyText": "Since it's a mutable buffer, how about assigning it to an empty buffer in the declaration, then appending the offsets from dirty as well as the end offset in an if block. We could also avoid the :+ \ud83d\ude01", "author": "mumrah", "createdAt": "2020-01-13T15:47:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTg1NzY1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTk4Mzg4Nw==", "url": "https://github.com/apache/kafka/pull/7932#discussion_r365983887", "bodyText": "thx @mumrah\nadded your suggestion(s)", "author": "trajakovic", "createdAt": "2020-01-13T19:25:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTg1NzY1Mg=="}], "type": "inlineReview"}, {"oid": "2c464ac94f7cf025d430dea6a3f12627e75a6794", "url": "https://github.com/apache/kafka/commit/2c464ac94f7cf025d430dea6a3f12627e75a6794", "message": "add mutable.ListBuffer import\nmake code mode Scala way", "committedDate": "2020-01-13T19:23:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTk5NTgxNg==", "url": "https://github.com/apache/kafka/pull/7932#discussion_r365995816", "bodyText": "It would be useful to add a comment on why we need to do this. Also, not sure if we need to do the test in the line below since nextSegment's start offset is guaranteed to be larger than all offsets in previous segments.", "author": "junrao", "createdAt": "2020-01-13T19:50:18Z", "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "diffHunk": "@@ -953,6 +962,12 @@ private[log] class Cleaner(val id: Int,\n       if(position == startPosition)\n         growBuffersOrFail(segment.log, position, maxLogMessageSize, records)\n     }\n+\n+    // check for missing offsets at the end of logSegment", "originalCommit": "2c464ac94f7cf025d430dea6a3f12627e75a6794", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjI1MzQ0Ng==", "url": "https://github.com/apache/kafka/pull/7932#discussion_r366253446", "bodyText": "@junrao  I'm gonna need help explaining why is this happening in live kafka since I'm unaware of origin of probelm when/why and how.\nalso, I'm gonna remove if condition and always updateLatsOffset to expected end of segment as you suggested", "author": "trajakovic", "createdAt": "2020-01-14T10:15:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTk5NTgxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjAzNzYyOQ==", "url": "https://github.com/apache/kafka/pull/7932#discussion_r366037629", "bodyText": "The error logging is a bit unintuitive. How about \"Cleaning point should pass offset gap\"? Ditto below.", "author": "junrao", "createdAt": "2020-01-13T21:28:19Z", "path": "core/src/test/scala/unit/kafka/log/LogCleanerTest.scala", "diffHunk": "@@ -1561,6 +1561,49 @@ class LogCleanerTest {\n     assertEquals(\"The tombstone should be retained.\", 1, log.logSegments.head.log.batches.iterator.next().lastOffset)\n   }\n \n+  /**\n+   * Verify that the clean is able to move beyond missing offsets records in dirty log\n+   */\n+  @Test\n+  def testCleaningBeyondMissingOffsets(): Unit = {\n+    val logProps = new Properties()\n+    logProps.put(LogConfig.SegmentBytesProp, 1024*1024: java.lang.Integer)\n+    logProps.put(LogConfig.CleanupPolicyProp, LogConfig.Compact)\n+    val logConfig = LogConfig(logProps)\n+    val cleaner = makeCleaner(Int.MaxValue)\n+\n+    {\n+      val log = makeLog(dir = TestUtils.randomPartitionLogDir(tmpdir), config = logConfig)\n+      writeToLog(log, (0 to 9) zip (0 to 9), (0L to 9L))\n+      // roll new segment with baseOffset 11, leaving previous with holes in offset range [9,10]\n+      log.roll(Some(11L))\n+\n+      // active segment record\n+      log.appendAsFollower(messageWithOffset(1015, 1015, 11L))\n+\n+      val (nextDirtyOffset, _) = cleaner.clean(LogToClean(log.topicPartition, log, 0L, log.activeSegment.baseOffset, needCompactionNow = true))\n+      assertEquals(\"Missing offsets should be skipped until next segment baseOffset\", log.activeSegment.baseOffset, nextDirtyOffset)", "originalCommit": "2c464ac94f7cf025d430dea6a3f12627e75a6794", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjI0ODE5Mw==", "url": "https://github.com/apache/kafka/pull/7932#discussion_r366248193", "bodyText": "understood", "author": "trajakovic", "createdAt": "2020-01-14T10:05:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjAzNzYyOQ=="}], "type": "inlineReview"}, {"oid": "630841d86ba826b407fb8915055255be0775fe4b", "url": "https://github.com/apache/kafka/commit/630841d86ba826b407fb8915055255be0775fe4b", "message": "update asserts to more meaningful messages", "committedDate": "2020-01-14T11:04:50Z", "type": "commit"}, {"oid": "dcd952ccaa29830429a9cbf70823fd0328cd3b13", "url": "https://github.com/apache/kafka/commit/dcd952ccaa29830429a9cbf70823fd0328cd3b13", "message": "remove always true condition\nupdate comment to reflect case it solves", "committedDate": "2020-01-14T11:12:55Z", "type": "commit"}]}