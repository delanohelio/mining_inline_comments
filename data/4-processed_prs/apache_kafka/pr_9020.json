{"pr_number": 9020, "pr_title": "KAFKA-10271 Performance regression while fetching a key from a single partition", "pr_createdAt": "2020-07-14T15:05:22Z", "pr_url": "https://github.com/apache/kafka/pull/9020", "timeline": [{"oid": "2fb966ff9d3f030c1b6c54e52bcee8032fada5fb", "url": "https://github.com/apache/kafka/commit/2fb966ff9d3f030c1b6c54e52bcee8032fada5fb", "message": "Performance issue, in case of withPartition parameter exists - do not return all state stores", "committedDate": "2020-07-14T14:46:23Z", "type": "commit"}, {"oid": "839379de35fe41c044f84aeffcf414a3c3bb91ff", "url": "https://github.com/apache/kafka/commit/839379de35fe41c044f84aeffcf414a3c3bb91ff", "message": "Find task in-place, avoid synchorinized calling to topicsGroup", "committedDate": "2020-07-15T11:35:55Z", "type": "commit"}, {"oid": "a15d28f518bdd008b393d9f280341da6fb564b56", "url": "https://github.com/apache/kafka/commit/a15d28f518bdd008b393d9f280341da6fb564b56", "message": "Find task in-place, avoid synchorinized calling to topicsGroup", "committedDate": "2020-07-15T11:45:12Z", "type": "commit"}, {"oid": "c1802752ed407e8a4e2a0ff00ea792b1466b3906", "url": "https://github.com/apache/kafka/commit/c1802752ed407e8a4e2a0ff00ea792b1466b3906", "message": "Find task in-place, avoid synchorinized calling to topicsGroup", "committedDate": "2020-07-15T11:50:17Z", "type": "commit"}, {"oid": "4209ce29861c1fea6cfe6628ed54da7e772e7f8f", "url": "https://github.com/apache/kafka/commit/4209ce29861c1fea6cfe6628ed54da7e772e7f8f", "message": "Find task in-place, avoid synchorinized calling to topicsGroup", "committedDate": "2020-07-15T15:54:45Z", "type": "commit"}, {"oid": "f5f2e46fbd9ec3f39c6c1f140d220a6052fdccd4", "url": "https://github.com/apache/kafka/commit/f5f2e46fbd9ec3f39c6c1f140d220a6052fdccd4", "message": "remove extra loops over all stores of all providers as a sanity check before returning the WrappingStoreProvider", "committedDate": "2020-07-15T17:01:00Z", "type": "commit"}, {"oid": "b7d38c6ed55dae174de77de429cb94fa1c26344c", "url": "https://github.com/apache/kafka/commit/b7d38c6ed55dae174de77de429cb94fa1c26344c", "message": "remove extra loops over all stores of all providers as a sanity check before returning the WrappingStoreProvider", "committedDate": "2020-07-15T17:30:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkwMDcyNg==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r455900726", "bodyText": "Is this really a different condition than the one on L65? It seems like the failure is still probably that the store \"migrated\" instead of \"doesn't exist\", right?", "author": "vvcephei", "createdAt": "2020-07-16T16:05:54Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/WrappingStoreProvider.java", "diffHunk": "@@ -46,11 +46,22 @@ public void setStoreQueryParameters(final StoreQueryParameters storeQueryParamet\n     public <T> List<T> stores(final String storeName,\n                               final QueryableStoreType<T> queryableStoreType) {\n         final List<T> allStores = new ArrayList<>();\n-        for (final StreamThreadStateStoreProvider provider : storeProviders) {\n-            final List<T> stores = provider.stores(storeQueryParameters);\n-            allStores.addAll(stores);\n+        for (final StreamThreadStateStoreProvider storeProvider : storeProviders) {\n+            final List<T> stores = storeProvider.stores(storeQueryParameters);\n+            if (!stores.isEmpty()) {\n+                allStores.addAll(stores);\n+                if (storeQueryParameters.partition() != null) {\n+                    break;\n+                }\n+            }\n         }\n         if (allStores.isEmpty()) {\n+            if (storeQueryParameters.partition() != null) {\n+                throw new InvalidStateStoreException(\n+                        String.format(\"The specified partition %d for store %s does not exist.\",", "originalCommit": "b7d38c6ed55dae174de77de429cb94fa1c26344c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkzOTQ5MQ==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r455939491", "bodyText": "L65 catches on rebalancing, while L60 is parameter validation for incorrect partition case.", "author": "dima5rr", "createdAt": "2020-07-16T17:07:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkwMDcyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc0NzI1MA==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r494747250", "bodyText": "Could you elaborate a bit more about this? If allStores.isEmpty() is empty, it is always possible that the specified store-partition or just store-\"null\" does not exist in this client. Why they are different failure cases?", "author": "guozhangwang", "createdAt": "2020-09-25T04:57:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkwMDcyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc5NjQyOQ==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r496796429", "bodyText": "Hey @dima5rr , I think Guozhang's question was hidden because the conversation was already \"resolved\". Do you mind answering this concern?", "author": "vvcephei", "createdAt": "2020-09-29T15:08:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkwMDcyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODA1NTU2Nw==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r498055567", "bodyText": "Hey @guozhangwang, you're right, this check is ambiguous, it's more likely parameter sanity validation when user explicitly specify a single partition.", "author": "dima5rr", "createdAt": "2020-10-01T08:01:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkwMDcyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODUzMzkyMg==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r498533922", "bodyText": "Got it, in that case how about we just encode the partition in the thrown's message so that upon throwing, people can still check if the partition is null or not when debugging?\nOtherwise, this PR all LGTM :)", "author": "guozhangwang", "createdAt": "2020-10-01T21:59:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkwMDcyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODYyNzk1MQ==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r498627951", "bodyText": "Hey @guozhangwang, I am just care that in case of partition is null, the error message is referenced in official FAQ.\nhttps://docs.confluent.io/current/streams/faq.html#handling-invalidstatestoreexception-the-state-store-may-have-migrated-to-another-instance", "author": "dima5rr", "createdAt": "2020-10-02T05:53:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkwMDcyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEwMTg0OA==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r499101848", "bodyText": "That's a fair point, let's just merge it as is then.", "author": "guozhangwang", "createdAt": "2020-10-03T01:10:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkwMDcyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkwNDc1Mg==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r455904752", "bodyText": "The nested early-return pattern is pretty hard to follow. Do you mind rewriting it to use if/else blocks? I know it was previously doing some early returns; it'd be better to migrate to a more maintainable style when we update the code, though.", "author": "vvcephei", "createdAt": "2020-07-16T16:11:40Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java", "diffHunk": "@@ -20,62 +20,49 @@\n import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.processor.StateStore;\n import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.InternalTopologyBuilder;\n import org.apache.kafka.streams.processor.internals.StreamThread;\n import org.apache.kafka.streams.processor.internals.Task;\n import org.apache.kafka.streams.state.QueryableStoreType;\n import org.apache.kafka.streams.state.QueryableStoreTypes;\n import org.apache.kafka.streams.state.TimestampedKeyValueStore;\n import org.apache.kafka.streams.state.TimestampedWindowStore;\n \n-import java.util.ArrayList;\n import java.util.Collections;\n-import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n-import java.util.Set;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n \n public class StreamThreadStateStoreProvider {\n \n     private final StreamThread streamThread;\n-    private final InternalTopologyBuilder internalTopologyBuilder;\n \n-    public StreamThreadStateStoreProvider(final StreamThread streamThread,\n-                                          final InternalTopologyBuilder internalTopologyBuilder) {\n+    public StreamThreadStateStoreProvider(final StreamThread streamThread) {\n         this.streamThread = streamThread;\n-        this.internalTopologyBuilder = internalTopologyBuilder;\n     }\n \n     @SuppressWarnings(\"unchecked\")\n     public <T> List<T> stores(final StoreQueryParameters storeQueryParams) {\n         final String storeName = storeQueryParams.storeName();\n         final QueryableStoreType<T> queryableStoreType = storeQueryParams.queryableStoreType();\n-        final TaskId keyTaskId = createKeyTaskId(storeName, storeQueryParams.partition());\n         if (streamThread.state() == StreamThread.State.DEAD) {\n             return Collections.emptyList();\n         }\n         final StreamThread.State state = streamThread.state();\n         if (storeQueryParams.staleStoresEnabled() ? state.isAlive() : state == StreamThread.State.RUNNING) {\n             final Map<TaskId, ? extends Task> tasks = storeQueryParams.staleStoresEnabled() ? streamThread.allTasks() : streamThread.activeTaskMap();\n-            final List<T> stores = new ArrayList<>();\n-            if (keyTaskId != null) {\n-                final Task task = tasks.get(keyTaskId);\n-                if (task == null) {\n+            if (storeQueryParams.partition() != null) {\n+                final Task streamTask = findStreamTask(tasks, storeName, storeQueryParams.partition());\n+                if (streamTask == null) {\n                     return Collections.emptyList();\n                 }\n-                final T store = validateAndListStores(task.getStore(storeName), queryableStoreType, storeName, keyTaskId);\n-                if (store != null) {\n-                    return Collections.singletonList(store);\n-                }\n-            } else {\n-                for (final Task streamTask : tasks.values()) {\n-                    final T store = validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id());\n-                    if (store != null) {\n-                        stores.add(store);\n-                    }\n-                }\n+                final T store = validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id());\n+                return store != null ? Collections.singletonList(store) : Collections.emptyList();", "originalCommit": "b7d38c6ed55dae174de77de429cb94fa1c26344c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkzNDU4MA==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r455934580", "bodyText": "sure", "author": "dima5rr", "createdAt": "2020-07-16T16:59:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTkwNDc1Mg=="}], "type": "inlineReview"}, {"oid": "89ac008bbe0763619a68d5cb455f73ec1aee54be", "url": "https://github.com/apache/kafka/commit/89ac008bbe0763619a68d5cb455f73ec1aee54be", "message": "refactor if/else flow to eliminate early-return", "committedDate": "2020-07-16T16:58:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk0ODcwNA==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r455948704", "bodyText": "Ah, sorry, I can see that my prior comment was ambiguous. This is what I meant:\n            if (storeQueryParams.partition() == null) {\n                return tasks.values().stream().\n                        map(streamTask -> validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id())).\n                        filter(Objects::nonNull).\n                        collect(Collectors.toList());\n            } else {\n                final Task streamTask = findStreamTask(tasks, storeName, storeQueryParams.partition());\n                if (streamTask == null) {\n                    return Collections.emptyList();\n                } else {\n                    final T store = validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id());\n                    return store == null ? Collections.emptyList() : Collections.singletonList(store);\n                }\n            }\nThe reason this is better for maintenence is that you only have to trace a path through the nested conditionals into a single inner block to understand what gets returned. I.e., code comprehension complexity is only the depth of the conditional tree.\nIn contrast, if we do early returns, you have to fully read all the conditional blocks that lead up to the one you're interested (depth-first traversal), so code comprehension is linear instead of logarithmic. If we mutate the collection, you actually have to read all the conditionals to understand what is going to happen, so code comprehension is also linear instead of logarithmic.", "author": "vvcephei", "createdAt": "2020-07-16T17:22:31Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java", "diffHunk": "@@ -51,18 +51,22 @@ public StreamThreadStateStoreProvider(final StreamThread streamThread) {\n         final StreamThread.State state = streamThread.state();\n         if (storeQueryParams.staleStoresEnabled() ? state.isAlive() : state == StreamThread.State.RUNNING) {\n             final Map<TaskId, ? extends Task> tasks = storeQueryParams.staleStoresEnabled() ? streamThread.allTasks() : streamThread.activeTaskMap();\n+            final List<T> stores = new ArrayList<>();\n             if (storeQueryParams.partition() != null) {\n                 final Task streamTask = findStreamTask(tasks, storeName, storeQueryParams.partition());\n-                if (streamTask == null) {\n-                    return Collections.emptyList();\n+                if (streamTask != null) {\n+                    final T store = validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id());\n+                    if (store != null) {\n+                        stores.add(store);\n+                    }\n                 }\n-                final T store = validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id());\n-                return store != null ? Collections.singletonList(store) : Collections.emptyList();\n+            } else {\n+                tasks.values().stream().\n+                        map(streamTask -> validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id())).\n+                        filter(Objects::nonNull).\n+                        forEach(stores::add);\n             }\n-            return tasks.values().stream().\n-                    map(streamTask -> validateAndListStores(streamTask.getStore(storeName), queryableStoreType, storeName, streamTask.id())).\n-                    filter(Objects::nonNull).\n-                    collect(Collectors.toList());\n+            return Collections.unmodifiableList(stores);", "originalCommit": "89ac008bbe0763619a68d5cb455f73ec1aee54be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk3NDQ3Ng==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r455974476", "bodyText": "Will concise it into functional way.", "author": "dima5rr", "createdAt": "2020-07-16T18:05:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk0ODcwNA=="}], "type": "inlineReview"}, {"oid": "158db402af5cfbbffeba297d3d9d3811ea75a979", "url": "https://github.com/apache/kafka/commit/158db402af5cfbbffeba297d3d9d3811ea75a979", "message": "concise if/else flow into functional", "committedDate": "2020-07-16T18:03:51Z", "type": "commit"}, {"oid": "b22417fee013cb18e19ad96d4f2d7c0845cf3452", "url": "https://github.com/apache/kafka/commit/b22417fee013cb18e19ad96d4f2d7c0845cf3452", "message": "optimize performance - avoid creating intermediate active tasks map", "committedDate": "2020-07-17T08:53:50Z", "type": "commit"}, {"oid": "da8554cff2f30fcb00f748cd81e7d676098850c9", "url": "https://github.com/apache/kafka/commit/da8554cff2f30fcb00f748cd81e7d676098850c9", "message": "optimize performance - avoid creating intermediate active tasks map", "committedDate": "2020-07-17T09:06:55Z", "type": "commit"}, {"oid": "fca67e47561ac63fb15c1fd13f3c19c3bead496e", "url": "https://github.com/apache/kafka/commit/fca67e47561ac63fb15c1fd13f3c19c3bead496e", "message": "[TESTS] remove expensive double-iteration from state provider", "committedDate": "2020-07-17T10:45:51Z", "type": "commit"}, {"oid": "0fb7fe29473956dad54e73440152ec90df86ae78", "url": "https://github.com/apache/kafka/commit/0fb7fe29473956dad54e73440152ec90df86ae78", "message": "Merge branch 'trunk' into KAFKA-10271", "committedDate": "2020-08-16T14:56:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc0NzMxOQ==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r494747319", "bodyText": "A good coverage improvement! Thanks.", "author": "guozhangwang", "createdAt": "2020-09-25T04:57:38Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/StreamStreamJoinIntegrationTest.java", "diffHunk": "@@ -60,6 +60,34 @@ public void prepareTopology() throws InterruptedException {\n         rightStream = builder.stream(INPUT_TOPIC_RIGHT);\n     }\n \n+    @Test\n+    public void shouldNotAccessJoinStoresWhenGivingName() throws InterruptedException {", "originalCommit": "0fb7fe29473956dad54e73440152ec90df86ae78", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc0ODA0OQ==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r494748049", "bodyText": "This is a great find, thanks!", "author": "guozhangwang", "createdAt": "2020-09-25T05:00:45Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java", "diffHunk": "@@ -104,19 +95,11 @@ public StreamThreadStateStoreProvider(final StreamThread streamThread,\n         }\n     }\n \n-    private TaskId createKeyTaskId(final String storeName, final Integer partition) {\n-        if (partition == null) {\n-            return null;\n-        }\n-        final List<String> sourceTopics = internalTopologyBuilder.stateStoreNameToSourceTopics().get(storeName);\n-        final Set<String> sourceTopicsSet = new HashSet<>(sourceTopics);\n-        final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = internalTopologyBuilder.topicGroups();\n-        for (final Map.Entry<Integer, InternalTopologyBuilder.TopicsInfo> topicGroup : topicGroups.entrySet()) {\n-            if (topicGroup.getValue().sourceTopics.containsAll(sourceTopicsSet)) {\n-                return new TaskId(topicGroup.getKey(), partition);\n-            }\n-        }\n-        throw new InvalidStateStoreException(\"Cannot get state store \" + storeName + \" because the requested partition \" +\n-            partition + \" is not available on this instance\");\n+    private Optional<Task> findStreamTask(final Collection<Task> tasks, final String storeName, final int partition) {", "originalCommit": "0fb7fe29473956dad54e73440152ec90df86ae78", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc0ODA3MA==", "url": "https://github.com/apache/kafka/pull/9020#discussion_r494748070", "bodyText": "LGTM.", "author": "guozhangwang", "createdAt": "2020-09-25T05:00:51Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/QueryableStoreProvider.java", "diffHunk": "@@ -56,25 +55,6 @@ public QueryableStoreProvider(final List<StreamThreadStateStoreProvider> storePr\n         if (!globalStore.isEmpty()) {\n             return queryableStoreType.create(globalStoreProvider, storeName);\n         }\n-        final List<T> allStores = new ArrayList<>();", "originalCommit": "0fb7fe29473956dad54e73440152ec90df86ae78", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9838391304ff1cb3d1eab7b9d4f0126c57a66a1b", "url": "https://github.com/apache/kafka/commit/9838391304ff1cb3d1eab7b9d4f0126c57a66a1b", "message": "KAFKA-9273: Extract testShouldAutoShutdownOnIncompleteMetadata from S\u2026 (#9108)\n\nThe main goal is to remove usage of embedded broker (EmbeddedKafkaCluster) in AbstractJoinIntegrationTest and its subclasses.\nThis is because the tests under this class are no longer using the embedded broker, except for two.\ntestShouldAutoShutdownOnIncompleteMetadata is one of such tests.\nFurthermore, this test does not actually perfom stream-table join; it is testing an edge case of joining with a non-existent topic, so it should be in a separate test.\n\nTesting strategy: run existing unit and integration test\n\nReviewers: Boyang Chen <boyang@confluent.io>, Bill Bejeck <bbejeck@apache.org>", "committedDate": "2020-10-05T09:08:29Z", "type": "commit"}, {"oid": "5d7c37fc50bb71e9ef9381fdb494b1d3b97571eb", "url": "https://github.com/apache/kafka/commit/5d7c37fc50bb71e9ef9381fdb494b1d3b97571eb", "message": "Merge remote-tracking branch 'upstream/trunk' into KAFKA-10271", "committedDate": "2020-10-05T12:10:49Z", "type": "commit"}, {"oid": "73def50d582d4c90bc5a53fed7463aa0a3b0d0d0", "url": "https://github.com/apache/kafka/commit/73def50d582d4c90bc5a53fed7463aa0a3b0d0d0", "message": "fix test to throw exception on store request", "committedDate": "2020-10-05T15:02:59Z", "type": "commit"}, {"oid": "f4f9355c5dc1b8768d0a44a81b00c2b7076e7957", "url": "https://github.com/apache/kafka/commit/f4f9355c5dc1b8768d0a44a81b00c2b7076e7957", "message": "Merge remote-tracking branch 'upstream/trunk' into KAFKA-10271", "committedDate": "2020-10-07T08:26:22Z", "type": "commit"}, {"oid": "702cc065a2a1e723e3c84268d02983025c144f51", "url": "https://github.com/apache/kafka/commit/702cc065a2a1e723e3c84268d02983025c144f51", "message": "waiting state store readiness by probing non-existing value", "committedDate": "2020-10-07T12:10:16Z", "type": "commit"}, {"oid": "274617cd232c8546ba68df6cdfe8e12bef67ed23", "url": "https://github.com/apache/kafka/commit/274617cd232c8546ba68df6cdfe8e12bef67ed23", "message": "Merge remote-tracking branch 'upstream/trunk' into KAFKA-10271", "committedDate": "2020-10-07T15:18:24Z", "type": "commit"}, {"oid": "049ec22974580acd023f5d06e4cb34bfd4ad935e", "url": "https://github.com/apache/kafka/commit/049ec22974580acd023f5d06e4cb34bfd4ad935e", "message": "Merge remote-tracking branch 'upstream/trunk' into KAFKA-10271", "committedDate": "2020-10-08T08:42:12Z", "type": "commit"}]}