{"pr_number": 8180, "pr_title": "MINOR: Check store directory empty to decide whether throw task corrupted exception with EOS", "pr_createdAt": "2020-02-27T00:11:49Z", "pr_url": "https://github.com/apache/kafka/pull/8180", "timeline": [{"oid": "ef6d1115eb11c26a7f14fda9d714a4cd4297dc95", "url": "https://github.com/apache/kafka/commit/ef6d1115eb11c26a7f14fda9d714a4cd4297dc95", "message": "the TODO is actually not needed", "committedDate": "2020-02-27T00:07:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg0MzY1Mg==", "url": "https://github.com/apache/kafka/pull/8180#discussion_r384843652", "bodyText": "Here I just re-arranged the parameter orders and below I use the log-prefix from the log-context rather than creating a new string, no critical changes here.", "author": "guozhangwang", "createdAt": "2020-02-27T00:12:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -161,19 +161,19 @@ public static String storeChangelogTopic(final String applicationId,\n      * @throws ProcessorStateException if the task directory does not exist and could not be created\n      */\n     public ProcessorStateManager(final TaskId taskId,\n-                                 final Collection<TopicPartition> sources,\n                                  final TaskType taskType,\n+                                 final LogContext logContext,", "originalCommit": "ef6d1115eb11c26a7f14fda9d714a4cd4297dc95", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQxMjg0Ng==", "url": "https://github.com/apache/kafka/pull/8180#discussion_r385412846", "bodyText": "If I understand this reasoning, it's predicated on the previous run doing something during shutdown. I'm not sure we can rely on any such assumptions. For example, what if the previous run got kill -9ed instead of requested to shutdown? What if that happened during a normal shutdown? What if the machine crashed, the filesystem got corrupted, and fsck replayed the journal to an arbitrary state during recovery?\nI think that we used to delete the checkpoint file as soon as we started processing under EOS, but I can't find that anymore in the code. Is that still the case? If that were true, then this assumption I'm concerned about wouldn't apply, since the checkpoint file wouldn't exist unless it were consistent with the store.", "author": "vvcephei", "createdAt": "2020-02-27T22:39:03Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -211,7 +211,15 @@ void initializeStoreOffsetsFromCheckpoint() {\n                         log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n                             store.stateStore.name(), store.offset, store.changelogPartition);\n                     } else {\n-                        // TODO K9113: for EOS when there's no checkpointed offset, we should treat it as TaskCorrupted\n+                        // with EOS, if the previous run did not shutdown gracefully, we would always close\n+                        // all tasks as dirty before complete the shutdown which will already wipe out the local\n+                        // stores including the checkpoint store. That means, here if the checkpoint file does not\n+                        // contain the corresponding offset, there are only two possibilities:\n+                        //\n+                        // 1. the local state store is also empty, in which case it is safe to restore from scratch.\n+                        // 2. the local state store is not empty but it only contains committed records (i.e. only\n+                        //    the checkpoint file when corrupted), in which case it is safe to restore and overwrite\n+                        //    from scratch too.", "originalCommit": "ef6d1115eb11c26a7f14fda9d714a4cd4297dc95", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2Nzc4NA==", "url": "https://github.com/apache/kafka/pull/8180#discussion_r385467784", "bodyText": "That logic is still here, actually now in trunk with or without EOS, we would delete the file right after loading to processor state manager: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java#L227", "author": "guozhangwang", "createdAt": "2020-02-28T01:43:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQxMjg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2ODAyNw==", "url": "https://github.com/apache/kafka/pull/8180#discussion_r385468027", "bodyText": "The rationale was that (if you remember :) the only gap where we are at risk of losing all is the commit interval between starting up and the first commit (without EOS we would write that checkpoint file again in the first commit).", "author": "guozhangwang", "createdAt": "2020-02-28T01:44:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQxMjg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTg5NzU5Ng==", "url": "https://github.com/apache/kafka/pull/8180#discussion_r385897596", "bodyText": "Thanks @guozhangwang ,\nIn that case, I'm not understanding the above comment. It says that if the previous shutdown didn't complete gracefully, then we'd delete the store and checkpoint file, but it sounds like in actuality, the checkpoint file wouldn't exist at all in that case, and therefore not be \"deleted\" on unclean shutdown. In fact, the two conclusions you listed seem to follow from what I just said, so maybe it's just the first part of the comment that I found confusing.\nCarry on :)", "author": "vvcephei", "createdAt": "2020-02-28T20:04:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQxMjg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTkwMzQ0Mg==", "url": "https://github.com/apache/kafka/pull/8180#discussion_r385903442", "bodyText": "Hmm, that is a good question.\nSo let's re-think through it:\n\nIf the state stores are already wiped, then checkpoint file would never exist too.\nThe vice versa, however, is not true: if there's no checkpoint file found or the partition not exist in checkpoint file, the state stores may or may not be empty.\n\nSo let's take a closer look at case 2), if the checkpoint file does not exist but the state stores are non-empty, it means\n2.a) we had a clean close before (hence the checkpoint file is written) but then that checkpoint file itself is corrupted or lost. In this case it is safe to just restart restoration from the beginning and overwrite the non-empty stores, since that store was closed and flushed cleanly and hence there's no uncommitted data.\n2.b) we had a fatal crash (e.g. kill -9) and the closeDirty does not execute at all, and hence there's no checkpoint file but the store may be non-empty. Then upon recovery the fsync may restore to any point in the past. That's the danger case which we may miss here.", "author": "guozhangwang", "createdAt": "2020-02-28T20:18:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQxMjg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTkwNDI5Ng==", "url": "https://github.com/apache/kafka/pull/8180#discussion_r385904296", "bodyText": "The tricky thing though, is that although by the time when we initializeOffsetsFromCheckpoint the store is already initialized, today we do not have a way to check if the StateStore is empty or not. But if we just be conservative for 2.b) and always wipe out the store and revive, we would fall into the endless loop.", "author": "guozhangwang", "createdAt": "2020-02-28T20:20:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQxMjg0Ng=="}], "type": "inlineReview"}, {"oid": "f7d8c783b8f5de108d8493841cba002c954eecee", "url": "https://github.com/apache/kafka/commit/f7d8c783b8f5de108d8493841cba002c954eecee", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-eos-task-corrupted", "committedDate": "2020-02-28T22:21:48Z", "type": "commit"}, {"oid": "29614dc0b5cc6dad4478557460db041cc9d2e4e5", "url": "https://github.com/apache/kafka/commit/29614dc0b5cc6dad4478557460db041cc9d2e4e5", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-eos-task-corrupted", "committedDate": "2020-03-04T00:05:15Z", "type": "commit"}, {"oid": "46b136544d154e25e512b9849d0dcb9e04005163", "url": "https://github.com/apache/kafka/commit/46b136544d154e25e512b9849d0dcb9e04005163", "message": "add back eosEnabled flag", "committedDate": "2020-03-04T16:54:58Z", "type": "commit"}, {"oid": "123a28cda8172d327aeb0adea974692286daee34", "url": "https://github.com/apache/kafka/commit/123a28cda8172d327aeb0adea974692286daee34", "message": "rebase from trunk", "committedDate": "2020-03-05T02:14:24Z", "type": "commit"}, {"oid": "2502956d38a7762c497e5523833bde763a809afe", "url": "https://github.com/apache/kafka/commit/2502956d38a7762c497e5523833bde763a809afe", "message": "rebase from trunk", "committedDate": "2020-03-06T00:22:34Z", "type": "commit"}, {"oid": "d277f630ce166a318fb83f039763887a32ea5520", "url": "https://github.com/apache/kafka/commit/d277f630ce166a318fb83f039763887a32ea5520", "message": "revert the change to check empty store dirs", "committedDate": "2020-03-06T17:33:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTA0NTg3OQ==", "url": "https://github.com/apache/kafka/pull/8180#discussion_r389045879", "bodyText": "This is an improvement I want to add along with the PR: since we delete the checkpoint file after completed loading, and before we initialize to RESTORING if there's an exception we could lose that checkpoint. So here in Restoring / Created state upon closing I also added the checkpoint logic here.", "author": "guozhangwang", "createdAt": "2020-03-06T17:39:23Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -413,48 +413,50 @@ public void closeDirty() {\n      */\n     private void close(final boolean clean) {\n         if (state() == State.CREATED) {\n-            // the task is created and not initialized, do nothing\n-            transitionTo(State.CLOSING);\n-        } else {\n-            if (state() == State.RUNNING) {\n-                closeTopology(clean);\n+            // the task is created and not initialized, just re-write the checkpoint file\n+            executeAndMaybeSwallow(clean, () -> {", "originalCommit": "d277f630ce166a318fb83f039763887a32ea5520", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTA0NjgxOA==", "url": "https://github.com/apache/kafka/pull/8180#discussion_r389046818", "bodyText": "We could have added the stores, but then before transiting to RESTORING an exception happens; hence here I always call closeStateManager which would just be an no-op if the lock is not grabbed / stores not added.", "author": "guozhangwang", "createdAt": "2020-03-06T17:40:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -413,48 +413,50 @@ public void closeDirty() {\n      */\n     private void close(final boolean clean) {\n         if (state() == State.CREATED) {\n-            // the task is created and not initialized, do nothing\n-            transitionTo(State.CLOSING);\n-        } else {\n-            if (state() == State.RUNNING) {\n-                closeTopology(clean);\n+            // the task is created and not initialized, just re-write the checkpoint file\n+            executeAndMaybeSwallow(clean, () -> {\n+                stateMgr.checkpoint(Collections.emptyMap());\n+            }, \"state manager checkpoint\");\n \n-                if (clean) {\n-                    commitState();\n-                    // whenever we have successfully committed state, it is safe to checkpoint\n-                    // the state as well no matter if EOS is enabled or not\n-                    stateMgr.checkpoint(checkpointableOffsets());\n-                } else {\n-                    executeAndMaybeSwallow(false, stateMgr::flush, \"state manager flush\");\n-                }\n+            transitionTo(State.CLOSING);\n+        } else if (state() == State.RUNNING) {\n+            closeTopology(clean);\n \n-                transitionTo(State.CLOSING);\n-            } else if (state() == State.RESTORING) {\n-                executeAndMaybeSwallow(clean, () -> {\n-                    stateMgr.flush();\n-                    stateMgr.checkpoint(Collections.emptyMap());\n-                }, \"state manager flush and checkpoint\");\n-\n-                transitionTo(State.CLOSING);\n-            } else if (state() == State.SUSPENDED) {\n-                // do not need to commit / checkpoint, since when suspending we've already committed the state\n-                transitionTo(State.CLOSING);\n+            if (clean) {\n+                commitState();\n+                // whenever we have successfully committed state, it is safe to checkpoint\n+                // the state as well no matter if EOS is enabled or not\n+                stateMgr.checkpoint(checkpointableOffsets());\n+            } else {\n+                executeAndMaybeSwallow(false, stateMgr::flush, \"state manager flush\");\n             }\n \n-            if (state() == State.CLOSING) {\n-                // if EOS is enabled, we wipe out the whole state store for unclean close\n-                // since they are invalid to use anymore\n-                final boolean wipeStateStore = !clean && !eosDisabled;\n+            transitionTo(State.CLOSING);\n+        } else if (state() == State.RESTORING) {\n+            executeAndMaybeSwallow(clean, () -> {\n+                stateMgr.flush();\n+                stateMgr.checkpoint(Collections.emptyMap());\n+            }, \"state manager flush and checkpoint\");\n \n-                // first close state manager (which is idempotent) then close the record collector (which could throw),\n-                // if the latter throws and we re-close dirty which would close the state manager again.\n-                StateManagerUtil.closeStateManager(log, logPrefix, clean,\n-                    wipeStateStore, stateMgr, stateDirectory, TaskType.ACTIVE);\n+            transitionTo(State.CLOSING);\n+        } else if (state() == State.SUSPENDED) {\n+            // do not need to commit / checkpoint, since when suspending we've already committed the state\n+            transitionTo(State.CLOSING);\n+        }\n \n-                executeAndMaybeSwallow(clean, recordCollector::close, \"record collector close\");\n-            } else {\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while closing active task \" + id);\n-            }\n+        if (state() == State.CLOSING) {", "originalCommit": "d277f630ce166a318fb83f039763887a32ea5520", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8057d8ea5f095a30fea81ad05789dbbfda044344", "url": "https://github.com/apache/kafka/commit/8057d8ea5f095a30fea81ad05789dbbfda044344", "message": "nit", "committedDate": "2020-03-06T17:43:56Z", "type": "commit"}, {"oid": "24ada914ec418815f4608e50bbe052db2b38a128", "url": "https://github.com/apache/kafka/commit/24ada914ec418815f4608e50bbe052db2b38a128", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-eos-task-corrupted", "committedDate": "2020-03-06T21:55:07Z", "type": "commit"}, {"oid": "59ebd0d40ac5069f9dc1a4f1ccdbc52485595fc8", "url": "https://github.com/apache/kafka/commit/59ebd0d40ac5069f9dc1a4f1ccdbc52485595fc8", "message": "add more unit tests", "committedDate": "2020-03-06T23:12:27Z", "type": "commit"}, {"oid": "d1bea170bbbf209ad20e5493ae832fd247ebfc3d", "url": "https://github.com/apache/kafka/commit/d1bea170bbbf209ad20e5493ae832fd247ebfc3d", "message": "checkstyle", "committedDate": "2020-03-06T23:37:12Z", "type": "commit"}]}