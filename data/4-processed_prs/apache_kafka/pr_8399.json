{"pr_number": 8399, "pr_title": "KAFKA-3720: Change TimeoutException to BufferExhaustedException when no memory can be allocated for a record within max.block.ms", "pr_createdAt": "2020-03-31T22:53:32Z", "pr_url": "https://github.com/apache/kafka/pull/8399", "timeline": [{"oid": "e6fd25ee9378cc47f0cd0e9e5633abb2fbb33fc0", "url": "https://github.com/apache/kafka/commit/e6fd25ee9378cc47f0cd0e9e5633abb2fbb33fc0", "message": "KAFKA-3720: Change TimeoutException to BufferExhaustedException and increase buffer-exhausted-records metric when no memory can be allocated for a record withhin max.block.ms.", "committedDate": "2020-03-31T22:32:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM3NTQ2MQ==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r401375461", "bodyText": "Could you add comment for this inheritance? this change is for keeping compatibility.", "author": "chia7712", "createdAt": "2020-04-01T06:07:45Z", "path": "clients/src/main/java/org/apache/kafka/clients/producer/BufferExhaustedException.java", "diffHunk": "@@ -16,13 +16,13 @@\n  */\n package org.apache.kafka.clients.producer;\n \n-import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.errors.TimeoutException;\n \n /**\n  * This exception is thrown if the producer is in non-blocking mode and the rate of data production exceeds the rate at\n  * which data can be sent for long enough for the allocated buffer to be exhausted.\n  */\n-public class BufferExhaustedException extends KafkaException {\n+public class BufferExhaustedException extends TimeoutException {", "originalCommit": "e6fd25ee9378cc47f0cd0e9e5633abb2fbb33fc0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTQ2MDE5MQ==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r401460191", "bodyText": "Done", "author": "soenkeliebau", "createdAt": "2020-04-01T08:59:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM3NTQ2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTYxNTE0NA==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r401615144", "bodyText": "Is this for backwards compatibility? It seems to me that part of the reason is to allow the users to handle all timeout related type of exceptions with one catch clause.", "author": "ijuma", "createdAt": "2020-04-01T13:29:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM3NTQ2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTYxNzQyNg==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r401617426", "bodyText": "Is this for backwards compatibility?\n\nthe client code catching TimeoutException to handle memory issue will be broken if BufferExhaustedException does not extend TimeoutException.", "author": "chia7712", "createdAt": "2020-04-01T13:32:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM3NTQ2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTYxOTYyNA==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r401619624", "bodyText": "It is a bit hen and egg I guess. The main reason for adding it in this case was compatibility (at least to my mind), but what you say is a very welcome side effect.\nHappy to add a clarification to the comment of course if we feel this makes sense.", "author": "soenkeliebau", "createdAt": "2020-04-01T13:35:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTM3NTQ2MQ=="}], "type": "inlineReview"}, {"oid": "928b45d326e9e5bedc0efed48cb2fa905adef8d8", "url": "https://github.com/apache/kafka/commit/928b45d326e9e5bedc0efed48cb2fa905adef8d8", "message": "Added comment on why BufferExhaustedException subclasses TimeoutException.", "committedDate": "2020-04-01T08:32:28Z", "type": "commit"}, {"oid": "714731e8d401ef2f1bc12952a5856a534d3d7089", "url": "https://github.com/apache/kafka/commit/714731e8d401ef2f1bc12952a5856a534d3d7089", "message": "Fixed broken test. Had to refactor slightly, as verifySendFailure was used to test two different failure scenarios that both used to throw TimeoutException, but now one of them throws BufferExhaustedException.", "committedDate": "2020-04-01T11:22:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY3MDk1NA==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r402670954", "bodyText": "Could we do this inside RecordAccumulator, which is where the \"buffer-exhausted-records\" sensor is defined?", "author": "junrao", "createdAt": "2020-04-03T00:28:13Z", "path": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java", "diffHunk": "@@ -947,6 +947,8 @@ private void throwIfProducerClosed() {\n             // for other exceptions throw directly\n         } catch (ApiException e) {\n             log.debug(\"Exception occurred during message send:\", e);\n+            if (e instanceof BufferExhaustedException)\n+                this.metrics.sensor(\"buffer-exhausted-records\").record();", "originalCommit": "714731e8d401ef2f1bc12952a5856a534d3d7089", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc5OTIzOA==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r402799238", "bodyText": "@junrao , counter-proposal, if I may: can we maybe move the sensor definition to BufferPool and record the metric there, as that is where the exception is originally thrown.\nIn RecordsAccumulator we would have to define a new catch block just for this purpose, whereas BufferPool has a dedicated conditional ready to use.", "author": "soenkeliebau", "createdAt": "2020-04-03T07:52:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY3MDk1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEwODM2OA==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r403108368", "bodyText": "Yes, that seems reasonable.", "author": "junrao", "createdAt": "2020-04-03T16:02:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY3MDk1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM2Njk3NA==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r403366974", "bodyText": "I've moved things around to this effect.", "author": "soenkeliebau", "createdAt": "2020-04-03T22:30:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY3MDk1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTMwMTYxNg==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r405301616", "bodyText": "ping @junrao", "author": "soenkeliebau", "createdAt": "2020-04-08T07:05:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY3MDk1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY5MTQ3NQ==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r402691475", "bodyText": "It's fine to keep compatibility for now. However, I do wonder if we should just throw ApiException to the caller since the producer can block for max.block.ms. This needs a KIP discussion.", "author": "junrao", "createdAt": "2020-04-03T01:44:55Z", "path": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java", "diffHunk": "@@ -947,6 +947,8 @@ private void throwIfProducerClosed() {\n             // for other exceptions throw directly\n         } catch (ApiException e) {\n             log.debug(\"Exception occurred during message send:\", e);\n+            if (e instanceof BufferExhaustedException)", "originalCommit": "714731e8d401ef2f1bc12952a5856a534d3d7089", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc5OTgzOA==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r402799838", "bodyText": "I haven't looked at it, but do all ApiExceptions that may be caught here observe max.block.ms?\nOr do you mean just throw ApiException in case the buffer is exhausted?", "author": "soenkeliebau", "createdAt": "2020-04-03T07:53:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY5MTQ3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEwOTgxMw==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r403109813", "bodyText": "I was thinking that all retriable errors should observe max.block.ms if not already. For non-retirable errors, just throw the exception immediately.", "author": "junrao", "createdAt": "2020-04-03T16:04:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY5MTQ3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM2Njg3MQ==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r403366871", "bodyText": "I'll do some digging on this topic. But if I am not misunderstanding you, we can handle that in a separate change and merge this for now, right?", "author": "soenkeliebau", "createdAt": "2020-04-03T22:30:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY5MTQ3NQ=="}], "type": "inlineReview"}, {"oid": "8fba83628f3c0f1f094d11ec2f8dfa38d0f7c615", "url": "https://github.com/apache/kafka/commit/8fba83628f3c0f1f094d11ec2f8dfa38d0f7c615", "message": "Addressed comments from Jun: moved metrics recording to BufferPool, where the exception originates.\nReordered imports in PlaintextProducerSendTest.", "committedDate": "2020-04-03T22:26:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MzQzMg==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r405893432", "bodyText": "Since we mark the sensor here, there seems to be no need for the caller to know this is a BufferExhaustedException. Could we just throw TimeoutException and get rid of BufferExhaustedException?", "author": "junrao", "createdAt": "2020-04-09T00:41:46Z", "path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/BufferPool.java", "diffHunk": "@@ -151,7 +157,8 @@ public ByteBuffer allocate(int size, long maxTimeToBlockMs) throws InterruptedEx\n                             throw new KafkaException(\"Producer closed while allocating memory\");\n \n                         if (waitingTimeElapsed) {\n-                            throw new TimeoutException(\"Failed to allocate memory within the configured max blocking time \" + maxTimeToBlockMs + \" ms.\");\n+                            this.metrics.sensor(\"buffer-exhausted-records\").record();\n+                            throw new BufferExhaustedException(\"Failed to allocate memory within the configured max blocking time \" + maxTimeToBlockMs + \" ms.\");", "originalCommit": "8fba83628f3c0f1f094d11ec2f8dfa38d0f7c615", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjUxNDEwMw==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r406514103", "bodyText": "We could absolutely do that.\nI'd say that depends on whether it would be useful for code that uses the producer to distinguish between a timeout during metadata refresh and a timeout waiting for the buffer to free up enough space.\nPersonally I see no harm in allowing this distinction and since BufferExhaustedException extends TimeoutException no one is forced to do this, they can just catch both in one try block.\nOn the other hand I also see no huge benefit, as you would not treat these two any differently in code - your record wasn't sent, you either do something about it or ignore it. Unless you want to dynamically reconfigure your producer with a larger buffer when this occurred I don't see a real use-case. Maybe someone else can come up with one?", "author": "soenkeliebau", "createdAt": "2020-04-09T22:34:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MzQzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2NjA2Mw==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r406966063", "bodyText": "Ok, we can keep this as its.", "author": "junrao", "createdAt": "2020-04-10T22:17:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MzQzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2NDY3MA==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r406964670", "bodyText": "This waits on real Timer. So waiting 2 secs in a unit test is too long. Perhaps try 10ms?", "author": "junrao", "createdAt": "2020-04-10T22:12:23Z", "path": "clients/src/test/java/org/apache/kafka/clients/producer/internals/BufferPoolTest.java", "diffHunk": "@@ -152,8 +152,18 @@ private CountDownLatch asyncAllocate(final BufferPool pool, final int size) {\n     }\n \n     /**\n-     * Test if Timeout exception is thrown when there is not enough memory to allocate and the elapsed time is greater than the max specified block time.\n-     * And verify that the allocation attempt finishes soon after the maxBlockTimeMs.\n+     * Test if BufferExhausted exception is thrown when there is not enough memory to allocate and the elapsed\n+     * time is greater than the max specified block time.\n+     */\n+    @Test(expected = BufferExhaustedException.class)\n+    public void testBufferExhaustedExceptionIsThrown() throws Exception {\n+        BufferPool pool = new BufferPool(2, 1, metrics, time, metricGroup);\n+        pool.allocate(1, maxBlockTimeMs);\n+        pool.allocate(2, maxBlockTimeMs);", "originalCommit": "8fba83628f3c0f1f094d11ec2f8dfa38d0f7c615", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk4MTY2MA==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r406981660", "bodyText": "Happy to do that of course. The maxBlockTimeMs affects the entire class of tests though, I'd like to move that to a separate PR to make the change easier to trace in case tests become unstable due to this.\nI ran a couple thousand tests with different values and for me testBlockTimeout became unstable with a value of 10ms - afaik can tell we are betting on a race condition in line 188 . We allocate three bytes, start delayed deallocations, wait a little and then hope that at least one deallocation took place by the time we check. Which worked for 2000 ms, but apparently breaks sometimes for 10 ms.\nIt is an easy fix by changing the condition to 7 instead of 8, but I'm not sure how much actual worth that assertion has after that.\nHappy to discuss this further, but maybe we can first agree on if a new PR makes sense. I think it makes sense to separate this out tbh.", "author": "soenkeliebau", "createdAt": "2020-04-10T23:26:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2NDY3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2NDk2Mg==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r406964962", "bodyText": "Similar to the above, perhaps reduce maxBlockTimeMs to 10ms?", "author": "junrao", "createdAt": "2020-04-10T22:13:32Z", "path": "clients/src/test/java/org/apache/kafka/clients/producer/internals/BufferPoolTest.java", "diffHunk": "@@ -171,14 +181,14 @@ public void testBlockTimeout() throws Exception {\n         try {\n             pool.allocate(10, maxBlockTimeMs);\n             fail(\"The buffer allocated more memory than its maximum value 10\");\n-        } catch (TimeoutException e) {\n+        } catch (BufferExhaustedException e) {", "originalCommit": "8fba83628f3c0f1f094d11ec2f8dfa38d0f7c615", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk4MjA1Ng==", "url": "https://github.com/apache/kafka/pull/8399#discussion_r406982056", "bodyText": "thanks @junrao - I answered in your related comment above.", "author": "soenkeliebau", "createdAt": "2020-04-10T23:27:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk2NDk2Mg=="}], "type": "inlineReview"}]}