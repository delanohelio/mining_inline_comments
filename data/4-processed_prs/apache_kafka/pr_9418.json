{"pr_number": 9418, "pr_title": "KAFKA-10601; Add support for append linger to Raft implementation", "pr_createdAt": "2020-10-12T23:06:10Z", "pr_url": "https://github.com/apache/kafka/pull/9418", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0NDEyOA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r504344128", "bodyText": "fix typo", "author": "hachikuji", "createdAt": "2020-10-14T01:13:48Z", "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "diffHunk": "@@ -19,33 +19,51 @@\n import org.apache.kafka.common.record.Records;\n \n import java.io.IOException;\n+import java.util.List;\n import java.util.concurrent.CompletableFuture;\n \n-public interface RaftClient {\n+public interface RaftClient<T> {\n+\n+    interface Listener<T> {\n+        /**\n+         * Callback which is invoked when records written through {@link #scheduleAppend(int, List)}\n+         * become committed.\n+         *\n+         * Note that there is not a one-to-one correspondence between writes through\n+         * {@link #scheduleAppend(int, List)} and this callback. The Raft implementation\n+         * is free to batch together the records from multiple append calls provided\n+         * that batch boundaries are respected. This means that each batch specified\n+         * through {@link #scheduleAppend(int, List)} is guaranteed to be a subset of\n+         * a batch passed to {@link #handleCommit(int, long, List)}.\n+         *\n+         * @param epoch the epoch in which the write was accepted\n+         * @param lastOffset the last offset of the recor", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTAzNjA2Mw==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r505036063", "bodyText": "How about either\nOptionalLong scheduleAppend(int epoch, List<T> records);\n\nor\nvoid scheduleAppend(int epoch, List<T> records) throws BusyException;\n\nI am okay with either solution but I am wondering why did you decide to return a null for this case instead of throwing some exception?", "author": "jsancio", "createdAt": "2020-10-14T22:45:22Z", "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "diffHunk": "@@ -19,33 +19,51 @@\n import org.apache.kafka.common.record.Records;\n \n import java.io.IOException;\n+import java.util.List;\n import java.util.concurrent.CompletableFuture;\n \n-public interface RaftClient {\n+public interface RaftClient<T> {\n+\n+    interface Listener<T> {\n+        /**\n+         * Callback which is invoked when records written through {@link #scheduleAppend(int, List)}\n+         * become committed.\n+         *\n+         * Note that there is not a one-to-one correspondence between writes through\n+         * {@link #scheduleAppend(int, List)} and this callback. The Raft implementation\n+         * is free to batch together the records from multiple append calls provided\n+         * that batch boundaries are respected. This means that each batch specified\n+         * through {@link #scheduleAppend(int, List)} is guaranteed to be a subset of\n+         * a batch passed to {@link #handleCommit(int, long, List)}.\n+         *\n+         * @param epoch the epoch in which the write was accepted\n+         * @param lastOffset the offset of the last record in the record list\n+         * @param records the set of records that were committed\n+         */\n+        void handleCommit(int epoch, long lastOffset, List<T> records);\n+    }\n \n     /**\n      * Initialize the client. This should only be called once and it must be\n      * called before any of the other APIs can be invoked.\n      *\n      * @throws IOException For any IO errors during initialization\n      */\n-    void initialize() throws IOException;\n+    void initialize(Listener<T> listener) throws IOException;\n \n     /**\n-     * Append a new entry to the log. The client must be in the leader state to\n-     * accept an append: it is up to the state machine implementation\n-     * to ensure this using {@link #currentLeaderAndEpoch()}.\n-     *\n-     * TODO: One improvement we can make here is to allow the caller to specify\n-     * the current leader epoch in the record set. That would ensure that each\n-     * leader change must be \"observed\" by the state machine before new appends\n-     * are accepted.\n+     * Append a list of records to the log. The write will be scheduled for some time\n+     * in the future. There is no guarantee that appended records will be written to\n+     * the log and eventually committed. However, it is guaranteed that if any of the\n+     * records become committed, then all of them will be.\n      *\n-     * @param records The records to append to the log\n-     * @param timeoutMs Maximum time to wait for the append to complete\n-     * @return A future containing the last offset and epoch of the appended records (if successful)\n+     * @param epoch the current leader epoch\n+     * @param records the list of records to append\n+     * @return the offset within the current epoch that the log entries will be appended,\n+     *         or null if the leader was unable to accept the write (e.g. due to memory\n+     *         being reached).\n      */\n-    CompletableFuture<OffsetAndEpoch> append(Records records, AckMode ackMode, long timeoutMs);\n+    Long scheduleAppend(int epoch, List<T> records);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjYzNjIyNw==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r506636227", "bodyText": "Yeah, this was more of a workaround until we thought of something better. The current idea we are thinking about is letting the Raft layer return a backoff time, so the type would semantically be Either[Offset, BackoffMs].", "author": "hachikuji", "createdAt": "2020-10-16T17:55:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTAzNjA2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTAzOTY3Ng==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r505039676", "bodyText": "I vote yes for this.\nI think if we use this for writing snapshot from the state machine, then minimum size is a more interesting metrics for flushing to disk vs lingerMs.\nIf we implement this so that either one has to be true then the client can set the lingerMs or minSize to MAX_VALUE if it wants to ignore those values.", "author": "jsancio", "createdAt": "2020-10-14T22:50:11Z", "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.common.utils.Timer;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.Closeable;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * TODO: Also flush after minimum size limit is reached?", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjYzODEwNQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r506638105", "bodyText": "Yeah, I think we should do it. I will create a JIRA and leave this for a follow-up.", "author": "hachikuji", "createdAt": "2020-10-16T17:58:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTAzOTY3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMDgxMA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508910810", "bodyText": "nit: not clear why we want an intermediate class instead of just having a DataOutputStreamWritable class and its constructor parameter declared as DataOutputStream?", "author": "guozhangwang", "createdAt": "2020-10-20T23:59:45Z", "path": "clients/src/main/java/org/apache/kafka/common/protocol/DataOutputWritable.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.common.protocol;\n+\n+import org.apache.kafka.common.utils.ByteUtils;\n+import org.apache.kafka.common.utils.Utils;\n+\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+public class DataOutputWritable implements Writable {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYwMzExMA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r509603110", "bodyText": "Fair enough. We have no current need for DataOutputWritable, so I will remove this.", "author": "hachikuji", "createdAt": "2020-10-21T19:18:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMDgxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMTYxOQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508911619", "bodyText": "I vaguely remember @rajinisivaram has another PR which may be overlapping with this fix, will let her to check and confirm if it is fine.", "author": "guozhangwang", "createdAt": "2020-10-21T00:02:16Z", "path": "core/src/main/scala/kafka/common/RecordValidationException.scala", "diffHunk": "@@ -23,5 +23,6 @@ import org.apache.kafka.common.requests.ProduceResponse.RecordError\n import scala.collection.Seq\n \n class RecordValidationException(val invalidException: ApiException,\n-                                val recordErrors: Seq[RecordError]) extends RuntimeException {\n+                                val recordErrors: Seq[RecordError])\n+  extends RuntimeException(invalidException) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTc0MDE1MA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r509740150", "bodyText": "Yeah, doesn't need to be done here. I just noticed the trace was missing when debugging a failure.", "author": "hachikuji", "createdAt": "2020-10-21T22:02:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMTYxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTg0Mjc4Mw==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511842783", "bodyText": "I had only changed client-side code, so changes here wouldn't have conflicted.", "author": "rajinisivaram", "createdAt": "2020-10-26T10:02:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMTYxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMzA4Nw==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508913087", "bodyText": "nit: maybe add a comment indicating this function is for testing only (and hence we do not care about the correlation id).", "author": "guozhangwang", "createdAt": "2020-10-21T00:07:48Z", "path": "core/src/main/scala/kafka/raft/KafkaNetworkChannel.scala", "diffHunk": "@@ -216,11 +216,9 @@ class KafkaNetworkChannel(time: Time,\n     endpoints.put(id, node)\n   }\n \n-  def postInboundRequest(header: RequestHeader,\n-                         request: AbstractRequest,\n-                         onResponseReceived: ResponseHandler): Unit = {\n+  def postInboundRequest(request: AbstractRequest, onResponseReceived: ResponseHandler): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTczMjIzNg==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r509732236", "bodyText": "This was actually a bug. The implementation was treating correlationId as unique across all connections, which of course was wrong. My fix was just to overwrite the correlationId from the header with one that could be unique, but obviously this loses traceability through the Raft layer. If it's ok with you, I'd like to address this problem more generally in a follow-up.", "author": "hachikuji", "createdAt": "2020-10-21T21:53:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMzA4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTA4NDk1Ng==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511084956", "bodyText": "Currently, postInboundRequest in only used by TestRaftRequestHandler and KafkaNetworkChannelTest. Are you thinking that we will use the same or similar method when integrating with the broker code?", "author": "jsancio", "createdAt": "2020-10-23T18:57:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMzA4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTA4NTY3OA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511085678", "bodyText": "Unrelated with this change but when reading the code. I was confused by the name pollInboundResponses. That function returns both pending request and responses.", "author": "jsancio", "createdAt": "2020-10-23T18:59:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMzA4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjE4OTczMQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r512189731", "bodyText": "Unrelated with this change but when reading the code. I was confused by the name pollInboundResponses. That function returns both pending request and responses.\n\nI don't think that's right. It only delivers responses for outbound requests (in other words, inbound responses).\nIn regard to the confusion, I took an unconventional turn in the beginning by consolidating inbound and outbound requests through a single API. This was useful because it allowed inputs and outputs to be serialized, which made it easier to have deterministic simulation tests. The main defect from the perspective of Raft is that fetch requests cannot be handled concurrently. I think we are going to have to address this problem in the long term, but it is outside the scope of this PR.", "author": "hachikuji", "createdAt": "2020-10-26T18:44:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMzA4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjE5NTYxNg==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r512195616", "bodyText": "I don't think that's right. It only delivers responses for outbound requests (in other words, inbound responses).\n\nHmm. I misspoke a bit. I should have said that pollInboundResponses returns both inbound requests and responses. These are my observations:\npostInboundRequest puts requests in undelivered:\nhttps://github.com/apache/kafka/pull/9418/files#diff-c9b2a05e69be1d2391d91a33f408eab4fde1245d4ea9cf9f52305c8cb89208ceR224\npollInboundResponses returns everything in undelivered:\nhttps://github.com/apache/kafka/pull/9418/files#diff-c9b2a05e69be1d2391d91a33f408eab4fde1245d4ea9cf9f52305c8cb89208ceR201-R202\nThe raft client gets all of the inbound messages (requests and reponses) by calling receive which in term calls pollInboundResponses:\nhttps://github.com/apache/kafka/pull/9418/files#diff-1da15c51e641ea46ea5c86201ab8f21cfee9e7c575102a39c7bae0d5ffd7de39L1596", "author": "jsancio", "createdAt": "2020-10-26T18:54:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMzA4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjE5Njk2NA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r512196964", "bodyText": "My mistake. This was being used for both inbound requests and responses. Let me change the name and add some docs.", "author": "hachikuji", "createdAt": "2020-10-26T18:56:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMzA4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMzk1MQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508913951", "bodyText": "Could you elaborate a bit why we would not expect these api keys in the test handler any more?", "author": "guozhangwang", "createdAt": "2020-10-21T00:10:43Z", "path": "core/src/main/scala/kafka/tools/TestRaftRequestHandler.scala", "diffHunk": "@@ -56,73 +47,8 @@ class TestRaftRequestHandler(\n              | ApiKeys.END_QUORUM_EPOCH\n              | ApiKeys.FETCH =>\n           val requestBody = request.body[AbstractRequest]\n-          networkChannel.postInboundRequest(\n-            request.header,\n-            requestBody,\n-            response => sendResponse(request, Some(response)))\n-\n-        case ApiKeys.API_VERSIONS =>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYxNTY4MQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r509615681", "bodyText": "I felt it was too difficult to approximate a controller workload using one or more producers because of the fact that we can only handle one request at a time. So I created a separate workload generator which executes on the leader and I removed all of this somewhat hacky handling logic which allowed us to use a producer.", "author": "hachikuji", "createdAt": "2020-10-21T19:37:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxMzk1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxNTIzNg==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508915236", "bodyText": "If offset == Long.MaxValue would sleeping save us anything? Should we terminate the testing server then?", "author": "guozhangwang", "createdAt": "2020-10-21T00:15:11Z", "path": "core/src/main/scala/kafka/tools/TestRaftServer.scala", "diffHunk": "@@ -272,7 +291,79 @@ class TestRaftServer(val config: KafkaConfig) extends Logging {\n     )\n   }\n \n-  class RaftIoThread(client: KafkaRaftClient) extends ShutdownableThread(\"raft-io-thread\") {\n+  class RaftWorkloadGenerator(\n+    client: KafkaRaftClient[Array[Byte]],\n+    time: Time,\n+    brokerId: Int,\n+    recordsPerSec: Int,\n+    recordSize: Int\n+  ) extends ShutdownableThread(name = \"raft-workload-generator\") with RaftClient.Listener[Array[Byte]] {\n+\n+    private val stats = new WriteStats(time, printIntervalMs = 5000)\n+    private val payload = new Array[Byte](recordSize)\n+    private val pendingAppends = new util.ArrayDeque[PendingAppend]()\n+\n+    private var latestLeaderAndEpoch = new LeaderAndEpoch(OptionalInt.empty, 0)\n+    private var isLeader = false\n+    private var throttler: ThroughputThrottler = _\n+    private var recordCount = 0\n+\n+    override def doWork(): Unit = {\n+      if (latestLeaderAndEpoch != client.currentLeaderAndEpoch()) {\n+        latestLeaderAndEpoch = client.currentLeaderAndEpoch()\n+        isLeader = latestLeaderAndEpoch.leaderId.orElse(-1) == brokerId\n+        if (isLeader) {\n+          pendingAppends.clear()\n+          throttler = new ThroughputThrottler(time, recordsPerSec)\n+          recordCount = 0\n+        }\n+      }\n+\n+      if (isLeader) {\n+        recordCount += 1\n+\n+        val startTimeMs = time.milliseconds()\n+        val sendTimeMs = if (throttler.maybeThrottle(recordCount, startTimeMs)) {\n+          time.milliseconds()\n+        } else {\n+          startTimeMs\n+        }\n+\n+        val offset = client.scheduleAppend(latestLeaderAndEpoch.epoch, Collections.singletonList(payload))\n+        if (offset == null || offset == Long.MaxValue) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYyMDc5NQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r509620795", "bodyText": "I will try to document this better, but Long.MaxValue is how we decided to handle the case where the epoch in scheduleAppend does not match the current epoch. This can happen because the raft epoch is updated asynchronously and there is no way to ensure the state machine has seen the latest value. The expectation is that the state machine will update its uncommitted state with an offset which is impossible to become committed. After it observes the epoch change, this uncommitted state will be discarded.\nNote that although I added the explicit check here, it is not technically necessary. Let me consider removing it.", "author": "hachikuji", "createdAt": "2020-10-21T19:44:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxNTIzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxNjAwMw==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508916003", "bodyText": "Should this ever happen? If not, then we could shutdown the server after logging the error.", "author": "guozhangwang", "createdAt": "2020-10-21T00:17:55Z", "path": "core/src/main/scala/kafka/tools/TestRaftServer.scala", "diffHunk": "@@ -272,7 +291,79 @@ class TestRaftServer(val config: KafkaConfig) extends Logging {\n     )\n   }\n \n-  class RaftIoThread(client: KafkaRaftClient) extends ShutdownableThread(\"raft-io-thread\") {\n+  class RaftWorkloadGenerator(\n+    client: KafkaRaftClient[Array[Byte]],\n+    time: Time,\n+    brokerId: Int,\n+    recordsPerSec: Int,\n+    recordSize: Int\n+  ) extends ShutdownableThread(name = \"raft-workload-generator\") with RaftClient.Listener[Array[Byte]] {\n+\n+    private val stats = new WriteStats(time, printIntervalMs = 5000)\n+    private val payload = new Array[Byte](recordSize)\n+    private val pendingAppends = new util.ArrayDeque[PendingAppend]()\n+\n+    private var latestLeaderAndEpoch = new LeaderAndEpoch(OptionalInt.empty, 0)\n+    private var isLeader = false\n+    private var throttler: ThroughputThrottler = _\n+    private var recordCount = 0\n+\n+    override def doWork(): Unit = {\n+      if (latestLeaderAndEpoch != client.currentLeaderAndEpoch()) {\n+        latestLeaderAndEpoch = client.currentLeaderAndEpoch()\n+        isLeader = latestLeaderAndEpoch.leaderId.orElse(-1) == brokerId\n+        if (isLeader) {\n+          pendingAppends.clear()\n+          throttler = new ThroughputThrottler(time, recordsPerSec)\n+          recordCount = 0\n+        }\n+      }\n+\n+      if (isLeader) {\n+        recordCount += 1\n+\n+        val startTimeMs = time.milliseconds()\n+        val sendTimeMs = if (throttler.maybeThrottle(recordCount, startTimeMs)) {\n+          time.milliseconds()\n+        } else {\n+          startTimeMs\n+        }\n+\n+        val offset = client.scheduleAppend(latestLeaderAndEpoch.epoch, Collections.singletonList(payload))\n+        if (offset == null || offset == Long.MaxValue) {\n+          time.sleep(10)\n+        } else {\n+          pendingAppends.offer(PendingAppend(latestLeaderAndEpoch.epoch, offset, sendTimeMs))\n+        }\n+      } else {\n+        time.sleep(500)\n+      }\n+    }\n+\n+    override def handleCommit(epoch: Int, lastOffset: Long, records: util.List[Array[Byte]]): Unit = {\n+      var offset = lastOffset - records.size() + 1\n+      val currentTimeMs = time.milliseconds()\n+\n+      for (record <- records.asScala) {\n+        val pendingAppend = pendingAppends.poll()\n+        if (pendingAppend.epoch != epoch || pendingAppend.offset!= offset) {\n+          warn(s\"Expected next commit at offset ${pendingAppend.offset}, \" +", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYzMzUyNw==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r509633527", "bodyText": "That's true. In a follow-up, the handleCommit API will be expanded a bit to cover appends through replication as well, but for now, I think we can raise an error.", "author": "hachikuji", "createdAt": "2020-10-21T19:58:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxNjAwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxNjY1Nw==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508916657", "bodyText": "Why we need to import the inner classes?", "author": "guozhangwang", "createdAt": "2020-10-21T00:20:11Z", "path": "core/src/main/scala/kafka/tools/TestRaftServer.scala", "diffHunk": "@@ -19,37 +19,44 @@ package kafka.tools\n \n import java.io.File\n import java.nio.file.Files\n-import java.util.concurrent.CountDownLatch\n-import java.util.{Properties, Random}\n+import java.util\n+import java.util.concurrent.{CountDownLatch, TimeUnit}\n+import java.util.{Collections, OptionalInt, Random}\n \n-import joptsimple.OptionParser\n+import com.yammer.metrics.core.MetricName\n+import joptsimple.OptionException\n import kafka.log.{Log, LogConfig, LogManager}\n import kafka.network.SocketServer\n import kafka.raft.{KafkaFuturePurgatory, KafkaMetadataLog, KafkaNetworkChannel}\n import kafka.security.CredentialProvider\n import kafka.server.{BrokerTopicStats, KafkaConfig, KafkaRequestHandlerPool, KafkaServer, LogDirFailureChannel}\n+import kafka.tools.TestRaftServer.{ByteArraySerde, PendingAppend, ThroughputThrottler, WriteStats}", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYzNjM1Nw==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r509636357", "bodyText": "Moved the import inside TestRaftServer.", "author": "hachikuji", "createdAt": "2020-10-21T20:00:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxNjY1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxODE5MA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508918190", "bodyText": "Even if the leader has changed back, previously pending appends may still get committed right? I think it is sufficient to just poll-and-drop them in handleCommit when leader changes and not clearing the queue here.", "author": "guozhangwang", "createdAt": "2020-10-21T00:25:44Z", "path": "core/src/main/scala/kafka/tools/TestRaftServer.scala", "diffHunk": "@@ -272,7 +291,79 @@ class TestRaftServer(val config: KafkaConfig) extends Logging {\n     )\n   }\n \n-  class RaftIoThread(client: KafkaRaftClient) extends ShutdownableThread(\"raft-io-thread\") {\n+  class RaftWorkloadGenerator(\n+    client: KafkaRaftClient[Array[Byte]],\n+    time: Time,\n+    brokerId: Int,\n+    recordsPerSec: Int,\n+    recordSize: Int\n+  ) extends ShutdownableThread(name = \"raft-workload-generator\") with RaftClient.Listener[Array[Byte]] {\n+\n+    private val stats = new WriteStats(time, printIntervalMs = 5000)\n+    private val payload = new Array[Byte](recordSize)\n+    private val pendingAppends = new util.ArrayDeque[PendingAppend]()\n+\n+    private var latestLeaderAndEpoch = new LeaderAndEpoch(OptionalInt.empty, 0)\n+    private var isLeader = false\n+    private var throttler: ThroughputThrottler = _\n+    private var recordCount = 0\n+\n+    override def doWork(): Unit = {\n+      if (latestLeaderAndEpoch != client.currentLeaderAndEpoch()) {\n+        latestLeaderAndEpoch = client.currentLeaderAndEpoch()\n+        isLeader = latestLeaderAndEpoch.leaderId.orElse(-1) == brokerId\n+        if (isLeader) {\n+          pendingAppends.clear()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTcxMTMwMg==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r509711302", "bodyText": "You are right that the appends may still be committed, but in this patch, the handleCommit API is only invoked for appends within the current epoch. I thought it seemed simpler for now to just reset state at the start of the epoch. We can be more clever in the future once handleCommit is extended to account for replication.", "author": "hachikuji", "createdAt": "2020-10-21T21:24:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxODE5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkxOTMzNg==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508919336", "bodyText": "nit: appendLingerMs?", "author": "guozhangwang", "createdAt": "2020-10-21T00:29:49Z", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -148,36 +162,44 @@ public KafkaRaftClient(RaftConfig raftConfig,\n             raftConfig.retryBackoffMs(),\n             raftConfig.requestTimeoutMs(),\n             1000,\n+            raftConfig.lingerMs(),\n             logContext,\n             new Random());\n     }\n \n-    public KafkaRaftClient(NetworkChannel channel,\n-                           ReplicatedLog log,\n-                           QuorumState quorum,\n-                           Time time,\n-                           Metrics metrics,\n-                           FuturePurgatory<LogOffset> fetchPurgatory,\n-                           FuturePurgatory<LogOffset> appendPurgatory,\n-                           Map<Integer, InetSocketAddress> voterAddresses,\n-                           int electionBackoffMaxMs,\n-                           int retryBackoffMs,\n-                           int requestTimeoutMs,\n-                           int fetchMaxWaitMs,\n-                           LogContext logContext,\n-                           Random random) {\n+    public KafkaRaftClient(\n+        RecordSerde<T> serde,\n+        NetworkChannel channel,\n+        ReplicatedLog log,\n+        QuorumState quorum,\n+        MemoryPool memoryPool,\n+        Time time,\n+        Metrics metrics,\n+        FuturePurgatory<LogOffset> fetchPurgatory,\n+        FuturePurgatory<LogOffset> appendPurgatory,\n+        Map<Integer, InetSocketAddress> voterAddresses,\n+        int electionBackoffMaxMs,\n+        int retryBackoffMs,\n+        int requestTimeoutMs,\n+        int fetchMaxWaitMs,\n+        int lingerMs,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyMTQ2NQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508921465", "bodyText": "For now since we only have a single thread processing all incoming req/resp, this is okay; but when we multi-thread processing requests this would no longer be safe, since it is possible that some batches gets replicated and committed while not being flushed locally yet.", "author": "guozhangwang", "createdAt": "2020-10-21T00:37:10Z", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1443,15 +1485,79 @@ private void pollShutdown(GracefulShutdown shutdown) throws IOException {\n         }\n     }\n \n+    private void appendBatch(\n+        LeaderState state,\n+        BatchAccumulator.CompletedBatch<T> batch,\n+        long appendTimeMs\n+    ) {\n+        try {\n+            List<T> records = batch.records;\n+            int epoch = state.epoch();\n+\n+            LogAppendInfo info = appendAsLeader(batch.data);\n+            OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(info.lastOffset, epoch);\n+            CompletableFuture<Long> future = appendPurgatory.await(\n+                LogOffset.awaitCommitted(offsetAndEpoch.offset),\n+                Integer.MAX_VALUE\n+            );\n+\n+            future.whenComplete((commitTimeMs, exception) -> {\n+                int numRecords = batch.records.size();\n+                if (exception != null) {\n+                    logger.debug(\"Failed to commit {} records at {}\", numRecords, offsetAndEpoch, exception);\n+                } else {\n+                    long elapsedTime = Math.max(0, commitTimeMs - appendTimeMs);\n+                    double elapsedTimePerRecord = (double) elapsedTime / numRecords;\n+                    kafkaRaftMetrics.updateCommitLatency(elapsedTimePerRecord, appendTimeMs);\n+                    logger.debug(\"Completed commit of {} records at {}\", numRecords, offsetAndEpoch);\n+                    listener.handleCommit(epoch, info.lastOffset, records);\n+                }\n+            });\n+        } finally {\n+            batch.release();\n+        }\n+    }\n+\n+    private long maybeAppendBatches(\n+        LeaderState state,\n+        long currentTimeMs\n+    ) {\n+        long timeUnitFlush = accumulator.timeUntilFlush(currentTimeMs);\n+        if (timeUnitFlush <= 0) {\n+            List<BatchAccumulator.CompletedBatch<T>> batches = accumulator.flush();\n+            Iterator<BatchAccumulator.CompletedBatch<T>> iterator = batches.iterator();\n+\n+            try {\n+                while (iterator.hasNext()) {\n+                    BatchAccumulator.CompletedBatch<T> batch = iterator.next();\n+                    appendBatch(state, batch, currentTimeMs);\n+                }\n+                flushLeaderLog(state, currentTimeMs);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTcxNDk0OQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r509714949", "bodyText": "Yes, I agree with you. Of course it is ok if unflushed data gets replicated. The main thing we need to protect is incrementing the high watermark.", "author": "hachikuji", "createdAt": "2020-10-21T21:29:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyMTQ2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyMjA2NA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508922064", "bodyText": "Why return MAX_VALUE instead of null here? If we want to use null to indicate memory full and use MAX_VALUE to indicate not leader, the javadoc should reflecting this.\nAnyways, I think returning sth like a combo(Offset, ErrorCode, backoffMs) would be preferred in the end state.", "author": "guozhangwang", "createdAt": "2020-10-21T00:39:31Z", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1605,100 +1708,18 @@ public void poll() throws IOException {\n         }\n     }\n \n-    private void failPendingAppends(KafkaException exception) {\n-        for (UnwrittenAppend unwrittenAppend : unwrittenAppends) {\n-            unwrittenAppend.fail(exception);\n-        }\n-        unwrittenAppends.clear();\n-    }\n-\n-    private void pollPendingAppends(LeaderState state, long currentTimeMs) {\n-        int numAppends = 0;\n-        int maxNumAppends = unwrittenAppends.size();\n-\n-        while (!unwrittenAppends.isEmpty() && numAppends < maxNumAppends) {\n-            final UnwrittenAppend unwrittenAppend = unwrittenAppends.poll();\n-\n-            if (unwrittenAppend.future.isDone())\n-                continue;\n-\n-            if (unwrittenAppend.isTimedOut(currentTimeMs)) {\n-                unwrittenAppend.fail(new TimeoutException(\"Request timeout \" + unwrittenAppend.requestTimeoutMs\n-                    + \" expired before the records could be appended to the log\"));\n-            } else {\n-                int epoch = quorum.epoch();\n-                LogAppendInfo info = appendAsLeader(unwrittenAppend.records);\n-                OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(info.lastOffset, epoch);\n-                long numRecords = info.lastOffset - info.firstOffset + 1;\n-                logger.debug(\"Completed write of {} records at {}\", numRecords, offsetAndEpoch);\n-\n-                if (unwrittenAppend.ackMode == AckMode.LEADER) {\n-                    unwrittenAppend.complete(offsetAndEpoch);\n-                } else if (unwrittenAppend.ackMode == AckMode.QUORUM) {\n-                    CompletableFuture<Long> future = appendPurgatory.await(\n-                        LogOffset.awaitCommitted(offsetAndEpoch.offset),\n-                        unwrittenAppend.requestTimeoutMs);\n-\n-                    future.whenComplete((completionTimeMs, exception) -> {\n-                        if (exception != null) {\n-                            logger.error(\"Failed to commit append at {} due to {}\", offsetAndEpoch, exception);\n-\n-                            unwrittenAppend.fail(exception);\n-                        } else {\n-                            long elapsedTime = Math.max(0, completionTimeMs - currentTimeMs);\n-                            double elapsedTimePerRecord = (double) elapsedTime / numRecords;\n-                            kafkaRaftMetrics.updateCommitLatency(elapsedTimePerRecord, currentTimeMs);\n-                            unwrittenAppend.complete(offsetAndEpoch);\n-\n-                            logger.debug(\"Completed commit of {} records at {}\", numRecords, offsetAndEpoch);\n-                        }\n-                    });\n-                }\n-            }\n-\n-            numAppends++;\n-        }\n-\n-        if (numAppends > 0) {\n-            flushLeaderLog(state, currentTimeMs);\n-        }\n-    }\n-\n-    /**\n-     * Append a set of records to the log. Successful completion of the future indicates a success of\n-     * the append, with the uncommitted base offset and epoch.\n-     *\n-     * @param records The records to write to the log\n-     * @param ackMode The commit mode for the appended records\n-     * @param timeoutMs The maximum time to wait for the append operation to complete (including\n-     *                  any time needed for replication)\n-     * @return The uncommitted base offset and epoch of the appended records\n-     */\n     @Override\n-    public CompletableFuture<OffsetAndEpoch> append(\n-        Records records,\n-        AckMode ackMode,\n-        long timeoutMs\n-    ) {\n-        if (records.sizeInBytes() == 0)\n-            throw new IllegalArgumentException(\"Attempt to append empty record set\");\n-\n-        if (shutdown.get() != null)\n-            throw new IllegalStateException(\"Cannot append records while we are shutting down\");\n-\n-        if (quorum.isObserver())\n-            throw new IllegalStateException(\"Illegal attempt to write to an observer\");\n-\n-        CompletableFuture<OffsetAndEpoch> future = new CompletableFuture<>();\n-        UnwrittenAppend unwrittenAppend = new UnwrittenAppend(\n-            records, time.milliseconds(), timeoutMs, ackMode, future);\n+    public Long scheduleAppend(int epoch, List<T> records) {\n+        BatchAccumulator<T> accumulator = this.accumulator;\n+        if (accumulator == null) {\n+            return Long.MAX_VALUE;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTcxOTc4NQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r509719785", "bodyText": "Yeah, see my comment above about the handling of Long.MAX_VALUE. This is an attempt to reduce the error handling in the state machine. The model that we are working toward here is the following:\n\nthe state machine gets notified that the node has become leader in some epoch\nthe state machine can schedule appends with this epoch and it will get back the expected append offset\nthe state machine treats scheduled appends as uncommitted until the call to handleCommit\nif the node resigns its leadership, the state machine will get notified and it will be expected to drop uncommitted data\n\nBy using a sort of impossible offset sentinel, the state machine just needs to wait for the notification that the leader has resigned.", "author": "hachikuji", "createdAt": "2020-10-21T21:35:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyMjA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM4OTU4MA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r512389580", "bodyText": "Sounds good.", "author": "guozhangwang", "createdAt": "2020-10-27T03:06:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyMjA2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyMjY5NA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508922694", "bodyText": "Should we handle null / max-value here?", "author": "guozhangwang", "createdAt": "2020-10-21T00:41:31Z", "path": "raft/src/main/java/org/apache/kafka/raft/ReplicatedCounter.java", "diffHunk": "@@ -104,29 +62,17 @@ public synchronized boolean isWritable() {\n     public synchronized void increment() {\n         if (!isWritable())\n             throw new KafkaException(\"Counter is not currently writable\");\n-        int initialValue = uncommitted.get();\n-        int incrementedValue = uncommitted.incrementAndGet();\n-        Records records = MemoryRecords.withRecords(CompressionType.NONE, serialize(incrementedValue));\n-        client.append(records, AckMode.LEADER, Integer.MAX_VALUE).whenComplete((offsetAndEpoch, throwable) -> {\n-            if (offsetAndEpoch != null) {\n-                log.debug(\"Appended increment at offset {}: {} -> {}\",\n-                    offsetAndEpoch.offset, initialValue, incrementedValue);\n-            } else {\n-                uncommitted.set(initialValue);\n-                log.debug(\"Failed append of increment: {} -> {}\", initialValue, incrementedValue, throwable);\n-            }\n-        });\n-    }\n-\n-    private SimpleRecord serialize(int value) {\n-        ByteBuffer buffer = ByteBuffer.allocate(4);\n-        Type.INT32.write(buffer, value);\n-        buffer.flip();\n-        return new SimpleRecord(buffer);\n+        uncommitted += 1;\n+        long offset = client.scheduleAppend(currentLeaderAndEpoch.epoch, Collections.singletonList(uncommitted));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyNDg1OQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508924859", "bodyText": "Is this going to be used for non-testing code in the future? If it is only going to be for metrics purposes maybe we can allow it to be non thread-safe just to not blocking on other critical paths.", "author": "guozhangwang", "createdAt": "2020-10-21T00:49:26Z", "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.common.utils.Timer;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.Closeable;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * TODO: Also flush after minimum size limit is reached?\n+ */\n+public class BatchAccumulator<T> implements Closeable {\n+    private final int epoch;\n+    private final Time time;\n+    private final Timer lingerTimer;\n+    private final int lingerMs;\n+    private final int maxBatchSize;\n+    private final CompressionType compressionType;\n+    private final MemoryPool memoryPool;\n+    private final ReentrantLock lock;\n+    private final RecordSerde<T> serde;\n+\n+    private long nextOffset;\n+    private BatchBuilder<T> currentBatch;\n+    private List<CompletedBatch<T>> completed;\n+\n+    public BatchAccumulator(\n+        int epoch,\n+        long baseOffset,\n+        int lingerMs,\n+        int maxBatchSize,\n+        MemoryPool memoryPool,\n+        Time time,\n+        CompressionType compressionType,\n+        RecordSerde<T> serde\n+    ) {\n+        this.epoch = epoch;\n+        this.lingerMs = lingerMs;\n+        this.maxBatchSize = maxBatchSize;\n+        this.memoryPool = memoryPool;\n+        this.time = time;\n+        this.lingerTimer = time.timer(lingerMs);\n+        this.compressionType = compressionType;\n+        this.serde = serde;\n+        this.nextOffset = baseOffset;\n+        this.completed = new ArrayList<>();\n+        this.lock = new ReentrantLock();\n+    }\n+\n+    /**\n+     * Append a list of records into an atomic batch. We guarantee all records\n+     * are included in the same underlying record batch so that either all of\n+     * the records become committed or none of them do.\n+     *\n+     * @param epoch the expected leader epoch\n+     * @param records the list of records to include in a batch\n+     * @return the offset of the last message or {@link Long#MAX_VALUE} if the epoch\n+     *         does not match\n+     */\n+    public Long append(int epoch, List<T> records) {\n+        if (epoch != this.epoch) {\n+            // If the epoch does not match, then the state machine probably\n+            // has not gotten the notification about the latest epoch change.\n+            // In this case, ignore the append and return a large offset value\n+            // which will never be committed.\n+            return Long.MAX_VALUE;\n+        }\n+\n+        Object serdeContext = serde.newWriteContext();\n+        int batchSize = 0;\n+        for (T record : records) {\n+            batchSize += serde.recordSize(record, serdeContext);\n+        }\n+\n+        if (batchSize > maxBatchSize) {\n+            throw new IllegalArgumentException(\"The total size of \" + records + \" is \" + batchSize +\n+                \", which exceeds the maximum allowed batch size of \" + maxBatchSize);\n+        }\n+\n+        lock.lock();\n+        try {\n+            BatchBuilder<T> batch = maybeAllocateBatch(batchSize);\n+            if (batch == null) {\n+                return null;\n+            }\n+\n+            if (isEmpty()) {\n+                lingerTimer.update();\n+                lingerTimer.reset(lingerMs);\n+            }\n+\n+            for (T record : records) {\n+                batch.appendRecord(record, serdeContext);\n+                nextOffset += 1;\n+            }\n+\n+            return nextOffset - 1;\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    private BatchBuilder<T> maybeAllocateBatch(int batchSize) {\n+        if (currentBatch == null) {\n+            startNewBatch();\n+        } else if (!currentBatch.hasRoomFor(batchSize)) {\n+            completeCurrentBatch();\n+        }\n+        return currentBatch;\n+    }\n+\n+    private void completeCurrentBatch() {\n+        MemoryRecords data = currentBatch.build();\n+        completed.add(new CompletedBatch<>(\n+            currentBatch.baseOffset(),\n+            currentBatch.records(),\n+            data,\n+            memoryPool,\n+            currentBatch.initialBuffer()\n+        ));\n+        currentBatch = null;\n+        startNewBatch();\n+    }\n+\n+    private void startNewBatch() {\n+        ByteBuffer buffer = memoryPool.tryAllocate(maxBatchSize);\n+        if (buffer != null) {\n+            currentBatch = new BatchBuilder<>(\n+                buffer,\n+                serde,\n+                compressionType,\n+                nextOffset,\n+                time.milliseconds(),\n+                false,\n+                RecordBatch.NO_PARTITION_LEADER_EPOCH,\n+                maxBatchSize\n+            );\n+        }\n+    }\n+\n+    /**\n+     * Check whether there are any batches which need flushing now.\n+     *\n+     * @param currentTimeMs current time in milliseconds\n+     * @return true if there are batches ready to flush, false otherwise\n+     */\n+    public boolean needsFlush(long currentTimeMs) {\n+        return timeUntilFlush(currentTimeMs) <= 0;\n+    }\n+\n+    /**\n+     * Check the time remaining until the next needed flush. If the accumulator\n+     * is empty, then {@link Long#MAX_VALUE} will be returned.\n+     *\n+     * @param currentTimeMs current time in milliseconds\n+     * @return the delay in milliseconds before the next expected flush\n+     */\n+    public long timeUntilFlush(long currentTimeMs) {\n+        lock.lock();\n+        try {\n+            lingerTimer.update(currentTimeMs);\n+            if (isEmpty()) {\n+                return Long.MAX_VALUE;\n+            } else {\n+                return lingerTimer.remainingMs();\n+            }\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    private boolean isEmpty() {\n+        lock.lock();\n+        try {\n+            if (currentBatch != null && currentBatch.nonEmpty()) {\n+                return false;\n+            } else {\n+                return completed.isEmpty();\n+            }\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    /**\n+     * Get the leader epoch, which is constant for each instance.\n+     *\n+     * @return the leader epoch\n+     */\n+    public int epoch() {\n+        return epoch;\n+    }\n+\n+    /**\n+     * Flush completed batches. The caller is expected to first check whether\n+     * a flush is expected using {@link #needsFlush(long)} in order to avoid\n+     * unnecessary flushing.\n+     *\n+     * @return the list of completed batches\n+     */\n+    public List<CompletedBatch<T>> flush() {\n+        lock.lock();\n+        try {\n+            if (currentBatch != null && currentBatch.nonEmpty()) {\n+                completeCurrentBatch();\n+            }\n+\n+            List<CompletedBatch<T>> res = completed;\n+            this.completed = new ArrayList<>();\n+            return res;\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    /**\n+     * Get the number of batches including the one that is currently being\n+     * written to (if it exists).\n+     *\n+     * @return\n+     */\n+    public int count() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTcyMzEyNw==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r509723127", "bodyText": "I am not sure. We don't have a metric yet, so I thought we might as well start with a thread-safe implementation. If we add a metric in the future, we can probably use an AtomicInteger or something and eliminate the locking.", "author": "hachikuji", "createdAt": "2020-10-21T21:40:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyNDg1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyNTIwNw==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r508925207", "bodyText": "nit: can we consolidate this class with the existing one (of course, it needs to be generalized on T record) than creating a new class? Ditto for BatchAccumulator and BatchMemoryPool.", "author": "guozhangwang", "createdAt": "2020-10-21T00:50:52Z", "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchBuilder.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.protocol.DataOutputStreamWritable;\n+import org.apache.kafka.common.record.AbstractRecords;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.DefaultRecord;\n+import org.apache.kafka.common.record.DefaultRecordBatch;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.common.utils.ByteBufferOutputStream;\n+import org.apache.kafka.common.utils.ByteUtils;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.DataOutputStream;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class BatchBuilder<T> {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTcyNjI1MA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r509726250", "bodyText": "I am not sure I follow. Are you suggesting making BatchBuilder a nested class? I think what it's doing is complex enough that I wanted a separate class that could be tested in isolation.", "author": "hachikuji", "createdAt": "2020-10-21T21:44:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyNTIwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM5MDMwOA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r512390308", "bodyText": "I think the pairs of those classes:\n\nBatchAccumulator and RecordAccumulator\nBatchMemoryPool and BufferPool\nBatchBuilder and MemoryRecordsBuilder\n\nare similar can maybe can be consolidated to a single class each.", "author": "guozhangwang", "createdAt": "2020-10-27T03:09:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyNTIwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjg0NzE5NQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r512847195", "bodyText": "Yes, that is fair. Initially I had planned to reuse as much as possible, but the existing classes had too much baggage. I wanted implementations which were simple and efficient for the needs of the raft layer.", "author": "hachikuji", "createdAt": "2020-10-27T16:34:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkyNTIwNw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEwMTU4MQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511101581", "bodyText": "What are you trying to protect with this check? For example, the signature could be public long append(List<T> records) with the accumulator writing the correct epoch.\nIn #9482 you implemented handleClaim in to only fire when the Listener's \"acknowledged\" offset + 1 is >= to the leader's epoch start offset.\nThinking through the code's behaviour, I see this check catching the case the the raft replica lost leadership and won leadership before the Listener was able to asynchronously process handleResign and handleClaim.", "author": "jsancio", "createdAt": "2020-10-23T19:22:25Z", "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.common.utils.Timer;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.Closeable;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * TODO: Also flush after minimum size limit is reached?\n+ */\n+public class BatchAccumulator<T> implements Closeable {\n+    private final int epoch;\n+    private final Time time;\n+    private final Timer lingerTimer;\n+    private final int lingerMs;\n+    private final int maxBatchSize;\n+    private final CompressionType compressionType;\n+    private final MemoryPool memoryPool;\n+    private final ReentrantLock lock;\n+    private final RecordSerde<T> serde;\n+\n+    private long nextOffset;\n+    private BatchBuilder<T> currentBatch;\n+    private List<CompletedBatch<T>> completed;\n+\n+    public BatchAccumulator(\n+        int epoch,\n+        long baseOffset,\n+        int lingerMs,\n+        int maxBatchSize,\n+        MemoryPool memoryPool,\n+        Time time,\n+        CompressionType compressionType,\n+        RecordSerde<T> serde\n+    ) {\n+        this.epoch = epoch;\n+        this.lingerMs = lingerMs;\n+        this.maxBatchSize = maxBatchSize;\n+        this.memoryPool = memoryPool;\n+        this.time = time;\n+        this.lingerTimer = time.timer(lingerMs);\n+        this.compressionType = compressionType;\n+        this.serde = serde;\n+        this.nextOffset = baseOffset;\n+        this.completed = new ArrayList<>();\n+        this.lock = new ReentrantLock();\n+    }\n+\n+    /**\n+     * Append a list of records into an atomic batch. We guarantee all records\n+     * are included in the same underlying record batch so that either all of\n+     * the records become committed or none of them do.\n+     *\n+     * @param epoch the expected leader epoch\n+     * @param records the list of records to include in a batch\n+     * @return the offset of the last message or {@link Long#MAX_VALUE} if the epoch\n+     *         does not match\n+     */\n+    public Long append(int epoch, List<T> records) {\n+        if (epoch != this.epoch) {\n+            // If the epoch does not match, then the state machine probably\n+            // has not gotten the notification about the latest epoch change.\n+            // In this case, ignore the append and return a large offset value\n+            // which will never be committed.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTIwMDIzOA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511200238", "bodyText": "Right. It is important to ensure that the state machine has observed the latest leader epoch. Otherwise there may be committed data inflight which the state machine has yet to see.", "author": "hachikuji", "createdAt": "2020-10-23T23:50:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEwMTU4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEwMzMzNg==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511103336", "bodyText": "We should document this returns null and what it means when null is returned.", "author": "jsancio", "createdAt": "2020-10-23T19:24:34Z", "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.common.utils.Timer;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.Closeable;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * TODO: Also flush after minimum size limit is reached?\n+ */\n+public class BatchAccumulator<T> implements Closeable {\n+    private final int epoch;\n+    private final Time time;\n+    private final Timer lingerTimer;\n+    private final int lingerMs;\n+    private final int maxBatchSize;\n+    private final CompressionType compressionType;\n+    private final MemoryPool memoryPool;\n+    private final ReentrantLock lock;\n+    private final RecordSerde<T> serde;\n+\n+    private long nextOffset;\n+    private BatchBuilder<T> currentBatch;\n+    private List<CompletedBatch<T>> completed;\n+\n+    public BatchAccumulator(\n+        int epoch,\n+        long baseOffset,\n+        int lingerMs,\n+        int maxBatchSize,\n+        MemoryPool memoryPool,\n+        Time time,\n+        CompressionType compressionType,\n+        RecordSerde<T> serde\n+    ) {\n+        this.epoch = epoch;\n+        this.lingerMs = lingerMs;\n+        this.maxBatchSize = maxBatchSize;\n+        this.memoryPool = memoryPool;\n+        this.time = time;\n+        this.lingerTimer = time.timer(lingerMs);\n+        this.compressionType = compressionType;\n+        this.serde = serde;\n+        this.nextOffset = baseOffset;\n+        this.completed = new ArrayList<>();\n+        this.lock = new ReentrantLock();\n+    }\n+\n+    /**\n+     * Append a list of records into an atomic batch. We guarantee all records\n+     * are included in the same underlying record batch so that either all of\n+     * the records become committed or none of them do.\n+     *\n+     * @param epoch the expected leader epoch\n+     * @param records the list of records to include in a batch\n+     * @return the offset of the last message or {@link Long#MAX_VALUE} if the epoch\n+     *         does not match", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTExMjI4MQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511112281", "bodyText": "Throughout this type we use flush to mean \"collect the current batch\" and start a new one. When I started reading this code I was assuming that it meant flush to disk/IO. Should we use a different verb for this? I think Java tends to use \"collect\" for this type of operation. What do you think?", "author": "jsancio", "createdAt": "2020-10-23T19:35:56Z", "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.common.utils.Timer;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.Closeable;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * TODO: Also flush after minimum size limit is reached?\n+ */\n+public class BatchAccumulator<T> implements Closeable {\n+    private final int epoch;\n+    private final Time time;\n+    private final Timer lingerTimer;\n+    private final int lingerMs;\n+    private final int maxBatchSize;\n+    private final CompressionType compressionType;\n+    private final MemoryPool memoryPool;\n+    private final ReentrantLock lock;\n+    private final RecordSerde<T> serde;\n+\n+    private long nextOffset;\n+    private BatchBuilder<T> currentBatch;\n+    private List<CompletedBatch<T>> completed;\n+\n+    public BatchAccumulator(\n+        int epoch,\n+        long baseOffset,\n+        int lingerMs,\n+        int maxBatchSize,\n+        MemoryPool memoryPool,\n+        Time time,\n+        CompressionType compressionType,\n+        RecordSerde<T> serde\n+    ) {\n+        this.epoch = epoch;\n+        this.lingerMs = lingerMs;\n+        this.maxBatchSize = maxBatchSize;\n+        this.memoryPool = memoryPool;\n+        this.time = time;\n+        this.lingerTimer = time.timer(lingerMs);\n+        this.compressionType = compressionType;\n+        this.serde = serde;\n+        this.nextOffset = baseOffset;\n+        this.completed = new ArrayList<>();\n+        this.lock = new ReentrantLock();\n+    }\n+\n+    /**\n+     * Append a list of records into an atomic batch. We guarantee all records\n+     * are included in the same underlying record batch so that either all of\n+     * the records become committed or none of them do.\n+     *\n+     * @param epoch the expected leader epoch\n+     * @param records the list of records to include in a batch\n+     * @return the offset of the last message or {@link Long#MAX_VALUE} if the epoch\n+     *         does not match\n+     */\n+    public Long append(int epoch, List<T> records) {\n+        if (epoch != this.epoch) {\n+            // If the epoch does not match, then the state machine probably\n+            // has not gotten the notification about the latest epoch change.\n+            // In this case, ignore the append and return a large offset value\n+            // which will never be committed.\n+            return Long.MAX_VALUE;\n+        }\n+\n+        Object serdeContext = serde.newWriteContext();\n+        int batchSize = 0;\n+        for (T record : records) {\n+            batchSize += serde.recordSize(record, serdeContext);\n+        }\n+\n+        if (batchSize > maxBatchSize) {\n+            throw new IllegalArgumentException(\"The total size of \" + records + \" is \" + batchSize +\n+                \", which exceeds the maximum allowed batch size of \" + maxBatchSize);\n+        }\n+\n+        lock.lock();\n+        try {\n+            BatchBuilder<T> batch = maybeAllocateBatch(batchSize);\n+            if (batch == null) {\n+                return null;\n+            }\n+\n+            if (isEmpty()) {\n+                lingerTimer.update();\n+                lingerTimer.reset(lingerMs);\n+            }\n+\n+            for (T record : records) {\n+                batch.appendRecord(record, serdeContext);\n+                nextOffset += 1;\n+            }\n+\n+            return nextOffset - 1;\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    private BatchBuilder<T> maybeAllocateBatch(int batchSize) {\n+        if (currentBatch == null) {\n+            startNewBatch();\n+        } else if (!currentBatch.hasRoomFor(batchSize)) {\n+            completeCurrentBatch();\n+        }\n+        return currentBatch;\n+    }\n+\n+    private void completeCurrentBatch() {\n+        MemoryRecords data = currentBatch.build();\n+        completed.add(new CompletedBatch<>(\n+            currentBatch.baseOffset(),\n+            currentBatch.records(),\n+            data,\n+            memoryPool,\n+            currentBatch.initialBuffer()\n+        ));\n+        currentBatch = null;\n+        startNewBatch();\n+    }\n+\n+    private void startNewBatch() {\n+        ByteBuffer buffer = memoryPool.tryAllocate(maxBatchSize);\n+        if (buffer != null) {\n+            currentBatch = new BatchBuilder<>(\n+                buffer,\n+                serde,\n+                compressionType,\n+                nextOffset,\n+                time.milliseconds(),\n+                false,\n+                RecordBatch.NO_PARTITION_LEADER_EPOCH,\n+                maxBatchSize\n+            );\n+        }\n+    }\n+\n+    /**\n+     * Check whether there are any batches which need flushing now.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjIxNzIxOA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r512217218", "bodyText": "Perhaps drain is a better verb in this context.", "author": "hachikuji", "createdAt": "2020-10-26T19:33:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTExMjI4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEzMjEwMQ==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511132101", "bodyText": "I think it would be good to have a description of what this type is doing and how to use it. Reading the code and seeing how the buffer is shared, I am under the impression that this type should not be reused after the build method is called. Is that correct?\nIf this is correct, I think all of the public methods should check that they are not being called after build is called.", "author": "jsancio", "createdAt": "2020-10-23T20:08:20Z", "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchBuilder.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.protocol.DataOutputStreamWritable;\n+import org.apache.kafka.common.record.AbstractRecords;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.DefaultRecord;\n+import org.apache.kafka.common.record.DefaultRecordBatch;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.common.utils.ByteBufferOutputStream;\n+import org.apache.kafka.common.utils.ByteUtils;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.DataOutputStream;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class BatchBuilder<T> {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjIyNDc2Nw==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r512224767", "bodyText": "Sure, we can do that. I was being a bit looser with API validation since this is bound for internal use only.", "author": "hachikuji", "createdAt": "2020-10-26T19:46:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEzMjEwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEzMzU0Ng==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r511133546", "bodyText": "Let's check that previouslyAllocated's capacity is batchSize else buffer.limit(sizeBytes) is going to throw with a less useful stacktrace.", "author": "jsancio", "createdAt": "2020-10-23T20:11:54Z", "path": "raft/src/main/java/org/apache/kafka/raft/internals/BatchMemoryPool.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.memory.MemoryPool;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * Simple memory pool which maintains a limited number of fixed-size buffers.\n+ */\n+public class BatchMemoryPool implements MemoryPool {\n+    private final ReentrantLock lock;\n+    private final Deque<ByteBuffer> free;\n+    private final int maxBatches;\n+    private final int batchSize;\n+\n+    private int numAllocatedBatches = 0;\n+\n+    public BatchMemoryPool(int maxBatches, int batchSize) {\n+        this.maxBatches = maxBatches;\n+        this.batchSize = batchSize;\n+        this.free = new ArrayDeque<>(maxBatches);\n+        this.lock = new ReentrantLock();\n+    }\n+\n+    @Override\n+    public ByteBuffer tryAllocate(int sizeBytes) {\n+        if (sizeBytes > batchSize) {\n+            throw new IllegalArgumentException(\"Cannot allocate buffers larger than max \" +\n+                \"batch size of \" + batchSize);\n+        }\n+\n+        lock.lock();\n+        try {\n+            ByteBuffer buffer = free.poll();\n+            if (buffer == null && numAllocatedBatches < maxBatches) {\n+                buffer = ByteBuffer.allocate(batchSize);\n+                numAllocatedBatches += 1;\n+            }\n+\n+            if (buffer != null) {\n+                buffer.clear();\n+                buffer.limit(sizeBytes);\n+            }\n+            return buffer;\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    @Override\n+    public void release(ByteBuffer previouslyAllocated) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e194616b3ad241d0375feecd51a11dac89926444", "url": "https://github.com/apache/kafka/commit/e194616b3ad241d0375feecd51a11dac89926444", "message": "KAFKA-10601; Add support for append linger to Raft implementation", "committedDate": "2020-10-23T23:45:36Z", "type": "commit"}, {"oid": "b2446f4defac4b640e2fc5b7424a67f43dd79e54", "url": "https://github.com/apache/kafka/commit/b2446f4defac4b640e2fc5b7424a67f43dd79e54", "message": "Remove unused class", "committedDate": "2020-10-23T23:45:36Z", "type": "commit"}, {"oid": "b7fb4b0e7f88f2149bf42a9af37b947c3dea07a9", "url": "https://github.com/apache/kafka/commit/b7fb4b0e7f88f2149bf42a9af37b947c3dea07a9", "message": "Remove unused `MetadataRecordSerde`", "committedDate": "2020-10-23T23:45:37Z", "type": "commit"}, {"oid": "d9993b602edde63f44d4bb986ecc2d33767287d1", "url": "https://github.com/apache/kafka/commit/d9993b602edde63f44d4bb986ecc2d33767287d1", "message": "Remove unused `DataOutputWritable`", "committedDate": "2020-10-23T23:45:37Z", "type": "commit"}, {"oid": "00fac83174380afeb68b5a9fe979a5fd0774c23c", "url": "https://github.com/apache/kafka/commit/00fac83174380afeb68b5a9fe979a5fd0774c23c", "message": "Clarify handling when the epoch in `scheduleAppend` does not match", "committedDate": "2020-10-23T23:45:37Z", "type": "commit"}, {"oid": "c5a769c5a1b3d10a410e0fdc3c8f2fccaf7782b0", "url": "https://github.com/apache/kafka/commit/c5a769c5a1b3d10a410e0fdc3c8f2fccaf7782b0", "message": "Rename linger time to `append.linger.ms`", "committedDate": "2020-10-23T23:45:37Z", "type": "commit"}, {"oid": "cf381c0fff222263dacb1edc97e38a6b1fd723b8", "url": "https://github.com/apache/kafka/commit/cf381c0fff222263dacb1edc97e38a6b1fd723b8", "message": "Throw an error if `handleCommit` includes unexpected append", "committedDate": "2020-10-23T23:45:37Z", "type": "commit"}, {"oid": "3dabc1178f9dcb665c677362f8a3bb487fa4fc35", "url": "https://github.com/apache/kafka/commit/3dabc1178f9dcb665c677362f8a3bb487fa4fc35", "message": "`ReplicatedCounter` should check return of `scheduleAppend`", "committedDate": "2020-10-23T23:45:37Z", "type": "commit"}, {"oid": "ae3c2dc21230619220d74133f9e76195f957e6da", "url": "https://github.com/apache/kafka/commit/ae3c2dc21230619220d74133f9e76195f957e6da", "message": "Fix jdk8 compilation error", "committedDate": "2020-10-23T23:45:37Z", "type": "commit"}, {"oid": "ae3c2dc21230619220d74133f9e76195f957e6da", "url": "https://github.com/apache/kafka/commit/ae3c2dc21230619220d74133f9e76195f957e6da", "message": "Fix jdk8 compilation error", "committedDate": "2020-10-23T23:45:37Z", "type": "forcePushed"}, {"oid": "0490965ad582fbae717edbec413abd29a170691a", "url": "https://github.com/apache/kafka/commit/0490965ad582fbae717edbec413abd29a170691a", "message": "Address various comments", "committedDate": "2020-10-26T20:12:55Z", "type": "commit"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "fac5c0a9507f9f40f99dac017242d12500a97d5d", "url": "https://github.com/apache/kafka/commit/fac5c0a9507f9f40f99dac017242d12500a97d5d", "message": "Do not block IO thread in order to drain accumulator", "committedDate": "2020-10-27T03:03:07Z", "type": "commit"}, {"oid": "fac5c0a9507f9f40f99dac017242d12500a97d5d", "url": "https://github.com/apache/kafka/commit/fac5c0a9507f9f40f99dac017242d12500a97d5d", "message": "Do not block IO thread in order to drain accumulator", "committedDate": "2020-10-27T03:03:07Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTU2NjE1OA==", "url": "https://github.com/apache/kafka/pull/9418#discussion_r515566158", "bodyText": "@hachikuji Should we move this state to LeaderState? We can delegate maybeCloseAccumulator to the transitionTo... functions.\nI have a similar requirements for snapshot. I am currently adding a SnapshotWriter to FollowerState to track this state. I haven't implemented it yet but I am thinking of making FollowerState Closeable which will handle the cleanup. The transitionTo... functions have the additional responsibility of calling close if necessary.\nWhat do you think?", "author": "jsancio", "createdAt": "2020-11-01T02:32:40Z", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -276,6 +299,17 @@ private void onBecomeLeader(long currentTimeMs) {\n         resetConnections();\n \n         kafkaRaftMetrics.maybeUpdateElectionLatency(currentTimeMs);\n+\n+        accumulator = new BatchAccumulator<>(", "originalCommit": "fac5c0a9507f9f40f99dac017242d12500a97d5d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}