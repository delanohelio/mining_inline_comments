{"pr_number": 9110, "pr_title": "MINOR: Ensure a reason is logged for all segment deletion operations", "pr_createdAt": "2020-08-01T02:21:28Z", "pr_url": "https://github.com/apache/kafka/pull/9110", "timeline": [{"oid": "3431b846518371e6f97bc0ffc4405362eba3d802", "url": "https://github.com/apache/kafka/commit/3431b846518371e6f97bc0ffc4405362eba3d802", "message": "MINOR: Ensure a reason is logged for every segment deletion", "committedDate": "2020-08-01T02:20:13Z", "type": "commit"}, {"oid": "149f6ae5355b85f13152a17b595cf8f775109dd3", "url": "https://github.com/apache/kafka/commit/149f6ae5355b85f13152a17b595cf8f775109dd3", "message": "Better names", "committedDate": "2020-08-01T02:23:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNDA4Nw==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r463914087", "bodyText": "If we passed in the deletion reason further into the deleteSegmentFiles method, it seems we can print the reason string just once for a batch of segments being deleted. And within the reason string, we can provide the reason for deleting the batch:\nhttps://github.com/confluentinc/ce-kafka/blob/master/core/src/main/scala/kafka/log/Log.scala#L2519\nhttps://github.com/confluentinc/ce-kafka/blob/master/core/src/main/scala/kafka/log/Log.scala#L2526\nex: info(\"Deleting segments due to $reason: ${segments.mkString(\",\")}\"\nwhere $reason provides due to retention time 1200000ms breach.\nThe drawback is that sometimes we can not print segment-specific information since the error message is at a batch level. But generally it may be that segment-level deletion information could bloat our server logging, so it may be better to batch the logging instead.\nWhat are your thoughts?", "author": "kowshik", "createdAt": "2020-08-01T03:11:26Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2227,14 +2210,17 @@ class Log(@volatile private var _dir: File,\n    * @param segments The log segments to schedule for deletion\n    * @param asyncDelete Whether the segment files should be deleted asynchronously\n    */\n-  private def removeAndDeleteSegments(segments: Iterable[LogSegment], asyncDelete: Boolean): Unit = {\n+  private def removeAndDeleteSegments(segments: Iterable[LogSegment],\n+                                      asyncDelete: Boolean,\n+                                      reason: SegmentDeletionReason): Unit = {\n     if (segments.nonEmpty) {\n       lock synchronized {\n         // As most callers hold an iterator into the `segments` collection and `removeAndDeleteSegment` mutates it by\n         // removing the deleted segment, we should force materialization of the iterator here, so that results of the\n         // iteration remain valid and deterministic.\n         val toDelete = segments.toList\n         toDelete.foreach { segment =>\n+          info(s\"${reason.reasonString(this, segment)}\")", "originalCommit": "149f6ae5355b85f13152a17b595cf8f775109dd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk3NTA2OQ==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r463975069", "bodyText": "While verbose, I think having the granularity of each segment is useful. This allows us to easily reason about why a particular segment was deleted. Note that we switched from a single log per batch to a log per segment in #8850.", "author": "dhruvilshah3", "createdAt": "2020-08-01T15:56:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNDA4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4MDk2Mg==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r463980962", "bodyText": "Can we think about cases where this could be an issue? Say delete records is used, causing a large number of segments to be deleted Could that trigger excessive logging?", "author": "ijuma", "createdAt": "2020-08-01T17:06:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNDA4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDAxMjg0Mg==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r464012842", "bodyText": "@dhruvilshah3 Sounds good!", "author": "kowshik", "createdAt": "2020-08-02T00:05:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNDA4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDEwMDgyMw==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r464100823", "bodyText": "@ijuma We log one message per deleted segment. This could cause temporary increase in log volume when DeleteRecords is used or when retention is lowered, for example.\nOverall, we have a few options with different tradeoffs:\n\nLog a common reason per batch being deleted, including base offsets of segments being deleted. eg.\n\nDeleting segments due to retention time 999ms breach. BaseOffsets: (0,5,...).\n\n\nLog a common reason per batch being deleted, including base offsets and metadata of segments. eg.\n\nDeleting segments due to retention time 999ms breach: LogSegment(baseOffset=0, size=360, lastModifiedTime=1596387738000, largestRecordTimestamp=Some(1596387737414)),LogSegment(baseOffset=5, size=360, lastModifiedTime=1596387738000, largestRecordTimestamp=Some(1596387737414)),...\n\n\nLog one message per segment being deleted. This is the current behavior. eg.\n\nSegment with base offset 0 will be deleted due to retention time 999ms breach based on the largest record timestamp from the segment, which is ...\nSegment with base offset 5 will be deleted due to retention time 999ms breach based on the largest record timestamp from the segment, which is ...\n...\n\nDoing (2) may be a reasonable tradeoff. It eliminates some of the redundancy at the cost of making it to glean per segment metadata. Let me know what you think.", "author": "dhruvilshah3", "createdAt": "2020-08-02T17:09:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNDA4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIyMTA2MA==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r465221060", "bodyText": "I think it is important to capture the segment level details. In the past, we have had trouble explaining precisely why a specific segment got deleted. For example, was it because of the last modified time or the record timestamp? When users are looking to understand why data is deleted, we should be able to provide a clear answer.\nMy personal preference is probably 3) because I hate dealing with lists of things in log messages. Simple grepping no longer work to extract the details. Big messages also messes up console scrolling and can choke downstream systems. For segments, I am not so worried about log noise because the rate of segment creation is not that high.", "author": "hachikuji", "createdAt": "2020-08-04T17:42:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNDA4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkzOTc0MQ==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r465939741", "bodyText": "I think this is reasonable. Logging a segment per line will make it easier for us to diagnose issues. I made the change to log a segment per line for retention-related deletions. We still batch all segments in a single line for all other deletion events, eg. log deletion, truncation, etc.", "author": "dhruvilshah3", "createdAt": "2020-08-05T19:01:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxNDA4Nw=="}], "type": "inlineReview"}, {"oid": "6660f983228be51def8064d03286c0afb6c8834a", "url": "https://github.com/apache/kafka/commit/6660f983228be51def8064d03286c0afb6c8834a", "message": "remove redundant logging", "committedDate": "2020-08-01T15:53:04Z", "type": "commit"}, {"oid": "6d931425cb85d89ff3fe79af357856486120087a", "url": "https://github.com/apache/kafka/commit/6d931425cb85d89ff3fe79af357856486120087a", "message": "Single log message per batch", "committedDate": "2020-08-02T17:16:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDIxMzI4Nw==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r464213287", "bodyText": "Should we keep largestTime from LHS, and, print both largestRecordTimestamp and largestTime ?", "author": "kowshik", "createdAt": "2020-08-03T06:16:34Z", "path": "core/src/main/scala/kafka/log/LogSegment.scala", "diffHunk": "@@ -413,7 +413,7 @@ class LogSegment private[log] (val log: FileRecords,\n   override def toString: String = \"LogSegment(baseOffset=\" + baseOffset +\n     \", size=\" + size +\n     \", lastModifiedTime=\" + lastModified +\n-    \", largestTime=\" + largestTimestamp +\n+    \", largestRecordTimestamp=\" + largestRecordTimestamp +", "originalCommit": "6d931425cb85d89ff3fe79af357856486120087a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU0OTQ0MA==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r464549440", "bodyText": "I'm ok with the change. I think it's better to reflect the underlying fields directly and redundant information just adds noise to the logs.", "author": "hachikuji", "createdAt": "2020-08-03T17:16:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDIxMzI4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDIxMzc3Mg==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r464213772", "bodyText": "nit: replace \".\" with \":\"\ns\"Current log size is ${log.size}: ${toDelete.mkString(\",\")}\"", "author": "kowshik", "createdAt": "2020-08-03T06:18:13Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2686,3 +2670,50 @@ object LogMetricNames {\n     List(NumLogSegments, LogStartOffset, LogEndOffset, Size)\n   }\n }\n+\n+sealed trait SegmentDeletionReason {\n+  def reasonString(log: Log, toDelete: Iterable[LogSegment]): String\n+}\n+\n+case object RetentionMsBreachDeletion extends SegmentDeletionReason {\n+  override def reasonString(log: Log, toDelete: Iterable[LogSegment]): String = {\n+    s\"Deleting segments due to retention time ${log.config.retentionMs}ms breach: ${toDelete.mkString(\",\")}\"\n+  }\n+}\n+\n+case object RetentionSizeBreachDeletion extends SegmentDeletionReason {\n+  override def reasonString(log: Log, toDelete: Iterable[LogSegment]): String = {\n+    s\"Deleting segments due to retention size ${log.config.retentionSize} breach. \" +\n+      s\"Current log size is ${log.size}. ${toDelete.mkString(\",\")}\"", "originalCommit": "6d931425cb85d89ff3fe79af357856486120087a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDIxMzg0MA==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r464213840", "bodyText": "Did you mean to use info level logging?", "author": "kowshik", "createdAt": "2020-08-03T06:18:28Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2227,13 +2210,16 @@ class Log(@volatile private var _dir: File,\n    * @param segments The log segments to schedule for deletion\n    * @param asyncDelete Whether the segment files should be deleted asynchronously\n    */\n-  private def removeAndDeleteSegments(segments: Iterable[LogSegment], asyncDelete: Boolean): Unit = {\n+  private def removeAndDeleteSegments(segments: Iterable[LogSegment],\n+                                      asyncDelete: Boolean,\n+                                      reason: SegmentDeletionReason): Unit = {\n     if (segments.nonEmpty) {\n       lock synchronized {\n         // As most callers hold an iterator into the `segments` collection and `removeAndDeleteSegment` mutates it by\n         // removing the deleted segment, we should force materialization of the iterator here, so that results of the\n         // iteration remain valid and deterministic.\n         val toDelete = segments.toList\n+        println(s\"${reason.reasonString(this, toDelete)}\")", "originalCommit": "6d931425cb85d89ff3fe79af357856486120087a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d93268010181f510c2ea3b3397781323047ea3ec", "url": "https://github.com/apache/kafka/commit/d93268010181f510c2ea3b3397781323047ea3ec", "message": "println -> info", "committedDate": "2020-08-03T07:20:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU0NTE5MA==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r464545190", "bodyText": "A little annoying to need to pass through segments just to be added to each log message individually. Maybe we could do it like this instead\ninfo(s\"Deleting segments due to ${reason.reasonString(this)}: $toDelete\")", "author": "hachikuji", "createdAt": "2020-08-03T17:08:26Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2227,13 +2210,16 @@ class Log(@volatile private var _dir: File,\n    * @param segments The log segments to schedule for deletion\n    * @param asyncDelete Whether the segment files should be deleted asynchronously\n    */\n-  private def removeAndDeleteSegments(segments: Iterable[LogSegment], asyncDelete: Boolean): Unit = {\n+  private def removeAndDeleteSegments(segments: Iterable[LogSegment],\n+                                      asyncDelete: Boolean,\n+                                      reason: SegmentDeletionReason): Unit = {\n     if (segments.nonEmpty) {\n       lock synchronized {\n         // As most callers hold an iterator into the `segments` collection and `removeAndDeleteSegment` mutates it by\n         // removing the deleted segment, we should force materialization of the iterator here, so that results of the\n         // iteration remain valid and deterministic.\n         val toDelete = segments.toList\n+        info(s\"${reason.reasonString(this, toDelete)}\")", "originalCommit": "d93268010181f510c2ea3b3397781323047ea3ec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU0Njg4Mg==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r464546882", "bodyText": "nit: is it necessary to add Deletion to all of these? Maybe only LogDeletion needs it since it is referring to deletion of the log itself.", "author": "hachikuji", "createdAt": "2020-08-03T17:11:37Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2686,3 +2670,50 @@ object LogMetricNames {\n     List(NumLogSegments, LogStartOffset, LogEndOffset, Size)\n   }\n }\n+\n+sealed trait SegmentDeletionReason {\n+  def reasonString(log: Log, toDelete: Iterable[LogSegment]): String\n+}\n+\n+case object RetentionMsBreachDeletion extends SegmentDeletionReason {", "originalCommit": "d93268010181f510c2ea3b3397781323047ea3ec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "77dae70b9f7ffc5dcd03194a8d8ebae1c35a1905", "url": "https://github.com/apache/kafka/commit/77dae70b9f7ffc5dcd03194a8d8ebae1c35a1905", "message": "Address review comments", "committedDate": "2020-08-03T17:38:36Z", "type": "commit"}, {"oid": "3254664556a9693d50c065ffeb39c2ab77781754", "url": "https://github.com/apache/kafka/commit/3254664556a9693d50c065ffeb39c2ab77781754", "message": "Address review comment", "committedDate": "2020-08-05T18:35:38Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU1MDQ5Mg==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r466550492", "bodyText": "Don't we need to get rid of the call to info here?", "author": "hachikuji", "createdAt": "2020-08-06T16:53:39Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -2227,13 +2210,16 @@ class Log(@volatile private var _dir: File,\n    * @param segments The log segments to schedule for deletion\n    * @param asyncDelete Whether the segment files should be deleted asynchronously\n    */\n-  private def removeAndDeleteSegments(segments: Iterable[LogSegment], asyncDelete: Boolean): Unit = {\n+  private def removeAndDeleteSegments(segments: Iterable[LogSegment],\n+                                      asyncDelete: Boolean,\n+                                      reason: SegmentDeletionReason): Unit = {\n     if (segments.nonEmpty) {\n       lock synchronized {\n         // As most callers hold an iterator into the `segments` collection and `removeAndDeleteSegment` mutates it by\n         // removing the deleted segment, we should force materialization of the iterator here, so that results of the\n         // iteration remain valid and deterministic.\n         val toDelete = segments.toList\n+        info(s\"${reason.logReason(this, toDelete)}\")", "originalCommit": "3254664556a9693d50c065ffeb39c2ab77781754", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYwMTU1Ng==", "url": "https://github.com/apache/kafka/pull/9110#discussion_r466601556", "bodyText": "Thanks for catching. I fixed this.", "author": "dhruvilshah3", "createdAt": "2020-08-06T18:20:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU1MDQ5Mg=="}], "type": "inlineReview"}, {"oid": "6df3797e99ad6354c69bab545862c9590e4a9aad", "url": "https://github.com/apache/kafka/commit/6df3797e99ad6354c69bab545862c9590e4a9aad", "message": "Remove unneeded `info` call", "committedDate": "2020-08-06T18:18:09Z", "type": "commit"}]}