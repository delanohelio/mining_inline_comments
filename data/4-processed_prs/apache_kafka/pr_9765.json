{"pr_number": 9765, "pr_title": "KAFKA-10763: Fix incomplete cooperative rebalances preventing connector/task revocations", "pr_createdAt": "2020-12-18T04:34:09Z", "pr_url": "https://github.com/apache/kafka/pull/9765", "timeline": [{"oid": "1375f01da6ce979307f7ba52362b453166c2d39e", "url": "https://github.com/apache/kafka/commit/1375f01da6ce979307f7ba52362b453166c2d39e", "message": "KAFKA-10763: Fix incomplete cooperative rebalances preventing connector/task revocations\n\nWhen two cooperative rebalances take place soon after one another, a prior rebalance may not complete before the next rebalance is started.\nUnder Eager rebalancing, no tasks would have been started, so the subsequent onRevoked call is intentionally skipped whenever rebalanceResolved was false.\nUnder Cooperative rebalancing, the same logic causes the DistributedHerder to skip stopping all of the connector/task revocations which occur in the second rebalance.\nThe DistributedHerder still removes the revoked connectors/tasks from its assignment, so that the DistributedHerder and Worker have different knowledge of running connectors/tasks.\nThis causes the connector/task instances that would have been stopped to disappear from the rebalance protocol, and left running until their workers are halted, or they fail.\nConnectors/Tasks which were then reassigned to other workers by the rebalance protocol would be duplicated, and run concurrently with zombie connectors/tasks.\nConnectors/Tasks which were reassigned back to the same worker would encounter exceptions in Worker, indicating that the connector/task existed and was already running.\n\n* Add a test for revoking and then reassigning a connector under normal circumstances\n* Add a test for revoking and then reassigning a connector following an incomplete cooperative rebalance\n* Change expectRebalance to make assignment fields mutable before passing them into the DistributedHerder\n* Only skip revocation for the Eager protocol, and never skip revocation for cooperative/sessioned protocols\n\nSigned-off-by: Greg Harris <gregh@confluent.io>", "committedDate": "2020-12-18T04:32:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAwMTU1NQ==", "url": "https://github.com/apache/kafka/pull/9765#discussion_r546001555", "bodyText": "It's hard to tell if this actually reproduces the issue or not due to the heavy mocking required. Is there a more direct way to reproduce? Maybe in RebalanceSourceConnectorsIntegrationTest or similar? Even if the IT ends up being flaky, having that repro would boost confidence in this fix.", "author": "ncliang", "createdAt": "2020-12-18T18:08:47Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java", "diffHunk": "@@ -566,6 +566,112 @@ public Boolean answer() throws Throwable {\n         PowerMock.verifyAll();\n     }\n \n+    @Test\n+    public void testRevoke() throws TimeoutException {\n+        revokeAndReassign(false);\n+    }\n+\n+    @Test\n+    public void testIncompleteRebalanceBeforeRevoke() throws TimeoutException {\n+        revokeAndReassign(true);\n+    }\n+\n+    public void revokeAndReassign(boolean incompleteRebalance) throws TimeoutException {", "originalCommit": "1375f01da6ce979307f7ba52362b453166c2d39e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTQ0ODUxMw==", "url": "https://github.com/apache/kafka/pull/9765#discussion_r551448513", "bodyText": "Yeah, this test was a bit difficult to wrap my head around at first, but I think it's the best way to target this section of the code. I don't believe that adding a new flakey test is prudent, and making a non-flakey test with less mocks might end up to be harder to follow than this mocked test.\nI think what would be a good test to add would be a variant which replaced this contrived reading-config-offset-topic-failure with a genuine WakeupException thrown from the end of tick, which I believe is the true root cause of this issue most of the time. This is not easy with the boilerplate in this test as-is, and requires a little bit of refactoring to set up the rebalance during that block.", "author": "gharris1727", "createdAt": "2021-01-04T17:09:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAwMTU1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAyNzg2Mg==", "url": "https://github.com/apache/kafka/pull/9765#discussion_r546027862", "bodyText": "Maybe add a comment explaining why the additional check is needed.", "author": "ncliang", "createdAt": "2020-12-18T18:56:26Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java", "diffHunk": "@@ -1740,7 +1741,7 @@ public void onRevoked(String leader, Collection<String> connectors, Collection<C\n             // Note that since we don't reset the assignment, we don't revoke leadership here. During a rebalance,\n             // it is still important to have a leader that can write configs, offsets, etc.\n \n-            if (rebalanceResolved) {\n+            if (rebalanceResolved || currentProtocolVersion >= CONNECT_PROTOCOL_V1) {", "originalCommit": "1375f01da6ce979307f7ba52362b453166c2d39e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}