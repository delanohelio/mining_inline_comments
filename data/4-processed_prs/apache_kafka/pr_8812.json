{"pr_number": 8812, "pr_title": "KAFKA-10101: Fix edge cases in Log.recoverLog and LogManager.loadLogs", "pr_createdAt": "2020-06-05T16:13:16Z", "pr_url": "https://github.com/apache/kafka/pull/8812", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAyNDYwNw==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436024607", "bodyText": "@junrao does this seem right? If we don't have any segments, we should be able to completely truncate the producer state manager, right?", "author": "ijuma", "createdAt": "2020-06-05T16:15:25Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -806,14 +806,20 @@ class Log(@volatile private var _dir: File,\n       }\n     }\n \n-    if (logSegments.nonEmpty) {\n-      val logEndOffset = activeSegment.readNextOffset\n-      if (logEndOffset < logStartOffset) {\n-        warn(s\"Deleting all segments because logEndOffset ($logEndOffset) is smaller than logStartOffset ($logStartOffset). \" +\n-          \"This could happen if segment files were deleted from the file system.\")\n-        removeAndDeleteSegments(logSegments, asyncDelete = true)\n-      }\n-    }\n+    val logEndOffsetOption: Option[Long] =\n+      if (logSegments.nonEmpty) {\n+        val logEndOffset = activeSegment.readNextOffset\n+        if (logEndOffset >= logStartOffset)\n+          Some(logEndOffset)\n+        else {\n+          warn(s\"Deleting all segments because logEndOffset ($logEndOffset) is smaller than logStartOffset ($logStartOffset). \" +\n+            \"This could happen if segment files were deleted from the file system.\")\n+          removeAndDeleteSegments(logSegments, asyncDelete = false)\n+          leaderEpochCache.foreach(_.clearAndFlush())\n+          producerStateManager.truncate()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE2Njc0Mw==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436166743", "bodyText": "Hmm, not sure about this. After KIP-360, we try to retain producer state as long as possible even when the corresponding entries have been removed from the log. However, we're in a strange state given that some of the later segments were apparently removed. Perhaps it is safer to treat this more like a new replica which is starting from scratch.", "author": "hachikuji", "createdAt": "2020-06-05T21:13:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAyNDYwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3ODk5NA==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436178994", "bodyText": "Yeah, that's what I was thinking. What's the best way to achieve this (treat this more like a new replica which is starting from scratch)?", "author": "ijuma", "createdAt": "2020-06-05T21:48:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAyNDYwNw=="}], "type": "inlineReview"}, {"oid": "0b2f2711f6bf0d0b397b81ebdf8d640a3f8fd6b3", "url": "https://github.com/apache/kafka/commit/0b2f2711f6bf0d0b397b81ebdf8d640a3f8fd6b3", "message": "KAFKA-10101: Fix edge cases in Log.recoverLog and LogManager.loadLogs\n\n1. Don't advance recovery point in `recoverLog` unless there was a clean\nshutdown.\n2. Ensure the recovery point is not ahead of the log end offset.\n3. Clean and flush leader epoch cache and truncate produce state manager\nif deleting segments due to log end offset being smaller than log start\noffset.\n4. If we are unable to delete clean shutdown file that exists, mark the\ndirectory as offline (this was the intent, but the code was wrong).", "committedDate": "2020-06-05T17:21:24Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE2NTg0Nw==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436165847", "bodyText": "nit: this is a big initializer. Are there parts we could move to a method?", "author": "hachikuji", "createdAt": "2020-06-05T21:10:41Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -806,14 +806,20 @@ class Log(@volatile private var _dir: File,\n       }\n     }\n \n-    if (logSegments.nonEmpty) {\n-      val logEndOffset = activeSegment.readNextOffset\n-      if (logEndOffset < logStartOffset) {\n-        warn(s\"Deleting all segments because logEndOffset ($logEndOffset) is smaller than logStartOffset ($logStartOffset). \" +\n-          \"This could happen if segment files were deleted from the file system.\")\n-        removeAndDeleteSegments(logSegments, asyncDelete = true)\n-      }\n-    }\n+    val logEndOffsetOption: Option[Long] =\n+      if (logSegments.nonEmpty) {", "originalCommit": "0b2f2711f6bf0d0b397b81ebdf8d640a3f8fd6b3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE4MDg2Mg==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436180862", "bodyText": "I can try.", "author": "ijuma", "createdAt": "2020-06-05T21:54:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE2NTg0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE2OTIyMw==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436169223", "bodyText": "I guess it is because of the semantics of DeleteRecords that we trust the checkpoint over the segment data. Might be worth a comment about that since it is a bit surprising.", "author": "hachikuji", "createdAt": "2020-06-05T21:20:00Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -806,14 +806,20 @@ class Log(@volatile private var _dir: File,\n       }\n     }\n \n-    if (logSegments.nonEmpty) {\n-      val logEndOffset = activeSegment.readNextOffset\n-      if (logEndOffset < logStartOffset) {\n-        warn(s\"Deleting all segments because logEndOffset ($logEndOffset) is smaller than logStartOffset ($logStartOffset). \" +\n-          \"This could happen if segment files were deleted from the file system.\")\n-        removeAndDeleteSegments(logSegments, asyncDelete = true)\n-      }\n-    }\n+    val logEndOffsetOption: Option[Long] =\n+      if (logSegments.nonEmpty) {\n+        val logEndOffset = activeSegment.readNextOffset\n+        if (logEndOffset >= logStartOffset)\n+          Some(logEndOffset)\n+        else {\n+          warn(s\"Deleting all segments because logEndOffset ($logEndOffset) is smaller than logStartOffset ($logStartOffset). \" +", "originalCommit": "0b2f2711f6bf0d0b397b81ebdf8d640a3f8fd6b3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE4MDk3Mw==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436180973", "bodyText": "I was also surprised, so I agree. :) Will do.", "author": "ijuma", "createdAt": "2020-06-05T21:54:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE2OTIyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3MjY5NA==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436172694", "bodyText": "Just checking, but the issue here is that we might mistakenly mark the directory is offline if the clean shutdown file did not exist?", "author": "hachikuji", "createdAt": "2020-06-05T21:29:58Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -360,7 +360,7 @@ class LogManager(logDirs: Seq[File],\n       for ((cleanShutdownFile, dirJobs) <- jobs) {\n         dirJobs.foreach(_.get)\n         try {\n-          cleanShutdownFile.delete()\n+          Files.deleteIfExists(cleanShutdownFile.toPath)", "originalCommit": "0b2f2711f6bf0d0b397b81ebdf8d640a3f8fd6b3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3ODEyMw==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436178123", "bodyText": "File.delete doesn't throw an exception and we don't check the result. So the previous code was very misleading. That's what I was trying to fix. And to avoid introducing an issue if the file did not exist, I am using deleteIfExists.", "author": "ijuma", "createdAt": "2020-06-05T21:45:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3MjY5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3NjQ0NQ==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436176445", "bodyText": "Hmm, logEndOffset is defined by nextOffsetMetadata, which is initialized after loadSegments returns. But recoverLog is called within loadSegments. So does this check work as expected or am I missing something?", "author": "hachikuji", "createdAt": "2020-06-05T21:40:55Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -826,8 +832,16 @@ class Log(@volatile private var _dir: File,\n         preallocate = config.preallocate))\n     }\n \n-    recoveryPoint = activeSegment.readNextOffset\n-    recoveryPoint\n+    // Update the recovery point if there was a clean shutdown and did not perform any changes to\n+    // the segment. Otherwise, we just ensure that the recovery point is not ahead of the log end\n+    // offset. To ensure correctness and to make it easier to reason about, it's best to only advance\n+    // the recovery point in flush(Long).\n+    if (hasCleanShutdownFile)\n+      logEndOffsetOption.foreach(recoveryPoint = _)\n+    else\n+      recoveryPoint = Math.min(recoveryPoint, logEndOffset)", "originalCommit": "0b2f2711f6bf0d0b397b81ebdf8d640a3f8fd6b3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE4MDYwOQ==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436180609", "bodyText": "I meant to use logEndOffsetOption, so this is a bug. :) It probably indicates that the variable name is bad (I kept it from before). If I had used the right variable, it would be:\ndef readNextOffset: Long = {\n    val fetchData = read(offsetIndex.lastOffset, log.sizeInBytes)\n    if (fetchData == null)\n      baseOffset\n    else\n      fetchData.records.batches.asScala.lastOption\n        .map(_.nextOffset)\n        .getOrElse(baseOffset)\n  }\nThe idea is that if we delete a bunch of segments, then the recovery point we passed to the Log constructor could be ahead of what remains.", "author": "ijuma", "createdAt": "2020-06-05T21:53:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3NjQ0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3NzEzOA==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436177138", "bodyText": "Can you help me understand what was wrong with this?", "author": "hachikuji", "createdAt": "2020-06-05T21:42:58Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -826,8 +832,16 @@ class Log(@volatile private var _dir: File,\n         preallocate = config.preallocate))\n     }\n \n-    recoveryPoint = activeSegment.readNextOffset", "originalCommit": "0b2f2711f6bf0d0b397b81ebdf8d640a3f8fd6b3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3OTM3Mw==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436179373", "bodyText": "Jun explained in the JIRA, The concern is that if there is a hard failure during recovery, you could end up with a situation where we persisted this, but we did not flush some of the segments. Does that make sense?", "author": "ijuma", "createdAt": "2020-06-05T21:49:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3NzEzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE4NzIwMQ==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r436187201", "bodyText": "Got it. So we may still be able to reopen an unflushed segment. That makes sense.", "author": "hachikuji", "createdAt": "2020-06-05T22:11:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3NzEzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MzQ5NjE5Nw==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r583496197", "bodyText": "Is it possible that a consumer could see \"phantom\" messages after recovery, even with this change?\n\nKafka Process dies with log data in page cache but not fsync'd\nRecovery process sees the un-fsync'd log data but it looks ok so recovery succeeds, nothing to do.\nConsumer fetches this data\nOS hard dies, losing page cache\nBroker is restarted and consumer tries to repeat fetch from same offset but data has gone.\n\nIt seems to me once recovery has run we should be sure that all log segments are persistently stored. I'm not sure if we're currently providing that guarantee. It would be pretty simple just to fsync each segment we recover.", "author": "purplefox", "createdAt": "2021-02-26T09:21:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3NzEzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MzY1NjU0NQ==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r583656545", "bodyText": "@purplefox To clarify, the case you are talking about is:\n\nA catastrophic scenario where a partition is offline (i.e. all replicas are down)\nThe OS was not shutdown\nThe OS did not flush the data to disk (this typically happens irrespective of our flushes due to OS configurations)\nThe replica that was the last member of the ISR comes back up, registers, to ZK and the Controller makes it the\nleader (since it was the last member of the ISR, if it was a different replica, it won't be given leadership without\nunclean leader election)\nThe hw is beyond the flushed segments\nConsumer fetches the data beyond the flushed segments\nOS hard dies\n\nThis is an interesting edge case, it seems incredibly unlikely, but possible if the hw can be beyond the flushed segments. @junrao & @hachikuji are we missing any detail that protects us against this?\nThe following has a similar impact:\n\nLeader accepts a write\nWrite is replicated, hw is incremented, but data is not flushed\nAll replicas die, but the ISR is not shrunk yet\nLeader receives a write, accepts it, replication doesn't happen since replicas are gone\nISR is shrunk, hw is incremented\nProducer won't receive a successful ack given min.isr=2, but the consumer reads data that is only in the leader\nLeader crashes and the unflushed data is gone (or hard disk dies and all the data in the leader is gone)\n\nFlushing the segments during recovery helps on some scenarios, but not the one I just mentioned (assuming I am not missing anything). @hachikuji had a \"strict min isr\" proposal where the ISR is never allowed to shrink below min.isr. I haven't thought about all the details, but perhaps that covers both issues. Thoughts @hachikuji?", "author": "ijuma", "createdAt": "2021-02-26T13:59:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3NzEzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MzgzNDExNw==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r583834117", "bodyText": "For the case that Tim mentioned, if we defer advancing the recovery point, at step 5, the broker will be forced to do log recovery for all unflushed data. If the data is corrupted on disk, it will be detected during recovery.\nFor the other case that Ismael mentioned, it is true that data can be lost in that case, but then this is the case where all replicas have failed.", "author": "junrao", "createdAt": "2021-02-26T18:26:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3NzEzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MzgzNTQwMQ==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r583835401", "bodyText": "I think it is a gap that there is no minimum replication factor before a write can get exposed. Any writes that end up seeing the NOT_ENOUGH_REPLICAS_AFTER_APPEND error code are more vulnerable. These are unacknowledged writes, and the producer is expected to retry, but the consumer can still read them once the ISR shrinks and we would still view it as \"data loss\" if the broker failed before they could be flushed to disk. With the \"strict min isr\" proposal, the leader is not allowed to shrink the ISR lower than some replication factor, which helps to plug this hole.\nGoing back to @purplefox's suggestion, it does seem like a good idea to flush segments beyond the recovery point during recovery. It kind of serves to constrain the initial state of the system which makes it easier to reason about (e.g. you only need to worry about the loss of unflushed data from the last restart). Some of the flush weaknesses probably still exist though regardless of this change.", "author": "hachikuji", "createdAt": "2021-02-26T18:28:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3NzEzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MzkwNzkyNQ==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r583907925", "bodyText": "We discussed this offline and we decided to stick with the fix in this PR for now and to file a separate JIRA to consider flushing unflushed segments during recovery. That would provide stronger guarantees after a restart.", "author": "ijuma", "createdAt": "2021-02-26T20:45:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3NzEzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDE2NzY5MA==", "url": "https://github.com/apache/kafka/pull/8812#discussion_r584167690", "bodyText": "Filed https://issues.apache.org/jira/browse/KAFKA-12386.", "author": "ijuma", "createdAt": "2021-02-27T17:59:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3NzEzOA=="}], "type": "inlineReview"}, {"oid": "91de172c89fa89b41ec77400dc83946e03c90dfa", "url": "https://github.com/apache/kafka/commit/91de172c89fa89b41ec77400dc83946e03c90dfa", "message": "KAFKA-10101: Fix edge cases in Log.recoverLog and LogManager.loadLogs\n\n1. Don't advance recovery point in `recoverLog` unless there was a clean\nshutdown.\n2. Ensure the recovery point is not ahead of the log end offset.\n3. Clean and flush leader epoch cache and truncate produce state manager\nif deleting segments due to log end offset being smaller than log start\noffset.\n4. If we are unable to delete clean shutdown file that exists, mark the\ndirectory as offline (this was the intent, but the code was wrong).", "committedDate": "2021-02-25T18:09:01Z", "type": "commit"}, {"oid": "91de172c89fa89b41ec77400dc83946e03c90dfa", "url": "https://github.com/apache/kafka/commit/91de172c89fa89b41ec77400dc83946e03c90dfa", "message": "KAFKA-10101: Fix edge cases in Log.recoverLog and LogManager.loadLogs\n\n1. Don't advance recovery point in `recoverLog` unless there was a clean\nshutdown.\n2. Ensure the recovery point is not ahead of the log end offset.\n3. Clean and flush leader epoch cache and truncate produce state manager\nif deleting segments due to log end offset being smaller than log start\noffset.\n4. If we are unable to delete clean shutdown file that exists, mark the\ndirectory as offline (this was the intent, but the code was wrong).", "committedDate": "2021-02-25T18:09:01Z", "type": "forcePushed"}, {"oid": "7b6abd68b61e9013707633fb903542a9ad4d947e", "url": "https://github.com/apache/kafka/commit/7b6abd68b61e9013707633fb903542a9ad4d947e", "message": "Address feedback and resolve FIXME", "committedDate": "2021-02-25T18:42:13Z", "type": "commit"}, {"oid": "796dd8be722f026a4d3cccf01ab86edf82e62348", "url": "https://github.com/apache/kafka/commit/796dd8be722f026a4d3cccf01ab86edf82e62348", "message": "Tweak logic", "committedDate": "2021-02-25T19:00:24Z", "type": "commit"}, {"oid": "46d5cefa5fd62584875f7cc021f7317b105c084d", "url": "https://github.com/apache/kafka/commit/46d5cefa5fd62584875f7cc021f7317b105c084d", "message": "Fix failing test", "committedDate": "2021-02-25T21:56:34Z", "type": "commit"}, {"oid": "46d5cefa5fd62584875f7cc021f7317b105c084d", "url": "https://github.com/apache/kafka/commit/46d5cefa5fd62584875f7cc021f7317b105c084d", "message": "Fix failing test", "committedDate": "2021-02-25T21:56:34Z", "type": "forcePushed"}]}