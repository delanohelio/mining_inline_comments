{"pr_number": 8818, "pr_title": "KAFKA-10086: Integration test for ensuring warmups are effective", "pr_createdAt": "2020-06-05T22:31:18Z", "pr_url": "https://github.com/apache/kafka/pull/8818", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5MzU2MA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436193560", "bodyText": "Bug 1: we can't remove active tasks in the cooperative algorithm, because this causes their state to get discarded (definitely for in-memory stores, and maybe for persistent ones, depending on the state cleaner).\nInstead, we convert them to standbys so they can keep warm.", "author": "vvcephei", "createdAt": "2020-06-05T22:34:27Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -1013,7 +1014,7 @@ private boolean addClientAssignments(final Map<String, Assignment> assignment,\n             final List<TopicPartition> activePartitionsList = new ArrayList<>();\n             final List<TaskId> assignedActiveList = new ArrayList<>();\n \n-            final boolean tasksRevoked = populateActiveTaskAndPartitionsLists(\n+            final Set<TaskId> activeTasksRemovedPendingRevokation = populateActiveTaskAndPartitionsLists(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5MzgzMg==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436193832", "bodyText": "just noticed this.", "author": "vvcephei", "createdAt": "2020-06-05T22:35:22Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -86,7 +87,7 @@\n     private boolean rebalanceInProgress = false;  // if we are in the middle of a rebalance, it is not safe to commit\n \n     // includes assigned & initialized tasks and unassigned tasks we locked temporarily during rebalance\n-    private Set<TaskId> lockedTaskDirectories = new HashSet<>();\n+    private final Set<TaskId> lockedTaskDirectories = new HashSet<>();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjgyNTI2NQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436825265", "bodyText": "Has checkstyle just been dropping the ball lately? Could we be unknowingly suppressing this...?", "author": "ableegoldman", "createdAt": "2020-06-08T16:10:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5MzgzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwOTAxNQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438409015", "bodyText": "Wondering the same...", "author": "mjsax", "createdAt": "2020-06-10T21:07:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5MzgzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQxOTMzNg==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438419336", "bodyText": "Checkstyle cannot verify final on fields, only variables and parameters.", "author": "vvcephei", "createdAt": "2020-06-10T21:30:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5MzgzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyNTQ0Nw==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438425447", "bodyText": "sigh", "author": "mjsax", "createdAt": "2020-06-10T21:44:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5MzgzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODUxNzIyNg==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438517226", "bodyText": "Yeah, I thought that was weird when I added the \"final\" checkstyle rule. Oh well...", "author": "vvcephei", "createdAt": "2020-06-11T03:06:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5MzgzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDA3NA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436194074", "bodyText": "Bug 2: tasks with only in-memory stores don't get task directories, so we were skipping them here. See the comment.", "author": "vvcephei", "createdAt": "2020-06-05T22:36:07Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -514,17 +515,24 @@ void handleLostAll() {\n \n     /**\n      * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n-     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryToLockAllNonEmptyTaskDirectories()}\n-     *\n-     * @return Map from task id to its total offset summed across all state stores\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryToLockAllNonEmptyTaskDirectories()}.\n+     * Does not include stateless or non-logged tasks.\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : lockedTaskDirectories) {\n+        // Not all tasks will create directories, and there may be directories for tasks we don't currently own,\n+        // so we consider all tasks that are either owned or on disk. This includes stateless tasks, which should\n+        // just have an empty changelogOffsets map.\n+        for (final TaskId id : union(HashSet::new, lockedTaskDirectories, tasks.keySet())) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5MjAwNQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438392005", "bodyText": "Why do we need to union both? Is task.keySet() not a super-set of lockedTaskDirectories ?", "author": "mjsax", "createdAt": "2020-06-10T20:33:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDA3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQxMjEyMQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438412121", "bodyText": "No, we attempt to lock every directory to make sure we're reporting offsets for old state that isn't currently assigned to anyone", "author": "ableegoldman", "createdAt": "2020-06-10T21:14:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDA3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQxOTU2Nw==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438419567", "bodyText": "Yep.", "author": "vvcephei", "createdAt": "2020-06-10T21:30:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDA3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDIyMQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436194221", "bodyText": "I independently implemented this check before Matthias's PR, and preferred this message.", "author": "vvcephei", "createdAt": "2020-06-05T22:36:47Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -605,13 +613,14 @@ private long sumOfChangelogOffsets(final TaskId id, final Map<TopicPartition, Lo\n             final long offset = changelogEntry.getValue();\n \n \n-            if (offset == Task.LATEST_OFFSET) { // this condition can only be true for active tasks; never for standby\n+            if (offset == Task.LATEST_OFFSET) {\n+                // this condition can only be true for active tasks; never for standby\n                 // for this case, the offset of all partitions is set to `LATEST_OFFSET`\n                 // and we \"forward\" the sentinel value directly\n                 return Task.LATEST_OFFSET;\n             } else {\n                 if (offset < 0) {\n-                    throw new IllegalStateException(\"Offset should not be negative.\");\n+                    throw new IllegalStateException(\"Expected not to get a sentinel offset, but got: \" + changelogEntry);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDM1Nw==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436194357", "bodyText": "Just a few cleanups in this first test.", "author": "vvcephei", "createdAt": "2020-06-05T22:37:17Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -75,21 +101,23 @@ public void shouldProperlyConfigureTheAssignor() throws NoSuchFieldException, Il\n \n         final String testId = safeUniqueTestName(getClass(), testName);\n         final String appId = \"appId_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n \n-        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, \"input\");\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDUyMg==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436194522", "bodyText": "Here are the new tests, one for in-memory and one for persistent. We expect exactly the same behavior for both.", "author": "vvcephei", "createdAt": "2020-06-05T22:37:50Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -142,4 +169,184 @@ public void shouldProperlyConfigureTheAssignor() throws NoSuchFieldException, Il\n             assertThat(taskAssignor, instanceOf(MyTaskAssignor.class));\n         }\n     }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDgzOQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436194839", "bodyText": "The fix for Bug 1 necessitated this test change.", "author": "vvcephei", "createdAt": "2020-06-05T22:39:04Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "diffHunk": "@@ -1634,16 +1636,15 @@ public void shouldReturnInterleavedAssignmentWithUnrevokedPartitionsRemovedWhenN\n \n         // The new consumer's assignment should be empty until c1 has the chance to revoke its partitions/tasks\n         assertThat(assignment.get(CONSUMER_2).partitions(), equalTo(emptyList()));\n-        assertThat(\n-            AssignmentInfo.decode(assignment.get(CONSUMER_2).userData()),\n-            equalTo(new AssignmentInfo(\n-                LATEST_SUPPORTED_VERSION,\n-                emptyList(),\n-                emptyMap(),\n-                emptyMap(),\n-                emptyMap(),\n-                0\n-            )));\n+\n+        final AssignmentInfo actualAssignment = AssignmentInfo.decode(assignment.get(CONSUMER_2).userData());\n+        assertThat(actualAssignment.version(), is(LATEST_SUPPORTED_VERSION));\n+        assertThat(actualAssignment.activeTasks(), empty());\n+        // Note we're not asserting anything about standbys. If the assignor gave an active task to CONSUMER_2, it would\n+        // be converted to a standby, but we don't know whether the assignor will do that.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwODQ4Nw==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438408487", "bodyText": "Not sure if I understand the comment. We implement the assignor so we should know what it does?", "author": "mjsax", "createdAt": "2020-06-10T21:06:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyMDEyMA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438420120", "bodyText": "The StreamsPartitionAssignor is independent of the TaskAssignor implementation, so the StreamsPartitionAssignorTest should make minimal assumptions about what the default TaskAssignor implementation will do.", "author": "vvcephei", "createdAt": "2020-06-10T21:32:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDgzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwNTI4Ng==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436205286", "bodyText": "By the way, this tripped me up quite a bit. Changing the clientState has absolutely no effect at this point. Might be nice to enforce that in code, but I'd like to defer it (I took a stab, but it's a lot of changes).", "author": "vvcephei", "createdAt": "2020-06-05T23:27:15Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -1084,12 +1088,15 @@ private boolean populateActiveTaskAndPartitionsLists(final List<TopicPartition>\n                 // If the partition is new to this consumer but is still owned by another, remove from the assignment\n                 // until it has been revoked and can safely be reassigned according to the COOPERATIVE protocol\n                 if (newPartitionForConsumer && allOwnedPartitions.contains(partition)) {\n-                    log.info(\"Removing task {} from assignment until it is safely revoked in followup rebalance\", taskId);\n-                    clientState.unassignActive(taskId);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwODU4Nw==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436208587", "bodyText": "I guess it was nice to have as a sanity check that the task really was assigned to this client, and that we haven't already tried to remove it or something. But maybe we can take that for granted...? (I only mention it because it helped me find a bug in the thread-level stickiness PR)", "author": "ableegoldman", "createdAt": "2020-06-05T23:45:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwNTI4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc1ODM1MQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436758351", "bodyText": "Ah, I see. I'll put it back with a comment that we're just using it for the internal assertions.", "author": "vvcephei", "createdAt": "2020-06-08T14:37:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwNTI4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NjU5Mw==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436196593", "bodyText": "\"apparently\" ? \ud83e\udd14", "author": "ableegoldman", "createdAt": "2020-06-05T22:46:25Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -514,17 +515,24 @@ void handleLostAll() {\n \n     /**\n      * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n-     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryToLockAllNonEmptyTaskDirectories()}\n-     *\n-     * @return Map from task id to its total offset summed across all state stores\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryToLockAllNonEmptyTaskDirectories()}.\n+     * Does not include stateless or non-logged tasks.\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : lockedTaskDirectories) {\n+        // Not all tasks will create directories, and there may be directories for tasks we don't currently own,\n+        // so we consider all tasks that are either owned or on disk. This includes stateless tasks, which should\n+        // just have an empty changelogOffsets map.\n+        for (final TaskId id : union(HashSet::new, lockedTaskDirectories, tasks.keySet())) {\n             final Task task = tasks.get(id);\n             if (task != null) {\n-                taskOffsetSums.put(id, sumOfChangelogOffsets(id, task.changelogOffsets()));\n+                final Map<TopicPartition, Long> changelogOffsets = task.changelogOffsets();\n+                if (changelogOffsets.isEmpty()) {\n+                    log.debug(\"Skipping to encode apparently stateless (or non-logged) offset sum for task {}\", id);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc1ODkyMw==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436758923", "bodyText": "Yep! We don't know that it's stateless, just that it didn't report any changelogs.", "author": "vvcephei", "createdAt": "2020-06-08T14:37:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NjU5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5OTAwNA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436199004", "bodyText": "What is NB?", "author": "ableegoldman", "createdAt": "2020-06-05T22:56:49Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -142,4 +169,184 @@ public void shouldProperlyConfigureTheAssignor() throws NoSuchFieldException, Il\n             assertThat(taskAssignor, instanceOf(MyTaskAssignor.class));\n         }\n     }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc2MDYyMA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436760620", "bodyText": "https://en.wikipedia.org/wiki/Nota_bene :)", "author": "vvcephei", "createdAt": "2020-06-08T14:40:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5OTAwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwOTA0NQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436209045", "bodyText": "Should we also add up the records restored here, since onRestoreEnd is not invoked if the task is closed during restoration?", "author": "ableegoldman", "createdAt": "2020-06-05T23:48:10Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -142,4 +169,184 @@ public void shouldProperlyConfigureTheAssignor() throws NoSuchFieldException, Il\n             assertThat(taskAssignor, instanceOf(MyTaskAssignor.class));\n         }\n     }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),\n+                mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()),\n+                mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName())\n+            )\n+        );\n+\n+        final StringBuilder kiloBuilder = new StringBuilder(1000);\n+        for (int i = 0; i < 1000; i++) {\n+            kiloBuilder.append('0');\n+        }\n+        final String kilo = kiloBuilder.toString();\n+\n+        try (final Producer<String, String> producer = new KafkaProducer<>(producerProperties)) {\n+            for (int i = 0; i < 1000; i++) {\n+                producer.send(new ProducerRecord<>(inputTopic, String.valueOf(i), kilo));\n+            }\n+        }\n+\n+        final Properties consumerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()),\n+                mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName())\n+            )\n+        );\n+\n+\n+        try (final KafkaStreams kafkaStreams0 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final KafkaStreams kafkaStreams1 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final Consumer<String, String> consumer = new KafkaConsumer<>(consumerProperties)) {\n+            kafkaStreams0.start();\n+\n+            // wait until all the input records are in the changelog\n+            TestUtils.waitForCondition(\n+                () -> getChangelogOffsetSum(changelogTopicPartitions, consumer) == 1000,\n+                120_000L,\n+                () -> \"Input records haven't all been written to the changelog: \" + getChangelogOffsetSum(changelogTopicPartitions, consumer)\n+            );\n+\n+            final AtomicLong instance1TotalRestored = new AtomicLong(-1);\n+            final CountDownLatch restoreCompleteLatch = new CountDownLatch(1);\n+            kafkaStreams1.setGlobalStateRestoreListener(new StateRestoreListener() {\n+                @Override\n+                public void onRestoreStart(final TopicPartition topicPartition,\n+                                           final String storeName,\n+                                           final long startingOffset,\n+                                           final long endingOffset) {\n+                }\n+\n+                @Override\n+                public void onBatchRestored(final TopicPartition topicPartition,\n+                                            final String storeName,\n+                                            final long batchEndOffset,\n+                                            final long numRestored) {\n+                }", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc2MDgzMQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436760831", "bodyText": "Good idea.", "author": "vvcephei", "createdAt": "2020-06-08T14:40:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwOTA0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjgyNjM0NA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436826344", "bodyText": "Cool, I was kind of hoping you would put this in a separate integration test class", "author": "ableegoldman", "createdAt": "2020-06-08T16:12:14Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5MzY0MA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438393640", "bodyText": "nit: simplify -> throws Exception (also above)", "author": "mjsax", "createdAt": "2020-06-10T20:36:48Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyMDYyMg==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438420622", "bodyText": "We'll have to agree to disagree on that ;)", "author": "vvcephei", "createdAt": "2020-06-10T21:33:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5MzY0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5MzkyOA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438393928", "bodyText": "Why set this? zero is the default anyway", "author": "mjsax", "createdAt": "2020-06-10T20:37:21Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),\n+                mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()),\n+                mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName())\n+            )\n+        );\n+\n+        final StringBuilder kiloBuilder = new StringBuilder(1000);\n+        for (int i = 0; i < 1000; i++) {\n+            kiloBuilder.append('0');\n+        }\n+        final String kilo = kiloBuilder.toString();\n+\n+        try (final Producer<String, String> producer = new KafkaProducer<>(producerProperties)) {\n+            for (int i = 0; i < 1000; i++) {\n+                producer.send(new ProducerRecord<>(inputTopic, String.valueOf(i), kilo));\n+            }\n+        }\n+\n+        final Properties consumerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()),\n+                mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName())\n+            )\n+        );\n+\n+\n+        try (final KafkaStreams kafkaStreams0 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final KafkaStreams kafkaStreams1 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final Consumer<String, String> consumer = new KafkaConsumer<>(consumerProperties)) {\n+            kafkaStreams0.start();\n+\n+            // wait until all the input records are in the changelog\n+            TestUtils.waitForCondition(\n+                () -> getChangelogOffsetSum(changelogTopicPartitions, consumer) == 1000,\n+                120_000L,\n+                () -> \"Input records haven't all been written to the changelog: \" + getChangelogOffsetSum(changelogTopicPartitions, consumer)\n+            );\n+\n+            final AtomicLong instance1TotalRestored = new AtomicLong(-1);\n+            final AtomicLong instance1NumRestored = new AtomicLong(-1);\n+            final CountDownLatch restoreCompleteLatch = new CountDownLatch(1);\n+            kafkaStreams1.setGlobalStateRestoreListener(new StateRestoreListener() {\n+                @Override\n+                public void onRestoreStart(final TopicPartition topicPartition,\n+                                           final String storeName,\n+                                           final long startingOffset,\n+                                           final long endingOffset) {\n+                }\n+\n+                @Override\n+                public void onBatchRestored(final TopicPartition topicPartition,\n+                                            final String storeName,\n+                                            final long batchEndOffset,\n+                                            final long numRestored) {\n+                    instance1NumRestored.accumulateAndGet(\n+                        numRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                }\n+\n+                @Override\n+                public void onRestoreEnd(final TopicPartition topicPartition,\n+                                         final String storeName,\n+                                         final long totalRestored) {\n+                    instance1TotalRestored.accumulateAndGet(\n+                        totalRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                    restoreCompleteLatch.countDown();\n+                }\n+            });\n+            final int assignmentsBeforeScaleOut = assignmentsCompleted.get();\n+            final AtomicInteger assignmentsAfterScaleOut = new AtomicInteger(0);\n+            kafkaStreams1.start();\n+            TestUtils.waitForCondition(\n+                () -> {\n+                    assignmentLock.lock();\n+                    try {\n+                        assignmentsAfterScaleOut.set(assignmentsCompleted.get());\n+                        if (assignmentsBeforeScaleOut < assignmentsAfterScaleOut.get()) {\n+                            // the first assignment after adding a node should be unstable while we warm up the state.\n+                            assertThat(assignmentStable.get(), is(false));\n+                            return true;\n+                        } else {\n+                            return false;\n+                        }\n+                    } finally {\n+                        assignmentLock.unlock();\n+                    }\n+                },\n+                120_000L,\n+                \"Never saw a first assignment after scale out: \" + assignmentsCompleted.get()\n+            );\n+\n+            TestUtils.waitForCondition(\n+                assignmentStable::get,\n+                120_000L,\n+                \"Assignment hasn't become stable: \" + assignmentsCompleted.get() +\n+                    \" Note, if this does fail, check and see if the new instance just failed to catch up within\" +\n+                    \" the probing rebalance interval. A full minute should be long enough to read ~500 records\" +\n+                    \" in any test environment, but you never know...\"\n+            );\n+\n+            restoreCompleteLatch.await();\n+            // We should finalize the restoration without having restored any records (because they're already in\n+            // the store. Otherwise, we failed to properly re-use the state from the standby.\n+            assertThat(instance1TotalRestored.get(), is(0L));\n+            // Belt-and-suspenders check that we never even attempt to restore any records.\n+            assertThat(instance1NumRestored.get(), is(-1L));\n+        }\n+    }\n+\n+    private static Properties streamsProperties(final String appId,\n+                                                final AssignmentListener configuredAssignmentListener) {\n+        return mkObjectProperties(\n+            mkMap(\n+                mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                mkEntry(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, \"0\"),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyMDg0NA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438420844", "bodyText": "Just to be explicit, since the test depends on the value of zero for its assertions.", "author": "vvcephei", "createdAt": "2020-06-10T21:33:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5MzkyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyNjUwOA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438426508", "bodyText": "Well, so it's a guard that we don't change the default? At least, a comment would be good -- otherwise, one might remove it.", "author": "mjsax", "createdAt": "2020-06-10T21:46:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5MzkyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQzNjYyNg==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438436626", "bodyText": "Not a guard per se, more like a narrative. The test explicitly expects no replicas, so we configure no replicas. As a reader of the test code, you shouldn't have to have memorized the default value of every configuration.", "author": "vvcephei", "createdAt": "2020-06-10T22:12:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5MzkyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5NDMzMA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438394330", "bodyText": "The comment above say, the minimum is 60secs -- seems we control this minimum -- could we reduce it?", "author": "mjsax", "createdAt": "2020-06-10T20:38:11Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),\n+                mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()),\n+                mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName())\n+            )\n+        );\n+\n+        final StringBuilder kiloBuilder = new StringBuilder(1000);\n+        for (int i = 0; i < 1000; i++) {\n+            kiloBuilder.append('0');\n+        }\n+        final String kilo = kiloBuilder.toString();\n+\n+        try (final Producer<String, String> producer = new KafkaProducer<>(producerProperties)) {\n+            for (int i = 0; i < 1000; i++) {\n+                producer.send(new ProducerRecord<>(inputTopic, String.valueOf(i), kilo));\n+            }\n+        }\n+\n+        final Properties consumerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()),\n+                mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName())\n+            )\n+        );\n+\n+\n+        try (final KafkaStreams kafkaStreams0 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final KafkaStreams kafkaStreams1 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final Consumer<String, String> consumer = new KafkaConsumer<>(consumerProperties)) {\n+            kafkaStreams0.start();\n+\n+            // wait until all the input records are in the changelog\n+            TestUtils.waitForCondition(\n+                () -> getChangelogOffsetSum(changelogTopicPartitions, consumer) == 1000,\n+                120_000L,\n+                () -> \"Input records haven't all been written to the changelog: \" + getChangelogOffsetSum(changelogTopicPartitions, consumer)\n+            );\n+\n+            final AtomicLong instance1TotalRestored = new AtomicLong(-1);\n+            final AtomicLong instance1NumRestored = new AtomicLong(-1);\n+            final CountDownLatch restoreCompleteLatch = new CountDownLatch(1);\n+            kafkaStreams1.setGlobalStateRestoreListener(new StateRestoreListener() {\n+                @Override\n+                public void onRestoreStart(final TopicPartition topicPartition,\n+                                           final String storeName,\n+                                           final long startingOffset,\n+                                           final long endingOffset) {\n+                }\n+\n+                @Override\n+                public void onBatchRestored(final TopicPartition topicPartition,\n+                                            final String storeName,\n+                                            final long batchEndOffset,\n+                                            final long numRestored) {\n+                    instance1NumRestored.accumulateAndGet(\n+                        numRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                }\n+\n+                @Override\n+                public void onRestoreEnd(final TopicPartition topicPartition,\n+                                         final String storeName,\n+                                         final long totalRestored) {\n+                    instance1TotalRestored.accumulateAndGet(\n+                        totalRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                    restoreCompleteLatch.countDown();\n+                }\n+            });\n+            final int assignmentsBeforeScaleOut = assignmentsCompleted.get();\n+            final AtomicInteger assignmentsAfterScaleOut = new AtomicInteger(0);\n+            kafkaStreams1.start();\n+            TestUtils.waitForCondition(\n+                () -> {\n+                    assignmentLock.lock();\n+                    try {\n+                        assignmentsAfterScaleOut.set(assignmentsCompleted.get());\n+                        if (assignmentsBeforeScaleOut < assignmentsAfterScaleOut.get()) {\n+                            // the first assignment after adding a node should be unstable while we warm up the state.\n+                            assertThat(assignmentStable.get(), is(false));\n+                            return true;\n+                        } else {\n+                            return false;\n+                        }\n+                    } finally {\n+                        assignmentLock.unlock();\n+                    }\n+                },\n+                120_000L,\n+                \"Never saw a first assignment after scale out: \" + assignmentsCompleted.get()\n+            );\n+\n+            TestUtils.waitForCondition(\n+                assignmentStable::get,\n+                120_000L,\n+                \"Assignment hasn't become stable: \" + assignmentsCompleted.get() +\n+                    \" Note, if this does fail, check and see if the new instance just failed to catch up within\" +\n+                    \" the probing rebalance interval. A full minute should be long enough to read ~500 records\" +\n+                    \" in any test environment, but you never know...\"\n+            );\n+\n+            restoreCompleteLatch.await();\n+            // We should finalize the restoration without having restored any records (because they're already in\n+            // the store. Otherwise, we failed to properly re-use the state from the standby.\n+            assertThat(instance1TotalRestored.get(), is(0L));\n+            // Belt-and-suspenders check that we never even attempt to restore any records.\n+            assertThat(instance1NumRestored.get(), is(-1L));\n+        }\n+    }\n+\n+    private static Properties streamsProperties(final String appId,\n+                                                final AssignmentListener configuredAssignmentListener) {\n+        return mkObjectProperties(\n+            mkMap(\n+                mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                mkEntry(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, \"0\"),\n+                mkEntry(StreamsConfig.ACCEPTABLE_RECOVERY_LAG_CONFIG, \"0\"), // make the warmup catch up completely\n+                mkEntry(StreamsConfig.MAX_WARMUP_REPLICAS_CONFIG, \"2\"),\n+                mkEntry(StreamsConfig.PROBING_REBALANCE_INTERVAL_MS_CONFIG, \"60000\"),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyMTIyMg==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438421222", "bodyText": "We could, but it would require a KIP update.\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-441%3A+Smooth+Scaling+Out+for+Kafka+Streams#KIP441:SmoothScalingOutforKafkaStreams-Parameters\n\nMust be at least 1 minute", "author": "vvcephei", "createdAt": "2020-06-10T21:34:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5NDMzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyNjY3MQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438426671", "bodyText": "I see.", "author": "mjsax", "createdAt": "2020-06-10T21:47:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5NDMzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5NTE3NA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438395174", "bodyText": "why do we set retries to zero?", "author": "mjsax", "createdAt": "2020-06-10T20:39:59Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyMTUyMQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438421521", "bodyText": "I just copied this config, maybe this particular setting is not necessary.", "author": "vvcephei", "createdAt": "2020-06-10T21:35:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5NTE3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyNzkyMA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438427920", "bodyText": "In general, we should rely on default IMHO. And as you set ack=all it seems you care about reliable data delivery, so setting retries=0 seems to contradict it somewhat.", "author": "mjsax", "createdAt": "2020-06-10T21:50:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5NTE3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQzNjA5OQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438436099", "bodyText": "Yeah, I don't recall which other test I copied it from.", "author": "vvcephei", "createdAt": "2020-06-10T22:11:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5NTE3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNDI0MA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438404240", "bodyText": "Seems we don't need variable assignmentsAfterScaleOut and can just call assignmentsCompleted.get() here?\nAt least, we can make it a local int variable?", "author": "mjsax", "createdAt": "2020-06-10T20:57:47Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),\n+                mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()),\n+                mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName())\n+            )\n+        );\n+\n+        final StringBuilder kiloBuilder = new StringBuilder(1000);\n+        for (int i = 0; i < 1000; i++) {\n+            kiloBuilder.append('0');\n+        }\n+        final String kilo = kiloBuilder.toString();\n+\n+        try (final Producer<String, String> producer = new KafkaProducer<>(producerProperties)) {\n+            for (int i = 0; i < 1000; i++) {\n+                producer.send(new ProducerRecord<>(inputTopic, String.valueOf(i), kilo));\n+            }\n+        }\n+\n+        final Properties consumerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()),\n+                mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName())\n+            )\n+        );\n+\n+\n+        try (final KafkaStreams kafkaStreams0 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final KafkaStreams kafkaStreams1 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final Consumer<String, String> consumer = new KafkaConsumer<>(consumerProperties)) {\n+            kafkaStreams0.start();\n+\n+            // wait until all the input records are in the changelog\n+            TestUtils.waitForCondition(\n+                () -> getChangelogOffsetSum(changelogTopicPartitions, consumer) == 1000,\n+                120_000L,\n+                () -> \"Input records haven't all been written to the changelog: \" + getChangelogOffsetSum(changelogTopicPartitions, consumer)\n+            );\n+\n+            final AtomicLong instance1TotalRestored = new AtomicLong(-1);\n+            final AtomicLong instance1NumRestored = new AtomicLong(-1);\n+            final CountDownLatch restoreCompleteLatch = new CountDownLatch(1);\n+            kafkaStreams1.setGlobalStateRestoreListener(new StateRestoreListener() {\n+                @Override\n+                public void onRestoreStart(final TopicPartition topicPartition,\n+                                           final String storeName,\n+                                           final long startingOffset,\n+                                           final long endingOffset) {\n+                }\n+\n+                @Override\n+                public void onBatchRestored(final TopicPartition topicPartition,\n+                                            final String storeName,\n+                                            final long batchEndOffset,\n+                                            final long numRestored) {\n+                    instance1NumRestored.accumulateAndGet(\n+                        numRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                }\n+\n+                @Override\n+                public void onRestoreEnd(final TopicPartition topicPartition,\n+                                         final String storeName,\n+                                         final long totalRestored) {\n+                    instance1TotalRestored.accumulateAndGet(\n+                        totalRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                    restoreCompleteLatch.countDown();\n+                }\n+            });\n+            final int assignmentsBeforeScaleOut = assignmentsCompleted.get();\n+            final AtomicInteger assignmentsAfterScaleOut = new AtomicInteger(0);\n+            kafkaStreams1.start();\n+            TestUtils.waitForCondition(\n+                () -> {\n+                    assignmentLock.lock();\n+                    try {\n+                        assignmentsAfterScaleOut.set(assignmentsCompleted.get());\n+                        if (assignmentsBeforeScaleOut < assignmentsAfterScaleOut.get()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyNDMzOA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438424338", "bodyText": "Yeah, looks like it. I was previously making an assertion about it outside the callback. Good catch!", "author": "vvcephei", "createdAt": "2020-06-10T21:41:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNDI0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNTEzNw==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438405137", "bodyText": "Should the assignment not because stable eventually again? Ie, are we subject to a potential race condition here? (Maybe this assertions is only safe if assignmentsBeforeScaleOut + 1 == assignmentsAfterScaleOut ?)", "author": "mjsax", "createdAt": "2020-06-10T20:59:41Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),\n+                mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()),\n+                mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName())\n+            )\n+        );\n+\n+        final StringBuilder kiloBuilder = new StringBuilder(1000);\n+        for (int i = 0; i < 1000; i++) {\n+            kiloBuilder.append('0');\n+        }\n+        final String kilo = kiloBuilder.toString();\n+\n+        try (final Producer<String, String> producer = new KafkaProducer<>(producerProperties)) {\n+            for (int i = 0; i < 1000; i++) {\n+                producer.send(new ProducerRecord<>(inputTopic, String.valueOf(i), kilo));\n+            }\n+        }\n+\n+        final Properties consumerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()),\n+                mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName())\n+            )\n+        );\n+\n+\n+        try (final KafkaStreams kafkaStreams0 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final KafkaStreams kafkaStreams1 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final Consumer<String, String> consumer = new KafkaConsumer<>(consumerProperties)) {\n+            kafkaStreams0.start();\n+\n+            // wait until all the input records are in the changelog\n+            TestUtils.waitForCondition(\n+                () -> getChangelogOffsetSum(changelogTopicPartitions, consumer) == 1000,\n+                120_000L,\n+                () -> \"Input records haven't all been written to the changelog: \" + getChangelogOffsetSum(changelogTopicPartitions, consumer)\n+            );\n+\n+            final AtomicLong instance1TotalRestored = new AtomicLong(-1);\n+            final AtomicLong instance1NumRestored = new AtomicLong(-1);\n+            final CountDownLatch restoreCompleteLatch = new CountDownLatch(1);\n+            kafkaStreams1.setGlobalStateRestoreListener(new StateRestoreListener() {\n+                @Override\n+                public void onRestoreStart(final TopicPartition topicPartition,\n+                                           final String storeName,\n+                                           final long startingOffset,\n+                                           final long endingOffset) {\n+                }\n+\n+                @Override\n+                public void onBatchRestored(final TopicPartition topicPartition,\n+                                            final String storeName,\n+                                            final long batchEndOffset,\n+                                            final long numRestored) {\n+                    instance1NumRestored.accumulateAndGet(\n+                        numRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                }\n+\n+                @Override\n+                public void onRestoreEnd(final TopicPartition topicPartition,\n+                                         final String storeName,\n+                                         final long totalRestored) {\n+                    instance1TotalRestored.accumulateAndGet(\n+                        totalRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                    restoreCompleteLatch.countDown();\n+                }\n+            });\n+            final int assignmentsBeforeScaleOut = assignmentsCompleted.get();\n+            final AtomicInteger assignmentsAfterScaleOut = new AtomicInteger(0);\n+            kafkaStreams1.start();\n+            TestUtils.waitForCondition(\n+                () -> {\n+                    assignmentLock.lock();\n+                    try {\n+                        assignmentsAfterScaleOut.set(assignmentsCompleted.get());\n+                        if (assignmentsBeforeScaleOut < assignmentsAfterScaleOut.get()) {\n+                            // the first assignment after adding a node should be unstable while we warm up the state.\n+                            assertThat(assignmentStable.get(), is(false));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyNjEwNw==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438426107", "bodyText": "Yeah, that's true. I think it mitigates the race condition that it'll take a full minute to do the next probing rebalance and get to a stable assignment, but it would be better to avoid the race.", "author": "vvcephei", "createdAt": "2020-06-10T21:45:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNTEzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNjc5MQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438406791", "bodyText": "This should never be called -- would it be simpler if the just throw an exception if and of the listener methods is called?", "author": "mjsax", "createdAt": "2020-06-10T21:02:51Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),\n+                mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()),\n+                mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName())\n+            )\n+        );\n+\n+        final StringBuilder kiloBuilder = new StringBuilder(1000);\n+        for (int i = 0; i < 1000; i++) {\n+            kiloBuilder.append('0');\n+        }\n+        final String kilo = kiloBuilder.toString();\n+\n+        try (final Producer<String, String> producer = new KafkaProducer<>(producerProperties)) {\n+            for (int i = 0; i < 1000; i++) {\n+                producer.send(new ProducerRecord<>(inputTopic, String.valueOf(i), kilo));\n+            }\n+        }\n+\n+        final Properties consumerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()),\n+                mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName())\n+            )\n+        );\n+\n+\n+        try (final KafkaStreams kafkaStreams0 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final KafkaStreams kafkaStreams1 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final Consumer<String, String> consumer = new KafkaConsumer<>(consumerProperties)) {\n+            kafkaStreams0.start();\n+\n+            // wait until all the input records are in the changelog\n+            TestUtils.waitForCondition(\n+                () -> getChangelogOffsetSum(changelogTopicPartitions, consumer) == 1000,\n+                120_000L,\n+                () -> \"Input records haven't all been written to the changelog: \" + getChangelogOffsetSum(changelogTopicPartitions, consumer)\n+            );\n+\n+            final AtomicLong instance1TotalRestored = new AtomicLong(-1);\n+            final AtomicLong instance1NumRestored = new AtomicLong(-1);\n+            final CountDownLatch restoreCompleteLatch = new CountDownLatch(1);\n+            kafkaStreams1.setGlobalStateRestoreListener(new StateRestoreListener() {\n+                @Override\n+                public void onRestoreStart(final TopicPartition topicPartition,\n+                                           final String storeName,\n+                                           final long startingOffset,\n+                                           final long endingOffset) {\n+                }\n+\n+                @Override\n+                public void onBatchRestored(final TopicPartition topicPartition,\n+                                            final String storeName,\n+                                            final long batchEndOffset,\n+                                            final long numRestored) {\n+                    instance1NumRestored.accumulateAndGet(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQzODM1Nw==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438438357", "bodyText": "I'm not sure; that seems to make assumptions about how exactly the restore logic would behave when you're already caught up.\nI guess the same reasoning would apply to the other callback, we should never call it with a nonzero number of restored records, so we could just throw an exception from the callback.\nIn both cases, though, it seems better to make assertions in the scope of the test, rather than deep inside a callback in one of the threads in one of the streams instances and hope that it somehow bubbles up into a test failure.", "author": "vvcephei", "createdAt": "2020-06-10T22:17:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNjc5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNzI3OA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438407278", "bodyText": "Ugly. Old formatting is nicer.", "author": "mjsax", "createdAt": "2020-06-10T21:03:47Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -59,7 +61,8 @@\n     public TestName testName = new TestName();\n \n     // Just a dummy implementation so we can check the config\n-    public static final class MyTaskAssignor extends HighAvailabilityTaskAssignor implements TaskAssignor { }\n+    public static final class MyTaskAssignor extends HighAvailabilityTaskAssignor implements TaskAssignor {\n+    }", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyMjk5MQ==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438422991", "bodyText": "Lol, harsh words (I agree though)", "author": "ableegoldman", "createdAt": "2020-06-10T21:38:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNzI3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyNTI5NA==", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438425294", "bodyText": "It was autoformatting I overlooked in the last revision.", "author": "vvcephei", "createdAt": "2020-06-10T21:43:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNzI3OA=="}], "type": "inlineReview"}, {"oid": "ebac2fb07bf99d47de80acc81bb381d2bd6a8988", "url": "https://github.com/apache/kafka/commit/ebac2fb07bf99d47de80acc81bb381d2bd6a8988", "message": "KAFKA-10086: Integration test for ensuring warmups are effective", "committedDate": "2020-06-11T03:02:04Z", "type": "commit"}, {"oid": "755800864fdc46257bc9c4a61b5815a17808f4b0", "url": "https://github.com/apache/kafka/commit/755800864fdc46257bc9c4a61b5815a17808f4b0", "message": "formatting", "committedDate": "2020-06-11T03:03:04Z", "type": "commit"}, {"oid": "49fabce40cdb1ed4723cb8438c582ee12e9f3321", "url": "https://github.com/apache/kafka/commit/49fabce40cdb1ed4723cb8438c582ee12e9f3321", "message": "checkstyle", "committedDate": "2020-06-11T03:12:01Z", "type": "commit"}, {"oid": "ca56b56483788f64c3f12d7eddfcc0c87fe036c4", "url": "https://github.com/apache/kafka/commit/ca56b56483788f64c3f12d7eddfcc0c87fe036c4", "message": "more style", "committedDate": "2020-06-11T03:33:46Z", "type": "commit"}]}