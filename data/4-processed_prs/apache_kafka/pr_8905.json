{"pr_number": 8905, "pr_title": "KAFKA-10173: Fix suppress changelog binary schema compatibility", "pr_createdAt": "2020-06-19T22:14:09Z", "pr_url": "https://github.com/apache/kafka/pull/8905", "timeline": [{"oid": "4c393332ac179d055d2e06a8f34170ed3d4ae628", "url": "https://github.com/apache/kafka/commit/4c393332ac179d055d2e06a8f34170ed3d4ae628", "message": "KAFKA-10173: Directly use Arrays.equals for version comparison", "committedDate": "2020-06-19T21:26:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2MjU5NA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443062594", "bodyText": "On the side, I realized we can consolidate this check and perform it first, rather than after we're already written bad data into the buffer.", "author": "vvcephei", "createdAt": "2020-06-19T22:14:54Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -286,6 +289,15 @@ private void logTombstone(final Bytes key) {\n \n     private void restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> batch) {\n         for (final ConsumerRecord<byte[], byte[]> record : batch) {\n+            if (record.partition() != partition) {\n+                throw new IllegalStateException(\n+                    String.format(\n+                        \"record partition [%d] is being restored by the wrong suppress partition [%d]\",\n+                        record.partition(),\n+                        partition\n+                    )\n+                );\n+            }", "originalCommit": "4c393332ac179d055d2e06a8f34170ed3d4ae628", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2Mjk3MQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443062971", "bodyText": "This is the fix (although it was probably fine before). The implementation of Header.equals is not specified by any contract, so it's safer to perform a direct comparison on the header values. Just as before, I'm comparing byte arrays to avoid deserializing the value.", "author": "vvcephei", "createdAt": "2020-06-19T22:16:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -339,7 +341,7 @@ private void restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> batch\n                             recordContext\n                         )\n                     );\n-                } else if (V_1_CHANGELOG_HEADERS.lastHeader(\"v\").equals(record.headers().lastHeader(\"v\"))) {\n+                } else if (Arrays.equals(record.headers().lastHeader(\"v\").value(), V_1_CHANGELOG_HEADER_VALUE)) {", "originalCommit": "4c393332ac179d055d2e06a8f34170ed3d4ae628", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEzNzcxOQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443137719", "bodyText": "my IDEA says this variable is never used.", "author": "chia7712", "createdAt": "2020-06-20T15:07:18Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -58,10 +59,12 @@\n public final class InMemoryTimeOrderedKeyValueBuffer<K, V> implements TimeOrderedKeyValueBuffer<K, V> {\n     private static final BytesSerializer KEY_SERIALIZER = new BytesSerializer();\n     private static final ByteArraySerializer VALUE_SERIALIZER = new ByteArraySerializer();\n+    private static final byte[] V_1_CHANGELOG_HEADER_VALUE = {(byte) 1};\n     private static final RecordHeaders V_1_CHANGELOG_HEADERS =", "originalCommit": "4c393332ac179d055d2e06a8f34170ed3d4ae628", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE4MDc0MA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443180740", "bodyText": "I saw it is used in line 342.", "author": "guozhangwang", "createdAt": "2020-06-21T04:33:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEzNzcxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE4NDAyOQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443184029", "bodyText": "my bad. The unused variable is V_1_CHANGELOG_HEADERS rather than V_1_CHANGELOG_HEADER_VALUE", "author": "chia7712", "createdAt": "2020-06-21T05:41:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEzNzcxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI1Mzc2MA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443253760", "bodyText": "Ah, right. My mistake. Thanks for pointing it out.", "author": "vvcephei", "createdAt": "2020-06-21T20:24:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEzNzcxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEzNzk0NQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443137945", "bodyText": "nit:\nWe seek the last header many times. Could we reuse the return value?", "author": "chia7712", "createdAt": "2020-06-20T15:10:31Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -299,16 +311,6 @@ private void restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> batch\n                         minTimestamp = sortedMap.isEmpty() ? Long.MAX_VALUE : sortedMap.firstKey().time();\n                     }\n                 }\n-\n-                if (record.partition() != partition) {\n-                    throw new IllegalStateException(\n-                        String.format(\n-                            \"record partition [%d] is being restored by the wrong suppress partition [%d]\",\n-                            record.partition(),\n-                            partition\n-                        )\n-                    );\n-                }\n             } else {\n                 if (record.headers().lastHeader(\"v\") == null) {", "originalCommit": "4c393332ac179d055d2e06a8f34170ed3d4ae628", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI1Mzc5Ng==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443253796", "bodyText": "Sure!", "author": "vvcephei", "createdAt": "2020-06-21T20:24:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEzNzk0NQ=="}], "type": "inlineReview"}, {"oid": "98786b338b41173a5232754d5d83c0f19392b908", "url": "https://github.com/apache/kafka/commit/98786b338b41173a5232754d5d83c0f19392b908", "message": "converting upgrade test to smoke test", "committedDate": "2020-06-24T15:25:01Z", "type": "commit"}, {"oid": "5ba0c59bd7706f18ef8e9604f699827e8f7e5f29", "url": "https://github.com/apache/kafka/commit/5ba0c59bd7706f18ef8e9604f699827e8f7e5f29", "message": "wip debugging suppress buffer", "committedDate": "2020-06-24T15:25:17Z", "type": "commit"}, {"oid": "f556a41f6ad96b4c2738df801196a2e5c3828a98", "url": "https://github.com/apache/kafka/commit/f556a41f6ad96b4c2738df801196a2e5c3828a98", "message": "asdf", "committedDate": "2020-06-24T22:15:09Z", "type": "commit"}, {"oid": "7af7935a67c75ee263d05187a765bb53ed6030d7", "url": "https://github.com/apache/kafka/commit/7af7935a67c75ee263d05187a765bb53ed6030d7", "message": "fix", "committedDate": "2020-06-24T22:47:46Z", "type": "commit"}, {"oid": "495aebc9609b09318f246cbe801e55662263d14e", "url": "https://github.com/apache/kafka/commit/495aebc9609b09318f246cbe801e55662263d14e", "message": "upgrade test passed (2.3.1 -> trunk)", "committedDate": "2020-06-25T03:29:42Z", "type": "commit"}, {"oid": "e29728f1312b1fdbdf3294ee12439b7f5414f23b", "url": "https://github.com/apache/kafka/commit/e29728f1312b1fdbdf3294ee12439b7f5414f23b", "message": "wip", "committedDate": "2020-06-25T04:09:08Z", "type": "commit"}, {"oid": "738eef739b53f395f4d7769f946fc8ba70ea89d7", "url": "https://github.com/apache/kafka/commit/738eef739b53f395f4d7769f946fc8ba70ea89d7", "message": "cleanup", "committedDate": "2020-06-25T04:21:34Z", "type": "commit"}, {"oid": "9cd927abc299fafa599324675900171e1f8b6360", "url": "https://github.com/apache/kafka/commit/9cd927abc299fafa599324675900171e1f8b6360", "message": "cleanup", "committedDate": "2020-06-25T04:27:17Z", "type": "commit"}, {"oid": "23964583a267948249ae300466f15c2de9b1b647", "url": "https://github.com/apache/kafka/commit/23964583a267948249ae300466f15c2de9b1b647", "message": "fix test", "committedDate": "2020-06-25T17:28:57Z", "type": "commit"}, {"oid": "29ab2070b0a7e8124de91a8cc2ce01b64ac6c1eb", "url": "https://github.com/apache/kafka/commit/29ab2070b0a7e8124de91a8cc2ce01b64ac6c1eb", "message": "direct encoding of restore test vx", "committedDate": "2020-06-25T19:29:29Z", "type": "commit"}, {"oid": "dab0fc1c157fb6183469056dbf715007f6721880", "url": "https://github.com/apache/kafka/commit/dab0fc1c157fb6183469056dbf715007f6721880", "message": "direct encoding of restore test v1", "committedDate": "2020-06-25T19:49:56Z", "type": "commit"}, {"oid": "776efb673d0b8b71f63d5bc92fc538d4bf9eeea2", "url": "https://github.com/apache/kafka/commit/776efb673d0b8b71f63d5bc92fc538d4bf9eeea2", "message": "add v2 and v3", "committedDate": "2020-06-25T20:19:11Z", "type": "commit"}, {"oid": "e6e0b48a32945c2ddaa16da98148b5ca3b7634f5", "url": "https://github.com/apache/kafka/commit/e6e0b48a32945c2ddaa16da98148b5ca3b7634f5", "message": "cleanup", "committedDate": "2020-06-25T20:31:19Z", "type": "commit"}, {"oid": "65a549f98d27010e43d07154b3c30e3a9e41f318", "url": "https://github.com/apache/kafka/commit/65a549f98d27010e43d07154b3c30e3a9e41f318", "message": "adding other old versions", "committedDate": "2020-06-26T02:00:45Z", "type": "commit"}, {"oid": "41a4db1a7c5bd2bb41a861e6f7889ddd9bf062a4", "url": "https://github.com/apache/kafka/commit/41a4db1a7c5bd2bb41a861e6f7889ddd9bf062a4", "message": "fallback to make 2.4 work", "committedDate": "2020-06-26T02:01:08Z", "type": "commit"}, {"oid": "63f1cc3ab18d301c085aeaecad98879a4d85e49b", "url": "https://github.com/apache/kafka/commit/63f1cc3ab18d301c085aeaecad98879a4d85e49b", "message": "remove 2.1. cf KAFKA-10203", "committedDate": "2020-06-26T03:07:58Z", "type": "commit"}, {"oid": "2465162c456560549b7678e3b13ed505023f8737", "url": "https://github.com/apache/kafka/commit/2465162c456560549b7678e3b13ed505023f8737", "message": "reduce cyclomatic complexity", "committedDate": "2020-06-26T03:24:27Z", "type": "commit"}, {"oid": "d513fe881aede6956c19dddae5827f9cc88a870e", "url": "https://github.com/apache/kafka/commit/d513fe881aede6956c19dddae5827f9cc88a870e", "message": "Remove ParallelGC jvm param\n\nI was getting this exception, and somehow, the parallel GC parameter was the culprit\n\n    java.lang.OutOfMemoryError: Java heap space\n        at org.apache.kafka.streams.kstream.internals.FullChangeSerde.decomposeLegacyFormattedArrayIntoChangeArrays(FullChangeSerde.java:82)\n        at org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.deserializeV2(TimeOrderedKeyValueBufferChangelogDeserializationHelper.java:90)\n        at org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.duckTypeV2(TimeOrderedKeyValueBufferChangelogDeserializationHelper.java:61)\n        at org.apache.kafka.streams.state.internals.InMemoryTimeOrderedKeyValueBuffer.restoreBatch(InMemoryTimeOrderedKeyValueBuffer.java:369)\n        at org.apache.kafka.streams.state.internals.InMemoryTimeOrderedKeyValueBuffer$$Lambda$284/0x00000001002cb440.restoreBatch(Unknown Source)\n        at org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferTest.shouldRestoreV3FormatWithV2Header(TimeOrderedKeyValueBufferTest.java:742)", "committedDate": "2020-06-26T15:24:59Z", "type": "commit"}, {"oid": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "url": "https://github.com/apache/kafka/commit/7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "message": "cleanup", "committedDate": "2020-06-26T15:44:38Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2NTU1OQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446265559", "bodyText": "@ijuma , you'll probably want to know about this.\nI have no idea why, but one of the new tests in this PR was failing with:\n    java.lang.OutOfMemoryError: Java heap space\n        at org.apache.kafka.streams.kstream.internals.FullChangeSerde.decomposeLegacyFormattedArrayIntoChangeArrays(FullChangeSerde.java:82)\n        at org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.deserializeV2(TimeOrderedKeyValueBufferChangelogDeserializationHelper.java:90)\n        at org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.duckTypeV2(TimeOrderedKeyValueBufferChangelogDeserializationHelper.java:61)\n        at org.apache.kafka.streams.state.internals.InMemoryTimeOrderedKeyValueBuffer.restoreBatch(InMemoryTimeOrderedKeyValueBuffer.java:369)\n        at org.apache.kafka.streams.state.internals.InMemoryTimeOrderedKeyValueBuffer$$Lambda$284/0x00000001002cb440.restoreBatch(Unknown Source)\n        at org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferTest.shouldRestoreV3FormatWithV2Header(TimeOrderedKeyValueBufferTest.java:742)\n\nI captured a flight recording and a heap dump on exit, but everything looked fine, and the heap was only a few megs at the time of the crash. I noticed first that if I just overrode all the jvm args, the test would pass, and through trial and error, I identified this one as the \"cause\".\nI get an OOMe every time with -XX:+UseParallelGC and I've never gotten it without the flag. WDYT about dropping it?", "author": "vvcephei", "createdAt": "2020-06-26T15:49:35Z", "path": "build.gradle", "diffHunk": "@@ -97,7 +97,7 @@ ext {\n   buildVersionFileName = \"kafka-version.properties\"\n \n   defaultMaxHeapSize = \"2g\"\n-  defaultJvmArgs = [\"-Xss4m\", \"-XX:+UseParallelGC\"]\n+  defaultJvmArgs = [\"-Xss4m\"]", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQwOTYxOQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446409619", "bodyText": "Aha! I figured it out. There actually was a bug in the test. While duck-typing, the code was trying to allocate an array of 1.8GB. It's funny that disabling this flag made this test pass on java 11 and 14. Maybe the flag partitions the heap on those versions or something, so the test didn't actually have the full 2GB available. Anyway, I'm about to push a fix and put the flag back.", "author": "vvcephei", "createdAt": "2020-06-26T21:05:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2NTU1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2NTgwNg==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446265806", "bodyText": "Only used in the test now, so I moved it.", "author": "vvcephei", "createdAt": "2020-06-26T15:50:01Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/FullChangeSerde.java", "diffHunk": "@@ -68,33 +68,6 @@ private FullChangeSerde(final Serde<T> inner) {\n         return new Change<>(newValue, oldValue);\n     }\n \n-    /**\n-     * We used to serialize a Change into a single byte[]. Now, we don't anymore, but we still keep this logic here\n-     * so that we can produce the legacy format to test that we can still deserialize it.\n-     */\n-    public static byte[] mergeChangeArraysIntoSingleLegacyFormattedArray(final Change<byte[]> serialChange) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2NjY1Nw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446266657", "bodyText": "This was correct before, since we check equality and enforce identity in the constructor, but Arrays.equals is extremely cheap when the arrays are identical, so explicitly doing an identity check instead of equality was a micro-optimization.", "author": "vvcephei", "createdAt": "2020-06-26T15:51:34Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/BufferValue.java", "diffHunk": "@@ -120,7 +120,7 @@ ByteBuffer serialize(final int endPadding) {\n \n         if (oldValue == null) {\n             buffer.putInt(NULL_VALUE_SENTINEL);\n-        } else if (priorValue == oldValue) {\n+        } else if (Arrays.equals(priorValue, oldValue)) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2NzI5MQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446267291", "bodyText": "We don't need to store the whole RecordHeaders for the old versions, just the actual version flag.", "author": "vvcephei", "createdAt": "2020-06-26T15:52:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -54,14 +56,17 @@\n import java.util.function.Supplier;\n \n import static java.util.Objects.requireNonNull;\n+import static org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.deserializeV3;\n+import static org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.duckTypeV2;\n \n public final class InMemoryTimeOrderedKeyValueBuffer<K, V> implements TimeOrderedKeyValueBuffer<K, V> {\n     private static final BytesSerializer KEY_SERIALIZER = new BytesSerializer();\n     private static final ByteArraySerializer VALUE_SERIALIZER = new ByteArraySerializer();\n-    private static final RecordHeaders V_1_CHANGELOG_HEADERS =\n-        new RecordHeaders(new Header[] {new RecordHeader(\"v\", new byte[] {(byte) 1})});\n-    private static final RecordHeaders V_2_CHANGELOG_HEADERS =\n-        new RecordHeaders(new Header[] {new RecordHeader(\"v\", new byte[] {(byte) 2})});\n+    private static final byte[] V_1_CHANGELOG_HEADER_VALUE = {(byte) 1};\n+    private static final byte[] V_2_CHANGELOG_HEADER_VALUE = {(byte) 2};\n+    private static final byte[] V_3_CHANGELOG_HEADER_VALUE = {(byte) 3};\n+    static final RecordHeaders CHANGELOG_HEADERS =\n+        new RecordHeaders(new Header[] {new RecordHeader(\"v\", V_3_CHANGELOG_HEADER_VALUE)});", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2OTEzMA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446269130", "bodyText": "See the comment on this method for why we need to duck-type version 2. I pulled these deserializations into a helper class because all the extra branches pushed our cyclomatic complexity over the limit.\nBut I kept the first two branches here because they aren't pure functions. They perform a lookup in the buffer itself as part of converting the old format.", "author": "vvcephei", "createdAt": "2020-06-26T15:55:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -361,26 +366,20 @@ private void restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> batch\n                             contextualRecord.recordContext()\n                         )\n                     );\n-                } else if (V_2_CHANGELOG_HEADERS.lastHeader(\"v\").equals(record.headers().lastHeader(\"v\"))) {\n-                    // in this case, the changelog value is a serialized BufferValue\n+                } else if (Arrays.equals(versionHeader.value(), V_2_CHANGELOG_HEADER_VALUE)) {\n+\n+                    final DeserializationResult deserializationResult = duckTypeV2(record, key);", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzODMxOA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446338318", "bodyText": "Could you clarify which comment are you referring to? I did not see any comments for the \"restoreBatch\" method..", "author": "guozhangwang", "createdAt": "2020-06-26T18:15:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2OTEzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM2Mzc5OA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446363798", "bodyText": "Sorry, the comments in duckTypeV2.\nBasically, because we released three versions that would write data in the \"v3\" format, but with the \"v2\" flag, when we see the v2 flag, the data might be in v2 format or v3 format. The only way to tell is to just try to deserialize it in v2 format, and if we get an exception, then to try with v3 format.", "author": "vvcephei", "createdAt": "2020-06-26T19:12:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2OTEzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3MDQ1MA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446270450", "bodyText": "If you look a few lines up, you'll see that we just serialized the \"old value\", so we don't need to serialize it again here.", "author": "vvcephei", "createdAt": "2020-06-26T15:58:09Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -481,8 +480,7 @@ public void put(final long time,\n         final BufferValue buffered = getBuffered(serializedKey);\n         final byte[] serializedPriorValue;\n         if (buffered == null) {\n-            final V priorValue = value.oldValue;\n-            serializedPriorValue = (priorValue == null) ? null : valueSerde.innerSerde().serializer().serialize(changelogTopic, priorValue);\n+            serializedPriorValue = serialChange.oldValue;", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3MjI3MA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446272270", "bodyText": "Don't need this anymore because start blocks until it's \"started\" now.", "author": "vvcephei", "createdAt": "2020-06-26T16:01:13Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/SmokeTestDriverIntegrationTest.java", "diffHunk": "@@ -104,10 +104,6 @@ public void shouldWorkWithRebalance() throws InterruptedException {\n             clients.add(smokeTestClient);\n             smokeTestClient.start(props);\n \n-            while (!clients.get(clients.size() - 1).started()) {\n-                Thread.sleep(100);\n-            }\n-", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3MjczNg==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446272736", "bodyText": "Moved from FullChangeSerde because it is only used in this test.", "author": "vvcephei", "createdAt": "2020-06-26T16:02:09Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/FullChangeSerdeTest.java", "diffHunk": "@@ -19,17 +19,46 @@\n import org.apache.kafka.common.serialization.Serdes;\n import org.junit.Test;\n \n+import java.nio.ByteBuffer;\n+\n import static org.hamcrest.CoreMatchers.nullValue;\n import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.core.Is.is;\n \n public class FullChangeSerdeTest {\n     private final FullChangeSerde<String> serde = FullChangeSerde.wrap(Serdes.String());\n \n+    /**\n+     * We used to serialize a Change into a single byte[]. Now, we don't anymore, but we still keep this logic here\n+     * so that we can produce the legacy format to test that we can still deserialize it.\n+     */\n+    private static byte[] mergeChangeArraysIntoSingleLegacyFormattedArray(final Change<byte[]> serialChange) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3MzIzMw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446273233", "bodyText": "imported the headers from the production code, so that it'll stay current.", "author": "vvcephei", "createdAt": "2020-06-26T16:03:09Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferTest.java", "diffHunk": "@@ -56,14 +55,13 @@\n import static java.nio.charset.StandardCharsets.UTF_8;\n import static java.util.Arrays.asList;\n import static java.util.Collections.singletonList;\n+import static org.apache.kafka.streams.state.internals.InMemoryTimeOrderedKeyValueBuffer.CHANGELOG_HEADERS;\n import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.Matchers.is;\n import static org.junit.Assert.fail;\n \n @RunWith(Parameterized.class)\n public class TimeOrderedKeyValueBufferTest<B extends TimeOrderedKeyValueBuffer<String, String>> {\n-    private static final RecordHeaders V_2_CHANGELOG_HEADERS =\n-        new RecordHeaders(new Header[] {new RecordHeader(\"v\", new byte[] {(byte) 2})});", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3NTE5Mw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446275193", "bodyText": "This was one of my major findings in KAFKA-10173. Because the test was serializing the \"old versions\" using code shared with the current logic, we could not detect when we accidentally changed the current serialization logic without bumping the version number.\nBy instead testing against fixed pre-serialized data, we should be a lot safer.\nI took inspiration from the way that Karsten reported the observed serialized data in the bug report. Hex-encoding the binary data makes the tests more readable than a long array of byte literals.", "author": "vvcephei", "createdAt": "2020-06-26T16:06:41Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferTest.java", "diffHunk": "@@ -372,12 +370,14 @@ public void shouldRestoreOldFormat() {\n \n         context.setRecordContext(new ProcessorRecordContext(0, 0, 0, \"\", null));\n \n-        final FullChangeSerde<String> serializer = FullChangeSerde.wrap(Serdes.String());\n+        // These serialized formats were captured by running version 2.1 code.\n+        // They verify that an upgrade from 2.1 will work.\n+        // Do not change them.\n+        final String toDeleteBinaryValue = \"0000000000000000FFFFFFFF00000006646F6F6D6564\";\n+        final String asdfBinaryValue = \"0000000000000002FFFFFFFF0000000471776572\";\n+        final String zxcvBinaryValue1 = \"00000000000000010000000870726576696F757300000005656F34696D\";\n+        final String zxcvBinaryValue2 = \"000000000000000100000005656F34696D000000046E657874\";", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3NTkzNQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446275935", "bodyText": "This is the version bump we should have done in 2.4.0. I'll backport this fix to the 2.4 branch.", "author": "vvcephei", "createdAt": "2020-06-26T16:08:05Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -361,26 +366,20 @@ private void restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> batch\n                             contextualRecord.recordContext()\n                         )\n                     );\n-                } else if (V_2_CHANGELOG_HEADERS.lastHeader(\"v\").equals(record.headers().lastHeader(\"v\"))) {\n-                    // in this case, the changelog value is a serialized BufferValue\n+                } else if (Arrays.equals(versionHeader.value(), V_2_CHANGELOG_HEADER_VALUE)) {\n+\n+                    final DeserializationResult deserializationResult = duckTypeV2(record, key);\n+                    cleanPut(deserializationResult.time(), deserializationResult.key(), deserializationResult.bufferValue());\n+\n+                } else if (Arrays.equals(versionHeader.value(), V_3_CHANGELOG_HEADER_VALUE)) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3NjY5Mg==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446276692", "bodyText": "I inlined these utilities to make this class more \"portable\". I.e., so that we can copy-paste it into the upgrade-test modules without dragging in a bunch of extra dependencies.", "author": "vvcephei", "createdAt": "2020-06-26T16:09:34Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -38,107 +37,128 @@\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n-import org.apache.kafka.test.TestUtils;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n public class SmokeTestClient extends SmokeTestUtil {\n \n     private final String name;\n \n-    private Thread thread;\n     private KafkaStreams streams;\n     private boolean uncaughtException = false;\n-    private boolean started;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SmokeTestClient(final String name) {\n-        super();\n-        this.name = name;\n+    private static void addShutdownHook(final String name, final Runnable runnable) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3ODMyNw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446278327", "bodyText": "This was another bug I happened to notice while scrutinizing this system test. createKafkaStreams was registering a state listener and exception handler. But the next line here was overriding the exception handler, so the one registered in createKafkaStreams was getting ignored. I noticed it because I registered a state listener here, which also caused the one registered in createKafkaStreams to get ignored.\nInlining solves this problem, and since createKafkaStreams had only one usage, it was needless anyway.", "author": "vvcephei", "createdAt": "2020-06-26T16:12:42Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -38,107 +37,128 @@\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n-import org.apache.kafka.test.TestUtils;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n public class SmokeTestClient extends SmokeTestUtil {\n \n     private final String name;\n \n-    private Thread thread;\n     private KafkaStreams streams;\n     private boolean uncaughtException = false;\n-    private boolean started;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SmokeTestClient(final String name) {\n-        super();\n-        this.name = name;\n+    private static void addShutdownHook(final String name, final Runnable runnable) {\n+        if (name != null) {\n+            Runtime.getRuntime().addShutdownHook(KafkaThread.nonDaemon(name, runnable));\n+        } else {\n+            Runtime.getRuntime().addShutdownHook(new Thread(runnable));\n+        }\n     }\n \n-    public boolean started() {\n-        return started;\n+    private static File tempDirectory() {\n+        final String prefix = \"kafka-\";\n+        final File file;\n+        try {\n+            file = Files.createTempDirectory(prefix).toFile();\n+        } catch (final IOException ex) {\n+            throw new RuntimeException(\"Failed to create a temp dir\", ex);\n+        }\n+        file.deleteOnExit();\n+\n+        addShutdownHook(\"delete-temp-file-shutdown-hook\", () -> {\n+            try {\n+                Utils.delete(file);\n+            } catch (final IOException e) {\n+                System.out.println(\"Error deleting \" + file.getAbsolutePath());\n+                e.printStackTrace(System.out);\n+            }\n+        });\n+\n+        return file;\n+    }\n+\n+    public SmokeTestClient(final String name) {\n+        this.name = name;\n     }\n \n     public boolean closed() {\n         return closed;\n     }\n \n     public void start(final Properties streamsProperties) {\n-        streams = createKafkaStreams(streamsProperties);\n+        final Topology build = getTopology();", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3ODcyMA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446278720", "bodyText": "A new message we can look for to wait until the instance has completed joining the group.", "author": "vvcephei", "createdAt": "2020-06-26T16:13:30Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -38,107 +37,128 @@\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n-import org.apache.kafka.test.TestUtils;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n public class SmokeTestClient extends SmokeTestUtil {\n \n     private final String name;\n \n-    private Thread thread;\n     private KafkaStreams streams;\n     private boolean uncaughtException = false;\n-    private boolean started;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SmokeTestClient(final String name) {\n-        super();\n-        this.name = name;\n+    private static void addShutdownHook(final String name, final Runnable runnable) {\n+        if (name != null) {\n+            Runtime.getRuntime().addShutdownHook(KafkaThread.nonDaemon(name, runnable));\n+        } else {\n+            Runtime.getRuntime().addShutdownHook(new Thread(runnable));\n+        }\n     }\n \n-    public boolean started() {\n-        return started;\n+    private static File tempDirectory() {\n+        final String prefix = \"kafka-\";\n+        final File file;\n+        try {\n+            file = Files.createTempDirectory(prefix).toFile();\n+        } catch (final IOException ex) {\n+            throw new RuntimeException(\"Failed to create a temp dir\", ex);\n+        }\n+        file.deleteOnExit();\n+\n+        addShutdownHook(\"delete-temp-file-shutdown-hook\", () -> {\n+            try {\n+                Utils.delete(file);\n+            } catch (final IOException e) {\n+                System.out.println(\"Error deleting \" + file.getAbsolutePath());\n+                e.printStackTrace(System.out);\n+            }\n+        });\n+\n+        return file;\n+    }\n+\n+    public SmokeTestClient(final String name) {\n+        this.name = name;\n     }\n \n     public boolean closed() {\n         return closed;\n     }\n \n     public void start(final Properties streamsProperties) {\n-        streams = createKafkaStreams(streamsProperties);\n+        final Topology build = getTopology();\n+        streams = new KafkaStreams(build, getStreamsConfig(streamsProperties));\n+\n+        final CountDownLatch countDownLatch = new CountDownLatch(1);\n+        streams.setStateListener((newState, oldState) -> {\n+            System.out.printf(\"%s %s: %s -> %s%n\", name, Instant.now(), oldState, newState);\n+            if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) {\n+                countDownLatch.countDown();\n+            }\n+\n+            if (newState == KafkaStreams.State.NOT_RUNNING) {\n+                closed = true;\n+            }\n+        });\n+\n         streams.setUncaughtExceptionHandler((t, e) -> {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n+            System.out.println(name + \": FATAL: An unexpected exception is encountered on thread \" + t + \": \" + e);\n+            e.printStackTrace(System.out);\n             uncaughtException = true;\n-            e.printStackTrace();\n+            streams.close(Duration.ofSeconds(30));\n         });\n \n-        Exit.addShutdownHook(\"streams-shutdown-hook\", () -> close());\n+        addShutdownHook(\"streams-shutdown-hook\", this::close);\n \n-        thread = new Thread(() -> streams.start());\n-        thread.start();\n+        streams.start();\n+        try {\n+            if (!countDownLatch.await(1, TimeUnit.MINUTES)) {\n+                System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't start in one minute\");\n+            }\n+        } catch (final InterruptedException e) {\n+            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: \" + e);\n+            e.printStackTrace(System.out);\n+        }\n+        System.out.println(name + \": SMOKE-TEST-CLIENT-STARTED\");", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3OTQxMQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446279411", "bodyText": "Found a missed condition, if the close timed out, there wouldn't be an exception, just a false return value.", "author": "vvcephei", "createdAt": "2020-06-26T16:14:52Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -38,107 +37,128 @@\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n-import org.apache.kafka.test.TestUtils;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n public class SmokeTestClient extends SmokeTestUtil {\n \n     private final String name;\n \n-    private Thread thread;\n     private KafkaStreams streams;\n     private boolean uncaughtException = false;\n-    private boolean started;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SmokeTestClient(final String name) {\n-        super();\n-        this.name = name;\n+    private static void addShutdownHook(final String name, final Runnable runnable) {\n+        if (name != null) {\n+            Runtime.getRuntime().addShutdownHook(KafkaThread.nonDaemon(name, runnable));\n+        } else {\n+            Runtime.getRuntime().addShutdownHook(new Thread(runnable));\n+        }\n     }\n \n-    public boolean started() {\n-        return started;\n+    private static File tempDirectory() {\n+        final String prefix = \"kafka-\";\n+        final File file;\n+        try {\n+            file = Files.createTempDirectory(prefix).toFile();\n+        } catch (final IOException ex) {\n+            throw new RuntimeException(\"Failed to create a temp dir\", ex);\n+        }\n+        file.deleteOnExit();\n+\n+        addShutdownHook(\"delete-temp-file-shutdown-hook\", () -> {\n+            try {\n+                Utils.delete(file);\n+            } catch (final IOException e) {\n+                System.out.println(\"Error deleting \" + file.getAbsolutePath());\n+                e.printStackTrace(System.out);\n+            }\n+        });\n+\n+        return file;\n+    }\n+\n+    public SmokeTestClient(final String name) {\n+        this.name = name;\n     }\n \n     public boolean closed() {\n         return closed;\n     }\n \n     public void start(final Properties streamsProperties) {\n-        streams = createKafkaStreams(streamsProperties);\n+        final Topology build = getTopology();\n+        streams = new KafkaStreams(build, getStreamsConfig(streamsProperties));\n+\n+        final CountDownLatch countDownLatch = new CountDownLatch(1);\n+        streams.setStateListener((newState, oldState) -> {\n+            System.out.printf(\"%s %s: %s -> %s%n\", name, Instant.now(), oldState, newState);\n+            if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) {\n+                countDownLatch.countDown();\n+            }\n+\n+            if (newState == KafkaStreams.State.NOT_RUNNING) {\n+                closed = true;\n+            }\n+        });\n+\n         streams.setUncaughtExceptionHandler((t, e) -> {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n+            System.out.println(name + \": FATAL: An unexpected exception is encountered on thread \" + t + \": \" + e);\n+            e.printStackTrace(System.out);\n             uncaughtException = true;\n-            e.printStackTrace();\n+            streams.close(Duration.ofSeconds(30));\n         });\n \n-        Exit.addShutdownHook(\"streams-shutdown-hook\", () -> close());\n+        addShutdownHook(\"streams-shutdown-hook\", this::close);\n \n-        thread = new Thread(() -> streams.start());\n-        thread.start();\n+        streams.start();\n+        try {\n+            if (!countDownLatch.await(1, TimeUnit.MINUTES)) {\n+                System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't start in one minute\");\n+            }\n+        } catch (final InterruptedException e) {\n+            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: \" + e);\n+            e.printStackTrace(System.out);\n+        }\n+        System.out.println(name + \": SMOKE-TEST-CLIENT-STARTED\");\n+        System.out.println(name + \" started at \" + Instant.now());\n     }\n \n     public void closeAsync() {\n         streams.close(Duration.ZERO);\n     }\n \n     public void close() {\n-        streams.close(Duration.ofSeconds(5));\n-        // do not remove these printouts since they are needed for health scripts\n-        if (!uncaughtException) {\n+        final boolean wasClosed = streams.close(Duration.ofMinutes(1));\n+\n+        if (wasClosed && !uncaughtException) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3OTczNw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446279737", "bodyText": "I moved all these to the system test propFile() definition.", "author": "vvcephei", "createdAt": "2020-06-26T16:15:29Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -38,107 +37,128 @@\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n-import org.apache.kafka.test.TestUtils;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n public class SmokeTestClient extends SmokeTestUtil {\n \n     private final String name;\n \n-    private Thread thread;\n     private KafkaStreams streams;\n     private boolean uncaughtException = false;\n-    private boolean started;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SmokeTestClient(final String name) {\n-        super();\n-        this.name = name;\n+    private static void addShutdownHook(final String name, final Runnable runnable) {\n+        if (name != null) {\n+            Runtime.getRuntime().addShutdownHook(KafkaThread.nonDaemon(name, runnable));\n+        } else {\n+            Runtime.getRuntime().addShutdownHook(new Thread(runnable));\n+        }\n     }\n \n-    public boolean started() {\n-        return started;\n+    private static File tempDirectory() {\n+        final String prefix = \"kafka-\";\n+        final File file;\n+        try {\n+            file = Files.createTempDirectory(prefix).toFile();\n+        } catch (final IOException ex) {\n+            throw new RuntimeException(\"Failed to create a temp dir\", ex);\n+        }\n+        file.deleteOnExit();\n+\n+        addShutdownHook(\"delete-temp-file-shutdown-hook\", () -> {\n+            try {\n+                Utils.delete(file);\n+            } catch (final IOException e) {\n+                System.out.println(\"Error deleting \" + file.getAbsolutePath());\n+                e.printStackTrace(System.out);\n+            }\n+        });\n+\n+        return file;\n+    }\n+\n+    public SmokeTestClient(final String name) {\n+        this.name = name;\n     }\n \n     public boolean closed() {\n         return closed;\n     }\n \n     public void start(final Properties streamsProperties) {\n-        streams = createKafkaStreams(streamsProperties);\n+        final Topology build = getTopology();\n+        streams = new KafkaStreams(build, getStreamsConfig(streamsProperties));\n+\n+        final CountDownLatch countDownLatch = new CountDownLatch(1);\n+        streams.setStateListener((newState, oldState) -> {\n+            System.out.printf(\"%s %s: %s -> %s%n\", name, Instant.now(), oldState, newState);\n+            if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) {\n+                countDownLatch.countDown();\n+            }\n+\n+            if (newState == KafkaStreams.State.NOT_RUNNING) {\n+                closed = true;\n+            }\n+        });\n+\n         streams.setUncaughtExceptionHandler((t, e) -> {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n+            System.out.println(name + \": FATAL: An unexpected exception is encountered on thread \" + t + \": \" + e);\n+            e.printStackTrace(System.out);\n             uncaughtException = true;\n-            e.printStackTrace();\n+            streams.close(Duration.ofSeconds(30));\n         });\n \n-        Exit.addShutdownHook(\"streams-shutdown-hook\", () -> close());\n+        addShutdownHook(\"streams-shutdown-hook\", this::close);\n \n-        thread = new Thread(() -> streams.start());\n-        thread.start();\n+        streams.start();\n+        try {\n+            if (!countDownLatch.await(1, TimeUnit.MINUTES)) {\n+                System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't start in one minute\");\n+            }\n+        } catch (final InterruptedException e) {\n+            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: \" + e);\n+            e.printStackTrace(System.out);\n+        }\n+        System.out.println(name + \": SMOKE-TEST-CLIENT-STARTED\");\n+        System.out.println(name + \" started at \" + Instant.now());\n     }\n \n     public void closeAsync() {\n         streams.close(Duration.ZERO);\n     }\n \n     public void close() {\n-        streams.close(Duration.ofSeconds(5));\n-        // do not remove these printouts since they are needed for health scripts\n-        if (!uncaughtException) {\n+        final boolean wasClosed = streams.close(Duration.ofMinutes(1));\n+\n+        if (wasClosed && !uncaughtException) {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-CLOSED\");\n-        }\n-        try {\n-            thread.join();\n-        } catch (final Exception ex) {\n-            // do not remove these printouts since they are needed for health scripts\n+        } else if (wasClosed) {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n-            // ignore\n+        } else {\n+            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't close\");\n         }\n     }\n \n     private Properties getStreamsConfig(final Properties props) {\n         final Properties fullProps = new Properties(props);\n         fullProps.put(StreamsConfig.APPLICATION_ID_CONFIG, \"SmokeTest\");\n         fullProps.put(StreamsConfig.CLIENT_ID_CONFIG, \"SmokeTest-\" + name);\n-        fullProps.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 3);\n-        fullProps.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);\n-        fullProps.put(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, 100);\n-        fullProps.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);\n-        fullProps.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);\n-        fullProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n-        fullProps.put(ProducerConfig.ACKS_CONFIG, \"all\");", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3OTg1Nw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446279857", "bodyText": "inlined above.", "author": "vvcephei", "createdAt": "2020-06-26T16:15:44Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -38,107 +37,128 @@\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n-import org.apache.kafka.test.TestUtils;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n public class SmokeTestClient extends SmokeTestUtil {\n \n     private final String name;\n \n-    private Thread thread;\n     private KafkaStreams streams;\n     private boolean uncaughtException = false;\n-    private boolean started;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SmokeTestClient(final String name) {\n-        super();\n-        this.name = name;\n+    private static void addShutdownHook(final String name, final Runnable runnable) {\n+        if (name != null) {\n+            Runtime.getRuntime().addShutdownHook(KafkaThread.nonDaemon(name, runnable));\n+        } else {\n+            Runtime.getRuntime().addShutdownHook(new Thread(runnable));\n+        }\n     }\n \n-    public boolean started() {\n-        return started;\n+    private static File tempDirectory() {\n+        final String prefix = \"kafka-\";\n+        final File file;\n+        try {\n+            file = Files.createTempDirectory(prefix).toFile();\n+        } catch (final IOException ex) {\n+            throw new RuntimeException(\"Failed to create a temp dir\", ex);\n+        }\n+        file.deleteOnExit();\n+\n+        addShutdownHook(\"delete-temp-file-shutdown-hook\", () -> {\n+            try {\n+                Utils.delete(file);\n+            } catch (final IOException e) {\n+                System.out.println(\"Error deleting \" + file.getAbsolutePath());\n+                e.printStackTrace(System.out);\n+            }\n+        });\n+\n+        return file;\n+    }\n+\n+    public SmokeTestClient(final String name) {\n+        this.name = name;\n     }\n \n     public boolean closed() {\n         return closed;\n     }\n \n     public void start(final Properties streamsProperties) {\n-        streams = createKafkaStreams(streamsProperties);\n+        final Topology build = getTopology();\n+        streams = new KafkaStreams(build, getStreamsConfig(streamsProperties));\n+\n+        final CountDownLatch countDownLatch = new CountDownLatch(1);\n+        streams.setStateListener((newState, oldState) -> {\n+            System.out.printf(\"%s %s: %s -> %s%n\", name, Instant.now(), oldState, newState);\n+            if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) {\n+                countDownLatch.countDown();\n+            }\n+\n+            if (newState == KafkaStreams.State.NOT_RUNNING) {\n+                closed = true;\n+            }\n+        });\n+\n         streams.setUncaughtExceptionHandler((t, e) -> {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n+            System.out.println(name + \": FATAL: An unexpected exception is encountered on thread \" + t + \": \" + e);\n+            e.printStackTrace(System.out);\n             uncaughtException = true;\n-            e.printStackTrace();\n+            streams.close(Duration.ofSeconds(30));\n         });\n \n-        Exit.addShutdownHook(\"streams-shutdown-hook\", () -> close());\n+        addShutdownHook(\"streams-shutdown-hook\", this::close);\n \n-        thread = new Thread(() -> streams.start());\n-        thread.start();\n+        streams.start();\n+        try {\n+            if (!countDownLatch.await(1, TimeUnit.MINUTES)) {\n+                System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't start in one minute\");\n+            }\n+        } catch (final InterruptedException e) {\n+            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: \" + e);\n+            e.printStackTrace(System.out);\n+        }\n+        System.out.println(name + \": SMOKE-TEST-CLIENT-STARTED\");\n+        System.out.println(name + \" started at \" + Instant.now());\n     }\n \n     public void closeAsync() {\n         streams.close(Duration.ZERO);\n     }\n \n     public void close() {\n-        streams.close(Duration.ofSeconds(5));\n-        // do not remove these printouts since they are needed for health scripts\n-        if (!uncaughtException) {\n+        final boolean wasClosed = streams.close(Duration.ofMinutes(1));\n+\n+        if (wasClosed && !uncaughtException) {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-CLOSED\");\n-        }\n-        try {\n-            thread.join();\n-        } catch (final Exception ex) {\n-            // do not remove these printouts since they are needed for health scripts\n+        } else if (wasClosed) {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n-            // ignore\n+        } else {\n+            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't close\");\n         }\n     }\n \n     private Properties getStreamsConfig(final Properties props) {\n         final Properties fullProps = new Properties(props);\n         fullProps.put(StreamsConfig.APPLICATION_ID_CONFIG, \"SmokeTest\");\n         fullProps.put(StreamsConfig.CLIENT_ID_CONFIG, \"SmokeTest-\" + name);\n-        fullProps.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 3);\n-        fullProps.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);\n-        fullProps.put(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, 100);\n-        fullProps.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);\n-        fullProps.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);\n-        fullProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n-        fullProps.put(ProducerConfig.ACKS_CONFIG, \"all\");\n-        fullProps.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());\n+        fullProps.put(StreamsConfig.STATE_DIR_CONFIG, tempDirectory().getAbsolutePath());\n         fullProps.putAll(props);\n         return fullProps;\n     }\n \n-    private KafkaStreams createKafkaStreams(final Properties props) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MDMyMw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446280323", "bodyText": "Oops! I noticed the lack of a newline in the output. It didn't matter for the tests because the greps aren't bounded by line.", "author": "vvcephei", "createdAt": "2020-06-26T16:16:38Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java", "diffHunk": "@@ -75,7 +75,7 @@ public void process(final Object key, final Object value) {\n \n                     @Override\n                     public void close() {\n-                        System.out.printf(\"Close processor for task %s\", context().taskId());\n+                        System.out.printf(\"Close processor for task %s%n\", context().taskId());", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MDg1Ng==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446280856", "bodyText": "All of these are just copy-pasted from the main module.", "author": "vvcephei", "createdAt": "2020-06-26T16:17:40Z", "path": "streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -0,0 +1,298 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.tests;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.common.utils.KafkaThread;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KGroupedStream;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Suppressed.BufferConfig;\n+import org.apache.kafka.streams.kstream.TimeWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n+\n+public class SmokeTestClient extends SmokeTestUtil {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MTIzNw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446281237", "bodyText": "Adding a param so that we can get away with just one broker in the upgrade test.", "author": "vvcephei", "createdAt": "2020-06-26T16:18:29Z", "path": "tests/kafkatest/services/streams.py", "diffHunk": "@@ -305,23 +305,62 @@ def start_node(self, node):\n class StreamsSmokeTestBaseService(StreamsTestBaseService):\n     \"\"\"Base class for Streams Smoke Test services providing some common settings and functionality\"\"\"\n \n-    def __init__(self, test_context, kafka, command, processing_guarantee = 'at_least_once', num_threads = 3):\n+    def __init__(self, test_context, kafka, command, processing_guarantee = 'at_least_once', num_threads = 3, replication_factor = 3):", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MTQ1Ng==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446281456", "bodyText": "Added for the upgrade test.", "author": "vvcephei", "createdAt": "2020-06-26T16:18:53Z", "path": "tests/kafkatest/services/streams.py", "diffHunk": "@@ -305,23 +305,62 @@ def start_node(self, node):\n class StreamsSmokeTestBaseService(StreamsTestBaseService):\n     \"\"\"Base class for Streams Smoke Test services providing some common settings and functionality\"\"\"\n \n-    def __init__(self, test_context, kafka, command, processing_guarantee = 'at_least_once', num_threads = 3):\n+    def __init__(self, test_context, kafka, command, processing_guarantee = 'at_least_once', num_threads = 3, replication_factor = 3):\n         super(StreamsSmokeTestBaseService, self).__init__(test_context,\n                                                           kafka,\n                                                           \"org.apache.kafka.streams.tests.StreamsSmokeTest\",\n                                                           command)\n         self.NUM_THREADS = num_threads\n         self.PROCESSING_GUARANTEE = processing_guarantee\n+        self.KAFKA_STREAMS_VERSION = \"\"\n+        self.UPGRADE_FROM = None\n+        self.REPLICATION_FACTOR = replication_factor\n+\n+    def set_version(self, kafka_streams_version):\n+        self.KAFKA_STREAMS_VERSION = kafka_streams_version\n+\n+    def set_upgrade_from(self, upgrade_from):\n+        self.UPGRADE_FROM = upgrade_from", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MTc2Nw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446281767", "bodyText": "Moved from the java code so that all the configs can be defined together.", "author": "vvcephei", "createdAt": "2020-06-26T16:19:32Z", "path": "tests/kafkatest/services/streams.py", "diffHunk": "@@ -305,23 +305,62 @@ def start_node(self, node):\n class StreamsSmokeTestBaseService(StreamsTestBaseService):\n     \"\"\"Base class for Streams Smoke Test services providing some common settings and functionality\"\"\"\n \n-    def __init__(self, test_context, kafka, command, processing_guarantee = 'at_least_once', num_threads = 3):\n+    def __init__(self, test_context, kafka, command, processing_guarantee = 'at_least_once', num_threads = 3, replication_factor = 3):\n         super(StreamsSmokeTestBaseService, self).__init__(test_context,\n                                                           kafka,\n                                                           \"org.apache.kafka.streams.tests.StreamsSmokeTest\",\n                                                           command)\n         self.NUM_THREADS = num_threads\n         self.PROCESSING_GUARANTEE = processing_guarantee\n+        self.KAFKA_STREAMS_VERSION = \"\"\n+        self.UPGRADE_FROM = None\n+        self.REPLICATION_FACTOR = replication_factor\n+\n+    def set_version(self, kafka_streams_version):\n+        self.KAFKA_STREAMS_VERSION = kafka_streams_version\n+\n+    def set_upgrade_from(self, upgrade_from):\n+        self.UPGRADE_FROM = upgrade_from\n \n     def prop_file(self):\n         properties = {streams_property.STATE_DIR: self.PERSISTENT_ROOT,\n                       streams_property.KAFKA_SERVERS: self.kafka.bootstrap_servers(),\n                       streams_property.PROCESSING_GUARANTEE: self.PROCESSING_GUARANTEE,\n-                      streams_property.NUM_THREADS: self.NUM_THREADS}\n+                      streams_property.NUM_THREADS: self.NUM_THREADS,\n+                      \"replication.factor\": self.REPLICATION_FACTOR,\n+                      \"num.standby.replicas\": 2,\n+                      \"buffered.records.per.partition\": 100,\n+                      \"commit.interval.ms\": 1000,\n+                      \"auto.offset.reset\": \"earliest\",\n+                      \"acks\": \"all\"}", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MjI2Mg==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446282262", "bodyText": "See KAFKA-10203 for why I couldn't go past 2.2", "author": "vvcephei", "createdAt": "2020-06-26T16:20:32Z", "path": "tests/kafkatest/tests/streams/streams_upgrade_test.py", "diffHunk": "@@ -37,6 +37,9 @@\n # can be replaced with metadata_2_versions\n backward_compatible_metadata_2_versions = [str(LATEST_0_10_2), str(LATEST_0_11_0), str(LATEST_1_0), str(LATEST_1_1)]\n metadata_3_or_higher_versions = [str(LATEST_2_0), str(LATEST_2_1), str(LATEST_2_2), str(LATEST_2_3), str(LATEST_2_4), str(LATEST_2_5), str(DEV_VERSION)]\n+smoke_test_versions = [str(LATEST_2_2), str(LATEST_2_3), str(LATEST_2_4), str(LATEST_2_5)]", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MjkxNA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446282914", "bodyText": "We were previously not testing 2.0+ at all. After rewriting this as a smoke test, it only applies to 2.2+. I also figured it makes more sense just to test upgrades to the current branch, rather than testing cross-upgrades between every pair of versions.", "author": "vvcephei", "createdAt": "2020-06-26T16:21:49Z", "path": "tests/kafkatest/tests/streams/streams_upgrade_test.py", "diffHunk": "@@ -189,8 +192,8 @@ def test_upgrade_downgrade_brokers(self, from_version, to_version):\n         processor.stop()\n         processor.node.account.ssh_capture(\"grep SMOKE-TEST-CLIENT-CLOSED %s\" % processor.STDOUT_FILE, allow_fail=False)\n \n-    @matrix(from_version=metadata_2_versions, to_version=metadata_2_versions)\n-    def test_simple_upgrade_downgrade(self, from_version, to_version):\n+    @matrix(from_version=smoke_test_versions, to_version=dev_version)", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MzU0MQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446343541", "bodyText": "+1, I think this is a great find.", "author": "guozhangwang", "createdAt": "2020-06-26T18:26:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MjkxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MzIyMA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446283220", "bodyText": "A lot of these changes are part of adapting the test to the smoke test app.", "author": "vvcephei", "createdAt": "2020-06-26T16:22:28Z", "path": "tests/kafkatest/tests/streams/streams_upgrade_test.py", "diffHunk": "@@ -201,14 +204,29 @@ def test_simple_upgrade_downgrade(self, from_version, to_version):\n         self.zk = ZookeeperService(self.test_context, num_nodes=1)\n         self.zk.start()\n \n-        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)\n+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics={", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MzY3Mw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446283673", "bodyText": "I refactored this method to start all the nodes concurrently, rather than one at a time. We still do a rolling upgrade, but there's no need to do a rolling startup.", "author": "vvcephei", "createdAt": "2020-06-26T16:23:24Z", "path": "tests/kafkatest/tests/streams/streams_upgrade_test.py", "diffHunk": "@@ -349,56 +370,42 @@ def get_version_string(self, version):\n     def start_all_nodes_with(self, version):", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MDc1NQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446340755", "bodyText": "I'm assuming 22..25 client / drive code are all copy-pastes here so I skipped reviewing them. LMK if they aren't.", "author": "guozhangwang", "createdAt": "2020-06-26T18:20:21Z", "path": "streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.tests;\n+\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.StreamsConfig;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.UUID;\n+\n+import static org.apache.kafka.streams.tests.SmokeTestDriver.generate;\n+import static org.apache.kafka.streams.tests.SmokeTestDriver.generatePerpetually;\n+\n+public class StreamsSmokeTest {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM2NDE5Ng==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446364196", "bodyText": "That's correct.", "author": "vvcephei", "createdAt": "2020-06-26T19:13:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MDc1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0NDQ0Mw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446344443", "bodyText": "Some docs, either here or directly inside InMemoryTimeOrderedKeyValueBuffer.java explaining the format difference would help a lot. You can see some examples like object GroupMetadataManager", "author": "guozhangwang", "createdAt": "2020-06-26T18:28:56Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferChangelogDeserializationHelper.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.internals.Change;\n+import org.apache.kafka.streams.kstream.internals.FullChangeSerde;\n+\n+import java.nio.ByteBuffer;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+final class TimeOrderedKeyValueBufferChangelogDeserializationHelper {\n+    private TimeOrderedKeyValueBufferChangelogDeserializationHelper() {}\n+\n+    static final class DeserializationResult {\n+        private final long time;\n+        private final Bytes key;\n+        private final BufferValue bufferValue;\n+\n+        private DeserializationResult(final long time, final Bytes key, final BufferValue bufferValue) {\n+            this.time = time;\n+            this.key = key;\n+            this.bufferValue = bufferValue;\n+        }\n+\n+        long time() {\n+            return time;\n+        }\n+\n+        Bytes key() {\n+            return key;\n+        }\n+\n+        BufferValue bufferValue() {\n+            return bufferValue;\n+        }\n+    }\n+\n+\n+    static DeserializationResult duckTypeV2(final ConsumerRecord<byte[], byte[]> record, final Bytes key) {\n+        DeserializationResult deserializationResult = null;\n+        RuntimeException v2DeserializationException = null;\n+        RuntimeException v3DeserializationException = null;\n+        try {\n+            deserializationResult = deserializeV2(record, key);\n+        } catch (final RuntimeException e) {\n+            v2DeserializationException = e;\n+        }\n+        // versions 2.4.0, 2.4.1, and 2.5.0 would have erroneously encoded a V3 record with the\n+        // V2 header, so we'll try duck-typing to see if this is decodable as V3\n+        if (deserializationResult == null) {\n+            try {\n+                deserializationResult = deserializeV3(record, key);\n+            } catch (final RuntimeException e) {\n+                v3DeserializationException = e;\n+            }\n+        }\n+\n+        if (deserializationResult == null) {\n+            // ok, it wasn't V3 either. Throw both exceptions:\n+            final RuntimeException exception =\n+                new RuntimeException(\"Couldn't deserialize record as v2 or v3: \" + record,\n+                                     v2DeserializationException);\n+            exception.addSuppressed(v3DeserializationException);\n+            throw exception;\n+        }\n+        return deserializationResult;\n+    }\n+\n+    private static DeserializationResult deserializeV2(final ConsumerRecord<byte[], byte[]> record,", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM2NDM2NA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446364364", "bodyText": "sure thing!", "author": "vvcephei", "createdAt": "2020-06-26T19:14:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0NDQ0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0NTc5MA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446345790", "bodyText": "I'm just thinking, maybe we should encode headers to tombstones too in case in the future we changed the semantics of tombstones?", "author": "guozhangwang", "createdAt": "2020-06-26T18:31:59Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -258,34 +263,43 @@ private void logValue(final Bytes key, final BufferKey bufferKey, final BufferVa\n         final int sizeOfBufferTime = Long.BYTES;\n         final ByteBuffer buffer = value.serialize(sizeOfBufferTime);\n         buffer.putLong(bufferKey.time());\n-\n+        final byte[] array = buffer.array();\n         ((RecordCollector.Supplier) context).recordCollector().send(\n-                changelogTopic,\n-                key,\n-                buffer.array(),\n-                V_2_CHANGELOG_HEADERS,\n-                partition,\n-                null,\n-                KEY_SERIALIZER,\n-                VALUE_SERIALIZER\n+            changelogTopic,\n+            key,\n+            array,\n+            CHANGELOG_HEADERS,\n+            partition,\n+            null,\n+            KEY_SERIALIZER,\n+            VALUE_SERIALIZER\n         );\n     }\n \n     private void logTombstone(final Bytes key) {\n         ((RecordCollector.Supplier) context).recordCollector().send(\n-                changelogTopic,\n-                key,\n-                null,\n-                null,\n-                partition,\n-                null,\n-                KEY_SERIALIZER,\n-                VALUE_SERIALIZER\n+            changelogTopic,\n+            key,\n+            null,\n+            null,", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM2NTk5Mg==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446365992", "bodyText": "I remember considering this when I added the first version header. The reason I didn't is that, since the initial version didn't have any headers, even if we change the tombstone format in the future, we'll always have to interpret a \"no header, null value\" record as being a \"legacy format\" tombstone, just like we have to interpret a \"no header, non-null value\" as being a \"legacy format\" data record.\nYou can think of \"no header\" as indicating \"version 0\". Since we haven't changed the format of tombstones yet, there's no value in adding a \"version 1\" flag. We should just wait until we do need to make such a change (if ever).", "author": "vvcephei", "createdAt": "2020-06-26T19:17:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0NTc5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MTgxMg==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446461812", "bodyText": "SG", "author": "guozhangwang", "createdAt": "2020-06-27T00:37:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0NTc5MA=="}], "type": "inlineReview"}, {"oid": "844321f187bb9cd4641ce087b632181761af1e5f", "url": "https://github.com/apache/kafka/commit/844321f187bb9cd4641ce087b632181761af1e5f", "message": "fix attempt to allocate arbitrary sized array", "committedDate": "2020-06-26T21:24:24Z", "type": "commit"}, {"oid": "c4200998e5361bc2500c66cfac4fd64045e20add", "url": "https://github.com/apache/kafka/commit/c4200998e5361bc2500c66cfac4fd64045e20add", "message": "factor out system test changes", "committedDate": "2020-06-26T21:59:20Z", "type": "commit"}, {"oid": "f5cc0b72f12cb3ee2efb207913240539b1ec1d61", "url": "https://github.com/apache/kafka/commit/f5cc0b72f12cb3ee2efb207913240539b1ec1d61", "message": "style", "committedDate": "2020-06-26T23:56:42Z", "type": "commit"}]}