{"pr_number": 9103, "pr_title": "KAFKA-10181: Use Envelope RPC to do redirection for (Incremental)AlterConfig, AlterClientQuota and CreateTopics", "pr_createdAt": "2020-07-30T04:55:03Z", "pr_url": "https://github.com/apache/kafka/pull/9103", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjczODIzMA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r462738230", "bodyText": "You commented on the previous PR about the style here. The reasoning is that this is a more common style than having period at the end in our codebase.", "author": "abbccdda", "createdAt": "2020-07-30T05:07:29Z", "path": "clients/src/main/java/org/apache/kafka/clients/ClientRequest.java", "diffHunk": "@@ -85,11 +89,12 @@ public ApiKeys apiKey() {\n     public RequestHeader makeHeader(short version) {\n         short requestApiKey = requestBuilder.apiKey().id;\n         return new RequestHeader(\n-            new RequestHeaderData().\n-                setRequestApiKey(requestApiKey).\n-                setRequestApiVersion(version).\n-                setClientId(clientId).\n-                setCorrelationId(correlationId),\n+            new RequestHeaderData()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzkxOTAyMA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r463919020", "bodyText": "Moved to AlterConfigsUtil", "author": "abbccdda", "createdAt": "2020-08-01T04:11:47Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2207,27 +2203,6 @@ void handleFailure(Throwable throwable) {\n         return futures;\n     }\n \n-    private IncrementalAlterConfigsRequestData toIncrementalAlterConfigsRequestData(final Collection<ConfigResource> resources,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDAyOTIyNw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464029227", "bodyText": "Moved to AlterConfigsUtil", "author": "abbccdda", "createdAt": "2020-08-02T04:18:20Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterConfigsRequest.java", "diffHunk": "@@ -64,23 +64,11 @@ public String value() {\n \n     public static class Builder extends AbstractRequest.Builder<AlterConfigsRequest> {\n \n-        private final AlterConfigsRequestData data = new AlterConfigsRequestData();\n+        private final AlterConfigsRequestData data;\n \n-        public Builder(Map<ConfigResource, Config> configs, boolean validateOnly) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDAyOTQxMQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464029411", "bodyText": "This is the new test, the rest of changes in this file are just side cleanups.", "author": "abbccdda", "createdAt": "2020-08-02T04:21:07Z", "path": "clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java", "diffHunk": "@@ -151,6 +147,20 @@ public void testDnsLookupFailure() {\n         assertFalse(client.ready(new Node(1234, \"badhost\", 1234), time.milliseconds()));\n     }\n \n+    @Test\n+    public void testIncludeInitialPrincipalNameAndClientIdInHeader() {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0MTk0Ng==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464241946", "bodyText": "Could we use Optional for these two as they are not always provided?", "author": "dajac", "createdAt": "2020-08-03T07:34:55Z", "path": "clients/src/main/java/org/apache/kafka/clients/ClientRequest.java", "diffHunk": "@@ -34,6 +34,8 @@\n     private final boolean expectResponse;\n     private final int requestTimeoutMs;\n     private final RequestCompletionHandler callback;\n+    private final String initialPrincipalName;\n+    private final String initialClientId;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NzAwNA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464567004", "bodyText": "It is not necessary as we don't check nulls for these fields.", "author": "abbccdda", "createdAt": "2020-08-03T17:44:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0MTk0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0MjQzNA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464242434", "bodyText": "nit: I would actually keep the callback as the last argument as it is a bit more natural to have the callback last.", "author": "dajac", "createdAt": "2020-08-03T07:36:02Z", "path": "clients/src/main/java/org/apache/kafka/clients/ClientRequest.java", "diffHunk": "@@ -51,7 +55,9 @@ public ClientRequest(String destination,\n                          long createdTimeMs,\n                          boolean expectResponse,\n                          int requestTimeoutMs,\n-                         RequestCompletionHandler callback) {\n+                         RequestCompletionHandler callback,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0NDc1Ng==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464244756", "bodyText": "nit: empty line could be removed.", "author": "dajac", "createdAt": "2020-08-03T07:41:05Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/AlterConfigsUtil.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.message.IncrementalAlterConfigsRequestData;\n+\n+import java.util.Collection;\n+import java.util.Map;\n+\n+public class AlterConfigsUtil {\n+\n+    public static IncrementalAlterConfigsRequestData generateIncrementalRequestData(final Map<ConfigResource, Collection<AlterConfigOp>> configs,\n+                                                                                    final boolean validateOnly) {\n+        return generateIncrementalRequestData(configs.keySet(), configs, validateOnly);\n+    }\n+\n+    public static IncrementalAlterConfigsRequestData generateIncrementalRequestData(final Collection<ConfigResource> resources,\n+                                                                                    final Map<ConfigResource, Collection<AlterConfigOp>> configs,\n+                                                                                    final boolean validateOnly) {\n+        IncrementalAlterConfigsRequestData data = new IncrementalAlterConfigsRequestData()\n+                                                      .setValidateOnly(validateOnly);\n+        for (ConfigResource resource : resources) {\n+            IncrementalAlterConfigsRequestData.AlterableConfigCollection alterableConfigSet =\n+                new IncrementalAlterConfigsRequestData.AlterableConfigCollection();\n+            for (AlterConfigOp configEntry : configs.get(resource))\n+                alterableConfigSet.add(new IncrementalAlterConfigsRequestData.AlterableConfig()\n+                                           .setName(configEntry.configEntry().name())\n+                                           .setValue(configEntry.configEntry().value())\n+                                           .setConfigOperation(configEntry.opType().id()));\n+            IncrementalAlterConfigsRequestData.AlterConfigsResource alterConfigsResource = new IncrementalAlterConfigsRequestData.AlterConfigsResource();\n+            alterConfigsResource.setResourceType(resource.type().id())\n+                .setResourceName(resource.name()).setConfigs(alterableConfigSet);\n+            data.resources().add(alterConfigsResource);\n+", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0NTc5Ng==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464245796", "bodyText": "I personally prefer the previous indentation which is, I believe, more common in our code base. Or do we plan to adopt a new formatting?", "author": "dajac", "createdAt": "2020-08-03T07:43:25Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterConfigsRequest.java", "diffHunk": "@@ -71,12 +71,12 @@ public Builder(Map<ConfigResource, Config> configs, boolean validateOnly) {\n             Objects.requireNonNull(configs, \"configs\");\n             for (Map.Entry<ConfigResource, Config> entry : configs.entrySet()) {\n                 AlterConfigsRequestData.AlterConfigsResource resource = new AlterConfigsRequestData.AlterConfigsResource()\n-                        .setResourceName(entry.getKey().name())\n-                        .setResourceType(entry.getKey().type().id());\n+                                                                            .setResourceName(entry.getKey().name())\n+                                                                            .setResourceType(entry.getKey().type().id());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0Njg4MA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464246880", "bodyText": "nit: Could we move it after clientInformation to keep the order inline with the order in the constructor?", "author": "dajac", "createdAt": "2020-08-03T07:45:52Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/RequestContext.java", "diffHunk": "@@ -47,13 +48,15 @@ public RequestContext(RequestHeader header,\n                           KafkaPrincipal principal,\n                           ListenerName listenerName,\n                           SecurityProtocol securityProtocol,\n-                          ClientInformation clientInformation) {\n+                          ClientInformation clientInformation,\n+                          boolean fromControlPlane) {\n         this.header = header;\n         this.connectionId = connectionId;\n         this.clientAddress = clientAddress;\n         this.principal = principal;\n         this.listenerName = listenerName;\n         this.securityProtocol = securityProtocol;\n+        this.fromControlPlane = fromControlPlane;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0NzI1NQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464247255", "bodyText": "Shall we use Optional here as well?", "author": "dajac", "createdAt": "2020-08-03T07:46:42Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/RequestHeader.java", "diffHunk": "@@ -37,11 +37,22 @@ public RequestHeader(Struct struct, short headerVersion) {\n     }\n \n     public RequestHeader(ApiKeys requestApiKey, short requestVersion, String clientId, int correlationId) {\n-        this(new RequestHeaderData().\n-                setRequestApiKey(requestApiKey.id).\n-                setRequestApiVersion(requestVersion).\n-                setClientId(clientId).\n-                setCorrelationId(correlationId),\n+        this(requestApiKey, requestVersion, clientId, correlationId, null, null);\n+    }\n+\n+    public RequestHeader(ApiKeys requestApiKey,\n+                         short requestVersion,\n+                         String clientId,\n+                         int correlationId,\n+                         String initialPrincipalName,\n+                         String initialClientId) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2OTA0MA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464569040", "bodyText": "Not necessary, as explained.", "author": "abbccdda", "createdAt": "2020-08-03T17:48:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0NzI1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3NTQ0MA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r465275440", "bodyText": "Actually, we check nulls for these two in isForwardingRequest method. I don't feel strongly about this but I usually better to use Optional when such values are not always present.", "author": "dajac", "createdAt": "2020-08-04T19:22:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0NzI1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0Nzc0MA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464247740", "bodyText": "Actually, we will also use it for quota. I think that we could say that both InitialPrincipalName and InitialClientId will be used for logging and quota purposes.", "author": "dajac", "createdAt": "2020-08-03T07:47:47Z", "path": "clients/src/main/resources/common/message/RequestHeader.json", "diffHunk": "@@ -37,6 +37,12 @@\n     // Since the client is sending the ApiVersionsRequest in order to discover what\n     // versions are supported, the client does not know the best version to use.\n     { \"name\": \"ClientId\", \"type\": \"string\", \"versions\": \"1+\", \"nullableVersions\": \"1+\", \"ignorable\": true,\n-      \"flexibleVersions\": \"none\", \"about\": \"The client ID string.\" }\n+      \"flexibleVersions\": \"none\", \"about\": \"The client ID string.\" },\n+    { \"name\": \"InitialPrincipalName\", \"type\": \"string\", \"tag\": 0, \"taggedVersions\": \"2+\",\n+      \"nullableVersions\": \"2+\", \"default\": \"null\", \"ignorable\": true,\n+      \"about\": \"Optional value of the initial principal name when the request is redirected by a broker, for audit logging purpose.\" },", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MDMyMg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464570322", "bodyText": "I don't think we need initial client id for audit logging, is there some other logging you have in mind?", "author": "abbccdda", "createdAt": "2020-08-03T17:51:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0Nzc0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MjM0MA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r465272340", "bodyText": "Yeah, I was actually thinking about the request log. I thought that it may be useful to print them out there as well: https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/network/RequestChannel.scala#L229.", "author": "dajac", "createdAt": "2020-08-04T19:16:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0Nzc0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0ODYzNw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464248637", "bodyText": "As 2.7 has not be release yet, we don't need to introduce a new version. We can reuse KAFKA_2_7_IV0.", "author": "dajac", "createdAt": "2020-08-03T07:49:43Z", "path": "core/src/main/scala/kafka/api/ApiVersion.scala", "diffHunk": "@@ -100,7 +100,9 @@ object ApiVersion {\n     // Introduced StopReplicaRequest V3 containing the leader epoch for each partition (KIP-570)\n     KAFKA_2_6_IV0,\n     // Introduced feature versioning support (KIP-584)\n-    KAFKA_2_7_IV0\n+    KAFKA_2_7_IV0,\n+    // Introduced redirection support (KIP-590)\n+    KAFKA_2_7_IV1", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0ODg2Mg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464248862", "bodyText": "Shall we use Option here?", "author": "dajac", "createdAt": "2020-08-03T07:50:12Z", "path": "core/src/main/scala/kafka/common/InterBrokerSendThread.scala", "diffHunk": "@@ -145,7 +147,9 @@ abstract class InterBrokerSendThread(name: String,\n \n case class RequestAndCompletionHandler(destination: Node,\n                                        request: AbstractRequest.Builder[_ <: AbstractRequest],\n-                                       handler: RequestCompletionHandler)\n+                                       handler: RequestCompletionHandler,\n+                                       initialPrincipalName: String = null,\n+                                       initialClientId: String = null)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI0OTI3NQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464249275", "bodyText": "nit: That was already present before your change but could we remove the extra space before the colon?", "author": "dajac", "createdAt": "2020-08-03T07:51:05Z", "path": "core/src/main/scala/kafka/network/RequestChannel.scala", "diffHunk": "@@ -309,7 +310,10 @@ object RequestChannel extends Logging {\n   }\n }\n \n-class RequestChannel(val queueSize: Int, val metricNamePrefix : String, time: Time) extends KafkaMetricsGroup {\n+class RequestChannel(val queueSize: Int,\n+                     val metricNamePrefix : String,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI1NTU1MQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464255551", "bodyText": "The usage of the square brackets and the colon looks weird here. The audit log does not look like a sentence anymore. I wonder if we could go with something like this instead: Principal = A on behalf of Principal = B is allowed.... We could also put the initial principal name only if it is set.", "author": "dajac", "createdAt": "2020-08-03T08:04:21Z", "path": "core/src/main/scala/kafka/security/authorizer/AclAuthorizer.scala", "diffHunk": "@@ -458,7 +459,7 @@ class AclAuthorizer extends Authorizer with Logging {\n       val apiKey = if (ApiKeys.hasId(requestContext.requestType)) ApiKeys.forId(requestContext.requestType).name else requestContext.requestType\n       val refCount = action.resourceReferenceCount\n \n-      s\"Principal = $principal is $authResult Operation = $operation from host = $host on resource = $resource for request = $apiKey with resourceRefCount = $refCount\"\n+      s\"[Principal = $principal, Initial Principal Name = $initialPrincipalName]: is $authResult Operation = $operation from host = $host on resource = $resource for request = $apiKey with resourceRefCount = $refCount\"", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI2MDE1OA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464260158", "bodyText": "nit: Remove extra space before authorizedResources.", "author": "dajac", "createdAt": "2020-08-03T08:14:21Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2599,13 +2664,57 @@ class KafkaApis(val requestChannel: RequestChannel,\n       }\n     }\n \n-    val authorizedResult = adminManager.incrementalAlterConfigs(authorizedResources, alterConfigsRequest.data.validateOnly)\n-    val unauthorizedResult = unauthorizedResources.keys.map { resource =>\n-      resource -> configsAuthorizationApiError(resource)\n+    def sendResponseCallback(results: Map[ConfigResource, ApiError]): Unit = {\n+      sendResponseMaybeThrottle(request, requestThrottleMs =>\n+        new IncrementalAlterConfigsResponse(requestThrottleMs, results.asJava))\n+    }\n+\n+    def notControllerResponse(): Unit = {\n+      val errorResult = configs.keys.map {\n+        resource => resource -> new ApiError(Errors.NOT_CONTROLLER, null)\n+      }.toMap\n+\n+      sendResponseCallback(errorResult)\n+    }\n+\n+    if (isForwardingRequest(request)) {\n+      if (!controller.isActive) {\n+        notControllerResponse()\n+      } else {\n+        val authorizedResult = adminManager.incrementalAlterConfigs(\n+          authorizedResources, incrementalAlterConfigsRequest.data.validateOnly)\n+\n+        // For forwarding requests, the authentication failure is not caused by\n+        // the original client, but by the broker.\n+        val unauthorizedResult = unauthorizedResources.keys.map { resource =>\n+          resource -> new ApiError(Errors.BROKER_AUTHORIZATION_FAILURE, null)\n+        }\n+        sendResponseCallback(authorizedResult ++ unauthorizedResult)\n+      }\n+    } else if (!controller.isActive && config.redirectionEnabled) {\n+      val redirectRequestBuilder = new IncrementalAlterConfigsRequest.Builder(\n+        AlterConfigsUtil.generateIncrementalRequestData( authorizedResources.map {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3MTE3NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464271174", "bodyText": "I presume that this does not work if we use the same listener for bother the control plane and the data plane.\nI also wonder if it is a good thing to have this extension here as it applies to all the authorization in the Api Layer. I think that we should be cautious and only do this for forwarded requests.", "author": "dajac", "createdAt": "2020-08-03T08:36:17Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2982,12 +3091,33 @@ class KafkaApis(val requestChannel: RequestChannel,\n                                 logIfDenied: Boolean = true,\n                                 refCount: Int = 1): Boolean = {\n     authorizer.forall { authZ =>\n-      val resource = new ResourcePattern(resourceType, resourceName, PatternType.LITERAL)\n-      val actions = Collections.singletonList(new Action(operation, resource, refCount, logIfAllowed, logIfDenied))\n-      authZ.authorize(requestContext, actions).get(0) == AuthorizationResult.ALLOWED\n+      if (authorizeAction(requestContext, operation,\n+        resourceType, resourceName, logIfAllowed, logIfDenied, refCount, authZ)) {\n+        true\n+      } else {\n+        operation match {\n+          case ALTER | ALTER_CONFIGS | CREATE | DELETE =>\n+            requestContext.fromControlPlane &&", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3MTczNQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464271735", "bodyText": "I presume that this does not work if the broker uses the same listener for the control plane and the data plane.", "author": "dajac", "createdAt": "2020-08-03T08:37:25Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2453,34 +2455,98 @@ class KafkaApis(val requestChannel: RequestChannel,\n \n   def handleAlterConfigsRequest(request: RequestChannel.Request): Unit = {\n     val alterConfigsRequest = request.body[AlterConfigsRequest]\n-    val (authorizedResources, unauthorizedResources) = alterConfigsRequest.configs.asScala.toMap.partition { case (resource, _) =>\n+    val requestResources = alterConfigsRequest.configs.asScala.toMap\n+\n+    val (authorizedResources, unauthorizedResources) = requestResources.partition { case (resource, _) =>\n       resource.`type` match {\n         case ConfigResource.Type.BROKER_LOGGER =>\n-          throw new InvalidRequestException(s\"AlterConfigs is deprecated and does not support the resource type ${ConfigResource.Type.BROKER_LOGGER}\")\n+          throw new InvalidRequestException(\n+            s\"AlterConfigs is deprecated and does not support the resource type ${ConfigResource.Type.BROKER_LOGGER}\")\n         case ConfigResource.Type.BROKER =>\n           authorize(request.context, ALTER_CONFIGS, CLUSTER, CLUSTER_NAME)\n         case ConfigResource.Type.TOPIC =>\n           authorize(request.context, ALTER_CONFIGS, TOPIC, resource.name)\n         case rt => throw new InvalidRequestException(s\"Unexpected resource type $rt\")\n       }\n     }\n-    val authorizedResult = adminManager.alterConfigs(authorizedResources, alterConfigsRequest.validateOnly)\n-    val unauthorizedResult = unauthorizedResources.keys.map { resource =>\n-      resource -> configsAuthorizationApiError(resource)\n+\n+    def sendResponseCallback(results: Map[ConfigResource, ApiError]): Unit = {\n+      sendResponseMaybeThrottle(request, requestThrottleMs =>\n+        new AlterConfigsResponse(results.asJava, requestThrottleMs))\n     }\n-    def responseCallback(requestThrottleMs: Int): AlterConfigsResponse = {\n-      val data = new AlterConfigsResponseData()\n-        .setThrottleTimeMs(requestThrottleMs)\n-      (authorizedResult ++ unauthorizedResult).foreach{ case (resource, error) =>\n-        data.responses().add(new AlterConfigsResourceResponse()\n-          .setErrorCode(error.error.code)\n-          .setErrorMessage(error.message)\n-          .setResourceName(resource.name)\n-          .setResourceType(resource.`type`.id))\n+\n+    def notControllerResponse(): Unit = {\n+      val errorResult = requestResources.keys.map {\n+        resource => resource -> new ApiError(Errors.NOT_CONTROLLER, null)\n+      }.toMap\n+\n+      sendResponseCallback(errorResult)\n+    }\n+\n+    if (isForwardingRequest(request)) {\n+      if (!controller.isActive) {\n+        notControllerResponse()\n+      } else {\n+        val authorizedResult = adminManager.alterConfigs(\n+          authorizedResources, alterConfigsRequest.validateOnly)\n+        // For forwarding requests, the authentication failure is not caused by\n+        // the original client, but by the broker.\n+        val unauthorizedResult = unauthorizedResources.keys.map { resource =>\n+          resource -> new ApiError(Errors.BROKER_AUTHORIZATION_FAILURE, null)\n+        }\n+\n+        sendResponseCallback(authorizedResult ++ unauthorizedResult)\n+      }\n+    } else if (!controller.isActive && config.redirectionEnabled) {\n+      val redirectRequestBuilder = new AlterConfigsRequest.Builder(\n+        authorizedResources.asJava, alterConfigsRequest.validateOnly())\n+\n+      brokerToControllerChannelManager.sendRequest(redirectRequestBuilder,\n+        new ForwardedAlterConfigsRequestCompletionHandler(request,\n+          unauthorizedResources.keys.map { resource =>\n+            resource -> configsAuthorizationApiError(resource)\n+          }.toMap),\n+        request.header.initialPrincipalName,\n+        request.header.initialClientId)\n+    } else {\n+      // When IBP is low, we would just handle the config request, as admin client doesn't know\n+      // how to find the controller.\n+      val authorizedResult = adminManager.alterConfigs(\n+        authorizedResources, alterConfigsRequest.validateOnly)\n+      val unauthorizedResult = unauthorizedResources.keys.map { resource =>\n+        resource -> configsAuthorizationApiError(resource)\n       }\n-      new AlterConfigsResponse(data)\n+\n+      sendResponseCallback(authorizedResult ++ unauthorizedResult)\n+    }\n+  }\n+\n+  private def isForwardingRequest(request: RequestChannel.Request): Boolean = {\n+    request.header.initialPrincipalName != null &&\n+      request.header.initialClientId != null &&\n+      request.context.fromControlPlane", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4Mjg5Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464582897", "bodyText": "Will requests only flow to data plane if they use the same listener?", "author": "abbccdda", "createdAt": "2020-08-03T18:16:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3MTczNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MzU5Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r465273597", "bodyText": "Sorry, I was not clear. If the control plane listener is not configured, control requests will go to the data plane listener. Based on your last commits, it seems that you have figured that out.", "author": "dajac", "createdAt": "2020-08-04T19:18:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3MTczNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3NDM3Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464274377", "bodyText": "nit: as admin client doesn't know how to find the controller is not relevant anymore. What about the following: When IBP is smaller than XYZ, forwarding is not supported therefore requests are handled directly?", "author": "dajac", "createdAt": "2020-08-03T08:42:43Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2599,13 +2664,57 @@ class KafkaApis(val requestChannel: RequestChannel,\n       }\n     }\n \n-    val authorizedResult = adminManager.incrementalAlterConfigs(authorizedResources, alterConfigsRequest.data.validateOnly)\n-    val unauthorizedResult = unauthorizedResources.keys.map { resource =>\n-      resource -> configsAuthorizationApiError(resource)\n+    def sendResponseCallback(results: Map[ConfigResource, ApiError]): Unit = {\n+      sendResponseMaybeThrottle(request, requestThrottleMs =>\n+        new IncrementalAlterConfigsResponse(requestThrottleMs, results.asJava))\n+    }\n+\n+    def notControllerResponse(): Unit = {\n+      val errorResult = configs.keys.map {\n+        resource => resource -> new ApiError(Errors.NOT_CONTROLLER, null)\n+      }.toMap\n+\n+      sendResponseCallback(errorResult)\n+    }\n+\n+    if (isForwardingRequest(request)) {\n+      if (!controller.isActive) {\n+        notControllerResponse()\n+      } else {\n+        val authorizedResult = adminManager.incrementalAlterConfigs(\n+          authorizedResources, incrementalAlterConfigsRequest.data.validateOnly)\n+\n+        // For forwarding requests, the authentication failure is not caused by\n+        // the original client, but by the broker.\n+        val unauthorizedResult = unauthorizedResources.keys.map { resource =>\n+          resource -> new ApiError(Errors.BROKER_AUTHORIZATION_FAILURE, null)\n+        }\n+        sendResponseCallback(authorizedResult ++ unauthorizedResult)\n+      }\n+    } else if (!controller.isActive && config.redirectionEnabled) {\n+      val redirectRequestBuilder = new IncrementalAlterConfigsRequest.Builder(\n+        AlterConfigsUtil.generateIncrementalRequestData( authorizedResources.map {\n+          case (resource, ops) => resource -> ops.asJavaCollection\n+        }.asJava, incrementalAlterConfigsRequest.data().validateOnly()))\n+\n+      brokerToControllerChannelManager.sendRequest(redirectRequestBuilder,\n+        new ForwardedIncrementalAlterConfigsRequestCompletionHandler(request,\n+          unauthorizedResources.keys.map { resource =>\n+            resource -> configsAuthorizationApiError(resource)\n+          }.toMap),\n+        request.header.initialPrincipalName,\n+        request.header.initialClientId)\n+    } else {\n+      // When IBP is low, we would just handle the config request even if we are not the controller,\n+      // as admin client doesn't know how to find the controller.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4NTE3Ng==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464585176", "bodyText": "Sg!", "author": "abbccdda", "createdAt": "2020-08-03T18:20:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3NDM3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3NjgwOQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464276809", "bodyText": "It looks like that we will propagate the NOT_CONTROLLER error back to the client. Is it intentional? As clients don't send this request to the controller (and new ones won't get the controller id anymore), it sounds weird to return them this error. We could perhaps return another generic error.", "author": "dajac", "createdAt": "2020-08-03T08:47:21Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2599,13 +2664,57 @@ class KafkaApis(val requestChannel: RequestChannel,\n       }\n     }\n \n-    val authorizedResult = adminManager.incrementalAlterConfigs(authorizedResources, alterConfigsRequest.data.validateOnly)\n-    val unauthorizedResult = unauthorizedResources.keys.map { resource =>\n-      resource -> configsAuthorizationApiError(resource)\n+    def sendResponseCallback(results: Map[ConfigResource, ApiError]): Unit = {\n+      sendResponseMaybeThrottle(request, requestThrottleMs =>\n+        new IncrementalAlterConfigsResponse(requestThrottleMs, results.asJava))\n+    }\n+\n+    def notControllerResponse(): Unit = {\n+      val errorResult = configs.keys.map {\n+        resource => resource -> new ApiError(Errors.NOT_CONTROLLER, null)\n+      }.toMap\n+\n+      sendResponseCallback(errorResult)\n+    }\n+\n+    if (isForwardingRequest(request)) {\n+      if (!controller.isActive) {\n+        notControllerResponse()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4MzMxMA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464583310", "bodyText": "Not this is propagating to the sender broker.", "author": "abbccdda", "createdAt": "2020-08-03T18:17:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3NjgwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MzkwMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r465273903", "bodyText": "Ack. I have missed the handling of NOT_CONTROLLER in the BrokerToControllerChannelManager.", "author": "dajac", "createdAt": "2020-08-04T19:19:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3NjgwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3NzQ3OQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464277479", "bodyText": "Have we considered using Scala functions as callbacks? It would be more aligned with the other callbacks that we have in Scala and also would avoid having to define classes for each handler that support forwarding. What do you think?", "author": "dajac", "createdAt": "2020-08-03T08:48:34Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2599,13 +2664,57 @@ class KafkaApis(val requestChannel: RequestChannel,\n       }\n     }\n \n-    val authorizedResult = adminManager.incrementalAlterConfigs(authorizedResources, alterConfigsRequest.data.validateOnly)\n-    val unauthorizedResult = unauthorizedResources.keys.map { resource =>\n-      resource -> configsAuthorizationApiError(resource)\n+    def sendResponseCallback(results: Map[ConfigResource, ApiError]): Unit = {\n+      sendResponseMaybeThrottle(request, requestThrottleMs =>\n+        new IncrementalAlterConfigsResponse(requestThrottleMs, results.asJava))\n+    }\n+\n+    def notControllerResponse(): Unit = {\n+      val errorResult = configs.keys.map {\n+        resource => resource -> new ApiError(Errors.NOT_CONTROLLER, null)\n+      }.toMap\n+\n+      sendResponseCallback(errorResult)\n+    }\n+\n+    if (isForwardingRequest(request)) {\n+      if (!controller.isActive) {\n+        notControllerResponse()\n+      } else {\n+        val authorizedResult = adminManager.incrementalAlterConfigs(\n+          authorizedResources, incrementalAlterConfigsRequest.data.validateOnly)\n+\n+        // For forwarding requests, the authentication failure is not caused by\n+        // the original client, but by the broker.\n+        val unauthorizedResult = unauthorizedResources.keys.map { resource =>\n+          resource -> new ApiError(Errors.BROKER_AUTHORIZATION_FAILURE, null)\n+        }\n+        sendResponseCallback(authorizedResult ++ unauthorizedResult)\n+      }\n+    } else if (!controller.isActive && config.redirectionEnabled) {\n+      val redirectRequestBuilder = new IncrementalAlterConfigsRequest.Builder(\n+        AlterConfigsUtil.generateIncrementalRequestData( authorizedResources.map {\n+          case (resource, ops) => resource -> ops.asJavaCollection\n+        }.asJava, incrementalAlterConfigsRequest.data().validateOnly()))\n+\n+      brokerToControllerChannelManager.sendRequest(redirectRequestBuilder,\n+        new ForwardedIncrementalAlterConfigsRequestCompletionHandler(request,\n+          unauthorizedResources.keys.map { resource =>\n+            resource -> configsAuthorizationApiError(resource)\n+          }.toMap),", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI5MjAxOQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464292019", "bodyText": "For my understanding, I suppose that we don't verify that redirection is enabled here to ensure that the controller can accept forwarded requests as soon as one broker in the cluster is configured with IBP 2.7. Am I getting this right?", "author": "dajac", "createdAt": "2020-08-03T09:15:10Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2599,13 +2664,57 @@ class KafkaApis(val requestChannel: RequestChannel,\n       }\n     }\n \n-    val authorizedResult = adminManager.incrementalAlterConfigs(authorizedResources, alterConfigsRequest.data.validateOnly)\n-    val unauthorizedResult = unauthorizedResources.keys.map { resource =>\n-      resource -> configsAuthorizationApiError(resource)\n+    def sendResponseCallback(results: Map[ConfigResource, ApiError]): Unit = {\n+      sendResponseMaybeThrottle(request, requestThrottleMs =>\n+        new IncrementalAlterConfigsResponse(requestThrottleMs, results.asJava))\n+    }\n+\n+    def notControllerResponse(): Unit = {\n+      val errorResult = configs.keys.map {\n+        resource => resource -> new ApiError(Errors.NOT_CONTROLLER, null)\n+      }.toMap\n+\n+      sendResponseCallback(errorResult)\n+    }\n+\n+    if (isForwardingRequest(request)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU4NDAwNQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r464584005", "bodyText": "Yes, the purpose is to always handle a forwarding request even if IBP is not 2.7 yet. This is because some brokers may already upgrade their IBP and they start sending forwarding requests, which is totally legitimate.", "author": "abbccdda", "createdAt": "2020-08-03T18:18:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI5MjAxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3NDAxOA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r465274018", "bodyText": "Ack. This is what I thought.", "author": "dajac", "createdAt": "2020-08-04T19:19:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI5MjAxOQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkwODg3Mw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r465908873", "bodyText": "I wonder if this is correct. Usually, we use CLUSTER_ACTION action with the CLUSTER resource. For instance, this is how we authorize control requests:\nauthorize(request.context, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME)\n\nI thought that we would do the same in this case. Don't we?", "author": "dajac", "createdAt": "2020-08-05T18:05:16Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2982,12 +3089,33 @@ class KafkaApis(val requestChannel: RequestChannel,\n                                 logIfDenied: Boolean = true,\n                                 refCount: Int = 1): Boolean = {\n     authorizer.forall { authZ =>\n-      val resource = new ResourcePattern(resourceType, resourceName, PatternType.LITERAL)\n-      val actions = Collections.singletonList(new Action(operation, resource, refCount, logIfAllowed, logIfDenied))\n-      authZ.authorize(requestContext, actions).get(0) == AuthorizationResult.ALLOWED\n+      if (authorizeAction(requestContext, operation,\n+        resourceType, resourceName, logIfAllowed, logIfDenied, refCount, authZ)) {\n+        true\n+      } else {\n+        operation match {\n+          case ALTER | ALTER_CONFIGS | CREATE | DELETE =>\n+            requestContext.maybeFromControlPlane &&\n+              authorizeAction(requestContext, CLUSTER_ACTION,\n+                resourceType, resourceName, logIfAllowed, logIfDenied, refCount, authZ)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU4MjI4MQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r466582281", "bodyText": "I'm not sure either, cc @rajinisivaram @cmccabe", "author": "abbccdda", "createdAt": "2020-08-06T17:45:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkwODg3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcxNDQyMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r466714423", "bodyText": "Actually I think you are right, will change here.", "author": "abbccdda", "createdAt": "2020-08-06T22:14:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkwODg3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIxNDUwNw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r467214507", "bodyText": "Can we get rid of whitespace-only changes like this, or at least move them to another PR?", "author": "cmccabe", "createdAt": "2020-08-07T18:54:46Z", "path": "core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala", "diffHunk": "@@ -497,10 +497,10 @@ class AuthorizerIntegrationTest extends BaseRequestTest {\n \n   private def alterConfigsRequest =\n     new AlterConfigsRequest.Builder(\n-      Collections.singletonMap(new ConfigResource(ConfigResource.Type.TOPIC, tp.topic),\n-        new AlterConfigsRequest.Config(Collections.singleton(\n-          new AlterConfigsRequest.ConfigEntry(LogConfig.MaxMessageBytesProp, \"1000000\")\n-        ))), true).build()\n+        Collections.singletonMap(new ConfigResource(ConfigResource.Type.TOPIC, tp.topic),\n+          new AlterConfigsRequest.Config(Collections.singleton(\n+            new AlterConfigsRequest.ConfigEntry(LogConfig.MaxMessageBytesProp, \"1000000\")\n+          ))), true).build()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIxNTY0Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r467215647", "bodyText": "Let me check around.", "author": "abbccdda", "createdAt": "2020-08-07T18:56:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIxNDUwNw=="}], "type": "inlineReview"}, {"oid": "1e35e371a05ef48761e729690cb67d944b9f1bc8", "url": "https://github.com/apache/kafka/commit/1e35e371a05ef48761e729690cb67d944b9f1bc8", "message": "fix tests", "committedDate": "2020-08-07T18:59:28Z", "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzE1ODc3OQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r473158779", "bodyText": "Add equality check for the sake of easymock verification", "author": "abbccdda", "createdAt": "2020-08-19T16:26:12Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterClientQuotasRequest.java", "diffHunk": "@@ -76,6 +77,16 @@ public AlterClientQuotasRequest build(short version) {\n         public String toString() {\n             return data.toString();\n         }\n+\n+        @Override\n+        public boolean equals(Object other) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTcxMDQwMA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r475710400", "bodyText": "Need to include:\nprivate static final long serialVersionUID = 1L;", "author": "cmccabe", "createdAt": "2020-08-24T15:43:18Z", "path": "clients/src/main/java/org/apache/kafka/common/errors/BrokerAuthorizationFailureException.java", "diffHunk": "@@ -0,0 +1,27 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.errors;\n+\n+/**\n+ * Exception used to indicate a broker side authorization failure during request redirection.\n+ */\n+public class BrokerAuthorizationFailureException extends AuthorizationException {\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg2NDI0OA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r476864248", "bodyText": "Interesting, why does the AuthorizationException have no serialVersionUID? Is it because we never use that error code explicitly?", "author": "abbccdda", "createdAt": "2020-08-25T23:36:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTcxMDQwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTcxMjU3Mw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r475712573", "bodyText": "How about: \"A broker failed to authorize itself to another component of the system.  This indicates an internal error on the broker cluster security setup\".\nThis isn't specific to forwarding... there might be other reasons why a broker would need to authorize itself and fail", "author": "cmccabe", "createdAt": "2020-08-24T15:46:38Z", "path": "clients/src/main/java/org/apache/kafka/common/protocol/Errors.java", "diffHunk": "@@ -325,7 +326,9 @@\n     UNSTABLE_OFFSET_COMMIT(88, \"There are unstable offsets that need to be cleared.\", UnstableOffsetCommitException::new),\n     THROTTLING_QUOTA_EXCEEDED(89, \"The throttling quota has been exceeded.\", ThrottlingQuotaExceededException::new),\n     PRODUCER_FENCED(90, \"There is a newer producer with the same transactionalId \" +\n-            \"which fences the current one.\", ProducerFencedException::new);\n+            \"which fences the current one.\", ProducerFencedException::new),\n+    BROKER_AUTHORIZATION_FAILURE(91, \"Authorization failed for the request during forwarding. \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTcxNDIzMA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r475714230", "bodyText": "In general we don't define equals or hashCode on these builders.  Why are we defining it here?", "author": "cmccabe", "createdAt": "2020-08-24T15:49:13Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterConfigsRequest.java", "diffHunk": "@@ -87,6 +87,16 @@ public Builder(Map<ConfigResource, Config> configs, boolean validateOnly) {\n         public AlterConfigsRequest build(short version) {\n             return new AlterConfigsRequest(data, version);\n         }\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg2MDc5Ng==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r476860796", "bodyText": "The purpose is for the mock tests to compare the expected builder in KafkaApisTest", "author": "abbccdda", "createdAt": "2020-08-25T23:33:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTcxNDIzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTcyNTM4OQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r475725389", "bodyText": "just as a note the alter isr PR may also have an object like this.  so maybe we want a name which is more specific to redirection.", "author": "cmccabe", "createdAt": "2020-08-24T16:06:10Z", "path": "core/src/main/scala/kafka/server/KafkaServer.scala", "diffHunk": "@@ -316,7 +316,10 @@ class KafkaServer(val config: KafkaConfig, time: Time = Time.SYSTEM, threadNameP\n         kafkaController = new KafkaController(config, zkClient, time, metrics, brokerInfo, brokerEpoch, tokenManager, threadNamePrefix)\n         kafkaController.startup()\n \n-        brokerToControllerChannelManager = new BrokerToControllerChannelManager(metadataCache, time, metrics, config, threadNamePrefix)\n+        if (config.redirectionEnabled) {\n+          brokerToControllerChannelManager = new BrokerToControllerChannelManager(metadataCache, time, metrics, config, threadNamePrefix)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTg3NTI4Mw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r489875283", "bodyText": "I'm still trying to decide how to make sure we could turn off the redirection in 2.7. Having a separate IBP for 3.0 may not work. @cmccabe", "author": "abbccdda", "createdAt": "2020-09-17T01:59:50Z", "path": "core/src/main/scala/kafka/server/KafkaConfig.scala", "diffHunk": "@@ -1542,6 +1542,9 @@ class KafkaConfig(val props: java.util.Map[_, _], doLog: Boolean, dynamicConfigO\n   /** ********* Feature configuration ***********/\n   def isFeatureVersioningEnabled = interBrokerProtocolVersion >= KAFKA_2_7_IV0\n \n+  /** ********* Redirection configuration ***********/\n+  def redirectionEnabled = interBrokerProtocolVersion >= KAFKA_2_7_IV1", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2MDkyNw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493160927", "bodyText": "nit: might be useful to document the expectation that resources is a subset of the key set of configs. The signature surprised me a little bit.\nAs an aside, this kind of convenience conversion seems more appropriate for IncrementalAlterConfigsRequest.Builder rather than a static class.", "author": "hachikuji", "createdAt": "2020-09-23T02:31:05Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/AlterConfigsUtil.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.message.IncrementalAlterConfigsRequestData;\n+\n+import java.util.Collection;\n+import java.util.Map;\n+\n+public class AlterConfigsUtil {\n+\n+    public static IncrementalAlterConfigsRequestData generateIncrementalRequestData(final Map<ConfigResource, Collection<AlterConfigOp>> configs,\n+                                                                                    final boolean validateOnly) {\n+        return generateIncrementalRequestData(configs.keySet(), configs, validateOnly);\n+    }\n+\n+    public static IncrementalAlterConfigsRequestData generateIncrementalRequestData(final Collection<ConfigResource> resources,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzIwNjA2OA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493206068", "bodyText": "The primary reason is that we would trigger the disallowed import if we do it in the request builder:\n[ant:checkstyle] [ERROR] /Users/boyang.chen/code/kafka/clients/src/main/java/org/apache/kafka/common/requests/IncrementalAlterConfigsRequest.java:20:1: Disallowed import - org.apache.kafka.clients.admin.AlterConfigOp. [ImportControl]\n\nLet me check if we could make exceptions here", "author": "abbccdda", "createdAt": "2020-09-23T05:28:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2MDkyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2NTUyMA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493165520", "bodyText": "Typically responses are immutable after construction. It seems kind of a brittle pattern to rely on being able to mutate the response we receive from the other broker. For example we inherit the throttle time which is a bit weird. Are we saving that much by not creating a new response?", "author": "hachikuji", "createdAt": "2020-09-23T02:48:53Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/IncrementalAlterConfigsResponse.java", "diffHunk": "@@ -25,23 +25,35 @@\n import org.apache.kafka.common.protocol.types.Struct;\n \n import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n import java.util.HashMap;\n+import java.util.List;\n import java.util.Map;\n \n public class IncrementalAlterConfigsResponse extends AbstractResponse {\n \n-    public static IncrementalAlterConfigsResponseData toResponseData(final int requestThrottleMs,\n-                                                                     final Map<ConfigResource, ApiError> results) {\n-        IncrementalAlterConfigsResponseData responseData = new IncrementalAlterConfigsResponseData();\n-        responseData.setThrottleTimeMs(requestThrottleMs);\n-        for (Map.Entry<ConfigResource, ApiError> entry : results.entrySet()) {\n-            responseData.responses().add(new AlterConfigsResourceResponse().\n-                    setResourceName(entry.getKey().name()).\n-                    setResourceType(entry.getKey().type().id()).\n-                    setErrorCode(entry.getValue().error().code()).\n-                    setErrorMessage(entry.getValue().message()));\n-        }\n-        return responseData;\n+    public IncrementalAlterConfigsResponse(final int requestThrottleMs,\n+                                           final Map<ConfigResource, ApiError> results) {\n+        this.data = new IncrementalAlterConfigsResponseData()\n+                        .setThrottleTimeMs(requestThrottleMs);\n+\n+        addResults(results);\n+    }\n+\n+    public IncrementalAlterConfigsResponse addResults(final Map<ConfigResource, ApiError> results) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc4MjQxMA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493782410", "bodyText": "I guess we could get rid of it and do the merge in caller level.", "author": "abbccdda", "createdAt": "2020-09-23T17:56:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2NTUyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2ODA2MQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493168061", "bodyText": "In general, the forwarded request may have a different version than the client request. I'm wondering if we should keep the version the same in case there are semantic differences. As an example, a newer version of the API may introduce unexpected error codes. Unless we have logic to convert those error codes, then we might break compatibility unexpectedly.", "author": "hachikuji", "createdAt": "2020-09-23T02:58:59Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -117,6 +117,89 @@ class KafkaApis(val requestChannel: RequestChannel,\n   val adminZkClient = new AdminZkClient(zkClient)\n   private val alterAclsPurgatory = new DelayedFuturePurgatory(purgatoryName = \"AlterAcls\", brokerId = config.brokerId)\n \n+  /**\n+   * The template to create a forward request handler.\n+   *\n+   * @tparam T request type\n+   * @tparam R response type\n+   * @tparam RK resource key\n+   * @tparam RV resource value\n+   */\n+  private[server] abstract class ForwardRequestHandler[T <: AbstractRequest,\n+    R <: AbstractResponse, RK, RV](request: RequestChannel.Request) extends Logging {\n+\n+    /**\n+     * Split the given resource into authorized and unauthorized sets.\n+     *\n+     * @return authorized resources and unauthorized resources\n+     */\n+    def resourceSplitByAuthorization(request: T): (Map[RK, RV], Map[RK, ApiError])\n+\n+    /**\n+     * Controller handling logic of the request.\n+     */\n+    def process(authorizedResources: Map[RK, RV],\n+                unauthorizedResult: Map[RK, ApiError],\n+                request: T): Unit\n+\n+    /**\n+     * Build a forward request to the controller.\n+     *\n+     * @param authorizedResources authorized resources by the forwarding broker\n+     * @param request the original request\n+     * @return forward request builder\n+     */\n+    def createRequestBuilder(authorizedResources: Map[RK, RV],\n+                             request: T): AbstractRequest.Builder[T]\n+\n+    /**\n+     * Merge the forward response with the previously unauthorized results.\n+     *\n+     * @param forwardResponse the forward request's response\n+     * @param unauthorizedResult original unauthorized results\n+     * @return combined response to the original client\n+     */\n+    def mergeResponse(forwardResponse: R,\n+                      unauthorizedResult: Map[RK, ApiError]): R\n+\n+    def handle(): Unit = {\n+      val requestBody = request.body[AbstractRequest].asInstanceOf[T]\n+      val (authorizedResources, unauthorizedResources) = resourceSplitByAuthorization(requestBody)\n+      if (isForwardingRequest(request)) {\n+        if (!controller.isActive) {\n+          sendErrorResponseMaybeThrottle(request, Errors.NOT_CONTROLLER.exception())\n+          } else {\n+            // For forwarding requests, the authentication failure is not caused by\n+            // the original client, but by the broker.\n+            val unauthorizedResult = unauthorizedResources.keys.map {\n+              resource => resource -> new ApiError(Errors.BROKER_AUTHORIZATION_FAILURE, null)\n+            }.toMap\n+\n+            process(authorizedResources, unauthorizedResult, requestBody)\n+          }\n+      } else if (!controller.isActive && config.redirectionEnabled &&\n+        authorizedResources.nonEmpty) {\n+        redirectionManager.forwardRequest(\n+          createRequestBuilder(authorizedResources, requestBody),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc5NTQzNg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493795436", "bodyText": "It's a bit hard since we are passing requestBuilder all the way to NetworkClient, so if we want a designated version to build the request, that may involve some non-trivial changes.", "author": "abbccdda", "createdAt": "2020-09-23T18:17:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2ODA2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzgyMzc3Ng==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493823776", "bodyText": "As discussed offline, we can pass the expected version down to the Builder. The abstract builder already supports an explicit range of versions. In any case, it doesn't seem like we have a choice.\nBy the way, one potential edge case here is that the broker receiving the request has upgraded to a later version than the controller. This would be possible in the middle of a rolling upgrade. I don't think there's an easy way to handle this. We could return UNSUPPORTED_VERSION to the client, but that would be surprising since the client chose a supported API based on ApiVersions and is not aware of the controller redirection.\nOne idea to address this problem is to gate version upgrades to redirectable APIs by the IBP. Basically all of these APIs have become inter-broker APIs through redirection so they need the safeguard of the IBP. Feels like we might have to do this.", "author": "hachikuji", "createdAt": "2020-09-23T18:56:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2ODA2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2ODY5Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493168697", "bodyText": "Get rid of this TODO. We do not need to remove IBP internal versions.", "author": "hachikuji", "createdAt": "2020-09-23T03:01:28Z", "path": "core/src/main/scala/kafka/api/ApiVersion.scala", "diffHunk": "@@ -103,6 +103,9 @@ object ApiVersion {\n     KAFKA_2_7_IV0,\n     // Bup Fetch protocol for Raft protocol (KIP-595)\n     KAFKA_2_7_IV1,\n+    // Enable redirection (KIP-590)\n+    // TODO: remove this IBP in the 2.7 release if redirection work could not be done before the freeze", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzIwOTk3NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493209974", "bodyText": "But in case we release AK 2.7, wouldn't this flag give user the confidence to upgrade to, which we don't want to happen?", "author": "abbccdda", "createdAt": "2020-09-23T05:41:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2ODY5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc0MjY3Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493742677", "bodyText": "I'm not sure I follow. Do you not want redirection to be part of 2.7?", "author": "hachikuji", "createdAt": "2020-09-23T16:50:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2ODY5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2OTE5Mg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493169192", "bodyText": "nit: why don't we add a case class and make this optional. for example:\ncase class InitialPrincipal(name: String, clientId: String)\nIn addition to reducing parameters, that makes the expectation that both are provided explicit.", "author": "hachikuji", "createdAt": "2020-09-23T03:03:34Z", "path": "core/src/main/scala/kafka/common/InterBrokerSendThread.scala", "diffHunk": "@@ -147,7 +147,9 @@ abstract class InterBrokerSendThread(name: String,\n \n case class RequestAndCompletionHandler(destination: Node,\n                                        request: AbstractRequest.Builder[_ <: AbstractRequest],\n-                                       handler: RequestCompletionHandler)\n+                                       handler: RequestCompletionHandler,\n+                                       initialPrincipalName: String = null,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2OTI3Mw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493169273", "bodyText": "nit: space after if", "author": "hachikuji", "createdAt": "2020-09-23T03:03:54Z", "path": "core/src/main/scala/kafka/security/authorizer/AclAuthorizer.scala", "diffHunk": "@@ -459,7 +459,10 @@ class AclAuthorizer extends Authorizer with Logging {\n       val apiKey = if (ApiKeys.hasId(requestContext.requestType)) ApiKeys.forId(requestContext.requestType).name else requestContext.requestType\n       val refCount = action.resourceReferenceCount\n \n-      s\"Principal = $principal is $authResult Operation = $operation from host = $host on resource = $resource for request = $apiKey with resourceRefCount = $refCount\"\n+      val initialPrincipalName = requestContext.initialPrincipalName\n+      val initialPrincipalMessage = if(initialPrincipalName != null) s\", on behalf of initial principal =$initialPrincipalName,\" else \"\"", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2OTY5NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493169694", "bodyText": "Can you explain why this change is needed?", "author": "hachikuji", "createdAt": "2020-09-23T03:05:42Z", "path": "core/src/main/scala/kafka/server/AdminManager.scala", "diffHunk": "@@ -513,15 +513,21 @@ class AdminManager(val config: KafkaConfig,\n     resource -> ApiError.NONE\n   }\n \n-  private def alterBrokerConfigs(resource: ConfigResource, validateOnly: Boolean,\n-                                 configProps: Properties, configEntriesMap: Map[String, String]): (ConfigResource, ApiError) = {\n+  private def alterBrokerConfigs(resource: ConfigResource,\n+                                 validateOnly: Boolean,\n+                                 configProps: Properties,\n+                                 configEntriesMap: Map[String, String]): (ConfigResource, ApiError) = {\n     val brokerId = getBrokerId(resource)\n     val perBrokerConfig = brokerId.nonEmpty\n     this.config.dynamicConfig.validate(configProps, perBrokerConfig)\n     validateConfigPolicy(resource, configEntriesMap)\n     if (!validateOnly) {\n-      if (perBrokerConfig)\n+      if (perBrokerConfig) {\n+        val previousConfigProps = config.dynamicConfig.currentDynamicBrokerConfigs\n         this.config.dynamicConfig.reloadUpdatedFilesWithoutConfigChange(configProps)\n+        this.config.dynamicConfig.maybeAugmentSSLStorePaths(configProps, previousConfigProps)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc1MTAyNg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493751026", "bodyText": "The rational is to trigger a reload of ssl store file by the ZK notification. @cmccabe @rajinisivaram came out this idea to augment the path to\n//path//to//ssl//store//file\n\nwhen a reload is requested on the receiver broker, and by propagating such a path other brokers would see a difference and thus reload their corresponding store files as well. In the meantime, we need to trim the path back to single slash after handling the notification:\n/path/to/ssl/store/file", "author": "abbccdda", "createdAt": "2020-09-23T17:03:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2OTY5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MDEzMg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493170132", "bodyText": "The comment doesn't seem to make sense here. Seems like the logic doesn't have anything to do with the controller?", "author": "hachikuji", "createdAt": "2020-09-23T03:07:28Z", "path": "core/src/main/scala/kafka/server/AdminManager.scala", "diffHunk": "@@ -547,7 +553,8 @@ class AdminManager(val config: KafkaConfig,\n       None\n     else {\n       val id = resourceNameToBrokerId(resource.name)\n-      if (id != this.config.brokerId)\n+      // Under redirection, it is possible to handle config changes targeting at brokers other than the controller.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc1Mjg1NQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493752855", "bodyText": "The logic is needed when there is an AlterConfigRequest targeting at a specific broker. Since the non-controller node will no longer handle AlterConfigs, it is possible to see a redirected changing request with a broker.id different than the controller broker.id.", "author": "abbccdda", "createdAt": "2020-09-23T17:07:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MDEzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MDQ5NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493170494", "bodyText": "This function has 3 callbacks... It would be nice if we could figure out how to pass through the ForwardRequestHandler directly.", "author": "hachikuji", "createdAt": "2020-09-23T03:09:02Z", "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala", "diffHunk": "@@ -117,10 +119,26 @@ class BrokerToControllerChannelManager(metadataCache: kafka.server.MetadataCache\n                                   callback: RequestCompletionHandler): Unit = {\n     requestQueue.put(BrokerToControllerQueueItem(request, callback))\n   }\n+\n+  private[server] def forwardRequest(requestBuilder: AbstractRequest.Builder[_ <: AbstractRequest],\n+                                     responseToOriginalClient: (RequestChannel.Request, Int => AbstractResponse,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MDgzMQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493170831", "bodyText": "nit: this is misaligned", "author": "hachikuji", "createdAt": "2020-09-23T03:10:24Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -117,6 +117,89 @@ class KafkaApis(val requestChannel: RequestChannel,\n   val adminZkClient = new AdminZkClient(zkClient)\n   private val alterAclsPurgatory = new DelayedFuturePurgatory(purgatoryName = \"AlterAcls\", brokerId = config.brokerId)\n \n+  /**\n+   * The template to create a forward request handler.\n+   *\n+   * @tparam T request type\n+   * @tparam R response type\n+   * @tparam RK resource key\n+   * @tparam RV resource value\n+   */\n+  private[server] abstract class ForwardRequestHandler[T <: AbstractRequest,\n+    R <: AbstractResponse, RK, RV](request: RequestChannel.Request) extends Logging {\n+\n+    /**\n+     * Split the given resource into authorized and unauthorized sets.\n+     *\n+     * @return authorized resources and unauthorized resources\n+     */\n+    def resourceSplitByAuthorization(request: T): (Map[RK, RV], Map[RK, ApiError])\n+\n+    /**\n+     * Controller handling logic of the request.\n+     */\n+    def process(authorizedResources: Map[RK, RV],\n+                unauthorizedResult: Map[RK, ApiError],\n+                request: T): Unit\n+\n+    /**\n+     * Build a forward request to the controller.\n+     *\n+     * @param authorizedResources authorized resources by the forwarding broker\n+     * @param request the original request\n+     * @return forward request builder\n+     */\n+    def createRequestBuilder(authorizedResources: Map[RK, RV],\n+                             request: T): AbstractRequest.Builder[T]\n+\n+    /**\n+     * Merge the forward response with the previously unauthorized results.\n+     *\n+     * @param forwardResponse the forward request's response\n+     * @param unauthorizedResult original unauthorized results\n+     * @return combined response to the original client\n+     */\n+    def mergeResponse(forwardResponse: R,\n+                      unauthorizedResult: Map[RK, ApiError]): R\n+\n+    def handle(): Unit = {\n+      val requestBody = request.body[AbstractRequest].asInstanceOf[T]\n+      val (authorizedResources, unauthorizedResources) = resourceSplitByAuthorization(requestBody)\n+      if (isForwardingRequest(request)) {\n+        if (!controller.isActive) {\n+          sendErrorResponseMaybeThrottle(request, Errors.NOT_CONTROLLER.exception())\n+          } else {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MTM1Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493171357", "bodyText": "We can't guarantee that this broker will still be the controller when we call process or that the broker we're forwarding to will still be the controller when it receives the request. In these cases, we need to return some retriable error to the client. Can you help me understand how this is implemented?", "author": "hachikuji", "createdAt": "2020-09-23T03:12:42Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -117,6 +117,89 @@ class KafkaApis(val requestChannel: RequestChannel,\n   val adminZkClient = new AdminZkClient(zkClient)\n   private val alterAclsPurgatory = new DelayedFuturePurgatory(purgatoryName = \"AlterAcls\", brokerId = config.brokerId)\n \n+  /**\n+   * The template to create a forward request handler.\n+   *\n+   * @tparam T request type\n+   * @tparam R response type\n+   * @tparam RK resource key\n+   * @tparam RV resource value\n+   */\n+  private[server] abstract class ForwardRequestHandler[T <: AbstractRequest,\n+    R <: AbstractResponse, RK, RV](request: RequestChannel.Request) extends Logging {\n+\n+    /**\n+     * Split the given resource into authorized and unauthorized sets.\n+     *\n+     * @return authorized resources and unauthorized resources\n+     */\n+    def resourceSplitByAuthorization(request: T): (Map[RK, RV], Map[RK, ApiError])\n+\n+    /**\n+     * Controller handling logic of the request.\n+     */\n+    def process(authorizedResources: Map[RK, RV],\n+                unauthorizedResult: Map[RK, ApiError],\n+                request: T): Unit\n+\n+    /**\n+     * Build a forward request to the controller.\n+     *\n+     * @param authorizedResources authorized resources by the forwarding broker\n+     * @param request the original request\n+     * @return forward request builder\n+     */\n+    def createRequestBuilder(authorizedResources: Map[RK, RV],\n+                             request: T): AbstractRequest.Builder[T]\n+\n+    /**\n+     * Merge the forward response with the previously unauthorized results.\n+     *\n+     * @param forwardResponse the forward request's response\n+     * @param unauthorizedResult original unauthorized results\n+     * @return combined response to the original client\n+     */\n+    def mergeResponse(forwardResponse: R,\n+                      unauthorizedResult: Map[RK, ApiError]): R\n+\n+    def handle(): Unit = {\n+      val requestBody = request.body[AbstractRequest].asInstanceOf[T]\n+      val (authorizedResources, unauthorizedResources) = resourceSplitByAuthorization(requestBody)\n+      if (isForwardingRequest(request)) {\n+        if (!controller.isActive) {\n+          sendErrorResponseMaybeThrottle(request, Errors.NOT_CONTROLLER.exception())\n+          } else {\n+            // For forwarding requests, the authentication failure is not caused by\n+            // the original client, but by the broker.\n+            val unauthorizedResult = unauthorizedResources.keys.map {\n+              resource => resource -> new ApiError(Errors.BROKER_AUTHORIZATION_FAILURE, null)\n+            }.toMap\n+\n+            process(authorizedResources, unauthorizedResult, requestBody)\n+          }\n+      } else if (!controller.isActive && config.redirectionEnabled &&\n+        authorizedResources.nonEmpty) {\n+        redirectionManager.forwardRequest(\n+          createRequestBuilder(authorizedResources, requestBody),\n+          sendResponseMaybeThrottle,\n+          request,\n+          response => {\n+            mergeResponse(response.responseBody.asInstanceOf[R], unauthorizedResources)\n+          })\n+      } else {\n+        // When IBP is smaller than 2.7, forwarding is not supported,\n+        // therefore requests are handled directly\n+        process(authorizedResources, unauthorizedResources, requestBody)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MTc5OQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493171799", "bodyText": "nit: this is subjective, but this style is a bit ugly. I would prefer the following:\noverride def resourceSplitByAuthorization(\n  createTopicsRequest: CreateTopicsRequest\n): (Map[String, CreatableTopic], Map[String, ApiError]) = {\nThat makes it easier visually to separate the return type and the function logic (again, in my opinion).", "author": "hachikuji", "createdAt": "2020-09-23T03:14:28Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -1733,68 +1817,109 @@ class KafkaApis(val requestChannel: RequestChannel,\n       sendResponseMaybeThrottle(controllerMutationQuota, request, createResponse, onComplete = None)\n     }\n \n-    val createTopicsRequest = request.body[CreateTopicsRequest]\n-    val results = new CreatableTopicResultCollection(createTopicsRequest.data.topics.size)\n-    if (!controller.isActive) {\n-      createTopicsRequest.data.topics.forEach { topic =>\n-        results.add(new CreatableTopicResult().setName(topic.name)\n-          .setErrorCode(Errors.NOT_CONTROLLER.code))\n-      }\n-      sendResponseCallback(results)\n-    } else {\n-      createTopicsRequest.data.topics.forEach { topic =>\n-        results.add(new CreatableTopicResult().setName(topic.name))\n-      }\n-      val hasClusterAuthorization = authorize(request.context, CREATE, CLUSTER, CLUSTER_NAME,\n-        logIfDenied = false)\n-      val topics = createTopicsRequest.data.topics.asScala.map(_.name)\n-      val authorizedTopics =\n-        if (hasClusterAuthorization) topics.toSet\n-        else filterByAuthorized(request.context, CREATE, TOPIC, topics)(identity)\n-      val authorizedForDescribeConfigs = filterByAuthorized(request.context, DESCRIBE_CONFIGS, TOPIC,\n-        topics, logIfDenied = false)(identity).map(name => name -> results.find(name)).toMap\n+    val forwardRequestHandler = new ForwardRequestHandler[CreateTopicsRequest,\n+      CreateTopicsResponse, String, CreatableTopic](request) {\n \n-      results.forEach { topic =>\n-        if (results.findAll(topic.name).size > 1) {\n-          topic.setErrorCode(Errors.INVALID_REQUEST.code)\n-          topic.setErrorMessage(\"Found multiple entries for this topic.\")\n-        } else if (!authorizedTopics.contains(topic.name)) {\n-          topic.setErrorCode(Errors.TOPIC_AUTHORIZATION_FAILED.code)\n-          topic.setErrorMessage(\"Authorization failed.\")\n-        }\n-        if (!authorizedForDescribeConfigs.contains(topic.name)) {\n-          topic.setTopicConfigErrorCode(Errors.TOPIC_AUTHORIZATION_FAILED.code)\n+      override def resourceSplitByAuthorization(createTopicsRequest: CreateTopicsRequest):\n+      (Map[String, CreatableTopic], Map[String, ApiError]) = {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MjQ1Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493172457", "bodyText": "nit: seems handle doesn't really need to be part of ForwardRequestHandler. Instead we could pull it out:\nprivate def handle(handler: ForwardRequestHandler): Unit = {\n...\nThe advantage of this is that it allows us to pull the type out of KafkaApis without inheriting all of the dependencies that are needed by handle.", "author": "hachikuji", "createdAt": "2020-09-23T03:17:08Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -117,6 +117,89 @@ class KafkaApis(val requestChannel: RequestChannel,\n   val adminZkClient = new AdminZkClient(zkClient)\n   private val alterAclsPurgatory = new DelayedFuturePurgatory(purgatoryName = \"AlterAcls\", brokerId = config.brokerId)\n \n+  /**\n+   * The template to create a forward request handler.\n+   *\n+   * @tparam T request type\n+   * @tparam R response type\n+   * @tparam RK resource key\n+   * @tparam RV resource value\n+   */\n+  private[server] abstract class ForwardRequestHandler[T <: AbstractRequest,\n+    R <: AbstractResponse, RK, RV](request: RequestChannel.Request) extends Logging {\n+\n+    /**\n+     * Split the given resource into authorized and unauthorized sets.\n+     *\n+     * @return authorized resources and unauthorized resources\n+     */\n+    def resourceSplitByAuthorization(request: T): (Map[RK, RV], Map[RK, ApiError])\n+\n+    /**\n+     * Controller handling logic of the request.\n+     */\n+    def process(authorizedResources: Map[RK, RV],\n+                unauthorizedResult: Map[RK, ApiError],\n+                request: T): Unit\n+\n+    /**\n+     * Build a forward request to the controller.\n+     *\n+     * @param authorizedResources authorized resources by the forwarding broker\n+     * @param request the original request\n+     * @return forward request builder\n+     */\n+    def createRequestBuilder(authorizedResources: Map[RK, RV],\n+                             request: T): AbstractRequest.Builder[T]\n+\n+    /**\n+     * Merge the forward response with the previously unauthorized results.\n+     *\n+     * @param forwardResponse the forward request's response\n+     * @param unauthorizedResult original unauthorized results\n+     * @return combined response to the original client\n+     */\n+    def mergeResponse(forwardResponse: R,\n+                      unauthorizedResult: Map[RK, ApiError]): R\n+\n+    def handle(): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MzcyMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493173723", "bodyText": "It would be helpful to have a comment explaining this. It does not seem obvious.", "author": "hachikuji", "createdAt": "2020-09-23T03:22:14Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -3064,12 +3272,33 @@ class KafkaApis(val requestChannel: RequestChannel,\n                                 logIfDenied: Boolean = true,\n                                 refCount: Int = 1): Boolean = {\n     authorizer.forall { authZ =>\n-      val resource = new ResourcePattern(resourceType, resourceName, PatternType.LITERAL)\n-      val actions = Collections.singletonList(new Action(operation, resource, refCount, logIfAllowed, logIfDenied))\n-      authZ.authorize(requestContext, actions).get(0) == AuthorizationResult.ALLOWED\n+      if (authorizeAction(requestContext, operation,\n+        resourceType, resourceName, logIfAllowed, logIfDenied, refCount, authZ)) {\n+        true\n+      } else {\n+        operation match {\n+          case ALTER | ALTER_CONFIGS | CREATE | DELETE =>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3NDA4NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493174084", "bodyText": "Good to see the unit tests in here. I think we also need at least a couple integration tests. For example, could we add something to CreateTopicsRequestTest to ensure that forwarding works as expected?", "author": "hachikuji", "createdAt": "2020-09-23T03:23:45Z", "path": "core/src/test/scala/unit/kafka/server/KafkaApisTest.scala", "diffHunk": "@@ -273,31 +275,632 @@ class KafkaApisTest {\n       .setIncludeSynonyms(true)\n       .setResources(List(new DescribeConfigsRequestData.DescribeConfigsResource()\n         .setResourceName(\"topic-1\")\n-        .setResourceType(ConfigResource.Type.TOPIC.id)).asJava)).build(requestHeader.apiVersion))\n+        .setResourceType(ConfigResource.Type.TOPIC.id)).asJava))\n+      .build(requestHeader.apiVersion),\n+      requestHeader = Option(requestHeader))\n     createKafkaApis(authorizer = Some(authorizer)).handleDescribeConfigsRequest(request)\n \n     verify(authorizer, adminManager)\n   }\n \n+  @Test\n+  def testAlterClientQuotasWithAuthorizer(): Unit = {\n+    val authorizer: Authorizer = EasyMock.niceMock(classOf[Authorizer])\n+\n+    authorizeResource(authorizer, AclOperation.ALTER_CONFIGS, ResourceType.CLUSTER,\n+      Resource.CLUSTER_NAME, AuthorizationResult.ALLOWED)\n+\n+    val quotaEntity = new ClientQuotaEntity(Collections.singletonMap(ClientQuotaEntity.USER, \"user\"))\n+    val quotas = Seq(new ClientQuotaAlteration(quotaEntity, Seq.empty.asJavaCollection))\n+\n+    val requestHeader = new RequestHeader(ApiKeys.ALTER_CLIENT_QUOTAS, ApiKeys.ALTER_CLIENT_QUOTAS.latestVersion,\n+      clientId, 0)\n+\n+    val request = buildRequest(new AlterClientQuotasRequest.Builder(quotas.asJavaCollection, false)\n+      .build(requestHeader.apiVersion))\n+\n+    EasyMock.expect(controller.isActive).andReturn(true)\n+\n+    expectNoThrottling()\n+\n+    EasyMock.expect(adminManager.alterClientQuotas(anyObject(), EasyMock.eq(false)))\n+      .andReturn(Map(quotaEntity -> ApiError.NONE))\n+\n+    EasyMock.replay(replicaManager, clientRequestQuotaManager, requestChannel, authorizer,\n+      adminManager, controller)\n+\n+    createKafkaApis(authorizer = Some(authorizer)).handleAlterClientQuotasRequest(request)\n+\n+    verify(authorizer, adminManager)\n+  }\n+\n+  @Test\n+  def testAlterClientQuotasWithNonControllerAndRedirectionDisabled(): Unit = {\n+    val authorizer: Authorizer = EasyMock.niceMock(classOf[Authorizer])\n+\n+    authorizeResource(authorizer, AclOperation.ALTER_CONFIGS, ResourceType.CLUSTER,\n+      Resource.CLUSTER_NAME, AuthorizationResult.ALLOWED)\n+\n+    val quotaEntity = new ClientQuotaEntity(Collections.singletonMap(ClientQuotaEntity.USER, \"user\"))\n+    val quotas = Seq(new ClientQuotaAlteration(quotaEntity, Seq.empty.asJavaCollection))\n+\n+    val requestHeader = new RequestHeader(ApiKeys.ALTER_CLIENT_QUOTAS, ApiKeys.ALTER_CLIENT_QUOTAS.latestVersion,\n+      clientId, 0)\n+\n+    val alterClientQuotasRequest = new AlterClientQuotasRequest.Builder(quotas.asJavaCollection, false)\n+      .build(requestHeader.apiVersion)\n+    val request = buildRequest(alterClientQuotasRequest)\n+\n+    EasyMock.expect(controller.isActive).andReturn(false)\n+\n+    val capturedResponse = expectNoThrottling()\n+\n+    EasyMock.expect(adminManager.alterClientQuotas(anyObject(), EasyMock.eq(false)))\n+      .andReturn(Map(quotaEntity -> ApiError.NONE))\n+\n+    EasyMock.replay(replicaManager, clientRequestQuotaManager, requestChannel, authorizer,\n+      adminManager, controller)\n+\n+    // Should just handle the config change since IBP is low\n+    createKafkaApis(interBrokerProtocolVersion = KAFKA_2_6_IV0,\n+      authorizer = Some(authorizer)).handleAlterClientQuotasRequest(request)\n+\n+    verifyAlterClientQuotaResult(alterClientQuotasRequest,\n+      capturedResponse, Map(quotaEntity -> Errors.NONE))\n+\n+    verify(authorizer, adminManager)\n+  }\n+\n+  @Test\n+  def testAlterClientQuotasWithRedirection(): Unit = {\n+    val authorizer: Authorizer = EasyMock.niceMock(classOf[Authorizer])\n+\n+    authorizeResource(authorizer, AclOperation.ALTER_CONFIGS, ResourceType.CLUSTER,\n+      Resource.CLUSTER_NAME, AuthorizationResult.ALLOWED)\n+\n+    val quotaEntity = new ClientQuotaEntity(Collections.singletonMap(ClientQuotaEntity.USER, \"user\"))\n+    val quotas = Seq(new ClientQuotaAlteration(quotaEntity, Seq.empty.asJavaCollection))\n+\n+    val requestHeader = new RequestHeader(ApiKeys.ALTER_CLIENT_QUOTAS, ApiKeys.ALTER_CLIENT_QUOTAS.latestVersion,\n+      clientId, 0)\n+\n+    val request = buildRequest(new AlterClientQuotasRequest.Builder(quotas.asJavaCollection, false)\n+      .build(requestHeader.apiVersion))\n+\n+    EasyMock.expect(controller.isActive).andReturn(false)\n+\n+    expectNoThrottling()\n+\n+    val redirectRequestBuilder = new AlterClientQuotasRequest.Builder(\n+      Set(new ClientQuotaAlteration(quotaEntity, Collections.emptySet())).asJava, false)\n+\n+    val capturedCallback = EasyMock.newCapture[ClientResponse => AbstractResponse]()\n+\n+    EasyMock.expect(redirectionManager.forwardRequest(\n+      EasyMock.eq(redirectRequestBuilder),\n+      anyObject[(RequestChannel.Request, Int => AbstractResponse,\n+        Option[Send => Unit]) => Unit](),\n+      EasyMock.eq(request),\n+      EasyMock.capture(capturedCallback),\n+      anyObject()\n+    )).once()\n+\n+    val clientResponse: ClientResponse = EasyMock.createNiceMock(classOf[ClientResponse])\n+    val alterClientQuotasResponse = new AlterClientQuotasResponse(\n+      Map(quotaEntity -> ApiError.NONE).asJava, 10\n+    )\n+    EasyMock.expect(clientResponse.responseBody).andReturn(alterClientQuotasResponse)\n+\n+    EasyMock.replay(replicaManager, clientRequestQuotaManager, requestChannel,\n+      authorizer, controller, redirectionManager, clientResponse)\n+\n+    createKafkaApis(authorizer = Some(authorizer)).handleAlterClientQuotasRequest(request)\n+\n+    assertEquals(alterClientQuotasResponse, capturedCallback.getValue.apply(clientResponse))\n+\n+    EasyMock.verify(controller, redirectionManager)\n+  }\n+\n+  @Test\n+  def testAlterClientQuotasAsForwardingRequestWithNonController(): Unit = {\n+    val authorizer: Authorizer = EasyMock.niceMock(classOf[Authorizer])\n+\n+    authorizeResource(authorizer, AclOperation.ALTER_CONFIGS, ResourceType.CLUSTER,\n+      Resource.CLUSTER_NAME, AuthorizationResult.ALLOWED)\n+\n+    val quotaEntity = new ClientQuotaEntity(Collections.singletonMap(ClientQuotaEntity.USER, \"user\"))\n+    val quotas = Seq(new ClientQuotaAlteration(quotaEntity, Seq.empty.asJavaCollection))\n+\n+    // Include extra header fields for forwarding request check\n+    val requestHeader = new RequestHeader(ApiKeys.ALTER_CLIENT_QUOTAS, ApiKeys.ALTER_CLIENT_QUOTAS.latestVersion,\n+      clientId, 0, \"initial-principal\", \"initial-client\")\n+\n+    val alterClientQuotasRequest = new AlterClientQuotasRequest.Builder(quotas.asJavaCollection, false)\n+      .build(requestHeader.apiVersion)\n+    val request = buildRequest(alterClientQuotasRequest,\n+      fromPrivilegedListener = true, requestHeader = Option(requestHeader))\n+\n+    EasyMock.expect(controller.isActive).andReturn(false)\n+\n+    val capturedResponse = expectNoThrottling()\n+\n+    EasyMock.replay(replicaManager, clientRequestQuotaManager, requestChannel, authorizer,\n+      adminManager, controller)\n+\n+    createKafkaApis(authorizer = Some(authorizer)).handleAlterClientQuotasRequest(request)\n+\n+    verifyAlterClientQuotaResult(alterClientQuotasRequest,\n+      capturedResponse, Map(quotaEntity -> Errors.NOT_CONTROLLER))\n+\n+    verify(authorizer, adminManager)\n+  }\n+\n+  @Test\n+  def testAlterClientQuotasAsForwardingRequest(): Unit = {\n+    val authorizer: Authorizer = EasyMock.niceMock(classOf[Authorizer])\n+\n+    authorizeResource(authorizer, AclOperation.ALTER_CONFIGS, ResourceType.CLUSTER,\n+      Resource.CLUSTER_NAME, AuthorizationResult.DENIED)\n+    // As a forwarding request, we would use CLUSTER_ACTION to do a separate round of auth.\n+    authorizeResource(authorizer, AclOperation.CLUSTER_ACTION, ResourceType.CLUSTER,\n+      Resource.CLUSTER_NAME, AuthorizationResult.DENIED)\n+\n+    val quotaEntity = new ClientQuotaEntity(Collections.singletonMap(ClientQuotaEntity.USER, \"user\"))\n+    val quotas = Seq(new ClientQuotaAlteration(quotaEntity, Seq.empty.asJavaCollection))\n+\n+    // Include extra header fields for forwarding request check\n+    val requestHeader = new RequestHeader(ApiKeys.ALTER_CLIENT_QUOTAS, ApiKeys.ALTER_CLIENT_QUOTAS.latestVersion,\n+      clientId, 0, \"initial-principal\", \"initial-client\")\n+\n+    val alterClientQuotasRequest = new AlterClientQuotasRequest.Builder(quotas.asJavaCollection, false)\n+      .build(requestHeader.apiVersion)\n+    val request = buildRequest(alterClientQuotasRequest,\n+      fromPrivilegedListener = true, requestHeader = Option(requestHeader))\n+\n+    EasyMock.expect(controller.isActive).andReturn(true)\n+\n+    val capturedResponse = expectNoThrottling()\n+\n+    EasyMock.replay(replicaManager, clientRequestQuotaManager, requestChannel, authorizer,\n+      adminManager, controller)\n+\n+    createKafkaApis(authorizer = Some(authorizer)).handleAlterClientQuotasRequest(request)\n+\n+    verifyAlterClientQuotaResult(alterClientQuotasRequest,\n+      capturedResponse, Map( quotaEntity -> Errors.BROKER_AUTHORIZATION_FAILURE))\n+\n+    verify(authorizer, adminManager)\n+  }\n+\n+  private def verifyAlterClientQuotaResult(alterClientQuotasRequest: AlterClientQuotasRequest,\n+                                           capturedResponse: Capture[RequestChannel.Response],\n+                                           expected: Map[ClientQuotaEntity, Errors]): Unit = {\n+    val response = readResponse(ApiKeys.ALTER_CLIENT_QUOTAS, alterClientQuotasRequest, capturedResponse)\n+      .asInstanceOf[AlterClientQuotasResponse]\n+    val futures = expected.keys.map(quotaEntity => quotaEntity -> new KafkaFutureImpl[Void]()).toMap\n+    response.complete(futures.asJava)\n+    futures.foreach {\n+      case (entity, future) =>\n+        future.whenComplete((_, thrown) =>\n+          assertEquals(thrown, expected(entity).exception())\n+        ).isDone\n+    }\n+  }\n+\n+  @Test\n+  def testCreateTopicsWithAuthorizer(): Unit = {\n+    val authorizer: Authorizer = EasyMock.niceMock(classOf[Authorizer])\n+\n+    val operation = AclOperation.CREATE\n+    val topicName = \"topic-1\"\n+    val requestHeader = new RequestHeader(ApiKeys.CREATE_TOPICS, ApiKeys.CREATE_TOPICS.latestVersion,\n+      clientId, 0)\n+\n+    EasyMock.expect(controller.isActive).andReturn(true)\n+\n+    authorizeResource(authorizer, operation, ResourceType.CLUSTER,\n+      Resource.CLUSTER_NAME, AuthorizationResult.ALLOWED, logIfDenied = false)\n+\n+    authorizeResource(authorizer, AclOperation.DESCRIBE_CONFIGS, ResourceType.TOPIC,\n+      topicName, AuthorizationResult.ALLOWED, logIfDenied = false)\n+\n+    expectNoThrottling()\n+\n+    val topicsAuthorized = new CreateTopicsRequestData.CreatableTopicCollection(1)\n+    val topicToCreate = new CreateTopicsRequestData.CreatableTopic()\n+      .setName(topicName)\n+    topicsAuthorized.add(topicToCreate)\n+\n+    val timeout = 10\n+    val request = buildRequest(new CreateTopicsRequest.Builder(new CreateTopicsRequestData()\n+      .setTimeoutMs(timeout)\n+      .setValidateOnly(false)\n+      .setTopics(topicsAuthorized))\n+      .build(requestHeader.apiVersion))\n+\n+    EasyMock.expect(clientControllerQuotaManager.newQuotaFor(\n+      EasyMock.eq(request), EasyMock.eq(6))).andReturn(UnboundedControllerMutationQuota)\n+\n+    EasyMock.expect(adminManager.createTopics(\n+      EasyMock.eq(timeout),\n+      EasyMock.eq(false),\n+      EasyMock.eq(Map(topicName -> topicToCreate)),\n+      anyObject(),\n+      EasyMock.eq(UnboundedControllerMutationQuota),\n+      anyObject()))\n+\n+    EasyMock.replay(replicaManager, clientRequestQuotaManager, clientControllerQuotaManager,\n+      requestChannel, authorizer, adminManager, controller)\n+\n+    createKafkaApis(authorizer = Some(authorizer)).handleCreateTopicsRequest(request)\n+\n+    verify(authorizer, adminManager, clientControllerQuotaManager)\n+  }\n+\n+  @Test\n+  def testCreateTopicsWithNonControllerAndRedirectionDisabled(): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzUxODAwNA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493518004", "bodyText": "nit: SSL => Ssl", "author": "rajinisivaram", "createdAt": "2020-09-23T12:15:37Z", "path": "core/src/main/scala/kafka/server/DynamicBrokerConfig.scala", "diffHunk": "@@ -331,6 +334,50 @@ class DynamicBrokerConfig(private val kafkaConfig: KafkaConfig) extends Logging\n       }\n   }\n \n+  private[server] def maybeAugmentSSLStorePaths(configProps: Properties, previousConfigProps: Map[String, String]): Unit ={", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzUxODcyOA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493518728", "bodyText": "Does this get reset somewhere or will we keep adding /?", "author": "rajinisivaram", "createdAt": "2020-09-23T12:16:25Z", "path": "core/src/main/scala/kafka/server/DynamicBrokerConfig.scala", "diffHunk": "@@ -331,6 +334,50 @@ class DynamicBrokerConfig(private val kafkaConfig: KafkaConfig) extends Logging\n       }\n   }\n \n+  private[server] def maybeAugmentSSLStorePaths(configProps: Properties, previousConfigProps: Map[String, String]): Unit ={\n+    val processedFiles = new mutable.HashSet[String]\n+    reconfigurables\n+      .filter(reconfigurable => ReloadableFileConfigs.exists(reconfigurable.reconfigurableConfigs.contains))\n+        .foreach({\n+          case reconfigurable: ListenerReconfigurable =>\n+            ReloadableFileConfigs.foreach(configName => {\n+              val prefixedName = reconfigurable.listenerName.configPrefix + configName\n+              if (!processedFiles.contains(prefixedName) && configProps.containsKey(prefixedName) &&\n+                configProps.get(prefixedName).equals(previousConfigProps.getOrElse(prefixedName, \"\"))) {\n+                val equivalentFileName = configProps.getProperty(prefixedName).replace(\"/\", \"//\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc3MTE0NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493771144", "bodyText": "Yes, we would trim it in trimSslStorePaths", "author": "abbccdda", "createdAt": "2020-09-23T17:37:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzUxODcyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzUxOTU4Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493519587", "bodyText": "SSL => Ssl", "author": "rajinisivaram", "createdAt": "2020-09-23T12:17:26Z", "path": "core/src/main/scala/kafka/server/DynamicBrokerConfig.scala", "diffHunk": "@@ -331,6 +334,50 @@ class DynamicBrokerConfig(private val kafkaConfig: KafkaConfig) extends Logging\n       }\n   }\n \n+  private[server] def maybeAugmentSSLStorePaths(configProps: Properties, previousConfigProps: Map[String, String]): Unit ={\n+    val processedFiles = new mutable.HashSet[String]\n+    reconfigurables\n+      .filter(reconfigurable => ReloadableFileConfigs.exists(reconfigurable.reconfigurableConfigs.contains))\n+        .foreach({\n+          case reconfigurable: ListenerReconfigurable =>\n+            ReloadableFileConfigs.foreach(configName => {\n+              val prefixedName = reconfigurable.listenerName.configPrefix + configName\n+              if (!processedFiles.contains(prefixedName) && configProps.containsKey(prefixedName) &&\n+                configProps.get(prefixedName).equals(previousConfigProps.getOrElse(prefixedName, \"\"))) {\n+                val equivalentFileName = configProps.getProperty(prefixedName).replace(\"/\", \"//\")\n+                configProps.setProperty(prefixedName, equivalentFileName)\n+                processedFiles.add(prefixedName)\n+              }\n+            })\n+        })\n+  }\n+\n+  private[server] def trimSSLStorePaths(configProps: Properties): Boolean = {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzUyMDE2MA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493520160", "bodyText": "This means update was requested, but not necessarily that file has changed?", "author": "rajinisivaram", "createdAt": "2020-09-23T12:18:05Z", "path": "core/src/main/scala/kafka/server/DynamicBrokerConfig.scala", "diffHunk": "@@ -331,6 +334,50 @@ class DynamicBrokerConfig(private val kafkaConfig: KafkaConfig) extends Logging\n       }\n   }\n \n+  private[server] def maybeAugmentSSLStorePaths(configProps: Properties, previousConfigProps: Map[String, String]): Unit ={\n+    val processedFiles = new mutable.HashSet[String]\n+    reconfigurables\n+      .filter(reconfigurable => ReloadableFileConfigs.exists(reconfigurable.reconfigurableConfigs.contains))\n+        .foreach({\n+          case reconfigurable: ListenerReconfigurable =>\n+            ReloadableFileConfigs.foreach(configName => {\n+              val prefixedName = reconfigurable.listenerName.configPrefix + configName\n+              if (!processedFiles.contains(prefixedName) && configProps.containsKey(prefixedName) &&\n+                configProps.get(prefixedName).equals(previousConfigProps.getOrElse(prefixedName, \"\"))) {\n+                val equivalentFileName = configProps.getProperty(prefixedName).replace(\"/\", \"//\")\n+                configProps.setProperty(prefixedName, equivalentFileName)\n+                processedFiles.add(prefixedName)\n+              }\n+            })\n+        })\n+  }\n+\n+  private[server] def trimSSLStorePaths(configProps: Properties): Boolean = {\n+    var fileChanged = false\n+    val processedFiles = new mutable.HashSet[String]\n+\n+    reconfigurables\n+      .filter(reconfigurable => ReloadableFileConfigs.exists(reconfigurable.reconfigurableConfigs.contains))\n+      .foreach {\n+        case reconfigurable: ListenerReconfigurable =>\n+        ReloadableFileConfigs.foreach(configName => {\n+          val prefixedName = reconfigurable.listenerName.configPrefix + configName\n+          if (!processedFiles.contains(prefixedName) && configProps.containsKey(prefixedName)) {\n+            val configFileName = configProps.getProperty(prefixedName)\n+            val equivalentFileName = configFileName.replace(\"//\", \"/\")\n+            if (!configFileName.equals(equivalentFileName)) {\n+              fileChanged = true", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc3MTI2Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493771267", "bodyText": "Yea", "author": "abbccdda", "createdAt": "2020-09-23T17:37:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzUyMDE2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzUyMDE2NQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493520165", "bodyText": "This means update was requested, but not necessarily that file has changed?", "author": "rajinisivaram", "createdAt": "2020-09-23T12:18:05Z", "path": "core/src/main/scala/kafka/server/DynamicBrokerConfig.scala", "diffHunk": "@@ -331,6 +334,50 @@ class DynamicBrokerConfig(private val kafkaConfig: KafkaConfig) extends Logging\n       }\n   }\n \n+  private[server] def maybeAugmentSSLStorePaths(configProps: Properties, previousConfigProps: Map[String, String]): Unit ={\n+    val processedFiles = new mutable.HashSet[String]\n+    reconfigurables\n+      .filter(reconfigurable => ReloadableFileConfigs.exists(reconfigurable.reconfigurableConfigs.contains))\n+        .foreach({\n+          case reconfigurable: ListenerReconfigurable =>\n+            ReloadableFileConfigs.foreach(configName => {\n+              val prefixedName = reconfigurable.listenerName.configPrefix + configName\n+              if (!processedFiles.contains(prefixedName) && configProps.containsKey(prefixedName) &&\n+                configProps.get(prefixedName).equals(previousConfigProps.getOrElse(prefixedName, \"\"))) {\n+                val equivalentFileName = configProps.getProperty(prefixedName).replace(\"/\", \"//\")\n+                configProps.setProperty(prefixedName, equivalentFileName)\n+                processedFiles.add(prefixedName)\n+              }\n+            })\n+        })\n+  }\n+\n+  private[server] def trimSSLStorePaths(configProps: Properties): Boolean = {\n+    var fileChanged = false\n+    val processedFiles = new mutable.HashSet[String]\n+\n+    reconfigurables\n+      .filter(reconfigurable => ReloadableFileConfigs.exists(reconfigurable.reconfigurableConfigs.contains))\n+      .foreach {\n+        case reconfigurable: ListenerReconfigurable =>\n+        ReloadableFileConfigs.foreach(configName => {\n+          val prefixedName = reconfigurable.listenerName.configPrefix + configName\n+          if (!processedFiles.contains(prefixedName) && configProps.containsKey(prefixedName)) {\n+            val configFileName = configProps.getProperty(prefixedName)\n+            val equivalentFileName = configFileName.replace(\"//\", \"/\")\n+            if (!configFileName.equals(equivalentFileName)) {\n+              fileChanged = true", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzUyMjE4Ng==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493522186", "bodyText": "Can't we put this logic in DynamicBrokerConfig?", "author": "rajinisivaram", "createdAt": "2020-09-23T12:20:21Z", "path": "core/src/main/scala/kafka/server/ConfigHandler.scala", "diffHunk": "@@ -203,7 +203,13 @@ class BrokerConfigHandler(private val brokerConfig: KafkaConfig,\n     if (brokerId == ConfigEntityName.Default)\n       brokerConfig.dynamicConfig.updateDefaultConfig(properties)\n     else if (brokerConfig.brokerId == brokerId.trim.toInt) {\n-      brokerConfig.dynamicConfig.updateBrokerConfig(brokerConfig.brokerId, properties)\n+      val persistentProps = brokerConfig.dynamicConfig.fromPersistentProps(properties, perBrokerConfig = true)\n+      // The filepath was changed for equivalent replacement, which means we should reload\n+      if (brokerConfig.dynamicConfig.trimSSLStorePaths(persistentProps)) {\n+        brokerConfig.dynamicConfig.reloadUpdatedFilesWithoutConfigChange(persistentProps)\n+      }", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc3MzMyNA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r493773324", "bodyText": "I feel it's more explicit to do it in here, as zk notification is the only target case.", "author": "abbccdda", "createdAt": "2020-09-23T17:41:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzUyMjE4Ng=="}], "type": "inlineReview"}, {"oid": "ccdcb996a98460d1d6db13edd2c5211f84728e1d", "url": "https://github.com/apache/kafka/commit/ccdcb996a98460d1d6db13edd2c5211f84728e1d", "message": "rebase", "committedDate": "2020-09-30T16:09:22Z", "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "0fcd219aa78558dc1a7cec93c25cea0e981df170", "url": "https://github.com/apache/kafka/commit/0fcd219aa78558dc1a7cec93c25cea0e981df170", "message": "unknown client test", "committedDate": "2020-10-09T16:51:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI4NjUyNA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r504286524", "bodyText": "nit: why don't we call it requestData to be consistent with the name used in the api spec?", "author": "hachikuji", "createdAt": "2020-10-13T22:08:10Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/EnvelopeRequest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import org.apache.kafka.common.message.DefaultPrincipalData;\n+import org.apache.kafka.common.message.EnvelopeRequestData;\n+import org.apache.kafka.common.message.EnvelopeResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n+import org.apache.kafka.common.security.auth.KafkaPrincipal;\n+import org.apache.kafka.common.security.auth.KafkaPrincipalSerde;\n+\n+import java.nio.ByteBuffer;\n+\n+public class EnvelopeRequest extends AbstractRequest {\n+\n+    public static class Builder extends AbstractRequest.Builder<EnvelopeRequest> {\n+\n+        private final EnvelopeRequestData data;\n+\n+        public Builder(ByteBuffer embedData, String clientHostName) {\n+            this(embedData, null, clientHostName);\n+        }\n+\n+        public Builder(ByteBuffer embedData,", "originalCommit": "d45fb1a048e3d17ca4796acd210ff20c21bdf19a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI4NzcyMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r504287723", "bodyText": "nit: I think it might be better to pull this out of the request class. The direction we're moving is toward dumber request/response classes. Eventually EnvelopeRequest will go away and we'll just use EnvelopeRequestData.", "author": "hachikuji", "createdAt": "2020-10-13T22:10:46Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/EnvelopeRequest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import org.apache.kafka.common.message.DefaultPrincipalData;\n+import org.apache.kafka.common.message.EnvelopeRequestData;\n+import org.apache.kafka.common.message.EnvelopeResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n+import org.apache.kafka.common.security.auth.KafkaPrincipal;\n+import org.apache.kafka.common.security.auth.KafkaPrincipalSerde;\n+\n+import java.nio.ByteBuffer;\n+\n+public class EnvelopeRequest extends AbstractRequest {\n+\n+    public static class Builder extends AbstractRequest.Builder<EnvelopeRequest> {\n+\n+        private final EnvelopeRequestData data;\n+\n+        public Builder(ByteBuffer embedData, String clientHostName) {\n+            this(embedData, null, clientHostName);\n+        }\n+\n+        public Builder(ByteBuffer embedData,\n+                       ByteBuffer serializedPrincipal,\n+                       String clientHostName) {\n+            super(ApiKeys.ENVELOPE);\n+            this.data = new EnvelopeRequestData()\n+                            .setRequestData(embedData)\n+                            .setRequestPrincipal(serializedPrincipal)\n+                            .setClientHostName(clientHostName);\n+        }\n+\n+        @Override\n+        public EnvelopeRequest build(short version) {\n+            return new EnvelopeRequest(data, version);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return data.toString();\n+        }\n+    }\n+\n+    private final EnvelopeRequestData data;\n+\n+    public EnvelopeRequest(EnvelopeRequestData data, short version) {\n+        super(ApiKeys.ENVELOPE, version);\n+        this.data = data;\n+    }\n+\n+    public EnvelopeRequest(Struct struct, short version) {\n+        super(ApiKeys.ENVELOPE, version);\n+        this.data = new EnvelopeRequestData(struct, version);\n+    }\n+\n+    public ByteBuffer requestData() {\n+        return data.requestData();\n+    }\n+\n+    public String clientHostName() {\n+        return data.clientHostName();\n+    }\n+\n+    public KafkaPrincipal requestPrincipal(KafkaPrincipalSerde principalSerde) {", "originalCommit": "d45fb1a048e3d17ca4796acd210ff20c21bdf19a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI4ODY1OQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r504288659", "bodyText": "Not sure why we need this change. I think the convention is to include NONE in error counts.", "author": "hachikuji", "createdAt": "2020-10-13T22:12:59Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/CreateTopicsResponse.java", "diffHunk": "@@ -70,9 +70,12 @@ public int throttleTimeMs() {\n     @Override\n     public Map<Errors, Integer> errorCounts() {\n         HashMap<Errors, Integer> counts = new HashMap<>();\n-        data.topics().forEach(result ->\n-            updateErrorCounts(counts, Errors.forCode(result.errorCode()))\n-        );\n+        for (CreateTopicsResponseData.CreatableTopicResult result : data.topics()) {\n+            Errors error = Errors.forCode(result.errorCode());\n+            if (error != Errors.NONE) {", "originalCommit": "d45fb1a048e3d17ca4796acd210ff20c21bdf19a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMyNzk4NQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r504327985", "bodyText": "I guess there are some inconsistency between different RPCs as I spotted cases excluding NONE. I would initiate a separate JIRA for the cleaning and revert the change here.", "author": "abbccdda", "createdAt": "2020-10-14T00:12:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI4ODY1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0MTA3MQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r504341071", "bodyText": "https://issues.apache.org/jira/browse/KAFKA-10607", "author": "abbccdda", "createdAt": "2020-10-14T01:01:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI4ODY1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5MzE0NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r504293144", "bodyText": "I'm wondering if we really need the IBP to leak into the common library. It should really only be a broker concern. Seems like the only point is so that we can continue to use the factory methods defined below from the broker code. Is that right? Could we instead move the factories to the broker?", "author": "hachikuji", "createdAt": "2020-10-13T22:24:33Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -43,15 +43,43 @@\n  */\n public class ApiVersionsResponse extends AbstractResponse {\n \n+    public static final int MIN_CONSTRAINT_IBP_VERSION = 31;", "originalCommit": "d45fb1a048e3d17ca4796acd210ff20c21bdf19a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQwODYzMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r504408633", "bodyText": "The tricky thing here is that if we handle the api version constraints on the broker side, it means we need to either make changes directly to the returned ApiVersionsResponse or spawn a new instance with applied constraints. That means leaking of the internal architecture of ApiVersionsResponse to the broker level and redundant conversions IMHO. The current approach makes sure the broker level logic is clean with only the necessity of passing the IBP number.", "author": "abbccdda", "createdAt": "2020-10-14T05:23:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5MzE0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5NTAyNA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r504295024", "bodyText": "In a similar vein, I think it's better to not include serialization logic in the response object. It tends to hide some of the details like byte buffer allocation that we might want to control at another level.", "author": "hachikuji", "createdAt": "2020-10-13T22:29:23Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/EnvelopeResponse.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import org.apache.kafka.common.message.EnvelopeResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Map;\n+\n+public class EnvelopeResponse extends AbstractResponse {\n+\n+    private final EnvelopeResponseData data;\n+\n+    public EnvelopeResponse(int throttleTimeMs, AbstractResponse innerResponse, short innerApiVersion) {", "originalCommit": "d45fb1a048e3d17ca4796acd210ff20c21bdf19a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5NTg3NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r504295874", "bodyText": "Same here. We can return ByteBuffer and leave parsing to higher layers.", "author": "hachikuji", "createdAt": "2020-10-13T22:31:40Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/EnvelopeResponse.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import org.apache.kafka.common.message.EnvelopeResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Map;\n+\n+public class EnvelopeResponse extends AbstractResponse {\n+\n+    private final EnvelopeResponseData data;\n+\n+    public EnvelopeResponse(int throttleTimeMs, AbstractResponse innerResponse, short innerApiVersion) {\n+        Struct dataStruct = innerResponse.toStruct(innerApiVersion);\n+        ByteBuffer buffer = ByteBuffer.allocate(dataStruct.sizeOf());\n+        dataStruct.writeTo(buffer);\n+        buffer.flip();\n+\n+        this.data = new EnvelopeResponseData()\n+                        .setThrottleTimeMs(throttleTimeMs)\n+                        .setResponseData(buffer);\n+    }\n+\n+    public EnvelopeResponse(EnvelopeResponseData data) {\n+        this.data = data;\n+    }\n+\n+    public AbstractResponse embedResponse(RequestHeader originalHeader) {", "originalCommit": "d45fb1a048e3d17ca4796acd210ff20c21bdf19a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5ODY2Mw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r504298663", "bodyText": "It is strange to couple the serialization of the principal with the version of the envelope request. This might help us in the case of default principal builder, but users with their own custom builder are on their own, right? I think it is better to be consistent and always leave versioning to the principal builder.", "author": "hachikuji", "createdAt": "2020-10-13T22:39:21Z", "path": "clients/src/main/java/org/apache/kafka/common/security/auth/KafkaPrincipalSerde.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.security.auth;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Serializer/Deserializer interface for {@link KafkaPrincipal} for the forwarding purpose.\n+ */\n+public interface KafkaPrincipalSerde {\n+\n+    ByteBuffer serialize(KafkaPrincipal principal, short version);", "originalCommit": "1059fa3d8d826343729ed6c6950284d39ae7d573", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5ODg3OQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r504298879", "bodyText": "nit: maybe print forwardingPrincipal only if it is defined", "author": "hachikuji", "createdAt": "2020-10-13T22:39:55Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/RequestContext.java", "diffHunk": "@@ -78,8 +101,7 @@ public RequestAndSize parseRequest(ByteBuffer buffer) {\n                         \", connectionId: \" + connectionId +\n                         \", listenerName: \" + listenerName +\n                         \", principal: \" + principal +\n-                        \", initialPrincipal: \" + initialPrincipalName() +\n-                        \", initialClientId: \" + header.initialClientId(), ex);\n+                        \", forwardingPrincipal: \" + forwardingPrincipal, ex);", "originalCommit": "1059fa3d8d826343729ed6c6950284d39ae7d573", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMwMDIzNA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r504300234", "bodyText": "Do we have a use case for this yet? I don't see that it gets used anywhere.", "author": "hachikuji", "createdAt": "2020-10-13T22:43:56Z", "path": "clients/src/main/resources/common/message/EnvelopeRequest.json", "diffHunk": "@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 58,\n+  \"type\": \"request\",\n+  \"name\": \"EnvelopeRequest\",\n+  // Request struct for redirection.\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"RequestData\", \"type\": \"bytes\", \"versions\": \"0+\", \"zeroCopy\": true,\n+      \"about\": \"The embedded request header and data.\"},\n+    { \"name\": \"PrincipalIdToken\", \"type\": \"bytes\", \"tag\": 0, \"taggedVersions\": \"0+\" },", "originalCommit": "1059fa3d8d826343729ed6c6950284d39ae7d573", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0NTY2NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r504345664", "bodyText": "Not yet, could be removed.", "author": "abbccdda", "createdAt": "2020-10-14T01:19:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMwMDIzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTc2NTY2NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r505765664", "bodyText": "What is the benefit of using a different error code instead of CLUSTER_AUTHORIZATION_FAILURE?", "author": "hachikuji", "createdAt": "2020-10-15T18:47:39Z", "path": "clients/src/main/java/org/apache/kafka/common/protocol/Errors.java", "diffHunk": "@@ -338,7 +339,9 @@\n     INCONSISTENT_VOTER_SET(94, \"Indicates that the either the sender or recipient of a \" +\n             \"voter-only request is not one of the expected voters\", InconsistentVoterSetException::new),\n     INVALID_UPDATE_VERSION(95, \"The given update version was invalid.\", InvalidUpdateVersionException::new),\n-    FEATURE_UPDATE_FAILED(96, \"Unable to update finalized features due to an unexpected server error.\", FeatureUpdateFailedException::new);\n+    FEATURE_UPDATE_FAILED(96, \"Unable to update finalized features due to an unexpected server error.\", FeatureUpdateFailedException::new),\n+    BROKER_AUTHORIZATION_FAILURE(97, \"Authorization failed for the request during forwarding. \" +", "originalCommit": "9ce9d905e07cae741b79e265a30c87ea4b1e463b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg0MjkwNA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r505842904", "bodyText": "CLUSTER_AUTHORIZATION_FAILURE normally indicates a client side security configuration error. We intentionally define a separate error code to let admin know that there is some security config trouble with the brokers, not the clients.", "author": "abbccdda", "createdAt": "2020-10-15T20:56:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTc2NTY2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTUyODAwOQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509528009", "bodyText": "Not sure I follow. All current inter-broker APIs are gated by ClusterAction and will return CLUSTER_AUTHORIZATION_FAILURE if the principal does not have access. There is no distinction between clients and brokers. It's not clear to me why we need something different here.", "author": "hachikuji", "createdAt": "2020-10-21T18:01:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTc2NTY2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTc2ODkwMQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r505768901", "bodyText": "I believe we need to set requiresDelayedAllocation for this API. Typically we will release the underlying buffer allocated for a request when RequestChannel.Request is constructed. However, since we are using \"zeroCopy,\" we need to hold onto the ByteBuffer reference until the API has been handled.", "author": "hachikuji", "createdAt": "2020-10-15T18:53:21Z", "path": "clients/src/main/java/org/apache/kafka/common/protocol/ApiKeys.java", "diffHunk": "@@ -251,7 +253,8 @@ public Struct parseResponse(short version, ByteBuffer buffer) {\n         DescribeQuorumRequestData.SCHEMAS, DescribeQuorumResponseData.SCHEMAS),\n     ALTER_ISR(56, \"AlterIsr\", AlterIsrRequestData.SCHEMAS, AlterIsrResponseData.SCHEMAS),\n     UPDATE_FEATURES(57, \"UpdateFeatures\",\n-        UpdateFeaturesRequestData.SCHEMAS, UpdateFeaturesResponseData.SCHEMAS);\n+        UpdateFeaturesRequestData.SCHEMAS, UpdateFeaturesResponseData.SCHEMAS),\n+    ENVELOPE(58, \"Envelope\", EnvelopeRequestData.SCHEMAS, EnvelopeResponseData.SCHEMAS);", "originalCommit": "9ce9d905e07cae741b79e265a30c87ea4b1e463b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg1MjkzOQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r505852939", "bodyText": "I think we do have that logic enforced by setting zeroCopy to true for request data field in the RPC json.", "author": "abbccdda", "createdAt": "2020-10-15T21:08:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTc2ODkwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTc5ODIzNA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r505798234", "bodyText": "It seems like we're trying to reuse this handler from the previous patch, but I'm not sure it still makes as much sense. A simpler structure might be something like the following:\n  private def maybeForward(\n    request: RequestChannel.Request,\n    handler: RequestChannel.Request => Unit\n  ): Unit = {\n    if (!controller.isActive && config.redirectionEnabled && request.context.principalSerde.isPresent) {\n      redirectionManager.forwardRequest(sendResponseMaybeThrottle, request)\n    } else {\n      // When IBP is smaller than 2.8 or the principal serde is undefined, forwarding is not supported,\n      // therefore requests are handled directly.\n      handler(request)\n    }\n  }\n\n  // then invoked like this\noverride def handle(request: RequestChannel.Request): Unit = {\n    try {\n      trace(s\"Handling request:${request.requestDesc(true)} from connection ${request.context.connectionId};\" +\n        s\"securityProtocol:${request.context.securityProtocol},principal:${request.context.principal}\")\n\n      request.header.apiKey match {\n      ...\n        case ApiKeys.ALTER_CONFIGS => maybeForward(request, handleAlterConfigsRequest)\n...\n\n\n  // unchanged\n  def handleAlterConfigs(request): Unit", "author": "hachikuji", "createdAt": "2020-10-15T19:47:58Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -121,6 +121,33 @@ class KafkaApis(val requestChannel: RequestChannel,\n   val adminZkClient = new AdminZkClient(zkClient)\n   private val alterAclsPurgatory = new DelayedFuturePurgatory(purgatoryName = \"AlterAcls\", brokerId = config.brokerId)\n \n+  /**\n+   * The template to create a forward request handler.\n+   */\n+  private[server] abstract class ForwardRequestHandler(request: RequestChannel.Request, isForwardRequest: Boolean,", "originalCommit": "9ce9d905e07cae741b79e265a30c87ea4b1e463b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTUzMzk2OA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509533968", "bodyText": "Rather than assuming highest supported version, we should include the version in the serialized data. The simple thing would be to write the version first, then write the payload.", "author": "hachikuji", "createdAt": "2020-10-21T18:06:33Z", "path": "clients/src/main/java/org/apache/kafka/common/security/authenticator/DefaultKafkaPrincipalBuilder.java", "diffHunk": "@@ -167,4 +171,24 @@ public void close() {\n             oldPrincipalBuilder.close();\n     }\n \n+    @Override\n+    public ByteBuffer serialize(KafkaPrincipal principal) {\n+        DefaultPrincipalData data = new DefaultPrincipalData()\n+                                        .setType(principal.getPrincipalType())\n+                                        .setName(principal.getName())\n+                                        .setTokenAuthenticated(principal.tokenAuthenticated());\n+        Struct dataStruct = data.toStruct(DefaultPrincipalData.HIGHEST_SUPPORTED_VERSION);\n+        ByteBuffer buffer = ByteBuffer.allocate(dataStruct.sizeOf());\n+        dataStruct.writeTo(buffer);\n+        buffer.flip();\n+        return buffer;\n+    }\n+\n+    @Override\n+    public KafkaPrincipal deserialize(ByteBuffer bytes) {\n+        DefaultPrincipalData data = new DefaultPrincipalData(\n+            DefaultPrincipalData.SCHEMAS[DefaultPrincipalData.SCHEMAS.length - 1].read(bytes),\n+            DefaultPrincipalData.HIGHEST_SUPPORTED_VERSION);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTUzNzU4Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509537587", "bodyText": "nit: can we move this back to where the request parsing logic is. Otherwise it becomes a bit hidden.", "author": "hachikuji", "createdAt": "2020-10-21T18:09:44Z", "path": "core/src/main/scala/kafka/network/RequestChannel.scala", "diffHunk": "@@ -94,19 +104,60 @@ object RequestChannel extends Logging {\n     @volatile var recordNetworkThreadTimeCallback: Option[Long => Unit] = None\n \n     val session = Session(context.principal, context.clientAddress)\n+\n     private val bodyAndSize: RequestAndSize = context.parseRequest(buffer)\n \n     def header: RequestHeader = context.header\n     def sizeOfBodyInBytes: Int = bodyAndSize.size\n \n-    //most request types are parsed entirely into objects at this point. for those we can release the underlying buffer.\n-    //some (like produce, or any time the schema contains fields of types BYTES or NULLABLE_BYTES) retain a reference\n-    //to the buffer. for those requests we cannot release the buffer early, but only when request processing is done.\n+    def buildResponse(abstractResponse: AbstractResponse,\n+                      error: Errors): Send =\n+      if (envelopeContext.isDefined) {\n+        // Right now only the NOT_CONTROLLER error gets handled by the forwarding\n+        // broker retry. Other errors should be fatal and propagated to the client.\n+        val envelopeError = if (error.equals(Errors.NOT_CONTROLLER))\n+          Errors.NOT_CONTROLLER\n+        else\n+          Errors.NONE\n+\n+        val envelopeResponse = new EnvelopeResponse(\n+          abstractResponse.throttleTimeMs(),\n+          abstractResponse.serializeBody(context.header.apiVersion),\n+          envelopeError\n+        )\n+        envelopeContext.get.brokerContext.buildResponse(envelopeResponse)\n+      } else\n+        context.buildResponse(abstractResponse)\n+\n+    def responseString(response: AbstractResponse): Option[String] =\n+      if (RequestChannel.isRequestLoggingEnabled) {\n+        Some(if (envelopeContext.isDefined)\n+          response.toString(envelopeContext.get.brokerContext.apiVersion)\n+        else\n+          response.toString(context.apiVersion))\n+      } else\n+        None\n+\n+    def headerForLogging(): RequestHeader =\n+      if (envelopeContext.isDefined)\n+        envelopeContext.get.brokerContext.header\n+      else\n+        context.header\n+\n+    // most request types are parsed entirely into objects at this point. for those we can release the underlying buffer.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTUzODU0NQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509538545", "bodyText": "nit: add braces to all of these methods. Even though they are not required, braces make it easier to see the scope", "author": "hachikuji", "createdAt": "2020-10-21T18:10:43Z", "path": "core/src/main/scala/kafka/network/RequestChannel.scala", "diffHunk": "@@ -94,19 +104,60 @@ object RequestChannel extends Logging {\n     @volatile var recordNetworkThreadTimeCallback: Option[Long => Unit] = None\n \n     val session = Session(context.principal, context.clientAddress)\n+\n     private val bodyAndSize: RequestAndSize = context.parseRequest(buffer)\n \n     def header: RequestHeader = context.header\n     def sizeOfBodyInBytes: Int = bodyAndSize.size\n \n-    //most request types are parsed entirely into objects at this point. for those we can release the underlying buffer.\n-    //some (like produce, or any time the schema contains fields of types BYTES or NULLABLE_BYTES) retain a reference\n-    //to the buffer. for those requests we cannot release the buffer early, but only when request processing is done.\n+    def buildResponse(abstractResponse: AbstractResponse,\n+                      error: Errors): Send =", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU0MDYxNw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509540617", "bodyText": "nit: use match", "author": "hachikuji", "createdAt": "2020-10-21T18:12:56Z", "path": "core/src/main/scala/kafka/network/RequestChannel.scala", "diffHunk": "@@ -94,19 +104,60 @@ object RequestChannel extends Logging {\n     @volatile var recordNetworkThreadTimeCallback: Option[Long => Unit] = None\n \n     val session = Session(context.principal, context.clientAddress)\n+\n     private val bodyAndSize: RequestAndSize = context.parseRequest(buffer)\n \n     def header: RequestHeader = context.header\n     def sizeOfBodyInBytes: Int = bodyAndSize.size\n \n-    //most request types are parsed entirely into objects at this point. for those we can release the underlying buffer.\n-    //some (like produce, or any time the schema contains fields of types BYTES or NULLABLE_BYTES) retain a reference\n-    //to the buffer. for those requests we cannot release the buffer early, but only when request processing is done.\n+    def buildResponse(abstractResponse: AbstractResponse,\n+                      error: Errors): Send =\n+      if (envelopeContext.isDefined) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU0MTM3Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509541377", "bodyText": "nit: use match", "author": "hachikuji", "createdAt": "2020-10-21T18:13:43Z", "path": "core/src/main/scala/kafka/network/RequestChannel.scala", "diffHunk": "@@ -250,7 +301,9 @@ object RequestChannel extends Logging {\n     }\n \n     def releaseBuffer(): Unit = {\n-      if (buffer != null) {\n+      if (envelopeContext.isDefined)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU0NTg5Mw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509545893", "bodyText": "nit: this is misaligned. It might be better to pull the body here into a separate method (e.g. parseEnvelopeRequest)", "author": "hachikuji", "createdAt": "2020-10-21T18:19:31Z", "path": "core/src/main/scala/kafka/network/SocketServer.scala", "diffHunk": "@@ -974,8 +973,39 @@ private[kafka] class Processor(val id: Int,\n                 val context = new RequestContext(header, connectionId, channel.socketAddress,\n                   channel.principal, listenerName, securityProtocol,\n                   channel.channelMetadataRegistry.clientInformation, isPrivilegedListener)\n-                val req = new RequestChannel.Request(processor = id, context = context,\n-                  startTimeNanos = nowNanos, memoryPool, receive.payload, requestChannel.metrics)\n+\n+                val principalSerde = Option(channel.principalSerde.orElse(null))\n+                val req =\n+                if (header.apiKey == ApiKeys.ENVELOPE) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU1MDEyNA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509550124", "bodyText": "We should have a check at the beginning of handle to restrict the \"forwardable\" APIs.", "author": "hachikuji", "createdAt": "2020-10-21T18:24:02Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -153,7 +177,7 @@ class KafkaApis(val requestChannel: RequestChannel,\n         case ApiKeys.LIST_GROUPS => handleListGroupsRequest(request)\n         case ApiKeys.SASL_HANDSHAKE => handleSaslHandshakeRequest(request)\n         case ApiKeys.API_VERSIONS => handleApiVersionsRequest(request)\n-        case ApiKeys.CREATE_TOPICS => handleCreateTopicsRequest(request)\n+        case ApiKeys.CREATE_TOPICS => maybeForward(request, handleCreateTopicsRequest)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY2MjkxNA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513662914", "bodyText": "Not sure why this was resolved. I don't see the check. Basically the first thing we should do in handle is check whether we have an envelope request and if it is authorized.", "author": "hachikuji", "createdAt": "2020-10-28T18:14:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU1MDEyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU1MDgwMg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509550802", "bodyText": "We use 'forward' and 'redirect' interchangeably throughout the PR, but the names do suggest different behavior. In my mind 'redirection' suggests that we are telling the client to go somewhere else, while 'forward' suggests that the broker is passing the request through to its destination. So maybe we can stick with 'forward' consistently (e.g. isForwardingEnabled)?", "author": "hachikuji", "createdAt": "2020-10-21T18:24:48Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -126,6 +125,31 @@ class KafkaApis(val requestChannel: RequestChannel,\n     info(\"Shutdown complete.\")\n   }\n \n+  private def maybeForward(request: RequestChannel.Request,\n+                           handler: RequestChannel.Request => Unit): Unit = {\n+    if (request.envelopeContext.isDefined && request.principalSerde.isEmpty) {\n+      sendErrorResponseMaybeThrottle(request, Errors.PRINCIPAL_DESERIALIZATION_FAILURE.exception())\n+    } else if (request.envelopeContext.isDefined &&\n+      (!request.context.fromPrivilegedListener ||\n+      !authorize(request.envelopeContext.get.brokerContext, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME))\n+    ) {\n+      // If the designated forwarding request is not coming from a privileged listener, or\n+      // it fails CLUSTER_ACTION permission, we would fail the authorization.\n+      sendErrorResponseMaybeThrottle(request, Errors.BROKER_AUTHORIZATION_FAILURE.exception())\n+    } else if (request.envelopeContext.isDefined && !controller.isActive) {\n+      sendErrorResponseMaybeThrottle(request, Errors.NOT_CONTROLLER.exception())\n+    } else if (!controller.isActive && couldDoRedirection(request)) {\n+      redirectionManager.forwardRequest(sendResponseMaybeThrottle, request)\n+    } else {\n+      // When IBP is smaller than 2.8 or the principal serde is undefined, forwarding is not supported,\n+      // therefore requests are handled directly.\n+      handler(request)\n+    }\n+  }\n+\n+  private def couldDoRedirection(request: RequestChannel.Request): Boolean =", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY3MDI2OA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509670268", "bodyText": "Sounds good.", "author": "abbccdda", "createdAt": "2020-10-21T20:31:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU1MDgwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU1MzUzMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509553533", "bodyText": "As mentioned above, you can see the rest of the cases in this class where we check CLUSTER_ACTION and they all return CLUSTER_AUTHORIZATION_FAILURE.", "author": "hachikuji", "createdAt": "2020-10-21T18:28:14Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -126,6 +125,31 @@ class KafkaApis(val requestChannel: RequestChannel,\n     info(\"Shutdown complete.\")\n   }\n \n+  private def maybeForward(request: RequestChannel.Request,\n+                           handler: RequestChannel.Request => Unit): Unit = {\n+    if (request.envelopeContext.isDefined && request.principalSerde.isEmpty) {\n+      sendErrorResponseMaybeThrottle(request, Errors.PRINCIPAL_DESERIALIZATION_FAILURE.exception())\n+    } else if (request.envelopeContext.isDefined &&\n+      (!request.context.fromPrivilegedListener ||\n+      !authorize(request.envelopeContext.get.brokerContext, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME))\n+    ) {\n+      // If the designated forwarding request is not coming from a privileged listener, or\n+      // it fails CLUSTER_ACTION permission, we would fail the authorization.\n+      sendErrorResponseMaybeThrottle(request, Errors.BROKER_AUTHORIZATION_FAILURE.exception())", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU1NTE3Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509555177", "bodyText": "nit: you can just use channel.principalSerde.asScala", "author": "hachikuji", "createdAt": "2020-10-21T18:29:41Z", "path": "core/src/main/scala/kafka/network/SocketServer.scala", "diffHunk": "@@ -974,8 +973,39 @@ private[kafka] class Processor(val id: Int,\n                 val context = new RequestContext(header, connectionId, channel.socketAddress,\n                   channel.principal, listenerName, securityProtocol,\n                   channel.channelMetadataRegistry.clientInformation, isPrivilegedListener)\n-                val req = new RequestChannel.Request(processor = id, context = context,\n-                  startTimeNanos = nowNanos, memoryPool, receive.payload, requestChannel.metrics)\n+\n+                val principalSerde = Option(channel.principalSerde.orElse(null))", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY0NTExNQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509645115", "bodyText": "Had a try but it seems java Optional doesn't have an asScala option", "author": "abbccdda", "createdAt": "2020-10-21T20:09:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU1NTE3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDUzMzg5Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r510533897", "bodyText": "You probably need the following:\nimport scala.compat.java8.OptionConverters._", "author": "hachikuji", "createdAt": "2020-10-23T00:37:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU1NTE3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU2MjAxMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509562013", "bodyText": "We want to avoid this serialization since it introduces the possibility for the request to be altered by the forwarding broker. The RequestChannel.Request object retains the reference to the original buffer, which we can use here, but we need to tell the channel to delay releasing the buffer using ApiKeys.requiresDelayedAllocation for all of the \"forwardable\" APIs.", "author": "hachikuji", "createdAt": "2020-10-21T18:35:40Z", "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManagerImpl.scala", "diffHunk": "@@ -129,6 +137,27 @@ class BrokerToControllerChannelManagerImpl(metadataCache: kafka.server.MetadataC\n     requestQueue.put(BrokerToControllerQueueItem(request, callback))\n     requestThread.wakeup()\n   }\n+\n+  override def forwardRequest(responseToOriginalClient: (RequestChannel.Request, Int =>\n+                                AbstractResponse, Option[Send => Unit]) => Unit,\n+                              request: RequestChannel.Request,\n+                              callback: Option[Send => Unit] = Option.empty): Unit = {\n+    val serializedRequestData = request.body[AbstractRequest].serialize(request.header)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU2NTAxOA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509565018", "bodyText": "Use defineInternal", "author": "hachikuji", "createdAt": "2020-10-21T18:38:58Z", "path": "core/src/main/scala/kafka/server/KafkaConfig.scala", "diffHunk": "@@ -1025,6 +1027,7 @@ object KafkaConfig {\n       .define(RequestTimeoutMsProp, INT, Defaults.RequestTimeoutMs, HIGH, RequestTimeoutMsDoc)\n       .define(ConnectionSetupTimeoutMsProp, LONG, Defaults.ConnectionSetupTimeoutMs, MEDIUM, ConnectionSetupTimeoutMsDoc)\n       .define(ConnectionSetupTimeoutMaxMsProp, LONG, Defaults.ConnectionSetupTimeoutMaxMs, MEDIUM, ConnectionSetupTimeoutMaxMsDoc)\n+      .define(quorumBasedControllerProp, BOOLEAN, false, LOW, \"Private configuration for turning on/off KIP-500 mode\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU2NTU1NQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r509565555", "bodyText": "How about enable.metadata.quorum?", "author": "hachikuji", "createdAt": "2020-10-21T18:39:36Z", "path": "core/src/main/scala/kafka/server/KafkaConfig.scala", "diffHunk": "@@ -352,6 +352,8 @@ object KafkaConfig {\n   val RequestTimeoutMsProp = CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG\n   val ConnectionSetupTimeoutMsProp = CommonClientConfigs.SOCKET_CONNECTION_SETUP_TIMEOUT_MS_CONFIG\n   val ConnectionSetupTimeoutMaxMsProp = CommonClientConfigs.SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS_CONFIG\n+  private[server] val quorumBasedControllerProp = \"quorum.based.controller\"", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDUzNjg5NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r510536894", "bodyText": "I was thinking a little bit about this and trying to decide if the envelope request should have a more literal representation of the client ip address. The way it is working right now, it looks like the following:\n\nUse Socket.getInetAddress to populate RequestContext.clientAddress.\nUse InetAddress.getHostName to populate the clientHostName field in the envelope request. This will do a reverse dns lookup based on the IP address from 1).\nNow we send clientHostName over the wire. It gets unpacked here by doing a dns lookup to get to the InetAddress object.\n\nSo it seems we should be skipping the dns translation and just using the IP address from 1). The InetAddress class gives us getAddress and getHostAddress. The first provides the raw byte representation of the ip address, while the latter provides a textual representation. I am thinking we should use getAddress and let this field be represented as bytes. What do you think?", "author": "hachikuji", "createdAt": "2020-10-23T00:49:41Z", "path": "core/src/main/scala/kafka/network/SocketServer.scala", "diffHunk": "@@ -1005,6 +1013,36 @@ private[kafka] class Processor(val id: Int,\n     selector.clearCompletedReceives()\n   }\n \n+  private def parseEnvelopeRequest(receive: NetworkReceive,\n+                                   nowNanos: Long,\n+                                   connectionId: String,\n+                                   context: RequestContext,\n+                                   principalSerde: Option[KafkaPrincipalSerde]) = {\n+    val envelopeRequest = context.parseRequest(receive.payload).request.asInstanceOf[EnvelopeRequest]\n+\n+    val originalHeader = RequestHeader.parse(envelopeRequest.requestData)\n+    // Leave the principal null here is ok since we will fail the request during Kafka API handling.\n+    val originalPrincipal = if (principalSerde.isDefined)\n+      principalSerde.get.deserialize(envelopeRequest.principalData)\n+    else\n+      null\n+\n+    // The forwarding broker and the active controller should have the same DNS resolution, and we need\n+    // to have the original client address for authentication purpose.\n+    val originalClientAddress = InetAddress.getByName(envelopeRequest.clientHostName)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTAxMjkxMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r511012913", "bodyText": "So the proposal is simply for saving the unnecessary dns translation? Not sure if representing as bytes would also serve the security purpose as well.", "author": "abbccdda", "createdAt": "2020-10-23T16:49:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDUzNjg5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDU0MDcyMA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r510540720", "bodyText": "Can we move some of the checks from maybeForward here? This is the flow I'm thinking about:\n\nFirst check authorization => CLUSTER_AUTHORIZATION_FAILURE\nVerify forwarding is enabled => INVALID_REQUEST\nVerify the api is forwardable => INVALID_REQUEST\n\nIf all of these pass, then the request continues down the normal handling path.", "author": "hachikuji", "createdAt": "2020-10-23T01:06:42Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -126,11 +125,44 @@ class KafkaApis(val requestChannel: RequestChannel,\n     info(\"Shutdown complete.\")\n   }\n \n+  private def checkForwarding(request: RequestChannel.Request): Unit = {\n+    if (!request.header.apiKey.forwardable && request.envelopeContext.isDefined) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTAwNzIyOA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r511007228", "bodyText": "For pt2, if the forwarding is not enabled on the active controller, but it has the capability, should we just serve the request?", "author": "abbccdda", "createdAt": "2020-10-23T16:39:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDU0MDcyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY2NzEwMQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513667101", "bodyText": "Unless the internal config is present, I think we should treat the envelope as non-existing. Once we are ready to enable it in the IBP, then we will accept the envelope request even if the local IBP is not high enough.", "author": "hachikuji", "createdAt": "2020-10-28T18:21:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDU0MDcyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDU1MjEyNA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r510552124", "bodyText": "Quotas are one aspect of this work that need more consideration. What we don't want is for the inter-broker channel to get affected by the individual client throttle, which is what will happen with the current patch. What I'd suggest for now is that we allow the broker to track client quotas and pass back the throttle value in the underlying response, but we set the envelope throttle time to 0 and ensure that the inter-broker channel does not get throttled.\nFor this, I think we we will need to change the logic in KafkaApis.sendResponseMaybeThrottle. If it is a forwarded request, we still need to check maybeRecordAndGetThrottleTimeMs, but we can skip the call to ClientQuotaManager.throttle. When the response is received on the forwarding broker, we will need to apply the throttle, which I think the patch already handles.\nOne challenging aspect is how this will affect quota metrics. Currently quota/throttling metrics are relatively simple because they are recorded separately by each broker. However, here the controller is the one that is tracking the throttling for the client across multiple inbound connections from multiple brokers. This means that the broker that is applying a throttle for a forwarded request may not have actually observed a quota violation. Other than causing some reporting confusion, I am not sure whether there are any other consequences to this.\ncc @apovzner @rajinisivaram", "author": "hachikuji", "createdAt": "2020-10-23T01:56:06Z", "path": "core/src/main/scala/kafka/network/RequestChannel.scala", "diffHunk": "@@ -94,19 +104,63 @@ object RequestChannel extends Logging {\n     @volatile var recordNetworkThreadTimeCallback: Option[Long => Unit] = None\n \n     val session = Session(context.principal, context.clientAddress)\n+\n     private val bodyAndSize: RequestAndSize = context.parseRequest(buffer)\n \n     def header: RequestHeader = context.header\n     def sizeOfBodyInBytes: Int = bodyAndSize.size\n \n-    //most request types are parsed entirely into objects at this point. for those we can release the underlying buffer.\n-    //some (like produce, or any time the schema contains fields of types BYTES or NULLABLE_BYTES) retain a reference\n-    //to the buffer. for those requests we cannot release the buffer early, but only when request processing is done.\n+    // most request types are parsed entirely into objects at this point. for those we can release the underlying buffer.\n+    // some (like produce, or any time the schema contains fields of types BYTES or NULLABLE_BYTES) retain a reference\n+    // to the buffer. for those requests we cannot release the buffer early, but only when request processing is done.\n     if (!header.apiKey.requiresDelayedAllocation) {\n       releaseBuffer()\n     }\n \n-    def requestDesc(details: Boolean): String = s\"$header -- ${loggableRequest.toString(details)}\"\n+    def buildResponse(abstractResponse: AbstractResponse,\n+                      error: Errors): Send = {\n+      envelopeContext match {\n+        case Some(envelopeContext) =>\n+          val envelopeResponse = new EnvelopeResponse(\n+            abstractResponse.throttleTimeMs(),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTg3MjAxOA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r511872018", "bodyText": "I guess the only quota that is affected for the RPCs we currently forward is request quotas. Totally agree that we shouldn't throttle inter-broker connections. There are a few other things to consider here:\n\nEvery forwarded request uses network thread and request handler time on two brokers. Are we saying that we can ignore the time spent on the forwarding broker because that is negligible? In a deployment with SSL on the external listener and PLAINTEXT on the inter-broker listener, there may be more network thread time used on the forwarding broker rather than the controller. Do we record these, but use the controller throttle time for throttling?\nAre we changing the semantics of quotas? For example, if a client sends a request1 to leastLoadedNode A which mutes the connection and then sends request2 to leastLoadedNode B that happens to be the controller, we would mute that connection too. Another client with the same principal would get muted on B, but not A because A's quota hasn't been violated. I think this should be ok, though a bit confusing.\nAre these measures good enough to protect the controller? This is the one that needs some more thought. Request quotas are configured to allocate a percentage of thread usage to each principal. Our quotas aren't very good at protecting against DOS attacks, but they help to limit usage for normal clients using the APIs. So if we can make sure the implementation for forwarded requests can handle this case, it would be good enough. In the old world, a client doing a lot of config updates would have just distributed the load across brokers as each node was throttled. Now, we distribute the iniital request across brokers as controller decides to throttle. Total rate for these requests across the cluster is dramatically reduced because all load is now on the controller. But from the controller broker's point of view, we are now allowing more requests through for the same quota from every client because a client can forward through n brokers. @apovzner may have more context on whether these request types actually hit request quotas in real deployments.", "author": "rajinisivaram", "createdAt": "2020-10-26T10:52:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDU1MjEyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDU1NDU4Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r510554587", "bodyText": "One challenge we have here is that there are two levels of errors. The current patch seems to conflate the two, which makes it confusing. I think we need a structure which allows us to separate the errors possible at the envelope level and those possible at the request level. What I'm thinking is this:\n\nFor cluster auth and principal serde errors, we should return the envelope error and null response body.\nFor everything else, we return envelope error NONE and just pass through whatever error is in the response.\n\nDoes that make sense?", "author": "hachikuji", "createdAt": "2020-10-23T02:06:38Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -126,11 +125,44 @@ class KafkaApis(val requestChannel: RequestChannel,\n     info(\"Shutdown complete.\")\n   }\n \n+  private def checkForwarding(request: RequestChannel.Request): Unit = {\n+    if (!request.header.apiKey.forwardable && request.envelopeContext.isDefined) {\n+      throw new IllegalStateException(\"Given RPC \" + request.header.apiKey + \" does not support forwarding.\")\n+    }\n+  }\n+\n+  private def maybeForward(request: RequestChannel.Request,\n+                           handler: RequestChannel.Request => Unit): Unit = {\n+    if (request.envelopeContext.isDefined && request.principalSerde.isEmpty) {\n+      sendErrorResponseMaybeThrottle(request, Errors.PRINCIPAL_DESERIALIZATION_FAILURE.exception())\n+    } else if (request.envelopeContext.isDefined &&\n+      (!request.context.fromPrivilegedListener ||\n+      !authorize(request.envelopeContext.get.brokerContext, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME))\n+    ) {\n+      // If the designated forwarding request is not coming from a privileged listener, or\n+      // it fails CLUSTER_ACTION permission, we would fail the authorization.\n+      sendErrorResponseMaybeThrottle(request, Errors.CLUSTER_AUTHORIZATION_FAILED.exception())", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDU2ODcwNg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r510568706", "bodyText": "The question would be how the forwarding broker should do the error handling for auth & principal serde exceptions. To me we should get a vanilla error response with UNKNOWN_SERVER_ERROR and get back to the original client? Besides that, I think we could add a differentiation here to avoid passing the serde-type errors to the client.", "author": "abbccdda", "createdAt": "2020-10-23T03:06:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDU1NDU4Nw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYwODMxOQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513608319", "bodyText": "Hmm.. It looks like we do not serialize the response header, but I think we probably should. Today it only includes the correlationId, but who knows how it will evolve in the future? Since we do serialize the request header, it seems better to be consistent.", "author": "hachikuji", "createdAt": "2020-10-28T16:56:18Z", "path": "core/src/main/scala/kafka/network/RequestChannel.scala", "diffHunk": "@@ -94,19 +104,63 @@ object RequestChannel extends Logging {\n     @volatile var recordNetworkThreadTimeCallback: Option[Long => Unit] = None\n \n     val session = Session(context.principal, context.clientAddress)\n+\n     private val bodyAndSize: RequestAndSize = context.parseRequest(buffer)\n \n     def header: RequestHeader = context.header\n     def sizeOfBodyInBytes: Int = bodyAndSize.size\n \n-    //most request types are parsed entirely into objects at this point. for those we can release the underlying buffer.\n-    //some (like produce, or any time the schema contains fields of types BYTES or NULLABLE_BYTES) retain a reference\n-    //to the buffer. for those requests we cannot release the buffer early, but only when request processing is done.\n+    // most request types are parsed entirely into objects at this point. for those we can release the underlying buffer.\n+    // some (like produce, or any time the schema contains fields of types BYTES or NULLABLE_BYTES) retain a reference\n+    // to the buffer. for those requests we cannot release the buffer early, but only when request processing is done.\n     if (!header.apiKey.requiresDelayedAllocation) {\n       releaseBuffer()\n     }\n \n-    def requestDesc(details: Boolean): String = s\"$header -- ${loggableRequest.toString(details)}\"\n+    def buildResponse(abstractResponse: AbstractResponse,\n+                      error: Errors): Send = {\n+      envelopeContext match {\n+        case Some(envelopeContext) =>\n+          val envelopeResponse = new EnvelopeResponse(\n+            abstractResponse.throttleTimeMs(),\n+            abstractResponse.serializeBody(context.header.apiVersion),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY0NzIzNw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513647237", "bodyText": "In fact, the schema doc says that the response header should be included.", "author": "hachikuji", "createdAt": "2020-10-28T17:50:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYwODMxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYxMjkzNQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513612935", "bodyText": "Since this is a public API, it's worth documenting that these apis should raise a consistent error, such as SerializationException, in case of an error.", "author": "hachikuji", "createdAt": "2020-10-28T17:02:38Z", "path": "clients/src/main/java/org/apache/kafka/common/security/auth/KafkaPrincipalSerde.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.security.auth;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Serializer/Deserializer interface for {@link KafkaPrincipal} for the forwarding purpose.\n+ */\n+public interface KafkaPrincipalSerde {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYxMzQzNQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513613435", "bodyText": "nit: for the the purpose of inter-broker forwarding", "author": "hachikuji", "createdAt": "2020-10-28T17:03:23Z", "path": "clients/src/main/java/org/apache/kafka/common/security/auth/KafkaPrincipalSerde.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.security.auth;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Serializer/Deserializer interface for {@link KafkaPrincipal} for the forwarding purpose.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYxNDI2Mg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513614262", "bodyText": "We may as well add a check here for the version so that we get a useful error in case we receive a version that we do not support.", "author": "hachikuji", "createdAt": "2020-10-28T17:04:30Z", "path": "clients/src/main/java/org/apache/kafka/common/security/authenticator/DefaultKafkaPrincipalBuilder.java", "diffHunk": "@@ -167,4 +171,26 @@ public void close() {\n             oldPrincipalBuilder.close();\n     }\n \n+    @Override\n+    public ByteBuffer serialize(KafkaPrincipal principal) {\n+        DefaultPrincipalData data = new DefaultPrincipalData()\n+                                        .setType(principal.getPrincipalType())\n+                                        .setName(principal.getName())\n+                                        .setTokenAuthenticated(principal.tokenAuthenticated());\n+        Struct dataStruct = data.toStruct(DefaultPrincipalData.HIGHEST_SUPPORTED_VERSION);\n+        ByteBuffer buffer = ByteBuffer.allocate(2 + dataStruct.sizeOf());\n+        buffer.putShort(DefaultPrincipalData.HIGHEST_SUPPORTED_VERSION);\n+        dataStruct.writeTo(buffer);\n+        buffer.flip();\n+        return buffer;\n+    }\n+\n+    @Override\n+    public KafkaPrincipal deserialize(ByteBuffer bytes) {\n+        short version = bytes.getShort();\n+        DefaultPrincipalData data = new DefaultPrincipalData(\n+            DefaultPrincipalData.SCHEMAS[version].read(bytes),", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYxNTkxNg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513615916", "bodyText": "nit: use upper-case TokenAuthenticated for consistency with other fields", "author": "hachikuji", "createdAt": "2020-10-28T17:06:37Z", "path": "clients/src/main/resources/common/message/DefaultPrincipalData.json", "diffHunk": "@@ -0,0 +1,30 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"type\": \"data\",\n+  \"name\": \"DefaultPrincipalData\",\n+  // The encoding format for default Kafka principal.\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    {\"name\": \"Type\", \"type\": \"string\", \"versions\": \"0+\",\n+      \"about\": \"The principal type\"},\n+    {\"name\": \"Name\", \"type\": \"string\", \"versions\": \"0+\",\n+      \"about\": \"The principal name\"},\n+    {\"name\": \"tokenAuthenticated\", \"type\": \"bool\", \"versions\": \"0+\",", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYxNjUwMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513616503", "bodyText": "Might be worth mentioning org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder explicitly.", "author": "hachikuji", "createdAt": "2020-10-28T17:07:26Z", "path": "clients/src/main/resources/common/message/DefaultPrincipalData.json", "diffHunk": "@@ -0,0 +1,30 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"type\": \"data\",\n+  \"name\": \"DefaultPrincipalData\",\n+  // The encoding format for default Kafka principal.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYxNzMwMA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513617300", "bodyText": "Perhaps add a little more detail?\n\nWhether the principal was authenticated by a delegation token on the forwarding broker", "author": "hachikuji", "createdAt": "2020-10-28T17:08:33Z", "path": "clients/src/main/resources/common/message/DefaultPrincipalData.json", "diffHunk": "@@ -0,0 +1,30 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"type\": \"data\",\n+  \"name\": \"DefaultPrincipalData\",\n+  // The encoding format for default Kafka principal.\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    {\"name\": \"Type\", \"type\": \"string\", \"versions\": \"0+\",\n+      \"about\": \"The principal type\"},\n+    {\"name\": \"Name\", \"type\": \"string\", \"versions\": \"0+\",\n+      \"about\": \"The principal name\"},\n+    {\"name\": \"tokenAuthenticated\", \"type\": \"bool\", \"versions\": \"0+\",\n+      \"about\": \"Whether the given principal is token authenticated.\"}", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYxOTY1Ng==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513619656", "bodyText": "Since principals should be small, it is tempting to just use simple byte arrays for this interface. This is typically simpler for users and gives us a stronger boundary between plugin and broker code.", "author": "hachikuji", "createdAt": "2020-10-28T17:11:52Z", "path": "clients/src/main/java/org/apache/kafka/common/security/auth/KafkaPrincipalSerde.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.security.auth;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Serializer/Deserializer interface for {@link KafkaPrincipal} for the forwarding purpose.\n+ */\n+public interface KafkaPrincipalSerde {\n+\n+    ByteBuffer serialize(KafkaPrincipal principal);\n+\n+    KafkaPrincipal deserialize(ByteBuffer bytes);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzc1MDE0MQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513750141", "bodyText": "I was under the impression that byte buffer provides more information such as a read position and capacity/limits, which makes the deserialization easier. If given a byte[], I'm afraid they need to convert to byte buffer internally eventually.", "author": "abbccdda", "createdAt": "2020-10-28T20:48:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYxOTY1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM3ODQ1OQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516378459", "bodyText": "I was under the impression that byte buffer provides more information such as a read position and capacity/limits, which makes the deserialization easier.\n\nHmm, not sure I get your point. Nothing is simpler than a byte array. The main question is whether we want to expose the actual request buffer to the plugin, especially since we still plan on using it afterwards. The plugin is treated as a trusted component in any case, so it might not make a big difference. Probably we should optimize here for simplicity.\n\nIf given a byte[], I'm afraid they need to convert to byte buffer internally eventually.\n\nThat may or may not be true. If it is, users can just use ByteBuffer.wrap.", "author": "hachikuji", "createdAt": "2020-11-03T01:09:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYxOTY1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYzNTg0Mw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513635843", "bodyText": "It looks like these changes made it to 2.7. We need to revert them before the release or it will not be safe to remove them. The danger is that we might use these tag ids for another purpose in the future, which will break the request parsing.", "author": "hachikuji", "createdAt": "2020-10-28T17:34:28Z", "path": "clients/src/main/resources/common/message/RequestHeader.json", "diffHunk": "@@ -37,12 +37,6 @@\n     // Since the client is sending the ApiVersionsRequest in order to discover what\n     // versions are supported, the client does not know the best version to use.\n     { \"name\": \"ClientId\", \"type\": \"string\", \"versions\": \"1+\", \"nullableVersions\": \"1+\", \"ignorable\": true,\n-      \"flexibleVersions\": \"none\", \"about\": \"The client ID string.\" },\n-    { \"name\": \"InitialPrincipalName\", \"type\": \"string\", \"tag\": 0, \"taggedVersions\": \"2+\",", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzgyNjQ3MQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513826471", "bodyText": "Sg, will initiate a PR for that.", "author": "abbccdda", "createdAt": "2020-10-28T23:44:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYzNTg0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY0NTQ3NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513645474", "bodyText": "I guess this shows an inconsistency between the envelope and the other inter-broker APIs. The throttle time field is only useful if we actually expect the forwarding broker to respect it and backoff. I wonder if we should just be consistent for now and leave this out.", "author": "hachikuji", "createdAt": "2020-10-28T17:48:22Z", "path": "clients/src/test/java/org/apache/kafka/common/protocol/ApiKeysTest.java", "diffHunk": "@@ -59,7 +59,8 @@ public void testResponseThrottleTime() {\n         for (ApiKeys apiKey: ApiKeys.values()) {\n             Schema responseSchema = apiKey.responseSchema(apiKey.latestVersion());\n             BoundField throttleTimeField = responseSchema.get(CommonFields.THROTTLE_TIME_MS.name);\n-            if (apiKey.clusterAction || authenticationKeys.contains(apiKey))\n+            // Envelope could be throttled, even though it requires cluster action.\n+            if (apiKey != ApiKeys.ENVELOPE && (apiKey.clusterAction || authenticationKeys.contains(apiKey)))", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzc1Nzk3OA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513757978", "bodyText": "Sounds good, will remove the throttle time field from the Envelope", "author": "abbccdda", "createdAt": "2020-10-28T21:02:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY0NTQ3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY0Njc2OQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513646769", "bodyText": "Would it make sense to add a default rule? If the api is forwardable, then we can assert it requires delayed deallocation.", "author": "hachikuji", "createdAt": "2020-10-28T17:50:11Z", "path": "clients/src/test/java/org/apache/kafka/common/protocol/ProtoUtilsTest.java", "diffHunk": "@@ -34,6 +34,11 @@ public void testDelayedAllocationSchemaDetection() throws Exception {\n                 case EXPIRE_DELEGATION_TOKEN:\n                 case RENEW_DELEGATION_TOKEN:\n                 case ALTER_USER_SCRAM_CREDENTIALS:\n+                case ENVELOPE:\n+                case ALTER_CONFIGS:\n+                case INCREMENTAL_ALTER_CONFIGS:\n+                case ALTER_CLIENT_QUOTAS:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY0ODEyMg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513648122", "bodyText": "It's not clear to me why we need to do this now since we are not enabling forwarding yet.", "author": "hachikuji", "createdAt": "2020-10-28T17:52:10Z", "path": "core/src/main/scala/kafka/api/ApiVersion.scala", "diffHunk": "@@ -104,7 +108,9 @@ object ApiVersion {\n     // Bup Fetch protocol for Raft protocol (KIP-595)\n     KAFKA_2_7_IV1,\n     // Introduced AlterIsr (KIP-497)\n-    KAFKA_2_7_IV2\n+    KAFKA_2_7_IV2,\n+    // Introduced IBP based constraints for ApiVersion (KIP-590)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzc0MzQ3Mw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513743473", "bodyText": "I think it's ok to remove this flag for now.", "author": "abbccdda", "createdAt": "2020-10-28T20:35:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY0ODEyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY1OTg4Mw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513659883", "bodyText": "Hmm.. The request logging will not be too useful if we cannot see what is in the embedded request and response. I think we should print the envelope structures separately. Longer term, we should figure out how to incorporate the envelope into https://cwiki.apache.org/confluence/display/KAFKA/KIP-673%3A+Emit+JSONs+with+new+auto-generated+schema.", "author": "hachikuji", "createdAt": "2020-10-28T18:09:37Z", "path": "core/src/main/scala/kafka/network/RequestChannel.scala", "diffHunk": "@@ -94,19 +104,63 @@ object RequestChannel extends Logging {\n     @volatile var recordNetworkThreadTimeCallback: Option[Long => Unit] = None\n \n     val session = Session(context.principal, context.clientAddress)\n+\n     private val bodyAndSize: RequestAndSize = context.parseRequest(buffer)\n \n     def header: RequestHeader = context.header\n     def sizeOfBodyInBytes: Int = bodyAndSize.size\n \n-    //most request types are parsed entirely into objects at this point. for those we can release the underlying buffer.\n-    //some (like produce, or any time the schema contains fields of types BYTES or NULLABLE_BYTES) retain a reference\n-    //to the buffer. for those requests we cannot release the buffer early, but only when request processing is done.\n+    // most request types are parsed entirely into objects at this point. for those we can release the underlying buffer.\n+    // some (like produce, or any time the schema contains fields of types BYTES or NULLABLE_BYTES) retain a reference\n+    // to the buffer. for those requests we cannot release the buffer early, but only when request processing is done.\n     if (!header.apiKey.requiresDelayedAllocation) {\n       releaseBuffer()\n     }\n \n-    def requestDesc(details: Boolean): String = s\"$header -- ${loggableRequest.toString(details)}\"\n+    def buildResponse(abstractResponse: AbstractResponse,\n+                      error: Errors): Send = {\n+      envelopeContext match {\n+        case Some(envelopeContext) =>\n+          val envelopeResponse = new EnvelopeResponse(\n+            abstractResponse.throttleTimeMs(),\n+            abstractResponse.serializeBody(context.header.apiVersion),\n+            error\n+          )\n+\n+          envelopeContext.brokerContext.buildResponse(envelopeResponse)\n+        case None =>\n+          context.buildResponse(abstractResponse)\n+      }\n+    }\n+\n+    def responseString(response: AbstractResponse): Option[String] = {\n+      if (RequestChannel.isRequestLoggingEnabled)\n+        Some(envelopeContext match {\n+          case Some(envelopeContext) =>\n+            response.toString(envelopeContext.brokerContext.apiVersion)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzc1OTgwMg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r513759802", "bodyText": "Sg, but I guess we need to keep it as is for now to try using the correct api version.", "author": "abbccdda", "createdAt": "2020-10-28T21:05:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY1OTg4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxODUwMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515418503", "bodyText": "Not sure I follow the point about the correct api version.", "author": "hachikuji", "createdAt": "2020-10-30T23:08:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY1OTg4Mw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "23381446f23ad627b8bff393f5df78d08c9ea955", "url": "https://github.com/apache/kafka/commit/23381446f23ad627b8bff393f5df78d08c9ea955", "message": "redirection with Envelope request", "committedDate": "2020-10-29T06:16:22Z", "type": "commit"}, {"oid": "256ac60a6629e4b25dfeb2723f4b049a0d00aa3f", "url": "https://github.com/apache/kafka/commit/256ac60a6629e4b25dfeb2723f4b049a0d00aa3f", "message": "address Jason's comments", "committedDate": "2020-10-29T06:16:23Z", "type": "commit"}, {"oid": "498c0f9b376f3bef5be7a7898ec2011d91875064", "url": "https://github.com/apache/kafka/commit/498c0f9b376f3bef5be7a7898ec2011d91875064", "message": "rebase", "committedDate": "2020-10-29T06:40:43Z", "type": "commit"}, {"oid": "498c0f9b376f3bef5be7a7898ec2011d91875064", "url": "https://github.com/apache/kafka/commit/498c0f9b376f3bef5be7a7898ec2011d91875064", "message": "rebase", "committedDate": "2020-10-29T06:40:43Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxMjQ0OQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515212449", "bodyText": "Probably the first thing we should check is isForwardingEnabled. If it is not, I suggest we close the connection, which is basically the broker's way of saying \"I don't know how to handle this.\"", "author": "hachikuji", "createdAt": "2020-10-30T16:11:00Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -126,13 +125,46 @@ class KafkaApis(val requestChannel: RequestChannel,\n     info(\"Shutdown complete.\")\n   }\n \n+  private def validateForwardRequest(request: RequestChannel.Request): Unit = {\n+    if (!request.context.fromPrivilegedListener ||", "originalCommit": "498c0f9b376f3bef5be7a7898ec2011d91875064", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxNTM2NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515215364", "bodyText": "Can we add a description explaining what this is for?", "author": "hachikuji", "createdAt": "2020-10-30T16:16:02Z", "path": "core/src/main/scala/kafka/server/KafkaConfig.scala", "diffHunk": "@@ -1025,6 +1027,7 @@ object KafkaConfig {\n       .define(RequestTimeoutMsProp, INT, Defaults.RequestTimeoutMs, HIGH, RequestTimeoutMsDoc)\n       .define(ConnectionSetupTimeoutMsProp, LONG, Defaults.ConnectionSetupTimeoutMs, MEDIUM, ConnectionSetupTimeoutMsDoc)\n       .define(ConnectionSetupTimeoutMaxMsProp, LONG, Defaults.ConnectionSetupTimeoutMaxMs, MEDIUM, ConnectionSetupTimeoutMaxMsDoc)\n+      .defineInternal(enableMetadataQuorumProp, BOOLEAN, false, LOW)", "originalCommit": "498c0f9b376f3bef5be7a7898ec2011d91875064", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxOTE5OA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515219198", "bodyText": "We should duplicate the buffer instead of modifying it directly.", "author": "hachikuji", "createdAt": "2020-10-30T16:22:25Z", "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManagerImpl.scala", "diffHunk": "@@ -129,6 +137,31 @@ class BrokerToControllerChannelManagerImpl(metadataCache: kafka.server.MetadataC\n     requestQueue.put(BrokerToControllerQueueItem(request, callback))\n     requestThread.wakeup()\n   }\n+\n+  override def forwardRequest(responseToOriginalClient: (RequestChannel.Request, Int =>\n+                                AbstractResponse, Option[Send => Unit]) => Unit,\n+                              request: RequestChannel.Request,\n+                              callback: Option[Send => Unit] = Option.empty): Unit = {\n+    val serializedPrincipal = request.principalSerde.get.serialize(request.context.principal)\n+    request.buffer.flip", "originalCommit": "498c0f9b376f3bef5be7a7898ec2011d91875064", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxOTcyNg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515219726", "bodyText": "We can leave this for a follow-up, but it would be nice if we could avoid this deserialization (and the subsequent re-serialization).", "author": "hachikuji", "createdAt": "2020-10-30T16:23:22Z", "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManagerImpl.scala", "diffHunk": "@@ -129,6 +137,31 @@ class BrokerToControllerChannelManagerImpl(metadataCache: kafka.server.MetadataC\n     requestQueue.put(BrokerToControllerQueueItem(request, callback))\n     requestThread.wakeup()\n   }\n+\n+  override def forwardRequest(responseToOriginalClient: (RequestChannel.Request, Int =>\n+                                AbstractResponse, Option[Send => Unit]) => Unit,\n+                              request: RequestChannel.Request,\n+                              callback: Option[Send => Unit] = Option.empty): Unit = {\n+    val serializedPrincipal = request.principalSerde.get.serialize(request.context.principal)\n+    request.buffer.flip\n+    val envelopeRequest = new EnvelopeRequest.Builder(\n+      request.buffer,\n+      serializedPrincipal,\n+      request.context.clientAddress.getAddress\n+    )\n+\n+    requestQueue.put(BrokerToControllerQueueItem(envelopeRequest,\n+      (response: ClientResponse) => responseToOriginalClient(\n+        request, _ => {\n+          val envelopeResponse = response.responseBody.asInstanceOf[EnvelopeResponse]\n+          if (envelopeResponse.error() != Errors.NONE) {\n+            request.body[AbstractRequest].getErrorResponse(Errors.UNKNOWN_SERVER_ERROR.exception())\n+          } else {\n+            AbstractResponse.deserializeBody(envelopeResponse.embedResponseData, request.header)", "originalCommit": "498c0f9b376f3bef5be7a7898ec2011d91875064", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM4OTA2OA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515389068", "bodyText": "Filed: https://issues.apache.org/jira/browse/KAFKA-10668", "author": "abbccdda", "createdAt": "2020-10-30T21:23:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxOTcyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIyMDQ4MA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515220480", "bodyText": "Probably useful to explain why we do this. A debug log message with the original error would be helpful as well.", "author": "hachikuji", "createdAt": "2020-10-30T16:24:37Z", "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManagerImpl.scala", "diffHunk": "@@ -129,6 +137,31 @@ class BrokerToControllerChannelManagerImpl(metadataCache: kafka.server.MetadataC\n     requestQueue.put(BrokerToControllerQueueItem(request, callback))\n     requestThread.wakeup()\n   }\n+\n+  override def forwardRequest(responseToOriginalClient: (RequestChannel.Request, Int =>\n+                                AbstractResponse, Option[Send => Unit]) => Unit,\n+                              request: RequestChannel.Request,\n+                              callback: Option[Send => Unit] = Option.empty): Unit = {\n+    val serializedPrincipal = request.principalSerde.get.serialize(request.context.principal)\n+    request.buffer.flip\n+    val envelopeRequest = new EnvelopeRequest.Builder(\n+      request.buffer,\n+      serializedPrincipal,\n+      request.context.clientAddress.getAddress\n+    )\n+\n+    requestQueue.put(BrokerToControllerQueueItem(envelopeRequest,\n+      (response: ClientResponse) => responseToOriginalClient(\n+        request, _ => {\n+          val envelopeResponse = response.responseBody.asInstanceOf[EnvelopeResponse]\n+          if (envelopeResponse.error() != Errors.NONE) {\n+            request.body[AbstractRequest].getErrorResponse(Errors.UNKNOWN_SERVER_ERROR.exception())", "originalCommit": "498c0f9b376f3bef5be7a7898ec2011d91875064", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIyOTE4MA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515229180", "bodyText": "I think this was one of my initial questions, but do we have a timeout for the request? Looking at the current logic in handleResponse, it seems like we will just retry indefinitely. That is probably what we want for requests generated by the broker (e.g. AlterIsr), but it is not so useful for client requests since the client itself will eventually give up and send a new request.", "author": "hachikuji", "createdAt": "2020-10-30T16:39:03Z", "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManagerImpl.scala", "diffHunk": "@@ -129,6 +137,31 @@ class BrokerToControllerChannelManagerImpl(metadataCache: kafka.server.MetadataC\n     requestQueue.put(BrokerToControllerQueueItem(request, callback))\n     requestThread.wakeup()\n   }\n+\n+  override def forwardRequest(responseToOriginalClient: (RequestChannel.Request, Int =>\n+                                AbstractResponse, Option[Send => Unit]) => Unit,\n+                              request: RequestChannel.Request,\n+                              callback: Option[Send => Unit] = Option.empty): Unit = {\n+    val serializedPrincipal = request.principalSerde.get.serialize(request.context.principal)\n+    request.buffer.flip\n+    val envelopeRequest = new EnvelopeRequest.Builder(\n+      request.buffer,\n+      serializedPrincipal,\n+      request.context.clientAddress.getAddress\n+    )\n+\n+    requestQueue.put(BrokerToControllerQueueItem(envelopeRequest,", "originalCommit": "498c0f9b376f3bef5be7a7898ec2011d91875064", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM4ODI1OA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515388258", "bodyText": "Yea, I think this could be done as a follow-up. Filed: https://issues.apache.org/jira/browse/KAFKA-10667", "author": "abbccdda", "createdAt": "2020-10-30T21:20:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIyOTE4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIzMDAyMA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515230020", "bodyText": "nit: can we create a helper for request.envelopeContext.isEmpty? Perhaps we can write this as !request.isForwarded?", "author": "hachikuji", "createdAt": "2020-10-30T16:40:21Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -3271,13 +3299,17 @@ class KafkaApis(val requestChannel: RequestChannel,\n                                         createResponse: Int => AbstractResponse,\n                                         onComplete: Option[Send => Unit] = None): Unit = {\n     val throttleTimeMs = maybeRecordAndGetThrottleTimeMs(request)\n-    quotas.request.throttle(request, throttleTimeMs, requestChannel.sendResponse)\n+    // Only throttle non-forwarded requests\n+    if (request.envelopeContext.isEmpty)\n+      quotas.request.throttle(request, throttleTimeMs, requestChannel.sendResponse)\n     sendResponse(request, Some(createResponse(throttleTimeMs)), onComplete)\n   }\n \n   private def sendErrorResponseMaybeThrottle(request: RequestChannel.Request, error: Throwable): Unit = {\n     val throttleTimeMs = maybeRecordAndGetThrottleTimeMs(request)\n-    quotas.request.throttle(request, throttleTimeMs, requestChannel.sendResponse)\n+    // Only throttle non-forwarded requests or cluster authorization failures\n+    if (error.isInstanceOf[ClusterAuthorizationException] || request.envelopeContext.isEmpty)", "originalCommit": "498c0f9b376f3bef5be7a7898ec2011d91875064", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM3NzQ1Mw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515377453", "bodyText": "Sg", "author": "abbccdda", "createdAt": "2020-10-30T20:51:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIzMDAyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIzMTAyMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515231023", "bodyText": "Hmm.. I had assumed we would be using the same channel manager. Can you explain why we need two? In the end, I think all of the requests get serialized on the controller, so I'm not sure we're buying much.", "author": "hachikuji", "createdAt": "2020-10-30T16:42:03Z", "path": "core/src/main/scala/kafka/server/KafkaServer.scala", "diffHunk": "@@ -322,6 +325,12 @@ class KafkaServer(val config: KafkaConfig, time: Time = Time.SYSTEM, threadNameP\n         kafkaController = new KafkaController(config, zkClient, time, metrics, brokerInfo, brokerEpoch, tokenManager, brokerFeatures, featureCache, threadNamePrefix)\n         kafkaController.startup()\n \n+        if (config.forwardingEnabled) {\n+          /* start forwarding manager */\n+          forwardingManager = new BrokerToControllerChannelManagerImpl(metadataCache, time, metrics, config, \"forwardingChannel\", threadNamePrefix)", "originalCommit": "498c0f9b376f3bef5be7a7898ec2011d91875064", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTI3NjczOA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515276738", "bodyText": "I agree we don't have a prioritization system on the controller yet, but in long term having two separate managers mean we don't block AlterISR unnecessarily, which seems to be definitely a higher priority message. cc @mumrah @cmccabe", "author": "abbccdda", "createdAt": "2020-10-30T17:48:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIzMTAyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM5NTQwMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515395403", "bodyText": "Got a follow-up ticket as well: https://issues.apache.org/jira/browse/KAFKA-10348", "author": "abbccdda", "createdAt": "2020-10-30T21:41:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIzMTAyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTI0NTY2NQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515245665", "bodyText": "As far as I can tell, the callback here is unused. Tracing this back to KafkaApis, the callback passed to sendResponseMaybeThrottle also appears to be unused. I think we can remove it from both APIs and simplify this a bit.", "author": "hachikuji", "createdAt": "2020-10-30T17:04:46Z", "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManagerImpl.scala", "diffHunk": "@@ -129,6 +137,31 @@ class BrokerToControllerChannelManagerImpl(metadataCache: kafka.server.MetadataC\n     requestQueue.put(BrokerToControllerQueueItem(request, callback))\n     requestThread.wakeup()\n   }\n+\n+  override def forwardRequest(responseToOriginalClient: (RequestChannel.Request, Int =>\n+                                AbstractResponse, Option[Send => Unit]) => Unit,\n+                              request: RequestChannel.Request,\n+                              callback: Option[Send => Unit] = Option.empty): Unit = {", "originalCommit": "498c0f9b376f3bef5be7a7898ec2011d91875064", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8c27f126e6d955a63aeca98d653bba312f35c985", "url": "https://github.com/apache/kafka/commit/8c27f126e6d955a63aeca98d653bba312f35c985", "message": "address more comments", "committedDate": "2020-10-30T21:46:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxMzE3Mw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515413173", "bodyText": "Can we use closeConnection. We do not want to even acknowledge that the api exists unless forwarding is enabled.", "author": "hachikuji", "createdAt": "2020-10-30T22:44:52Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -126,7 +126,9 @@ class KafkaApis(val requestChannel: RequestChannel,\n   }\n \n   private def validateForwardRequest(request: RequestChannel.Request): Unit = {\n-    if (!request.context.fromPrivilegedListener ||\n+    if (!config.forwardingEnabled) {\n+      throw new InvalidRequestException(s\"The handling of forwarding request $request is not enabled on this broker.\")", "originalCommit": "8c27f126e6d955a63aeca98d653bba312f35c985", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyMTU1Mw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515421553", "bodyText": "It's a bit tricky to do it here since we rely on exception catching to skip all the rest of handling logic, not sure it is worth to add this special case and do if-else to incur a large code change.", "author": "abbccdda", "createdAt": "2020-10-30T23:23:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxMzE3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyODM4NQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515428385", "bodyText": "Perhaps we could return a boolean to indicate whether the handling logic should execute. I think it is important to avoid exposing this api until we're ready for it.", "author": "hachikuji", "createdAt": "2020-10-31T00:00:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxMzE3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxNDUwMA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515414500", "bodyText": "nit: drop parenthesis for simple getter", "author": "hachikuji", "createdAt": "2020-10-30T22:50:24Z", "path": "core/src/main/scala/kafka/network/RequestChannel.scala", "diffHunk": "@@ -117,6 +117,8 @@ object RequestChannel extends Logging {\n       releaseBuffer()\n     }\n \n+    def isForwarded(): Boolean = envelopeContext.isDefined", "originalCommit": "8c27f126e6d955a63aeca98d653bba312f35c985", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxNzc4Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515417787", "bodyText": "This begs the question whether the api should even be advertised from non-privileged listeners if users cannot access it. I am thinking we can make this case similar to the behavior if forwarding is not enabled.  Here we can use this logic:\nif (!config.forwardingEnabled || !request.context.fromPrivilegedListener) {\n  closeConnection(request, util.Collections.emptyMap())\n} else if (!authorize(request.envelopeContext.get.brokerContext, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME)) {\n  throw new ClusterAuthorizationException(s\"Envelope request $request is not authorized\")\n} \n\nSimilarly, we can change the check in ApiVersion.apiVersionsResponse so that it skips the ENVELOPE API if the request is not from a privileged listener.", "author": "hachikuji", "createdAt": "2020-10-30T23:05:00Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -126,13 +125,48 @@ class KafkaApis(val requestChannel: RequestChannel,\n     info(\"Shutdown complete.\")\n   }\n \n+  private def validateForwardRequest(request: RequestChannel.Request): Unit = {\n+    if (!config.forwardingEnabled) {\n+      throw new InvalidRequestException(s\"The handling of forwarding request $request is not enabled on this broker.\")\n+    } else if (!request.context.fromPrivilegedListener ||", "originalCommit": "8c27f126e6d955a63aeca98d653bba312f35c985", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyMTE4OQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515421189", "bodyText": "I think this logic still conflates the envelope error and the inner response error. We might catch an exception raised from validateForwardRequest or from the request handler in KafkaApis.handle. Both paths flow through KafkaApis.handleError, so we do not have a way to distinguish the two cases. This means that an uncaught error from the underlying request will get sent back to the forwarded broker as an error in the envelope, which will cause us to translate it to UNKNOWN_SERVER_ERROR.\nI think we should handle envelope errors explicitly through a separate method. We can define a method here such as buildFailedEnvelope which can be used inside validateForwardRequest. Then inside buildResponse here, we can always return Errors.NONE as the envelope error.", "author": "hachikuji", "createdAt": "2020-10-30T23:21:24Z", "path": "core/src/main/scala/kafka/network/RequestChannel.scala", "diffHunk": "@@ -94,19 +104,64 @@ object RequestChannel extends Logging {\n     @volatile var recordNetworkThreadTimeCallback: Option[Long => Unit] = None\n \n     val session = Session(context.principal, context.clientAddress)\n+\n     private val bodyAndSize: RequestAndSize = context.parseRequest(buffer)\n \n     def header: RequestHeader = context.header\n     def sizeOfBodyInBytes: Int = bodyAndSize.size\n \n-    //most request types are parsed entirely into objects at this point. for those we can release the underlying buffer.\n-    //some (like produce, or any time the schema contains fields of types BYTES or NULLABLE_BYTES) retain a reference\n-    //to the buffer. for those requests we cannot release the buffer early, but only when request processing is done.\n+    // most request types are parsed entirely into objects at this point. for those we can release the underlying buffer.\n+    // some (like produce, or any time the schema contains fields of types BYTES or NULLABLE_BYTES) retain a reference\n+    // to the buffer. for those requests we cannot release the buffer early, but only when request processing is done.\n     if (!header.apiKey.requiresDelayedAllocation) {\n       releaseBuffer()\n     }\n \n-    def requestDesc(details: Boolean): String = s\"$header -- ${loggableRequest.toString(details)}\"\n+    def isForwarded(): Boolean = envelopeContext.isDefined\n+\n+    def buildResponse(abstractResponse: AbstractResponse,\n+                      error: Errors): Send = {", "originalCommit": "8c27f126e6d955a63aeca98d653bba312f35c985", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyNzkxMg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r515427912", "bodyText": "Perhaps it is obvious, but this logic does not give us any tight guarantees that the request will actually be handled by the broker that is currently the controller. For example, a new controller might get elected between the check in validateForwardRequest and the handler here. That is probably fine at the moment, because the zk logic in AdminManager can execute on any broker.\nIf we imagine instead how this will work with the kip-500 controller, I think the incoming request will get put on the controller's queue. By the time the request gets dequeued, we will be able to know for sure whether this node is the controller or not, so we will be able to have a much better guarantee.\nThe only reason I bring this up is that we are currently assuming that the NOT_CONTROLLER gets propagated in the envelope error field. We'll have to keep this in mind when we adapt this logic for the new controller.", "author": "hachikuji", "createdAt": "2020-10-30T23:57:37Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -126,13 +125,48 @@ class KafkaApis(val requestChannel: RequestChannel,\n     info(\"Shutdown complete.\")\n   }\n \n+  private def validateForwardRequest(request: RequestChannel.Request): Unit = {\n+    if (!config.forwardingEnabled) {\n+      throw new InvalidRequestException(s\"The handling of forwarding request $request is not enabled on this broker.\")\n+    } else if (!request.context.fromPrivilegedListener ||\n+      !authorize(request.envelopeContext.get.brokerContext, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME)) {\n+      // If the designated forwarding request is not coming from a privileged listener, or\n+      // it fails CLUSTER_ACTION permission, we would fail the authorization.\n+      throw new ClusterAuthorizationException(s\"Envelope request $request is not authorized\")\n+    } else if (!request.header.apiKey.forwardable) {\n+      throw new InvalidRequestException(\"Given RPC \" + request.header.apiKey + \" does not support forwarding\")\n+    } else if (request.principalSerde.isEmpty) {\n+      throw new PrincipalDeserializationFailureException(\"Principal serde is not defined for forwarding request\")\n+    } else if (!controller.isActive) {\n+      throw new NotControllerException(\"Non-active controller could not process forward request\")\n+    }\n+  }\n+\n+  private def maybeForward(request: RequestChannel.Request,\n+                           handler: RequestChannel.Request => Unit): Unit = {\n+    if (!request.isForwarded() && !controller.isActive && isForwardingEnabled(request)) {", "originalCommit": "8c27f126e6d955a63aeca98d653bba312f35c985", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d0945193c90e7f057cd0236b178b2ea3ffb9dd70", "url": "https://github.com/apache/kafka/commit/d0945193c90e7f057cd0236b178b2ea3ffb9dd70", "message": "additional comments", "committedDate": "2020-10-31T03:20:16Z", "type": "commit"}, {"oid": "632abebffda6f19e17c50f7fe38adaa6168cd6b0", "url": "https://github.com/apache/kafka/commit/632abebffda6f19e17c50f7fe38adaa6168cd6b0", "message": "rebuild envelope response", "committedDate": "2020-10-31T05:49:08Z", "type": "commit"}, {"oid": "882a36306f3196734067baf7bbcfbd5bedc5eb3c", "url": "https://github.com/apache/kafka/commit/882a36306f3196734067baf7bbcfbd5bedc5eb3c", "message": "comment edits", "committedDate": "2020-11-01T03:39:46Z", "type": "commit"}, {"oid": "4c5f72573d3478174fbc645c67a16f5b590e049b", "url": "https://github.com/apache/kafka/commit/4c5f72573d3478174fbc645c67a16f5b590e049b", "message": "fix throttling test", "committedDate": "2020-11-02T17:39:34Z", "type": "commit"}, {"oid": "4c5f72573d3478174fbc645c67a16f5b590e049b", "url": "https://github.com/apache/kafka/commit/4c5f72573d3478174fbc645c67a16f5b590e049b", "message": "fix throttling test", "committedDate": "2020-11-02T17:39:34Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM0NDk2NQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516344965", "bodyText": "nit: seems this change was not needed", "author": "hachikuji", "createdAt": "2020-11-02T23:57:03Z", "path": "clients/src/main/java/org/apache/kafka/clients/KafkaClient.java", "diffHunk": "@@ -180,7 +180,6 @@ ClientRequest newClientRequest(String nodeId, AbstractRequest.Builder<?> request\n \n     /**\n      * Create a new ClientRequest.\n-     *", "originalCommit": "4c5f72573d3478174fbc645c67a16f5b590e049b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM0OTY2NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516349664", "bodyText": "nit: I feel FailureException is redundant. Can we just call it PrincipalDeserializationException?\nAlso, I am not sure about this extending AuthorizationException. I would consider it more of an invalid request than an authorization failure, though the effect is the same. I think it's probably better to avoid categorizing it and just let it extend ApiException.", "author": "hachikuji", "createdAt": "2020-11-03T00:06:25Z", "path": "clients/src/main/java/org/apache/kafka/common/errors/PrincipalDeserializationFailureException.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.errors;\n+\n+/**\n+ * Exception used to indicate a kafka principal deserialization failure during request forwarding.\n+ */\n+public class PrincipalDeserializationFailureException extends AuthorizationException {", "originalCommit": "4c5f72573d3478174fbc645c67a16f5b590e049b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM1NjU2NQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516356565", "bodyText": "nit: every other property name uses a capital first letter", "author": "hachikuji", "createdAt": "2020-11-03T00:21:28Z", "path": "core/src/main/scala/kafka/server/KafkaConfig.scala", "diffHunk": "@@ -352,6 +352,8 @@ object KafkaConfig {\n   val RequestTimeoutMsProp = CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG\n   val ConnectionSetupTimeoutMsProp = CommonClientConfigs.SOCKET_CONNECTION_SETUP_TIMEOUT_MS_CONFIG\n   val ConnectionSetupTimeoutMaxMsProp = CommonClientConfigs.SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS_CONFIG\n+  private[server] val enableMetadataQuorumProp = \"enable.metadata.quorum\"", "originalCommit": "4c5f72573d3478174fbc645c67a16f5b590e049b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM2Mzg4Mw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516363883", "bodyText": "It is quite expensive to parameterize these test cases. I am not sure it is worthwhile. If forwarding works for one of these cases, why would the others be different? Since we are not planning to enable this feature yet, I think unit tests in KafkaApisTest and maybe one integration test are good enough.", "author": "hachikuji", "createdAt": "2020-11-03T00:37:20Z", "path": "core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala", "diffHunk": "@@ -70,12 +74,8 @@ import scala.collection.mutable.ArrayBuffer\n import scala.jdk.CollectionConverters._\n import scala.collection.Seq\n \n-object DynamicBrokerReconfigurationTest {\n-  val SecureInternal = \"INTERNAL\"\n-  val SecureExternal = \"EXTERNAL\"\n-}\n-\n-class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSetup {\n+@RunWith(value = classOf[Parameterized])\n+class DynamicBrokerReconfigurationTest(quorumBasedController: JBoolean) extends ZooKeeperTestHarness with SaslSetup {", "originalCommit": "4c5f72573d3478174fbc645c67a16f5b590e049b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM2NDQ5Nw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516364497", "bodyText": "I think it would be simpler to short-cut return.\nif (request.isForwarded && !validateForwardRequest(request))\n  return", "author": "hachikuji", "createdAt": "2020-11-03T00:38:40Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -126,74 +125,126 @@ class KafkaApis(val requestChannel: RequestChannel,\n     info(\"Shutdown complete.\")\n   }\n \n+  private def buildFailedEnvelopeResponse(request: RequestChannel.Request, error: Errors): Unit = {\n+    val throttleTimeMs = maybeRecordAndGetThrottleTimeMs(request)\n+    // Only throttle cluster authorization failures\n+    if (error == Errors.CLUSTER_AUTHORIZATION_FAILED)\n+      quotas.request.throttle(request, throttleTimeMs, requestChannel.sendResponse)\n+    sendResponse(request, None, None, error)\n+  }\n+\n+  private def validateForwardRequest(request: RequestChannel.Request): Boolean = {\n+    if (!config.forwardingEnabled || !request.context.fromPrivilegedListener) {\n+      // If the designated forwarding request is not coming from a privileged listener, or\n+      // forwarding is not enabled yet, we would not handle the request.\n+      closeConnection(request, Collections.emptyMap())\n+      false\n+    } else if (!authorize(request.envelopeContext.get.brokerContext, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME)) {\n+      // Forwarding request must have CLUSTER_ACTION authorization to reduce the risk of impersonation.\n+      buildFailedEnvelopeResponse(request, Errors.CLUSTER_AUTHORIZATION_FAILED)\n+      false\n+    } else if (!request.header.apiKey.forwardable) {\n+      buildFailedEnvelopeResponse(request, Errors.INVALID_REQUEST)\n+      false\n+    } else if (request.principalSerde.isEmpty) {\n+      buildFailedEnvelopeResponse(request, Errors.PRINCIPAL_DESERIALIZATION_FAILURE)\n+      false\n+    } else if (!controller.isActive) {\n+      buildFailedEnvelopeResponse(request, Errors.NOT_CONTROLLER)\n+      false\n+    } else\n+      true\n+  }\n+\n+  private def maybeForward(request: RequestChannel.Request,\n+                           handler: RequestChannel.Request => Unit): Unit = {\n+    if (!request.isForwarded && !controller.isActive && isForwardingEnabled(request)) {\n+      forwardingManager.forwardRequest(sendResponseMaybeThrottle, request)\n+    } else {\n+      // When the KIP-500 mode is off or the principal serde is undefined, forwarding is not supported,\n+      // therefore requests are handled directly.\n+      handler(request)\n+    }\n+  }\n+\n+  private def isForwardingEnabled(request: RequestChannel.Request): Boolean =\n+    config.forwardingEnabled && request.principalSerde.isDefined\n+\n   /**\n    * Top-level method that handles all requests and multiplexes to the right api\n    */\n   override def handle(request: RequestChannel.Request): Unit = {\n     try {\n       trace(s\"Handling request:${request.requestDesc(true)} from connection ${request.context.connectionId};\" +\n         s\"securityProtocol:${request.context.securityProtocol},principal:${request.context.principal}\")\n-      request.header.apiKey match {\n-        case ApiKeys.PRODUCE => handleProduceRequest(request)\n-        case ApiKeys.FETCH => handleFetchRequest(request)\n-        case ApiKeys.LIST_OFFSETS => handleListOffsetRequest(request)\n-        case ApiKeys.METADATA => handleTopicMetadataRequest(request)\n-        case ApiKeys.LEADER_AND_ISR => handleLeaderAndIsrRequest(request)\n-        case ApiKeys.STOP_REPLICA => handleStopReplicaRequest(request)\n-        case ApiKeys.UPDATE_METADATA => handleUpdateMetadataRequest(request)\n-        case ApiKeys.CONTROLLED_SHUTDOWN => handleControlledShutdownRequest(request)\n-        case ApiKeys.OFFSET_COMMIT => handleOffsetCommitRequest(request)\n-        case ApiKeys.OFFSET_FETCH => handleOffsetFetchRequest(request)\n-        case ApiKeys.FIND_COORDINATOR => handleFindCoordinatorRequest(request)\n-        case ApiKeys.JOIN_GROUP => handleJoinGroupRequest(request)\n-        case ApiKeys.HEARTBEAT => handleHeartbeatRequest(request)\n-        case ApiKeys.LEAVE_GROUP => handleLeaveGroupRequest(request)\n-        case ApiKeys.SYNC_GROUP => handleSyncGroupRequest(request)\n-        case ApiKeys.DESCRIBE_GROUPS => handleDescribeGroupRequest(request)\n-        case ApiKeys.LIST_GROUPS => handleListGroupsRequest(request)\n-        case ApiKeys.SASL_HANDSHAKE => handleSaslHandshakeRequest(request)\n-        case ApiKeys.API_VERSIONS => handleApiVersionsRequest(request)\n-        case ApiKeys.CREATE_TOPICS => handleCreateTopicsRequest(request)\n-        case ApiKeys.DELETE_TOPICS => handleDeleteTopicsRequest(request)\n-        case ApiKeys.DELETE_RECORDS => handleDeleteRecordsRequest(request)\n-        case ApiKeys.INIT_PRODUCER_ID => handleInitProducerIdRequest(request)\n-        case ApiKeys.OFFSET_FOR_LEADER_EPOCH => handleOffsetForLeaderEpochRequest(request)\n-        case ApiKeys.ADD_PARTITIONS_TO_TXN => handleAddPartitionToTxnRequest(request)\n-        case ApiKeys.ADD_OFFSETS_TO_TXN => handleAddOffsetsToTxnRequest(request)\n-        case ApiKeys.END_TXN => handleEndTxnRequest(request)\n-        case ApiKeys.WRITE_TXN_MARKERS => handleWriteTxnMarkersRequest(request)\n-        case ApiKeys.TXN_OFFSET_COMMIT => handleTxnOffsetCommitRequest(request)\n-        case ApiKeys.DESCRIBE_ACLS => handleDescribeAcls(request)\n-        case ApiKeys.CREATE_ACLS => handleCreateAcls(request)\n-        case ApiKeys.DELETE_ACLS => handleDeleteAcls(request)\n-        case ApiKeys.ALTER_CONFIGS => handleAlterConfigsRequest(request)\n-        case ApiKeys.DESCRIBE_CONFIGS => handleDescribeConfigsRequest(request)\n-        case ApiKeys.ALTER_REPLICA_LOG_DIRS => handleAlterReplicaLogDirsRequest(request)\n-        case ApiKeys.DESCRIBE_LOG_DIRS => handleDescribeLogDirsRequest(request)\n-        case ApiKeys.SASL_AUTHENTICATE => handleSaslAuthenticateRequest(request)\n-        case ApiKeys.CREATE_PARTITIONS => handleCreatePartitionsRequest(request)\n-        case ApiKeys.CREATE_DELEGATION_TOKEN => handleCreateTokenRequest(request)\n-        case ApiKeys.RENEW_DELEGATION_TOKEN => handleRenewTokenRequest(request)\n-        case ApiKeys.EXPIRE_DELEGATION_TOKEN => handleExpireTokenRequest(request)\n-        case ApiKeys.DESCRIBE_DELEGATION_TOKEN => handleDescribeTokensRequest(request)\n-        case ApiKeys.DELETE_GROUPS => handleDeleteGroupsRequest(request)\n-        case ApiKeys.ELECT_LEADERS => handleElectReplicaLeader(request)\n-        case ApiKeys.INCREMENTAL_ALTER_CONFIGS => handleIncrementalAlterConfigsRequest(request)\n-        case ApiKeys.ALTER_PARTITION_REASSIGNMENTS => handleAlterPartitionReassignmentsRequest(request)\n-        case ApiKeys.LIST_PARTITION_REASSIGNMENTS => handleListPartitionReassignmentsRequest(request)\n-        case ApiKeys.OFFSET_DELETE => handleOffsetDeleteRequest(request)\n-        case ApiKeys.DESCRIBE_CLIENT_QUOTAS => handleDescribeClientQuotasRequest(request)\n-        case ApiKeys.ALTER_CLIENT_QUOTAS => handleAlterClientQuotasRequest(request)\n-        case ApiKeys.DESCRIBE_USER_SCRAM_CREDENTIALS => handleDescribeUserScramCredentialsRequest(request)\n-        case ApiKeys.ALTER_USER_SCRAM_CREDENTIALS => handleAlterUserScramCredentialsRequest(request)\n-        case ApiKeys.ALTER_ISR => handleAlterIsrRequest(request)\n-        case ApiKeys.UPDATE_FEATURES => handleUpdateFeatures(request)\n-        // Until we are ready to integrate the Raft layer, these APIs are treated as\n-        // unexpected and we just close the connection.\n-        case ApiKeys.VOTE => closeConnection(request, util.Collections.emptyMap())\n-        case ApiKeys.BEGIN_QUORUM_EPOCH => closeConnection(request, util.Collections.emptyMap())\n-        case ApiKeys.END_QUORUM_EPOCH => closeConnection(request, util.Collections.emptyMap())\n-        case ApiKeys.DESCRIBE_QUORUM => closeConnection(request, util.Collections.emptyMap())\n+\n+      val isValidRequest = !request.isForwarded || validateForwardRequest(request)", "originalCommit": "4c5f72573d3478174fbc645c67a16f5b590e049b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM2NTgwMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516365803", "bodyText": "nit: validatedForwardedRequest", "author": "hachikuji", "createdAt": "2020-11-03T00:41:17Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -126,74 +125,126 @@ class KafkaApis(val requestChannel: RequestChannel,\n     info(\"Shutdown complete.\")\n   }\n \n+  private def buildFailedEnvelopeResponse(request: RequestChannel.Request, error: Errors): Unit = {\n+    val throttleTimeMs = maybeRecordAndGetThrottleTimeMs(request)\n+    // Only throttle cluster authorization failures\n+    if (error == Errors.CLUSTER_AUTHORIZATION_FAILED)\n+      quotas.request.throttle(request, throttleTimeMs, requestChannel.sendResponse)\n+    sendResponse(request, None, None, error)\n+  }\n+\n+  private def validateForwardRequest(request: RequestChannel.Request): Boolean = {", "originalCommit": "4c5f72573d3478174fbc645c67a16f5b590e049b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM2NjE5MQ==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516366191", "bodyText": "nit: we are doing more than building the response here, we are sending it. How about sendFailedEnvelopeResponse?", "author": "hachikuji", "createdAt": "2020-11-03T00:42:10Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -126,74 +125,126 @@ class KafkaApis(val requestChannel: RequestChannel,\n     info(\"Shutdown complete.\")\n   }\n \n+  private def buildFailedEnvelopeResponse(request: RequestChannel.Request, error: Errors): Unit = {", "originalCommit": "4c5f72573d3478174fbc645c67a16f5b590e049b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM3MjAyMA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516372020", "bodyText": "nit: instead of original, could we use forwarded in these names?", "author": "hachikuji", "createdAt": "2020-11-03T00:54:27Z", "path": "core/src/main/scala/kafka/network/SocketServer.scala", "diffHunk": "@@ -993,6 +1013,34 @@ private[kafka] class Processor(val id: Int,\n     selector.clearCompletedReceives()\n   }\n \n+  private def parseEnvelopeRequest(receive: NetworkReceive,\n+                                   nowNanos: Long,\n+                                   connectionId: String,\n+                                   context: RequestContext,\n+                                   principalSerde: Option[KafkaPrincipalSerde]) = {\n+    val envelopeRequest = context.parseRequest(receive.payload).request.asInstanceOf[EnvelopeRequest]\n+\n+    val originalHeader = RequestHeader.parse(envelopeRequest.requestData)", "originalCommit": "4c5f72573d3478174fbc645c67a16f5b590e049b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM3MzI2Mg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516373262", "bodyText": "nit: define return type", "author": "hachikuji", "createdAt": "2020-11-03T00:56:55Z", "path": "core/src/main/scala/kafka/network/SocketServer.scala", "diffHunk": "@@ -993,6 +1013,34 @@ private[kafka] class Processor(val id: Int,\n     selector.clearCompletedReceives()\n   }\n \n+  private def parseEnvelopeRequest(receive: NetworkReceive,\n+                                   nowNanos: Long,\n+                                   connectionId: String,\n+                                   context: RequestContext,\n+                                   principalSerde: Option[KafkaPrincipalSerde]) = {", "originalCommit": "4c5f72573d3478174fbc645c67a16f5b590e049b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM3NTMwMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516375303", "bodyText": "Can you add a javadoc for these methods and mention @throws SerializationException?", "author": "hachikuji", "createdAt": "2020-11-03T01:00:58Z", "path": "clients/src/main/java/org/apache/kafka/common/security/auth/KafkaPrincipalSerde.java", "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.security.auth;\n+\n+import org.apache.kafka.common.errors.SerializationException;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Serializer/Deserializer interface for {@link KafkaPrincipal} for the the purpose of inter-broker forwarding.\n+ * Any serialization/deserialization failure should raise a {@link SerializationException} to be consistent.\n+ */\n+public interface KafkaPrincipalSerde {\n+\n+    ByteBuffer serialize(KafkaPrincipal principal);", "originalCommit": "4c5f72573d3478174fbc645c67a16f5b590e049b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4MTA5MA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516381090", "bodyText": "nit: network prefix is not needed since we are already in this package", "author": "hachikuji", "createdAt": "2020-11-03T01:19:18Z", "path": "core/src/main/scala/kafka/network/SocketServer.scala", "diffHunk": "@@ -993,6 +1013,34 @@ private[kafka] class Processor(val id: Int,\n     selector.clearCompletedReceives()\n   }\n \n+  private def parseEnvelopeRequest(receive: NetworkReceive,\n+                                   nowNanos: Long,\n+                                   connectionId: String,\n+                                   context: RequestContext,\n+                                   principalSerde: Option[KafkaPrincipalSerde]) = {\n+    val envelopeRequest = context.parseRequest(receive.payload).request.asInstanceOf[EnvelopeRequest]\n+\n+    val originalHeader = RequestHeader.parse(envelopeRequest.requestData)\n+    // Leave the principal null here is ok since we will fail the request during Kafka API handling.\n+    val originalPrincipal = if (principalSerde.isDefined)\n+      principalSerde.get.deserialize(envelopeRequest.principalData)\n+    else\n+      null\n+\n+    val originalClientAddress = InetAddress.getByAddress(envelopeRequest.clientAddress)\n+    val originalContext = new RequestContext(originalHeader, connectionId,\n+      originalClientAddress, originalPrincipal, listenerName,\n+      securityProtocol, context.clientInformation, isPrivilegedListener)\n+\n+    val envelopeContext = new EnvelopeContext(\n+      brokerContext = context,\n+      receive.payload)\n+\n+    new network.RequestChannel.Request(processor = id, context = originalContext,", "originalCommit": "4c5f72573d3478174fbc645c67a16f5b590e049b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "68a5c33f8ca09c1e3a5b2ae3695b4f99061e87f4", "url": "https://github.com/apache/kafka/commit/68a5c33f8ca09c1e3a5b2ae3695b4f99061e87f4", "message": "run one single integration test", "committedDate": "2020-11-03T08:31:54Z", "type": "commit"}, {"oid": "b396bf789f0435f72d79241248b0a15407f29264", "url": "https://github.com/apache/kafka/commit/b396bf789f0435f72d79241248b0a15407f29264", "message": "fix test", "committedDate": "2020-11-03T17:14:31Z", "type": "commit"}, {"oid": "868d316c19cf5234363046b51633c9d178aacde2", "url": "https://github.com/apache/kafka/commit/868d316c19cf5234363046b51633c9d178aacde2", "message": "A few improvements for envelope handling", "committedDate": "2020-11-03T17:41:17Z", "type": "commit"}, {"oid": "8984adfacb37254ee7e04a42dc9e2472bb63fc10", "url": "https://github.com/apache/kafka/commit/8984adfacb37254ee7e04a42dc9e2472bb63fc10", "message": "revert ssl store changes", "committedDate": "2020-11-03T19:39:18Z", "type": "commit"}, {"oid": "8984adfacb37254ee7e04a42dc9e2472bb63fc10", "url": "https://github.com/apache/kafka/commit/8984adfacb37254ee7e04a42dc9e2472bb63fc10", "message": "revert ssl store changes", "committedDate": "2020-11-03T19:39:18Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkzMzE5NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516933194", "bodyText": "This inherits all tests from DynamicBrokerReconfigurationTest, which doesn't look to be intended. Can we just remove it? We can add it back once we get to testing the ssl path changes. For now I think the simple integration test for CreateTopics is good enough.\n(By the way, it's curious that testTrustStoreAlter still passes even after we have removed the path update logic.)", "author": "hachikuji", "createdAt": "2020-11-03T20:20:55Z", "path": "core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationWithForwardingIntegrationTest.scala", "diffHunk": "@@ -0,0 +1,41 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.Properties\n+\n+import org.junit.Test\n+\n+/**\n+ * Integration test suite for forwarding mechanism applied on AlterConfigs.\n+ * This class basically reused everything from {@link DynamicBrokerReconfigurationTest}\n+ * with the KIP-500 mode enabled for sasl listener alter test.\n+ */\n+class DynamicBrokerReconfigurationWithForwardingIntegrationTest extends DynamicBrokerReconfigurationTest {", "originalCommit": "8984adfacb37254ee7e04a42dc9e2472bb63fc10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1MTc2Ng==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516951766", "bodyText": "Yea, that's weird, let's move to the next PR for a discussion.", "author": "abbccdda", "createdAt": "2020-11-03T20:59:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkzMzE5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkzMzk2Mg==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516933962", "bodyText": "Do we need this change anymore?", "author": "hachikuji", "createdAt": "2020-11-03T20:22:34Z", "path": "core/src/test/scala/unit/kafka/admin/PreferredReplicaLeaderElectionCommandTest.scala", "diffHunk": "@@ -54,8 +55,15 @@ class PreferredReplicaLeaderElectionCommandTest extends ZooKeeperTestHarness wit\n \n   private def createTestTopicAndCluster(topicPartition: Map[TopicPartition, List[Int]],\n                                         authorizer: Option[String] = None): Unit = {\n+    val brokerConfigs = (0 until 3).map { node =>", "originalCommit": "8984adfacb37254ee7e04a42dc9e2472bb63fc10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1MDY4NA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516950684", "bodyText": "Seems ok to remove", "author": "abbccdda", "createdAt": "2020-11-03T20:56:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkzMzk2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkzNDY5Ng==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516934696", "bodyText": "I don't think we want to make this the default until we are ready to enable it. I would suggest we create a new ForwardRequestTest which extends BaseRequestTest. Then we can move the test case from CreateTopicsRequestTest.", "author": "hachikuji", "createdAt": "2020-11-03T20:24:08Z", "path": "core/src/test/scala/unit/kafka/server/BaseRequestTest.scala", "diffHunk": "@@ -45,6 +45,7 @@ abstract class BaseRequestTest extends IntegrationTestHarness {\n   override def modifyConfigs(props: Seq[Properties]): Unit = {\n     props.foreach { p =>\n       p.put(KafkaConfig.ControlledShutdownEnableProp, \"false\")\n+      p.put(KafkaConfig.EnableMetadataQuorumProp, \"true\")", "originalCommit": "8984adfacb37254ee7e04a42dc9e2472bb63fc10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkzODgxNA==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516938814", "bodyText": "nit: is this change needed?", "author": "hachikuji", "createdAt": "2020-11-03T20:32:19Z", "path": "core/src/test/scala/unit/kafka/api/ApiVersionTest.scala", "diffHunk": "@@ -104,7 +123,7 @@ class ApiVersionTest {\n       apiVersion.id\n     })\n \n-    val uniqueIds: Set[Int] = allIds.toSet\n+    val uniqueIds: Predef.Set[Int] = allIds.toSet", "originalCommit": "8984adfacb37254ee7e04a42dc9e2472bb63fc10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk0MjIyMw==", "url": "https://github.com/apache/kafka/pull/9103#discussion_r516942223", "bodyText": "Is this change needed? I am not sure I follow the comment about the privileged listener. That shouldn't affect ACLs I think.", "author": "hachikuji", "createdAt": "2020-11-03T20:39:33Z", "path": "core/src/test/scala/integration/kafka/api/SaslSslAdminIntegrationTest.scala", "diffHunk": "@@ -360,6 +360,8 @@ class SaslSslAdminIntegrationTest extends BaseAdminIntegrationTest with SaslSetu\n \n     // Test that we cannot create or delete ACLs when ALTER is denied.\n     authorizationAdmin.addClusterAcl(DENY, ALTER)\n+    // CLUSTER_ACTION shall also be forbid since the request goes to privilege listener\n+    authorizationAdmin.addClusterAcl(DENY, CLUSTER_ACTION)", "originalCommit": "8984adfacb37254ee7e04a42dc9e2472bb63fc10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f2bf1919726ab21ff5fcbdd20e664ae7f4ce3172", "url": "https://github.com/apache/kafka/commit/f2bf1919726ab21ff5fcbdd20e664ae7f4ce3172", "message": "test comments", "committedDate": "2020-11-03T22:16:40Z", "type": "commit"}, {"oid": "7d6fd7fdf11fbe24def47cf994466dbe75730405", "url": "https://github.com/apache/kafka/commit/7d6fd7fdf11fbe24def47cf994466dbe75730405", "message": "test fixes", "committedDate": "2020-11-04T01:10:36Z", "type": "commit"}, {"oid": "4e0b307a7276a67dfc874875718614d61dc29a9b", "url": "https://github.com/apache/kafka/commit/4e0b307a7276a67dfc874875718614d61dc29a9b", "message": "disable Envelope until ready", "committedDate": "2020-11-04T01:52:23Z", "type": "commit"}, {"oid": "bd4e1f5d788db78a80bba9b0d5ab8eeebfb7961e", "url": "https://github.com/apache/kafka/commit/bd4e1f5d788db78a80bba9b0d5ab8eeebfb7961e", "message": "set allow disabled api flag", "committedDate": "2020-11-04T04:30:02Z", "type": "commit"}, {"oid": "7f1797ba3fae3fe9339d114062e113e54f665b31", "url": "https://github.com/apache/kafka/commit/7f1797ba3fae3fe9339d114062e113e54f665b31", "message": "let remove all work", "committedDate": "2020-11-04T18:11:30Z", "type": "commit"}, {"oid": "4eb1650d08c23fdd7f169dfda680f0c1514d1156", "url": "https://github.com/apache/kafka/commit/4eb1650d08c23fdd7f169dfda680f0c1514d1156", "message": "Remove redundant envelope api filtering logic", "committedDate": "2020-11-04T19:16:22Z", "type": "commit"}, {"oid": "a4c3bf07f710e1bf12e4b864bfc302a648eec45b", "url": "https://github.com/apache/kafka/commit/a4c3bf07f710e1bf12e4b864bfc302a648eec45b", "message": "Change name of field indicating metadata quorum support", "committedDate": "2020-11-04T19:16:22Z", "type": "commit"}, {"oid": "f9844c4dc13337a68b89d6e1acf772af9246e221", "url": "https://github.com/apache/kafka/commit/f9844c4dc13337a68b89d6e1acf772af9246e221", "message": "Remove unneeded envelope handling in AbstractApiVersionsRequestTest", "committedDate": "2020-11-04T19:19:35Z", "type": "commit"}]}