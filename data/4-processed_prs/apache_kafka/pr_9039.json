{"pr_number": 9039, "pr_title": "KAFKA-5636: SlidingWindows (KIP-450)", "pr_createdAt": "2020-07-17T18:34:40Z", "pr_url": "https://github.com/apache/kafka/pull/9039", "timeline": [{"oid": "1b2b7583c481a7fbd3dedc35c838e3d98603e16d", "url": "https://github.com/apache/kafka/commit/1b2b7583c481a7fbd3dedc35c838e3d98603e16d", "message": "Initial classes for SlidingWindows, changes to processors", "committedDate": "2020-07-17T15:42:48Z", "type": "commit"}, {"oid": "3ee045a49a0c5e6478112af400e57c277b38f312", "url": "https://github.com/apache/kafka/commit/3ee045a49a0c5e6478112af400e57c277b38f312", "message": "updates to processor", "committedDate": "2020-07-17T18:31:10Z", "type": "commit"}, {"oid": "ff6ca4f62482d32f2f22a45647448727cfb67980", "url": "https://github.com/apache/kafka/commit/ff6ca4f62482d32f2f22a45647448727cfb67980", "message": "updated for grace period checks", "committedDate": "2020-07-17T20:19:34Z", "type": "commit"}, {"oid": "56a86f1718ba74e6ce4b808cb205ecf71a3d0b16", "url": "https://github.com/apache/kafka/commit/56a86f1718ba74e6ce4b808cb205ecf71a3d0b16", "message": "fixes for checkstyle", "committedDate": "2020-07-20T14:17:03Z", "type": "commit"}, {"oid": "0a3d6d490f0894ad128d4f981f9d7f87cf165f7d", "url": "https://github.com/apache/kafka/commit/0a3d6d490f0894ad128d4f981f9d7f87cf165f7d", "message": "fixes for checkstyle", "committedDate": "2020-07-20T14:20:42Z", "type": "commit"}, {"oid": "c6d358dfb62d09f4e8f406487b1f887c3db49267", "url": "https://github.com/apache/kafka/commit/c6d358dfb62d09f4e8f406487b1f887c3db49267", "message": "update algorithm, KIP changes, test", "committedDate": "2020-07-27T22:19:10Z", "type": "commit"}, {"oid": "0c1541ab543d2cfa0cb5e1e5288d39ec86187317", "url": "https://github.com/apache/kafka/commit/0c1541ab543d2cfa0cb5e1e5288d39ec86187317", "message": "Merge remote-tracking branch 'upstream/trunk' into slidingwindows", "committedDate": "2020-07-27T22:27:14Z", "type": "commit"}, {"oid": "7f1b886f9da1f0ee5d84806875f93088c7b3c4d8", "url": "https://github.com/apache/kafka/commit/7f1b886f9da1f0ee5d84806875f93088c7b3c4d8", "message": "updates for windowedBy and associated processors", "committedDate": "2020-07-28T16:36:31Z", "type": "commit"}, {"oid": "de89fe07b9ac515a0c430274200c39cadddbf64b", "url": "https://github.com/apache/kafka/commit/de89fe07b9ac515a0c430274200c39cadddbf64b", "message": "test fixes, updating algorithms", "committedDate": "2020-07-29T17:10:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTg4Nzg3Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r461887873", "bodyText": "Not sure if you did this or your IDE did it automatically, but nice \ud83d\udc4d", "author": "ableegoldman", "createdAt": "2020-07-28T21:23:12Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimeWindowedKStreamImpl.java", "diffHunk": "@@ -19,7 +19,16 @@\n import org.apache.kafka.common.serialization.Serde;\n import org.apache.kafka.common.serialization.Serdes;\n import org.apache.kafka.common.utils.Bytes;\n-import org.apache.kafka.streams.kstream.*;", "originalCommit": "7f1b886f9da1f0ee5d84806875f93088c7b3c4d8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyODQyNg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464728426", "bodyText": "The build should actually fail on wildcard imports... Do we have some checkstyle gaps? @lct45 can you maybe look into that (if not, also ok).", "author": "mjsax", "createdAt": "2020-08-04T00:19:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTg4Nzg3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyMjYwOA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462622608", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Create a new {@link SessionWindowedCogroupedKStream} instance that can be used to perform session\n          \n          \n            \n                 * Create a new {@link TimeWindowedCogroupedKStream} instance that can be used to perform sliding", "author": "ableegoldman", "createdAt": "2020-07-29T22:18:47Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/CogroupedKStream.java", "diffHunk": "@@ -275,6 +275,15 @@\n      */\n     <W extends Window> TimeWindowedCogroupedKStream<K, VOut> windowedBy(final Windows<W> windows);\n \n+    /**\n+     * Create a new {@link SessionWindowedCogroupedKStream} instance that can be used to perform session", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyNTA3Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462625072", "bodyText": "nit: call this timeDifferenceMs to be in sync with graceMs. Also it can be private", "author": "ableegoldman", "createdAt": "2020-07-29T22:24:57Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    public final long timeDifference;", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyNjc3Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462626773", "bodyText": "nit: also rename the method timeDifferenceMs to be consistent with gracePeriodMs", "author": "ableegoldman", "createdAt": "2020-07-29T22:29:17Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    public final long timeDifference;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifference, final long graceMs) {\n+        this.timeDifference = timeDifference;\n+        this.graceMs = graceMs;\n+\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs <= 0) {\n+            throw new IllegalArgumentException(\"Window timeDifference (timeDifference) must be larger than zero.\");\n+        }\n+        final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"afterWindowEnd\");\n+        final long graceMs = ApiUtils.validateMillisecondDuration(grace, msgPrefixGrace);\n+        if (graceMs < 0) {\n+            throw new IllegalArgumentException(\"Grace period must not be negative.\");\n+        }\n+\n+        return new SlidingWindows(timeDifferenceMs, graceMs);\n+    }\n+\n+\n+    public long timeDifference() {", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyNzE4Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462627186", "bodyText": "I think we can remove this suppression (and all the ones below)", "author": "ableegoldman", "createdAt": "2020-07-29T22:30:26Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    public final long timeDifference;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifference, final long graceMs) {\n+        this.timeDifference = timeDifference;\n+        this.graceMs = graceMs;\n+\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs <= 0) {\n+            throw new IllegalArgumentException(\"Window timeDifference (timeDifference) must be larger than zero.\");\n+        }\n+        final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"afterWindowEnd\");\n+        final long graceMs = ApiUtils.validateMillisecondDuration(grace, msgPrefixGrace);\n+        if (graceMs < 0) {\n+            throw new IllegalArgumentException(\"Grace period must not be negative.\");\n+        }\n+\n+        return new SlidingWindows(timeDifferenceMs, graceMs);\n+    }\n+\n+\n+    public long timeDifference() {\n+        return timeDifference;\n+    }\n+\n+\n+\n+\n+\n+    @SuppressWarnings(\"deprecation\") // continuing to support Windows#maintainMs/segmentInterval in fallback mode", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyNzI3Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462627276", "bodyText": "nit: extra space after return", "author": "ableegoldman", "createdAt": "2020-07-29T22:30:39Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    public final long timeDifference;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifference, final long graceMs) {\n+        this.timeDifference = timeDifference;\n+        this.graceMs = graceMs;\n+\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs <= 0) {\n+            throw new IllegalArgumentException(\"Window timeDifference (timeDifference) must be larger than zero.\");\n+        }\n+        final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"afterWindowEnd\");\n+        final long graceMs = ApiUtils.validateMillisecondDuration(grace, msgPrefixGrace);\n+        if (graceMs < 0) {\n+            throw new IllegalArgumentException(\"Grace period must not be negative.\");\n+        }\n+\n+        return new SlidingWindows(timeDifferenceMs, graceMs);\n+    }\n+\n+\n+    public long timeDifference() {\n+        return timeDifference;\n+    }\n+\n+\n+\n+\n+\n+    @SuppressWarnings(\"deprecation\") // continuing to support Windows#maintainMs/segmentInterval in fallback mode\n+    public long gracePeriodMs() {\n+        return graceMs;\n+    }\n+\n+\n+    @SuppressWarnings(\"deprecation\") // removing segments from Windows will fix this\n+    @Override\n+    public boolean equals(final Object o) {\n+        if (this == o) {\n+            return true;\n+        }\n+        if (o == null || getClass() != o.getClass()) {\n+            return false;\n+        }\n+        final SlidingWindows that = (SlidingWindows) o;\n+        return  timeDifference == that.timeDifference &&", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MDA1MA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462640050", "bodyText": "Seems like this should have also had a check for sessionWindows != null, right? Can we add that as well?", "author": "ableegoldman", "createdAt": "2020-07-29T23:06:31Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedStreamAggregateBuilder.java", "diffHunk": "@@ -132,16 +135,19 @@\n                                                                                     final boolean stateCreated,\n                                                                                     final StoreBuilder<?> storeBuilder,\n                                                                                     final Windows<W> windows,\n+                                                                                    final SlidingWindows slidingWindows,\n                                                                                     final SessionWindows sessionWindows,\n                                                                                     final Merger<? super K, VOut> sessionMerger) {\n \n         final ProcessorSupplier<K, ?> kStreamAggregate;\n \n-        if (windows == null && sessionWindows == null) {\n+        if (windows == null && slidingWindows == null && sessionWindows == null) {\n             kStreamAggregate = new KStreamAggregate<>(storeBuilder.name(), initializer, aggregator);\n-        } else if (windows != null && sessionWindows == null) {\n+        } else if (windows != null && slidingWindows == null && sessionWindows == null) {\n             kStreamAggregate = new KStreamWindowAggregate<>(windows, storeBuilder.name(), initializer, aggregator);\n-        } else if (windows == null && sessionMerger != null) {\n+        } else if (windows == null && slidingWindows != null && sessionWindows == null) {\n+            kStreamAggregate = new KStreamSlidingWindowAggregate<>(slidingWindows, storeBuilder.name(), initializer, aggregator);\n+        } else if (windows == null && slidingWindows == null && sessionMerger != null) {", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzAyNjg0Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463026843", "bodyText": "The original just had the check for sessionMerger != null, are there scenarios where the sessionMerger would be null but the sessionWindows wouldn't? I did think it was kind of inconsistent to check 'sessionMerger' just that one time and check 'sessionWindows' the other times so maybe it was a mistake", "author": "lct45", "createdAt": "2020-07-30T14:13:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MDA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNDA0MQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464734041", "bodyText": "I agree with Sophie that his check seems a little weird. We should check that either both (sessionWindows and sessionMerger) are null or not null.", "author": "mjsax", "createdAt": "2020-08-04T00:39:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MDA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTA3MzQzOQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465073439", "bodyText": "Would checking both be redundant? It looks like the method that ultimately calls this one will check that sessionMerger is not null for session windows, so I think either both of these will be null or neither will be null", "author": "lct45", "createdAt": "2020-08-04T14:02:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MDA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTA5NzU1OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465097558", "bodyText": "Well, yes and no. We can follow two strategies: (1) we rely on the user to only set all null (for the non-windowed aggregation case) or either one of windows, slidingWindow, or sessionWindow+sessionMerger and we don't do any verification if the method is called correctly or not. However, for this case, we don't need to do any redundant not-null check and we could just write:\nif (windows != null) {\n  ...\n} else if (slidingWindow != null) {\n  ...\n} else if (sessionWindow != null) { // we blindly assume that `sessionMerger` is not-null, too, for this case\n ...\n} else { // all `null`: non-winowed aggregation\n  ...\n}\n\nOr, (2) we do not \"trust\" the caller and do a proper check that the provided arguments make sense. And the existing code already has such a safe guard and does checks that not multiple windows are passed in and throws an IllegalArgumentException if the caller makes a mistake. I personally prefer to have a safe guard (especially on the non-hot code path) as it may prevent bugs. However, the current check is not complete, as it does not verify that sessionMerger must be not-null when sessionWindows is not-null; this may lead to a potentially cryptic NullPointerException later that is harder to understand. If we do the check and throw a proper IllegalArgumentException(\"sessionMerger cannot be null for sessionWindows\") and IllegalArgumentException(\"Unexpected sessionMerger parameter: should be null because sessionWindows is null\"); help to identify the issue quickly.", "author": "mjsax", "createdAt": "2020-08-04T14:34:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MDA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM2OTg2NQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465369865", "bodyText": "@mjsax why do we have a single method that accepts all three window types and then checks them all individually to enforce that only one type of window is actually \"set\"? Seems like we could enforce this implicitly by having a separate method for time, session, and non-windowed aggregates and then just calling the correct signature. ie SessionWindowedCogroupedKStreamImpl calls `build(...SessionWindows, SessionMerger) and so on.\nMaybe I'm missing something here because I wasn't following the cogroup KIP that closely, but is this even exposed to the user in any way? My understanding is that there's no way for this check to be violated by any kind of user input, because this method is only ever called directly by Streams internal code with null hardcoded for the unused window types. I think it's more of an internal consistency check for Streams than an input validation for the user (and it seems unnecessary: see above)", "author": "ableegoldman", "createdAt": "2020-08-04T22:40:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MDA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM3OTA3MQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465379071", "bodyText": "Not saying all this needs to be cleaned up in this PR. If we check one thing (eg sessionMerger) then we should check everything (eg sessionMerged != null && sessionWindows != null). We can decide whether we really need to check anything as followup", "author": "ableegoldman", "createdAt": "2020-08-04T23:07:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MDA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzMjMzMQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466032331", "bodyText": "We pass in all parameter to sharing to code that creates the StatefulProcessorNode -- not sure if it's the best way to structure the code and I am happy to split it up into multiple methods call (as long as we avoid code duplication). And yes, you are right, it's internal and the checks are just for us to avoid programming errors. Users should never be exposed to it. I personally tend to make a lot of mistakes and the more checks we have in place the better IMHO :)\nIf @lct45 want's she can just do a side cleanup PR to fix it, and rebase this PR after the cleanup PR was merged? Or we do it as follow up. Whatever works best for you.", "author": "mjsax", "createdAt": "2020-08-05T22:09:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MDA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQyNTkzMg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466425932", "bodyText": "I'm happy to do a PR! Looking into it now though, getStatefulProcessorNode is called by build, so I think to really separate it by type we'd need a different build and StatefulProcessorNode, otherwise we'd be moving the null checks into build and then calling the correct getStatefulProcessorNode, which does't seem to really fix anything. Thoughts? It's easy to create new build functions but I figured this might fall under not avoiding code duplication :)", "author": "lct45", "createdAt": "2020-08-06T13:49:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MDA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYxODQ1MA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466618450", "bodyText": "I do think we'd need separate build methods, since that's where we originally accept multiple windows as arguments (where all but one type is set to null in each caller). But most of build doesn't touch the windows arguments so you could probably factor out all the window-independent code into a single method and just have each build method call that", "author": "ableegoldman", "createdAt": "2020-08-06T18:52:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MDA1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0NjMxOQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462646319", "bodyText": "nit: alignment is off by one on the parameters", "author": "ableegoldman", "createdAt": "2020-07-29T23:26:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                            final InternalStreamsBuilder builder,", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0NzAyOQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462647029", "bodyText": "I think it's ok to skip this; since it's a new operator, there's no old topology to be compatible with", "author": "ableegoldman", "createdAt": "2020-07-29T23:29:01Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                            final InternalStreamsBuilder builder,\n+                            final Set<String> subTopologySourceNodes,\n+                            final String name,\n+                            final Serde<K> keySerde,\n+                            final Serde<V> valueSerde,\n+                            final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                            final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        // TODO: remove this when we do a topology-incompatible release\n+        // we used to burn a topology name here, so we have to keep doing it for compatibility\n+        if (new MaterializedInternal<>(materialized).storeName() == null) {\n+            builder.newStoreName(AGGREGATE_NAME);\n+        }", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0NzY1Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462647656", "bodyText": "I wonder why we have to do this for count but not for aggregate and reduce? Is this intentional or an oversight? cc @mjsax @vvcephei @guozhangwang", "author": "ableegoldman", "createdAt": "2020-07-29T23:30:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0NzAyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1MzA4OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462653088", "bodyText": "Can we add an else here with builder.withCachingDisabled()? It doesn't make a difference logically, it just seems easier to understand (again, also in SlidingWindowedCogroupedKStreamImpl)", "author": "ableegoldman", "createdAt": "2020-07-29T23:48:39Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                            final InternalStreamsBuilder builder,\n+                            final Set<String> subTopologySourceNodes,\n+                            final String name,\n+                            final Serde<K> keySerde,\n+                            final Serde<V> valueSerde,\n+                            final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                            final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        // TODO: remove this when we do a topology-incompatible release\n+        // we used to burn a topology name here, so we have to keep doing it for compatibility\n+        if (new MaterializedInternal<>(materialized).storeName() == null) {\n+            builder.newStoreName(AGGREGATE_NAME);\n+        }\n+\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @SuppressWarnings(\"deprecation\") // continuing to support Windows#maintainMs/segmentInterval in fallback mode\n+    private <VR> StoreBuilder<TimestampedWindowStore<K, VR>> materialize(final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            // new style retention: use Materialized retention and default segmentInterval\n+            final long retentionPeriod = materialized.retention().toMillis();\n+\n+            if ((windows.timeDifference() + windows.gracePeriodMs()) > retentionPeriod) {\n+                throw new IllegalArgumentException(\"The retention period of the window store \"\n+                        + name + \" must be no smaller than its window time difference plus the grace period.\"\n+                        + \" Got time difference=[\" + windows.timeDifference() + \"],\"\n+                        + \" grace=[\" + windows.gracePeriodMs() + \"],\"\n+                        + \" retention=[\" + retentionPeriod + \"]\");\n+            }\n+\n+            supplier = Stores.persistentTimestampedWindowStore(\n+                    materialized.storeName(),\n+                    Duration.ofMillis(retentionPeriod),\n+                    Duration.ofMillis(windows.timeDifference()),\n+                    false\n+            );\n+        }\n+        final StoreBuilder<TimestampedWindowStore<K, VR>> builder = Stores.timestampedWindowStoreBuilder(\n+                supplier,\n+                materialized.keySerde(),\n+                materialized.valueSerde()\n+        );\n+\n+        if (materialized.loggingEnabled()) {\n+            builder.withLoggingEnabled(materialized.logConfig());\n+        } else {\n+            builder.withLoggingDisabled();\n+        }\n+\n+        if (materialized.cachingEnabled()) {\n+            builder.withCachingEnabled();\n+        }", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1MzIzNw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462653237", "bodyText": "You should be able to remove this suppression and comment (here and in SlidingWindowedCogroupedKStreamImpl)", "author": "ableegoldman", "createdAt": "2020-07-29T23:49:12Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                            final InternalStreamsBuilder builder,\n+                            final Set<String> subTopologySourceNodes,\n+                            final String name,\n+                            final Serde<K> keySerde,\n+                            final Serde<V> valueSerde,\n+                            final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                            final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        // TODO: remove this when we do a topology-incompatible release\n+        // we used to burn a topology name here, so we have to keep doing it for compatibility\n+        if (new MaterializedInternal<>(materialized).storeName() == null) {\n+            builder.newStoreName(AGGREGATE_NAME);\n+        }\n+\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @SuppressWarnings(\"deprecation\") // continuing to support Windows#maintainMs/segmentInterval in fallback mode", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1MzUwNQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462653505", "bodyText": "let's just remove this comment since it's the only style retention here (also in SlidingWindowedCogroupedKStreamImpl)", "author": "ableegoldman", "createdAt": "2020-07-29T23:50:01Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                            final InternalStreamsBuilder builder,\n+                            final Set<String> subTopologySourceNodes,\n+                            final String name,\n+                            final Serde<K> keySerde,\n+                            final Serde<V> valueSerde,\n+                            final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                            final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        // TODO: remove this when we do a topology-incompatible release\n+        // we used to burn a topology name here, so we have to keep doing it for compatibility\n+        if (new MaterializedInternal<>(materialized).storeName() == null) {\n+            builder.newStoreName(AGGREGATE_NAME);\n+        }\n+\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @SuppressWarnings(\"deprecation\") // continuing to support Windows#maintainMs/segmentInterval in fallback mode\n+    private <VR> StoreBuilder<TimestampedWindowStore<K, VR>> materialize(final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            // new style retention: use Materialized retention and default segmentInterval", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1Mzc1Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462653753", "bodyText": "We can remove this", "author": "ableegoldman", "createdAt": "2020-07-29T23:50:47Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedCogroupedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+public class SlidingWindowedCogroupedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedCogroupedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+    private final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns;\n+\n+    SlidingWindowedCogroupedKStreamImpl(final SlidingWindows windows,\n+                                     final InternalStreamsBuilder builder,\n+                                     final Set<String> subTopologySourceNodes,\n+                                     final String name,\n+                                     final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                                     final StreamsGraphNode streamsGraphNode,\n+                                     final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns) {\n+        super(name, null, null, subTopologySourceNodes, streamsGraphNode, builder);\n+        //keySerde and valueSerde are null because there are many different groupStreams that they could be from\n+        this.windows = windows;\n+        this.aggregateBuilder = aggregateBuilder;\n+        this.groupPatterns = groupPatterns;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer) {\n+        return aggregate(initializer, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named) {\n+        return aggregate(initializer, named, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(\n+                materialized,\n+                builder,\n+                CogroupedKStreamImpl.AGGREGATE_NAME);\n+        return aggregateBuilder.build(\n+                groupPatterns,\n+                initializer,\n+                new NamedInternal(named),\n+                materialize(materializedInternal),\n+                materializedInternal.keySerde() != null ?\n+                        new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference())\n+                        : null,\n+                materializedInternal.valueSerde(),\n+                materializedInternal.queryableStoreName(),\n+                null,\n+                windows,\n+                null,\n+                null);\n+    }\n+\n+    @SuppressWarnings(\"deprecation\")\n+    // continuing to support Windows#maintainMs/segmentInterval in fallback mode", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1NTg1OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462655858", "bodyText": "I was wondering what this method is actually used for so I checked out the callers of KStreamWindowAggregate#windows. There's a method called extractGracePeriod in GraphGraceSearchUtil where we might actually need to make a small addition to include the new sliding window processor.\nI think it's for Suppression, which needs to figure out the grace period of the upstream operator since grace period doesn't get passed in directly to suppress", "author": "ableegoldman", "createdAt": "2020-07-29T23:58:03Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzAzNjA0OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463036048", "bodyText": "done!", "author": "lct45", "createdAt": "2020-07-30T14:26:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1NTg1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1ODkwMw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462658903", "bodyText": "Do you think we actually need to enforce that the retention period be a little longer for sliding windows? I was just thinking that since the range scan starts at timestamp - 2 * windows.timeDifference(), maybe we should actually enforce that the retention period be >= 2 * timeDifference + gracePeriod in case we need to get the aggregate value from some older window that has technically expired.\nHaven't checked the math so I'm not sure that's the correct value exactly, but it seems like it might need to be a little bigger. Any thoughts?", "author": "ableegoldman", "createdAt": "2020-07-30T00:08:03Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                            final InternalStreamsBuilder builder,\n+                            final Set<String> subTopologySourceNodes,\n+                            final String name,\n+                            final Serde<K> keySerde,\n+                            final Serde<V> valueSerde,\n+                            final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                            final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        // TODO: remove this when we do a topology-incompatible release\n+        // we used to burn a topology name here, so we have to keep doing it for compatibility\n+        if (new MaterializedInternal<>(materialized).storeName() == null) {\n+            builder.newStoreName(AGGREGATE_NAME);\n+        }\n+\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifference()) : null,\n+                materializedInternal.valueSerde());\n+\n+\n+    }\n+\n+    @SuppressWarnings(\"deprecation\") // continuing to support Windows#maintainMs/segmentInterval in fallback mode\n+    private <VR> StoreBuilder<TimestampedWindowStore<K, VR>> materialize(final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            // new style retention: use Materialized retention and default segmentInterval\n+            final long retentionPeriod = materialized.retention().toMillis();\n+\n+            if ((windows.timeDifference() + windows.gracePeriodMs()) > retentionPeriod) {", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzAzOTk4NQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463039985", "bodyText": "That's a good point. I think that >= 2* timeDifference + gracePeriod makes sense. Adding gracePeriod is just to help with out-of-order records, right? Since for a normal record we won't need anything beyond 2*timeDifference", "author": "lct45", "createdAt": "2020-07-30T14:31:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1ODkwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MDMyMA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462660320", "bodyText": "In general it's better to use a more descriptive variable name than a shorted one with a comment. It's not always possible to describe a variable exactly in a reasonable length, but I think in this case we can say curLeftWindowAlreadyExists or curLeftWindowAlreadyCreated or something\nMight be better to use AlreadyCreated when we're specifically talking about whether or not a window already exists in the window store, and can use Exists when we're talking about whether a window is possible regardless of whether it currently has been created or not", "author": "ableegoldman", "createdAt": "2020-07-30T00:13:10Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MTkyMg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462661922", "bodyText": "Does that make sense? In particular I feel like we're using Exists to mean one thing for left/RightWindowExists, and then we mean another thing entirely in prevRightWindowExists. ie prevRightWinAlreadyCreated is more similar to what we mean by the left/RightWindowExists variables", "author": "ableegoldman", "createdAt": "2020-07-30T00:19:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MDMyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NjMwNg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463046306", "bodyText": "Yeah this makes sense, the boolean naming has definitely been a struggle. I think it's clearer actually if I change prevRightWinExists to prevRightWinPossible since that's what we're saying. I agree that for leftWinExists and rightWinExists, alreadyCreated makes more sense.", "author": "lct45", "createdAt": "2020-07-30T14:40:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MDMyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MjEzMg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462662132", "bodyText": "this comment doesn't seem quite correct", "author": "ableegoldman", "createdAt": "2020-07-30T00:19:57Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MjQ5OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462662498", "bodyText": "Can we also name this variable a bit more clearly instead of the comment? Like foundClosestStartTimeWindow or something. Same with foundFirstEndTime", "author": "ableegoldman", "createdAt": "2020-07-30T00:21:20Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MzkxMQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462663911", "bodyText": "Actually maybe foundRight/LeftWindowAggregate would be good, since that's what \"the window with the closest start/end time to the record\" actually means to us", "author": "ableegoldman", "createdAt": "2020-07-30T00:26:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MjQ5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NzIzNw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463047237", "bodyText": "Yeah that's much clearer, and differentiates between the other bools better", "author": "lct45", "createdAt": "2020-07-30T14:41:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MjQ5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MzI5Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462663297", "bodyText": "maybe add a comment saying that this condition will only be hit on the very first record. Or it might be reasonable to pull this one condition out of the loop and just handle it before entering the loop", "author": "ableegoldman", "createdAt": "2020-07-30T00:24:13Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1NjQyMA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463056420", "bodyText": "I'm not sure if it can be moved out of the loop unless we also move a check for if the first is a window whose aggregate we need to update, which is easy to do but gets back to the redundant code that the algorithm had before having one while loop. I did update the comment though", "author": "lct45", "createdAt": "2020-07-30T14:53:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MzI5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNDMxNg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465414316", "bodyText": "unless we also move a check for if the first is a window whose aggregate we need to update\n\nWhich check? I was just thinking that, since the window starting at record.timestamp + 1 is basically a special case, we can just pull it out of the loop completely. We don't have to update anything since the record doesn't fall into this window, right? Basically just before entering the loop we check if next.key.window().start() == timestamp + 1 and if so set rightWinAlreadyCreated and then skip to the next record", "author": "ableegoldman", "createdAt": "2020-08-05T01:11:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MzI5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTcyNTAxMQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465725011", "bodyText": "Yeah that works unless the first window isn't the right window, in which case we would need to process it (save as the rightWinAgg, update its aggregate if the current record falls into it, etc) before going to the next record at the top of the while loop. It definitely works and is what we had before, it just makes the code a little less clean.", "author": "lct45", "createdAt": "2020-08-05T13:26:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MzI5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1MDA5Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466050097", "bodyText": "Well, if the first window isn't the right window then we just wouldn't call iterator.next again, right? So we wouldn't have to do anything at all", "author": "ableegoldman", "createdAt": "2020-08-05T23:00:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MzI5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2NzM4Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462667387", "bodyText": "Should this be inside the if (!foundFirst) condition above? We only want to save the aggregate of the first window we find with a start time less than the timestamp right?\nAlso, I think we might need to check that the max timestamp of this window is greater than the current record's timestamp. If not, then the right window will be empty.\nFor example, we have a record A at 10 and a record B at 11 and then process a record at 15. Obviously, the new right window will be empty. But the first window we'll find with a start time less than 15 will be [11, 21] with agg B.", "author": "ableegoldman", "createdAt": "2020-07-30T00:38:53Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2MzQwNg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463063406", "bodyText": "Yeah I think you're right that the aggregate should be in the if(), good catch.\nWe could check max timestamp, but this scenario should be covered already. Before we create a right window we do the boolean checks and since any in-order-record won't have either a foundLeftWinFirst or the prevRightWinAlreadyCreated and one of them needs to be true for the right window to get created after the while().\nI haven't fully thought through checking with maxTimestamp but it seems like that would work, if that way seems clearer I can alter the algorithm and run through the examples to make sure that covers everything", "author": "lct45", "createdAt": "2020-07-30T15:03:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2NzM4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2Nzk4OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462667988", "bodyText": "nit: put each parameter on its own line", "author": "ableegoldman", "createdAt": "2020-07-30T00:40:56Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {\n+                            prevRightWinExists = true;\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (windowStartTimes.contains(rightWinStart)) {\n+                                prevRightWinAlreadyCreated = true;\n+                            } else {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            //create the left window of the current record if it's not created\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                //confirms that the left window contains more than the current record\n+                if (prevRightWinExists) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));\n+                    newTimestamp = leftWinAgg.timestamp();\n+                } else {\n+                    //left window just contains the current record\n+                    aggValue = aggregator.apply(key, value, initializer.apply());\n+                    newTimestamp = timestamp;\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifference, timestamp);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(aggValue, Math.max(timestamp, newTimestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for\n+            if (!rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifference);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    //potentially need to change long to instant\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next = iterator.next();\n+\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    final long endTime = next.key.window().end();\n+                    final long startTime = next.key.window().start();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                        continue;\n+                    } else if (endTime == timestamp) {\n+                        leftWinExists = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        foundLeftFirst = isLeftWindow(next) ? true : false;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else {\n+                        rightWindowExists = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (windowStartTimes.contains(rightWinStart)) {\n+                    prevRightWinAlreadyCreated = true;\n+                } else {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }\n+            }\n+\n+            //create left window for new record\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                if (latestLeftTypeWindow != null) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));\n+                    newTimestamp = leftWinAgg.timestamp();\n+                } else {\n+                    aggValue = aggregator.apply(key, value, initializer.apply());\n+                    newTimestamp = timestamp;\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifference, timestamp);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(aggValue, Math.max(timestamp, newTimestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create right window for new record\n+            if (!rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifference);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+\n+        }\n+\n+        private boolean isLeftWindow(final KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> window) {\n+            return window.key.window().end() == window.value.timestamp();\n+        }\n+\n+        private void putAndForward(final Window window, final ValueAndTimestamp<Agg> valueAndTime, final K key,", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2OTI1MA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462669250", "bodyText": "Seems like we're aggregating with the new value twice; we call aggregator.apply once in this if/else branch but then also call it again in putAndForward, right?", "author": "ableegoldman", "createdAt": "2020-07-30T00:45:20Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {\n+                            prevRightWinExists = true;\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (windowStartTimes.contains(rightWinStart)) {\n+                                prevRightWinAlreadyCreated = true;\n+                            } else {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            //create the left window of the current record if it's not created\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                //confirms that the left window contains more than the current record\n+                if (prevRightWinExists) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA3Mzg0Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463073842", "bodyText": "Aha, I thought there was a scenario where the putAndForward function wouldn't be so simple. Yeah you're right, I updated it so the value of the record is only added in putAndForward", "author": "lct45", "createdAt": "2020-07-30T15:17:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2OTI1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY3NzQyNw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462677427", "bodyText": "Can we add a comment to clarify that we're checking whether it's a left window because that tells us there was a record at this window's end time", "author": "ableegoldman", "createdAt": "2020-07-30T01:16:27Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY3Nzc3NA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462677774", "bodyText": "I feel like I'm just way overthinking this, but I keep getting these variables confused. Maybe we could call this guy prevRightWindowCanExist? Does that seem to get at its underlying purpose?", "author": "ableegoldman", "createdAt": "2020-07-30T01:17:55Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA3NjU2NQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463076565", "bodyText": "updated above! changed to prevRightWindowPossible , lmk if that still seems confusing. I definitely kept getting them all mixed up so I think this will help", "author": "lct45", "createdAt": "2020-07-30T15:21:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY3Nzc3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY3ODc3Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462678777", "bodyText": "Can we remove this and just break out of the loop immediately at the end of the isLeftWindow condition block?", "author": "ableegoldman", "createdAt": "2020-07-30T01:21:38Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA3NzE5Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463077196", "bodyText": "100%", "author": "lct45", "createdAt": "2020-07-30T15:22:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY3ODc3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY3OTk5MQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462679991", "bodyText": "I think we need to pass in the new maximum window timestamp here, not the window start time", "author": "ableegoldman", "createdAt": "2020-07-30T01:26:22Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {\n+                            prevRightWinExists = true;\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (windowStartTimes.contains(rightWinStart)) {\n+                                prevRightWinAlreadyCreated = true;\n+                            } else {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            //create the left window of the current record if it's not created\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                //confirms that the left window contains more than the current record\n+                if (prevRightWinExists) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));\n+                    newTimestamp = leftWinAgg.timestamp();\n+                } else {\n+                    //left window just contains the current record\n+                    aggValue = aggregator.apply(key, value, initializer.apply());\n+                    newTimestamp = timestamp;\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifference, timestamp);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(aggValue, Math.max(timestamp, newTimestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for\n+            if (!rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifference);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    //potentially need to change long to instant\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next = iterator.next();\n+\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    final long endTime = next.key.window().end();\n+                    final long startTime = next.key.window().start();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                        continue;\n+                    } else if (endTime == timestamp) {\n+                        leftWinExists = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        foundLeftFirst = isLeftWindow(next) ? true : false;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else {\n+                        rightWindowExists = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (windowStartTimes.contains(rightWinStart)) {\n+                    prevRightWinAlreadyCreated = true;\n+                } else {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }\n+            }\n+\n+            //create left window for new record\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                if (latestLeftTypeWindow != null) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));\n+                    newTimestamp = leftWinAgg.timestamp();\n+                } else {\n+                    aggValue = aggregator.apply(key, value, initializer.apply());\n+                    newTimestamp = timestamp;\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifference, timestamp);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(aggValue, Math.max(timestamp, newTimestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create right window for new record\n+            if (!rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifference);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+\n+        }\n+\n+        private boolean isLeftWindow(final KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> window) {\n+            return window.key.window().end() == window.value.timestamp();\n+        }\n+\n+        private void putAndForward(final Window window, final ValueAndTimestamp<Agg> valueAndTime, final K key,\n+                                   final V value, final long closeTime, final long timestamp) {\n+            final long windowStart = window.start();\n+            final long windowEnd = window.end();\n+            if (windowEnd > closeTime) {\n+                //get aggregate from existing window\n+                final Agg oldAgg = getValueOrNull(valueAndTime);\n+                //add record's value to existing aggregate\n+                final Agg newAgg = aggregator.apply(key, value, oldAgg);\n+\n+                windowStore.put(key,\n+                        ValueAndTimestamp.make(newAgg, Math.max(timestamp, valueAndTime.timestamp())),\n+                        windowStart);\n+                tupleForwarder.maybeForward(\n+                        new Windowed<K>(key, window),\n+                        newAgg,\n+                        sendOldValues ? oldAgg : null,\n+                        windowStart);", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA3ODA1OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463078058", "bodyText": "good catch", "author": "lct45", "createdAt": "2020-07-30T15:23:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY3OTk5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY4MjI2Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462682266", "bodyText": "Ok I may have lost the trail of logic here...are we just checking prevRightWinExists as an indicator of whether we actually found any records to the left of our record within range? Could/should we check foundFirstEndTime instead?", "author": "ableegoldman", "createdAt": "2020-07-30T01:32:45Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {\n+                            prevRightWinExists = true;\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (windowStartTimes.contains(rightWinStart)) {\n+                                prevRightWinAlreadyCreated = true;\n+                            } else {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            //create the left window of the current record if it's not created\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                //confirms that the left window contains more than the current record\n+                if (prevRightWinExists) {", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4MzM0Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463083346", "bodyText": "Yeah that's what prevRightWinExists is doing right now, we could store the full valueAndTimestamp for foundFirstEndTime and check to see if the max timestamp is within the range of recordTime-timeDifference", "author": "lct45", "createdAt": "2020-07-30T15:31:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY4MjI2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4ODU4Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463088582", "bodyText": "changed to\n                if (leftWinAgg.timestamp() < timestamp &&  leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) { valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp); } else { //left window just contains the current record valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp); }", "author": "lct45", "createdAt": "2020-07-30T15:38:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY4MjI2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY4MjcxOA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462682718", "bodyText": "Since it's a left window, the max timestamp should always be timestamp, right?", "author": "ableegoldman", "createdAt": "2020-07-30T01:34:10Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {\n+                            prevRightWinExists = true;\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (windowStartTimes.contains(rightWinStart)) {\n+                                prevRightWinAlreadyCreated = true;\n+                            } else {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            //create the left window of the current record if it's not created\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                //confirms that the left window contains more than the current record\n+                if (prevRightWinExists) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));\n+                    newTimestamp = leftWinAgg.timestamp();\n+                } else {\n+                    //left window just contains the current record\n+                    aggValue = aggregator.apply(key, value, initializer.apply());\n+                    newTimestamp = timestamp;\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifference, timestamp);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(aggValue, Math.max(timestamp, newTimestamp));", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA4MzgyMQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463083821", "bodyText": "yes!", "author": "lct45", "createdAt": "2020-07-30T15:31:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY4MjcxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY4MzI1OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r462683258", "bodyText": "Can we put this condition into a method and give it a clear name to describe what this means? eg\nprivate boolean rightWindowHasNotBeenCreatedAndIsNonEmpty(..) {\n    return !rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)\n}", "author": "ableegoldman", "createdAt": "2020-07-30T01:36:00Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,428 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Instant;\n+import java.util.HashSet;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            // flag to help determine if out-of-order record\u2019s right window is non-empty\n+            boolean foundLeftFirst = false;\n+            //if current record's left/right windows already exist\n+            boolean leftWinExists = false;\n+            boolean rightWindowExists = false;\n+            //to determine if we're creating the previous record's right window, helps with determining empty windows\n+            boolean prevRightWinExists = false;\n+            boolean prevRightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time between timestamp and timestamp+timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifference()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundFirst = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundFirstEndTime = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWindowExists = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundFirst) {\n+                            foundFirst = true;\n+                            if (isLeftWindow(next)) {\n+                                foundLeftFirst = true;\n+                            }\n+                        }\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinExists = true;\n+                        continue;\n+                    } else {\n+                        if (!foundFirstEndTime) {\n+                            leftWinAgg = next.value;\n+                            foundFirstEndTime = true;\n+                        }\n+                        if (prevRightWinExists) {\n+                            break;\n+                        }\n+                        if (isLeftWindow(next)) {\n+                            prevRightWinExists = true;\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (windowStartTimes.contains(rightWinStart)) {\n+                                prevRightWinAlreadyCreated = true;\n+                            } else {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifference);\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            //create the left window of the current record if it's not created\n+            if (!leftWinExists) {\n+                final Agg aggValue;\n+                final long newTimestamp;\n+                //confirms that the left window contains more than the current record\n+                if (prevRightWinExists) {\n+                    aggValue = aggregator.apply(key, value, getValueOrNull(leftWinAgg));\n+                    newTimestamp = leftWinAgg.timestamp();\n+                } else {\n+                    //left window just contains the current record\n+                    aggValue = aggregator.apply(key, value, initializer.apply());\n+                    newTimestamp = timestamp;\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifference, timestamp);\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(aggValue, Math.max(timestamp, newTimestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for\n+            if (!rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)) {", "originalCommit": "de89fe07b9ac515a0c430274200c39cadddbf64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA5MTQzNw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463091437", "bodyText": "To go with your above comment about the maxTimestamp, I changed this to be if (!rightWinAlreadyCreated && rightWinAgg.timestamp() > timestamp) is this clearer or should it still be in a new method?", "author": "lct45", "createdAt": "2020-07-30T15:42:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY4MzI1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA5MjU3OQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r463092579", "bodyText": "Changing both of the if() for creating new windows cut down like half of the booleans too, which I think is good", "author": "lct45", "createdAt": "2020-07-30T15:44:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY4MzI1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNjU4NQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465416585", "bodyText": "Awesome! I might still recommend pulling the rightWinAgg != null && rightWinAgg.timestamp() > timestamp check out into a method called rightWindowIsNonEmpty or something, but it's definitely a lot easier to understand now even without that \ud83d\ude04", "author": "ableegoldman", "createdAt": "2020-08-05T01:19:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY4MzI1OA=="}], "type": "inlineReview"}, {"oid": "35e637d6c0d932129d69b50072d2d1b412af7d10", "url": "https://github.com/apache/kafka/commit/35e637d6c0d932129d69b50072d2d1b412af7d10", "message": ":review updates", "committedDate": "2020-07-31T19:59:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyODkwMQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464728901", "bodyText": "This PR is rather larger. Would it maybe make sense to split it into 2 and add co-group in it's own PR?", "author": "mjsax", "createdAt": "2020-08-04T00:21:03Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/CogroupedKStream.java", "diffHunk": "@@ -275,6 +275,15 @@\n      */\n     <W extends Window> TimeWindowedCogroupedKStream<K, VOut> windowedBy(final Windows<W> windows);\n \n+    /**\n+     * Create a new {@link TimeWindowedCogroupedKStream} instance that can be used to perform sliding\n+     * windowed aggregations.\n+     *\n+     * @param windows the specification of the aggregation {@link SlidingWindows}\n+     * @return an instance of {@link TimeWindowedCogroupedKStream}\n+     */\n+    TimeWindowedCogroupedKStream<K, VOut> windowedBy(final SlidingWindows windows);", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzNDU3Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465334577", "bodyText": "We could, but it would only pull out 3ish classes and not very many lines, so I don't think it would make this PR feel much smaller", "author": "lct45", "createdAt": "2020-08-04T21:15:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyODkwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM3MTYzNQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465371635", "bodyText": "Seems like the cogroup stuff makes up a pretty small amount of the overall PR, but up to Leah", "author": "ableegoldman", "createdAt": "2020-08-04T22:45:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyODkwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyOTExNg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466029116", "bodyText": "Ack. Was just a thought.", "author": "mjsax", "createdAt": "2020-08-05T22:01:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyODkwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODUxODc2MA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478518760", "bodyText": "I'm reviewing this whole PR as-is, so there's no need to do anything now, but @mjsax 's specific suggestion is beside the point. The general feedback is that this PR is too large, which it is. We shoot for under 1K, and it's the PR author's responsibility to figure out the best way to break it up.\nThis policy isn't just \"reviewers complaining,\" it's an important component of ensuring AK's quality. Long PRs overwhelm any reviewer's cognitive capacity to pay attention to every detail, so oversights are more likely to slip through into the codebase, and once they're there, you're really at the mercy of the testing layers to catch them. When the oversights are very subtle, they wind up getting released and then surface as user-reported bugs. Reviewers can't guarantee to notice every problem, but our capacity to notice problems is inversely proportional to the length of the PR.", "author": "vvcephei", "createdAt": "2020-08-27T15:45:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyODkwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcwNzk5Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478707997", "bodyText": "Coming back to this after completing the review, I'd say the biggest advice I'd share is to avoid whitespace changes and cleanups on the side when the PR is so long already. In fact, for my own super-complex PRs, I tend to go back over the whole diff and back out anything that's not critically important, just to lighten the load on the reviewers.\nCleanups are nice to have, but it's better to keep them in their own PRs or in more trivial ones.", "author": "vvcephei", "createdAt": "2020-08-27T21:32:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyODkwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTEyNg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464729126", "bodyText": "nit: double /**", "author": "mjsax", "createdAt": "2020-08-04T00:21:44Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTMxMQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464729311", "bodyText": "nit: if you want to have a new paragraph, you need to insert <p> tag -- otherwise, the empty line is just ignored all it's going to be one paragraph. -- If you don't want a paragraph, please remove the empty line.", "author": "mjsax", "createdAt": "2020-08-04T00:22:28Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTM0OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464729348", "bodyText": "as above.", "author": "mjsax", "createdAt": "2020-08-04T00:22:35Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTUzNA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464729534", "bodyText": "type [w]indows", "author": "mjsax", "createdAt": "2020-08-04T00:23:14Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTc0Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464729743", "bodyText": "and [a] given ?", "author": "mjsax", "createdAt": "2020-08-04T00:23:56Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM4MjE4OQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465382189", "bodyText": "Not sure, I think and given window grace makes grammatical sense. But either way", "author": "ableegoldman", "createdAt": "2020-08-04T23:17:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTc0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzMzM4MQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466033381", "bodyText": "I am not a native speaker... Don't ask me... Mr.John should know -- he has the proper education for it.", "author": "mjsax", "createdAt": "2020-08-05T22:12:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTc0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxNDc4Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468214782", "bodyText": "Haha, my specialty!\nThe distillation of this sentence is \"Windows are defined based on a record's timestamp, window size, and window grace period.\" I think the meaning is pretty clear, so no need to change anything.\nJust to point it out, there's structural ambiguity about whether the sentence is saying \"a record's (timestamp, window size, window grace period)\" (I.e., three properties of the record), or whether there are three top-level things that define the window. The latter was intended. I think actually inserting \"the\" before \"window\" both times would clear it up: \"Windows are defined based on a record's timestamp, the window size, and the window grace period.\"\nAnother note is that because the second item in the list is so long, the structure of the list gets a little lost. It would be better in this case to use the Oxford comma to clearly delineate the boundary between the second and third items.\nSo, although I think this is fine as-is, if you want me to break out the red pen, I'd say:\n * Sliding Windows are defined based on a record's timestamp, the window size based on the given maximum time difference (inclusive) between\n * records in the same window, and the given window grace period.", "author": "vvcephei", "createdAt": "2020-08-10T22:11:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTc0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMDY4Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464730686", "bodyText": "We must use HTML list markup to get bullet points rendered, ie, <ul> and <li> (cf https://www.w3schools.com/html/html_lists.asp)", "author": "mjsax", "createdAt": "2020-08-04T00:26:58Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMTA5Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464731092", "bodyText": "are processed -> sounds like processing time semantics; maybe better occur in the stream (i.e., event timestamps)", "author": "mjsax", "createdAt": "2020-08-04T00:28:25Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMTU1NA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464731554", "bodyText": "Reference to CogroupedKStream is missing", "author": "mjsax", "createdAt": "2020-08-04T00:29:58Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMTc2Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464731762", "bodyText": "as above (won't comment on this again) -- please address throughput the whole PR.", "author": "mjsax", "createdAt": "2020-08-04T00:30:46Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMjIwNA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464732204", "bodyText": "timeDifference (timeDifference) (redundant) should be time difference (timeDifference)\nmust be larger than zero. -> must not be negative.", "author": "mjsax", "createdAt": "2020-08-04T00:32:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs <= 0) {\n+            throw new IllegalArgumentException(\"Window timeDifference (timeDifference) must be larger than zero.\");", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMjQ2Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464732462", "bodyText": "I guess a timeDifference of zero should be allowed to define a sliding window of size 1ms", "author": "mjsax", "createdAt": "2020-08-04T00:33:30Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMjU3Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464732572", "bodyText": "as above -> should be timeDifferenceMs < 0", "author": "mjsax", "createdAt": "2020-08-04T00:33:53Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs <= 0) {", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczMzAxNA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464733014", "bodyText": "For consistency: Grace period (grace) must ? Frankly, I am not sure if we need to have \"natural language\" and the parameter name in those error messages -- also above. But we should do it in a consistent manner IMHO.", "author": "mjsax", "createdAt": "2020-08-04T00:35:30Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and given window grace period.\n+ *\n+ * While the window is sliding over the input data stream, a new window is created each time a record enters\n+ * the sliding window or a record drops out of the sliding window.\n+ *\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * - window {@code [3000;8000]} contains [1] (created when first record enters the window)\n+ * - window {@code [4200;9200]} contains [1,2] (created when second record enters the window)\n+ * - window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)\n+ * - window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)\n+ * - window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)\n+ *\n+ * Note that while SlidingWindows are of a fixed size {@link TimeWindows}, the start and end points\n+ * depend on when events are processed, similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(Windows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period.\n+     *\n+     * Reject out-of-order events that arrive after {@code grace}. A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size or grace is zero or negative or can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs <= 0) {\n+            throw new IllegalArgumentException(\"Window timeDifference (timeDifference) must be larger than zero.\");\n+        }\n+        final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"afterWindowEnd\");\n+        final long graceMs = ApiUtils.validateMillisecondDuration(grace, msgPrefixGrace);\n+        if (graceMs < 0) {\n+            throw new IllegalArgumentException(\"Grace period must not be negative.\");", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNTE3Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464735173", "bodyText": "I think we should do this check in init and use a Runnable that we just call blindly (ie, depending on the check, we instantiate the one or other Runnable and each Runnable implements a different algorithm.", "author": "mjsax", "createdAt": "2020-08-04T00:43:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI2MDMxOA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r467260318", "bodyText": "Sounds good, I'll change that when I implement the reverse iterator in the next PR", "author": "lct45", "createdAt": "2020-08-07T20:44:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNTE3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNTI1Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464735252", "bodyText": "Why do we suppress instead of fix the issue? (or add an exception to the suppress.xml file if we really need it)", "author": "mjsax", "createdAt": "2020-08-04T00:44:11Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNjA1Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464736052", "bodyText": "I think we should also drop if value == null ? (It seem this null check is missing in the existing time/session-window aggregate processors, too)", "author": "mjsax", "createdAt": "2020-08-04T00:47:16Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNjU1MA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464736550", "bodyText": "Maybe we have the same issue in other processors, too (we might even have a ticket for it?) but won't we need to preserve observedStreamTime across restarts? It's transient atm... (Just want to confirm -- maybe it's ok as other processor do it the same way and we need to fix if for all of them at once?)", "author": "mjsax", "createdAt": "2020-08-04T00:49:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM3NDAwNA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465374004", "bodyText": "We definitely have the issue in all processors right now lol. It's not any more of a problem for this sliding windows algorithm as for any other operator that defines a grace period, at least. We might end up not dropping a late record that we should have; for sliding windows we'd get one extra window (with this record at the window end) whereas for a hopping/tumbling window we'd get N extra windows (however many overlaps there are)", "author": "ableegoldman", "createdAt": "2020-08-04T22:52:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNjU1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzMjgwNA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466032804", "bodyText": "Thanks for confirming. Let keep this issue out for this PR than.", "author": "mjsax", "createdAt": "2020-08-05T22:10:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNjU1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNjk2OQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464736969", "bodyText": "The comment seems redundant -- it just says exactly what the next line of code says.", "author": "mjsax", "createdAt": "2020-08-04T00:51:12Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczNzYwMg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464737602", "bodyText": "No need to pass in an Instant -- we should just pass in the long directly.\nIt might not be clear from the type hierarchy, but the overloads that accept long are only deprecated for the ReadOnlyXxx store, but are still available on the \"read/write\" stores to avoid unnecessary runtime overhead.", "author": "mjsax", "createdAt": "2020-08-04T00:53:54Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifferenceMs()),", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczODIxMg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464738212", "bodyText": "Why + 1 ?", "author": "mjsax", "createdAt": "2020-08-04T00:56:11Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifferenceMs()),\n+                            Instant.ofEpochMilli(timestamp + 1))", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzNjk3Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465336976", "bodyText": "So we can check to see if the record that is being processed has an already existing right window (that would start at timestamp+1) without doing another call to the store", "author": "lct45", "createdAt": "2020-08-04T21:20:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczODIxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM3OTU2Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465379566", "bodyText": "Maybe we can add this answer as a comment in the code for future readers", "author": "ableegoldman", "createdAt": "2020-08-04T23:09:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczODIxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczODkzMg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464738932", "bodyText": "as above. (also, this code seems to be duplicated; we should move it into process() before we call the the actual processReverse or processInOrder methods.", "author": "mjsax", "createdAt": "2020-08-04T00:59:18Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifferenceMs()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifferenceMs(), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+            if (key == null) {", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczOTA4NA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464739084", "bodyText": "as above", "author": "mjsax", "createdAt": "2020-08-04T00:59:53Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifferenceMs()),\n+                            Instant.ofEpochMilli(timestamp + 1))\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifferenceMs(), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,\n+                            key,\n+                            Instant.ofEpochMilli(timestamp - 2 * windows.timeDifferenceMs()),", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczOTE1Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464739153", "bodyText": "nit: move key to the next line", "author": "mjsax", "createdAt": "2020-08-04T01:00:07Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.time.Instant;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        @SuppressWarnings(\"checkstyle:LineLength\")\n+        public void processReverse(final K key, final V value) {\n+            if (key == null) {\n+                log.warn(\n+                        \"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(key,", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzOTMyMg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465339322", "bodyText": "fixed for both", "author": "lct45", "createdAt": "2020-08-04T21:25:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczOTE1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczOTk3Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464739977", "bodyText": "why * 2 ?", "author": "mjsax", "createdAt": "2020-08-04T01:03:19Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                               final InternalStreamsBuilder builder,\n+                               final Set<String> subTopologySourceNodes,\n+                               final String name,\n+                               final Serde<K> keySerde,\n+                               final Serde<V> valueSerde,\n+                               final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                               final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    private <VR> StoreBuilder<TimestampedWindowStore<K, VR>> materialize(final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            final long retentionPeriod = materialized.retention() != null ? materialized.retention().toMillis() : windows.gracePeriodMs() + 2 * windows.timeDifferenceMs();\n+\n+            if ((windows.timeDifferenceMs() * 2 + windows.gracePeriodMs()) > retentionPeriod) {", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTEwMTA1OQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465101059", "bodyText": "@lct45 With regard to above: the error message to does align to the required minimum retention time. It says must be no smaller than its window time difference plus the grace period....", "author": "mjsax", "createdAt": "2020-08-04T14:39:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDczOTk3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDEwOA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464740108", "bodyText": "why * 2 ?", "author": "mjsax", "createdAt": "2020-08-04T01:03:51Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                               final InternalStreamsBuilder builder,\n+                               final Set<String> subTopologySourceNodes,\n+                               final String name,\n+                               final Serde<K> keySerde,\n+                               final Serde<V> valueSerde,\n+                               final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                               final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    private <VR> StoreBuilder<TimestampedWindowStore<K, VR>> materialize(final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            final long retentionPeriod = materialized.retention() != null ? materialized.retention().toMillis() : windows.gracePeriodMs() + 2 * windows.timeDifferenceMs();", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTA3OTI5NA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465079294", "bodyText": "We want to be able to find the furthest window for which we can create a corresponding right window, so for any record the furthest window we will ever need will start at timestamp - 2 * timeDifference, but we will need to have these around to calculate new windows, hence the longer retention time.", "author": "lct45", "createdAt": "2020-08-04T14:10:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDEwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTA5OTkxMw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465099913", "bodyText": "Ack. Makes sense. Might be worth to add a comment why we need an \"unexpected\" large retention time.", "author": "mjsax", "createdAt": "2020-08-04T14:37:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDEwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTExMTY2NQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465111665", "bodyText": "Good catch, will do", "author": "lct45", "createdAt": "2020-08-04T14:53:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDEwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwNzQ0OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465407448", "bodyText": "+1 to add a comment on the extra retention (here and in SlidingWindowedCoGroupedKStreamImpl)", "author": "ableegoldman", "createdAt": "2020-08-05T00:45:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDEwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDMyNQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464740325", "bodyText": "Why this?", "author": "mjsax", "createdAt": "2020-08-04T01:04:45Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStreamThreadTest.java", "diffHunk": "@@ -221,7 +221,6 @@ public void shouldTransitionToRunningOnStart() throws Exception {\n         globalStreamThread.shutdown();\n     }\n \n-    @Test", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIwNTA4OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465205088", "bodyText": "Whoops, not on purpose. Thanks for the check", "author": "lct45", "createdAt": "2020-08-04T17:15:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDMyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDcxMg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464740712", "bodyText": "we should not use this annotation (even if we still have code that used it... we are working on migrating test away lazily). We should instead use assertThrows and also verify the exception error message.\nSame below.", "author": "mjsax", "createdAt": "2020-08-04T01:06:18Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDk2MA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464740960", "bodyText": "Beside the fact, that zero should be valid IMHO, what do we gain by testing 0 and -1 ?", "author": "mjsax", "createdAt": "2020-08-04T01:07:06Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE5MzczNQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465193735", "bodyText": "I think just confirming that the correct error will be thrown when someone sets a timeDifference we don't want. I'll update all the windowSize to be timeDifference and I agree, no need to check that it isn't 0", "author": "lct45", "createdAt": "2020-08-04T16:55:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MDk2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTEwMw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464741103", "bodyText": "This line seems to be unnecessary for this test?", "author": "mjsax", "createdAt": "2020-08-04T01:07:51Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE5NDg0NQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465194845", "bodyText": "Re-examining the test, it looks like it does the same thing as gracePeriodMustNotBeNegative() so I think the test can be removed entirely", "author": "lct45", "createdAt": "2020-08-04T16:57:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTEwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTIwMg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464741202", "bodyText": "This is also a pattern we try to move off. Use assertThrows instead.", "author": "mjsax", "createdAt": "2020-08-04T01:08:16Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));\n+\n+        try {", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTY4OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464741688", "bodyText": "Why do we need three tests? If you want to \"randomize\" it, maybe just use Random to generate difference and grace input instead of hard coding them?", "author": "mjsax", "createdAt": "2020-08-04T01:10:08Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));\n+\n+        try {\n+            SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(-1L));\n+            fail(\"should not accept negatives\");\n+        } catch (final IllegalArgumentException e) {\n+            //expected\n+        }\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForPositiveCases() {\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwODg3Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465408876", "bodyText": "+1 to using a random number instead of multiple lines. If it does happen to fail on a specific random number, we should be sure to print that number for reproducing it later. See TaskAssignorConvergenceTest#runRandomizedScenario for example", "author": "ableegoldman", "createdAt": "2020-08-05T00:50:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTY4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTgwMQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464741801", "bodyText": "As above.", "author": "mjsax", "createdAt": "2020-08-04T01:10:33Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));\n+\n+        try {\n+            SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(-1L));\n+            fail(\"should not accept negatives\");\n+        } catch (final IllegalArgumentException e) {\n+            //expected\n+        }\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForPositiveCases() {\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));\n+\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForNegativeCases() {\n+\n+        verifyInEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(2)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyInEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(9)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTk4Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464741983", "bodyText": "What is the difference between verifyInEquality and assertNotEquals ?", "author": "mjsax", "createdAt": "2020-08-04T01:11:13Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));\n+\n+        try {\n+            SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(-1L));\n+            fail(\"should not accept negatives\");\n+        } catch (final IllegalArgumentException e) {\n+            //expected\n+        }\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForPositiveCases() {\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));\n+\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForNegativeCases() {\n+\n+        verifyInEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(2)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyInEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(9)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));\n+\n+\n+        verifyInEquality(\n+                SlidingWindows.withTimeDifferenceAndGrace(ofMillis(4), ofMillis(2)),\n+                SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(2))\n+        );\n+\n+        assertNotEquals(", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM0ODY3OQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465348679", "bodyText": "They look to be fairly similar, and it seems like the tests use both consistently. verifyInEquality seems to be more thorough, and to be consistent with the above equalsAndHashcodeShouldBeValidForPositiveCases I think I'll use verifyInEquality for this test, unless someone has an objection", "author": "lct45", "createdAt": "2020-08-04T21:46:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MTk4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MjMzMA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464742330", "bodyText": "This should be two test:\n\nshouldNotBeEqualForDifferentTimeDifference\nshouldNotBeEqualForDifferentGracePeriod", "author": "mjsax", "createdAt": "2020-08-04T01:12:30Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.junit.Test;\n+\n+import static java.time.Duration.ofMillis;\n+import static org.apache.kafka.streams.EqualityCheck.verifyEquality;\n+import static org.apache.kafka.streams.EqualityCheck.verifyInEquality;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.fail;\n+\n+@SuppressWarnings(\"deprecation\")\n+public class SlidingWindowsTest {\n+\n+    private static final long ANY_SIZE = 123L;\n+\n+    @Test\n+    public void shouldSetWindowSize() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(ANY_SIZE), ofMillis(3)).timeDifferenceMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeZero() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(0), ofMillis(5));\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void windowSizeMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(-1), ofMillis(5));\n+    }\n+\n+    @Test\n+    public void shouldSetGracePeriod() {\n+        assertEquals(ANY_SIZE, SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(ANY_SIZE)).gracePeriodMs());\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void graceMustNotBeNegative() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(-1));\n+    }\n+\n+    @Test\n+    public void gracePeriodShouldEnforceBoundaries() {\n+        SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(0L));\n+\n+        try {\n+            SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3L), ofMillis(-1L));\n+            fail(\"should not accept negatives\");\n+        } catch (final IllegalArgumentException e) {\n+            //expected\n+        }\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForPositiveCases() {\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(3)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(1)));\n+\n+        verifyEquality(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)), SlidingWindows.withTimeDifferenceAndGrace(ofMillis(3), ofMillis(4)));\n+\n+    }\n+\n+    @Test\n+    public void equalsAndHashcodeShouldBeValidForNegativeCases() {", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwOTMzMA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465409330", "bodyText": "Took me a second to understand this test, \"NegativeCases\" made me think the timeDifference/grace were supposed to be negative. +1 to Matthias's suggestion for naming (and splitting into two tests)", "author": "ableegoldman", "createdAt": "2020-08-05T00:52:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0MjMzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0NDUwNA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464744504", "bodyText": "Why is endTime = Long.MAX_VALUE? Should it not be firstBatchTimestamp ?", "author": "mjsax", "createdAt": "2020-08-04T01:20:48Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java", "diffHunk": "@@ -459,6 +460,95 @@ public void shouldGroupByKey() throws Exception {\n         )));\n     }\n \n+\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() throws Exception {\n+        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 0);\n+        final long firstBatchTimestamp = 2000L;\n+        final long timeDifference = 1000L;\n+        produceMessages(firstBatchTimestamp);\n+        final long secondBatchTimestamp = firstBatchTimestamp + timeDifference / 2;\n+        produceMessages(secondBatchTimestamp);\n+        final long thirdBatchTimestamp = secondBatchTimestamp + timeDifference - 100L;\n+        produceMessages(thirdBatchTimestamp);\n+\n+        final Serde<Windowed<String>> windowedSerde = WindowedSerdes.timeWindowedSerdeFrom(String.class);\n+        groupedStream\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(timeDifference), ofMillis(2000L)))\n+                .reduce(reducer)\n+                .toStream()\n+                .to(outputTopic, Produced.with(windowedSerde, Serdes.String()));\n+\n+        startStreams();\n+\n+        final List<KeyValueTimestamp<Windowed<String>, String>> windowedOutput = receiveMessages(\n+                new TimeWindowedDeserializer<>(),\n+                new StringDeserializer(),\n+                String.class,\n+                25);\n+\n+        // read from ConsoleConsumer\n+        final String resultFromConsoleConsumer = readWindowedKeyedMessagesViaConsoleConsumer(\n+                new TimeWindowedDeserializer<String>(),\n+                new StringDeserializer(),\n+                String.class,\n+                25,\n+                true);\n+\n+        final Comparator<KeyValueTimestamp<Windowed<String>, String>> comparator =\n+                Comparator.comparing((KeyValueTimestamp<Windowed<String>, String> o) -> o.key().key())\n+                        .thenComparing(KeyValueTimestamp::value);\n+\n+        windowedOutput.sort(comparator);\n+        final long firstBatchLeftWindow = firstBatchTimestamp - timeDifference;\n+        final long firstBatchRightWindow = firstBatchTimestamp + 1;\n+        final long secondBatchLeftWindow = secondBatchTimestamp - timeDifference;\n+        final long secondBatchRightWindow = secondBatchTimestamp + 1;\n+        final long thirdBatchLeftWindow = thirdBatchTimestamp - timeDifference;\n+\n+        final List<KeyValueTimestamp<Windowed<String>, String>> expectResult = Arrays.asList(\n+                new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(firstBatchLeftWindow, Long.MAX_VALUE)), \"A\", firstBatchTimestamp),", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE5MTUwNQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465191505", "bodyText": "There seems to be a bug in TimeWindowedDeserializer related to this ticket that ends up setting the windowSize to Long.MAX_VALUE. For the purposes of testing, I don't think having it as the max value is totally awful (just somewhat awful) and the window end calculations are all tested in a different set of tests done through topology driver. I'll make a ticket for this bug and try to get it fixed when I'm done with testing", "author": "lct45", "createdAt": "2020-08-04T16:51:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0NDUwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyNjAwNw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466026007", "bodyText": "The ticket is already fixed. You need to pass in the windowSize into the the constructor of TimeWindowDeserializer to get rid of the problem.", "author": "mjsax", "createdAt": "2020-08-05T21:54:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0NDUwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2MzU4Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466063583", "bodyText": "No she's right, this problem is not resolved at all. You can pass in windowSize to the constructor for TimeWindowedDeserializer all you want but it just gets ignored because the actual deserializer object you instantiate is thrown away. Whether you're reading in records through a Java Consumer or the console consumer (for some reason this test does both), the actual deserializer is always constructed within the consumer based on the configs. There's a config for the windowed inner class which is properly set in TimeWindowedDeserializer#configure but no config for the windowSize so there's no way to set it at the moment.\ntl;dr there's no point in having serde constructors accept parameters, they need to be set through configure", "author": "ableegoldman", "createdAt": "2020-08-05T23:42:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0NDUwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3MDMxNw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466070317", "bodyText": "Created a ticket for this here: https://issues.apache.org/jira/browse/KAFKA-10366 let me know if the description isn't clear", "author": "lct45", "createdAt": "2020-08-06T00:05:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0NDUwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIzMTg0Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r467231847", "bodyText": "Thanks. I missed the point that this trick to pass in the windowSize only works for KafkaStreams when we pass in Serdes object that are used as provided...", "author": "mjsax", "createdAt": "2020-08-07T19:33:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0NDUwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI5NjExMQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r467296111", "bodyText": "Clearly Kafka Streams is superior to the plain Consumer \ud83d\ude08", "author": "ableegoldman", "createdAt": "2020-08-07T21:50:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0NDUwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0NDk2Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464744963", "bodyText": "firstBatchLeftWindow -> firstBatchLeftWindowStart\nMaybe also introduce firstBatchLeftWindowEnd = firstBatchTimestamp", "author": "mjsax", "createdAt": "2020-08-04T01:22:32Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java", "diffHunk": "@@ -459,6 +460,95 @@ public void shouldGroupByKey() throws Exception {\n         )));\n     }\n \n+\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() throws Exception {\n+        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 0);\n+        final long firstBatchTimestamp = 2000L;\n+        final long timeDifference = 1000L;\n+        produceMessages(firstBatchTimestamp);\n+        final long secondBatchTimestamp = firstBatchTimestamp + timeDifference / 2;\n+        produceMessages(secondBatchTimestamp);\n+        final long thirdBatchTimestamp = secondBatchTimestamp + timeDifference - 100L;\n+        produceMessages(thirdBatchTimestamp);\n+\n+        final Serde<Windowed<String>> windowedSerde = WindowedSerdes.timeWindowedSerdeFrom(String.class);\n+        groupedStream\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(timeDifference), ofMillis(2000L)))\n+                .reduce(reducer)\n+                .toStream()\n+                .to(outputTopic, Produced.with(windowedSerde, Serdes.String()));\n+\n+        startStreams();\n+\n+        final List<KeyValueTimestamp<Windowed<String>, String>> windowedOutput = receiveMessages(\n+                new TimeWindowedDeserializer<>(),\n+                new StringDeserializer(),\n+                String.class,\n+                25);\n+\n+        // read from ConsoleConsumer\n+        final String resultFromConsoleConsumer = readWindowedKeyedMessagesViaConsoleConsumer(\n+                new TimeWindowedDeserializer<String>(),\n+                new StringDeserializer(),\n+                String.class,\n+                25,\n+                true);\n+\n+        final Comparator<KeyValueTimestamp<Windowed<String>, String>> comparator =\n+                Comparator.comparing((KeyValueTimestamp<Windowed<String>, String> o) -> o.key().key())\n+                        .thenComparing(KeyValueTimestamp::value);\n+\n+        windowedOutput.sort(comparator);\n+        final long firstBatchLeftWindow = firstBatchTimestamp - timeDifference;", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0NjAxMw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464746013", "bodyText": "Maybe add comment to clarify which input should trigger which output:\n// process A @ 2000ms\nnew KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(firstBatchLeftWindow, Long.MAX_VALUE)), \"A\", firstBatchTimestamp),\n// process A @ 2500ms\nnew KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(firstBatchRightWindow, Long.MAX_VALUE)), \"A\", firstBatchTimestamp),\nnew KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(secondBatchLeftWindow, Long.MAX_VALUE)), \"A:A\", secondBatchTimestamp),\n// process A @ 2900ms\n...", "author": "mjsax", "createdAt": "2020-08-04T01:26:26Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java", "diffHunk": "@@ -459,6 +460,95 @@ public void shouldGroupByKey() throws Exception {\n         )));\n     }\n \n+\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() throws Exception {\n+        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 0);\n+        final long firstBatchTimestamp = 2000L;\n+        final long timeDifference = 1000L;\n+        produceMessages(firstBatchTimestamp);\n+        final long secondBatchTimestamp = firstBatchTimestamp + timeDifference / 2;\n+        produceMessages(secondBatchTimestamp);\n+        final long thirdBatchTimestamp = secondBatchTimestamp + timeDifference - 100L;\n+        produceMessages(thirdBatchTimestamp);\n+\n+        final Serde<Windowed<String>> windowedSerde = WindowedSerdes.timeWindowedSerdeFrom(String.class);\n+        groupedStream\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(timeDifference), ofMillis(2000L)))\n+                .reduce(reducer)\n+                .toStream()\n+                .to(outputTopic, Produced.with(windowedSerde, Serdes.String()));\n+\n+        startStreams();\n+\n+        final List<KeyValueTimestamp<Windowed<String>, String>> windowedOutput = receiveMessages(\n+                new TimeWindowedDeserializer<>(),\n+                new StringDeserializer(),\n+                String.class,\n+                25);\n+\n+        // read from ConsoleConsumer\n+        final String resultFromConsoleConsumer = readWindowedKeyedMessagesViaConsoleConsumer(\n+                new TimeWindowedDeserializer<String>(),\n+                new StringDeserializer(),\n+                String.class,\n+                25,\n+                true);\n+\n+        final Comparator<KeyValueTimestamp<Windowed<String>, String>> comparator =\n+                Comparator.comparing((KeyValueTimestamp<Windowed<String>, String> o) -> o.key().key())\n+                        .thenComparing(KeyValueTimestamp::value);\n+\n+        windowedOutput.sort(comparator);\n+        final long firstBatchLeftWindow = firstBatchTimestamp - timeDifference;\n+        final long firstBatchRightWindow = firstBatchTimestamp + 1;\n+        final long secondBatchLeftWindow = secondBatchTimestamp - timeDifference;\n+        final long secondBatchRightWindow = secondBatchTimestamp + 1;\n+        final long thirdBatchLeftWindow = thirdBatchTimestamp - timeDifference;\n+\n+        final List<KeyValueTimestamp<Windowed<String>, String>> expectResult = Arrays.asList(\n+                new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(firstBatchLeftWindow, Long.MAX_VALUE)), \"A\", firstBatchTimestamp),", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTA5NDk2Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465094963", "bodyText": "Because the windows are sorted, the windows created by each record aren't consecutive, so I added comments describing each window, but only did it for A since all the other keys are processed the exact same way. Sample comment: // A @ secondBatchTimestamp left window created when A @ secondBatchTimestamp processed", "author": "lct45", "createdAt": "2020-08-04T14:30:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0NjAxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0Njk4MQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r464746981", "bodyText": "We should add a fourth batch with ts like 10K to get the windows when the second batch drops outs, too.", "author": "mjsax", "createdAt": "2020-08-04T01:30:05Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java", "diffHunk": "@@ -459,6 +460,95 @@ public void shouldGroupByKey() throws Exception {\n         )));\n     }\n \n+\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() throws Exception {\n+        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 0);\n+        final long firstBatchTimestamp = 2000L;\n+        final long timeDifference = 1000L;\n+        produceMessages(firstBatchTimestamp);\n+        final long secondBatchTimestamp = firstBatchTimestamp + timeDifference / 2;\n+        produceMessages(secondBatchTimestamp);\n+        final long thirdBatchTimestamp = secondBatchTimestamp + timeDifference - 100L;\n+        produceMessages(thirdBatchTimestamp);", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIxOTUyMg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465219522", "bodyText": "To clarify, are you wanting to add records that would fall after the third batch outside of all the existing windows, or so that it will fall into the third batch's windows but not the second batch's windows?", "author": "lct45", "createdAt": "2020-08-04T17:40:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0Njk4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM4MDc1Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465380753", "bodyText": "Just to clarify, we don't get any additional output when the stream time is advanced and older windows drop out of the grace period. We've already forwarded their final state when the last record to update that window was processed. Not sure if that's what you meant by \"get the windows when the batch drops out\" or not?", "author": "ableegoldman", "createdAt": "2020-08-04T23:13:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0Njk4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyODg2Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466028866", "bodyText": "I guess testing both cases would be good. Even if testing the former (fall outside of all existing windows) was my original intent.\nAnd thank for comment Sophie: I tend to forget that we should produce all (non-empty) right windows already upfront/eagerly (and not delayed/lazily when stream-time advances beyond window-end time). In any case, it seems to be a good test case to make sure we don't (re-)emit an (unexpected) window if stream-time jumps ahead?", "author": "mjsax", "createdAt": "2020-08-05T22:00:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0Njk4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA0MjUwNQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466042505", "bodyText": "That should be covered in KStreamSlidingWindowAggregateTest, which goes through more of the edge cases using the TopologyTestDriver which is a little easier to manipulate than this set up", "author": "lct45", "createdAt": "2020-08-05T22:38:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc0Njk4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM4MjY2Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465382663", "bodyText": "It took me a second to understand the structure of this sentence, can we insert an and after the record's timestamp?", "author": "ableegoldman", "createdAt": "2020-08-04T23:19:29Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, window size based on the given maximum time difference (inclusive) between", "originalCommit": "35e637d6c0d932129d69b50072d2d1b412af7d10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "dc2f65f711a54e995094a1885df1dc728c479b1b", "url": "https://github.com/apache/kafka/commit/dc2f65f711a54e995094a1885df1dc728c479b1b", "message": "review updates, test additions", "committedDate": "2020-08-05T00:09:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM5OTAzMQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465399031", "bodyText": "extra /* here", "author": "ableegoldman", "createdAt": "2020-08-05T00:13:33Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ /*", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM5OTg1Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465399853", "bodyText": "Technically this is an XOR not an OR \ud83d\ude1b", "author": "ableegoldman", "createdAt": "2020-08-05T00:16:39Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedStreamAggregateBuilder.java", "diffHunk": "@@ -132,19 +135,25 @@\n                                                                                     final boolean stateCreated,\n                                                                                     final StoreBuilder<?> storeBuilder,\n                                                                                     final Windows<W> windows,\n+                                                                                    final SlidingWindows slidingWindows,\n                                                                                     final SessionWindows sessionWindows,\n                                                                                     final Merger<? super K, VOut> sessionMerger) {\n-\n         final ProcessorSupplier<K, ?> kStreamAggregate;\n \n-        if (windows == null && sessionWindows == null) {\n+        if (windows == null && slidingWindows == null && sessionWindows == null && sessionMerger == null) {\n             kStreamAggregate = new KStreamAggregate<>(storeBuilder.name(), initializer, aggregator);\n-        } else if (windows != null && sessionWindows == null) {\n+        } else if (windows != null && slidingWindows == null && sessionWindows == null && sessionMerger == null) {\n             kStreamAggregate = new KStreamWindowAggregate<>(windows, storeBuilder.name(), initializer, aggregator);\n-        } else if (windows == null && sessionMerger != null) {\n-            kStreamAggregate = new KStreamSessionWindowAggregate<>(sessionWindows, storeBuilder.name(), initializer, aggregator, sessionMerger);\n+        } else if (windows == null && slidingWindows != null && sessionWindows == null && sessionMerger == null) {\n+            kStreamAggregate = new KStreamSlidingWindowAggregate<>(slidingWindows, storeBuilder.name(), initializer, aggregator);\n+        } else if (windows == null && slidingWindows == null && sessionWindows != null) {\n+            if (sessionMerger == null) {\n+                throw new IllegalArgumentException(\"sessionMerger cannot be null for sessionWindows\");\n+            } else {\n+                kStreamAggregate = new KStreamSessionWindowAggregate<>(sessionWindows, storeBuilder.name(), initializer, aggregator, sessionMerger);\n+            }\n         } else {\n-            throw new IllegalArgumentException(\"must include windows OR sessionWindows + sessionMerger OR all must be null\");\n+            throw new IllegalArgumentException(\"must include windows OR slidingWindows OR sessionWindows + sessionMerger OR all must be null\");", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwMDQwMw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465400403", "bodyText": "can you revert the line changes here and below? Nothing wrong with them, but the fewer lines/classes changed in the PR, the better", "author": "ableegoldman", "createdAt": "2020-08-05T00:18:42Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamWindowAggregate.java", "diffHunk": "@@ -120,7 +120,6 @@ public void process(final K key, final V value) {\n             final long timestamp = context().timestamp();\n             observedStreamTime = Math.max(observedStreamTime, timestamp);\n             final long closeTime = observedStreamTime - windows.gracePeriodMs();\n-", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwMTE4OQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465401189", "bodyText": "Think you missed changing this in the cogrouped class, this should be 2*timeDifference right?", "author": "ableegoldman", "createdAt": "2020-08-05T00:21:40Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedCogroupedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+public class SlidingWindowedCogroupedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedCogroupedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+    private final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns;\n+\n+    SlidingWindowedCogroupedKStreamImpl(final SlidingWindows windows,\n+                                        final InternalStreamsBuilder builder,\n+                                        final Set<String> subTopologySourceNodes,\n+                                        final String name,\n+                                        final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                                        final StreamsGraphNode streamsGraphNode,\n+                                        final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns) {\n+        super(name, null, null, subTopologySourceNodes, streamsGraphNode, builder);\n+        //keySerde and valueSerde are null because there are many different groupStreams that they could be from\n+        this.windows = windows;\n+        this.aggregateBuilder = aggregateBuilder;\n+        this.groupPatterns = groupPatterns;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer) {\n+        return aggregate(initializer, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named) {\n+        return aggregate(initializer, named, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(\n+                materialized,\n+                builder,\n+                CogroupedKStreamImpl.AGGREGATE_NAME);\n+        return aggregateBuilder.build(\n+                groupPatterns,\n+                initializer,\n+                new NamedInternal(named),\n+                materialize(materializedInternal),\n+                materializedInternal.keySerde() != null ?\n+                        new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs())\n+                        : null,\n+                materializedInternal.valueSerde(),\n+                materializedInternal.queryableStoreName(),\n+                null,\n+                windows,\n+                null,\n+                null);\n+    }\n+\n+    private StoreBuilder<TimestampedWindowStore<K, V>> materialize(\n+            final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            final long retentionPeriod = materialized.retention().toMillis();\n+\n+            if ((windows.timeDifferenceMs() + windows.gracePeriodMs()) > retentionPeriod) {", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwMjQ3Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465402473", "bodyText": "I think we need a null check here like we have down in SlidingWindowedKStreamImpl#materialize", "author": "ableegoldman", "createdAt": "2020-08-05T00:26:16Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedCogroupedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+public class SlidingWindowedCogroupedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedCogroupedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+    private final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns;\n+\n+    SlidingWindowedCogroupedKStreamImpl(final SlidingWindows windows,\n+                                        final InternalStreamsBuilder builder,\n+                                        final Set<String> subTopologySourceNodes,\n+                                        final String name,\n+                                        final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                                        final StreamsGraphNode streamsGraphNode,\n+                                        final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns) {\n+        super(name, null, null, subTopologySourceNodes, streamsGraphNode, builder);\n+        //keySerde and valueSerde are null because there are many different groupStreams that they could be from\n+        this.windows = windows;\n+        this.aggregateBuilder = aggregateBuilder;\n+        this.groupPatterns = groupPatterns;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer) {\n+        return aggregate(initializer, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named) {\n+        return aggregate(initializer, named, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(\n+                materialized,\n+                builder,\n+                CogroupedKStreamImpl.AGGREGATE_NAME);\n+        return aggregateBuilder.build(\n+                groupPatterns,\n+                initializer,\n+                new NamedInternal(named),\n+                materialize(materializedInternal),\n+                materializedInternal.keySerde() != null ?\n+                        new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs())\n+                        : null,\n+                materializedInternal.valueSerde(),\n+                materializedInternal.queryableStoreName(),\n+                null,\n+                windows,\n+                null,\n+                null);\n+    }\n+\n+    private StoreBuilder<TimestampedWindowStore<K, V>> materialize(\n+            final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            final long retentionPeriod = materialized.retention().toMillis();", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwNzYzOA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465407638", "bodyText": "Should be no smaller than twice its window time difference...", "author": "ableegoldman", "createdAt": "2020-08-05T00:46:16Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.Reducer;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Set;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.AGGREGATE_NAME;\n+import static org.apache.kafka.streams.kstream.internals.KGroupedStreamImpl.REDUCE_NAME;\n+\n+public class SlidingWindowedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final GroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+\n+    SlidingWindowedKStreamImpl(final SlidingWindows windows,\n+                               final InternalStreamsBuilder builder,\n+                               final Set<String> subTopologySourceNodes,\n+                               final String name,\n+                               final Serde<K> keySerde,\n+                               final Serde<V> valueSerde,\n+                               final GroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                               final StreamsGraphNode streamsGraphNode) {\n+        super(name, keySerde, valueSerde, subTopologySourceNodes, streamsGraphNode, builder);\n+        this.windows = Objects.requireNonNull(windows, \"windows can't be null\");\n+        this.aggregateBuilder = aggregateBuilder;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count() {\n+        return count(NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named) {\n+        return doCount(named, Materialized.with(keySerde, Serdes.Long()));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        return count(NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, Long> count(final Named named, final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        return doCount(named, materialized);\n+    }\n+\n+    private KTable<Windowed<K>, Long> doCount(final Named named,\n+                                              final Materialized<K, Long, WindowStore<Bytes, byte[]>> materialized) {\n+        final MaterializedInternal<K, Long, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(Serdes.Long());\n+        }\n+\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.countInitializer, aggregateBuilder.countAggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator) {\n+        return aggregate(initializer, aggregator, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named) {\n+        return aggregate(initializer, aggregator, named, Materialized.with(keySerde, null));\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, aggregator, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public <VR> KTable<Windowed<K>, VR> aggregate(final Initializer<VR> initializer,\n+                                                  final Aggregator<? super K, ? super V, VR> aggregator,\n+                                                  final Named named,\n+                                                  final Materialized<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, AGGREGATE_NAME);\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        final String aggregateName = new NamedInternal(named).orElseGenerateWithPrefix(builder, AGGREGATE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(aggregateName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), initializer, aggregator),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer) {\n+        return reduce(reducer, NamedInternal.empty());\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer, final Named named) {\n+        return reduce(reducer, named, Materialized.with(keySerde, valueSerde));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return reduce(reducer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> reduce(final Reducer<V> reducer,\n+                                         final Named named,\n+                                         final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(reducer, \"reducer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal =\n+                new MaterializedInternal<>(materialized, builder, REDUCE_NAME);\n+\n+        if (materializedInternal.keySerde() == null) {\n+            materializedInternal.withKeySerde(keySerde);\n+        }\n+        if (materializedInternal.valueSerde() == null) {\n+            materializedInternal.withValueSerde(valueSerde);\n+        }\n+\n+        final String reduceName = new NamedInternal(named).orElseGenerateWithPrefix(builder, REDUCE_NAME);\n+\n+        return aggregateBuilder.build(\n+                new NamedInternal(reduceName),\n+                materialize(materializedInternal),\n+                new KStreamSlidingWindowAggregate<>(windows, materializedInternal.storeName(), aggregateBuilder.reduceInitializer, aggregatorForReducer(reducer)),\n+                materializedInternal.queryableStoreName(),\n+                materializedInternal.keySerde() != null ? new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs()) : null,\n+                materializedInternal.valueSerde());\n+    }\n+\n+    private <VR> StoreBuilder<TimestampedWindowStore<K, VR>> materialize(final MaterializedInternal<K, VR, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {\n+            final long retentionPeriod = materialized.retention() != null ? materialized.retention().toMillis() : windows.gracePeriodMs() + 2 * windows.timeDifferenceMs();\n+\n+            if ((windows.timeDifferenceMs() * 2 + windows.gracePeriodMs()) > retentionPeriod) {\n+                throw new IllegalArgumentException(\"The retention period of the window store \"\n+                        + name + \" must be no smaller than its window time difference plus the grace period.\"", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMjQ4Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465412482", "bodyText": "Can you leave a TODO here to make sure we remember to change this to reverseFetch? Seems unlikely we'd forget, but you never know", "author": "ableegoldman", "createdAt": "2020-08-05T01:03:59Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNDg2Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465414863", "bodyText": "We don't technically need continue at the end of each condition, right?", "author": "ableegoldman", "createdAt": "2020-08-05T01:13:02Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxODk0Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465418942", "bodyText": "We need to check leftWinAgg for null, right? Also, is it ever possible for leftWinAgg to be non-null but not satisfy this condition? Maybe we can just check leftWinAgg != null and if so, then assert that leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs() is always true (eg throw an IllegalStateException if it's not)", "author": "ableegoldman", "createdAt": "2020-08-05T01:28:01Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc1MTU5Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465751597", "bodyText": "Hmmmm yeah, I think if there's something in the left window then we will always initialize leftWinAgg to something other than null, good catch. We essentially check leftWinAgg.timestamp() < timestamp in the while loop, so I don't think that should cause a problem, and leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs() will never be true because that window would be out of range, right? And we only take the first left agg. Long way of saying, I think we can do the null check and nothing else but will know slightly more when we can test it", "author": "lct45", "createdAt": "2020-08-05T14:05:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxODk0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1MjgyNQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466052825", "bodyText": "Sounds good. We definitely need the null check just to avoid getting an NPE, but whether we only need the null check is something we should put to the test", "author": "ableegoldman", "createdAt": "2020-08-05T23:08:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxODk0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxOTUzMQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465419531", "bodyText": "Can we remove the Math.max thing for now and just drop records that are too early for us to process for now?", "author": "ableegoldman", "createdAt": "2020-08-05T01:30:09Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTE5MA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465421190", "bodyText": "Just a note to other reviewers: we're planning to revisit the issue of \"early\" records later and are just dropping them for now to make the general algorithm easier to review and understand. It needs some special handling for the edge case of records that arrive earlier than the full sliding window due to the inability to store windows with negative start times", "author": "ableegoldman", "createdAt": "2020-08-05T01:36:28Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxNzM1OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468217358", "bodyText": "#9157", "author": "ableegoldman", "createdAt": "2020-08-10T22:18:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTE5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYyNzgwMA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478627800", "bodyText": "Is this condition supposed to be checking whether records are \"early\" with respect to now? It looks like it should be:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if (timestamp < windows.timeDifferenceMs()) {\n          \n          \n            \n                        if (timestamp < (observedStreamTime - windows.timeDifferenceMs())) {", "author": "vvcephei", "createdAt": "2020-08-27T18:53:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTE5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcxNTY2NA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478715664", "bodyText": "No, the condition is correct. In this context \"early\" just means \"within timeDifferenceMs of the zero timestamp\". We need some special handling to cover this full range of all record timestamps due to the inability to store negative timestamps. This algorithm works correctly for all records outside of this regardless of \"now\"", "author": "ableegoldman", "createdAt": "2020-08-27T21:50:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTE5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcxNjc2OQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478716769", "bodyText": "To be honest, it might not be so bad to just leave things as is and drop early records, since any sensible timestamps are unlikely to be that close to the epoch. But I do believe users may want to use lower timestamps in their unit testing (1598565116374 is not a very human readable number) and would be surprised to see these records just dropped.", "author": "ableegoldman", "createdAt": "2020-08-27T21:53:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTE5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQyOTQxMA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r480429410", "bodyText": "Thanks for the confirmation! I agree with your thinking.", "author": "vvcephei", "createdAt": "2020-08-31T22:16:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTE5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTI2NQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465421265", "bodyText": "nit: put this on one line", "author": "ableegoldman", "createdAt": "2020-08-05T01:36:42Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTUxNA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465421514", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        //to keep find the left type window closest to the record\n          \n          \n            \n                        // keep the left type window closest to the record", "author": "ableegoldman", "createdAt": "2020-08-05T01:37:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            //to keep find the left type window closest to the record", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTY1Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465421657", "bodyText": "comment doesn't seem to match the query bounds (missing a +1?)", "author": "ableegoldman", "createdAt": "2020-08-05T01:38:18Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTg4MA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465421880", "bodyText": "Is there any reason this wouldn't just be window.start + timeDifferenceMs?", "author": "ableegoldman", "createdAt": "2020-08-05T01:39:08Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            // to catch the current record's right window, if it exists, without more calls to the store\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+                    final long endTime = next.key.window().end();", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc1NjM1Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465756356", "bodyText": "Nope, since we aren't storing truncated windows", "author": "lct45", "createdAt": "2020-08-05T14:12:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTg4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1MTc3OQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r466051779", "bodyText": "Ok cool, just checking. Either endTime = window.start + timeDifferenceMs or endTime = window.end is fine as long as we're consistent (I guess we only define it in two places, the forward and reverse algorithms?)", "author": "ableegoldman", "createdAt": "2020-08-05T23:05:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyMTg4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyNDM0Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465424346", "bodyText": "Same here, let's not pin the start time to 0 for now and just drop the early records", "author": "ableegoldman", "createdAt": "2020-08-05T01:48:06Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            // to catch the current record's right window, if it exists, without more calls to the store\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+                    final long endTime = next.key.window().end();\n+                    final long startTime = next.key.window().start();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                        continue;\n+                    } else if (endTime == timestamp) {\n+                        leftWinAlreadyCreated = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else {\n+                        rightWinAlreadyCreated = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (!windowStartTimes.contains(rightWinStart)) {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }\n+            }\n+\n+            //create left window for new record\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //there's a right window that the new record could create --> new record's left window is not empty\n+                if (latestLeftTypeWindow != null) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyNTA5MA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r465425090", "bodyText": "Yeah especially since we use the same condition for both the forward and reverse case, let's just pull the rightWinAgg != null && rightWinAgg.timestamp() > timestamp out into a separate method", "author": "ableegoldman", "createdAt": "2020-08-05T01:50:32Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,391 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+        private boolean reverseIteratorImplemented = false;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            if (reverseIteratorImplemented) {\n+                processReverse(key, value);\n+            } else {\n+                processInOrder(key, value);\n+            }\n+        }\n+\n+        public void processReverse(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            try (\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                //if we've already seen the window with the closest start time to the record\n+                boolean foundRightWinAgg = false;\n+                //if we've already seen the window with the closest end time to the record\n+                boolean foundLeftWinAgg = false;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+\n+                    //determine if current record's right window exists, will only be true at most once, on the first pass\n+                    if (next.key.window().start() == timestamp + 1) {\n+                        rightWinAlreadyCreated = true;\n+                        continue;\n+                    } else if (next.key.window().end() > timestamp) {\n+                        if (!foundRightWinAgg) {\n+                            foundRightWinAgg = true;\n+                            rightWinAgg = next.value;\n+                        }\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (next.key.window().end() == timestamp) {\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        leftWinAlreadyCreated = true;\n+                        continue;\n+                    } else {\n+                        if (!foundLeftWinAgg) {\n+                            leftWinAgg = next.value;\n+                            foundLeftWinAgg = true;\n+                        }\n+                        //If it's a left window, there is a record at this window's end time who may need a corresponding right window\n+                        if (isLeftWindow(next)) {\n+                            final long rightWinStart = next.key.window().end() + 1;\n+                            if (!windowStartTimes.contains(rightWinStart)) {\n+                                final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                            }\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+            //create the left window of the current record if it's not created\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //confirms that the left window contains more than the current record\n+                if (leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    //left window just contains the current record\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create the right window for the current record, if need be\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        public void processInOrder(final K key, final V value) {\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            observedStreamTime = Math.max(observedStreamTime,\n+                    timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            //to keep find the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                    //Fetch all the windows that have a start time <= timestamp and >= timestamp-2*timeDifference\n+                    final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                            key,\n+                            key,\n+                            timestamp - 2 * windows.timeDifferenceMs(),\n+                            // to catch the current record's right window, if it exists, without more calls to the store\n+                            timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+                    final long endTime = next.key.window().end();\n+                    final long startTime = next.key.window().start();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                        continue;\n+                    } else if (endTime == timestamp) {\n+                        leftWinAlreadyCreated = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                        continue;\n+                    } else {\n+                        rightWinAlreadyCreated = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (!windowStartTimes.contains(rightWinStart)) {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }\n+            }\n+\n+            //create left window for new record\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //there's a right window that the new record could create --> new record's left window is not empty\n+                if (latestLeftTypeWindow != null) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(Math.max(0, timestamp - windows.timeDifferenceMs()), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create right window for new record\n+            if (!rightWinAlreadyCreated && rightWinAgg != null && rightWinAgg.timestamp() > timestamp) {", "originalCommit": "dc2f65f711a54e995094a1885df1dc728c479b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9cca939e7b54f60cbbf0fc8abdd0f94fbbe19f45", "url": "https://github.com/apache/kafka/commit/9cca939e7b54f60cbbf0fc8abdd0f94fbbe19f45", "message": "sophie's reviews", "committedDate": "2020-08-05T14:20:20Z", "type": "commit"}, {"oid": "e5a0d4b4a60f55d2ee7f412486ed8b567d493e25", "url": "https://github.com/apache/kafka/commit/e5a0d4b4a60f55d2ee7f412486ed8b567d493e25", "message": "cleaning up pr", "committedDate": "2020-08-05T15:37:05Z", "type": "commit"}, {"oid": "0afd88e97047c97e91e6d6273eb909e76eb782f4", "url": "https://github.com/apache/kafka/commit/0afd88e97047c97e91e6d6273eb909e76eb782f4", "message": "Merge branch 'trunk' of github.com:apache/kafka into slidingwindows", "committedDate": "2020-08-05T18:54:34Z", "type": "commit"}, {"oid": "24d91d8c33d3d3660f548623d6bcc7ef727d0862", "url": "https://github.com/apache/kafka/commit/24d91d8c33d3d3660f548623d6bcc7ef727d0862", "message": "updated tests", "committedDate": "2020-08-05T19:43:05Z", "type": "commit"}, {"oid": "65231139fb27f8e380d8bf6552a31c09f5fd28ff", "url": "https://github.com/apache/kafka/commit/65231139fb27f8e380d8bf6552a31c09f5fd28ff", "message": "grouped k stream and suppression tests", "committedDate": "2020-08-07T15:17:24Z", "type": "commit"}, {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "url": "https://github.com/apache/kafka/commit/34b3f5a2f6dee16a514698f39187eddcfab134bc", "message": "removing reverse iterator, to be implemented later", "committedDate": "2020-08-07T20:41:17Z", "type": "commit"}, {"oid": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "url": "https://github.com/apache/kafka/commit/34b3f5a2f6dee16a514698f39187eddcfab134bc", "message": "removing reverse iterator, to be implemented later", "committedDate": "2020-08-07T20:41:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxNjYyMA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468216620", "bodyText": "Comment on the reverse case left behind", "author": "ableegoldman", "createdAt": "2020-08-10T22:16:48Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,303 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error", "originalCommit": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxNzQzNw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468217437", "bodyText": "nit: extra line breaks", "author": "ableegoldman", "createdAt": "2020-08-10T22:19:00Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,303 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                          final String storeName,\n+                                          final Initializer<Agg> initializer,\n+                                          final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                    threadId,\n+                    context.taskId().toString(),\n+                    internalProcessorContext.currentNode().name(),\n+                    metrics\n+            );\n+            //catch unsupported operation error\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                    windowStore,\n+                    context,\n+                    new TimestampedCacheFlushListener<>(context),\n+                    sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                        \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                        \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                        value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            processInOrder(key, value, timestamp);\n+        }\n+\n+", "originalCommit": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxNzk4Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468217987", "bodyText": "You should check to make sure all of these are still needed. In particular I bet we can get rid of the CogroupedStreamAggregateBuilder suppression once your cleanup PR is merged and this one is rebased", "author": "ableegoldman", "createdAt": "2020-08-10T22:20:29Z", "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -163,6 +163,12 @@\n     <suppress checks=\"(FinalLocalVariable|UnnecessaryParentheses|BooleanExpressionComplexity|CyclomaticComplexity|WhitespaceAfter|LocalVariableName)\"\n               files=\"Murmur3.java\"/>\n \n+    <suppress checks=\"(NPathComplexity|MethodLength|CyclomaticComplexity)\"\n+              files=\"KStreamSlidingWindowAggregate.java\"/>\n+\n+    <suppress checks=\"(CyclomaticComplexity)\"\n+              files=\"CogroupedStreamAggregateBuilder.java\"/>\n+", "originalCommit": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxODU0MA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468218540", "bodyText": "I think we usually leave the arguments on the same line as the method declaration (even if that line ends up way too long)", "author": "ableegoldman", "createdAt": "2020-08-10T22:22:02Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedCogroupedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+public class SlidingWindowedCogroupedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedCogroupedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+    private final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns;\n+\n+    SlidingWindowedCogroupedKStreamImpl(final SlidingWindows windows,\n+                                        final InternalStreamsBuilder builder,\n+                                        final Set<String> subTopologySourceNodes,\n+                                        final String name,\n+                                        final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                                        final StreamsGraphNode streamsGraphNode,\n+                                        final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns) {\n+        super(name, null, null, subTopologySourceNodes, streamsGraphNode, builder);\n+        //keySerde and valueSerde are null because there are many different groupStreams that they could be from\n+        this.windows = windows;\n+        this.aggregateBuilder = aggregateBuilder;\n+        this.groupPatterns = groupPatterns;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer) {\n+        return aggregate(initializer, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named) {\n+        return aggregate(initializer, named, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(\n+                materialized,\n+                builder,\n+                CogroupedKStreamImpl.AGGREGATE_NAME);\n+        return aggregateBuilder.build(\n+                groupPatterns,\n+                initializer,\n+                new NamedInternal(named),\n+                materialize(materializedInternal),\n+                materializedInternal.keySerde() != null ?\n+                        new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs())\n+                        : null,\n+                materializedInternal.valueSerde(),\n+                materializedInternal.queryableStoreName(),\n+                null,\n+                windows,\n+                null,\n+                null);\n+    }\n+\n+    private StoreBuilder<TimestampedWindowStore<K, V>> materialize(\n+            final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materialized) {", "originalCommit": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxOTExMw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468219113", "bodyText": "Kind of hard to tell, but is the alignment in this method a bit off?  Might be good to just highlight and auto-indent everything, intellij will take care of any issues if it's configured properly", "author": "ableegoldman", "createdAt": "2020-08-10T22:23:34Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImpl.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedCogroupedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.internals.graph.StreamsGraphNode;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import java.time.Duration;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+public class SlidingWindowedCogroupedKStreamImpl<K, V> extends AbstractStream<K, V> implements TimeWindowedCogroupedKStream<K, V> {\n+    private final SlidingWindows windows;\n+    private final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder;\n+    private final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns;\n+\n+    SlidingWindowedCogroupedKStreamImpl(final SlidingWindows windows,\n+                                        final InternalStreamsBuilder builder,\n+                                        final Set<String> subTopologySourceNodes,\n+                                        final String name,\n+                                        final CogroupedStreamAggregateBuilder<K, V> aggregateBuilder,\n+                                        final StreamsGraphNode streamsGraphNode,\n+                                        final Map<KGroupedStreamImpl<K, ?>, Aggregator<? super K, ? super Object, V>> groupPatterns) {\n+        super(name, null, null, subTopologySourceNodes, streamsGraphNode, builder);\n+        //keySerde and valueSerde are null because there are many different groupStreams that they could be from\n+        this.windows = windows;\n+        this.aggregateBuilder = aggregateBuilder;\n+        this.groupPatterns = groupPatterns;\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer) {\n+        return aggregate(initializer, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        return aggregate(initializer, NamedInternal.empty(), materialized);\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named) {\n+        return aggregate(initializer, named, Materialized.with(null, null));\n+    }\n+\n+    @Override\n+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,\n+                                            final Named named,\n+                                            final Materialized<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        Objects.requireNonNull(initializer, \"initializer can't be null\");\n+        Objects.requireNonNull(named, \"named can't be null\");\n+        Objects.requireNonNull(materialized, \"materialized can't be null\");\n+        final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materializedInternal = new MaterializedInternal<>(\n+                materialized,\n+                builder,\n+                CogroupedKStreamImpl.AGGREGATE_NAME);\n+        return aggregateBuilder.build(\n+                groupPatterns,\n+                initializer,\n+                new NamedInternal(named),\n+                materialize(materializedInternal),\n+                materializedInternal.keySerde() != null ?\n+                        new FullTimeWindowedSerde<>(materializedInternal.keySerde(), windows.timeDifferenceMs())\n+                        : null,\n+                materializedInternal.valueSerde(),\n+                materializedInternal.queryableStoreName(),\n+                null,\n+                windows,\n+                null,\n+                null);\n+    }\n+\n+    private StoreBuilder<TimestampedWindowStore<K, V>> materialize(\n+            final MaterializedInternal<K, V, WindowStore<Bytes, byte[]>> materialized) {\n+        WindowBytesStoreSupplier supplier = (WindowBytesStoreSupplier) materialized.storeSupplier();\n+        if (supplier == null) {", "originalCommit": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MDA2OQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468290069", "bodyText": "nit: extra spaces after the ->", "author": "ableegoldman", "createdAt": "2020-08-11T02:28:07Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+                groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .count()\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .reduce(MockReducer.STRING_ADDER)\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.with(Serdes.String(), Serdes.String()))\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+                Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"1\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"1+2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"3\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"10+20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"1\", 100L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"1+2\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"2\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"3\", 500L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"20\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"10+20\", 200L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeAggregated() {\n+        windowedStream.aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"0+1\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"0+1+2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"0+2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"0+3\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"0+20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"0+10+20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"0+10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"0+1\", 100L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+1+2\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"0+2\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"0+3\", 500L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+20\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"0+10+20\", 200L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"0+10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () ->    windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER));", "originalCommit": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MTU2Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468291563", "bodyText": "The input is the same for each test so the output is too, right? Maybe we can we pull all the output verification into a single method", "author": "ableegoldman", "createdAt": "2020-08-11T02:34:01Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+                groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .count()\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .reduce(MockReducer.STRING_ADDER)\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.with(Serdes.String(), Serdes.String()))\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+                Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"1\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"1+2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"3\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"10+20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"1\", 100L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"1+2\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"2\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"3\", 500L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"20\", 150L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"10+20\", 200L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeAggregated() {\n+        windowedStream.aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"0+1\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"0+1+2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"0+2\"),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"0+3\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"0+20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"0+10+20\"),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"0+10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"0+1\", 100L)),", "originalCommit": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODY1Mzk1MQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468653951", "bodyText": "I pulled it out for all except one, because there's one call to windowStore that returns a <Windowed<String>, Long> and the other calls to windowStore return a <Windowed<String>, String>", "author": "lct45", "createdAt": "2020-08-11T15:07:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MTU2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc5NjM1NA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468796354", "bodyText": "Update: the windows themselves are the same but the value is different for each test", "author": "lct45", "createdAt": "2020-08-11T18:53:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MTU2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MjcxMQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468292711", "bodyText": "Can we add some tests to verify the other Materialized properties, specifically the retention? You can just pick a single operator (eg reduce) and write a test to make sure data is available (only) within the retention period.\nAlso, do you think we can write a test to verify that the default retention is as expected when we don't specify it?", "author": "ableegoldman", "createdAt": "2020-08-11T02:38:04Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+                groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .count()\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .reduce(MockReducer.STRING_ADDER)\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.with(Serdes.String(), Serdes.String()))\n+                .toStream()\n+                .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+                equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+                equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+                equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+                equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+                equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+                supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                        .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+                equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+                Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                        .withKeySerde(Serdes.String())\n+                        .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                        driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                        StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                        KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")", "originalCommit": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODY5MDQyOQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468690429", "bodyText": "I'm not sure if I'm just missing something, but it doesn't look like there's a way to check what retention is. I created a test to make sure anything lower than our bound throws an exception, but I can't find anywhere the retention time is exposed for me to check what it's set to", "author": "lct45", "createdAt": "2020-08-11T15:58:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MjcxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgxNDM2OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468814368", "bodyText": "Yeah sorry I should have been more clear, I just meant push some data through and try to query the store to make sure it is/isn't there according to the retention period. You're right, it's not directly exposed anywhere", "author": "ableegoldman", "createdAt": "2020-08-11T19:27:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MjcxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MzI0NA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468293244", "bodyText": "This comment needs to be updated, looks like we do allow a grace period of zero in the code/tests", "author": "ableegoldman", "createdAt": "2020-08-11T02:40:05Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding windows are defined by a record's timestamp, with window size based on the given maximum time difference (inclusive) between\n+ * records in the same window and a given window grace period. While the window is sliding over the input data stream, a new window is\n+ * created each time a record enters the sliding window or a record drops out of the sliding window.\n+ * <p>\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * <ul>\n+ *     <li>window {@code [3000;8000]} contains [1] (created when first record enters the window)</li>\n+ *     <li>window {@code [4200;9200]} contains [1,2] (created when second record enters the window)</li>\n+ *     <li>window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)</li>\n+ *     <li>window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)</li>\n+ *     <li>window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)</li>\n+ * </ul>\n+ *<p>\n+ * Note that while SlidingWindows are of a fixed size, as are {@link TimeWindows}, the start and end points of the window\n+ * depend on when events occur in the stream (i.e., event timestamps), similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(SlidingWindows)\n+ * @see CogroupedKStream#windowedBy(SlidingWindows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period. Reject out-of-order events that arrive after {@code grace}.\n+     * A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size is < 0 or grace <= 0, or either can't be represented as {@code long milliseconds}", "originalCommit": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5MzkzMA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468293930", "bodyText": "assertThrows \ud83d\ude42", "author": "ableegoldman", "createdAt": "2020-08-11T02:42:39Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImplTest.java", "diffHunk": "@@ -145,6 +146,11 @@ public void shouldNotHaveNullWindowOnWindowedBySession() {\n         cogroupedStream.windowedBy((SessionWindows) null);\n     }\n \n+    @Test(expected = NullPointerException.class)", "originalCommit": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5NDE4MQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r468294181", "bodyText": "Awesome, thanks for cleaning up some of these older tests \ud83d\ude04", "author": "ableegoldman", "createdAt": "2020-08-11T02:43:41Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KGroupedStreamImplTest.java", "diffHunk": "@@ -77,82 +80,212 @@ public void before() {\n         groupedStream = stream.groupByKey(Grouped.with(Serdes.String(), Serdes.String()));\n     }\n \n-    @Test(expected = NullPointerException.class)", "originalCommit": "34b3f5a2f6dee16a514698f39187eddcfab134bc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1071098d77ebda24f4a5791ca449efbb0cdabd48", "url": "https://github.com/apache/kafka/commit/1071098d77ebda24f4a5791ca449efbb0cdabd48", "message": "sophie's comments and testing updates", "committedDate": "2020-08-12T15:54:25Z", "type": "commit"}, {"oid": "1071098d77ebda24f4a5791ca449efbb0cdabd48", "url": "https://github.com/apache/kafka/commit/1071098d77ebda24f4a5791ca449efbb0cdabd48", "message": "sophie's comments and testing updates", "committedDate": "2020-08-12T15:54:25Z", "type": "forcePushed"}, {"oid": "824f8702114bff71944a2004cf8eadbd7144c785", "url": "https://github.com/apache/kafka/commit/824f8702114bff71944a2004cf8eadbd7144c785", "message": "trunk updates", "committedDate": "2020-08-17T14:38:32Z", "type": "commit"}, {"oid": "1163c2faa84d3bb05c178ce67ecb047a92b9a054", "url": "https://github.com/apache/kafka/commit/1163c2faa84d3bb05c178ce67ecb047a92b9a054", "message": "cogrouped builder updates", "committedDate": "2020-08-17T14:44:58Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTc5NzQyMg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471797422", "bodyText": "Do we still need this one after the cleanup you did?", "author": "ableegoldman", "createdAt": "2020-08-17T21:57:26Z", "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -167,6 +167,9 @@\n     <suppress checks=\"(FinalLocalVariable|UnnecessaryParentheses|BooleanExpressionComplexity|CyclomaticComplexity|WhitespaceAfter|LocalVariableName)\"\n               files=\"Murmur3.java\"/>\n \n+    <suppress checks=\"(CyclomaticComplexity)\"\n+              files=\"CogroupedStreamAggregateBuilder.java\"/>", "originalCommit": "1163c2faa84d3bb05c178ce67ecb047a92b9a054", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQzNjQwMA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r472436400", "bodyText": "We don't!", "author": "lct45", "createdAt": "2020-08-18T19:41:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTc5NzQyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTc5ODI1Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471798256", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        final HashSet<Long> windowStartTimes = new HashSet<Long>();\n          \n          \n            \n                        final Set<Long> windowStartTimes = new HashSet<>();\n          \n      \n    \n    \n  \n\nAlso I think this set is pretty clearly named, so we probably don't need a comment for it", "author": "ableegoldman", "createdAt": "2020-08-17T21:59:31Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                         final String storeName,\n+                                         final Initializer<Agg> initializer,\n+                                         final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                threadId,\n+                context.taskId().toString(),\n+                internalProcessorContext.currentNode().name(),\n+                metrics\n+            );\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                windowStore,\n+                context,\n+                new TimestampedCacheFlushListener<>(context),\n+                sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                    \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                    \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            processInOrder(key, value, timestamp);\n+        }\n+\n+        public void processInOrder(final K key, final V value, final long timestamp) {\n+\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();", "originalCommit": "1163c2faa84d3bb05c178ce67ecb047a92b9a054", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTc5ODgwMQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471798801", "bodyText": "nit: can we use the full word Window in method names at least", "author": "ableegoldman", "createdAt": "2020-08-17T22:00:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                         final String storeName,\n+                                         final Initializer<Agg> initializer,\n+                                         final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                threadId,\n+                context.taskId().toString(),\n+                internalProcessorContext.currentNode().name(),\n+                metrics\n+            );\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                windowStore,\n+                context,\n+                new TimestampedCacheFlushListener<>(context),\n+                sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                    \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                    \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            processInOrder(key, value, timestamp);\n+        }\n+\n+        public void processInOrder(final K key, final V value, final long timestamp) {\n+\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final HashSet<Long> windowStartTimes = new HashSet<Long>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            // keep the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                    key,\n+                    key,\n+                    timestamp - 2 * windows.timeDifferenceMs(),\n+                    // to catch the current record's right window, if it exists, without more calls to the store\n+                    timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+                    final long startTime = next.key.window().start();\n+                    final long endTime = startTime + windows.timeDifferenceMs();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                    } else if (endTime == timestamp) {\n+                        leftWinAlreadyCreated = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                    } else {\n+                        rightWinAlreadyCreated = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (!windowStartTimes.contains(rightWinStart)) {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }\n+            }\n+\n+            //create left window for new record\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //there's a right window that the new record could create --> new record's left window is not empty\n+                if (latestLeftTypeWindow != null) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifferenceMs(), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create right window for new record\n+            if (!rightWinAlreadyCreated && rightWinIsNotEmpty(rightWinAgg, timestamp)) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        private boolean rightWinIsNotEmpty(final ValueAndTimestamp<Agg> rightWinAgg, final long timestamp) {", "originalCommit": "1163c2faa84d3bb05c178ce67ecb047a92b9a054", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTc5OTY5OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471799698", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void ShouldDropWindowsOutsideOfRetention() {\n          \n          \n            \n                public void shouldDropWindowsOutsideOfRetention() {", "author": "ableegoldman", "createdAt": "2020-08-17T22:03:01Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,438 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+            groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .count()\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .reduce(MockReducer.STRING_ADDER)\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.with(Serdes.String(), Serdes.String()))\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+            Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+            MockReducer.STRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeAggregated() {\n+        windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            MockAggregator.TOSTRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"0+1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"0+1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"0+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"0+3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"0+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"0+10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"0+10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"0+1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"0+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"0+3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"0+10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"0+10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            null,\n+            Materialized.as(\"store\")));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfNamedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Named) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnCountIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.count((Materialized<String, Long, WindowStore<Bytes, byte[]>>) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowIllegalArgumentWhenRetentionIsTooSmall() {\n+        assertThrows(IllegalArgumentException.class, () -> windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized\n+                    .<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                    .withKeySerde(Serdes.String())\n+                    .withValueSerde(Serdes.String())\n+                    .withRetention(ofMillis(1L))\n+            )\n+        );\n+    }\n+\n+    @Test\n+    public void ShouldDropWindowsOutsideOfRetention() {", "originalCommit": "1163c2faa84d3bb05c178ce67ecb047a92b9a054", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwMTA4Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471801086", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final WindowBytesStoreSupplier storeSupplies = Stores.inMemoryWindowStore(\"aggregated\", ofMillis(1200L), ofMillis(100L), false);\n          \n          \n            \n                    final WindowBytesStoreSupplier storeSupplier = Stores.inMemoryWindowStore(\"aggregated\", ofMillis(1200L), ofMillis(100L), false);", "author": "ableegoldman", "createdAt": "2020-08-17T22:06:28Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,438 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+            groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .count()\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .reduce(MockReducer.STRING_ADDER)\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.with(Serdes.String(), Serdes.String()))\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+            Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+            MockReducer.STRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeAggregated() {\n+        windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            MockAggregator.TOSTRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"0+1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"0+1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"0+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"0+3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"0+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"0+10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"0+10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"0+1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"0+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"0+3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"0+10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"0+10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            null,\n+            Materialized.as(\"store\")));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfNamedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Named) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnCountIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.count((Materialized<String, Long, WindowStore<Bytes, byte[]>>) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowIllegalArgumentWhenRetentionIsTooSmall() {\n+        assertThrows(IllegalArgumentException.class, () -> windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized\n+                    .<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                    .withKeySerde(Serdes.String())\n+                    .withValueSerde(Serdes.String())\n+                    .withRetention(ofMillis(1L))\n+            )\n+        );\n+    }\n+\n+    @Test\n+    public void ShouldDropWindowsOutsideOfRetention() {\n+        final WindowBytesStoreSupplier storeSupplies = Stores.inMemoryWindowStore(\"aggregated\", ofMillis(1200L), ofMillis(100L), false);", "originalCommit": "1163c2faa84d3bb05c178ce67ecb047a92b9a054", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwMTc0Mw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471801743", "bodyText": "nit: you could use the version of fetch that just takes a single key instead of a key range, since there's only one key here", "author": "ableegoldman", "createdAt": "2020-08-17T22:08:09Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,438 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+            groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .count()\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .reduce(MockReducer.STRING_ADDER)\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.with(Serdes.String(), Serdes.String()))\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+            Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+            MockReducer.STRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeAggregated() {\n+        windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            MockAggregator.TOSTRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"0+1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"0+1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"0+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"0+3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"0+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"0+10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"0+10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"0+1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"0+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"0+3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"0+10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"0+10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            null,\n+            Materialized.as(\"store\")));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfNamedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Named) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnCountIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.count((Materialized<String, Long, WindowStore<Bytes, byte[]>>) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowIllegalArgumentWhenRetentionIsTooSmall() {\n+        assertThrows(IllegalArgumentException.class, () -> windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized\n+                    .<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                    .withKeySerde(Serdes.String())\n+                    .withValueSerde(Serdes.String())\n+                    .withRetention(ofMillis(1L))\n+            )\n+        );\n+    }\n+\n+    @Test\n+    public void ShouldDropWindowsOutsideOfRetention() {\n+        final WindowBytesStoreSupplier storeSupplies = Stores.inMemoryWindowStore(\"aggregated\", ofMillis(1200L), ofMillis(100L), false);\n+        windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            MockAggregator.TOSTRING_ADDER,\n+            Materialized.<String, String>as(storeSupplies)\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String())\n+                .withCachingDisabled());\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(TOPIC, new StringSerializer(), new StringSerializer());\n+\n+            inputTopic.pipeInput(\"1\", \"2\", 100L);\n+            inputTopic.pipeInput(\"1\", \"3\", 500L);\n+            inputTopic.pipeInput(\"1\", \"4\", 1000L);\n+            inputTopic.pipeInput(\"1\", \"5\", 2000L);\n+\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"1\", ofEpochMilli(0), ofEpochMilli(10000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(900, 1000)), \"0+4\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(1900, 2000)), \"0+5\"))));", "originalCommit": "1163c2faa84d3bb05c178ce67ecb047a92b9a054", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ0MDA0NQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r472440045", "bodyText": "That fetch only returns a WindowStoreIterator instead of a KeyValueIterator, which I don't think is a huge deal but we wouldn't get the start/end time of the window which is nice to have for the test", "author": "lct45", "createdAt": "2020-08-18T19:48:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwMTc0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ0MTgwMg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r472441802", "bodyText": "Oh right, forgot that it doesn't have the window times either. Nevermind then", "author": "ableegoldman", "createdAt": "2020-08-18T19:51:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwMTc0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyMDExNQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471820115", "bodyText": "Can we insert one that's like right on the border of the retention period? So if the streamtime at the end is 2,000 then the window cut off is 800 (or start time of 700), and verify that anything starting before 699 is gone and everything after that is there.", "author": "ableegoldman", "createdAt": "2020-08-17T23:01:12Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java", "diffHunk": "@@ -0,0 +1,438 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Named;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.TimeWindowedKStream;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowBytesStoreSupplier;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.time.Instant.ofEpochMilli;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertThrows;\n+\n+public class SlidingWindowedKStreamImplTest {\n+\n+    private static final String TOPIC = \"input\";\n+    private final StreamsBuilder builder = new StreamsBuilder();\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private TimeWindowedKStream<String, String> windowedStream;\n+\n+    @Before\n+    public void before() {\n+        final KStream<String, String> stream = builder.stream(TOPIC, Consumed.with(Serdes.String(), Serdes.String()));\n+        windowedStream = stream.\n+            groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(100L), ofMillis(1000L)));\n+    }\n+\n+    @Test\n+    public void shouldCountSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, Long> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .count()\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(1L, 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(2L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(1L, 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(2L, 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(1L, 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(1L, 200L)));\n+    }\n+\n+    @Test\n+    public void shouldReduceSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .reduce(MockReducer.STRING_ADDER)\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldAggregateSlidingWindows() {\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.with(Serdes.String(), Serdes.String()))\n+            .toStream()\n+            .process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+        }\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(0L, 100L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1\", 100L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(101L, 201L))),\n+            equalTo(ValueAndTimestamp.make(\"0+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+1+2\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"1\", new TimeWindow(400L, 500L))),\n+            equalTo(ValueAndTimestamp.make(\"0+3\", 500L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(100L, 200L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10+20\", 200L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(50L, 150L))),\n+            equalTo(ValueAndTimestamp.make(\"0+20\", 150L)));\n+        assertThat(\n+            supplier.theCapturedProcessor().lastValueAndTimestampPerKey\n+                .get(new Windowed<>(\"2\", new TimeWindow(151L, 251L))),\n+            equalTo(ValueAndTimestamp.make(\"0+10\", 200L)));\n+    }\n+\n+    @Test\n+    public void shouldMaterializeCount() {\n+        windowedStream.count(\n+            Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"count-store\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.Long()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, Long> windowStore = driver.getWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, Long>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), 1L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), 2L),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), 1L))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"count-store\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(1L, 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(2L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(1L, 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(1L, 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(2L, 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(1L, 200L)))));            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeReduced() {\n+        windowedStream.reduce(\n+            MockReducer.STRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"reduced\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"reduced\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldMaterializeAggregated() {\n+        windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            MockAggregator.TOSTRING_ADDER,\n+            Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String()));\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            processData(driver);\n+            {\n+                final WindowStore<String, String> windowStore = driver.getWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, String>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), \"0+1\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), \"0+1+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), \"0+2\"),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), \"0+3\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), \"0+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), \"0+10+20\"),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), \"0+10\"))));\n+            }\n+            {\n+                final WindowStore<String, ValueAndTimestamp<Long>> windowStore =\n+                    driver.getTimestampedWindowStore(\"aggregated\");\n+                final List<KeyValue<Windowed<String>, ValueAndTimestamp<Long>>> data =\n+                    StreamsTestUtils.toList(windowStore.fetch(\"1\", \"2\", ofEpochMilli(0), ofEpochMilli(1000L)));\n+                assertThat(data, equalTo(Arrays.asList(\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(0, 100)), ValueAndTimestamp.make(\"0+1\", 100L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+1+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(101, 201)), ValueAndTimestamp.make(\"0+2\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"1\", new TimeWindow(400, 500)), ValueAndTimestamp.make(\"0+3\", 500L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(50, 150)), ValueAndTimestamp.make(\"0+20\", 150L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(100, 200)), ValueAndTimestamp.make(\"0+10+20\", 200L)),\n+                    KeyValue.pair(new Windowed<>(\"2\", new TimeWindow(151, 251)), ValueAndTimestamp.make(\"0+10\", 200L)))));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfInitializerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(null, MockAggregator.TOSTRING_ADDER, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfAggregatorIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            null,\n+            Materialized.as(\"store\")));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedAggregateIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.aggregate(MockInitializer.STRING_INIT, MockAggregator.TOSTRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnMaterializedReduceIfReducerIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(null, Materialized.as(\"store\")));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Materialized) null));\n+    }\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowNullPointerOnMaterializedReduceIfNamedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.reduce(MockReducer.STRING_ADDER, (Named) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowNullPointerOnCountIfMaterializedIsNull() {\n+        assertThrows(NullPointerException.class, () -> windowedStream.count((Materialized<String, Long, WindowStore<Bytes, byte[]>>) null));\n+    }\n+\n+    @Test\n+    public void shouldThrowIllegalArgumentWhenRetentionIsTooSmall() {\n+        assertThrows(IllegalArgumentException.class, () -> windowedStream\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized\n+                    .<String, String, WindowStore<Bytes, byte[]>>as(\"aggregated\")\n+                    .withKeySerde(Serdes.String())\n+                    .withValueSerde(Serdes.String())\n+                    .withRetention(ofMillis(1L))\n+            )\n+        );\n+    }\n+\n+    @Test\n+    public void ShouldDropWindowsOutsideOfRetention() {\n+        final WindowBytesStoreSupplier storeSupplies = Stores.inMemoryWindowStore(\"aggregated\", ofMillis(1200L), ofMillis(100L), false);\n+        windowedStream.aggregate(\n+            MockInitializer.STRING_INIT,\n+            MockAggregator.TOSTRING_ADDER,\n+            Materialized.<String, String>as(storeSupplies)\n+                .withKeySerde(Serdes.String())\n+                .withValueSerde(Serdes.String())\n+                .withCachingDisabled());\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(TOPIC, new StringSerializer(), new StringSerializer());\n+\n+            inputTopic.pipeInput(\"1\", \"2\", 100L);\n+            inputTopic.pipeInput(\"1\", \"3\", 500L);\n+            inputTopic.pipeInput(\"1\", \"4\", 1000L);", "originalCommit": "1163c2faa84d3bb05c178ce67ecb047a92b9a054", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyMzA2MQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471823061", "bodyText": "For readability, could we mark the final results for each window? We want to make sure all the intermediate results are as expected, but what we really care about is what we got in the end. It would just help to have the critical output easier to find and get oriented in the tests", "author": "ableegoldman", "createdAt": "2020-08-17T23:09:53Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,466 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @Test\n+    public void testAggBasic() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"1\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"3\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"1\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"1\", 21);\n+            inputTopic1.pipeInput(\"C\", \"1\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // A@10 left window created when A@10 processed", "originalCommit": "1163c2faa84d3bb05c178ce67ecb047a92b9a054", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyNDIwOQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471824209", "bodyText": "It might be nice to use different values for each record (at least within the same key). I don't think there are really any edge cases we should worry about when records have the same value so we may as well use a distinct one to make the tests a bit easier to read", "author": "ableegoldman", "createdAt": "2020-08-17T23:13:31Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,466 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @Test\n+    public void testAggBasic() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"2\", 12L);", "originalCommit": "1163c2faa84d3bb05c178ce67ecb047a92b9a054", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyNTQ1Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471825457", "bodyText": "I still don't exactly understand why we have a join test in the KStreamXXWindowAggregateTest, but thanks for adding it for sliding windows. I'm sure there was a good reason for it, probably long ago", "author": "ableegoldman", "createdAt": "2020-08-17T23:17:24Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,466 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @Test\n+    public void testAggBasic() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"1\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"1\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"3\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"1\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"1\", 21);\n+            inputTopic1.pipeInput(\"C\", \"1\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+1\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+1\", 20),\n+                        // A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+1\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+1+1\", 22),\n+                        // A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+1+3\", 20),\n+                        // A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+1+3\", 20),\n+                        // A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+1+1+3\", 22),\n+                        // A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+3\", 15),\n+                        // A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+1+1\", 22),\n+\n+                        // B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+2\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+2+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+2\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+2\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+2+2+2\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+2+1\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+2+1\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+1\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+2+2+2+1\", 19),\n+                        // B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+1+2\", 25),\n+                        // B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+2\", 25),\n+                        // B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+2+1+2\", 25),\n+                        // B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+2+2+2+3\", 18),\n+                        // B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+2+2+2+1+3\", 19),\n+                        // B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+2+1+3\", 19),\n+                        // B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+2+1+3\", 19),\n+                        // B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+2+2+3\", 14),\n+\n+                        // C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+3\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+4\", 15),\n+                        // C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+3+4\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+4+1\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+1\", 16),\n+                        // C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+3+4+1\", 16),\n+                        // C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+4+1+1\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+1+1\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+1\", 21),\n+                        // C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+3+4+1+1\", 21),\n+                        // C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+1+1+1\", 23),\n+                        // C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+1+1\", 23),\n+                        // C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+1\", 23),\n+                        // C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+4+1+1+1\", 23),\n+\n+                        // D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {", "originalCommit": "1163c2faa84d3bb05c178ce67ecb047a92b9a054", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcwMjUxNw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478702517", "bodyText": "The Achilles heel of implementing new KTable features has historically been that we forgot to test them in a context that required the ValueGetter to work properly, of which Join is a notable use case. I'd actually say it should be required for every KTable operator to have a test where it's the source of a Join. For stateless operators, we should test both with and without a Materialized argument on the operator.", "author": "vvcephei", "createdAt": "2020-08-27T21:20:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyNTQ1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcxNzEyNw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478717127", "bodyText": "Aha, so there was a good reason for it \ud83d\ude04", "author": "ableegoldman", "createdAt": "2020-08-27T21:53:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyNTQ1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgyNzM3MQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r471827371", "bodyText": "Sorry that I only just got to looking through this class \ud83d\ude1e . The tests here look good but can we add some more test coverage of possible edge cases? I know we can't test early records until the next PR, but we should probably have more than just the one test of the core functionality.\nI know it's really annoying to have to think through all the intermediate output, so maybe you can write a helper method that just grabs the final result of each window in the output? Then we could have a number of tests that go through a larger number of input records without you having to spend all day manually processing them yourself \ud83d\ude04", "author": "ableegoldman", "createdAt": "2020-08-17T23:23:35Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,466 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @Test\n+    public void testAggBasic() {", "originalCommit": "1163c2faa84d3bb05c178ce67ecb047a92b9a054", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "296a6c3035f7eda71729d5cec7e16778383b7fad", "url": "https://github.com/apache/kafka/commit/296a6c3035f7eda71729d5cec7e16778383b7fad", "message": "test updates and clean up", "committedDate": "2020-08-19T18:12:59Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk2NjQ2NQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r474966465", "bodyText": "Can we actually wrap the whole testProcessorRandomInput test in the try-catch? Or at least, everything after the initial setup? Would be nice to have the seed in case something weird happens during the processing itself", "author": "ableegoldman", "createdAt": "2020-08-21T20:59:10Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,579 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @Test\n+    public void testAggBasic() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+        builder\n+                .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(MockInitializer.STRING_INIT, MockAggregator.toStringInstance(\"+\"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()));\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(null, \"1\");\n+            assertThat(appender.getMessages(), hasItem(\"Skipping record due to null key or value. value=[1] topic=[topic] partition=[0] offset=[0]\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingExpiredWindowByGrace() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));\n+        stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(90L)))\n+                .aggregate(\n+                        () -> \"\",\n+                        MockAggregator.toStringInstance(\"+\"),\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()\n+                )\n+                .toStream()\n+                .map((key, value) -> new KeyValue<>(key.toString(), value))\n+                .to(\"output\");\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"k\", \"100\", 200L);\n+            inputTopic.pipeInput(\"k\", \"0\", 100L);\n+            inputTopic.pipeInput(\"k\", \"1\", 101L);\n+            inputTopic.pipeInput(\"k\", \"2\", 102L);\n+            inputTopic.pipeInput(\"k\", \"3\", 103L);\n+            inputTopic.pipeInput(\"k\", \"4\", 104L);\n+            inputTopic.pipeInput(\"k\", \"5\", 105L);\n+            inputTopic.pipeInput(\"k\", \"6\", 15L);\n+\n+            assertLatenessMetrics(driver, is(7.0), is(185.0), is(96.25));\n+\n+            assertThat(appender.getMessages(), hasItems(\n+                    // left window for k@100\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[90,100] expiration=[110] streamTime=[200]\",\n+                    // left window for k@101\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[91,101] expiration=[110] streamTime=[200]\",\n+                    // left window for k@102\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[92,102] expiration=[110] streamTime=[200]\",\n+                    // left window for k@103\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[93,103] expiration=[110] streamTime=[200]\",\n+                    // left window for k@104\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[94,104] expiration=[110] streamTime=[200]\",\n+                    // left window for k@105\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[95,105] expiration=[110] streamTime=[200]\",\n+                    // left window for k@15\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[15] window=[5,15] expiration=[110] streamTime=[200]\"\n+            ));\n+            final TestOutputTopic<String, String> outputTopic =\n+                    driver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+            assertThat(outputTopic.readRecord(), equalTo(new TestRecord<>(\"[k@190/200]\", \"+100\", null, 200L)));\n+            assertTrue(outputTopic.isEmpty());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testProcessorRandomInput() {\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(10000)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+\n+        final List<ValueAndTimestamp<String>> input = Arrays.asList(\n+            ValueAndTimestamp.make(\"A\", 10L),\n+            ValueAndTimestamp.make(\"A\", 15L),\n+            ValueAndTimestamp.make(\"A\", 16L),\n+            ValueAndTimestamp.make(\"A\", 18L),\n+            ValueAndTimestamp.make(\"A\", 30L),\n+            ValueAndTimestamp.make(\"A\", 40L),\n+            ValueAndTimestamp.make(\"A\", 55L),\n+            ValueAndTimestamp.make(\"A\", 56L),\n+            ValueAndTimestamp.make(\"A\", 58L),\n+            ValueAndTimestamp.make(\"A\", 58L),\n+            ValueAndTimestamp.make(\"A\", 62L),\n+            ValueAndTimestamp.make(\"A\", 63L),\n+            ValueAndTimestamp.make(\"A\", 63L),\n+            ValueAndTimestamp.make(\"A\", 63L),\n+            ValueAndTimestamp.make(\"A\", 76L),\n+            ValueAndTimestamp.make(\"A\", 77L),\n+            ValueAndTimestamp.make(\"A\", 80L)\n+        );\n+\n+        final long seed = new Random().nextLong();\n+        final Random shuffle = new Random(seed);\n+        Collections.shuffle(input, shuffle);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            for (int i = 0; i < input.size(); i++) {\n+                inputTopic1.pipeInput(\"A\", input.get(i).value(), input.get(i).timestamp());\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> results = new HashMap<>();\n+\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (results.putIfAbsent(start, valueAndTimestamp) != null) {\n+                results.replace(start, valueAndTimestamp);\n+            }\n+        }\n+        randomEqualityCheck(results, seed);\n+    }\n+\n+    private void randomEqualityCheck(final Map<Long, ValueAndTimestamp<String>> actual, final Long seed) {\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+A\", 10L));\n+        expected.put(11L, ValueAndTimestamp.make(\"0+A+A+A\", 18L));\n+        expected.put(5L, ValueAndTimestamp.make(\"0+A+A\", 15L));\n+        expected.put(16L, ValueAndTimestamp.make(\"0+A+A\", 18L));\n+        expected.put(6L, ValueAndTimestamp.make(\"0+A+A+A\", 16L));\n+        expected.put(17L, ValueAndTimestamp.make(\"0+A\", 18L));\n+        expected.put(8L, ValueAndTimestamp.make(\"0+A+A+A+A\", 18L));\n+        expected.put(20L, ValueAndTimestamp.make(\"0+A\", 30L));\n+        expected.put(31L, ValueAndTimestamp.make(\"0+A\", 40L));\n+        expected.put(30L, ValueAndTimestamp.make(\"0+A+A\", 40L));\n+        expected.put(45L, ValueAndTimestamp.make(\"0+A\", 55L));\n+        expected.put(56L, ValueAndTimestamp.make(\"0+A+A+A+A+A+A+A\", 63L));\n+        expected.put(46L, ValueAndTimestamp.make(\"0+A+A\", 56L));\n+        expected.put(57L, ValueAndTimestamp.make(\"0+A+A+A+A+A+A\", 63L));\n+        expected.put(48L, ValueAndTimestamp.make(\"0+A+A+A+A\", 58L));\n+        expected.put(59L, ValueAndTimestamp.make(\"0+A+A+A+A\", 63L));\n+        expected.put(52L, ValueAndTimestamp.make(\"0+A+A+A+A+A\", 62L));\n+        expected.put(63L, ValueAndTimestamp.make(\"0+A+A+A\", 63L));\n+        expected.put(53L, ValueAndTimestamp.make(\"0+A+A+A+A+A+A+A+A\", 63L));\n+        expected.put(66L, ValueAndTimestamp.make(\"0+A\", 76L));\n+        expected.put(77L, ValueAndTimestamp.make(\"0+A+A\", 80L));\n+        expected.put(67L, ValueAndTimestamp.make(\"0+A+A\", 77L));\n+        expected.put(78L, ValueAndTimestamp.make(\"0+A\", 80L));\n+        expected.put(70L, ValueAndTimestamp.make(\"0+A+A+A\", 80L));\n+\n+        try {\n+            assertEquals(expected, actual);\n+        } catch (final AssertionError t) {\n+            throw new AssertionError(\n+                \"Assertion failed in randomized test. Reproduce with seed: \" + seed + \".\",\n+                t\n+            );\n+        } catch (final Throwable t) {", "originalCommit": "296a6c3035f7eda71729d5cec7e16778383b7fad", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk3NzgzNg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r474977836", "bodyText": "Just a minor note, can we order the expected results by window timestamp?", "author": "ableegoldman", "createdAt": "2020-08-21T21:28:42Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,579 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @Test\n+    public void testAggBasic() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+        builder\n+                .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(MockInitializer.STRING_INIT, MockAggregator.toStringInstance(\"+\"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()));\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(null, \"1\");\n+            assertThat(appender.getMessages(), hasItem(\"Skipping record due to null key or value. value=[1] topic=[topic] partition=[0] offset=[0]\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingExpiredWindowByGrace() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));\n+        stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(90L)))\n+                .aggregate(\n+                        () -> \"\",\n+                        MockAggregator.toStringInstance(\"+\"),\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()\n+                )\n+                .toStream()\n+                .map((key, value) -> new KeyValue<>(key.toString(), value))\n+                .to(\"output\");\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"k\", \"100\", 200L);\n+            inputTopic.pipeInput(\"k\", \"0\", 100L);\n+            inputTopic.pipeInput(\"k\", \"1\", 101L);\n+            inputTopic.pipeInput(\"k\", \"2\", 102L);\n+            inputTopic.pipeInput(\"k\", \"3\", 103L);\n+            inputTopic.pipeInput(\"k\", \"4\", 104L);\n+            inputTopic.pipeInput(\"k\", \"5\", 105L);\n+            inputTopic.pipeInput(\"k\", \"6\", 15L);\n+\n+            assertLatenessMetrics(driver, is(7.0), is(185.0), is(96.25));\n+\n+            assertThat(appender.getMessages(), hasItems(\n+                    // left window for k@100\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[90,100] expiration=[110] streamTime=[200]\",\n+                    // left window for k@101\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[91,101] expiration=[110] streamTime=[200]\",\n+                    // left window for k@102\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[92,102] expiration=[110] streamTime=[200]\",\n+                    // left window for k@103\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[93,103] expiration=[110] streamTime=[200]\",\n+                    // left window for k@104\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[94,104] expiration=[110] streamTime=[200]\",\n+                    // left window for k@105\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[95,105] expiration=[110] streamTime=[200]\",\n+                    // left window for k@15\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[15] window=[5,15] expiration=[110] streamTime=[200]\"\n+            ));\n+            final TestOutputTopic<String, String> outputTopic =\n+                    driver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+            assertThat(outputTopic.readRecord(), equalTo(new TestRecord<>(\"[k@190/200]\", \"+100\", null, 200L)));\n+            assertTrue(outputTopic.isEmpty());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testProcessorRandomInput() {\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(10000)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+\n+        final List<ValueAndTimestamp<String>> input = Arrays.asList(\n+            ValueAndTimestamp.make(\"A\", 10L),\n+            ValueAndTimestamp.make(\"A\", 15L),\n+            ValueAndTimestamp.make(\"A\", 16L),\n+            ValueAndTimestamp.make(\"A\", 18L),\n+            ValueAndTimestamp.make(\"A\", 30L),\n+            ValueAndTimestamp.make(\"A\", 40L),\n+            ValueAndTimestamp.make(\"A\", 55L),\n+            ValueAndTimestamp.make(\"A\", 56L),\n+            ValueAndTimestamp.make(\"A\", 58L),\n+            ValueAndTimestamp.make(\"A\", 58L),\n+            ValueAndTimestamp.make(\"A\", 62L),\n+            ValueAndTimestamp.make(\"A\", 63L),\n+            ValueAndTimestamp.make(\"A\", 63L),\n+            ValueAndTimestamp.make(\"A\", 63L),\n+            ValueAndTimestamp.make(\"A\", 76L),\n+            ValueAndTimestamp.make(\"A\", 77L),\n+            ValueAndTimestamp.make(\"A\", 80L)\n+        );\n+\n+        final long seed = new Random().nextLong();\n+        final Random shuffle = new Random(seed);\n+        Collections.shuffle(input, shuffle);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            for (int i = 0; i < input.size(); i++) {\n+                inputTopic1.pipeInput(\"A\", input.get(i).value(), input.get(i).timestamp());\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> results = new HashMap<>();\n+\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (results.putIfAbsent(start, valueAndTimestamp) != null) {\n+                results.replace(start, valueAndTimestamp);\n+            }\n+        }\n+        randomEqualityCheck(results, seed);\n+    }\n+\n+    private void randomEqualityCheck(final Map<Long, ValueAndTimestamp<String>> actual, final Long seed) {\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+A\", 10L));", "originalCommit": "296a6c3035f7eda71729d5cec7e16778383b7fad", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "bb609a89ec7543943d03a1939e61f8208847ce8e", "url": "https://github.com/apache/kafka/commit/bb609a89ec7543943d03a1939e61f8208847ce8e", "message": "randomized and small test improvements", "committedDate": "2020-08-24T17:07:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkxOTQxMw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r475919413", "bodyText": "nit: can you call this something a bit more direct, eg verifyRandomTestResults ?", "author": "ableegoldman", "createdAt": "2020-08-24T22:02:05Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,690 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testAggregateSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 15L);\n+            inputTopic.pipeInput(\"A\", \"3\", 20L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+1\", 10L));\n+        expected.put(5L, ValueAndTimestamp.make(\"0+1+2\", 15L));\n+        expected.put(10L, ValueAndTimestamp.make(\"0+1+2+3\", 20L));\n+        expected.put(11L, ValueAndTimestamp.make(\"0+2+3\", 20L));\n+        expected.put(12L, ValueAndTimestamp.make(\"0+2+3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"0+3+4\", 22L));\n+        expected.put(20L, ValueAndTimestamp.make(\"0+3+4+5\", 30L));\n+        expected.put(21L, ValueAndTimestamp.make(\"0+4+5\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"0+5\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testReduceSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 14L);\n+            inputTopic.pipeInput(\"A\", \"3\", 15L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 26L);\n+            inputTopic.pipeInput(\"A\", \"6\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"1\", 10L));\n+        expected.put(4L, ValueAndTimestamp.make(\"1+2\", 14L));\n+        expected.put(5L, ValueAndTimestamp.make(\"1+2+3\", 15L));\n+        expected.put(11L, ValueAndTimestamp.make(\"2+3\", 15L));\n+        expected.put(12L, ValueAndTimestamp.make(\"2+3+4\", 22L));\n+        expected.put(15L, ValueAndTimestamp.make(\"3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"4+5\", 26L));\n+        expected.put(20L, ValueAndTimestamp.make(\"4+5+6\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"5+6\", 30L));\n+        expected.put(27L, ValueAndTimestamp.make(\"6\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @Test\n+    public void testAggregateLargeInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+        builder\n+                .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(MockInitializer.STRING_INIT, MockAggregator.toStringInstance(\"+\"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()));\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(null, \"1\");\n+            assertThat(appender.getMessages(), hasItem(\"Skipping record due to null key or value. value=[1] topic=[topic] partition=[0] offset=[0]\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingExpiredWindowByGrace() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));\n+        stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(90L)))\n+                .aggregate(\n+                        () -> \"\",\n+                        MockAggregator.toStringInstance(\"+\"),\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()\n+                )\n+                .toStream()\n+                .map((key, value) -> new KeyValue<>(key.toString(), value))\n+                .to(\"output\");\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"k\", \"100\", 200L);\n+            inputTopic.pipeInput(\"k\", \"0\", 100L);\n+            inputTopic.pipeInput(\"k\", \"1\", 101L);\n+            inputTopic.pipeInput(\"k\", \"2\", 102L);\n+            inputTopic.pipeInput(\"k\", \"3\", 103L);\n+            inputTopic.pipeInput(\"k\", \"4\", 104L);\n+            inputTopic.pipeInput(\"k\", \"5\", 105L);\n+            inputTopic.pipeInput(\"k\", \"6\", 15L);\n+\n+            assertLatenessMetrics(driver, is(7.0), is(185.0), is(96.25));\n+\n+            assertThat(appender.getMessages(), hasItems(\n+                    // left window for k@100\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[90,100] expiration=[110] streamTime=[200]\",\n+                    // left window for k@101\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[91,101] expiration=[110] streamTime=[200]\",\n+                    // left window for k@102\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[92,102] expiration=[110] streamTime=[200]\",\n+                    // left window for k@103\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[93,103] expiration=[110] streamTime=[200]\",\n+                    // left window for k@104\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[94,104] expiration=[110] streamTime=[200]\",\n+                    // left window for k@105\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[95,105] expiration=[110] streamTime=[200]\",\n+                    // left window for k@15\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[15] window=[5,15] expiration=[110] streamTime=[200]\"\n+            ));\n+            final TestOutputTopic<String, String> outputTopic =\n+                    driver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+            assertThat(outputTopic.readRecord(), equalTo(new TestRecord<>(\"[k@190/200]\", \"+100\", null, 200L)));\n+            assertTrue(outputTopic.isEmpty());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testProcessorRandomInput() {\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(10000)))\n+            .aggregate(\n+                () -> \"\",\n+                (key, value, aggregate) -> {\n+                    aggregate += value;\n+                    final char[] ch = aggregate.toCharArray();\n+                    Arrays.sort(ch);\n+                    aggregate = String.valueOf(ch);\n+                    return aggregate;\n+                },\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        final long seed = new Random().nextLong();\n+        final Random shuffle = new Random(seed);\n+\n+        try {\n+\n+            final List<ValueAndTimestamp<String>> input = Arrays.asList(\n+                ValueAndTimestamp.make(\"A\", 10L),\n+                ValueAndTimestamp.make(\"B\", 15L),\n+                ValueAndTimestamp.make(\"C\", 16L),\n+                ValueAndTimestamp.make(\"D\", 18L),\n+                ValueAndTimestamp.make(\"E\", 30L),\n+                ValueAndTimestamp.make(\"F\", 40L),\n+                ValueAndTimestamp.make(\"G\", 55L),\n+                ValueAndTimestamp.make(\"H\", 56L),\n+                ValueAndTimestamp.make(\"I\", 58L),\n+                ValueAndTimestamp.make(\"J\", 58L),\n+                ValueAndTimestamp.make(\"K\", 62L),\n+                ValueAndTimestamp.make(\"L\", 63L),\n+                ValueAndTimestamp.make(\"M\", 63L),\n+                ValueAndTimestamp.make(\"N\", 63L),\n+                ValueAndTimestamp.make(\"O\", 76L),\n+                ValueAndTimestamp.make(\"P\", 77L),\n+                ValueAndTimestamp.make(\"Q\", 80L)\n+            );\n+\n+            Collections.shuffle(input, shuffle);\n+            try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+                final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+                for (int i = 0; i < input.size(); i++) {\n+                    inputTopic1.pipeInput(\"A\", input.get(i).value(), input.get(i).timestamp());\n+                }\n+            }\n+\n+            final Map<Long, ValueAndTimestamp<String>> results = new HashMap<>();\n+\n+            for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+                final Windowed<String> window = (Windowed<String>) entry.key();\n+                final Long start = window.window().start();\n+                final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+                if (results.putIfAbsent(start, valueAndTimestamp) != null) {\n+                    results.replace(start, valueAndTimestamp);\n+                }\n+            }\n+            randomEqualityCheck(results, seed);\n+        } catch (final AssertionError t) {\n+            throw new AssertionError(\n+                \"Assertion failed in randomized test. Reproduce with seed: \" + seed + \".\",\n+                t\n+            );\n+        } catch (final Throwable t) {\n+            final StringBuilder sb =\n+                new StringBuilder()\n+                    .append(\"Exception in randomized scenario. Reproduce with seed: \")\n+                    .append(seed)\n+                    .append(\".\");\n+            throw new AssertionError(sb.toString(), t);\n+        }\n+    }\n+\n+    private void randomEqualityCheck(final Map<Long, ValueAndTimestamp<String>> actual, final Long seed) {", "originalCommit": "bb609a89ec7543943d03a1939e61f8208847ce8e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkyMjc2MQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r475922761", "bodyText": "nit: testAggregateRandomInput to match up with other test names", "author": "ableegoldman", "createdAt": "2020-08-24T22:10:32Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,690 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testAggregateSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 15L);\n+            inputTopic.pipeInput(\"A\", \"3\", 20L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+1\", 10L));\n+        expected.put(5L, ValueAndTimestamp.make(\"0+1+2\", 15L));\n+        expected.put(10L, ValueAndTimestamp.make(\"0+1+2+3\", 20L));\n+        expected.put(11L, ValueAndTimestamp.make(\"0+2+3\", 20L));\n+        expected.put(12L, ValueAndTimestamp.make(\"0+2+3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"0+3+4\", 22L));\n+        expected.put(20L, ValueAndTimestamp.make(\"0+3+4+5\", 30L));\n+        expected.put(21L, ValueAndTimestamp.make(\"0+4+5\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"0+5\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testReduceSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 14L);\n+            inputTopic.pipeInput(\"A\", \"3\", 15L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 26L);\n+            inputTopic.pipeInput(\"A\", \"6\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"1\", 10L));\n+        expected.put(4L, ValueAndTimestamp.make(\"1+2\", 14L));\n+        expected.put(5L, ValueAndTimestamp.make(\"1+2+3\", 15L));\n+        expected.put(11L, ValueAndTimestamp.make(\"2+3\", 15L));\n+        expected.put(12L, ValueAndTimestamp.make(\"2+3+4\", 22L));\n+        expected.put(15L, ValueAndTimestamp.make(\"3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"4+5\", 26L));\n+        expected.put(20L, ValueAndTimestamp.make(\"4+5+6\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"5+6\", 30L));\n+        expected.put(27L, ValueAndTimestamp.make(\"6\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @Test\n+    public void testAggregateLargeInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+        builder\n+                .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(MockInitializer.STRING_INIT, MockAggregator.toStringInstance(\"+\"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()));\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(null, \"1\");\n+            assertThat(appender.getMessages(), hasItem(\"Skipping record due to null key or value. value=[1] topic=[topic] partition=[0] offset=[0]\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingExpiredWindowByGrace() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));\n+        stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(90L)))\n+                .aggregate(\n+                        () -> \"\",\n+                        MockAggregator.toStringInstance(\"+\"),\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()\n+                )\n+                .toStream()\n+                .map((key, value) -> new KeyValue<>(key.toString(), value))\n+                .to(\"output\");\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"k\", \"100\", 200L);\n+            inputTopic.pipeInput(\"k\", \"0\", 100L);\n+            inputTopic.pipeInput(\"k\", \"1\", 101L);\n+            inputTopic.pipeInput(\"k\", \"2\", 102L);\n+            inputTopic.pipeInput(\"k\", \"3\", 103L);\n+            inputTopic.pipeInput(\"k\", \"4\", 104L);\n+            inputTopic.pipeInput(\"k\", \"5\", 105L);\n+            inputTopic.pipeInput(\"k\", \"6\", 15L);\n+\n+            assertLatenessMetrics(driver, is(7.0), is(185.0), is(96.25));\n+\n+            assertThat(appender.getMessages(), hasItems(\n+                    // left window for k@100\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[90,100] expiration=[110] streamTime=[200]\",\n+                    // left window for k@101\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[91,101] expiration=[110] streamTime=[200]\",\n+                    // left window for k@102\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[92,102] expiration=[110] streamTime=[200]\",\n+                    // left window for k@103\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[93,103] expiration=[110] streamTime=[200]\",\n+                    // left window for k@104\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[94,104] expiration=[110] streamTime=[200]\",\n+                    // left window for k@105\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[95,105] expiration=[110] streamTime=[200]\",\n+                    // left window for k@15\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[15] window=[5,15] expiration=[110] streamTime=[200]\"\n+            ));\n+            final TestOutputTopic<String, String> outputTopic =\n+                    driver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+            assertThat(outputTopic.readRecord(), equalTo(new TestRecord<>(\"[k@190/200]\", \"+100\", null, 200L)));\n+            assertTrue(outputTopic.isEmpty());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testProcessorRandomInput() {", "originalCommit": "bb609a89ec7543943d03a1939e61f8208847ce8e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTkyMzc4OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r475923788", "bodyText": "Can you leave a brief comment here explaining why we're doing something slightly more complicated in the aggregator for this test", "author": "ableegoldman", "createdAt": "2020-08-24T22:13:19Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,690 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testAggregateSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 15L);\n+            inputTopic.pipeInput(\"A\", \"3\", 20L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+1\", 10L));\n+        expected.put(5L, ValueAndTimestamp.make(\"0+1+2\", 15L));\n+        expected.put(10L, ValueAndTimestamp.make(\"0+1+2+3\", 20L));\n+        expected.put(11L, ValueAndTimestamp.make(\"0+2+3\", 20L));\n+        expected.put(12L, ValueAndTimestamp.make(\"0+2+3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"0+3+4\", 22L));\n+        expected.put(20L, ValueAndTimestamp.make(\"0+3+4+5\", 30L));\n+        expected.put(21L, ValueAndTimestamp.make(\"0+4+5\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"0+5\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testReduceSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 14L);\n+            inputTopic.pipeInput(\"A\", \"3\", 15L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 26L);\n+            inputTopic.pipeInput(\"A\", \"6\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"1\", 10L));\n+        expected.put(4L, ValueAndTimestamp.make(\"1+2\", 14L));\n+        expected.put(5L, ValueAndTimestamp.make(\"1+2+3\", 15L));\n+        expected.put(11L, ValueAndTimestamp.make(\"2+3\", 15L));\n+        expected.put(12L, ValueAndTimestamp.make(\"2+3+4\", 22L));\n+        expected.put(15L, ValueAndTimestamp.make(\"3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"4+5\", 26L));\n+        expected.put(20L, ValueAndTimestamp.make(\"4+5+6\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"5+6\", 30L));\n+        expected.put(27L, ValueAndTimestamp.make(\"6\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @Test\n+    public void testAggregateLargeInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+        builder\n+                .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(MockInitializer.STRING_INIT, MockAggregator.toStringInstance(\"+\"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()));\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(null, \"1\");\n+            assertThat(appender.getMessages(), hasItem(\"Skipping record due to null key or value. value=[1] topic=[topic] partition=[0] offset=[0]\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingExpiredWindowByGrace() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));\n+        stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(90L)))\n+                .aggregate(\n+                        () -> \"\",\n+                        MockAggregator.toStringInstance(\"+\"),\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()\n+                )\n+                .toStream()\n+                .map((key, value) -> new KeyValue<>(key.toString(), value))\n+                .to(\"output\");\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"k\", \"100\", 200L);\n+            inputTopic.pipeInput(\"k\", \"0\", 100L);\n+            inputTopic.pipeInput(\"k\", \"1\", 101L);\n+            inputTopic.pipeInput(\"k\", \"2\", 102L);\n+            inputTopic.pipeInput(\"k\", \"3\", 103L);\n+            inputTopic.pipeInput(\"k\", \"4\", 104L);\n+            inputTopic.pipeInput(\"k\", \"5\", 105L);\n+            inputTopic.pipeInput(\"k\", \"6\", 15L);\n+\n+            assertLatenessMetrics(driver, is(7.0), is(185.0), is(96.25));\n+\n+            assertThat(appender.getMessages(), hasItems(\n+                    // left window for k@100\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[90,100] expiration=[110] streamTime=[200]\",\n+                    // left window for k@101\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[91,101] expiration=[110] streamTime=[200]\",\n+                    // left window for k@102\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[92,102] expiration=[110] streamTime=[200]\",\n+                    // left window for k@103\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[93,103] expiration=[110] streamTime=[200]\",\n+                    // left window for k@104\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[94,104] expiration=[110] streamTime=[200]\",\n+                    // left window for k@105\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[95,105] expiration=[110] streamTime=[200]\",\n+                    // left window for k@15\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[15] window=[5,15] expiration=[110] streamTime=[200]\"\n+            ));\n+            final TestOutputTopic<String, String> outputTopic =\n+                    driver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+            assertThat(outputTopic.readRecord(), equalTo(new TestRecord<>(\"[k@190/200]\", \"+100\", null, 200L)));\n+            assertTrue(outputTopic.isEmpty());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testProcessorRandomInput() {\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(10000)))\n+            .aggregate(\n+                () -> \"\",\n+                (key, value, aggregate) -> {\n+                    aggregate += value;\n+                    final char[] ch = aggregate.toCharArray();\n+                    Arrays.sort(ch);\n+                    aggregate = String.valueOf(ch);\n+                    return aggregate;", "originalCommit": "bb609a89ec7543943d03a1939e61f8208847ce8e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "64d4cbbdec80580a91c13a57e4d091efebf749d7", "url": "https://github.com/apache/kafka/commit/64d4cbbdec80580a91c13a57e4d091efebf749d7", "message": "testing clean up", "committedDate": "2020-08-26T16:30:45Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODU4ODYwMg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478588602", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *     <li>window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)</li>\n          \n          \n            \n             *     <li>window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)</li>\n          \n          \n            \n             *     <li>window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)</li>\n          \n          \n            \n             *     <li>window {@code [7400;12400]} contains [1,2,3] (created when third record enters the window)</li>\n          \n          \n            \n             *     <li>window {@code [8001;13001]} contains [2,3] (created when the first record drops out of the window)</li>\n          \n          \n            \n             *     <li>window {@code [9201;14201]} contains [3] (created when the second record drops out of the window)</li>", "author": "vvcephei", "createdAt": "2020-08-27T17:41:25Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, the window size based on the given maximum time difference (inclusive) between\n+ * records in the same window, and the given window grace period. While the window is sliding over the input data stream, a new window is\n+ * created each time a record enters the sliding window or a record drops out of the sliding window.\n+ * <p>\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * <ul>\n+ *     <li>window {@code [3000;8000]} contains [1] (created when first record enters the window)</li>\n+ *     <li>window {@code [4200;9200]} contains [1,2] (created when second record enters the window)</li>\n+ *     <li>window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)</li>\n+ *     <li>window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)</li>\n+ *     <li>window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)</li>", "originalCommit": "64d4cbbdec80580a91c13a57e4d091efebf749d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYxNjU3MQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478616571", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"afterWindowEnd\");\n          \n          \n            \n                    final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"grace\");", "author": "vvcephei", "createdAt": "2020-08-27T18:32:25Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream;\n+\n+import org.apache.kafka.streams.internals.ApiUtils;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+import java.time.Duration;\n+import java.util.Objects;\n+import static org.apache.kafka.streams.internals.ApiUtils.prepareMillisCheckFailMsgPrefix;\n+\n+/**\n+ * A sliding window used for aggregating events.\n+ * <p>\n+ * Sliding Windows are defined based on a record's timestamp, the window size based on the given maximum time difference (inclusive) between\n+ * records in the same window, and the given window grace period. While the window is sliding over the input data stream, a new window is\n+ * created each time a record enters the sliding window or a record drops out of the sliding window.\n+ * <p>\n+ * Records that come after set grace period will be ignored, i.e., a window is closed when\n+ * {@code stream-time > window-end + grace-period}.\n+ * <p>\n+ * For example, if we have a time difference of 5000ms and the following data arrives:\n+ * <pre>\n+ * +--------------------------------------+\n+ * |    key    |    value    |    time    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     1       |    8000    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     2       |    9200    |\n+ * +-----------+-------------+------------+\n+ * |    A      |     3       |    12400   |\n+ * +-----------+-------------+------------+\n+ * </pre>\n+ * We'd have the following 5 windows:\n+ * <ul>\n+ *     <li>window {@code [3000;8000]} contains [1] (created when first record enters the window)</li>\n+ *     <li>window {@code [4200;9200]} contains [1,2] (created when second record enters the window)</li>\n+ *     <li>window {@code [7400;124000]} contains [1,2,3] (created when third record enters the window)</li>\n+ *     <li>window {@code [8001;130001]} contains [2,3] (created when the first record drops out of the window)</li>\n+ *     <li>window {@code [9201;142001]} contains [3] (created when the second record drops out of the window)</li>\n+ * </ul>\n+ *<p>\n+ * Note that while SlidingWindows are of a fixed size, as are {@link TimeWindows}, the start and end points of the window\n+ * depend on when events occur in the stream (i.e., event timestamps), similar to {@link SessionWindows}.\n+ * <p>\n+ * For time semantics, see {@link TimestampExtractor}.\n+ *\n+ * @see TimeWindows\n+ * @see SessionWindows\n+ * @see UnlimitedWindows\n+ * @see JoinWindows\n+ * @see KGroupedStream#windowedBy(SlidingWindows)\n+ * @see CogroupedKStream#windowedBy(SlidingWindows)\n+ * @see TimestampExtractor\n+ */\n+\n+public final class SlidingWindows {\n+\n+    /** The size of the windows in milliseconds, defined by the max time difference between records. */\n+    private final long timeDifferenceMs;\n+\n+    /** The grace period in milliseconds. */\n+    private final long graceMs;\n+\n+    private SlidingWindows(final long timeDifferenceMs, final long graceMs) {\n+        this.timeDifferenceMs = timeDifferenceMs;\n+        this.graceMs = graceMs;\n+    }\n+\n+    /**\n+     * Return a window definition with the window size based on the given maximum time difference (inclusive) between\n+     * records in the same window and given window grace period. Reject out-of-order events that arrive after {@code grace}.\n+     * A window is closed when {@code stream-time > window-end + grace-period}.\n+     *\n+     * @param timeDifference the max time difference (inclusive) between two records in a window\n+     * @param grace the grace period to admit out-of-order events to a window\n+     * @return a new window definition\n+     * @throws IllegalArgumentException if the specified window size is < 0 or grace < 0, or either can't be represented as {@code long milliseconds}\n+     */\n+    public static SlidingWindows withTimeDifferenceAndGrace(final Duration timeDifference, final Duration grace) throws IllegalArgumentException {\n+        final String msgPrefixSize = prepareMillisCheckFailMsgPrefix(timeDifference, \"timeDifference\");\n+        final long timeDifferenceMs = ApiUtils.validateMillisecondDuration(timeDifference, msgPrefixSize);\n+        if (timeDifferenceMs < 0) {\n+            throw new IllegalArgumentException(\"Window time difference must not be negative.\");\n+        }\n+        final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, \"afterWindowEnd\");", "originalCommit": "64d4cbbdec80580a91c13a57e4d091efebf749d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYxNzE0Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478617147", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        (Aggregator<? super K, ? super Object, VOut>) aggregator);\n          \n          \n            \n                                      (Aggregator<? super K, ? super Object, VOut>) aggregator);", "author": "vvcephei", "createdAt": "2020-08-27T18:33:32Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImpl.java", "diffHunk": "@@ -61,7 +62,7 @@\n         Objects.requireNonNull(groupedStream, \"groupedStream can't be null\");\n         Objects.requireNonNull(aggregator, \"aggregator can't be null\");\n         groupPatterns.put((KGroupedStreamImpl<K, ?>) groupedStream,\n-                          (Aggregator<? super K, ? super Object, VOut>) aggregator);\n+            (Aggregator<? super K, ? super Object, VOut>) aggregator);", "originalCommit": "64d4cbbdec80580a91c13a57e4d091efebf749d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY2NDE0OQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478664149", "bodyText": "minor: this could be declared final at the assignment on line 161", "author": "vvcephei", "createdAt": "2020-08-27T20:03:13Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,303 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                         final String storeName,\n+                                         final Initializer<Agg> initializer,\n+                                         final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                threadId,\n+                context.taskId().toString(),\n+                internalProcessorContext.currentNode().name(),\n+                metrics\n+            );\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                windowStore,\n+                context,\n+                new TimestampedCacheFlushListener<>(context),\n+                sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                    \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                    \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            processInOrder(key, value, timestamp);\n+        }\n+\n+        public void processInOrder(final K key, final V value, final long timestamp) {\n+\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final Set<Long> windowStartTimes = new HashSet<>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            // keep the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                    key,\n+                    key,\n+                    timestamp - 2 * windows.timeDifferenceMs(),\n+                    // to catch the current record's right window, if it exists, without more calls to the store\n+                    timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;", "originalCommit": "64d4cbbdec80580a91c13a57e4d091efebf749d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcyNjQyNw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478726427", "bodyText": "Is that possible? It's reassigned for every iteration of the while()", "author": "lct45", "createdAt": "2020-08-27T22:17:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY2NDE0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc1MTc4Nw==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478751787", "bodyText": "I think he means, instead of declaring it once up here and then reassigning it every iteration, we can just do final KeyValue<> next = iterator.next() down on line 161. We don't need it outside the loop", "author": "ableegoldman", "createdAt": "2020-08-27T23:35:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY2NDE0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc1MzU2OA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478753568", "bodyText": "Aha, that makes sense", "author": "lct45", "createdAt": "2020-08-27T23:41:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY2NDE0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY2OTk4Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478669986", "bodyText": "Might not be a bad idea to have an assertion here that the timestamp is actually in the window boundaries.", "author": "vvcephei", "createdAt": "2020-08-27T20:14:32Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,303 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                         final String storeName,\n+                                         final Initializer<Agg> initializer,\n+                                         final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                threadId,\n+                context.taskId().toString(),\n+                internalProcessorContext.currentNode().name(),\n+                metrics\n+            );\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                windowStore,\n+                context,\n+                new TimestampedCacheFlushListener<>(context),\n+                sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                    \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                    \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            processInOrder(key, value, timestamp);\n+        }\n+\n+        public void processInOrder(final K key, final V value, final long timestamp) {\n+\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final Set<Long> windowStartTimes = new HashSet<>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            // keep the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                    key,\n+                    key,\n+                    timestamp - 2 * windows.timeDifferenceMs(),\n+                    // to catch the current record's right window, if it exists, without more calls to the store\n+                    timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+                    final long startTime = next.key.window().start();\n+                    final long endTime = startTime + windows.timeDifferenceMs();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                    } else if (endTime == timestamp) {\n+                        leftWinAlreadyCreated = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                    } else {\n+                        rightWinAlreadyCreated = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (!windowStartTimes.contains(rightWinStart)) {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }\n+            }\n+\n+            //create left window for new record\n+            if (!leftWinAlreadyCreated) {\n+                final ValueAndTimestamp<Agg> valueAndTime;\n+                //there's a right window that the new record could create --> new record's left window is not empty\n+                if (latestLeftTypeWindow != null) {\n+                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);\n+                } else {\n+                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                }\n+                final TimeWindow window = new TimeWindow(timestamp - windows.timeDifferenceMs(), timestamp);\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+            //create right window for new record\n+            if (!rightWinAlreadyCreated && rightWindowIsNotEmpty(rightWinAgg, timestamp)) {\n+                final TimeWindow window = new TimeWindow(timestamp + 1, timestamp + 1 + windows.timeDifferenceMs());\n+                final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(getValueOrNull(rightWinAgg), Math.max(rightWinAgg.timestamp(), timestamp));\n+                putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+            }\n+        }\n+\n+        private boolean rightWindowIsNotEmpty(final ValueAndTimestamp<Agg> rightWinAgg, final long timestamp) {\n+            return rightWinAgg != null && rightWinAgg.timestamp() > timestamp;\n+        }\n+\n+        private boolean isLeftWindow(final KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> window) {\n+            return window.key.window().end() == window.value.timestamp();\n+        }\n+\n+        private void putAndForward(final Window window,\n+                                   final ValueAndTimestamp<Agg> valueAndTime,\n+                                   final K key,\n+                                   final V value,\n+                                   final long closeTime,\n+                                   final long timestamp) {", "originalCommit": "64d4cbbdec80580a91c13a57e4d091efebf749d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODczMDc5Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478730796", "bodyText": "I don't think that would be true all the time, since the current record's right window wouldn't contain the current record and is created through this method. If it helps for clarity, I can add a check that if we're not creating the right window then the timestamp needs to be within the window, and otherwise confirm that we're creating the right window", "author": "lct45", "createdAt": "2020-08-27T22:29:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY2OTk4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc1NjM0Ng==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478756346", "bodyText": "Now that you bring it up, that's kind of a weird case for this method, and it's currently handled in a pretty subtle way. For example down on line 233 we are effectively checking for this case, and line 234 just happens to work correctly for it. But it's not at all obvious that we're even handling this case. Can we avoid the ternary operator when setting newAgg and newTimestamp and just use a normal if/else to explicitly set both of these for the special case? (ie if (windowStart == timestamp + 1)...)", "author": "ableegoldman", "createdAt": "2020-08-27T23:51:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY2OTk4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM2MjY2NQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r479362665", "bodyText": "Yeah it's definitely vague, I'll update", "author": "lct45", "createdAt": "2020-08-28T15:01:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY2OTk4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY3MTE5MA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478671190", "bodyText": "Is it already guaranteed that this window actually contains the current record? It doesn't look like we're checking that endTime >= timestamp anywhere, and it seems like the start of the range (timestamp - 2 * windows.timeDifferenceMs()) could give back a window that starts and ends before the current record's timestamp.", "author": "vvcephei", "createdAt": "2020-08-27T20:16:55Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java", "diffHunk": "@@ -0,0 +1,303 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Initializer;\n+import org.apache.kafka.streams.kstream.Window;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.internals.InternalProcessorContext;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.apache.kafka.streams.state.TimestampedWindowStore;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrLateRecordDropSensor;\n+import static org.apache.kafka.streams.processor.internals.metrics.TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor;\n+import static org.apache.kafka.streams.state.ValueAndTimestamp.getValueOrNull;\n+\n+public class KStreamSlidingWindowAggregate<K, V, Agg> implements KStreamAggProcessorSupplier<K, Windowed<K>, V, Agg> {\n+    private final Logger log = LoggerFactory.getLogger(getClass());\n+\n+    private final String storeName;\n+    private final SlidingWindows windows;\n+    private final Initializer<Agg> initializer;\n+    private final Aggregator<? super K, ? super V, Agg> aggregator;\n+\n+    private boolean sendOldValues = false;\n+\n+    public KStreamSlidingWindowAggregate(final SlidingWindows windows,\n+                                         final String storeName,\n+                                         final Initializer<Agg> initializer,\n+                                         final Aggregator<? super K, ? super V, Agg> aggregator) {\n+        this.windows = windows;\n+        this.storeName = storeName;\n+        this.initializer = initializer;\n+        this.aggregator = aggregator;\n+    }\n+\n+    @Override\n+    public Processor<K, V> get() {\n+        return new KStreamSlidingWindowAggregateProcessor();\n+    }\n+\n+    public SlidingWindows windows() {\n+        return windows;\n+    }\n+\n+    @Override\n+    public void enableSendingOldValues() {\n+        sendOldValues = true;\n+    }\n+\n+    private class KStreamSlidingWindowAggregateProcessor extends AbstractProcessor<K, V> {\n+        private TimestampedWindowStore<K, Agg> windowStore;\n+        private TimestampedTupleForwarder<Windowed<K>, Agg> tupleForwarder;\n+        private StreamsMetricsImpl metrics;\n+        private InternalProcessorContext internalProcessorContext;\n+        private Sensor lateRecordDropSensor;\n+        private Sensor droppedRecordsSensor;\n+        private long observedStreamTime = ConsumerRecord.NO_TIMESTAMP;\n+\n+        @SuppressWarnings(\"unchecked\")\n+        @Override\n+        public void init(final ProcessorContext context) {\n+            super.init(context);\n+            internalProcessorContext = (InternalProcessorContext) context;\n+            metrics = internalProcessorContext.metrics();\n+            final String threadId = Thread.currentThread().getName();\n+            lateRecordDropSensor = droppedRecordsSensorOrLateRecordDropSensor(\n+                threadId,\n+                context.taskId().toString(),\n+                internalProcessorContext.currentNode().name(),\n+                metrics\n+            );\n+            droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(threadId, context.taskId().toString(), metrics);\n+            windowStore = (TimestampedWindowStore<K, Agg>) context.getStateStore(storeName);\n+            tupleForwarder = new TimestampedTupleForwarder<>(\n+                windowStore,\n+                context,\n+                new TimestampedCacheFlushListener<>(context),\n+                sendOldValues);\n+        }\n+\n+        @Override\n+        public void process(final K key, final V value) {\n+            if (key == null || value == null) {\n+                log.warn(\n+                    \"Skipping record due to null key or value. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+\n+            final long timestamp = context().timestamp();\n+            //don't process records that don't fall within a full sliding window\n+            if (timestamp < windows.timeDifferenceMs()) {\n+                log.warn(\n+                    \"Skipping record due to early arrival. value=[{}] topic=[{}] partition=[{}] offset=[{}]\",\n+                    value, context().topic(), context().partition(), context().offset()\n+                );\n+                droppedRecordsSensor.record();\n+                return;\n+            }\n+            processInOrder(key, value, timestamp);\n+        }\n+\n+        public void processInOrder(final K key, final V value, final long timestamp) {\n+\n+            observedStreamTime = Math.max(observedStreamTime, timestamp);\n+            final long closeTime = observedStreamTime - windows.gracePeriodMs();\n+\n+            //store start times of windows we find\n+            final Set<Long> windowStartTimes = new HashSet<>();\n+\n+            // aggregate that will go in the current record\u2019s left/right window (if needed)\n+            ValueAndTimestamp<Agg> leftWinAgg = null;\n+            ValueAndTimestamp<Agg> rightWinAgg = null;\n+\n+            //if current record's left/right windows already exist\n+            boolean leftWinAlreadyCreated = false;\n+            boolean rightWinAlreadyCreated = false;\n+\n+            // keep the left type window closest to the record\n+            Window latestLeftTypeWindow = null;\n+            try (\n+                final KeyValueIterator<Windowed<K>, ValueAndTimestamp<Agg>> iterator = windowStore.fetch(\n+                    key,\n+                    key,\n+                    timestamp - 2 * windows.timeDifferenceMs(),\n+                    // to catch the current record's right window, if it exists, without more calls to the store\n+                    timestamp + 1)\n+            ) {\n+                KeyValue<Windowed<K>, ValueAndTimestamp<Agg>> next;\n+                while (iterator.hasNext()) {\n+                    next = iterator.next();\n+                    windowStartTimes.add(next.key.window().start());\n+                    final long startTime = next.key.window().start();\n+                    final long endTime = startTime + windows.timeDifferenceMs();\n+\n+                    if (endTime < timestamp) {\n+                        leftWinAgg = next.value;\n+                        if (isLeftWindow(next)) {\n+                            latestLeftTypeWindow = next.key.window();\n+                        }\n+                    } else if (endTime == timestamp) {\n+                        leftWinAlreadyCreated = true;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                    } else if (endTime > timestamp && startTime <= timestamp) {\n+                        rightWinAgg = next.value;\n+                        putAndForward(next.key.window(), next.value, key, value, closeTime, timestamp);\n+                    } else {\n+                        rightWinAlreadyCreated = true;\n+                    }\n+                }\n+            }\n+\n+            //create right window for previous record\n+            if (latestLeftTypeWindow != null) {\n+                final long rightWinStart = latestLeftTypeWindow.end() + 1;\n+                if (!windowStartTimes.contains(rightWinStart)) {\n+                    final TimeWindow window = new TimeWindow(rightWinStart, rightWinStart + windows.timeDifferenceMs());\n+                    final ValueAndTimestamp<Agg> valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);\n+                    putAndForward(window, valueAndTime, key, value, closeTime, timestamp);\n+                }", "originalCommit": "64d4cbbdec80580a91c13a57e4d091efebf749d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcyODgyOQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478728829", "bodyText": "While the range might give a window that starts and ends before the current record's timestamp, the current record would fall into the right window of the records within those windows.\nEX: TimeDifference = 10, record @ 30, range from (10,31). The earliest start time of a window we can have is 10, so the earliest leftTypeWindow we can find is from [10,20]. If there's a record at 2, it's right window would be [21,31], which our record @ 30 would fall within. Because this is true for the furthest possible record, it'll be true for the others that we find.", "author": "lct45", "createdAt": "2020-08-27T22:24:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY3MTE5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTM0NDc4NA==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r479344784", "bodyText": "Thanks. From the other thread, it sounds like I misunderstood putAndForward as adding the value to the window.", "author": "vvcephei", "createdAt": "2020-08-28T14:32:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODY3MTE5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcwMzQ1Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478703452", "bodyText": "I'd normally say we should have a test also to verify we log properly on early records, but you already opened the PR to add early record handling, so we're good.", "author": "vvcephei", "createdAt": "2020-08-27T21:22:51Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,692 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testAggregateSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 15L);\n+            inputTopic.pipeInput(\"A\", \"3\", 20L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+1\", 10L));\n+        expected.put(5L, ValueAndTimestamp.make(\"0+1+2\", 15L));\n+        expected.put(10L, ValueAndTimestamp.make(\"0+1+2+3\", 20L));\n+        expected.put(11L, ValueAndTimestamp.make(\"0+2+3\", 20L));\n+        expected.put(12L, ValueAndTimestamp.make(\"0+2+3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"0+3+4\", 22L));\n+        expected.put(20L, ValueAndTimestamp.make(\"0+3+4+5\", 30L));\n+        expected.put(21L, ValueAndTimestamp.make(\"0+4+5\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"0+5\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testReduceSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 14L);\n+            inputTopic.pipeInput(\"A\", \"3\", 15L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 26L);\n+            inputTopic.pipeInput(\"A\", \"6\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"1\", 10L));\n+        expected.put(4L, ValueAndTimestamp.make(\"1+2\", 14L));\n+        expected.put(5L, ValueAndTimestamp.make(\"1+2+3\", 15L));\n+        expected.put(11L, ValueAndTimestamp.make(\"2+3\", 15L));\n+        expected.put(12L, ValueAndTimestamp.make(\"2+3+4\", 22L));\n+        expected.put(15L, ValueAndTimestamp.make(\"3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"4+5\", 26L));\n+        expected.put(20L, ValueAndTimestamp.make(\"4+5+6\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"5+6\", 30L));\n+        expected.put(27L, ValueAndTimestamp.make(\"6\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @Test\n+    public void testAggregateLargeInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {", "originalCommit": "64d4cbbdec80580a91c13a57e4d091efebf749d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcwNDM1OQ==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478704359", "bodyText": "Awesome test. Thanks!", "author": "vvcephei", "createdAt": "2020-08-27T21:24:45Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java", "diffHunk": "@@ -0,0 +1,692 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.kstream.internals;\n+\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.TestOutputTopic;\n+import org.apache.kafka.streams.TopologyTestDriver;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.SlidingWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;\n+import org.apache.kafka.streams.state.ValueAndTimestamp;\n+import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.streams.TestInputTopic;\n+import org.apache.kafka.streams.test.TestRecord;\n+import org.apache.kafka.test.MockAggregator;\n+import org.apache.kafka.test.MockInitializer;\n+import org.apache.kafka.test.MockProcessor;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.apache.kafka.test.MockReducer;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.hamcrest.Matcher;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import static java.time.Duration.ofMillis;\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class KStreamSlidingWindowAggregateTest {\n+    private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n+    private final String threadId = Thread.currentThread().getName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testAggregateSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .aggregate(\n+                MockInitializer.STRING_INIT,\n+                MockAggregator.TOSTRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 15L);\n+            inputTopic.pipeInput(\"A\", \"3\", 20L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"0+1\", 10L));\n+        expected.put(5L, ValueAndTimestamp.make(\"0+1+2\", 15L));\n+        expected.put(10L, ValueAndTimestamp.make(\"0+1+2+3\", 20L));\n+        expected.put(11L, ValueAndTimestamp.make(\"0+2+3\", 20L));\n+        expected.put(12L, ValueAndTimestamp.make(\"0+2+3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"0+3+4\", 22L));\n+        expected.put(20L, ValueAndTimestamp.make(\"0+3+4+5\", 30L));\n+        expected.put(21L, ValueAndTimestamp.make(\"0+4+5\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"0+5\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testReduceSmallInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KTable<Windowed<String>, String> table = builder\n+            .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+            .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+            .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+            .reduce(\n+                MockReducer.STRING_ADDER,\n+                Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic-Canonized\").withValueSerde(Serdes.String())\n+            );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table.toStream().process(supplier);\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic.pipeInput(\"A\", \"2\", 14L);\n+            inputTopic.pipeInput(\"A\", \"3\", 15L);\n+            inputTopic.pipeInput(\"A\", \"4\", 22L);\n+            inputTopic.pipeInput(\"A\", \"5\", 26L);\n+            inputTopic.pipeInput(\"A\", \"6\", 30L);\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> actual = new HashMap<>();\n+        for (final KeyValueTimestamp<Object, Object> entry : supplier.theCapturedProcessor().processed) {\n+            final Windowed<String> window = (Windowed<String>) entry.key();\n+            final Long start = window.window().start();\n+            final ValueAndTimestamp valueAndTimestamp = ValueAndTimestamp.make((String) entry.value(), entry.timestamp());\n+            if (actual.putIfAbsent(start, valueAndTimestamp) != null) {\n+                actual.replace(start, valueAndTimestamp);\n+            }\n+        }\n+\n+        final Map<Long, ValueAndTimestamp<String>> expected = new HashMap<>();\n+        expected.put(0L, ValueAndTimestamp.make(\"1\", 10L));\n+        expected.put(4L, ValueAndTimestamp.make(\"1+2\", 14L));\n+        expected.put(5L, ValueAndTimestamp.make(\"1+2+3\", 15L));\n+        expected.put(11L, ValueAndTimestamp.make(\"2+3\", 15L));\n+        expected.put(12L, ValueAndTimestamp.make(\"2+3+4\", 22L));\n+        expected.put(15L, ValueAndTimestamp.make(\"3+4\", 22L));\n+        expected.put(16L, ValueAndTimestamp.make(\"4+5\", 26L));\n+        expected.put(20L, ValueAndTimestamp.make(\"4+5+6\", 30L));\n+        expected.put(23L, ValueAndTimestamp.make(\"5+6\", 30L));\n+        expected.put(27L, ValueAndTimestamp.make(\"6\", 30L));\n+\n+        assertEquals(expected, actual);\n+    }\n+\n+    @Test\n+    public void testAggregateLargeInput() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(50)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table2.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"A\", \"2\", 20L);\n+            inputTopic1.pipeInput(\"A\", \"3\", 22L);\n+            inputTopic1.pipeInput(\"A\", \"4\", 15L);\n+\n+            inputTopic1.pipeInput(\"B\", \"1\", 12L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 13L);\n+            inputTopic1.pipeInput(\"B\", \"3\", 18L);\n+            inputTopic1.pipeInput(\"B\", \"4\", 19L);\n+            inputTopic1.pipeInput(\"B\", \"5\", 25L);\n+            inputTopic1.pipeInput(\"B\", \"6\", 14L);\n+\n+            inputTopic1.pipeInput(\"C\", \"1\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"2\", 15L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"4\", 21);\n+            inputTopic1.pipeInput(\"C\", \"5\", 23L);\n+\n+            inputTopic1.pipeInput(\"D\", \"4\", 11L);\n+            inputTopic1.pipeInput(\"D\", \"2\", 12L);\n+            inputTopic1.pipeInput(\"D\", \"3\", 29L);\n+            inputTopic1.pipeInput(\"D\", \"5\", 16L);\n+        }\n+\n+        assertEquals(\n+                asList(\n+                        // FINAL WINDOW: A@10 left window created when A@10 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)), \"0+1\", 10),\n+                        // A@10 right window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2\", 20),\n+                        // A@20 left window created when A@20 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2\", 20),\n+                        // FINAL WINDOW: A@20 right window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(21, 31)), \"0+3\", 22),\n+                        // A@22 left window created when A@22 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3\", 22),\n+                        // FINAL WINDOW: A@20 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(10, 20)), \"0+1+2+4\", 20),\n+                        // FINAL WINDOW: A@10 right window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)), \"0+2+4\", 20),\n+                        // FINAL WINDOW: A@22 left window updated when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(12, 22)), \"0+2+3+4\", 22),\n+                        // FINAL WINDOW: A@15 left window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)), \"0+1+4\", 15),\n+                        // FINAL WINDOW: A@15 right window created when A@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(16, 26)), \"0+2+3\", 22),\n+\n+                        // FINAL WINDOW: B@12 left window created when B@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(2, 12)), \"0+1\", 12),\n+                        // B@12 right window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2\", 13),\n+                        // FINAL WINDOW: B@13 left window created when B@13 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(3, 13)), \"0+1+2\", 13),\n+                        // B@12 right window updated when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3\", 18),\n+                        // B@13 right window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3\", 18),\n+                        // B@18 left window created when B@18 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3\", 18),\n+                        // B@12 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4\", 19),\n+                        // B@13 right window updated when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4\", 19),\n+                        // B@18 right window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4\", 19),\n+                        // B@19 left window created when B@19 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4\", 19),\n+                        // FINAL WINDOW: B@18 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(19, 29)), \"0+4+5\", 25),\n+                        // FINAL WINDOW: B@19 right window updated when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)), \"0+5\", 25),\n+                        // FINAL WINDOW: B@25 left window created when B@25 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(15, 25)), \"0+3+4+5\", 25),\n+                        // FINAL WINDOW: B@18 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(8, 18)), \"0+1+2+3+6\", 18),\n+                        // FINAL WINDOW: B@19 left window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(9, 19)), \"0+1+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@12 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(13, 23)), \"0+2+3+4+6\", 19),\n+                        // FINAL WINDOW: B@13 right window updated when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(14, 24)), \"0+3+4+6\", 19),\n+                        // FINAL WINDOW: B@14 left window created when B@14 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(4, 14)), \"0+1+2+6\", 14),\n+\n+                        // FINAL WINDOW: C@11 left window created when C@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(1, 11)), \"0+1\", 11),\n+                        // C@11 right window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2\", 15),\n+                        // FINAL WINDOW: C@15 left window created when C@15 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(5, 15)), \"0+1+2\", 15),\n+                        // C@11 right window updated when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3\", 16),\n+                        // C@15 right window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3\", 16),\n+                        // FINAL WINDOW: C@16 left window created when C@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(6, 16)), \"0+1+2+3\", 16),\n+                        // FINAL WINDOW: C@11 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(12, 22)), \"0+2+3+4\", 21),\n+                        // C@15 right window updated when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4\", 21),\n+                        // C@16 right window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4\", 21),\n+                        // FINAL WINDOW: C@21 left window created when C@21 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(11, 21)), \"0+1+2+3+4\", 21),\n+                        // FINAL WINDOW: C@15 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(16, 26)), \"0+3+4+5\", 23),\n+                        // FINAL WINDOW: C@16 right window updated when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(17, 27)), \"0+4+5\", 23),\n+                        // FINAL WINDOW: C@21 right window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(22, 32)), \"0+5\", 23),\n+                        // FINAL WINDOW: C@23 left window created when C@23 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)), \"0+2+3+4+5\", 23),\n+\n+                        // FINAL WINDOW: D@11 left window created when D@11 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(1, 11)), \"0+4\", 11),\n+                        // D@11 right window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2\", 12),\n+                        // FINAL WINDOW: D@12 left window created when D@12 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(2, 12)), \"0+4+2\", 12),\n+                        // FINAL WINDOW: D@29 left window created when D@29 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(19, 29)), \"0+3\", 29),\n+                        // FINAL WINDOW: D@11 right window updated when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(12, 22)), \"0+2+5\", 16),\n+                        // FINAL WINDOW: D@12 right window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(13, 23)), \"0+5\", 16),\n+                        // FINAL WINDOW: D@16 left window created when D@16 processed\n+                        new KeyValueTimestamp<>(new Windowed<>(\"D\", new TimeWindow(6, 16)), \"0+4+2+5\", 16)\n+                        ),\n+                supplier.theCapturedProcessor().processed\n+        );\n+    }\n+\n+    @Test\n+    public void testJoin() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+        final String topic2 = \"topic2\";\n+\n+        final KTable<Windowed<String>, String> table1 = builder\n+                .stream(topic1, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonized\").withValueSerde(Serdes.String())\n+                );\n+\n+        final MockProcessorSupplier<Windowed<String>, String> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        final KTable<Windowed<String>, String> table2 = builder\n+                .stream(topic2, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(\n+                        MockInitializer.STRING_INIT,\n+                        MockAggregator.TOSTRING_ADDER,\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic2-Canonized\").withValueSerde(Serdes.String())\n+                );\n+        table2.toStream().process(supplier);\n+\n+        table1.join(table2, (p1, p2) -> p1 + \"%\" + p2).toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic1 =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> inputTopic2 =\n+                    driver.createInputTopic(topic2, new StringSerializer(), new StringSerializer());\n+            inputTopic1.pipeInput(\"A\", \"1\", 10L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 11L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 12L);\n+\n+            final List<MockProcessor<Windowed<String>, String>> processors = supplier.capturedProcessors(3);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // left windows created by the first set of records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(1, 11)),  \"0+2\",  11),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3\",  12)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic1.pipeInput(\"A\", \"1\", 15L);\n+            inputTopic1.pipeInput(\"B\", \"2\", 16L);\n+            inputTopic1.pipeInput(\"C\", \"3\", 19L);\n+\n+            processors.get(0).checkAndClearProcessResult(\n+                    // right windows from previous records are created, and left windows from new records to table 1\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(12, 22)),  \"0+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3\",  19),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(9, 19)),  \"0+3+3\",  19)\n+            );\n+            processors.get(1).checkAndClearProcessResult();\n+            processors.get(2).checkAndClearProcessResult();\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 10L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 30L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 12L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 35L);\n+\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // left windows from first set of records sent to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(20, 30)),  \"0+b\",  30),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+c\",  12),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(25, 35)),  \"0+c\",  35)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(0, 10)),  \"0+1%0+a\",  10),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(2, 12)),  \"0+3%0+c\",  12)\n+            );\n+\n+            inputTopic2.pipeInput(\"A\", \"a\", 15L);\n+            inputTopic2.pipeInput(\"B\", \"b\", 16L);\n+            inputTopic2.pipeInput(\"C\", \"c\", 17L);\n+\n+            processors.get(0).checkAndClearProcessResult();\n+            processors.get(1).checkAndClearProcessResult(\n+                    // right windows from previous records are created (where applicable), and left windows from new records to table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+c\",  17),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(7, 17)),  \"0+c+c\",  17)\n+            );\n+            processors.get(2).checkAndClearProcessResult(\n+                    // set of join windows from windows created by table 1 and table 2\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(11, 21)),  \"0+1%0+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"A\", new TimeWindow(5, 15)),  \"0+1+1%0+a+a\",  15),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"B\", new TimeWindow(6, 16)),  \"0+2+2%0+b\",  16),\n+                    new KeyValueTimestamp<>(new Windowed<>(\"C\", new TimeWindow(13, 23)),  \"0+3%0+c\",  19)\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingNullKey() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+        builder\n+                .stream(topic, Consumed.with(Serdes.String(), Serdes.String()))\n+                .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(100)))\n+                .aggregate(MockInitializer.STRING_INIT, MockAggregator.toStringInstance(\"+\"), Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()));\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(null, \"1\");\n+            assertThat(appender.getMessages(), hasItem(\"Skipping record due to null key or value. value=[1] topic=[topic] partition=[0] offset=[0]\"));\n+        }\n+    }\n+\n+    @Test\n+    public void shouldLogAndMeterWhenSkippingExpiredWindowByGrace() {\n+        final String builtInMetricsVersion = StreamsConfig.METRICS_LATEST;\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic = \"topic\";\n+\n+        final KStream<String, String> stream1 = builder.stream(topic, Consumed.with(Serdes.String(), Serdes.String()));\n+        stream1.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))\n+                .windowedBy(SlidingWindows.withTimeDifferenceAndGrace(ofMillis(10), ofMillis(90L)))\n+                .aggregate(\n+                        () -> \"\",\n+                        MockAggregator.toStringInstance(\"+\"),\n+                        Materialized.<String, String, WindowStore<Bytes, byte[]>>as(\"topic1-Canonicalized\").withValueSerde(Serdes.String()).withCachingDisabled().withLoggingDisabled()\n+                )\n+                .toStream()\n+                .map((key, value) -> new KeyValue<>(key.toString(), value))\n+                .to(\"output\");\n+\n+        props.setProperty(StreamsConfig.BUILT_IN_METRICS_VERSION_CONFIG, builtInMetricsVersion);\n+\n+        try (final LogCaptureAppender appender = LogCaptureAppender.createAndRegister(KStreamSlidingWindowAggregate.class);\n+             final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+\n+            final TestInputTopic<String, String> inputTopic =\n+                    driver.createInputTopic(topic, new StringSerializer(), new StringSerializer());\n+            inputTopic.pipeInput(\"k\", \"100\", 200L);\n+            inputTopic.pipeInput(\"k\", \"0\", 100L);\n+            inputTopic.pipeInput(\"k\", \"1\", 101L);\n+            inputTopic.pipeInput(\"k\", \"2\", 102L);\n+            inputTopic.pipeInput(\"k\", \"3\", 103L);\n+            inputTopic.pipeInput(\"k\", \"4\", 104L);\n+            inputTopic.pipeInput(\"k\", \"5\", 105L);\n+            inputTopic.pipeInput(\"k\", \"6\", 15L);\n+\n+            assertLatenessMetrics(driver, is(7.0), is(185.0), is(96.25));\n+\n+            assertThat(appender.getMessages(), hasItems(\n+                    // left window for k@100\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[1] timestamp=[100] window=[90,100] expiration=[110] streamTime=[200]\",\n+                    // left window for k@101\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[2] timestamp=[101] window=[91,101] expiration=[110] streamTime=[200]\",\n+                    // left window for k@102\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[3] timestamp=[102] window=[92,102] expiration=[110] streamTime=[200]\",\n+                    // left window for k@103\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[4] timestamp=[103] window=[93,103] expiration=[110] streamTime=[200]\",\n+                    // left window for k@104\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[5] timestamp=[104] window=[94,104] expiration=[110] streamTime=[200]\",\n+                    // left window for k@105\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[6] timestamp=[105] window=[95,105] expiration=[110] streamTime=[200]\",\n+                    // left window for k@15\n+                    \"Skipping record for expired window. key=[k] topic=[topic] partition=[0] offset=[7] timestamp=[15] window=[5,15] expiration=[110] streamTime=[200]\"\n+            ));\n+            final TestOutputTopic<String, String> outputTopic =\n+                    driver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+            assertThat(outputTopic.readRecord(), equalTo(new TestRecord<>(\"[k@190/200]\", \"+100\", null, 200L)));\n+            assertTrue(outputTopic.isEmpty());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void testAggregateRandomInput() {", "originalCommit": "64d4cbbdec80580a91c13a57e4d091efebf749d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcwNTk0Mg==", "url": "https://github.com/apache/kafka/pull/9039#discussion_r478705942", "bodyText": "Aside from join, forgetting to test new operators in front of Suppress has also been an issue. It's great to see this test here!", "author": "vvcephei", "createdAt": "2020-08-27T21:28:18Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/SuppressScenarioTest.java", "diffHunk": "@@ -459,6 +460,89 @@ public void shouldSupportFinalResultsForTimeWindowsWithLargeJump() {\n         }\n     }\n \n+    @Test\n+    public void shouldSupportFinalResultsForSlidingWindows() {", "originalCommit": "64d4cbbdec80580a91c13a57e4d091efebf749d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "48328f651602f2078add6f780bb42199cd2c0316", "url": "https://github.com/apache/kafka/commit/48328f651602f2078add6f780bb42199cd2c0316", "message": "udpates wtih john's comments", "committedDate": "2020-08-28T15:04:32Z", "type": "commit"}, {"oid": "fdfb1cde038e3750bb6d8c3900558bacc18feabc", "url": "https://github.com/apache/kafka/commit/fdfb1cde038e3750bb6d8c3900558bacc18feabc", "message": "udpates wtih john's comments", "committedDate": "2020-08-28T15:42:10Z", "type": "commit"}, {"oid": "1ce06f9321e8c74eeb0c80daaf1acb0ca70f8bd8", "url": "https://github.com/apache/kafka/commit/1ce06f9321e8c74eeb0c80daaf1acb0ca70f8bd8", "message": "Merge branch 'slidingwindows' of github.com:lct45/kafka into slidingwindows", "committedDate": "2020-08-28T15:43:54Z", "type": "commit"}, {"oid": "1c367ff37e0805bf0e9a318dc96e918c09377e5e", "url": "https://github.com/apache/kafka/commit/1c367ff37e0805bf0e9a318dc96e918c09377e5e", "message": "Merge remote-tracking branch 'apache/trunk' into pull/9039", "committedDate": "2020-08-31T21:06:15Z", "type": "commit"}, {"oid": "de97db6cf39eb34eab0207f3cc45a5085317b2a4", "url": "https://github.com/apache/kafka/commit/de97db6cf39eb34eab0207f3cc45a5085317b2a4", "message": "fix conflict with trunk", "committedDate": "2020-08-31T22:19:50Z", "type": "commit"}]}