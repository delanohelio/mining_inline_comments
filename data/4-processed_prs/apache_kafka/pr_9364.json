{"pr_number": 9364, "pr_title": "KAFKA-10471 Mark broker crash during log loading as unclean shutdown", "pr_createdAt": "2020-10-02T09:05:28Z", "pr_url": "https://github.com/apache/kafka/pull/9364", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1MTIzOA==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499051238", "bodyText": "Could we add the new param to the javadoc above?", "author": "junrao", "createdAt": "2020-10-02T21:08:27Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -244,7 +244,8 @@ class Log(@volatile private var _dir: File,\n           val producerIdExpirationCheckIntervalMs: Int,\n           val topicPartition: TopicPartition,\n           val producerStateManager: ProducerStateManager,\n-          logDirFailureChannel: LogDirFailureChannel) extends Logging with KafkaMetricsGroup {\n+          logDirFailureChannel: LogDirFailureChannel,\n+          val hadCleanShutdown: Boolean = true) extends Logging with KafkaMetricsGroup {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTcxNTE1MQ==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499715151", "bodyText": "Done", "author": "RamanVerma", "createdAt": "2020-10-05T16:13:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1MTIzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1Mjg0OA==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499052848", "bodyText": "It seems that all callers set expectDeletedFiles to false. So, do we need this param?", "author": "junrao", "createdAt": "2020-10-02T21:13:12Z", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -4447,9 +4504,10 @@ class LogTest {\n \n   private def recoverAndCheck(config: LogConfig,\n                               expectedKeys: Iterable[Long],\n-                              expectDeletedFiles: Boolean = true): Log = {\n+                              expectDeletedFiles: Boolean = true,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTcxODc4NA==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499718784", "bodyText": "I think you meant the hadCleanShutdown parameter I added. I did not add the expectDeletedFiles parameter.\nI will remove the new parameter and add a comment here to indicate that the method always assumes we had a hard reset.", "author": "RamanVerma", "createdAt": "2020-10-05T16:18:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1Mjg0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTkyMTMyMg==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r509921322", "bodyText": "Done", "author": "RamanVerma", "createdAt": "2020-10-22T06:56:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1Mjg0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1NTIyOA==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499055228", "bodyText": "The callers of createLog() in line 652 and 2205 seem to need lastShutdownClean to be false.", "author": "junrao", "createdAt": "2020-10-02T21:20:09Z", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -4429,9 +4485,10 @@ class LogTest {\n                         scheduler: Scheduler = mockTime.scheduler,\n                         time: Time = mockTime,\n                         maxProducerIdExpirationMs: Int = 60 * 60 * 1000,\n-                        producerIdExpirationCheckIntervalMs: Int = LogManager.ProducerIdExpirationCheckIntervalMs): Log = {\n+                        producerIdExpirationCheckIntervalMs: Int = LogManager.ProducerIdExpirationCheckIntervalMs,\n+                        lastShutdownClean: Boolean = true): Log = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTkyMTg1OQ==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r509921859", "bodyText": "Done", "author": "RamanVerma", "createdAt": "2020-10-22T06:57:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1NTIyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1ODM2MA==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499058360", "bodyText": "The change is unnecessary.", "author": "junrao", "createdAt": "2020-10-02T21:28:56Z", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -2882,11 +2953,8 @@ class LogTest {\n     records.foreach(segment.append _)\n     segment.close()\n \n-    // Create clean shutdown file so that we do not split during the load\n-    createCleanShutdownFile()\n-\n     val logConfig = LogTest.createLogConfig(indexIntervalBytes = 1, fileDeleteDelayMs = 1000)\n-    val log = createLog(logDir, logConfig, recoveryPoint = Long.MaxValue)\n+    val log = createLog(logDir, logConfig, recoveryPoint = Long.MaxValue, lastShutdownClean = true)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTcyNDQ1NQ==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499724455", "bodyText": "Removed", "author": "RamanVerma", "createdAt": "2020-10-05T16:27:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1ODM2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1ODY5MQ==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499058691", "bodyText": "The change is unnecessary.", "author": "junrao", "createdAt": "2020-10-02T21:29:52Z", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -3073,9 +3139,8 @@ class LogTest {\n     // check if recovery was attempted. Even if the recovery point is 0L, recovery should not be attempted as the\n     // clean shutdown file exists.\n     recoveryPoint = log.logEndOffset\n-    log = createLog(logDir, logConfig)\n+    log = createLog(logDir, logConfig, lastShutdownClean = true)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTczMTA2MQ==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499731061", "bodyText": "Removed.", "author": "RamanVerma", "createdAt": "2020-10-05T16:39:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1ODY5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDUyNzU0NA==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r510527544", "bodyText": "This is an existing issue, but cleanShutdownFile.delete() doesn't seem to throw IOException.", "author": "junrao", "createdAt": "2020-10-23T00:10:42Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -298,26 +300,38 @@ class LogManager(logDirs: Seq[File],\n   /**\n    * Recover and load all logs in the given data directories\n    */\n-  private def loadLogs(): Unit = {\n+  private[log] def loadLogs(): Unit = {\n     info(s\"Loading logs from log dirs $liveLogDirs\")\n     val startMs = time.hiResClockMs()\n     val threadPools = ArrayBuffer.empty[ExecutorService]\n     val offlineDirs = mutable.Set.empty[(String, IOException)]\n-    val jobs = mutable.Map.empty[File, Seq[Future[_]]]\n+    val jobs = ArrayBuffer.empty[Seq[Future[_]]]\n     var numTotalLogs = 0\n \n     for (dir <- liveLogDirs) {\n       val logDirAbsolutePath = dir.getAbsolutePath\n+      var hadCleanShutdown: Boolean = false\n       try {\n         val pool = Executors.newFixedThreadPool(numRecoveryThreadsPerDataDir)\n         threadPools.append(pool)\n \n         val cleanShutdownFile = new File(dir, Log.CleanShutdownFile)\n         if (cleanShutdownFile.exists) {\n           info(s\"Skipping recovery for all logs in $logDirAbsolutePath since clean shutdown file was found\")\n+          // Cache the clean shutdown status and use that for rest of log loading workflow. Delete the CleanShutdownFile\n+          // so that if broker crashes while loading the log, it is considered hard shutdown during the next boot up. KAFKA-10471\n+          try {\n+            cleanShutdownFile.delete()\n+          } catch {\n+            case e: IOException =>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTI3NzcxMw==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r511277713", "bodyText": "java.nio.file.Files API throws IOException but not the one used here java.io.File. Will remove this exception", "author": "RamanVerma", "createdAt": "2020-10-24T02:58:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDUyNzU0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDUzMDc3Nw==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r510530777", "bodyText": "Hmm, it seems that this tests expects a clean shutdown.", "author": "junrao", "createdAt": "2020-10-23T00:23:28Z", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -1257,7 +1328,7 @@ class LogTest {\n     log.close()\n \n     // After reloading log, producer state should not be regenerated\n-    val reloadedLog = createLog(logDir, logConfig, logStartOffset = 1L)\n+    val reloadedLog = createLog(logDir, logConfig, logStartOffset = 1L, lastShutdownClean = false)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU0NTg5MA==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r511545890", "bodyText": "Jun, this test was not creating a clean shut down file before opening the log again. So, it would have been going through log recovery code path earlier as well.", "author": "RamanVerma", "createdAt": "2020-10-25T04:33:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDUzMDc3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk5MTE4OQ==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r510991189", "bodyText": "It seems that this expects a clean shutdown.", "author": "junrao", "createdAt": "2020-10-23T16:10:39Z", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -2131,12 +2202,12 @@ class LogTest {\n       assertEquals(\"Should have same number of time index entries as before.\", numTimeIndexEntries, log.activeSegment.timeIndex.entries)\n     }\n \n-    log = createLog(logDir, logConfig, recoveryPoint = lastOffset)\n+    log = createLog(logDir, logConfig, recoveryPoint = lastOffset, lastShutdownClean = false)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU0NTcxMg==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r511545712", "bodyText": "Jun, this test was not creating a clean shut down file before opening the log again. So, it would have gone through the recovery path. Hence, I have set lastShutdownClean parameter to false. Similarly, for line 2210.", "author": "RamanVerma", "createdAt": "2020-10-25T04:30:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk5MTE4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk5NTU4Nw==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r510995587", "bodyText": "It seem the createLog() call on line 3976 inside testRecoverOnlyLastSegment() needs to have lastShutdownClean = false.", "author": "junrao", "createdAt": "2020-10-23T16:18:11Z", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -3623,7 +3690,7 @@ class LogTest {\n     log.close()\n \n     // reopen the log and recover from the beginning\n-    val recoveredLog = createLog(logDir, LogConfig())\n+    val recoveredLog = createLog(logDir, LogConfig(), lastShutdownClean = false)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTAwNTU4NA==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r511005584", "bodyText": "This is an existing problem. Since no callers are explicitly setting expectDeletedFiles, could we just remove this param?", "author": "junrao", "createdAt": "2020-10-23T16:36:23Z", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -4445,11 +4504,9 @@ class LogTest {\n     (log, segmentWithOverflow)\n   }\n \n-  private def recoverAndCheck(config: LogConfig,\n-                              expectedKeys: Iterable[Long],\n-                              expectDeletedFiles: Boolean = true): Log = {\n-    LogTest.recoverAndCheck(logDir, config, expectedKeys, brokerTopicStats, mockTime, mockTime.scheduler,\n-      expectDeletedFiles)\n+  private def recoverAndCheck(config: LogConfig, expectedKeys: Iterable[Long], expectDeletedFiles: Boolean = true) = {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjMxNDYxMw==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r512314613", "bodyText": "This line seems unnecessary since hadCleanShutdown is initialized to false.", "author": "junrao", "createdAt": "2020-10-26T22:48:15Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -298,26 +300,32 @@ class LogManager(logDirs: Seq[File],\n   /**\n    * Recover and load all logs in the given data directories\n    */\n-  private def loadLogs(): Unit = {\n+  private[log] def loadLogs(): Unit = {\n     info(s\"Loading logs from log dirs $liveLogDirs\")\n     val startMs = time.hiResClockMs()\n     val threadPools = ArrayBuffer.empty[ExecutorService]\n     val offlineDirs = mutable.Set.empty[(String, IOException)]\n-    val jobs = mutable.Map.empty[File, Seq[Future[_]]]\n+    val jobs = ArrayBuffer.empty[Seq[Future[_]]]\n     var numTotalLogs = 0\n \n     for (dir <- liveLogDirs) {\n       val logDirAbsolutePath = dir.getAbsolutePath\n+      var hadCleanShutdown: Boolean = false\n       try {\n         val pool = Executors.newFixedThreadPool(numRecoveryThreadsPerDataDir)\n         threadPools.append(pool)\n \n         val cleanShutdownFile = new File(dir, Log.CleanShutdownFile)\n         if (cleanShutdownFile.exists) {\n           info(s\"Skipping recovery for all logs in $logDirAbsolutePath since clean shutdown file was found\")\n+          // Cache the clean shutdown status and use that for rest of log loading workflow. Delete the CleanShutdownFile\n+          // so that if broker crashes while loading the log, it is considered hard shutdown during the next boot up. KAFKA-10471\n+          cleanShutdownFile.delete()\n+          hadCleanShutdown = true\n         } else {\n           // log recovery itself is being performed by `Log` class during initialization\n           info(s\"Attempting recovery for all logs in $logDirAbsolutePath since no clean shutdown file was found\")\n+          hadCleanShutdown = false", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjMxNzI4OA==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r512317288", "bodyText": "It seems we don't need to set hadCleanShutdown to true.", "author": "junrao", "createdAt": "2020-10-26T22:55:08Z", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -978,10 +1051,10 @@ class LogTest {\n       producerIdExpirationCheckIntervalMs = 30000,\n       topicPartition = Log.parseTopicPartitionName(logDir),\n       producerStateManager = stateManager,\n-      logDirFailureChannel = null)\n+      logDirFailureChannel = null,\n+      hadCleanShutdown = true)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjMxNzQ5Mw==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r512317493", "bodyText": "It seems we don't need to set hadCleanShutdown to true.", "author": "junrao", "createdAt": "2020-10-26T22:55:45Z", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -1021,10 +1092,10 @@ class LogTest {\n       producerIdExpirationCheckIntervalMs = 30000,\n       topicPartition = Log.parseTopicPartitionName(logDir),\n       producerStateManager = stateManager,\n-      logDirFailureChannel = null)\n+      logDirFailureChannel = null,\n+      hadCleanShutdown = true)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3b3eccfdd6d33fbb5227abb324b599cf482a639b", "url": "https://github.com/apache/kafka/commit/3b3eccfdd6d33fbb5227abb324b599cf482a639b", "message": "KAFKA-10471 - Mark broker crash during log loading as unclean shutdown\n\nLogManager writes a clean shutdown file when the broker shuts down. The\npresence of this file indicates that the broker had a clean shutdown and\nlog recovery is not needed upon the next boot up.\n\nEarlier, LogManager would check for this file at the start of log loading workflow,\nand delete it after the log has been loaded. If the broker were to crash\nwhile loading logs, the file would not be deleted and mislead LogManager when it\ntries to load logs upon next boot up. Hence, a crash during log loading\nwill not be considered a hard reset of broker.\n\nAs part of this fix, we delete the clean shutdown file as soon as we\nlook it up, at the start of log loading workflow. Thereafter, we maintain a boolean\nflag to indicate if broker underwent clean shutdown or not. So, if the\nbroker were to crash while logs are being loaded, LogManager will be\nable to detect this as a hard reset.", "committedDate": "2020-10-30T07:09:35Z", "type": "commit"}, {"oid": "784591db20b3a7908db3392d093599126c374651", "url": "https://github.com/apache/kafka/commit/784591db20b3a7908db3392d093599126c374651", "message": "Addressed review comments", "committedDate": "2020-10-30T07:09:35Z", "type": "commit"}, {"oid": "0c93b076c05e96ed3442c1b82870ba9df3d2f793", "url": "https://github.com/apache/kafka/commit/0c93b076c05e96ed3442c1b82870ba9df3d2f793", "message": "Addressed review comments", "committedDate": "2020-10-30T07:09:35Z", "type": "commit"}, {"oid": "248b78e4a15f3e95d5f2eebe897577d791f5de4f", "url": "https://github.com/apache/kafka/commit/248b78e4a15f3e95d5f2eebe897577d791f5de4f", "message": "Review comments", "committedDate": "2020-10-30T07:09:35Z", "type": "commit"}, {"oid": "e6a6d55c7a8cd5141b9373ffc0e2fe545b9d8afe", "url": "https://github.com/apache/kafka/commit/e6a6d55c7a8cd5141b9373ffc0e2fe545b9d8afe", "message": "Addressed review comments", "committedDate": "2020-10-30T07:09:35Z", "type": "commit"}, {"oid": "e6a6d55c7a8cd5141b9373ffc0e2fe545b9d8afe", "url": "https://github.com/apache/kafka/commit/e6a6d55c7a8cd5141b9373ffc0e2fe545b9d8afe", "message": "Addressed review comments", "committedDate": "2020-10-30T07:09:35Z", "type": "forcePushed"}, {"oid": "cdfd934e70a527d15275716581950938ec80a2b0", "url": "https://github.com/apache/kafka/commit/cdfd934e70a527d15275716581950938ec80a2b0", "message": "Minor log message change", "committedDate": "2020-10-30T07:41:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTI2NTU2Ng==", "url": "https://github.com/apache/kafka/pull/9364#discussion_r515265566", "bodyText": "It seems that hadCleanShutdown doesn't need to be exposed as a public val?", "author": "junrao", "createdAt": "2020-10-30T17:32:32Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -244,7 +246,8 @@ class Log(@volatile private var _dir: File,\n           val producerIdExpirationCheckIntervalMs: Int,\n           val topicPartition: TopicPartition,\n           val producerStateManager: ProducerStateManager,\n-          logDirFailureChannel: LogDirFailureChannel) extends Logging with KafkaMetricsGroup {\n+          logDirFailureChannel: LogDirFailureChannel,\n+          val hadCleanShutdown: Boolean = true) extends Logging with KafkaMetricsGroup {", "originalCommit": "cdfd934e70a527d15275716581950938ec80a2b0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "49f1b10c84c4fcb33afe930477a85fb54f606ab2", "url": "https://github.com/apache/kafka/commit/49f1b10c84c4fcb33afe930477a85fb54f606ab2", "message": "Review comment", "committedDate": "2020-10-30T22:49:10Z", "type": "commit"}]}