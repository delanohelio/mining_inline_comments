{"pr_number": 8709, "pr_title": "KAFKA-9952; Remove immediate fetch completion logic on high watermark updates", "pr_createdAt": "2020-05-21T21:31:00Z", "pr_url": "https://github.com/apache/kafka/pull/8709", "timeline": [{"oid": "ce3d96575345358ccf4a960e27886e0c7f7f9220", "url": "https://github.com/apache/kafka/commit/ce3d96575345358ccf4a960e27886e0c7f7f9220", "message": "KAFKA-9952; Remove immediate fetch completion logic on high watermark updates", "committedDate": "2020-05-21T21:27:14Z", "type": "commit"}, {"oid": "52b62fdac7e2feaa2fa8af911005bdd021047fd2", "url": "https://github.com/apache/kafka/commit/52b62fdac7e2feaa2fa8af911005bdd021047fd2", "message": "Minor", "committedDate": "2020-05-22T15:46:32Z", "type": "commit"}, {"oid": "bd0858dcdc648a1615b7299b370d0d93f0e400d5", "url": "https://github.com/apache/kafka/commit/bd0858dcdc648a1615b7299b370d0d93f0e400d5", "message": "Fix update follower benchmark", "committedDate": "2020-05-22T20:38:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1Mzk0Ng==", "url": "https://github.com/apache/kafka/pull/8709#discussion_r429653946", "bodyText": "Interesting. This could cause an early return for the leader case too, right?", "author": "ijuma", "createdAt": "2020-05-24T16:38:49Z", "path": "core/src/main/scala/kafka/server/DelayedFetch.scala", "diffHunk": "@@ -120,14 +119,6 @@ class DelayedFetch(delayMs: Long,\n                   accumulatedSize += bytesAvailable\n               }\n             }\n-\n-            if (fetchMetadata.isFromFollower) {\n-              // Case H check if the follower has the latest HW from the leader\n-              if (partition.getReplica(fetchMetadata.replicaId)\n-                .exists(r => offsetSnapshot.highWatermark.messageOffset > r.lastSentHighWatermark)) {\n-                return forceComplete()\n-              }\n-            }", "originalCommit": "bd0858dcdc648a1615b7299b370d0d93f0e400d5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU0NjM5MQ==", "url": "https://github.com/apache/kafka/pull/8709#discussion_r430546391", "bodyText": "Yeah, we missed this in the other patch.", "author": "hachikuji", "createdAt": "2020-05-26T16:28:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1Mzk0Ng=="}], "type": "inlineReview"}, {"oid": "f75e4a521aa990c17b9c9d9d58d0ca9f2b14eef6", "url": "https://github.com/apache/kafka/commit/f75e4a521aa990c17b9c9d9d58d0ca9f2b14eef6", "message": "Cleanup LogReadResult and remove unused field", "committedDate": "2020-05-26T15:55:44Z", "type": "commit"}, {"oid": "b1dd8a335f3ce082601b0609ac16d1ada0706d96", "url": "https://github.com/apache/kafka/commit/b1dd8a335f3ce082601b0609ac16d1ada0706d96", "message": "Fix comments", "committedDate": "2020-05-26T16:14:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwNzc1Mw==", "url": "https://github.com/apache/kafka/pull/8709#discussion_r430707753", "bodyText": "Could we use the toString from the case class?", "author": "ijuma", "createdAt": "2020-05-26T21:06:16Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -109,9 +109,19 @@ case class LogReadResult(info: FetchDataInfo,\n   def withEmptyFetchInfo: LogReadResult =\n     copy(info = FetchDataInfo(LogOffsetMetadata.UnknownOffsetMetadata, MemoryRecords.EMPTY))\n \n-  override def toString =\n-    s\"Fetch Data: [$info], HW: [$highWatermark], leaderLogStartOffset: [$leaderLogStartOffset], leaderLogEndOffset: [$leaderLogEndOffset], \" +\n-    s\"followerLogStartOffset: [$followerLogStartOffset], fetchTimeMs: [$fetchTimeMs], readSize: [$readSize], lastStableOffset: [$lastStableOffset], error: [$error]\"\n+  override def toString = {\n+    \"LogReadResult(\" +\n+      s\"info=$info, \" +\n+      s\"highWatermark=$highWatermark, \" +\n+      s\"leaderLogStartOffset=$leaderLogStartOffset, \" +\n+      s\"leaderLogEndOffset=$leaderLogEndOffset, \" +\n+      s\"followerLogStartOffset=$followerLogStartOffset, \" +\n+      s\"fetchTimeMs=$fetchTimeMs, \" +\n+      s\"preferredReadReplica=$preferredReadReplica, \" +\n+      s\"lastStableOffset=$lastStableOffset, \" +\n+      s\"error=$error\" +\n+      \")\"", "originalCommit": "b1dd8a335f3ce082601b0609ac16d1ada0706d96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0MDc2NA==", "url": "https://github.com/apache/kafka/pull/8709#discussion_r430740764", "bodyText": "Do we get the labels from the default toString? In the past, I thought it would only show the values.", "author": "hachikuji", "createdAt": "2020-05-26T22:24:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwNzc1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0ODA5OQ==", "url": "https://github.com/apache/kafka/pull/8709#discussion_r430748099", "bodyText": "That's true. You have to write toString yourself if you want labels. We could write a utility method like so (didn't try too compile it and skipped some details like commas):\ndef productToString(product: Product): String = {\n  val builder = new StringBuilder\n  sb.append(product.prefix)\n  for (i <- 0 until product.productArity) {\n    builder.append(product.productElementName(i))\n      .append(\"=\")\n      .append(product.productElement(i))\n  }\n  sb.build()\n}\nThen we can call that method from any case class toString where we want this format. Avoids some duplication and forgetting to update toString when new fields are added.", "author": "ijuma", "createdAt": "2020-05-26T22:45:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwNzc1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI4ODExMA==", "url": "https://github.com/apache/kafka/pull/8709#discussion_r431288110", "bodyText": "Ok, I added something like that.", "author": "hachikuji", "createdAt": "2020-05-27T16:42:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwNzc1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMwMDM1Nw==", "url": "https://github.com/apache/kafka/pull/8709#discussion_r431300357", "bodyText": "Sweet!", "author": "ijuma", "createdAt": "2020-05-27T16:59:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwNzc1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwODQzMw==", "url": "https://github.com/apache/kafka/pull/8709#discussion_r430708433", "bodyText": "Do we still need to compute adjustedMaxBytes? Also, do you know why we don't need readSize anymore? What change made it unnecessary?", "author": "ijuma", "createdAt": "2020-05-26T21:07:39Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1100,10 +1100,8 @@ class ReplicaManager(val config: KafkaConfig,\n             leaderLogEndOffset = readInfo.logEndOffset,\n             followerLogStartOffset = followerLogStartOffset,\n             fetchTimeMs = fetchTimeMs,\n-            readSize = adjustedMaxBytes,", "originalCommit": "b1dd8a335f3ce082601b0609ac16d1ada0706d96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0MDYyOA==", "url": "https://github.com/apache/kafka/pull/8709#discussion_r430740628", "bodyText": "We can get the read size already from the Records object. The code must have been changed at some point to use this. It looks like adjustedMaxBytes is still sent through in the call to Partition.readRecords.", "author": "hachikuji", "createdAt": "2020-05-26T22:24:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwODQzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0NTk1OQ==", "url": "https://github.com/apache/kafka/pull/8709#discussion_r430745959", "bodyText": "Thanks, makes sense.", "author": "ijuma", "createdAt": "2020-05-26T22:39:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcwODQzMw=="}], "type": "inlineReview"}, {"oid": "b1dd8a335f3ce082601b0609ac16d1ada0706d96", "url": "https://github.com/apache/kafka/commit/b1dd8a335f3ce082601b0609ac16d1ada0706d96", "message": "Fix comments", "committedDate": "2020-05-26T16:14:55Z", "type": "forcePushed"}]}