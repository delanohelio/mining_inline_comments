{"pr_number": 9508, "pr_title": "KAFKA-10648: Add Prefix Scan support to State Stores", "pr_createdAt": "2020-10-27T05:18:06Z", "pr_url": "https://github.com/apache/kafka/pull/9508", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4MjUwMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518682502", "bodyText": "Please leave an empty line above to separate parameter description from the description of the method.", "author": "cadonna", "createdAt": "2020-11-06T11:08:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/ReadOnlyKeyValueStore.java", "diffHunk": "@@ -107,4 +108,18 @@\n      * @throws InvalidStateStoreException if the store is not initialized\n      */\n     long approximateNumEntries();\n+\n+    /**\n+     * Get an iterator over keys which have the specified prefix. The type of the prefix can be different from that of\n+     * the key. That's why, callers should also pass a serializer for the prefix to convert the prefix into the\n+     * format in which the keys are stored underneath in the stores\n+     * @param prefix The prefix.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4ODE0Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518688143", "bodyText": "We usually put every sentence on its own line In javadocs. Unfortunately, we are not always consistent as you can see in the javadocs in this file.\nI guess this iterator has similar requirements as the other iterators, i.e., it must be closed, it must be safe from java.util.ConcurrentModificationException, and it must not return null values. If so, please add this requirements to the javadocs.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Get an iterator over keys which have the specified prefix. The type of the prefix can be different from that of\n          \n          \n            \n                 * the key. That's why, callers should also pass a serializer for the prefix to convert the prefix into the\n          \n          \n            \n                 * format in which the keys are stored underneath in the stores\n          \n          \n            \n                 * Return an iterator over all keys with the specified prefix. \n          \n          \n            \n                 * Since the type of the prefix can be different from that of the key, a serializer to convert the\n          \n          \n            \n                 * prefix into the format in which the keys are stored in the stores needs to be passed to this method.", "author": "cadonna", "createdAt": "2020-11-06T11:19:56Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/ReadOnlyKeyValueStore.java", "diffHunk": "@@ -107,4 +108,18 @@\n      * @throws InvalidStateStoreException if the store is not initialized\n      */\n     long approximateNumEntries();\n+\n+    /**\n+     * Get an iterator over keys which have the specified prefix. The type of the prefix can be different from that of\n+     * the key. That's why, callers should also pass a serializer for the prefix to convert the prefix into the\n+     * format in which the keys are stored underneath in the stores", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4OTAzMA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518689030", "bodyText": "I would move this method above approximateNumEntries, so that all methods that return iterators are in one block.", "author": "cadonna", "createdAt": "2020-11-06T11:21:41Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/ReadOnlyKeyValueStore.java", "diffHunk": "@@ -107,4 +108,18 @@\n      * @throws InvalidStateStoreException if the store is not initialized\n      */\n     long approximateNumEntries();\n+\n+    /**\n+     * Get an iterator over keys which have the specified prefix. The type of the prefix can be different from that of\n+     * the key. That's why, callers should also pass a serializer for the prefix to convert the prefix into the\n+     * format in which the keys are stored underneath in the stores\n+     * @param prefix The prefix.\n+     * @param prefixKeySerializer Serializer for the Prefix key type\n+     * @param <PS> Prefix Serializer type\n+     * @param <P> Prefix Type.\n+     * @return The iterator for keys having the specified prefix.\n+     */\n+    default <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(P prefix, PS prefixKeySerializer) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY5NzMwNA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518697304", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {\n          \n          \n            \n                    return wrapped().prefixScan(prefix, prefixKeySerializer);\n          \n          \n            \n                }\n          \n          \n            \n                public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, \n          \n          \n            \n                                                                                                final PS prefixKeySerializer) {\n          \n          \n            \n                    return wrapped().prefixScan(prefix, prefixKeySerializer);\n          \n          \n            \n                }", "author": "cadonna", "createdAt": "2020-11-06T11:39:18Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStore.java", "diffHunk": "@@ -97,6 +98,11 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+        return wrapped().prefixScan(prefix, prefixKeySerializer);\n+    }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY5ODc3OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518698778", "bodyText": "Unit test for this method is missing.", "author": "cadonna", "createdAt": "2020-11-06T11:42:27Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStore.java", "diffHunk": "@@ -97,6 +98,11 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODcwMDYyOA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518700628", "bodyText": "This method needs unit testing.", "author": "cadonna", "createdAt": "2020-11-06T11:46:26Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java", "diffHunk": "@@ -103,6 +105,19 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU4MzEwNw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527583107", "bodyText": "This method still needs unit testing.", "author": "cadonna", "createdAt": "2020-11-20T10:04:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODcwMDYyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODcwMjEyNw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518702127", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new DelegatingPeekingKeyValueIterator<>(\n          \n          \n            \n                            name,\n          \n          \n            \n                            new InMemoryKeyValueIterator(map.subMap(from, true, to, false).keySet(), true));\n          \n          \n            \n                    return new DelegatingPeekingKeyValueIterator<>(\n          \n          \n            \n                        name,\n          \n          \n            \n                        new InMemoryKeyValueIterator(map.subMap(from, true, to, false).keySet(), true)\n          \n          \n            \n                    );", "author": "cadonna", "createdAt": "2020-11-06T11:49:38Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java", "diffHunk": "@@ -103,6 +105,19 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+        Objects.requireNonNull(prefix, \"prefix cannot be null\");\n+        Objects.requireNonNull(prefixKeySerializer, \"prefixKeySerializer cannot be null\");\n+\n+        final Bytes from = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n+        final Bytes to = Bytes.increment(from);\n+\n+        return new DelegatingPeekingKeyValueIterator<>(\n+                name,\n+                new InMemoryKeyValueIterator(map.subMap(from, true, to, false).keySet(), true));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODczMjE4OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518732188", "bodyText": "This method needs unit testing.", "author": "cadonna", "createdAt": "2020-11-06T12:53:10Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +230,15 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODczNzcwNA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518737704", "bodyText": "Please use meaningful names for the paramters instead of x and y.", "author": "cadonna", "createdAt": "2020-11-06T13:04:08Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.rocksdb.RocksIterator;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+\n+class RocksDBPrefixIterator extends RocksDbIterator {\n+    private byte[] rawPrefix;\n+\n+    RocksDBPrefixIterator(final String name,\n+                          final RocksIterator newIterator,\n+                          final Set<KeyValueIterator<Bytes, byte[]>> openIterators,\n+                          final Bytes prefix) {\n+        super(name, newIterator, openIterators, true);\n+        this.rawPrefix = prefix.get();\n+        newIterator.seek(rawPrefix);\n+    }\n+\n+    private boolean prefixEquals(final byte[] x, final byte[] y) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc0MTA1Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518741056", "bodyText": "This class needs unit testing.", "author": "cadonna", "createdAt": "2020-11-06T13:10:52Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.rocksdb.RocksIterator;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+\n+class RocksDBPrefixIterator extends RocksDbIterator {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTk2OTkxMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r519969912", "bodyText": "This one, do I need explicit unit tests? I noticed that something like RocksDbRangeIterator also doesn't have unit tests.", "author": "vamossagar12", "createdAt": "2020-11-09T16:59:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc0MTA1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYxMjk0Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527612942", "bodyText": "Yes, you need explicit unit tests. If there are no unit tests for RocksDbRangeIterator then that is a hole in our test coverage. You do not need to add the tests for RocksDbRangeIterator but could you please create a ticket to document the missing unit tests? Of course, you are very welcome to assign the ticket to yourself and add the missing tests in a separate PR.", "author": "cadonna", "createdAt": "2020-11-20T10:56:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc0MTA1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc0NDExMw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518744113", "bodyText": "Please put the second parameter on a new line and align the two parameters like:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {\n          \n          \n            \n                public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, \n          \n          \n            \n                                                                                                final PS prefixKeySerializer) {\n          \n      \n    \n    \n  \n\n(On GitHub the suggestions seems a bit strange)", "author": "cadonna", "createdAt": "2020-11-06T13:16:48Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -308,6 +309,20 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc0NzkwNQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518747905", "bodyText": "Why is the prefix scan not supported here? The timstamped stores are the default stores in Kafka Streams.", "author": "cadonna", "createdAt": "2020-11-06T13:23:59Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java", "diffHunk": "@@ -256,6 +256,11 @@ public void close() {\n             oldColumnFamily.close();\n             newColumnFamily.close();\n         }\n+\n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            throw new UnsupportedOperationException();\n+        }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc1ODE4Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518758182", "bodyText": "You do not need to test the prefix scan by using the topology test driver because you actually do not need a topology. Your addition is limited to the state stores and those you should unit test. Furthermore, this test is misplaced because you did not change anything in ProcessorTopology. Please remove this test and add unit tests where I indicated the need for them.\nAlthough, you will find a lot of test named test*, we name new tests with should* followed by the expected result, e.g., shouldThrowIllegalStateException or shouldGetRecordsWithPrefixedKey.", "author": "cadonna", "createdAt": "2020-11-06T13:42:26Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorTopologyTest.java", "diffHunk": "@@ -273,6 +275,43 @@ public void testDrivingStatefulTopology() {\n         assertNull(store.get(\"key4\"));\n     }\n \n+    @Test\n+    public void testPrefixScanStatefulTopology() {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc1OTUyOA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518759528", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2020-11-06T13:44:43Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,118 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k1\")),\n+                stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+                stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k2\")),\n+                stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+                stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k3\")),\n+                stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+                stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final String[] valuesWithPrefix = new String[3];\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix[numberOfKeysReturned++] = new String(next.value);\n+        }\n+        // Since there are 3 keys prefixed with prefix, the count should be 3\n+        assertEquals(3, numberOfKeysReturned);\n+        // The order might seem inverted to the order in which keys were inserted, but since Rocksdb stores keys\n+        // lexicographically, prefix_1 would still be the first key that is returned.\n+        assertEquals(valuesWithPrefix[0], \"f\");\n+        assertEquals(valuesWithPrefix[1], \"d\");\n+        assertEquals(valuesWithPrefix[2], \"b\");\n+\n+        // Lastly, simple key value lookups should still work :)\n+        assertEquals(\n+                \"c\",\n+                stringDeserializer.deserialize(\n+                        null,\n+                        rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"k2\")))));\n+", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc2Mjk2OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518762968", "bodyText": "Please fix the indentation and the newlines also in the other tests as follows:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                            new Bytes(stringSerializer.serialize(null, \"k1\")),\n          \n          \n            \n                            stringSerializer.serialize(null, \"a\")));\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n          \n          \n            \n                            stringSerializer.serialize(null, \"b\")));\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                            new Bytes(stringSerializer.serialize(null, \"k2\")),\n          \n          \n            \n                            stringSerializer.serialize(null, \"c\")));\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n          \n          \n            \n                            stringSerializer.serialize(null, \"d\")));\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                            new Bytes(stringSerializer.serialize(null, \"k3\")),\n          \n          \n            \n                            stringSerializer.serialize(null, \"e\")));\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n          \n          \n            \n                            stringSerializer.serialize(null, \"f\")));\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                        new Bytes(stringSerializer.serialize(null, \"k1\")),\n          \n          \n            \n                        stringSerializer.serialize(null, \"a\"))\n          \n          \n            \n                    );\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                        new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n          \n          \n            \n                        stringSerializer.serialize(null, \"b\"))\n          \n          \n            \n                    );\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                        new Bytes(stringSerializer.serialize(null, \"k2\")),\n          \n          \n            \n                        stringSerializer.serialize(null, \"c\"))\n          \n          \n            \n                    );\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                        new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n          \n          \n            \n                        stringSerializer.serialize(null, \"d\"))\n          \n          \n            \n                    );\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                        new Bytes(stringSerializer.serialize(null, \"k3\")),\n          \n          \n            \n                        stringSerializer.serialize(null, \"e\"))\n          \n          \n            \n                    );\n          \n          \n            \n                    entries.add(new KeyValue<>(\n          \n          \n            \n                        new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n          \n          \n            \n                        stringSerializer.serialize(null, \"f\"))\n          \n          \n            \n                    );", "author": "cadonna", "createdAt": "2020-11-06T13:50:29Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,118 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k1\")),\n+                stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+                stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k2\")),\n+                stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+                stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k3\")),\n+                stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+                stringSerializer.serialize(null, \"f\")));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc2NTM5MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518765391", "bodyText": "We prefer to use assertThat() in new test, because it is better readable. This line would then become:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    assertEquals(3, numberOfKeysReturned);\n          \n          \n            \n                   assertThat(numberOfKeysReturned, is(3));\n          \n      \n    \n    \n  \n\nPlease replace also the other verifications with assertThat() and fix the indentation (should be 4 spaces, not 8).", "author": "cadonna", "createdAt": "2020-11-06T13:54:36Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,118 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k1\")),\n+                stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+                stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k2\")),\n+                stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+                stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k3\")),\n+                stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+                stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final String[] valuesWithPrefix = new String[3];\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix[numberOfKeysReturned++] = new String(next.value);\n+        }\n+        // Since there are 3 keys prefixed with prefix, the count should be 3\n+        assertEquals(3, numberOfKeysReturned);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc2NTc0MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518765741", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2020-11-06T13:55:13Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,118 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k1\")),\n+                stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+                stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k2\")),\n+                stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+                stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k3\")),\n+                stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+                stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final String[] valuesWithPrefix = new String[3];\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix[numberOfKeysReturned++] = new String(next.value);\n+        }\n+        // Since there are 3 keys prefixed with prefix, the count should be 3\n+        assertEquals(3, numberOfKeysReturned);\n+        // The order might seem inverted to the order in which keys were inserted, but since Rocksdb stores keys\n+        // lexicographically, prefix_1 would still be the first key that is returned.\n+        assertEquals(valuesWithPrefix[0], \"f\");\n+        assertEquals(valuesWithPrefix[1], \"d\");\n+        assertEquals(valuesWithPrefix[2], \"b\");\n+\n+        // Lastly, simple key value lookups should still work :)\n+        assertEquals(\n+                \"c\",\n+                stringDeserializer.deserialize(\n+                        null,\n+                        rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"k2\")))));\n+\n+    }\n+\n+    @Test\n+    public void shouldReturnUUIDsWithStringPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        final Serializer<UUID> uuidSerializer = Serdes.UUID().serializer();\n+        final UUID uuid1 = UUID.randomUUID();\n+        final UUID uuid2 = UUID.randomUUID();\n+        final String prefix = uuid1.toString().substring(0, 4);\n+        entries.add(new KeyValue<>(\n+                new Bytes(uuidSerializer.serialize(null, uuid1)),\n+                stringSerializer.serialize(null, \"a\")));\n+\n+        entries.add(new KeyValue<>(\n+                new Bytes(uuidSerializer.serialize(null, uuid2)),\n+                stringSerializer.serialize(null, \"b\")));\n+\n+", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc2OTkxMA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r518769910", "bodyText": "I would rather use a List instead of the array and then verify if the length of the list is three. That will give us a more meaningful assertion error than the IndexOutOfBoundsException that we would get with the array in case of failure.", "author": "cadonna", "createdAt": "2020-11-06T14:02:08Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,118 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k1\")),\n+                stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+                stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k2\")),\n+                stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+                stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"k3\")),\n+                stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+                new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+                stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final String[] valuesWithPrefix = new String[3];\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix[numberOfKeysReturned++] = new String(next.value);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU4MzYxNA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527583614", "bodyText": "nit: The last parenthesis should go to a new line. I know that we are not consistent throughout the code base (that is why this comment is prefixed with \"nit\") but that is actually the code style, we agreed upon. If you need to push another commit you can fix this, otherwise it's fine.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new DelegatingPeekingKeyValueIterator<>(\n          \n          \n            \n                        name,\n          \n          \n            \n                        new InMemoryKeyValueIterator(map.subMap(from, true, to, false).keySet(), true));\n          \n          \n            \n                    return new DelegatingPeekingKeyValueIterator<>(\n          \n          \n            \n                        name,\n          \n          \n            \n                        new InMemoryKeyValueIterator(map.subMap(from, true, to, false).keySet(), true)\n          \n          \n            \n                    );", "author": "cadonna", "createdAt": "2020-11-20T10:05:09Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java", "diffHunk": "@@ -103,6 +105,19 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+        Objects.requireNonNull(prefix, \"prefix cannot be null\");\n+        Objects.requireNonNull(prefixKeySerializer, \"prefixKeySerializer cannot be null\");\n+\n+        final Bytes from = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n+        final Bytes to = Bytes.increment(from);\n+\n+        return new DelegatingPeekingKeyValueIterator<>(\n+            name,\n+            new InMemoryKeyValueIterator(map.subMap(from, true, to, false).keySet(), true));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU4NzUyMA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527587520", "bodyText": "nit: Sorry, I overlooked this last time. We actually use 4 space and not 8.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new MeteredKeyValueIterator(\n          \n          \n            \n                            wrapped().prefixScan(prefix, prefixKeySerializer),\n          \n          \n            \n                            rangeSensor\n          \n          \n            \n                    );\n          \n          \n            \n                    return new MeteredKeyValueIterator(\n          \n          \n            \n                        wrapped().prefixScan(prefix, prefixKeySerializer),\n          \n          \n            \n                        rangeSensor\n          \n          \n            \n                    );", "author": "cadonna", "createdAt": "2020-11-20T10:12:07Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +230,15 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+\n+        return new MeteredKeyValueIterator(\n+                wrapped().prefixScan(prefix, prefixKeySerializer),\n+                rangeSensor\n+        );", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU5NTQ3Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527595476", "bodyText": "I am not sure whether we should use the rangeSensor here or introduce a new prefixScanSensor. I looked into the KIP discussion but could not find any reference to metrics. Either I missed it or we missed it in the KIP discussion. What do the reviewers of the KIP think @ableegoldman @vvcephei @guozhangwang?", "author": "cadonna", "createdAt": "2020-11-20T10:25:41Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +230,15 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+\n+        return new MeteredKeyValueIterator(\n+                wrapped().prefixScan(prefix, prefixKeySerializer),\n+                rangeSensor", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDQwMzE5Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r530403193", "bodyText": "@cadonna , you are right, the KIP and the KIP discussion didn't involve a discussion on this. I would love to hear the thoughts of yours and the reviewers.", "author": "vamossagar12", "createdAt": "2020-11-25T14:13:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU5NTQ3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjQzODQ4Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r532438482", "bodyText": "I think, to be consistent we should add a new sensor for the prefix scan. We have a sensor for each operation on a key-value store. The only exception is reverseRange(), which is a variant of range and does not necessarily qualify for its own sensor.", "author": "cadonna", "createdAt": "2020-11-30T09:03:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU5NTQ3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM5MjUxNw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r533392517", "bodyText": "Created prefixScan sensor.", "author": "vamossagar12", "createdAt": "2020-12-01T13:06:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU5NTQ3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU5OTk2Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527599966", "bodyText": "The following request is not a requirement to get the PR approved, but rather optional extra work to improve the code base. Could you rewrite this test with a mock for the inner state store as in MeteredKeyValueStoreTest?", "author": "cadonna", "createdAt": "2020-11-20T10:33:19Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemoryKeyValueStoreTest.java", "diffHunk": "@@ -359,6 +361,31 @@ public void shouldReverseIterateOverRange() {\n         ), results);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(bytesKey(\"k1\"), bytesValue(\"1\")));\n+        entries.add(new KeyValue<>(bytesKey(\"k2\"), bytesValue(\"2\")));\n+        entries.add(new KeyValue<>(bytesKey(\"p2\"), bytesValue(\"2\")));\n+        entries.add(new KeyValue<>(bytesKey(\"p1\"), bytesValue(\"2\")));\n+        entries.add(new KeyValue<>(bytesKey(\"p0\"), bytesValue(\"2\")));\n+        store.putAll(entries);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"p\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            keys.add(next.key.toString());\n+            values.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(3));\n+        assertThat(keys, is(Arrays.asList(\"p0\", \"p1\", \"p2\")));\n+        assertThat(values, is(Arrays.asList(\"2\", \"2\", \"2\")));\n+    }", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIxMzc0Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r532213742", "bodyText": "@cadonna , I noticed that for this particular Test class, some test cases like shouldDelegateDeprecatedInit and shouldDelegateInit  use mocking the inner store and other ones use the explicitly created object. If you are ok, I can create a separate ticket and take care of making all test cases similar to MeteredKeyValueStoreTest where all of them use the same mocked inner store and let this one as is. Let me know if that's ok with you.", "author": "vamossagar12", "createdAt": "2020-11-29T13:59:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU5OTk2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjQ0MDk0Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r532440942", "bodyText": "Creating a ticket and take care that the ticket gets resolved, sound good to me. As I wote, my request is not a requirement for approval.", "author": "cadonna", "createdAt": "2020-11-30T09:07:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU5OTk2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM5MzU1Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r533393552", "bodyText": "Sure Thanks https://issues.apache.org/jira/browse/KAFKA-10788", "author": "vamossagar12", "createdAt": "2020-12-01T13:08:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU5OTk2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYwMTUyMA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527601520", "bodyText": "See my comment in CachingInMemoryKeyValueStoreTest. Also this not required.", "author": "cadonna", "createdAt": "2020-11-20T10:35:27Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +200,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(hello, world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"h\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            keys.add(next.key.toString());\n+            values.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(2));\n+        assertThat(keys, is(Arrays.asList(\"hello\", \"hi\")));\n+        assertThat(values, is(Arrays.asList(\"world\", \"there\")));\n+    }\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIxMzg5Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r532213896", "bodyText": "Same comment as in CachingInMemoryKeyValueStoreTest", "author": "vamossagar12", "createdAt": "2020-11-29T14:01:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYwMTUyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM5NDI3NA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r533394274", "bodyText": "Created ticket here: https://issues.apache.org/jira/browse/KAFKA-10789", "author": "vamossagar12", "createdAt": "2020-12-01T13:09:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYwMTUyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYwMzA4Nw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527603087", "bodyText": "Why not simply:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                synchronized Iterator<Bytes> keyRange(final Bytes from, final Bytes to, final boolean toInclusive) {\n          \n          \n            \n                    if (toInclusive)\n          \n          \n            \n                        keyRange(from, to);\n          \n          \n            \n                    return keySetIterator(cache.navigableKeySet().subSet(from, true, to, false), true);\n          \n          \n            \n                }\n          \n          \n            \n                synchronized Iterator<Bytes> keyRange(final Bytes from, final Bytes to, final boolean toInclusive) {\n          \n          \n            \n                    return keySetIterator(cache.navigableKeySet().subSet(from, true, to, toInclusive), true);\n          \n          \n            \n                }", "author": "cadonna", "createdAt": "2020-11-20T10:38:02Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java", "diffHunk": "@@ -284,6 +284,12 @@ public boolean isEmpty() {\n         return keySetIterator(cache.navigableKeySet().subSet(from, true, to, true), true);\n     }\n \n+    synchronized Iterator<Bytes> keyRange(final Bytes from, final Bytes to, final boolean toInclusive) {\n+        if (toInclusive)\n+            keyRange(from, to);\n+        return keySetIterator(cache.navigableKeySet().subSet(from, true, to, false), true);\n+    }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYwODAxNA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527608014", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (next == null) return allDone();\n          \n          \n            \n                    else {\n          \n          \n            \n                        if (prefixEquals(this.rawPrefix, next.key.get())) return next;\n          \n          \n            \n                        else return allDone();\n          \n          \n            \n                    }\n          \n          \n            \n                    if (next == null || !prefixEquals(this.rawPrefix, next.key.get())) {\n          \n          \n            \n                        return allDone();\n          \n          \n            \n                    } else {\n          \n          \n            \n                        return next;\n          \n          \n            \n                    }", "author": "cadonna", "createdAt": "2020-11-20T10:47:07Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.rocksdb.RocksIterator;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+\n+class RocksDBPrefixIterator extends RocksDbIterator {\n+    private final byte[] rawPrefix;\n+\n+    RocksDBPrefixIterator(final String name,\n+                          final RocksIterator newIterator,\n+                          final Set<KeyValueIterator<Bytes, byte[]>> openIterators,\n+                          final Bytes prefix) {\n+        super(name, newIterator, openIterators, true);\n+        this.rawPrefix = prefix.get();\n+        newIterator.seek(rawPrefix);\n+    }\n+\n+    private boolean prefixEquals(final byte[] prefix1, final byte[] prefix2) {\n+        final int min = Math.min(prefix1.length, prefix2.length);\n+        final ByteBuffer prefix1Slice = ByteBuffer.wrap(prefix1, 0, min);\n+        final ByteBuffer prefix2Slice = ByteBuffer.wrap(prefix2, 0, min);\n+        return prefix1Slice.equals(prefix2Slice);\n+    }\n+\n+    @Override\n+    public KeyValue<Bytes, byte[]> makeNext() {\n+        final KeyValue<Bytes, byte[]> next = super.makeNext();\n+        if (next == null) return allDone();\n+        else {\n+            if (prefixEquals(this.rawPrefix, next.key.get())) return next;\n+            else return allDone();\n+        }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYxNTE4OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527615188", "bodyText": "With this code abcd would be a prefix of abc. Is this intended? Those cases should be tested in the missing unit tests for this class.", "author": "cadonna", "createdAt": "2020-11-20T11:00:26Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.rocksdb.RocksIterator;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+\n+class RocksDBPrefixIterator extends RocksDbIterator {\n+    private final byte[] rawPrefix;\n+\n+    RocksDBPrefixIterator(final String name,\n+                          final RocksIterator newIterator,\n+                          final Set<KeyValueIterator<Bytes, byte[]>> openIterators,\n+                          final Bytes prefix) {\n+        super(name, newIterator, openIterators, true);\n+        this.rawPrefix = prefix.get();\n+        newIterator.seek(rawPrefix);\n+    }\n+\n+    private boolean prefixEquals(final byte[] prefix1, final byte[] prefix2) {\n+        final int min = Math.min(prefix1.length, prefix2.length);\n+        final ByteBuffer prefix1Slice = ByteBuffer.wrap(prefix1, 0, min);\n+        final ByteBuffer prefix2Slice = ByteBuffer.wrap(prefix2, 0, min);\n+        return prefix1Slice.equals(prefix2Slice);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDQxNDkzOA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r530414938", "bodyText": "Ticket for unit test cases for RocksDBRangeIterator: https://issues.apache.org/jira/browse/KAFKA-10766.\nRegarding abcd being returned as the prefix for abc, I added a local test where I added an entry with key abc\n       entries.add(new KeyValue<>(new Bytes(stringSerializer.serialize(null, \"abc\")),stringSerializer.serialize(null, \"f\")));\nand did a prefixScan for abcd\n`\nnumberOfKeysReturned = 0;\n    while (keysWithPrefix1.hasNext()) {\n        System.out.println(Arrays.toString(keysWithPrefix1.next().key.get()));\n        numberOfKeysReturned++;\n    }\n\n    assertThat(numberOfKeysReturned, is(0));\n\n`\nThat is because the iterator first seeks to the rawPrefix passed (abcd) in this case and then finds keys which match the prefix and are lexicographically more than it.\nBut as you said, going by the method prefixEquals, yes there's a possibility there.", "author": "vamossagar12", "createdAt": "2020-11-25T14:29:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYxNTE4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjQ1MTIxNw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r532451217", "bodyText": "I see what you mean. But why do you bother then to find the minimum length of the two prefixes? You could just use the length of the first prefix.", "author": "cadonna", "createdAt": "2020-11-30T09:24:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYxNTE4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM5NDc3OQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r533394779", "bodyText": "Yeah I have actually simplified the logic. Don't think I needed prefix1 and prefix2 as one of the prefixes is always fixed(i.e the prefix we are searching for).\nI have created a unit test class for RocksDBPrefixIterator.", "author": "vamossagar12", "createdAt": "2020-12-01T13:10:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYxNTE4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYyMzg3MA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527623870", "bodyText": "Do we need this verification? This is covered by other unit test methods, isn't it? Could you please remove it from here?", "author": "cadonna", "createdAt": "2020-11-20T11:17:10Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,115 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        // Since there are 3 keys prefixed with prefix, the count should be 3\n+        assertThat(numberOfKeysReturned, is(3));\n+        // The order might seem inverted to the order in which keys were inserted, but since Rocksdb stores keys\n+        // lexicographically, prefix_1 would still be the first key that is returned.\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        // Lastly, simple key value lookups should still work :)\n+        assertThat(\"c\", is(stringDeserializer.deserialize(null,\n+            rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"k2\"))))));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYyNjY4Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527626682", "bodyText": "Could you please remove the inline comments? They do not add too much information.", "author": "cadonna", "createdAt": "2020-11-20T11:22:59Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,115 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        // Since there are 3 keys prefixed with prefix, the count should be 3\n+        assertThat(numberOfKeysReturned, is(3));\n+        // The order might seem inverted to the order in which keys were inserted, but since Rocksdb stores keys\n+        // lexicographically, prefix_1 would still be the first key that is returned.\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        // Lastly, simple key value lookups should still work :)\n+        assertThat(\"c\", is(stringDeserializer.deserialize(null,\n+            rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"k2\"))))));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYyNzczMA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527627730", "bodyText": "Please remove this comment.", "author": "cadonna", "createdAt": "2020-11-20T11:25:01Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,115 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        // Since there are 3 keys prefixed with prefix, the count should be 3\n+        assertThat(numberOfKeysReturned, is(3));\n+        // The order might seem inverted to the order in which keys were inserted, but since Rocksdb stores keys\n+        // lexicographically, prefix_1 would still be the first key that is returned.\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        // Lastly, simple key value lookups should still work :)\n+        assertThat(\"c\", is(stringDeserializer.deserialize(null,\n+            rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"k2\"))))));\n+    }\n+\n+    @Test\n+    public void shouldReturnUUIDsWithStringPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        final Serializer<UUID> uuidSerializer = Serdes.UUID().serializer();\n+        final UUID uuid1 = UUID.randomUUID();\n+        final UUID uuid2 = UUID.randomUUID();\n+        final String prefix = uuid1.toString().substring(0, 4);\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid1)),\n+            stringSerializer.serialize(null, \"a\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid2)),\n+            stringSerializer.serialize(null, \"b\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(prefix, stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+        assertThat(valuesWithPrefix.get(0), is(\"a\"));\n+    }\n+\n+    @Test\n+    public void shouldReturnNoKeys() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"a\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"b\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"c\")),\n+            stringSerializer.serialize(null, \"e\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"d\", stringSerializer);\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            keysWithPrefix.next();\n+            numberOfKeysReturned++;\n+        }\n+        // Since there are no keys prefixed with d, the count should be 0", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYyOTY5Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527629692", "bodyText": "Please remove comment.", "author": "cadonna", "createdAt": "2020-11-20T11:28:51Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java", "diffHunk": "@@ -431,4 +441,36 @@ public Bytes peekNextKey() {\n             }\n         }\n     }\n+\n+    private class RocksDBDualCFPrefixIterator extends RocksDBDualCFIterator {\n+        private final byte[] rawPrefix;\n+\n+        // In Prefix scan mode, we always move in forward direction.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYzMzUyMw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527633523", "bodyText": "This method need unit testing. I realized that also here we missed to cover all methods with unit tests. For example, there are also no unit tests for all(). Could you please add the ones for prefixScan()? Also here you are really welcome to add unit tests for the other methods in a separate PR, but it is not a requirement for the approval of this PR.", "author": "cadonna", "createdAt": "2020-11-20T11:36:53Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -201,6 +201,14 @@ public MemoryLRUCacheBytesIterator reverseAll(final String namespace) {\n         return new MemoryLRUCacheBytesIterator(cache.reverseAllKeys(), cache);\n     }\n \n+    public MemoryLRUCacheBytesIterator prefixScan(final String namespace, final Bytes from, final Bytes to) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDQyMDI3OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r530420278", "bodyText": "I created a ticket for adding test cases for the missed methods in ThreadCache. https://issues.apache.org/jira/browse/KAFKA-10767", "author": "vamossagar12", "createdAt": "2020-11-25T14:37:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYzMzUyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYzOTE0Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r527639142", "bodyText": "I guess, it would be more consistent to add your verifications to iteratorsShouldNotMigrateData() where all other iterators are tested.", "author": "cadonna", "createdAt": "2020-11-20T11:48:07Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStoreTest.java", "diffHunk": "@@ -130,6 +134,8 @@ public void shouldMigrateDataFromDefaultToTimestampColumnFamily() throws Excepti\n         // approx: 7 entries on old CF, 0 in new CF\n         assertThat(rocksDBStore.approximateNumEntries(), is(7L));\n \n+        // prefix scan should return 7 keys with prefix \"key\"\n+        assertThat(runPrefixScan(\"key\"), is(7));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "url": "https://github.com/apache/kafka/commit/0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2020-12-01T13:03:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQxMTcwMQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544411701", "bodyText": "This is not strictly necessary since you test the mock result you provide which has nothing to do with the code under test.", "author": "cadonna", "createdAt": "2020-12-16T15:53:05Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,23 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+                .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n+        assertThat(iterator.next().value, equalTo(VALUE));\n+        assertFalse(iterator.hasNext());", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUwMTQ1NQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544501455", "bodyText": "Please change prefixScan to prefix-scan. For metrics name we do not use camel case. See put-all or put-if-absent for examples.", "author": "cadonna", "createdAt": "2020-12-16T17:50:51Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -110,6 +110,14 @@ private StateStoreMetrics() {}\n     private static final String RANGE_AVG_LATENCY_DESCRIPTION = AVG_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n     private static final String RANGE_MAX_LATENCY_DESCRIPTION = MAX_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n \n+    private static final String PREFIX_SCAN = \"prefixScan\";\n+    private static final String PREFIX_SCAN_DESCRIPTION = \"calls to prefixScan\";", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUwNDc5MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544504791", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new MeteredKeyValueIterator(\n          \n          \n            \n                        wrapped().prefixScan(prefix, prefixKeySerializer),\n          \n          \n            \n                        prefixScanSensor\n          \n          \n            \n                    );\n          \n          \n            \n                    return new MeteredKeyValueIterator(wrapped().prefixScan(prefix, prefixKeySerializer), prefixScanSensor);", "author": "cadonna", "createdAt": "2020-12-16T17:55:34Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +232,15 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+\n+        return new MeteredKeyValueIterator(\n+            wrapped().prefixScan(prefix, prefixKeySerializer),\n+            prefixScanSensor\n+        );", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUwNTc1OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544505758", "bodyText": "nit: Please use 4 instead of 8 spaces indentation.", "author": "cadonna", "createdAt": "2020-12-16T17:56:59Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,23 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+                .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUxNzAxMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544517012", "bodyText": "Sorry, if I haven't noticed it before. Could you move this method up before approximateNumEntries() to have all operations in one block?", "author": "cadonna", "createdAt": "2020-12-16T18:13:55Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -627,6 +645,11 @@ public void addToBatch(final byte[] key,\n         public void close() {\n             columnFamily.close();\n         }\n+\n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            return new RocksDBPrefixIterator(name, db.newIterator(columnFamily), openIterators, prefix);\n+        }", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzMzk5Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545033996", "bodyText": "nit: I would just add parameter toInclusive to the existing method instead of creating an overload.", "author": "cadonna", "createdAt": "2020-12-17T11:56:54Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java", "diffHunk": "@@ -284,6 +284,10 @@ public boolean isEmpty() {\n         return keySetIterator(cache.navigableKeySet().subSet(from, true, to, true), true);\n     }\n \n+    synchronized Iterator<Bytes> keyRange(final Bytes from, final Bytes to, final boolean toInclusive) {\n+        return keySetIterator(cache.navigableKeySet().subSet(from, true, to, toInclusive), true);\n+    }", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545040031", "bodyText": "I actually do not completely understand why we need a specific iterator for the prefix scan. We could just as good extend RocksDBRangeIterator to consider or not consider the end result of the range. We can do that because those iterator implementations are internal and the public API does not care which iterator is used as long as it implements interface KeyValueIterator and the behavior is correct. Extending and re-using RocksDBRangeIterator would lead to less code to maintain. Note, I agree that we need the public method prefixScan(), but what the implementation uses internally is not relevant for the KIP as long as it is correct. Did I miss something that imposes the implementation for a separate iterator?", "author": "cadonna", "createdAt": "2020-12-17T12:07:11Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.rocksdb.RocksIterator;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+\n+class RocksDBPrefixIterator extends RocksDbIterator {", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk1ODk3MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545958971", "bodyText": "Well the only reason i chose to add a separate iterator is that for prefix scan there is no end key which could be known upfront. We could pass a null end key but that's prohibited in RocksDBRangeIterator constructor and i don't think we should be changing that condition or we could pass in the same value or from and to and add a condition in makeNext() for prefix scan. i thought having a separate iterator might be cleaner. You have some other approach in mind?", "author": "vamossagar12", "createdAt": "2020-12-18T16:51:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk4NjIwNA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545986204", "bodyText": "Could you not use the same technique as you use in the in-memory key-value state store and the cache?\n        final Bytes from = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n        final Bytes to = Bytes.increment(from);", "author": "cadonna", "createdAt": "2020-12-18T17:39:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjY0MzI3Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r546643272", "bodyText": "I tested with that approach. It worked for all cases apart from the abcd/abce case that you mentioned below. So, we will need a way to signal to the Iterator that for the case of range iteration for prefixScan, it should ignore the last key. I suppose we will have to add another flag similar to the ones used in in memory key-value store.\nBTW, the other reason why I created a separate prefix iterator was that there was an old unused prefix iterator, So, I thought of repurposing it :)", "author": "vamossagar12", "createdAt": "2020-12-21T11:02:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4MzQxOQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r553483419", "bodyText": "hey @cadonna , did you get a chance to look at the comment?", "author": "vamossagar12", "createdAt": "2021-01-07T17:45:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEyNDY3NQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r555124675", "bodyText": "Sorry for the late reply but I was on holidays.\n\nI suppose we will have to add another flag similar to the ones used in in memory key-value store.\n\nThis is what I meant with \"extend RocksDBRangeIterator\".\nI think less code is easier maintainable. In addition, I think the code is easier to follow if cache, in-memory, and RocksDB use the same technique for the prefix scan unless we have a technique with better performance that is specific to one of those components. So I would be in favor of just using RocksDBRangeIterator.", "author": "cadonna", "createdAt": "2021-01-11T15:25:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEzNDg2Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r555134866", "bodyText": "Thanks for the reply. Sure, I would make the relevant changes.", "author": "vamossagar12", "createdAt": "2021-01-11T15:39:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTE0MDg3Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r555140872", "bodyText": "Awesome!", "author": "cadonna", "createdAt": "2021-01-11T15:48:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjQ5Nzk2NA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r556497964", "bodyText": "As discussed, I have extended RocksDBRangeIterator to include the toInclusive flag.\n1 point to note here is that with this change, RocksDBPrefixIterator is no longer needed so I have removed it and it's relevant test case.\nAlso, the unit tests for prefix scan, i have a separate PR 9717 where i am writing the unit tests for RangeIterator. I plan to add the prefix scan cases there. would that be ok?", "author": "vamossagar12", "createdAt": "2021-01-13T12:53:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NDQwMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545044402", "bodyText": "If you consider my comment in NamedCache, you could also just call range() here. Or -- if you want to be more descriptive -- having prefixScan() and range() in this class calling a private overload of range() with a flag that excludes or includes the end of the range. That would deduplicate code.", "author": "cadonna", "createdAt": "2020-12-17T12:14:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -201,6 +201,14 @@ public MemoryLRUCacheBytesIterator reverseAll(final String namespace) {\n         return new MemoryLRUCacheBytesIterator(cache.reverseAllKeys(), cache);\n     }\n \n+    public MemoryLRUCacheBytesIterator prefixScan(final String namespace, final Bytes from, final Bytes to) {\n+        final NamedCache cache = getCache(namespace);\n+        if (cache == null) {\n+            return new MemoryLRUCacheBytesIterator(Collections.emptyIterator(), new NamedCache(namespace, this.metrics));\n+        }\n+        return new MemoryLRUCacheBytesIterator(cache.keyRange(from, to, false), cache);", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1MzI5OQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545053299", "bodyText": "For new sensors like this, we only need to consider built-in metrics version LATEST. Hence, you should not call throughputAndLatencySensor(), but only call the parts that are relevant for LATEST and not for FROM_0100_TO_24. You also need to adapt the corresponding unit test for that. See also KIP-444 for more details.", "author": "cadonna", "createdAt": "2020-12-17T12:30:03Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -307,6 +315,26 @@ public static Sensor rangeSensor(final String threadId,\n         );\n     }\n \n+    public static Sensor prefixScanSensor(final String threadId,\n+                                     final String taskId,\n+                                     final String storeType,\n+                                     final String storeName,\n+                                     final StreamsMetricsImpl streamsMetrics) {\n+        return throughputAndLatencySensor(", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTc5NA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545055794", "bodyText": "In all the tests for the prefix scan you should also verify boundary conditions, e.g., if you have a prefix abcd, you should verify that abce is not matched since this is the first key that should not be matched.", "author": "cadonna", "createdAt": "2020-12-17T12:34:18Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemoryKeyValueStoreTest.java", "diffHunk": "@@ -359,6 +361,34 @@ public void shouldReverseIterateOverRange() {\n         ), results);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjUwMzE0OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r556503148", "bodyText": "i have added wherever the boundary conditions were not getting tested.", "author": "vamossagar12", "createdAt": "2021-01-13T13:02:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTc5NA=="}], "type": "inlineReview"}, {"oid": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "url": "https://github.com/apache/kafka/commit/1210e2475f23648bbcdd338fc9a7beb38541c0c4", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2021-01-13T12:47:23Z", "type": "forcePushed"}, {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "url": "https://github.com/apache/kafka/commit/822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2021-01-13T13:01:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIwMTEwOQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558201109", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new MeteredKeyValueIterator(wrapped().prefixScan(prefix,\n          \n          \n            \n                        prefixKeySerializer), prefixScanSensor);\n          \n          \n            \n                    return new MeteredKeyValueIterator(wrapped().prefixScan(prefix, prefixKeySerializer), prefixScanSensor);", "author": "cadonna", "createdAt": "2021-01-15T10:21:52Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +232,13 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+\n+        return new MeteredKeyValueIterator(wrapped().prefixScan(prefix,\n+            prefixKeySerializer), prefixScanSensor);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIxNzEyMQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558217121", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        return new RocksDBRangeIterator(\n          \n          \n            \n                            name,\n          \n          \n            \n                            db.newIterator(columnFamily),\n          \n          \n            \n                            openIterators,\n          \n          \n            \n                            prefix,\n          \n          \n            \n                            to,\n          \n          \n            \n                            true,\n          \n          \n            \n                            false);\n          \n          \n            \n                        return new RocksDBRangeIterator(\n          \n          \n            \n                            name,\n          \n          \n            \n                            db.newIterator(columnFamily),\n          \n          \n            \n                            openIterators,\n          \n          \n            \n                            prefix,\n          \n          \n            \n                            to,\n          \n          \n            \n                            true,\n          \n          \n            \n                            false\n          \n          \n            \n                        );", "author": "cadonna", "createdAt": "2021-01-15T10:33:38Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -594,6 +616,20 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n             return new RocksDbIterator(name, innerIterWithTimestamp, openIterators, forward);\n         }\n \n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            final Bytes to = Bytes.increment(prefix);\n+            return new RocksDBRangeIterator(\n+                name,\n+                db.newIterator(columnFamily),\n+                openIterators,\n+                prefix,\n+                to,\n+                true,\n+                false);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIxNzM1NQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558217355", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T10:33:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -594,6 +616,20 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n             return new RocksDbIterator(name, innerIterWithTimestamp, openIterators, forward);\n         }\n \n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            final Bytes to = Bytes.increment(prefix);\n+            return new RocksDBRangeIterator(\n+                name,\n+                db.newIterator(columnFamily),\n+                openIterators,\n+                prefix,\n+                to,\n+                true,\n+                false);\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMDE3OQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558220179", "bodyText": "I guess you forgot to remove this. \ud83d\ude42", "author": "cadonna", "createdAt": "2021-01-15T10:37:23Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -308,6 +309,24 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix,\n+                                                                                    final PS prefixKeySerializer) {\n+        Objects.requireNonNull(prefix, \"prefix cannot be null\");\n+        Objects.requireNonNull(prefixKeySerializer, \"prefixKeySerializer cannot be null\");\n+\n+        validateStoreOpen();\n+        final Bytes prefixBytes = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n+\n+        /*final Bytes from = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n+        final Bytes to = Bytes.increment(from);*/", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMTk3NA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558221974", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T10:40:29Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,130 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abc\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abcd\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abce\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(3));\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefixAsabcd = rocksDBStore.prefixScan(\"abcd\", stringSerializer);\n+        numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefixAsabcd.hasNext()) {\n+            keysWithPrefixAsabcd.next().key.get();\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+\n+    }\n+\n+    @Test\n+    public void shouldReturnUUIDsWithStringPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        final Serializer<UUID> uuidSerializer = Serdes.UUID().serializer();\n+        final UUID uuid1 = UUID.randomUUID();\n+        final UUID uuid2 = UUID.randomUUID();\n+        final String prefix = uuid1.toString().substring(0, 4);\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid1)),\n+            stringSerializer.serialize(null, \"a\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid2)),\n+            stringSerializer.serialize(null, \"b\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(prefix, stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+        assertThat(valuesWithPrefix.get(0), is(\"a\"));\n+    }\n+\n+    @Test\n+    public void shouldReturnNoKeys() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"a\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"b\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"c\")),\n+            stringSerializer.serialize(null, \"e\")));\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMjA2Nw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558222067", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T10:40:39Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,130 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abc\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abcd\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abce\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(3));\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefixAsabcd = rocksDBStore.prefixScan(\"abcd\", stringSerializer);\n+        numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefixAsabcd.hasNext()) {\n+            keysWithPrefixAsabcd.next().key.get();\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+\n+    }\n+\n+    @Test\n+    public void shouldReturnUUIDsWithStringPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        final Serializer<UUID> uuidSerializer = Serdes.UUID().serializer();\n+        final UUID uuid1 = UUID.randomUUID();\n+        final UUID uuid2 = UUID.randomUUID();\n+        final String prefix = uuid1.toString().substring(0, 4);\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid1)),\n+            stringSerializer.serialize(null, \"a\")));\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMzAwMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558223002", "bodyText": "Could you split up this unit test into two, one for prefix prefix and one for prefix abcd? The former tests the general case whereas the latter tests the corner case where the the end of the range should not be returned.", "author": "cadonna", "createdAt": "2021-01-15T10:42:29Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,130 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMzcwOA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558223708", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            true);\n          \n          \n            \n                            true\n          \n          \n            \n                        );", "author": "cadonna", "createdAt": "2021-01-15T10:43:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -580,7 +601,8 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n                 openIterators,\n                 from,\n                 to,\n-                forward);\n+                forward,\n+                true);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyNTAxNA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558225014", "bodyText": "I think here IntelliJ's formatting confused you. \ud83d\ude42\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            to,\n          \n          \n            \n                    true,\n          \n          \n            \n                  false);\n          \n          \n            \n                            to,\n          \n          \n            \n                            true,\n          \n          \n            \n                            false\n          \n          \n            \n                        );", "author": "cadonna", "createdAt": "2021-01-15T10:46:08Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java", "diffHunk": "@@ -218,6 +219,19 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n             return new RocksDBDualCFIterator(name, innerIterWithTimestamp, innerIterNoTimestamp, forward);\n         }\n \n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            final Bytes to = Bytes.increment(prefix);\n+            return new RocksDBDualCFRangeIterator(\n+                name,\n+                db.newIterator(newColumnFamily),\n+                db.newIterator(oldColumnFamily),\n+                prefix,\n+                to,\n+        true,\n+      false);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyNjg4Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558226883", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    try (final KeyValueIterator<Bytes, byte[]> it =\n          \n          \n            \n                                 rocksDBStore.prefixScan(\"key1\", stringSerializer)) {\n          \n          \n            \n                    try (final KeyValueIterator<Bytes, byte[]> it = rocksDBStore.prefixScan(\"key1\", stringSerializer)) {", "author": "cadonna", "createdAt": "2021-01-15T10:49:21Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStoreTest.java", "diffHunk": "@@ -337,6 +341,22 @@ private void iteratorsShouldNotMigrateData() {\n             }\n             assertFalse(it.hasNext());\n         }\n+\n+        try (final KeyValueIterator<Bytes, byte[]> it =\n+                     rocksDBStore.prefixScan(\"key1\", stringSerializer)) {", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzMTk0NQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558231945", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T10:58:55Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -307,6 +315,34 @@ public static Sensor rangeSensor(final String threadId,\n         );\n     }\n \n+    public static Sensor prefixScanSensor(final String taskId,\n+                                          final String storeType,\n+                                          final String storeName,\n+                                          final StreamsMetricsImpl streamsMetrics) {\n+\n+        final String latencyMetricName = PREFIX_SCAN + LATENCY_SUFFIX;\n+        final Map<String, String> tagMap = streamsMetrics.storeLevelTagMap(taskId, storeType, storeName);\n+\n+        final Sensor sensor = streamsMetrics.storeLevelSensor(taskId, storeName, PREFIX_SCAN, RecordingLevel.DEBUG);\n+        addInvocationRateToSensor(\n+            sensor,\n+            STATE_STORE_LEVEL_GROUP,\n+            tagMap,\n+            PREFIX_SCAN,\n+            PREFIX_SCAN_RATE_DESCRIPTION\n+        );\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzMjIyMA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558232220", "bodyText": "This description is not needed since there is no total metric.", "author": "cadonna", "createdAt": "2021-01-15T10:59:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -110,6 +110,14 @@ private StateStoreMetrics() {}\n     private static final String RANGE_AVG_LATENCY_DESCRIPTION = AVG_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n     private static final String RANGE_MAX_LATENCY_DESCRIPTION = MAX_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n \n+    private static final String PREFIX_SCAN = \"prefix-scan\";\n+    private static final String PREFIX_SCAN_DESCRIPTION = \"calls to prefix-scan\";\n+    private static final String PREFIX_SCAN_TOTAL_DESCRIPTION = TOTAL_DESCRIPTION + PREFIX_SCAN_DESCRIPTION;", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzNDYzNw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558234637", "bodyText": "I do not think you need to put an entry if you use mocks.", "author": "cadonna", "createdAt": "2021-01-15T11:04:08Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,25 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        inner.put(eq(Bytes.increment(KEY_BYTES)), aryEq(VALUE_BYTES));\n+        expectLastCall();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        metered.put(Bytes.increment(KEY_BYTES).toString(), VALUE);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzNDg3NA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558234874", "bodyText": "This line is not needed in this case. A method call without a return value is expected on the mock if you simply call the method on the mock in the replay phase.", "author": "cadonna", "createdAt": "2021-01-15T11:04:32Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,25 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        inner.put(eq(Bytes.increment(KEY_BYTES)), aryEq(VALUE_BYTES));\n+        expectLastCall();", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNDAwMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558304002", "bodyText": "I think we do not need this method. We can just call range() in CachingKeyValueStore.", "author": "cadonna", "createdAt": "2021-01-15T13:25:15Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -218,6 +222,10 @@ public MemoryLRUCacheBytesIterator reverseAll(final String namespace) {\n         return new MemoryLRUCacheBytesIterator(cache.reverseAllKeys(), cache);\n     }\n \n+    public MemoryLRUCacheBytesIterator prefixScan(final String namespace, final Bytes from, final Bytes to) {", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzIyOA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307228", "bodyText": "If we remove prefixScan(), you can remove this test, but we need a test for range() that excludes the end limit.", "author": "cadonna", "createdAt": "2021-01-15T13:30:37Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ThreadCacheTest.java", "diffHunk": "@@ -285,6 +285,25 @@ public void shouldPeekAndIterateOverRange() {\n         assertEquals(5, bytesIndex);\n     }\n \n+    @Test\n+    public void testPrefixScan() {", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzc1Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307756", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T13:31:32Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,42 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+\n+        expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+\n+        StreamsMetricsImpl.addInvocationRateToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,\n+            storeTagMap,\n+            metricName,\n+            descriptionOfRate\n+        );\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzc5MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307791", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T13:31:37Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,42 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+\n+        expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzkyMw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307923", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T13:31:56Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,42 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "17be91a37214bf77430c65d9300a5120e4348df9", "url": "https://github.com/apache/kafka/commit/17be91a37214bf77430c65d9300a5120e4348df9", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2021-01-20T12:27:29Z", "type": "commit"}, {"oid": "17be91a37214bf77430c65d9300a5120e4348df9", "url": "https://github.com/apache/kafka/commit/17be91a37214bf77430c65d9300a5120e4348df9", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2021-01-20T12:27:29Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDM5NDQ3Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564394476", "bodyText": "This method needs unit testing. Try to use a mock for the cache in the test.", "author": "cadonna", "createdAt": "2021-01-26T10:11:07Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java", "diffHunk": "@@ -291,6 +292,16 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         return new MergedSortedCacheKeyValueBytesStoreIterator(cacheIterator, storeIterator, true);\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjIzNDAzNA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r566234034", "bodyText": "Similar comment as below. Unit tests are in CachingInMemoryKeyValueStoreTest which already extends AbstractKeyValueStoreTest and creates an in memory cache store.", "author": "vamossagar12", "createdAt": "2021-01-28T16:31:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDM5NDQ3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzQ3ODg3MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r567478871", "bodyText": "Oh, I see. I missed those. Sorry! That is fine then, although I think unit tests with mocks would be better.", "author": "cadonna", "createdAt": "2021-01-31T20:21:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDM5NDQ3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDE1NzcyNw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r570157727", "bodyText": "Actually, I had created another ticket to streamline tests for CachingKVStore: https://issues.apache.org/jira/browse/KAFKA-10788. @rohitrmd  had volunteered to take this up.", "author": "vamossagar12", "createdAt": "2021-02-04T11:41:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDM5NDQ3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNTMwOA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564405308", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n          \n          \n            \n                    final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(hi.toString(), new StringSerializer());\n          \n      \n    \n    \n  \n\nIn such a way, you can reuse variable hi and there. Similar is true for my suggestions below.", "author": "cadonna", "createdAt": "2021-01-26T10:27:55Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNjI2OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564406268", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final List<String> keys = new ArrayList<>();\n          \n          \n            \n                    final List<String> values = new ArrayList<>();\n          \n          \n            \n                    final List<Bytes> keys = new ArrayList<>();\n          \n          \n            \n                    final List<Bytes> values = new ArrayList<>();", "author": "cadonna", "createdAt": "2021-01-26T10:29:29Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNjUxMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564406512", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        keys.add(next.key.toString());\n          \n          \n            \n                        values.add(new String(next.value));\n          \n          \n            \n                        keys.add(next.key);\n          \n          \n            \n                        values.add(Bytes.wrap(next.value));", "author": "cadonna", "createdAt": "2021-01-26T10:29:53Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            keys.add(next.key.toString());\n+            values.add(new String(next.value));", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNjg4Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564406883", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    assertThat(keys, is(Collections.singletonList(\"hi\")));\n          \n          \n            \n                    assertThat(values, is(Collections.singletonList(\"there\")));\n          \n          \n            \n                    assertThat(keys, is(Collections.singletonList(hi)));\n          \n          \n            \n                    assertThat(values, is(Collections.singletonList(Bytes.wrap(there))));", "author": "cadonna", "createdAt": "2021-01-26T10:30:22Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            keys.add(next.key.toString());\n+            values.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(1));\n+        assertThat(keys, is(Collections.singletonList(\"hi\")));\n+        assertThat(values, is(Collections.singletonList(\"there\")));", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwOTkzNw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564409937", "bodyText": "@vamossagar12 I can still not find the unit test for this method.", "author": "cadonna", "createdAt": "2021-01-26T10:35:35Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java", "diffHunk": "@@ -103,6 +105,20 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjIzMzAzOQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r566233039", "bodyText": "For this, do you want me to add the test cases here?https://github.com/apache/kafka/blob/17be91a37214bf77430c65d9300a5120e4348df9/streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStoreTest.java\nThere are tests in CachingInMemoryKeyValueStoreTest, which is where the tests for other methods like range etc have been added.", "author": "vamossagar12", "createdAt": "2021-01-28T16:30:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwOTkzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzQ3OTU0Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r567479543", "bodyText": "I think those tests never call the prefixScan() on the underlying in-memory state store because all entries fit into the cache. You would need to add another test that flushes the cache before you call prefixScan(). I would prefer a test that directly tests the in-memory store without any cache in between.", "author": "cadonna", "createdAt": "2021-01-31T20:27:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwOTkzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDE1MzEyMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r570153122", "bodyText": "Here is the new ticket: https://issues.apache.org/jira/browse/KAFKA-12289 and the PR for the ticket:\n#10052", "author": "vamossagar12", "createdAt": "2021-02-04T11:33:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwOTkzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQxNTY4Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564415683", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));\n          \n          \n            \n                    final KafkaMetric metric = metric(\"prefix-scan-rate\");", "author": "cadonna", "createdAt": "2021-01-26T10:44:39Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,22 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n+        assertThat(iterator.next().value, equalTo(VALUE));\n+        iterator.close();\n+\n+        final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQyMTkzNg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564421936", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final Sensor sensor =\n          \n          \n            \n                            StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);\n          \n          \n            \n                    final Sensor sensor = StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);", "author": "cadonna", "createdAt": "2021-01-26T10:55:02Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,39 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+        expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+        StreamsMetricsImpl.addInvocationRateToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,\n+            storeTagMap,\n+            metricName,\n+            descriptionOfRate\n+        );\n+        StreamsMetricsImpl.addAvgAndMaxToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,\n+            storeTagMap,\n+            latencyMetricName(metricName),\n+            descriptionOfAvg,\n+            descriptionOfMax\n+        );\n+        replay(StreamsMetricsImpl.class, streamsMetrics);\n+\n+        final Sensor sensor =\n+                StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4a41206daa0ffbc7516059d29a7ddda109f64b5e", "url": "https://github.com/apache/kafka/commit/4a41206daa0ffbc7516059d29a7ddda109f64b5e", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:16:52Z", "type": "commit"}, {"oid": "33be9113c6225063a1af489c5eca62f7645250ab", "url": "https://github.com/apache/kafka/commit/33be9113c6225063a1af489c5eca62f7645250ab", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:17:12Z", "type": "commit"}, {"oid": "25980a0b3e6fdedf2fe707f78591dd5c9ba840c9", "url": "https://github.com/apache/kafka/commit/25980a0b3e6fdedf2fe707f78591dd5c9ba840c9", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:17:21Z", "type": "commit"}, {"oid": "a2ea51336e4ea2010f1d93dd87d4b1526281cadb", "url": "https://github.com/apache/kafka/commit/a2ea51336e4ea2010f1d93dd87d4b1526281cadb", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:17:32Z", "type": "commit"}, {"oid": "dddad17ad5102e937150bd7115c215b92807e734", "url": "https://github.com/apache/kafka/commit/dddad17ad5102e937150bd7115c215b92807e734", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:17:49Z", "type": "commit"}, {"oid": "d2479a41c4d90e44c7dacb8028368cfa4a846cbb", "url": "https://github.com/apache/kafka/commit/d2479a41c4d90e44c7dacb8028368cfa4a846cbb", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:18:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODUwODA3OQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r568508079", "bodyText": "To get rid of the test failure, you need to change this:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final KafkaMetric metric = metric(\"prefix-scan-rate\");\n          \n          \n            \n                    final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));\n          \n      \n    \n    \n  \n\nSorry, the failure of the test is my bad. I missed the issue with the different metrics versions when I requested to change this in a previous review.", "author": "cadonna", "createdAt": "2021-02-02T10:55:13Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,22 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n+        assertThat(iterator.next().value, equalTo(VALUE));\n+        iterator.close();\n+\n+        final KafkaMetric metric = metric(\"prefix-scan-rate\");", "originalCommit": "d2479a41c4d90e44c7dacb8028368cfa4a846cbb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8eca3c9c2852172896001178f8e7a115fd392aeb", "url": "https://github.com/apache/kafka/commit/8eca3c9c2852172896001178f8e7a115fd392aeb", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-02-03T10:49:01Z", "type": "commit"}]}