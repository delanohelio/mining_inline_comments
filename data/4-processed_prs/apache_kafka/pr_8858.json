{"pr_number": 8858, "pr_title": "KAFKA-10153: Error Reporting in Connect Documentation", "pr_createdAt": "2020-06-12T07:00:05Z", "pr_url": "https://github.com/apache/kafka/pull/8858", "timeline": [{"oid": "36c23fda5610344c64887cec185d4c81d657bdd5", "url": "https://github.com/apache/kafka/commit/36c23fda5610344c64887cec185d4c81d657bdd5", "message": "KAFKA-10153: Documentation for the Errant Record Reporter", "committedDate": "2020-06-12T06:58:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ2NzI3Mw==", "url": "https://github.com/apache/kafka/pull/8858#discussion_r439467273", "bodyText": "Let's add a new section just before \"Resuming from Previous Offsets\" that talks about how sink tasks can use the error reporter, similar to the \"Example Usage\" in the KIP, but don't show using the returned future, since the guarantees provided by the framework about completing the writing of the errors are likely good enough for most connectors, and developers of any more sophisticated connector that needs stronger guarantees can read the JavaDoc.", "author": "rhauch", "createdAt": "2020-06-12T14:52:03Z", "path": "docs/connect.html", "diffHunk": "@@ -258,6 +258,29 @@ <h4><a id=\"connect_rest\" href=\"#connect_rest\">REST API</a></h4>\n         <li><code>GET /</code>- return basic information about the Kafka Connect cluster such as the version of the Connect worker that serves the REST request (including git commit ID of the source code) and the Kafka cluster ID that is connected to.\n     </ul>\n \n+    <h4><a id=\"connect_errorreporting\" href=\"#connect_errorreporting\">Error Reporting in Connect</a></h4>\n+\n+    <p>Kafka Connect provides error reporting for errors encountered along various stages of processing in Kafka Connect. A connector can be configured to enable log reporting, which logs the error context along with the standard application logs, as well as to enable dead letter queue reporting (within sink connectors only), which will write the original record (from the Kafka topic the sink connector is consuming from) into a configurable Kafka topic. Configuring these reporters will enable for automatic error reporting along both the conversion and transformation stages of Connect. For error reporting for records after they reach the connector, the <code>Errant Record Reporter</code> must be implemented within the specific connector along with the configuration.</p>\n+\n+    <h5><a id=\"connect_errantrecordreporting\" href=\"#connect-errantrecordreporting\">Errant Record Reporter</a></h5>\n+\n+    <p>The <code>ErrantRecordReporter</code> can be initialized within the <code>SinkTask</code> while preserving backwards compatibility with the following example:</p>\n+\n+    <pre class=\"brush: java;\">\n+        private ErrantRecordReporter reporter;\n+\n+        @Override\n+        public void start(Map<String, String> props) {\n+            ...\n+            try {\n+                reporter = context.failedRecordReporter(); // may be null if DLQ not enabled\n+            } catch (NoSuchMethodException | NoClassDefFoundError e) {\n+                // Will occur in Connect runtimes earlier than 2.6\n+                reporter = null;\n+            }\n+        }\n+    </pre>\n+", "originalCommit": "36c23fda5610344c64887cec185d4c81d657bdd5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ3ODg1NA==", "url": "https://github.com/apache/kafka/pull/8858#discussion_r439478854", "bodyText": "A few suggestions for readability and to add some information that could have been added with KIP-298:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                <p>Kafka Connect provides error reporting for errors encountered along various stages of processing in Kafka Connect. A connector can be configured to enable log reporting, which logs the error context along with the standard application logs, as well as to enable dead letter queue reporting (within sink connectors only), which will write the original record (from the Kafka topic the sink connector is consuming from) into a configurable Kafka topic. Configuring these reporters will enable for automatic error reporting along both the conversion and transformation stages of Connect. For error reporting for records after they reach the connector, the <code>Errant Record Reporter</code> must be implemented within the specific connector along with the configuration.</p>\n          \n          \n            \n                <p>Kafka Connect provides error reporting to handle errors encountered along various stages of processing. By default, any error encountered during conversion or within transformations will cause the connector to fail. Each connector configuration can also enable tolerating such errors by skipping them, optionally writing each error and the details of the failed operation and problematic record (with various levels of detail) to the Connect application log. These mechanisms also capture errors when a sink connector is processing the messages consumed from its Kafka topics, and all of the errors can be written to a configurable \"dead letter queue\" (DLQ) Kafka topic.</p>\n          \n          \n            \n            \n          \n          \n            \n                <p>To report errors to the log and/or DLQ, set <code>errors.tolerance=all</code> in the connector configuration. Likewise, set <code>errors.log.enable=true</code> in the connector configuration to log details of each error and problem record's topic, partition, and offset. For additional debugging purposes, set <code>errors.log.include.messages=true</code> to also log the problem record key, value, and headers to the log (note this may log sensitive information).\n          \n          \n            \n            \n          \n          \n            \n                <p>To report errors within a sink connector's converter, transforms, or in the connector itself to a dead letter queue topic, set <code>errors.deadletterqueue.topic.name</code>, and optionally  <code>errors.deadletterqueue.context.headers.enable=true</code>.</p>\n          \n          \n            \n            \n          \n          \n            \n                <p>For example, a connector will fail immediately upon an error or exception. Although it is not necessary to add extra configuration properties for this behavior, adding the following properties to a sink connector configuration would achieve this \"fail fast\" behavior:</p>\n          \n          \n            \n                \n          \n          \n            \n                <pre class=\"brush: text;\">\n          \n          \n            \n                    # disable retries on failure\n          \n          \n            \n                    errors.retry.timeout=0\n          \n          \n            \n                    \n          \n          \n            \n                    # do not log the error and their contexts\n          \n          \n            \n                    errors.log.enable=false\n          \n          \n            \n                     \n          \n          \n            \n                    # do not record errors in a dead letter queue topic\n          \n          \n            \n                    errors.deadletterqueue.topic.name=\n          \n          \n            \n                     \n          \n          \n            \n                    # Fail on first error\n          \n          \n            \n                    errors.tolerance=none\n          \n          \n            \n                </pre>\n          \n          \n            \n            \n          \n          \n            \n                <p>The following configuration shows how to setup error handling with multiple retries, logging both to the application logs and a Kafka topic with infinite tolerance:</p>\n          \n          \n            \n            \n          \n          \n            \n                <pre class=\"brush: text;\">\n          \n          \n            \n                    # retry for at most 10 minutes times waiting up to 30 seconds between consecutive failures\n          \n          \n            \n                    errors.retry.timeout=600000\n          \n          \n            \n                    errors.retry.delay.max.ms=30000\n          \n          \n            \n                     \n          \n          \n            \n                    # log error context along with application logs, but do not include configs and messages\n          \n          \n            \n                    errors.log.enable=true\n          \n          \n            \n                    errors.log.include.messages=false\n          \n          \n            \n                    \n          \n          \n            \n                    # produce error context into the Kafka topic\n          \n          \n            \n                    errors.deadletterqueue.topic.name=my-connector-errors\n          \n          \n            \n                    \n          \n          \n            \n                    # Tolerate all errors.\n          \n          \n            \n                    errors.tolerance=all\n          \n          \n            \n                </pre>\n          \n      \n    \n    \n  \n\n@aakashnshah, @wicknicks: what do you think?", "author": "rhauch", "createdAt": "2020-06-12T15:10:58Z", "path": "docs/connect.html", "diffHunk": "@@ -258,6 +258,29 @@ <h4><a id=\"connect_rest\" href=\"#connect_rest\">REST API</a></h4>\n         <li><code>GET /</code>- return basic information about the Kafka Connect cluster such as the version of the Connect worker that serves the REST request (including git commit ID of the source code) and the Kafka cluster ID that is connected to.\n     </ul>\n \n+    <h4><a id=\"connect_errorreporting\" href=\"#connect_errorreporting\">Error Reporting in Connect</a></h4>\n+\n+    <p>Kafka Connect provides error reporting for errors encountered along various stages of processing in Kafka Connect. A connector can be configured to enable log reporting, which logs the error context along with the standard application logs, as well as to enable dead letter queue reporting (within sink connectors only), which will write the original record (from the Kafka topic the sink connector is consuming from) into a configurable Kafka topic. Configuring these reporters will enable for automatic error reporting along both the conversion and transformation stages of Connect. For error reporting for records after they reach the connector, the <code>Errant Record Reporter</code> must be implemented within the specific connector along with the configuration.</p>", "originalCommit": "36c23fda5610344c64887cec185d4c81d657bdd5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc2MTA5NQ==", "url": "https://github.com/apache/kafka/pull/8858#discussion_r439761095", "bodyText": "@rhauch This looks good to me! I just have a few suggestions to your suggestion that I will add in the commit.", "author": "aakashnshah", "createdAt": "2020-06-13T18:59:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ3ODg1NA=="}], "type": "inlineReview"}, {"oid": "5961b1cd1a14721048da6419283771b84a77ec9e", "url": "https://github.com/apache/kafka/commit/5961b1cd1a14721048da6419283771b84a77ec9e", "message": "added a section, used and modified suggestions", "committedDate": "2020-06-18T05:37:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyMzEzMA==", "url": "https://github.com/apache/kafka/pull/8858#discussion_r448023130", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                <p>To report errors within a connector's converter, transforms, or in specifically the sink connector itself to the log, set <code>errors.log.enable=true</code> in the connector configuration to log details of each error and problem record's topic, partition, and offset. For additional debugging purposes, set <code>errors.log.include.messages=true</code> to also log the problem record key, value, and headers to the log (note this may log sensitive information).\n          \n          \n            \n                <p>To report errors within a connector's converter, transforms, or within the sink connector itself to the log, set <code>errors.log.enable=true</code> in the connector configuration to log details of each error and problem record's topic, partition, and offset. For additional debugging purposes, set <code>errors.log.include.messages=true</code> to also log the problem record key, value, and headers to the log (note this may log sensitive information).", "author": "rhauch", "createdAt": "2020-06-30T22:53:35Z", "path": "docs/connect.html", "diffHunk": "@@ -258,6 +258,48 @@ <h4><a id=\"connect_rest\" href=\"#connect_rest\">REST API</a></h4>\n         <li><code>GET /</code>- return basic information about the Kafka Connect cluster such as the version of the Connect worker that serves the REST request (including git commit ID of the source code) and the Kafka cluster ID that is connected to.\n     </ul>\n \n+    <h4><a id=\"connect_errorreporting\" href=\"#connect_errorreporting\">Error Reporting in Connect</a></h4>\n+\n+    <p>Kafka Connect provides error reporting to handle errors encountered along various stages of processing. By default, any error encountered during conversion or within transformations will cause the connector to fail. Each connector configuration can also enable tolerating such errors by skipping them, optionally writing each error and the details of the failed operation and problematic record (with various levels of detail) to the Connect application log. These mechanisms also capture errors when a sink connector is processing the messages consumed from its Kafka topics, and all of the errors can be written to a configurable \"dead letter queue\" (DLQ) Kafka topic.</p>\n+\n+    <p>To report errors within a connector's converter, transforms, or in specifically the sink connector itself to the log, set <code>errors.log.enable=true</code> in the connector configuration to log details of each error and problem record's topic, partition, and offset. For additional debugging purposes, set <code>errors.log.include.messages=true</code> to also log the problem record key, value, and headers to the log (note this may log sensitive information).", "originalCommit": "5961b1cd1a14721048da6419283771b84a77ec9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyMzM2OA==", "url": "https://github.com/apache/kafka/pull/8858#discussion_r448023368", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                <p>To report errors within a connector's converter, transforms, or in specifically the sink connector itself to a dead letter queue topic, set <code>errors.deadletterqueue.topic.name</code>, and optionally  <code>errors.deadletterqueue.context.headers.enable=true</code>.</p>\n          \n          \n            \n                <p>To report errors within a connector's converter, transforms, or within the sink connector itself to a dead letter queue topic, set <code>errors.deadletterqueue.topic.name</code>, and optionally  <code>errors.deadletterqueue.context.headers.enable=true</code>.</p>", "author": "rhauch", "createdAt": "2020-06-30T22:54:17Z", "path": "docs/connect.html", "diffHunk": "@@ -258,6 +258,48 @@ <h4><a id=\"connect_rest\" href=\"#connect_rest\">REST API</a></h4>\n         <li><code>GET /</code>- return basic information about the Kafka Connect cluster such as the version of the Connect worker that serves the REST request (including git commit ID of the source code) and the Kafka cluster ID that is connected to.\n     </ul>\n \n+    <h4><a id=\"connect_errorreporting\" href=\"#connect_errorreporting\">Error Reporting in Connect</a></h4>\n+\n+    <p>Kafka Connect provides error reporting to handle errors encountered along various stages of processing. By default, any error encountered during conversion or within transformations will cause the connector to fail. Each connector configuration can also enable tolerating such errors by skipping them, optionally writing each error and the details of the failed operation and problematic record (with various levels of detail) to the Connect application log. These mechanisms also capture errors when a sink connector is processing the messages consumed from its Kafka topics, and all of the errors can be written to a configurable \"dead letter queue\" (DLQ) Kafka topic.</p>\n+\n+    <p>To report errors within a connector's converter, transforms, or in specifically the sink connector itself to the log, set <code>errors.log.enable=true</code> in the connector configuration to log details of each error and problem record's topic, partition, and offset. For additional debugging purposes, set <code>errors.log.include.messages=true</code> to also log the problem record key, value, and headers to the log (note this may log sensitive information).\n+\n+    <p>To report errors within a connector's converter, transforms, or in specifically the sink connector itself to a dead letter queue topic, set <code>errors.deadletterqueue.topic.name</code>, and optionally  <code>errors.deadletterqueue.context.headers.enable=true</code>.</p>", "originalCommit": "5961b1cd1a14721048da6419283771b84a77ec9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyMzgyNw==", "url": "https://github.com/apache/kafka/pull/8858#discussion_r448023827", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    errors.deadletterqueue.topic.name=\n          \n          \n            \n                    # errors.deadletterqueue.topic.name=", "author": "rhauch", "createdAt": "2020-06-30T22:55:42Z", "path": "docs/connect.html", "diffHunk": "@@ -258,6 +258,48 @@ <h4><a id=\"connect_rest\" href=\"#connect_rest\">REST API</a></h4>\n         <li><code>GET /</code>- return basic information about the Kafka Connect cluster such as the version of the Connect worker that serves the REST request (including git commit ID of the source code) and the Kafka cluster ID that is connected to.\n     </ul>\n \n+    <h4><a id=\"connect_errorreporting\" href=\"#connect_errorreporting\">Error Reporting in Connect</a></h4>\n+\n+    <p>Kafka Connect provides error reporting to handle errors encountered along various stages of processing. By default, any error encountered during conversion or within transformations will cause the connector to fail. Each connector configuration can also enable tolerating such errors by skipping them, optionally writing each error and the details of the failed operation and problematic record (with various levels of detail) to the Connect application log. These mechanisms also capture errors when a sink connector is processing the messages consumed from its Kafka topics, and all of the errors can be written to a configurable \"dead letter queue\" (DLQ) Kafka topic.</p>\n+\n+    <p>To report errors within a connector's converter, transforms, or in specifically the sink connector itself to the log, set <code>errors.log.enable=true</code> in the connector configuration to log details of each error and problem record's topic, partition, and offset. For additional debugging purposes, set <code>errors.log.include.messages=true</code> to also log the problem record key, value, and headers to the log (note this may log sensitive information).\n+\n+    <p>To report errors within a connector's converter, transforms, or in specifically the sink connector itself to a dead letter queue topic, set <code>errors.deadletterqueue.topic.name</code>, and optionally  <code>errors.deadletterqueue.context.headers.enable=true</code>.</p>\n+\n+    <p>For example, below shows a configuration that will cause a connector will fail immediately upon an error or exception. Although it is not necessary to add extra configuration properties for this behavior, adding the following properties to a sink connector configuration would achieve this \"fail fast\" behavior:</p>\n+\n+    <pre class=\"brush: text;\">\n+        # disable retries on failure\n+        errors.retry.timeout=0\n+\n+        # do not log the error and their contexts\n+        errors.log.enable=false\n+\n+        # do not record errors in a dead letter queue topic\n+        errors.deadletterqueue.topic.name=", "originalCommit": "5961b1cd1a14721048da6419283771b84a77ec9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyNDQwMw==", "url": "https://github.com/apache/kafka/pull/8858#discussion_r448024403", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                <p>The following configuration shows how to setup error handling with multiple retries, logging both to the application logs and a Kafka topic with infinite tolerance:</p>\n          \n          \n            \n                <p>The following configuration properties can be added to a connector configuration to setup error handling with multiple retries, logging to the application logs and the <code>my-connector-errors</code> Kafka topic, and tolerating all errors rather than failing the connector or task:</p>", "author": "rhauch", "createdAt": "2020-06-30T22:57:24Z", "path": "docs/connect.html", "diffHunk": "@@ -258,6 +258,48 @@ <h4><a id=\"connect_rest\" href=\"#connect_rest\">REST API</a></h4>\n         <li><code>GET /</code>- return basic information about the Kafka Connect cluster such as the version of the Connect worker that serves the REST request (including git commit ID of the source code) and the Kafka cluster ID that is connected to.\n     </ul>\n \n+    <h4><a id=\"connect_errorreporting\" href=\"#connect_errorreporting\">Error Reporting in Connect</a></h4>\n+\n+    <p>Kafka Connect provides error reporting to handle errors encountered along various stages of processing. By default, any error encountered during conversion or within transformations will cause the connector to fail. Each connector configuration can also enable tolerating such errors by skipping them, optionally writing each error and the details of the failed operation and problematic record (with various levels of detail) to the Connect application log. These mechanisms also capture errors when a sink connector is processing the messages consumed from its Kafka topics, and all of the errors can be written to a configurable \"dead letter queue\" (DLQ) Kafka topic.</p>\n+\n+    <p>To report errors within a connector's converter, transforms, or in specifically the sink connector itself to the log, set <code>errors.log.enable=true</code> in the connector configuration to log details of each error and problem record's topic, partition, and offset. For additional debugging purposes, set <code>errors.log.include.messages=true</code> to also log the problem record key, value, and headers to the log (note this may log sensitive information).\n+\n+    <p>To report errors within a connector's converter, transforms, or in specifically the sink connector itself to a dead letter queue topic, set <code>errors.deadletterqueue.topic.name</code>, and optionally  <code>errors.deadletterqueue.context.headers.enable=true</code>.</p>\n+\n+    <p>For example, below shows a configuration that will cause a connector will fail immediately upon an error or exception. Although it is not necessary to add extra configuration properties for this behavior, adding the following properties to a sink connector configuration would achieve this \"fail fast\" behavior:</p>\n+\n+    <pre class=\"brush: text;\">\n+        # disable retries on failure\n+        errors.retry.timeout=0\n+\n+        # do not log the error and their contexts\n+        errors.log.enable=false\n+\n+        # do not record errors in a dead letter queue topic\n+        errors.deadletterqueue.topic.name=\n+\n+        # Fail on first error\n+        errors.tolerance=none\n+    </pre>\n+\n+    <p>The following configuration shows how to setup error handling with multiple retries, logging both to the application logs and a Kafka topic with infinite tolerance:</p>", "originalCommit": "5961b1cd1a14721048da6419283771b84a77ec9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyNTEzMQ==", "url": "https://github.com/apache/kafka/pull/8858#discussion_r448025131", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                <p>The <code>ErrantRecordReporter</code> can be used to report errors encountered after records have been sent to a sink connector. The following is an example implementation and use case of the <code>ErrantRecordReporter</code> in the <code>SinkTask</code> class:</p>\n          \n          \n            \n                <p>When <a href=\"#connect_errorreporting\">error reporting</a> is enabled for a connector, the connector can use an <code>ErrantRecordReporter</code> to report problems with individual records sent to a sink connector. The following example shows how to obtain and use the <code>ErrantRecordReporter</code> in a <code>SinkTask</code> subclass, while safely handling the case when the connector is installed in an older Connect runtime that doesn't have this reporter feature:</p>", "author": "rhauch", "createdAt": "2020-06-30T22:59:44Z", "path": "docs/connect.html", "diffHunk": "@@ -429,6 +471,42 @@ <h5><a id=\"connect_sinktasks\" href=\"#connect_sinktasks\">Sink Tasks</a></h5>\n     <p>The <code>flush()</code> method is used during the offset commit process, which allows tasks to recover from failures and resume from a safe point such that no events will be missed. The method should push any outstanding data to the destination system and then block until the write has been acknowledged. The <code>offsets</code> parameter can often be ignored, but is useful in some cases where implementations want to store offset information in the destination store to provide exactly-once\n     delivery. For example, an HDFS connector could do this and use atomic move operations to make sure the <code>flush()</code> operation atomically commits the data and offsets to a final location in HDFS.</p>\n \n+    <h5><a id=\"connect_errantrecordreporter\" href=\"connect_errantrecordreporter\">Errant Record Reporter</a></h5>\n+\n+    <p>The <code>ErrantRecordReporter</code> can be used to report errors encountered after records have been sent to a sink connector. The following is an example implementation and use case of the <code>ErrantRecordReporter</code> in the <code>SinkTask</code> class:</p>", "originalCommit": "5961b1cd1a14721048da6419283771b84a77ec9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyNzcwMg==", "url": "https://github.com/apache/kafka/pull/8858#discussion_r448027702", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                <p>For example, below shows a configuration that will cause a connector will fail immediately upon an error or exception. Although it is not necessary to add extra configuration properties for this behavior, adding the following properties to a sink connector configuration would achieve this \"fail fast\" behavior:</p>\n          \n          \n            \n                <p>For example, the following configuration properties can be added to a connector configuration to cause the connector to fail immediately upon an error or exception. Although it is not necessary to add extra configuration properties for this behavior, adding the following properties to a sink connector configuration would achieve this \"fail fast\" behavior:</p>", "author": "rhauch", "createdAt": "2020-06-30T23:07:34Z", "path": "docs/connect.html", "diffHunk": "@@ -258,6 +258,48 @@ <h4><a id=\"connect_rest\" href=\"#connect_rest\">REST API</a></h4>\n         <li><code>GET /</code>- return basic information about the Kafka Connect cluster such as the version of the Connect worker that serves the REST request (including git commit ID of the source code) and the Kafka cluster ID that is connected to.\n     </ul>\n \n+    <h4><a id=\"connect_errorreporting\" href=\"#connect_errorreporting\">Error Reporting in Connect</a></h4>\n+\n+    <p>Kafka Connect provides error reporting to handle errors encountered along various stages of processing. By default, any error encountered during conversion or within transformations will cause the connector to fail. Each connector configuration can also enable tolerating such errors by skipping them, optionally writing each error and the details of the failed operation and problematic record (with various levels of detail) to the Connect application log. These mechanisms also capture errors when a sink connector is processing the messages consumed from its Kafka topics, and all of the errors can be written to a configurable \"dead letter queue\" (DLQ) Kafka topic.</p>\n+\n+    <p>To report errors within a connector's converter, transforms, or in specifically the sink connector itself to the log, set <code>errors.log.enable=true</code> in the connector configuration to log details of each error and problem record's topic, partition, and offset. For additional debugging purposes, set <code>errors.log.include.messages=true</code> to also log the problem record key, value, and headers to the log (note this may log sensitive information).\n+\n+    <p>To report errors within a connector's converter, transforms, or in specifically the sink connector itself to a dead letter queue topic, set <code>errors.deadletterqueue.topic.name</code>, and optionally  <code>errors.deadletterqueue.context.headers.enable=true</code>.</p>\n+\n+    <p>For example, below shows a configuration that will cause a connector will fail immediately upon an error or exception. Although it is not necessary to add extra configuration properties for this behavior, adding the following properties to a sink connector configuration would achieve this \"fail fast\" behavior:</p>", "originalCommit": "5961b1cd1a14721048da6419283771b84a77ec9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2feda83b0211a42b9bb20441e4e906656cb7da87", "url": "https://github.com/apache/kafka/commit/2feda83b0211a42b9bb20441e4e906656cb7da87", "message": "Update docs/connect.html\n\nCo-authored-by: Randall Hauch <rhauch@gmail.com>", "committedDate": "2020-06-30T23:43:23Z", "type": "commit"}, {"oid": "882961bb25e7b576f441b6f6127812c84233ad9b", "url": "https://github.com/apache/kafka/commit/882961bb25e7b576f441b6f6127812c84233ad9b", "message": "Update docs/connect.html\n\nCo-authored-by: Randall Hauch <rhauch@gmail.com>", "committedDate": "2020-06-30T23:45:38Z", "type": "commit"}, {"oid": "9bb722b82e3f0f1d1dd8fd9cd55a5f0c7093ab79", "url": "https://github.com/apache/kafka/commit/9bb722b82e3f0f1d1dd8fd9cd55a5f0c7093ab79", "message": "Update docs/connect.html\n\nCo-authored-by: Randall Hauch <rhauch@gmail.com>", "committedDate": "2020-06-30T23:47:34Z", "type": "commit"}, {"oid": "0f52a66a8ab61e17940c48615457be67267bc3b4", "url": "https://github.com/apache/kafka/commit/0f52a66a8ab61e17940c48615457be67267bc3b4", "message": "Update docs/connect.html\n\nCo-authored-by: Randall Hauch <rhauch@gmail.com>", "committedDate": "2020-06-30T23:47:43Z", "type": "commit"}, {"oid": "8f1af2cca609beb814038500378a3e2ceb3bf2ed", "url": "https://github.com/apache/kafka/commit/8f1af2cca609beb814038500378a3e2ceb3bf2ed", "message": "Update docs/connect.html\n\nCo-authored-by: Randall Hauch <rhauch@gmail.com>", "committedDate": "2020-06-30T23:47:51Z", "type": "commit"}, {"oid": "c7a9ff8c70b94e874d6b291c1039a000b612c423", "url": "https://github.com/apache/kafka/commit/c7a9ff8c70b94e874d6b291c1039a000b612c423", "message": "Update docs/connect.html\n\nCo-authored-by: Randall Hauch <rhauch@gmail.com>", "committedDate": "2020-06-30T23:47:57Z", "type": "commit"}, {"oid": "4a4b03371247526e7e4d55ba262c62971c8abbd5", "url": "https://github.com/apache/kafka/commit/4a4b03371247526e7e4d55ba262c62971c8abbd5", "message": "Update docs/connect.html", "committedDate": "2020-07-01T15:15:21Z", "type": "commit"}]}