{"pr_number": 8964, "pr_title": "KAFKA-9450: Decouple flushing state from commiting", "pr_createdAt": "2020-07-01T03:20:48Z", "pr_url": "https://github.com/apache/kafka/pull/8964", "timeline": [{"oid": "a4c949f7a3f42dd792803100b061fcbab5bf976d", "url": "https://github.com/apache/kafka/commit/a4c949f7a3f42dd792803100b061fcbab5bf976d", "message": "first commit", "committedDate": "2020-06-30T01:10:33Z", "type": "commit"}, {"oid": "36fa307009d3326153a58e702d3598eb7a73a80a", "url": "https://github.com/apache/kafka/commit/36fa307009d3326153a58e702d3598eb7a73a80a", "message": "major fixes", "committedDate": "2020-06-30T23:21:55Z", "type": "commit"}, {"oid": "1952371dc840c3a70fe246956a1b0285b78390b1", "url": "https://github.com/apache/kafka/commit/1952371dc840c3a70fe246956a1b0285b78390b1", "message": "flush cache before commit", "committedDate": "2020-07-01T01:34:24Z", "type": "commit"}, {"oid": "e0713dd346101eb098958619b580406b5af6517d", "url": "https://github.com/apache/kafka/commit/e0713dd346101eb098958619b580406b5af6517d", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into K9450-decouple-flush-state-from-commit", "committedDate": "2020-07-01T01:42:02Z", "type": "commit"}, {"oid": "cea36626a4e104823dd81775d2efef40d96f7ea5", "url": "https://github.com/apache/kafka/commit/cea36626a4e104823dd81775d2efef40d96f7ea5", "message": "suppression buffer needs to be flushed too", "committedDate": "2020-07-01T02:33:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA5Mzg0Ng==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r448093846", "bodyText": "I'm a bit torn about this optimization to avoid double checkpointing, because on the other hand, even if we have not made any progress since loaded the checkpoint, we'd still need to make a checkpoint upon closing if we have never made one before -- and I use emptyMap as an indicator for that.\nGiven that upon suspending we are now less likely to checkpoint, the chance that we would double checkpointing (when transiting to suspended, and when transiting to closed) is smaller, and hence I'm thinking maybe I'd just remove this optimization to make the logic a bit simpler. LMK WDYT.", "author": "guozhangwang", "createdAt": "2020-07-01T03:23:53Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -38,13 +41,39 @@\n  */\n final class StateManagerUtil {\n     static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n+    static final long OFFSET_DELTA_THRESHOLD_FOR_CHECKPOINT = 10_000L;\n \n     private StateManagerUtil() {}\n \n     static RecordConverter converterForStore(final StateStore store) {\n         return isTimestamped(store) ? rawValueToTimestampedValue() : identity();\n     }\n \n+    static boolean checkpointNeeded(final boolean enforceCheckpoint,\n+                                    final Map<TopicPartition, Long> oldOffsetSnapshot,\n+                                    final Map<TopicPartition, Long> newOffsetSnapshot) {\n+        // we should always have the old snapshot post completing the register state stores;\n+        // if it is null it means the registration is not done and hence we should not overwrite the checkpoint\n+        if (oldOffsetSnapshot == null)\n+            return false;\n+\n+        // if the previous snapshot is empty while the current snapshot is not then we should always checkpoint;\n+        // note if the task is stateless or stateful but no stores logged, the snapshot would also be empty\n+        // and hence it's okay to not checkpoint\n+        if (oldOffsetSnapshot.isEmpty() && !newOffsetSnapshot.isEmpty())\n+            return true;\n+\n+        // we can checkpoint if the the difference between the current and the previous snapshot is large enough\n+        long totalOffsetDelta = 0L;\n+        for (final Map.Entry<TopicPartition, Long> entry : newOffsetSnapshot.entrySet()) {\n+            totalOffsetDelta += Math.abs(oldOffsetSnapshot.getOrDefault(entry.getKey(), 0L) - entry.getValue());\n+        }\n+\n+        // when enforcing checkpoint is required, we should overwrite the checkpoint if it is different from the old one;\n+        // otherwise, we only overwrite the checkpoint if it is largely different from the old one\n+        return enforceCheckpoint ? totalOffsetDelta > 0 : totalOffsetDelta > OFFSET_DELTA_THRESHOLD_FOR_CHECKPOINT;", "originalCommit": "cea36626a4e104823dd81775d2efef40d96f7ea5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzNjg5Mw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r458436893", "bodyText": "Is this out of date? It seems like we now never checkpoint during suspension so we don't have to bother with this optimization", "author": "ableegoldman", "createdAt": "2020-07-21T22:59:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA5Mzg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg1MzUwMA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r459853500", "bodyText": "Sorry I meant the \"commit\" during suspension: when we suspend a task we would commit the offsets and in postCommit we may write the checkpoint file, and then when we close the suspended task we may commit again (note the prepareCommit call should return empty offsets since there's nothing new to commit, but in postCommit we would checkpoint the file again, i.e. double writing checkpoint.\nSo what I did here is to not enforce checkpointing in postCommit during suspension, and only enforce checkpointing in postCommit during closing.", "author": "guozhangwang", "createdAt": "2020-07-24T05:05:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA5Mzg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTc1NjA5Mg==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r461756092", "bodyText": "Can you elaborate on why not enforce the checkpoint during suspension in handleRevocation (for the tasks to be closed)? It seems like we just re-introduce this problem of potentially double checkpointing and I'm not sure I understand why we need to checkpoint only in handleAssignment. Is this an optimization or a correctness issue or a code cleanness thing?", "author": "ableegoldman", "createdAt": "2020-07-28T17:37:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA5Mzg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk2NzkzNg==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r461967936", "bodyText": "I think we do not need to enforce checkpoint during suspension but only need to do that during closure / recycling; if a suspended task is resumed then we do not need to write checkpoint in between.\nBut admittedly moving forward most suspended tasks would be closed or recycled \ud83d\ude42 So I can change that back.", "author": "guozhangwang", "createdAt": "2020-07-29T00:24:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA5Mzg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2MjUxMA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466062510", "bodyText": "I would expect that writing the checkpoint file should be cheap anyway (it's just some metadata, how bad can it be?) -- or are you worried about the blocking flush to the file system? Thus, I don't consider double-checkpointing as a real issue? Might be a case of pre-mature optimization? It might simplify our code if we just blindly write the checkpoint data even if it did not change? -- But I am fine either way.", "author": "mjsax", "createdAt": "2020-08-05T23:39:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA5Mzg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4NDA3OQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466084079", "bodyText": "In the current code we actually call postCommit(true) both during suspension and closure. It's just that in checkpointNeeded we would only return true with enforce if it has changed --- personally I think this is a general optimization worth applying.", "author": "guozhangwang", "createdAt": "2020-08-06T00:56:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA5Mzg0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA5NDI1NA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r448094254", "bodyText": "I decided to extract out the update of the changelog offsets from actually writing the offsets since even if we do not want to write the file, we still need to update the offsets.\nThe reason I did not yet remove the parameter from checkpoint is that global-task is still using it. I plan to remove it when consolidating the global task.", "author": "guozhangwang", "createdAt": "2020-07-01T03:25:32Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -542,13 +530,22 @@ public void closeCleanAndRecycleState() {\n         log.info(\"Closed clean and recycled state\");\n     }\n \n-    private void writeCheckpoint() {\n+    /**\n+     * The following exceptions maybe thrown from the state manager flushing call\n+     *\n+     * @throws TaskMigratedException recoverable error sending changelog records that would cause the task to be removed\n+     * @throws StreamsException fatal error when flushing the state store, for example sending changelog records failed\n+     *                          or flushing state store get IO errors; such error should cause the thread to die\n+     */\n+    @Override\n+    protected void maybeWriteCheckpoint(final boolean enforceCheckpoint) {", "originalCommit": "cea36626a4e104823dd81775d2efef40d96f7ea5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyNTM5MQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r458425391", "bodyText": "Was there a reason to not just add #updateChangelogOffsets to the StateManager interface and remove the checkpointable offsets argument from #checkpoint?", "author": "ableegoldman", "createdAt": "2020-07-21T22:28:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA5NDI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg1OTcyMw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r459859723", "bodyText": "Let me try and see if that works; will get back to you.", "author": "guozhangwang", "createdAt": "2020-07-24T05:35:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA5NDI1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODY5MDAyMg==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r448690022", "bodyText": "Inspired by https://github.com/apache/kafka/pull/8962/files#diff-f0037ae2fd44bdd0d84f3ef37b43bc05R277, I think we should actually enforce checkpoint (i.e. set to true) when suspending since the suspended task may not be closed but recycled. cc @ableegoldman\nI will push a new commit changing to true.", "author": "guozhangwang", "createdAt": "2020-07-02T00:55:05Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -478,9 +479,11 @@ void handleRevocation(final Collection<TopicPartition> revokedPartitions) {\n \n         commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n \n+        // We do not need to enforce checkpointing upon suspending a task: if it is resumed later we just\n+        // proceed normally; if it is closed we would checkpoint then\n         for (final Task task : revokedTasks) {\n             try {\n-                task.postCommit();\n+                task.postCommit(false);", "originalCommit": "cea36626a4e104823dd81775d2efef40d96f7ea5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU1NTQ2NQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r450555465", "bodyText": "Actually after rebased on latest trunk I think upon suspending we can still not enforcing the checkpointing, but only enforce upon closing / recycling in handleAssignment.", "author": "guozhangwang", "createdAt": "2020-07-07T01:03:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODY5MDAyMg=="}], "type": "inlineReview"}, {"oid": "2158381e553706f1facbd21989eafef06d73f5b4", "url": "https://github.com/apache/kafka/commit/2158381e553706f1facbd21989eafef06d73f5b4", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into K9450-decouple-flush-state-from-commit", "committedDate": "2020-07-07T00:29:41Z", "type": "commit"}, {"oid": "4394dd2316783c286d1b8c042863b558b53fdec7", "url": "https://github.com/apache/kafka/commit/4394dd2316783c286d1b8c042863b558b53fdec7", "message": "rebased on trunk", "committedDate": "2020-07-07T02:20:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU3NjEwMw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r450576103", "bodyText": "This is just a placeholder for making less frequent flushing than commits, currently I'm making it num.records based, but it's open for better proposal to decide when flushing should be executed.", "author": "guozhangwang", "createdAt": "2020-07-07T02:23:55Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -38,13 +41,39 @@\n  */\n final class StateManagerUtil {\n     static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n+    static final long OFFSET_DELTA_THRESHOLD_FOR_CHECKPOINT = 10_000L;", "originalCommit": "4394dd2316783c286d1b8c042863b558b53fdec7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDIxMTI5MQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r460211291", "bodyText": "It would be awesome if we could do it based on the size of the pending data and the configured memtable size. Not sure if that's really feasible, just throwing it out there", "author": "ableegoldman", "createdAt": "2020-07-24T18:10:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU3NjEwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU5MDg5MQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r460590891", "bodyText": "You mean when the accumulated pending data is larger than the memtable, we should checkpoint?", "author": "guozhangwang", "createdAt": "2020-07-27T00:05:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU3NjEwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIyOTE3Mw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r461229173", "bodyText": "Something like that. We know rocksdb will render the memtable immutable once it reaches the configured memtable size, after that it will flush once the number of immutable memtables reaches the configured value. Probably makes sense to align our checkpoint/flushing to the configured rocksdb flushing.\nWould be cool if we could piggy-back on the rocksdb options and avoid a new config in Streams altogether, but obviously not everyone uses rocksdb", "author": "ableegoldman", "createdAt": "2020-07-27T23:33:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU3NjEwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTI2Njc5NQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r461266795", "bodyText": "Yeah my major concern is to tie the flushing policy with rocksdb -- although it is the default persistent stores now, we should avoid tying with a specific type of stores.", "author": "guozhangwang", "createdAt": "2020-07-28T01:40:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU3NjEwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU3NzE5MQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r450577191", "bodyText": "Admit this is kinda hacky, but I'd have to do this for cached store and suppression buffer. Moving forward I think the first can be removed when we decouple caching with emitting, but for suppression buffer maybe we can have a more general way to fix it, for example maybe we could just have changelogger to always buffer itself so that suppression buffers do not need to buffer itself to changelogger. cc @vvcephei", "author": "guozhangwang", "createdAt": "2020-07-07T02:28:02Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -454,6 +456,41 @@ public void flush() {\n         }\n     }\n \n+    public void flushCache() {\n+        RuntimeException firstException = null;\n+        // attempting to flush the stores\n+        if (!stores.isEmpty()) {\n+            log.debug(\"Flushing all store caches registered in the state manager: {}\", stores);\n+            for (final StateStoreMetadata metadata : stores.values()) {\n+                final StateStore store = metadata.stateStore;\n+\n+                try {\n+                    // buffer should be flushed to send all records to changelog\n+                    if (store instanceof TimeOrderedKeyValueBuffer) {", "originalCommit": "4394dd2316783c286d1b8c042863b558b53fdec7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDIxMjA4NA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r460212084", "bodyText": "WDYT about adding a generic FlushingRequiredStore marker interface that all caching state stores and the suppression buffer would both implement. It seems weird to handle them separately.\nWe could even make this public and allow user custom state stores to implement this, but that might be opening a can of worms we will greatly regret \ud83d\ude09", "author": "ableegoldman", "createdAt": "2020-07-24T18:12:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU3NzE5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU5MjI0OQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r460592249", "bodyText": "My plan is actually to remove the flushCache once we decoupled caching with emitting (see the TODO comment on the caller).", "author": "guozhangwang", "createdAt": "2020-07-27T00:16:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU3NzE5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIzMDIzMA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r461230230", "bodyText": "So you're saying we'd still need to flush the suppression buffer but not the cache once we decouple caching from emitting? Or that we can remove this flushCache method altogether once that is done? Or that it will still do some flushing, but will not resemble the current flushCache method at all", "author": "ableegoldman", "createdAt": "2020-07-27T23:36:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU3NzE5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTI2ODQzNg==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r461268436", "bodyText": "I'm thinking we can remove the whole flushCache method.", "author": "guozhangwang", "createdAt": "2020-07-28T01:46:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU3NzE5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTc0NzI4NA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r461747284", "bodyText": "Is there a ticket for that?", "author": "ableegoldman", "createdAt": "2020-07-28T17:23:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU3NzE5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1MTk5MQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466051991", "bodyText": "I agree. IMHO, we should get rid of KTable/store cache all together and only have a \"changelog-writer-cache\" that regular stores and suppress() can use to reduce the write load on the changelog topic. For downstream rate control, users should use suppress() (and we might want to try to unify suppress() and the upstream store somehow eventually to avoid the current redundancy)", "author": "mjsax", "createdAt": "2020-08-05T23:06:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU3NzE5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MjMxOQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466082319", "bodyText": "I'm going to create the ticket of that very soon.", "author": "guozhangwang", "createdAt": "2020-08-06T00:50:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDU3NzE5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAyMTIyNQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r451021225", "bodyText": "As we discussed in the other PR, I'm removing the logic for TM to check on task state and replaced it with checking that there should be nothing to commit (since suspend / pre-commit / post-commit are not all idempotent).", "author": "guozhangwang", "createdAt": "2020-07-07T17:16:13Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -243,18 +242,24 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n \n         for (final Task task : tasksToClose) {\n             try {\n-                if (task.isActive()) {", "originalCommit": "4394dd2316783c286d1b8c042863b558b53fdec7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2OTYwNA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466069604", "bodyText": "What other PR?", "author": "mjsax", "createdAt": "2020-08-06T00:03:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAyMTIyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEwNDY3MQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466104671", "bodyText": "This is the PR from @ableegoldman #8962", "author": "guozhangwang", "createdAt": "2020-08-06T02:13:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTAyMTIyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI4MjQyMQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r458282421", "bodyText": "nit: can you use braces here?", "author": "ableegoldman", "createdAt": "2020-07-21T17:52:35Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -454,6 +456,41 @@ public void flush() {\n         }\n     }\n \n+    public void flushCache() {\n+        RuntimeException firstException = null;\n+        // attempting to flush the stores\n+        if (!stores.isEmpty()) {\n+            log.debug(\"Flushing all store caches registered in the state manager: {}\", stores);\n+            for (final StateStoreMetadata metadata : stores.values()) {\n+                final StateStore store = metadata.stateStore;\n+\n+                try {\n+                    // buffer should be flushed to send all records to changelog\n+                    if (store instanceof TimeOrderedKeyValueBuffer) {\n+                        store.flush();\n+                    } else if (store instanceof CachedStateStore) {\n+                        ((CachedStateStore) store).flushCache();\n+                    }\n+                    log.trace(\"Flushed cache or buffer {}\", store.name());\n+                } catch (final RuntimeException exception) {\n+                    if (firstException == null) {\n+                        // do NOT wrap the error if it is actually caused by Streams itself\n+                        if (exception instanceof StreamsException)\n+                            firstException = exception;\n+                        else\n+                            firstException = new ProcessorStateException(", "originalCommit": "4394dd2316783c286d1b8c042863b558b53fdec7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg1MDIxMg==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r459850212", "bodyText": "Ack", "author": "guozhangwang", "createdAt": "2020-07-24T04:48:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI4MjQyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI4NDgyMQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r458284821", "bodyText": "TODO: we still need to keep it as part of the checkpoint for global tasks\n\nTook me a while to realize that \"it\" refers to the argument here -- can you clarify the comment?", "author": "ableegoldman", "createdAt": "2020-07-21T17:56:31Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -520,20 +557,27 @@ void transitionTaskType(final TaskType newType, final LogContext logContext) {\n         log.debug(\"Transitioning state manager for {} task {} to {}\", oldType, taskId, newType);\n     }\n \n-    @Override\n-    public void checkpoint(final Map<TopicPartition, Long> writtenOffsets) {\n-        // first update each state store's current offset, then checkpoint\n-        // those stores that are only logged and persistent to the checkpoint file\n+    void updateChangelogOffsets(final Map<TopicPartition, Long> writtenOffsets) {\n         for (final Map.Entry<TopicPartition, Long> entry : writtenOffsets.entrySet()) {\n             final StateStoreMetadata store = findStore(entry.getKey());\n \n             if (store != null) {\n                 store.setOffset(entry.getValue());\n \n                 log.debug(\"State store {} updated to written offset {} at changelog {}\",\n-                    store.stateStore.name(), store.offset, store.changelogPartition);\n+                        store.stateStore.name(), store.offset, store.changelogPartition);\n             }\n         }\n+    }\n+\n+    @Override\n+    public void checkpoint(final Map<TopicPartition, Long> writtenOffsets) {\n+        // first update each state store's current offset, then checkpoint\n+        // those stores that are only logged and persistent to the checkpoint file\n+        // TODO: we still need to keep it as part of the checkpoint for global tasks; this could be\n+        //       removed though when we consolidate global tasks / state managers into this one", "originalCommit": "4394dd2316783c286d1b8c042863b558b53fdec7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMTE3Ng==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r458421176", "bodyText": "Happy to see this go \ud83d\ude42", "author": "ableegoldman", "createdAt": "2020-07-21T22:18:01Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -120,14 +120,12 @@ public void suspend() {\n         switch (state()) {\n             case CREATED:\n                 log.info(\"Suspended created\");\n-                checkpointNeededForSuspended = false;", "originalCommit": "4394dd2316783c286d1b8c042863b558b53fdec7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMzk0Mw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r458423943", "bodyText": "Should we add an assertion to ProcessorStateManager#checkpoint that the passed in offsets are always empty?", "author": "ableegoldman", "createdAt": "2020-07-21T22:24:39Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java", "diffHunk": "@@ -49,6 +61,31 @@\n         this.stateDirectory = stateDirectory;\n     }\n \n+    protected void initializeCheckpoint() {\n+        // we will delete the local checkpoint file after registering the state stores and loading them into the\n+        // state manager, therefore we should initialize the snapshot as empty to indicate over-write checkpoint needed\n+        offsetSnapshotSinceLastFlush = Collections.emptyMap();\n+    }\n+\n+    /**\n+     * The following exceptions maybe thrown from the state manager flushing call\n+     *\n+     * @throws TaskMigratedException recoverable error sending changelog records that would cause the task to be removed\n+     * @throws StreamsException fatal error when flushing the state store, for example sending changelog records failed\n+     *                          or flushing state store get IO errors; such error should cause the thread to die\n+     */\n+    protected void maybeWriteCheckpoint(final boolean enforceCheckpoint) {\n+        final Map<TopicPartition, Long> offsetSnapshot = stateMgr.changelogOffsets();\n+        if (StateManagerUtil.checkpointNeeded(enforceCheckpoint, offsetSnapshotSinceLastFlush, offsetSnapshot)) {\n+            // since there's no written offsets we can checkpoint with empty map,\n+            // and the state's current offset would be used to checkpoint\n+            stateMgr.flush();\n+            stateMgr.checkpoint(Collections.emptyMap());", "originalCommit": "4394dd2316783c286d1b8c042863b558b53fdec7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg1MDgzMQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r459850831", "bodyText": "The passed in parameters are not always empty unfortunately since global tasks would still pass in non-empty map.", "author": "guozhangwang", "createdAt": "2020-07-24T04:51:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMzk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDIxMjcxOQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r460212719", "bodyText": "But ProcessorStateManager doesn't handle global tasks", "author": "ableegoldman", "createdAt": "2020-07-24T18:13:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMzk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU5OTE3Mw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r460599173", "bodyText": "I see what did you mean now. Will do.", "author": "guozhangwang", "createdAt": "2020-07-27T01:01:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMzk0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyODU0MA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r458428540", "bodyText": "Hm. What if we hit a TaskMigratedException during handleRevocation? We would never finish committing them so commitNeeded would still return true and prepareCommit would return non-empty offsets right?\nIt's kind of a bummer that we can't enforce that the task was committed. What we really need to do is enforce that we attempted to commit the task -- regardless of whether or not it was successful. If the commit failed we know that either it was fatal or it was due to TaskMigrated, in which case the task will have to be closed as dirty anyways.\nThis might be beyond the scope of this PR, but just to throw out one hacky idea we could add a commitSuccessful parameter to postCommit and then always invoke that after a commit so that commitNeeded is set to false. (If commitSuccessful is false we just skip everything else in postCommit)", "author": "ableegoldman", "createdAt": "2020-07-21T22:36:10Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -243,18 +242,24 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n \n         for (final Task task : tasksToClose) {\n             try {\n-                if (task.isActive()) {\n-                    // Active tasks are revoked and suspended/committed during #handleRevocation\n-                    if (!task.state().equals(State.SUSPENDED)) {\n-                        log.error(\"Active task {} should be suspended prior to attempting to close but was in {}\",\n-                                  task.id(), task.state());\n-                        throw new IllegalStateException(\"Active task \" + task.id() + \" should have been suspended\");\n-                    }\n-                } else {\n-                    task.suspend();\n-                    task.prepareCommit();\n-                    task.postCommit();\n+                // Always try to first suspend and commit the task before closing it;\n+                // some tasks may already be suspended which should be a no-op.\n+                //\n+                // Also since active tasks should already be suspended / committed and\n+                // standby tasks should have no offsets to commit, we should expect nothing to commit\n+                task.suspend();\n+\n+                final Map<TopicPartition, OffsetAndMetadata> offsets = task.prepareCommit();\n+\n+                if (!offsets.isEmpty()) {\n+                    log.error(\"Task {} should has been committed prior to attempting to close, but it reports non-empty offsets {} to commit\",", "originalCommit": "4394dd2316783c286d1b8c042863b558b53fdec7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg1NzQ3Nw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r459857477", "bodyText": "I thought if we hit a TaskMigrated during handleRevocation then it would be thrown all the way from consumer and cause the thread to handle it via handleLostAll and hence handleAssignment should not be called.\nAfter checking the source code I think not callers are throwing the exception directly and may wrap it as a KafkaException, but even in that case we would just crash the client, right?", "author": "guozhangwang", "createdAt": "2020-07-24T05:25:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyODU0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5NDA2Mg==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r460194062", "bodyText": "I thought we would catch and save the first exception thrown by a rebalance listener callback, and then rethrow after all rebalance callbacks have been invoked? In this case that would mean handleAssignment would still get called, and then we would throw an IllegalStateException and bail on the rest of handleAssignment for no reason.\nThe IllegalStateException itself is not the problem, since only the first exception (the TaskMigrated) would ultimately be thrown up to poll. But we should still go through the rest of handleAssignment in order to properly clean up the active tasks and manage the standbys (since we don't need to close standbys in case of TaskMigrated)", "author": "ableegoldman", "createdAt": "2020-07-24T17:35:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyODU0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYyMTAzMA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r460621030", "bodyText": "I've reordered in handleRevocation so that commit is called before suspension; also even if commit failed with TaskMigrated, we would still try to continue completing the suspension so in handleAssignment we should have all active tasks suspended or should be closed dirty.", "author": "guozhangwang", "createdAt": "2020-07-27T02:54:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyODU0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzNTgyNA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r458435824", "bodyText": "This is also maybe beyond the scope of this PR, but it seems like there's no reason to do things like this anymore. Specifically, today we enforce the order suspend -> pre/postCommit -> close where suspend only closes the topology and we only use the SUSPENDED state to enforce that we suspended before closing. Why not swap the order so that we pre/postCommit -> suspend -> close and then we can move this call from postCommit to suspend where it makes more sense. Thoughts?", "author": "ableegoldman", "createdAt": "2020-07-21T22:56:13Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -466,15 +458,8 @@ public void postCommit() {\n                  */\n                 partitionGroup.clear();", "originalCommit": "4394dd2316783c286d1b8c042863b558b53fdec7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg1ODMxNA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r459858314", "bodyText": "Hmm, what's the original reason that we must suspend before commit?", "author": "guozhangwang", "createdAt": "2020-07-24T05:29:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzNTgyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzODM2Ng==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r458438366", "bodyText": "Can you move this comment down a line? We should avoid confusion since we actually don't initialize the (flush) snapshot with the current offsets, just the (commit) snapshot.\nTo be honest, it's already pretty confusing that we initialize the two snapshots differently. Maybe you could add a quick sentence explaining why for our own future reference", "author": "ableegoldman", "createdAt": "2020-07-21T23:03:47Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -93,8 +93,8 @@ public boolean isActive() {\n     public void initializeIfNeeded() {\n         if (state() == State.CREATED) {\n             StateManagerUtil.registerStateStores(log, logPrefix, topology, stateMgr, stateDirectory, processorContext);\n-\n             // initialize the snapshot with the current offsets as we don't need to commit then until they change", "originalCommit": "4394dd2316783c286d1b8c042863b558b53fdec7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg1OTE1Mg==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r459859152", "bodyText": "We maintain two snapshots in standby because its commitNeeded is overridden such that we only commit if the offsets has changed since last time committed, whereas in active we have the flag itself to indicate so.\nI can add the above explanation for offsetSnapshotSinceLastCommit, would that be clearer?", "author": "guozhangwang", "createdAt": "2020-07-24T05:32:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzODM2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDIxNDE0Nw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r460214147", "bodyText": "Sounds good. I think the most important thing is to not have the comment that says initialize the snapshot with the current offsets... right above the line that initializes the snapshot as empty \ud83d\ude41", "author": "ableegoldman", "createdAt": "2020-07-24T18:16:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzODM2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4ODY1MQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r460588651", "bodyText": "Ack", "author": "guozhangwang", "createdAt": "2020-07-26T23:46:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzODM2Ng=="}], "type": "inlineReview"}, {"oid": "c8bef6e1f6289f1c34d6b77336df1abbe91c1102", "url": "https://github.com/apache/kafka/commit/c8bef6e1f6289f1c34d6b77336df1abbe91c1102", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into K9450-decouple-flush-state-from-commit", "committedDate": "2020-07-24T01:09:31Z", "type": "commit"}, {"oid": "3b49c2a39872b7065b821e0434018ac6dc3fc8d9", "url": "https://github.com/apache/kafka/commit/3b49c2a39872b7065b821e0434018ac6dc3fc8d9", "message": "github comments", "committedDate": "2020-07-24T18:10:50Z", "type": "commit"}, {"oid": "9137d40e2dab98ae5b943f796ffe35ea13e5cbf0", "url": "https://github.com/apache/kafka/commit/9137d40e2dab98ae5b943f796ffe35ea13e5cbf0", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into K9450-decouple-flush-state-from-commit", "committedDate": "2020-07-26T23:46:21Z", "type": "commit"}, {"oid": "bae8bb5a1118aad6890c60102283ddc463f6c179", "url": "https://github.com/apache/kafka/commit/bae8bb5a1118aad6890c60102283ddc463f6c179", "message": "re-order commit and suspend", "committedDate": "2020-07-27T04:41:30Z", "type": "commit"}, {"oid": "0da4bce957cf5cb9497623b0da01328f77156424", "url": "https://github.com/apache/kafka/commit/0da4bce957cf5cb9497623b0da01328f77156424", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into K9450-decouple-flush-state-from-commit", "committedDate": "2020-07-27T23:13:41Z", "type": "commit"}, {"oid": "579b141434f8d3b9bd1972ccb04a4cea74171cd0", "url": "https://github.com/apache/kafka/commit/579b141434f8d3b9bd1972ccb04a4cea74171cd0", "message": "use list not set to preserve ordering", "committedDate": "2020-07-27T23:21:01Z", "type": "commit"}, {"oid": "c0dc73fe813b77130858222426eb5bc0f8ba3d35", "url": "https://github.com/apache/kafka/commit/c0dc73fe813b77130858222426eb5bc0f8ba3d35", "message": "enforce checkpoint during suspension", "committedDate": "2020-07-29T01:15:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM5NTE0MA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r463395140", "bodyText": "It's still making me a bit uncomfortable that we call prepare/postCommit during handleRevocation. and handleAssignment. Maybe I'm being paranoid but experience has shown it's been difficult for us to keep track of which methods need to be called when, and in what order.\nIt seems like, now that we've decoupled flushing from committing, the only reason for calling pre/postCommit in handleRevocation is so that the record collector is flushed before committing offsets. So what if we extract the record collector flushing out into a separate StreamTask method that is only ever called in TaskManager#commitOffsetsOrTransaction? I haven't thought this all the way through but it just seems like we may as well go all the way in decoupling flushing from committing and split them out into separate methods. Maybe preCommit and postCommit have become relics of the past", "author": "ableegoldman", "createdAt": "2020-07-31T04:10:09Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,7 +313,96 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 addNewTask(task);\n             }\n         }\n+    }\n+\n+    private void handleCloseAndRecycle(final List<Task> tasksToRecycle,\n+                                       final List<Task> tasksToCloseClean,\n+                                       final List<Task> tasksToCloseDirty,\n+                                       final Map<TaskId, Set<TopicPartition>> activeTasksToCreate,\n+                                       final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate,\n+                                       final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions) {\n+        if (!tasksToCloseDirty.isEmpty()) {\n+            throw new IllegalArgumentException(\"Tasks to close-dirty should be empty\");\n+        }\n+\n+        // for all tasks to close or recycle, we should first right a checkpoint as in post-commit\n+        final List<Task> tasksToCheckpoint = new ArrayList<>(tasksToCloseClean);\n+        tasksToCheckpoint.addAll(tasksToRecycle);\n+        for (final Task task : tasksToCheckpoint) {\n+            try {\n+                // Always try to first suspend and commit the task before checkpointing it;\n+                // some tasks may already be suspended which should be a no-op.\n+                //\n+                // Also since active tasks should already be suspended / committed and\n+                // standby tasks should have no offsets to commit, we should expect nothing to commit\n+                task.suspend();\n+\n+                // Note that we are not actually committing here but just check if we need to write checkpoint file:\n+                // 1) for active tasks prepareCommit should return empty if it has committed during suspension successfully,\n+                //    and their changelog positions should not change at all postCommit would not write the checkpoint again.\n+                // 2) for standby tasks prepareCommit should always return empty, and then in postCommit we would probably\n+                //    write the checkpoint file.\n+                final Map<TopicPartition, OffsetAndMetadata> offsets = task.prepareCommit();", "originalCommit": "c0dc73fe813b77130858222426eb5bc0f8ba3d35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM5NTQ5Nw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r463395497", "bodyText": "I know that's kind of another large change, so feel free to tell me to drop it \ud83d\ude42  Or one of us can consider as followup work.", "author": "ableegoldman", "createdAt": "2020-07-31T04:11:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM5NTE0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzczODEzOA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r463738138", "bodyText": "Alternatively, now that we enforce checkpoint during suspension, we could just remove the pre/postCommit for active tasks in handleAssignment. It just seems nice to be able to assert that we never call pre/postCommit after a task is suspended", "author": "ableegoldman", "createdAt": "2020-07-31T17:29:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM5NTE0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc4MDQxOA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r463780418", "bodyText": "I'm leaning towards the second option here: in handleAssignment we actually do not commit at all, but only use 1) preCommit to validate certain states, and use 2) postCommit and expecting it to be a no-op actually. Let me see if I can get rid of those two in handleAssignment.", "author": "guozhangwang", "createdAt": "2020-07-31T19:02:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM5NTE0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc5MDE5MA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r463790190", "bodyText": "I tried some ways and ended-up with explicitly specifying suspend / postCommit for standby tasks only, and use prepareCommit to check if the previous revocation has failed or not. Personally I'm happy with the current workflow now.", "author": "guozhangwang", "createdAt": "2020-07-31T19:25:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM5NTE0MA=="}], "type": "inlineReview"}, {"oid": "1ea8905e5259d669128e980fe84071368d2263b8", "url": "https://github.com/apache/kafka/commit/1ea8905e5259d669128e980fe84071368d2263b8", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into K9450-decouple-flush-state-from-commit", "committedDate": "2020-07-31T19:02:27Z", "type": "commit"}, {"oid": "b70fc0025aac54ab564f2250e50a92aa2fb65b4a", "url": "https://github.com/apache/kafka/commit/b70fc0025aac54ab564f2250e50a92aa2fb65b4a", "message": "minor tweaks", "committedDate": "2020-07-31T19:23:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg2NzA5OA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r463867098", "bodyText": "Seems like there's the missing possibility that it's not TimeOrdered or Cached. Should we log a different message than \"Flushed cache or buffer\" in that case, to indicate we didn't flush it?", "author": "vvcephei", "createdAt": "2020-07-31T22:08:16Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -461,6 +463,42 @@ public void flush() {\n         }\n     }\n \n+    public void flushCache() {\n+        RuntimeException firstException = null;\n+        // attempting to flush the stores\n+        if (!stores.isEmpty()) {\n+            log.debug(\"Flushing all store caches registered in the state manager: {}\", stores);\n+            for (final StateStoreMetadata metadata : stores.values()) {\n+                final StateStore store = metadata.stateStore;\n+\n+                try {\n+                    // buffer should be flushed to send all records to changelog\n+                    if (store instanceof TimeOrderedKeyValueBuffer) {\n+                        store.flush();\n+                    } else if (store instanceof CachedStateStore) {\n+                        ((CachedStateStore) store).flushCache();\n+                    }", "originalCommit": "b70fc0025aac54ab564f2250e50a92aa2fb65b4a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MzE3OQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r464683179", "bodyText": "For stores that's not time-ordered or cached, we should not flush them indeed. In fact moving forward I think we would not flush cache store anyways since they will be removed. I.e. generally speaking we should not flush cache always. In that sense the log4j entry looks reasonable to me?", "author": "guozhangwang", "createdAt": "2020-08-03T21:56:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg2NzA5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg3MDA2Mg==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r463870062", "bodyText": "What's the rationale for subtracting the larger value from the smaller one and then taking the absolute value?", "author": "vvcephei", "createdAt": "2020-07-31T22:19:25Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -38,13 +41,39 @@\n  */\n final class StateManagerUtil {\n     static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n+    static final long OFFSET_DELTA_THRESHOLD_FOR_CHECKPOINT = 10_000L;\n \n     private StateManagerUtil() {}\n \n     static RecordConverter converterForStore(final StateStore store) {\n         return isTimestamped(store) ? rawValueToTimestampedValue() : identity();\n     }\n \n+    static boolean checkpointNeeded(final boolean enforceCheckpoint,\n+                                    final Map<TopicPartition, Long> oldOffsetSnapshot,\n+                                    final Map<TopicPartition, Long> newOffsetSnapshot) {\n+        // we should always have the old snapshot post completing the register state stores;\n+        // if it is null it means the registration is not done and hence we should not overwrite the checkpoint\n+        if (oldOffsetSnapshot == null)\n+            return false;\n+\n+        // if the previous snapshot is empty while the current snapshot is not then we should always checkpoint;\n+        // note if the task is stateless or stateful but no stores logged, the snapshot would also be empty\n+        // and hence it's okay to not checkpoint\n+        if (oldOffsetSnapshot.isEmpty() && !newOffsetSnapshot.isEmpty())\n+            return true;\n+\n+        // we can checkpoint if the the difference between the current and the previous snapshot is large enough\n+        long totalOffsetDelta = 0L;\n+        for (final Map.Entry<TopicPartition, Long> entry : newOffsetSnapshot.entrySet()) {\n+            totalOffsetDelta += Math.abs(oldOffsetSnapshot.getOrDefault(entry.getKey(), 0L) - entry.getValue());", "originalCommit": "b70fc0025aac54ab564f2250e50a92aa2fb65b4a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4Mzk4Nw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r464683987", "bodyText": "Was trying to make sure we do not get an NPE but I think that's not necessary.. will change.", "author": "guozhangwang", "createdAt": "2020-08-03T21:58:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg3MDA2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg3MzcwOA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r463873708", "bodyText": "It seems a little odd to have handleCloseAndRecycle not do this but just update the taskToCloseDirty list, since it handles everything else.", "author": "vvcephei", "createdAt": "2020-07-31T22:34:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -267,80 +266,26 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 // check for tasks that were owned previously but have changed active/standby status\n                 tasksToRecycle.add(task);\n             } else {\n-                tasksToClose.add(task);\n-            }\n-        }\n-\n-        for (final Task task : tasksToClose) {\n-            try {\n-                if (task.isActive()) {\n-                    // Active tasks are revoked and suspended/committed during #handleRevocation\n-                    if (!task.state().equals(State.SUSPENDED)) {\n-                        log.error(\"Active task {} should be suspended prior to attempting to close but was in {}\",\n-                                  task.id(), task.state());\n-                        throw new IllegalStateException(\"Active task \" + task.id() + \" should have been suspended\");\n-                    }\n-                } else {\n-                    task.suspend();\n-                    task.prepareCommit();\n-                    task.postCommit();\n-                }\n-                completeTaskCloseClean(task);\n-                cleanUpTaskProducer(task, taskCloseExceptions);\n-                tasks.remove(task.id());\n-            } catch (final RuntimeException e) {\n-                final String uncleanMessage = String.format(\n-                    \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n-                    task.id());\n-                log.error(uncleanMessage, e);\n-                taskCloseExceptions.put(task.id(), e);\n-                // We've already recorded the exception (which is the point of clean).\n-                // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                dirtyTasks.add(task);\n+                tasksToCloseClean.add(task);\n             }\n         }\n \n-        for (final Task oldTask : tasksToRecycle) {\n-            final Task newTask;\n-            try {\n-                if (oldTask.isActive()) {\n-                    if (!oldTask.state().equals(State.SUSPENDED)) {\n-                        // Active tasks are revoked and suspended/committed during #handleRevocation\n-                        log.error(\"Active task {} should be suspended prior to attempting to close but was in {}\",\n-                                  oldTask.id(), oldTask.state());\n-                        throw new IllegalStateException(\"Active task \" + oldTask.id() + \" should have been suspended\");\n-                    }\n-                    final Set<TopicPartition> partitions = standbyTasksToCreate.remove(oldTask.id());\n-                    newTask = standbyTaskCreator.createStandbyTaskFromActive((StreamTask) oldTask, partitions);\n-                    cleanUpTaskProducer(oldTask, taskCloseExceptions);\n-                } else {\n-                    oldTask.suspend();\n-                    oldTask.prepareCommit();\n-                    oldTask.postCommit();\n-                    final Set<TopicPartition> partitions = activeTasksToCreate.remove(oldTask.id());\n-                    newTask = activeTaskCreator.createActiveTaskFromStandby((StandbyTask) oldTask, partitions, mainConsumer);\n-                }\n-                tasks.remove(oldTask.id());\n-                addNewTask(newTask);\n-            } catch (final RuntimeException e) {\n-                final String uncleanMessage = String.format(\"Failed to recycle task %s cleanly. Attempting to close remaining tasks before re-throwing:\", oldTask.id());\n-                log.error(uncleanMessage, e);\n-                taskCloseExceptions.put(oldTask.id(), e);\n-                dirtyTasks.add(oldTask);\n-            }\n-        }\n+        // close and recycle those tasks\n+        handleCloseAndRecycle(tasksToRecycle, tasksToCloseClean, tasksToCloseDirty, activeTasksToCreate, standbyTasksToCreate, taskCloseExceptions);\n \n-        for (final Task task : dirtyTasks) {\n+        // for tasks that cannot be cleanly closed or recycled, close them dirty\n+        for (final Task task : tasksToCloseDirty) {", "originalCommit": "b70fc0025aac54ab564f2250e50a92aa2fb65b4a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4NDY2MA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r464684660", "bodyText": "That's a good point, I will update.", "author": "guozhangwang", "createdAt": "2020-08-03T22:00:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg3MzcwOA=="}], "type": "inlineReview"}, {"oid": "c632c3dc2581a13dd882ebc293313f9438cba339", "url": "https://github.com/apache/kafka/commit/c632c3dc2581a13dd882ebc293313f9438cba339", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into K9450-decouple-flush-state-from-commit", "committedDate": "2020-08-03T21:51:57Z", "type": "commit"}, {"oid": "906e067af14301919758243c7666e54830633a71", "url": "https://github.com/apache/kafka/commit/906e067af14301919758243c7666e54830633a71", "message": "github comment, plus a fix for system test", "committedDate": "2020-08-03T23:01:27Z", "type": "commit"}, {"oid": "4e9b54624fd831b6affb13d45279b96804b91010", "url": "https://github.com/apache/kafka/commit/4e9b54624fd831b6affb13d45279b96804b91010", "message": "fix unit tests", "committedDate": "2020-08-04T04:31:24Z", "type": "commit"}, {"oid": "19b10055fdaf04ebe7b8434741afc3c1a21440bb", "url": "https://github.com/apache/kafka/commit/19b10055fdaf04ebe7b8434741afc3c1a21440bb", "message": "improved eos test driver", "committedDate": "2020-08-04T05:09:28Z", "type": "commit"}, {"oid": "6f2d692434a556dcdcf91b38e593138a8a334513", "url": "https://github.com/apache/kafka/commit/6f2d692434a556dcdcf91b38e593138a8a334513", "message": "minor fix", "committedDate": "2020-08-04T06:31:39Z", "type": "commit"}, {"oid": "f463d8f368f8dd2a81d25a2c64c308151216bc96", "url": "https://github.com/apache/kafka/commit/f463d8f368f8dd2a81d25a2c64c308151216bc96", "message": "tweak unit tests", "committedDate": "2020-08-04T17:06:26Z", "type": "commit"}, {"oid": "335a3459aac9b3b4df825786cef0f30c214210a8", "url": "https://github.com/apache/kafka/commit/335a3459aac9b3b4df825786cef0f30c214210a8", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into K9450-decouple-flush-state-from-commit", "committedDate": "2020-08-04T22:06:43Z", "type": "commit"}, {"oid": "122ff1d0bc85a25e40c526ecdf10dbf985560e98", "url": "https://github.com/apache/kafka/commit/122ff1d0bc85a25e40c526ecdf10dbf985560e98", "message": "commit first then suspend in closing", "committedDate": "2020-08-04T23:53:20Z", "type": "commit"}, {"oid": "913031f247f0591bba4b5dc4c9e8b664d7298fa5", "url": "https://github.com/apache/kafka/commit/913031f247f0591bba4b5dc4c9e8b664d7298fa5", "message": "first commit then suspend in closing", "committedDate": "2020-08-04T23:58:18Z", "type": "commit"}, {"oid": "aa95e1367eb31660a156eb484e2d455ca35029a1", "url": "https://github.com/apache/kafka/commit/aa95e1367eb31660a156eb484e2d455ca35029a1", "message": "remove debugging info", "committedDate": "2020-08-05T16:52:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA0OTY1NA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466049654", "bodyText": "Why do we actually delete the checkpoint file after registering? For non-eos, it seems we only need to delete the checkopint file if we wipe out the whole store?\nOr is it a \"simplification\" do not distinguish between eos/non-eos for this case?", "author": "mjsax", "createdAt": "2020-08-05T22:59:15Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java", "diffHunk": "@@ -49,6 +61,30 @@\n         this.stateDirectory = stateDirectory;\n     }\n \n+    protected void initializeCheckpoint() {\n+        // we will delete the local checkpoint file after registering the state stores and loading them into the", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MjE0NA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466082144", "bodyText": "Yes it is a simplification: we are going to write the checkpoint file after initialization and on the first commit we would write checkpoint file again, so that we only have a short window of risk.", "author": "guozhangwang", "createdAt": "2020-08-06T00:49:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA0OTY1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1NjIzNQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466056235", "bodyText": "Nit: add { }", "author": "mjsax", "createdAt": "2020-08-05T23:19:27Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -390,9 +399,12 @@ private static void verifyReceivedAllRecords(final Map<TopicPartition, List<Cons\n                 final int expectedValue = integerDeserializer.deserialize(expected.topic(), expected.value());\n \n                 if (!receivedKey.equals(expectedKey) || receivedValue != expectedValue) {\n-                    throw new RuntimeException(\"Result verification failed for \" + receivedRecord + \" expected <\" + expectedKey + \",\" + expectedValue + \"> but was <\" + receivedKey + \",\" + receivedValue + \">\");\n+                    exception = new RuntimeException(\"Result verification failed for \" + receivedRecord + \" expected <\" + expectedKey + \",\" + expectedValue + \"> but was <\" + receivedKey + \",\" + receivedValue + \">\");\n                 }\n             }\n+\n+            if (exception != null)", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MjQyNA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466082424", "bodyText": "ack", "author": "guozhangwang", "createdAt": "2020-08-06T00:50:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1NjIzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1NjQ4MQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466056481", "bodyText": "Nit: add { }", "author": "mjsax", "createdAt": "2020-08-05T23:20:12Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -379,9 +379,18 @@ private static void verifyReceivedAllRecords(final Map<TopicPartition, List<Cons\n         final IntegerDeserializer integerDeserializer = new IntegerDeserializer();\n         for (final Map.Entry<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> partitionRecords : receivedRecords.entrySet()) {\n             final TopicPartition inputTopicPartition = new TopicPartition(\"data\", partitionRecords.getKey().partition());\n-            final Iterator<ConsumerRecord<byte[], byte[]>> expectedRecord = expectedRecords.get(inputTopicPartition).iterator();\n+            final List<ConsumerRecord<byte[], byte[]>> receivedRecordsForPartition = partitionRecords.getValue();\n+            final List<ConsumerRecord<byte[], byte[]>> expectedRecordsForPartition = expectedRecords.get(inputTopicPartition);\n+\n+            System.out.println(partitionRecords.getKey() + \" with \" + receivedRecordsForPartition.size() + \", \" +\n+                    inputTopicPartition + \" with \" + expectedRecordsForPartition.size());\n+\n+            final Iterator<ConsumerRecord<byte[], byte[]>> expectedRecord = expectedRecordsForPartition.iterator();\n+            RuntimeException exception = null;\n+            for (final ConsumerRecord<byte[], byte[]> receivedRecord : receivedRecordsForPartition) {\n+                if (!expectedRecord.hasNext())", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1NjY3Mw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466056673", "bodyText": "Should be break for this case?", "author": "mjsax", "createdAt": "2020-08-05T23:20:42Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -379,9 +379,18 @@ private static void verifyReceivedAllRecords(final Map<TopicPartition, List<Cons\n         final IntegerDeserializer integerDeserializer = new IntegerDeserializer();\n         for (final Map.Entry<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> partitionRecords : receivedRecords.entrySet()) {\n             final TopicPartition inputTopicPartition = new TopicPartition(\"data\", partitionRecords.getKey().partition());\n-            final Iterator<ConsumerRecord<byte[], byte[]>> expectedRecord = expectedRecords.get(inputTopicPartition).iterator();\n+            final List<ConsumerRecord<byte[], byte[]>> receivedRecordsForPartition = partitionRecords.getValue();\n+            final List<ConsumerRecord<byte[], byte[]>> expectedRecordsForPartition = expectedRecords.get(inputTopicPartition);\n+\n+            System.out.println(partitionRecords.getKey() + \" with \" + receivedRecordsForPartition.size() + \", \" +\n+                    inputTopicPartition + \" with \" + expectedRecordsForPartition.size());\n+\n+            final Iterator<ConsumerRecord<byte[], byte[]>> expectedRecord = expectedRecordsForPartition.iterator();\n+            RuntimeException exception = null;\n+            for (final ConsumerRecord<byte[], byte[]> receivedRecord : receivedRecordsForPartition) {\n+                if (!expectedRecord.hasNext())\n+                    exception = new RuntimeException(\"Result verification failed for \" + receivedRecord + \" since there's no more expected record\");", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MjgzMA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466082830", "bodyText": "It is failed in fact: we expect the received records is no smaller than the expected record.", "author": "guozhangwang", "createdAt": "2020-08-06T00:51:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1NjY3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcxNzIxMg==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466717212", "bodyText": "Sure. The question is, why do we not break the for loop? If we run out of expectedRecord there does not seem to be a reason to iterator to the end of receivedRecord (and assign exception over and over again)?", "author": "mjsax", "createdAt": "2020-08-06T22:22:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1NjY3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1NjgyMw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466056823", "bodyText": "Should be break here?", "author": "mjsax", "createdAt": "2020-08-05T23:21:06Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -390,9 +399,12 @@ private static void verifyReceivedAllRecords(final Map<TopicPartition, List<Cons\n                 final int expectedValue = integerDeserializer.deserialize(expected.topic(), expected.value());\n \n                 if (!receivedKey.equals(expectedKey) || receivedValue != expectedValue) {\n-                    throw new RuntimeException(\"Result verification failed for \" + receivedRecord + \" expected <\" + expectedKey + \",\" + expectedValue + \"> but was <\" + receivedKey + \",\" + receivedValue + \">\");\n+                    exception = new RuntimeException(\"Result verification failed for \" + receivedRecord + \" expected <\" + expectedKey + \",\" + expectedValue + \"> but was <\" + receivedKey + \",\" + receivedValue + \">\");", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1NzYwMA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466057600", "bodyText": "nit: formatting:\nfirstException = new ProcessorStateException(\n    format(\"%sFailed to flush cache of store %s\", logPrefix, store.name()),\n    exception\n);", "author": "mjsax", "createdAt": "2020-08-05T23:23:35Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -461,6 +463,42 @@ public void flush() {\n         }\n     }\n \n+    public void flushCache() {\n+        RuntimeException firstException = null;\n+        // attempting to flush the stores\n+        if (!stores.isEmpty()) {\n+            log.debug(\"Flushing all store caches registered in the state manager: {}\", stores);\n+            for (final StateStoreMetadata metadata : stores.values()) {\n+                final StateStore store = metadata.stateStore;\n+\n+                try {\n+                    // buffer should be flushed to send all records to changelog\n+                    if (store instanceof TimeOrderedKeyValueBuffer) {\n+                        store.flush();\n+                    } else if (store instanceof CachedStateStore) {\n+                        ((CachedStateStore) store).flushCache();\n+                    }\n+                    log.trace(\"Flushed cache or buffer {}\", store.name());\n+                } catch (final RuntimeException exception) {\n+                    if (firstException == null) {\n+                        // do NOT wrap the error if it is actually caused by Streams itself\n+                        if (exception instanceof StreamsException) {\n+                            firstException = exception;\n+                        } else {\n+                            firstException = new ProcessorStateException(\n+                                    format(\"%sFailed to flush cache of store %s\", logPrefix, store.name()), exception);", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MzA4Ng==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466083086", "bodyText": "ack", "author": "guozhangwang", "createdAt": "2020-08-06T00:52:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1NzYwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2MjcyMg==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466062722", "bodyText": "nit: add { }", "author": "mjsax", "createdAt": "2020-08-05T23:39:49Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -38,13 +41,39 @@\n  */\n final class StateManagerUtil {\n     static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n+    static final long OFFSET_DELTA_THRESHOLD_FOR_CHECKPOINT = 10_000L;\n \n     private StateManagerUtil() {}\n \n     static RecordConverter converterForStore(final StateStore store) {\n         return isTimestamped(store) ? rawValueToTimestampedValue() : identity();\n     }\n \n+    static boolean checkpointNeeded(final boolean enforceCheckpoint,\n+                                    final Map<TopicPartition, Long> oldOffsetSnapshot,\n+                                    final Map<TopicPartition, Long> newOffsetSnapshot) {\n+        // we should always have the old snapshot post completing the register state stores;\n+        // if it is null it means the registration is not done and hence we should not overwrite the checkpoint\n+        if (oldOffsetSnapshot == null)", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2Mjc0Mg==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466062742", "bodyText": "nit: add { }", "author": "mjsax", "createdAt": "2020-08-05T23:39:53Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -38,13 +41,39 @@\n  */\n final class StateManagerUtil {\n     static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n+    static final long OFFSET_DELTA_THRESHOLD_FOR_CHECKPOINT = 10_000L;\n \n     private StateManagerUtil() {}\n \n     static RecordConverter converterForStore(final StateStore store) {\n         return isTimestamped(store) ? rawValueToTimestampedValue() : identity();\n     }\n \n+    static boolean checkpointNeeded(final boolean enforceCheckpoint,\n+                                    final Map<TopicPartition, Long> oldOffsetSnapshot,\n+                                    final Map<TopicPartition, Long> newOffsetSnapshot) {\n+        // we should always have the old snapshot post completing the register state stores;\n+        // if it is null it means the registration is not done and hence we should not overwrite the checkpoint\n+        if (oldOffsetSnapshot == null)\n+            return false;\n+\n+        // if the previous snapshot is empty while the current snapshot is not then we should always checkpoint;\n+        // note if the task is stateless or stateful but no stores logged, the snapshot would also be empty\n+        // and hence it's okay to not checkpoint\n+        if (oldOffsetSnapshot.isEmpty() && !newOffsetSnapshot.isEmpty())", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2NTIzOQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466065239", "bodyText": "This line just says what the code say itself. The question seem to be, \"why\" and this question is not answered in the comment.\nFrom my understanding, because we delete the checkpoint file when the task is initialized, we want to write the checkpoint file again as soon as possible (ie, on first commit) instead of waiting until we hit the OFFSET_DELTA_THRESHOLD_FOR_CHECKPOINT to keep the time frame in which we have consistent state but not checkpoint file low?\nThus, I am wondering if we should just not delete the checkpoint file when we init the task for non-eos instead? And just remove this additional condition?", "author": "mjsax", "createdAt": "2020-08-05T23:48:11Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -38,13 +41,39 @@\n  */\n final class StateManagerUtil {\n     static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n+    static final long OFFSET_DELTA_THRESHOLD_FOR_CHECKPOINT = 10_000L;\n \n     private StateManagerUtil() {}\n \n     static RecordConverter converterForStore(final StateStore store) {\n         return isTimestamped(store) ? rawValueToTimestampedValue() : identity();\n     }\n \n+    static boolean checkpointNeeded(final boolean enforceCheckpoint,\n+                                    final Map<TopicPartition, Long> oldOffsetSnapshot,\n+                                    final Map<TopicPartition, Long> newOffsetSnapshot) {\n+        // we should always have the old snapshot post completing the register state stores;\n+        // if it is null it means the registration is not done and hence we should not overwrite the checkpoint\n+        if (oldOffsetSnapshot == null)\n+            return false;\n+\n+        // if the previous snapshot is empty while the current snapshot is not then we should always checkpoint;", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2NjE5OA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466066198", "bodyText": "It seems like almost every method might throw StreamsException and/or TaskMigratedExcetpion -- is it really worth to have those comments all over the place?", "author": "mjsax", "createdAt": "2020-08-05T23:51:27Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -344,40 +343,41 @@ public void resume() {\n     }\n \n     /**\n+     * @throws StreamsException fatal error that should cause the thread to die\n+     * @throws TaskMigratedException recoverable error that would cause the task to be removed", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4NTE1Mw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466085153", "bodyText": "I think it is a good habit keeping that since those functions are called by various callers and if the caller do not catch them, the the ancestor callers should -- I've personally found such debugging very helpful.", "author": "guozhangwang", "createdAt": "2020-08-06T01:00:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2NjE5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcxNzYwOA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466717608", "bodyText": "Works for me -- I am just \"worried\" that those comments might get out-of-sync with the code at some point leading to bug, as we rely on the comments when changing the caller code...", "author": "mjsax", "createdAt": "2020-08-06T22:23:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2NjE5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2NjY4MQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466066681", "bodyText": "Why do we remove this? It might be personal preference, but the old code was \"cleaner\" IMHO? (Feel free to ignore  this comment.)", "author": "mjsax", "createdAt": "2020-08-05T23:53:19Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -344,40 +343,41 @@ public void resume() {\n     }\n \n     /**\n+     * @throws StreamsException fatal error that should cause the thread to die\n+     * @throws TaskMigratedException recoverable error that would cause the task to be removed\n      * @return offsets that should be committed for this task\n      */\n     @Override\n     public Map<TopicPartition, OffsetAndMetadata> prepareCommit() {\n-        final Map<TopicPartition, OffsetAndMetadata> offsetsToCommit;", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2Njg3MQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466066871", "bodyText": "nit: Why this? We know that we are in state created ?", "author": "mjsax", "createdAt": "2020-08-05T23:53:57Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -428,53 +428,28 @@ public void resume() {\n         return committableOffsets;\n     }\n \n-    /**\n-     * This should only be called if the attempted commit succeeded for this task\n-     */\n     @Override\n-    public void postCommit() {\n-        commitRequested = false;\n-\n+    public void postCommit(final boolean enforceCheckpoint) {\n         switch (state()) {\n             case CREATED:\n                 // We should never write a checkpoint for a CREATED task as we may overwrite an existing checkpoint\n                 // with empty uninitialized offsets\n-                log.debug(\"Skipped writing checkpoint for created task\");\n+                log.debug(\"Skipped writing checkpoint for {} task\", state());", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA5OTkwOA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466099908", "bodyText": "I know, but I just want to make the text consistent (in other places it would use state() which is capitalized..) really nit thing.", "author": "guozhangwang", "createdAt": "2020-08-06T01:55:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2Njg3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2Nzc3Ng==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466067776", "bodyText": "Nit: Can we keep SUSPENDED after RUNNING case? We use the same order in all methods and always follow the \"natural\" state transition order, that is CREATE, RESTORING, RUNNING, SUSPENDED, CLOSED.", "author": "mjsax", "createdAt": "2020-08-05T23:56:56Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -428,53 +428,28 @@ public void resume() {\n         return committableOffsets;\n     }\n \n-    /**\n-     * This should only be called if the attempted commit succeeded for this task\n-     */\n     @Override\n-    public void postCommit() {\n-        commitRequested = false;\n-\n+    public void postCommit(final boolean enforceCheckpoint) {\n         switch (state()) {\n             case CREATED:\n                 // We should never write a checkpoint for a CREATED task as we may overwrite an existing checkpoint\n                 // with empty uninitialized offsets\n-                log.debug(\"Skipped writing checkpoint for created task\");\n+                log.debug(\"Skipped writing checkpoint for {} task\", state());\n \n                 break;\n \n             case RESTORING:\n-                writeCheckpoint();\n-                log.debug(\"Finalized commit for restoring task\");\n+            case SUSPENDED:", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEwMzI3Nw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466103277", "bodyText": "Seems IDEA liked it the other way (it auto merges these two cases).", "author": "guozhangwang", "createdAt": "2020-08-06T02:08:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2Nzc3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2OTIwNQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466069205", "bodyText": "Why do we maintain List instead of Set ? It seem more natural to me to have a Set (ie, should we instead switch tasksToRecyle to use a Set, too)?", "author": "mjsax", "createdAt": "2020-08-06T00:01:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -248,12 +248,11 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n         );\n \n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n-\n         final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n         final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final List<Task> tasksToClose = new LinkedList<>();\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-        final Set<Task> dirtyTasks = new HashSet<>();\n+        final List<Task> tasksToRecycle = new LinkedList<>();\n+        final List<Task> tasksToCloseClean = new LinkedList<>();\n+        final List<Task> tasksToCloseDirty = new LinkedList<>();", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEwMzU2Mg==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466103562", "bodyText": "Two reasons: 1) in unit tests we are validating some scenarios with ordering assumptions, 2) I'd like to have log4j entries to have ordering as well.", "author": "guozhangwang", "createdAt": "2020-08-06T02:09:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2OTIwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEwOTU0NQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466109545", "bodyText": "Can we use an ordered set then? Just to make sure we can't end up with a task appearing more than once in the same list/set", "author": "ableegoldman", "createdAt": "2020-08-06T02:31:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2OTIwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjExMzg5OA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466113898", "bodyText": "Sounds good, I can try that.", "author": "guozhangwang", "createdAt": "2020-08-06T02:48:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA2OTIwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3MDEzMw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466070133", "bodyText": "Seems like double logging? We have a log.error each time before taskCloseExceptions.put() is called in handleCloseAndRecycle", "author": "mjsax", "createdAt": "2020-08-06T00:05:09Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -267,80 +266,19 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 // check for tasks that were owned previously but have changed active/standby status\n                 tasksToRecycle.add(task);\n             } else {\n-                tasksToClose.add(task);\n-            }\n-        }\n-\n-        for (final Task task : tasksToClose) {\n-            try {\n-                if (task.isActive()) {\n-                    // Active tasks are revoked and suspended/committed during #handleRevocation\n-                    if (!task.state().equals(State.SUSPENDED)) {\n-                        log.error(\"Active task {} should be suspended prior to attempting to close but was in {}\",\n-                                  task.id(), task.state());\n-                        throw new IllegalStateException(\"Active task \" + task.id() + \" should have been suspended\");\n-                    }\n-                } else {\n-                    task.suspend();\n-                    task.prepareCommit();\n-                    task.postCommit();\n-                }\n-                completeTaskCloseClean(task);\n-                cleanUpTaskProducer(task, taskCloseExceptions);\n-                tasks.remove(task.id());\n-            } catch (final RuntimeException e) {\n-                final String uncleanMessage = String.format(\n-                    \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n-                    task.id());\n-                log.error(uncleanMessage, e);\n-                taskCloseExceptions.put(task.id(), e);\n-                // We've already recorded the exception (which is the point of clean).\n-                // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                dirtyTasks.add(task);\n-            }\n-        }\n-\n-        for (final Task oldTask : tasksToRecycle) {\n-            final Task newTask;\n-            try {\n-                if (oldTask.isActive()) {\n-                    if (!oldTask.state().equals(State.SUSPENDED)) {\n-                        // Active tasks are revoked and suspended/committed during #handleRevocation\n-                        log.error(\"Active task {} should be suspended prior to attempting to close but was in {}\",\n-                                  oldTask.id(), oldTask.state());\n-                        throw new IllegalStateException(\"Active task \" + oldTask.id() + \" should have been suspended\");\n-                    }\n-                    final Set<TopicPartition> partitions = standbyTasksToCreate.remove(oldTask.id());\n-                    newTask = standbyTaskCreator.createStandbyTaskFromActive((StreamTask) oldTask, partitions);\n-                    cleanUpTaskProducer(oldTask, taskCloseExceptions);\n-                } else {\n-                    oldTask.suspend();\n-                    oldTask.prepareCommit();\n-                    oldTask.postCommit();\n-                    final Set<TopicPartition> partitions = activeTasksToCreate.remove(oldTask.id());\n-                    newTask = activeTaskCreator.createActiveTaskFromStandby((StandbyTask) oldTask, partitions, mainConsumer);\n-                }\n-                tasks.remove(oldTask.id());\n-                addNewTask(newTask);\n-            } catch (final RuntimeException e) {\n-                final String uncleanMessage = String.format(\"Failed to recycle task %s cleanly. Attempting to close remaining tasks before re-throwing:\", oldTask.id());\n-                log.error(uncleanMessage, e);\n-                taskCloseExceptions.put(oldTask.id(), e);\n-                dirtyTasks.add(oldTask);\n+                tasksToCloseClean.add(task);\n             }\n         }\n \n-        for (final Task task : dirtyTasks) {\n-            closeTaskDirty(task);\n-            cleanUpTaskProducer(task, taskCloseExceptions);\n-            tasks.remove(task.id());\n-        }\n+        // close and recycle those tasks\n+        handleCloseAndRecycle(tasksToRecycle, tasksToCloseClean, tasksToCloseDirty, activeTasksToCreate, standbyTasksToCreate, taskCloseExceptions);\n \n         if (!taskCloseExceptions.isEmpty()) {\n+            log.error(\"Hit exceptions while closing / recycling tasks: {}\", taskCloseExceptions);", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEwNTQ5Ng==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466105496", "bodyText": "Sounds good.", "author": "guozhangwang", "createdAt": "2020-08-06T02:16:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3MDEzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3MDQxOQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466070419", "bodyText": "should it be putIfAbsent (cf cleanUpTaskProducer())", "author": "mjsax", "createdAt": "2020-08-06T00:06:10Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,7 +306,102 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 addNewTask(task);\n             }\n         }\n+    }\n+\n+    private void handleCloseAndRecycle(final List<Task> tasksToRecycle,\n+                                       final List<Task> tasksToCloseClean,\n+                                       final List<Task> tasksToCloseDirty,\n+                                       final Map<TaskId, Set<TopicPartition>> activeTasksToCreate,\n+                                       final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate,\n+                                       final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions) {\n+        if (!tasksToCloseDirty.isEmpty()) {\n+            throw new IllegalArgumentException(\"Tasks to close-dirty should be empty\");\n+        }\n+\n+        // for all tasks to close or recycle, we should first right a checkpoint as in post-commit\n+        final List<Task> tasksToCheckpoint = new ArrayList<>(tasksToCloseClean);\n+        tasksToCheckpoint.addAll(tasksToRecycle);\n+        for (final Task task : tasksToCheckpoint) {\n+            try {\n+                // Note that we are not actually committing here but just check if we need to write checkpoint file:\n+                // 1) for active tasks prepareCommit should return empty if it has committed during suspension successfully,\n+                //    and their changelog positions should not change at all postCommit would not write the checkpoint again.\n+                // 2) for standby tasks prepareCommit should always return empty, and then in postCommit we would probably\n+                //    write the checkpoint file.\n+                final Map<TopicPartition, OffsetAndMetadata> offsets = task.prepareCommit();\n+                if (!offsets.isEmpty()) {\n+                    log.error(\"Task {} should has been committed when it was suspended, but it reports non-empty \" +\n+                                    \"offsets {} to commit; it means it fails during last commit and hence should be closed dirty\",\n+                            task.id(), offsets);\n+\n+                    tasksToCloseDirty.add(task);\n+                } else if (!task.isActive()) {\n+                    // For standby tasks, always try to first suspend before committing (checkpointing) it;\n+                    // Since standby tasks do not actually need to commit offsets but only need to\n+                    // flush / checkpoint state stores, so we only need to call postCommit here.\n+                    task.suspend();\n+\n+                    task.postCommit(true);\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\n+                        \"Failed to checkpoint task %s. Attempting to close remaining tasks before re-throwing:\",\n+                        task.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(task.id(), e);\n+                // We've already recorded the exception (which is the point of clean).\n+                // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n+                tasksToCloseDirty.add(task);\n+            }\n+        }\n \n+        for (final Task task : tasksToCloseClean) {\n+            try {\n+                if (!tasksToCloseDirty.contains(task)) {\n+                    completeTaskCloseClean(task);\n+                    cleanUpTaskProducer(task, taskCloseExceptions);\n+                    tasks.remove(task.id());\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\n+                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n+                        task.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(task.id(), e);\n+                tasksToCloseDirty.add(task);\n+            }\n+        }\n+\n+        for (final Task task : tasksToRecycle) {\n+            final Task newTask;\n+            try {\n+                if (!tasksToCloseDirty.contains(task)) {\n+                    if (task.isActive()) {\n+                        final Set<TopicPartition> partitions = standbyTasksToCreate.remove(task.id());\n+                        newTask = standbyTaskCreator.createStandbyTaskFromActive((StreamTask) task, partitions);\n+                        cleanUpTaskProducer(task, taskCloseExceptions);\n+                    } else {\n+                        final Set<TopicPartition> partitions = activeTasksToCreate.remove(task.id());\n+                        newTask = activeTaskCreator.createActiveTaskFromStandby((StandbyTask) task, partitions, mainConsumer);\n+                    }\n+                    tasks.remove(task.id());\n+                    addNewTask(newTask);\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\"Failed to recycle task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(task.id(), e);", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3MDUwMw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466070503", "bodyText": "should it be putIfAbsent (cf cleanUpTaskProducer())", "author": "mjsax", "createdAt": "2020-08-06T00:06:32Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,7 +306,102 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 addNewTask(task);\n             }\n         }\n+    }\n+\n+    private void handleCloseAndRecycle(final List<Task> tasksToRecycle,\n+                                       final List<Task> tasksToCloseClean,\n+                                       final List<Task> tasksToCloseDirty,\n+                                       final Map<TaskId, Set<TopicPartition>> activeTasksToCreate,\n+                                       final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate,\n+                                       final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions) {\n+        if (!tasksToCloseDirty.isEmpty()) {\n+            throw new IllegalArgumentException(\"Tasks to close-dirty should be empty\");\n+        }\n+\n+        // for all tasks to close or recycle, we should first right a checkpoint as in post-commit\n+        final List<Task> tasksToCheckpoint = new ArrayList<>(tasksToCloseClean);\n+        tasksToCheckpoint.addAll(tasksToRecycle);\n+        for (final Task task : tasksToCheckpoint) {\n+            try {\n+                // Note that we are not actually committing here but just check if we need to write checkpoint file:\n+                // 1) for active tasks prepareCommit should return empty if it has committed during suspension successfully,\n+                //    and their changelog positions should not change at all postCommit would not write the checkpoint again.\n+                // 2) for standby tasks prepareCommit should always return empty, and then in postCommit we would probably\n+                //    write the checkpoint file.\n+                final Map<TopicPartition, OffsetAndMetadata> offsets = task.prepareCommit();\n+                if (!offsets.isEmpty()) {\n+                    log.error(\"Task {} should has been committed when it was suspended, but it reports non-empty \" +\n+                                    \"offsets {} to commit; it means it fails during last commit and hence should be closed dirty\",\n+                            task.id(), offsets);\n+\n+                    tasksToCloseDirty.add(task);\n+                } else if (!task.isActive()) {\n+                    // For standby tasks, always try to first suspend before committing (checkpointing) it;\n+                    // Since standby tasks do not actually need to commit offsets but only need to\n+                    // flush / checkpoint state stores, so we only need to call postCommit here.\n+                    task.suspend();\n+\n+                    task.postCommit(true);\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\n+                        \"Failed to checkpoint task %s. Attempting to close remaining tasks before re-throwing:\",\n+                        task.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(task.id(), e);\n+                // We've already recorded the exception (which is the point of clean).\n+                // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n+                tasksToCloseDirty.add(task);\n+            }\n+        }\n \n+        for (final Task task : tasksToCloseClean) {\n+            try {\n+                if (!tasksToCloseDirty.contains(task)) {\n+                    completeTaskCloseClean(task);\n+                    cleanUpTaskProducer(task, taskCloseExceptions);\n+                    tasks.remove(task.id());\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\n+                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n+                        task.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(task.id(), e);", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3MDU0Mw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466070543", "bodyText": "should it be putIfAbsent (cf cleanUpTaskProducer())", "author": "mjsax", "createdAt": "2020-08-06T00:06:40Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,7 +306,102 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 addNewTask(task);\n             }\n         }\n+    }\n+\n+    private void handleCloseAndRecycle(final List<Task> tasksToRecycle,\n+                                       final List<Task> tasksToCloseClean,\n+                                       final List<Task> tasksToCloseDirty,\n+                                       final Map<TaskId, Set<TopicPartition>> activeTasksToCreate,\n+                                       final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate,\n+                                       final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions) {\n+        if (!tasksToCloseDirty.isEmpty()) {\n+            throw new IllegalArgumentException(\"Tasks to close-dirty should be empty\");\n+        }\n+\n+        // for all tasks to close or recycle, we should first right a checkpoint as in post-commit\n+        final List<Task> tasksToCheckpoint = new ArrayList<>(tasksToCloseClean);\n+        tasksToCheckpoint.addAll(tasksToRecycle);\n+        for (final Task task : tasksToCheckpoint) {\n+            try {\n+                // Note that we are not actually committing here but just check if we need to write checkpoint file:\n+                // 1) for active tasks prepareCommit should return empty if it has committed during suspension successfully,\n+                //    and their changelog positions should not change at all postCommit would not write the checkpoint again.\n+                // 2) for standby tasks prepareCommit should always return empty, and then in postCommit we would probably\n+                //    write the checkpoint file.\n+                final Map<TopicPartition, OffsetAndMetadata> offsets = task.prepareCommit();\n+                if (!offsets.isEmpty()) {\n+                    log.error(\"Task {} should has been committed when it was suspended, but it reports non-empty \" +\n+                                    \"offsets {} to commit; it means it fails during last commit and hence should be closed dirty\",\n+                            task.id(), offsets);\n+\n+                    tasksToCloseDirty.add(task);\n+                } else if (!task.isActive()) {\n+                    // For standby tasks, always try to first suspend before committing (checkpointing) it;\n+                    // Since standby tasks do not actually need to commit offsets but only need to\n+                    // flush / checkpoint state stores, so we only need to call postCommit here.\n+                    task.suspend();\n+\n+                    task.postCommit(true);\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\n+                        \"Failed to checkpoint task %s. Attempting to close remaining tasks before re-throwing:\",\n+                        task.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(task.id(), e);", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3MjU1OQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466072559", "bodyText": "This seems to be a \"hack\" -- IMHO, as task should be only in either one set/list, but never in both... Can we change the first loop to use an explicit iterator and remove a task from tasksToCloseClean when we add it to tasksToCloseDirty", "author": "mjsax", "createdAt": "2020-08-06T00:14:25Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,7 +306,102 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 addNewTask(task);\n             }\n         }\n+    }\n+\n+    private void handleCloseAndRecycle(final List<Task> tasksToRecycle,\n+                                       final List<Task> tasksToCloseClean,\n+                                       final List<Task> tasksToCloseDirty,\n+                                       final Map<TaskId, Set<TopicPartition>> activeTasksToCreate,\n+                                       final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate,\n+                                       final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions) {\n+        if (!tasksToCloseDirty.isEmpty()) {\n+            throw new IllegalArgumentException(\"Tasks to close-dirty should be empty\");\n+        }\n+\n+        // for all tasks to close or recycle, we should first right a checkpoint as in post-commit\n+        final List<Task> tasksToCheckpoint = new ArrayList<>(tasksToCloseClean);\n+        tasksToCheckpoint.addAll(tasksToRecycle);\n+        for (final Task task : tasksToCheckpoint) {\n+            try {\n+                // Note that we are not actually committing here but just check if we need to write checkpoint file:\n+                // 1) for active tasks prepareCommit should return empty if it has committed during suspension successfully,\n+                //    and their changelog positions should not change at all postCommit would not write the checkpoint again.\n+                // 2) for standby tasks prepareCommit should always return empty, and then in postCommit we would probably\n+                //    write the checkpoint file.\n+                final Map<TopicPartition, OffsetAndMetadata> offsets = task.prepareCommit();\n+                if (!offsets.isEmpty()) {\n+                    log.error(\"Task {} should has been committed when it was suspended, but it reports non-empty \" +\n+                                    \"offsets {} to commit; it means it fails during last commit and hence should be closed dirty\",\n+                            task.id(), offsets);\n+\n+                    tasksToCloseDirty.add(task);\n+                } else if (!task.isActive()) {\n+                    // For standby tasks, always try to first suspend before committing (checkpointing) it;\n+                    // Since standby tasks do not actually need to commit offsets but only need to\n+                    // flush / checkpoint state stores, so we only need to call postCommit here.\n+                    task.suspend();\n+\n+                    task.postCommit(true);\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\n+                        \"Failed to checkpoint task %s. Attempting to close remaining tasks before re-throwing:\",\n+                        task.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(task.id(), e);\n+                // We've already recorded the exception (which is the point of clean).\n+                // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n+                tasksToCloseDirty.add(task);\n+            }\n+        }\n \n+        for (final Task task : tasksToCloseClean) {\n+            try {\n+                if (!tasksToCloseDirty.contains(task)) {", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEwNzI4MA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466107280", "bodyText": "I've thought about that, but the tricky thing is that I'm iterating the union of two lists. I can, remove the tasksToCloseDirty from the other two after the iteration to make it cleaner.", "author": "guozhangwang", "createdAt": "2020-08-06T02:23:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3MjU1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3MzEzNQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466073135", "bodyText": "While I usually prefer checks like this, it seems unnecessary here? (Feel free to ignore.)", "author": "mjsax", "createdAt": "2020-08-06T00:16:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,7 +306,102 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 addNewTask(task);\n             }\n         }\n+    }\n+\n+    private void handleCloseAndRecycle(final List<Task> tasksToRecycle,\n+                                       final List<Task> tasksToCloseClean,\n+                                       final List<Task> tasksToCloseDirty,\n+                                       final Map<TaskId, Set<TopicPartition>> activeTasksToCreate,\n+                                       final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate,\n+                                       final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions) {\n+        if (!tasksToCloseDirty.isEmpty()) {\n+            throw new IllegalArgumentException(\"Tasks to close-dirty should be empty\");\n+        }", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3MzQ0Nw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466073447", "bodyText": "We should remove task from taskToCloseClean/taskToRecycle here to maintain the invariant that a task is only in either one of the three sets/list but never in multiple at the same time. (cf. my comment below)\nI understand that you want to share this loop for \"cleanClose\" and \"recycle\" we can extract it into its own method to achieve this.", "author": "mjsax", "createdAt": "2020-08-06T00:17:27Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,7 +306,102 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 addNewTask(task);\n             }\n         }\n+    }\n+\n+    private void handleCloseAndRecycle(final List<Task> tasksToRecycle,\n+                                       final List<Task> tasksToCloseClean,\n+                                       final List<Task> tasksToCloseDirty,\n+                                       final Map<TaskId, Set<TopicPartition>> activeTasksToCreate,\n+                                       final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate,\n+                                       final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions) {\n+        if (!tasksToCloseDirty.isEmpty()) {\n+            throw new IllegalArgumentException(\"Tasks to close-dirty should be empty\");\n+        }\n+\n+        // for all tasks to close or recycle, we should first right a checkpoint as in post-commit\n+        final List<Task> tasksToCheckpoint = new ArrayList<>(tasksToCloseClean);\n+        tasksToCheckpoint.addAll(tasksToRecycle);\n+        for (final Task task : tasksToCheckpoint) {\n+            try {\n+                // Note that we are not actually committing here but just check if we need to write checkpoint file:\n+                // 1) for active tasks prepareCommit should return empty if it has committed during suspension successfully,\n+                //    and their changelog positions should not change at all postCommit would not write the checkpoint again.\n+                // 2) for standby tasks prepareCommit should always return empty, and then in postCommit we would probably\n+                //    write the checkpoint file.\n+                final Map<TopicPartition, OffsetAndMetadata> offsets = task.prepareCommit();\n+                if (!offsets.isEmpty()) {\n+                    log.error(\"Task {} should has been committed when it was suspended, but it reports non-empty \" +\n+                                    \"offsets {} to commit; it means it fails during last commit and hence should be closed dirty\",\n+                            task.id(), offsets);\n+\n+                    tasksToCloseDirty.add(task);", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3MzQ4MQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466073481", "bodyText": "as above.", "author": "mjsax", "createdAt": "2020-08-06T00:17:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,7 +306,102 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 addNewTask(task);\n             }\n         }\n+    }\n+\n+    private void handleCloseAndRecycle(final List<Task> tasksToRecycle,\n+                                       final List<Task> tasksToCloseClean,\n+                                       final List<Task> tasksToCloseDirty,\n+                                       final Map<TaskId, Set<TopicPartition>> activeTasksToCreate,\n+                                       final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate,\n+                                       final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions) {\n+        if (!tasksToCloseDirty.isEmpty()) {\n+            throw new IllegalArgumentException(\"Tasks to close-dirty should be empty\");\n+        }\n+\n+        // for all tasks to close or recycle, we should first right a checkpoint as in post-commit\n+        final List<Task> tasksToCheckpoint = new ArrayList<>(tasksToCloseClean);\n+        tasksToCheckpoint.addAll(tasksToRecycle);\n+        for (final Task task : tasksToCheckpoint) {\n+            try {\n+                // Note that we are not actually committing here but just check if we need to write checkpoint file:\n+                // 1) for active tasks prepareCommit should return empty if it has committed during suspension successfully,\n+                //    and their changelog positions should not change at all postCommit would not write the checkpoint again.\n+                // 2) for standby tasks prepareCommit should always return empty, and then in postCommit we would probably\n+                //    write the checkpoint file.\n+                final Map<TopicPartition, OffsetAndMetadata> offsets = task.prepareCommit();\n+                if (!offsets.isEmpty()) {\n+                    log.error(\"Task {} should has been committed when it was suspended, but it reports non-empty \" +\n+                                    \"offsets {} to commit; it means it fails during last commit and hence should be closed dirty\",\n+                            task.id(), offsets);\n+\n+                    tasksToCloseDirty.add(task);\n+                } else if (!task.isActive()) {\n+                    // For standby tasks, always try to first suspend before committing (checkpointing) it;\n+                    // Since standby tasks do not actually need to commit offsets but only need to\n+                    // flush / checkpoint state stores, so we only need to call postCommit here.\n+                    task.suspend();\n+\n+                    task.postCommit(true);\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\n+                        \"Failed to checkpoint task %s. Attempting to close remaining tasks before re-throwing:\",\n+                        task.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(task.id(), e);\n+                // We've already recorded the exception (which is the point of clean).\n+                // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n+                tasksToCloseDirty.add(task);", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3MzUxMA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466073510", "bodyText": "as above", "author": "mjsax", "createdAt": "2020-08-06T00:17:44Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,7 +306,102 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 addNewTask(task);\n             }\n         }\n+    }\n+\n+    private void handleCloseAndRecycle(final List<Task> tasksToRecycle,\n+                                       final List<Task> tasksToCloseClean,\n+                                       final List<Task> tasksToCloseDirty,\n+                                       final Map<TaskId, Set<TopicPartition>> activeTasksToCreate,\n+                                       final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate,\n+                                       final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions) {\n+        if (!tasksToCloseDirty.isEmpty()) {\n+            throw new IllegalArgumentException(\"Tasks to close-dirty should be empty\");\n+        }\n+\n+        // for all tasks to close or recycle, we should first right a checkpoint as in post-commit\n+        final List<Task> tasksToCheckpoint = new ArrayList<>(tasksToCloseClean);\n+        tasksToCheckpoint.addAll(tasksToRecycle);\n+        for (final Task task : tasksToCheckpoint) {\n+            try {\n+                // Note that we are not actually committing here but just check if we need to write checkpoint file:\n+                // 1) for active tasks prepareCommit should return empty if it has committed during suspension successfully,\n+                //    and their changelog positions should not change at all postCommit would not write the checkpoint again.\n+                // 2) for standby tasks prepareCommit should always return empty, and then in postCommit we would probably\n+                //    write the checkpoint file.\n+                final Map<TopicPartition, OffsetAndMetadata> offsets = task.prepareCommit();\n+                if (!offsets.isEmpty()) {\n+                    log.error(\"Task {} should has been committed when it was suspended, but it reports non-empty \" +\n+                                    \"offsets {} to commit; it means it fails during last commit and hence should be closed dirty\",\n+                            task.id(), offsets);\n+\n+                    tasksToCloseDirty.add(task);\n+                } else if (!task.isActive()) {\n+                    // For standby tasks, always try to first suspend before committing (checkpointing) it;\n+                    // Since standby tasks do not actually need to commit offsets but only need to\n+                    // flush / checkpoint state stores, so we only need to call postCommit here.\n+                    task.suspend();\n+\n+                    task.postCommit(true);\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\n+                        \"Failed to checkpoint task %s. Attempting to close remaining tasks before re-throwing:\",\n+                        task.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(task.id(), e);\n+                // We've already recorded the exception (which is the point of clean).\n+                // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n+                tasksToCloseDirty.add(task);\n+            }\n+        }\n \n+        for (final Task task : tasksToCloseClean) {\n+            try {\n+                if (!tasksToCloseDirty.contains(task)) {\n+                    completeTaskCloseClean(task);\n+                    cleanUpTaskProducer(task, taskCloseExceptions);\n+                    tasks.remove(task.id());\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\n+                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n+                        task.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(task.id(), e);\n+                tasksToCloseDirty.add(task);", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3MzU3MQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466073571", "bodyText": "as above.", "author": "mjsax", "createdAt": "2020-08-06T00:17:56Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,7 +306,102 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 addNewTask(task);\n             }\n         }\n+    }\n+\n+    private void handleCloseAndRecycle(final List<Task> tasksToRecycle,\n+                                       final List<Task> tasksToCloseClean,\n+                                       final List<Task> tasksToCloseDirty,\n+                                       final Map<TaskId, Set<TopicPartition>> activeTasksToCreate,\n+                                       final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate,\n+                                       final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions) {\n+        if (!tasksToCloseDirty.isEmpty()) {\n+            throw new IllegalArgumentException(\"Tasks to close-dirty should be empty\");\n+        }\n+\n+        // for all tasks to close or recycle, we should first right a checkpoint as in post-commit\n+        final List<Task> tasksToCheckpoint = new ArrayList<>(tasksToCloseClean);\n+        tasksToCheckpoint.addAll(tasksToRecycle);\n+        for (final Task task : tasksToCheckpoint) {\n+            try {\n+                // Note that we are not actually committing here but just check if we need to write checkpoint file:\n+                // 1) for active tasks prepareCommit should return empty if it has committed during suspension successfully,\n+                //    and their changelog positions should not change at all postCommit would not write the checkpoint again.\n+                // 2) for standby tasks prepareCommit should always return empty, and then in postCommit we would probably\n+                //    write the checkpoint file.\n+                final Map<TopicPartition, OffsetAndMetadata> offsets = task.prepareCommit();\n+                if (!offsets.isEmpty()) {\n+                    log.error(\"Task {} should has been committed when it was suspended, but it reports non-empty \" +\n+                                    \"offsets {} to commit; it means it fails during last commit and hence should be closed dirty\",\n+                            task.id(), offsets);\n+\n+                    tasksToCloseDirty.add(task);\n+                } else if (!task.isActive()) {\n+                    // For standby tasks, always try to first suspend before committing (checkpointing) it;\n+                    // Since standby tasks do not actually need to commit offsets but only need to\n+                    // flush / checkpoint state stores, so we only need to call postCommit here.\n+                    task.suspend();\n+\n+                    task.postCommit(true);\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\n+                        \"Failed to checkpoint task %s. Attempting to close remaining tasks before re-throwing:\",\n+                        task.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(task.id(), e);\n+                // We've already recorded the exception (which is the point of clean).\n+                // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n+                tasksToCloseDirty.add(task);\n+            }\n+        }\n \n+        for (final Task task : tasksToCloseClean) {\n+            try {\n+                if (!tasksToCloseDirty.contains(task)) {\n+                    completeTaskCloseClean(task);\n+                    cleanUpTaskProducer(task, taskCloseExceptions);\n+                    tasks.remove(task.id());\n+                }\n+            } catch (final RuntimeException e) {\n+                final String uncleanMessage = String.format(\n+                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n+                        task.id());\n+                log.error(uncleanMessage, e);\n+                taskCloseExceptions.put(task.id(), e);\n+                tasksToCloseDirty.add(task);\n+            }\n+        }\n+\n+        for (final Task task : tasksToRecycle) {\n+            final Task newTask;\n+            try {\n+                if (!tasksToCloseDirty.contains(task)) {", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3NTg5MQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466075891", "bodyText": "Why this renaming? -- the set does not contain \"all\" non-revoked active tasks -- only those, that need committing?", "author": "mjsax", "createdAt": "2020-08-06T00:25:59Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -479,24 +512,20 @@ boolean tryToCompleteRestoration() {\n     void handleRevocation(final Collection<TopicPartition> revokedPartitions) {\n         final Set<TopicPartition> remainingRevokedPartitions = new HashSet<>(revokedPartitions);\n \n-        final Set<Task> revokedTasks = new HashSet<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-\n+        final Set<Task> revokedActiveTasks = new HashSet<>();\n+        final Set<Task> nonRevokedActiveTasks = new HashSet<>();\n+        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsPerTask = new HashMap<>();\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n         for (final Task task : activeTaskIterable()) {\n             if (remainingRevokedPartitions.containsAll(task.inputPartitions())) {\n-                try {\n-                    task.suspend();\n-                    revokedTasks.add(task);\n-                } catch (final RuntimeException e) {\n-                    log.error(\"Caught the following exception while trying to suspend revoked task \" + task.id(), e);\n-                    firstException.compareAndSet(null, new StreamsException(\"Failed to suspend \" + task.id(), e));\n-                }\n+                // when the task input partitions are included in the revoked list,\n+                // this is an active task and should be revoked\n+                revokedActiveTasks.add(task);\n+                remainingRevokedPartitions.removeAll(task.inputPartitions());\n             } else if (task.commitNeeded()) {\n-                additionalTasksForCommitting.add(task);\n+                nonRevokedActiveTasks.add(task);", "originalCommit": "aa95e1367eb31660a156eb484e2d455ca35029a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEwODM0NA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r466108344", "bodyText": "Yeah I think I agree with you -- after a second thought I think this renaming is not very accurate. Will call it commitNeededActiveTasks.", "author": "guozhangwang", "createdAt": "2020-08-06T02:27:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3NTg5MQ=="}], "type": "inlineReview"}, {"oid": "db2c1e30adc6aed42879781184aacede6b88778b", "url": "https://github.com/apache/kafka/commit/db2c1e30adc6aed42879781184aacede6b88778b", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into K9450-decouple-flush-state-from-commit", "committedDate": "2020-08-06T00:45:36Z", "type": "commit"}, {"oid": "2503a74c5e025ad21835928d690c2fbc22bd3e69", "url": "https://github.com/apache/kafka/commit/2503a74c5e025ad21835928d690c2fbc22bd3e69", "message": "do not delete upon loading without eos", "committedDate": "2020-08-06T03:35:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI4OTY0MA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r467289640", "bodyText": "Seems the comment is outdated? we should initialize the snapshot as empty", "author": "mjsax", "createdAt": "2020-08-07T21:42:27Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java", "diffHunk": "@@ -49,6 +59,30 @@\n         this.stateDirectory = stateDirectory;\n     }\n \n+    protected void initializeCheckpoint() {\n+        // we will delete the local checkpoint file after registering the state stores and loading them into the\n+        // state manager, therefore we should initialize the snapshot as empty to indicate over-write checkpoint needed", "originalCommit": "2503a74c5e025ad21835928d690c2fbc22bd3e69", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc4MDQ1NA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r468780454", "bodyText": "ack.", "author": "guozhangwang", "createdAt": "2020-08-11T18:28:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI4OTY0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI5MTM0Ng==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r467291346", "bodyText": "In the old code, we actually get a copy of the Map, while within initializeCheckpoint(); don't -- is this on purpose? It it safe?\nAlso, do we actually need the method? The old code was just doing the exact some thing? It's just one-liner method -- what do we gain?", "author": "mjsax", "createdAt": "2020-08-07T21:44:44Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -93,9 +90,7 @@ public boolean isActive() {\n     public void initializeIfNeeded() {\n         if (state() == State.CREATED) {\n             StateManagerUtil.registerStateStores(log, logPrefix, topology, stateMgr, stateDirectory, processorContext);\n-\n-            // initialize the snapshot with the current offsets as we don't need to commit then until they change\n-            offsetSnapshotSinceLastCommit = new HashMap<>(stateMgr.changelogOffsets());\n+            initializeCheckpoint();", "originalCommit": "2503a74c5e025ad21835928d690c2fbc22bd3e69", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc4MTI2NA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r468781264", "bodyText": "Good point, will revert.", "author": "guozhangwang", "createdAt": "2020-08-11T18:30:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI5MTM0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI5NjMzNQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r467296335", "bodyText": "Should we add a test for EOS, that the checkpoint file is deleted?", "author": "mjsax", "createdAt": "2020-08-07T21:51:06Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java", "diffHunk": "@@ -412,7 +412,7 @@ public void shouldInitializeOffsetsFromCheckpointFile() throws IOException {\n             stateMgr.registerStore(nonPersistentStore, nonPersistentStore.stateRestoreCallback);\n             stateMgr.initializeStoreOffsetsFromCheckpoint(true);\n \n-            assertFalse(checkpointFile.exists());\n+            assertTrue(checkpointFile.exists());", "originalCommit": "2503a74c5e025ad21835928d690c2fbc22bd3e69", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI5NzM5NA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r467297394", "bodyText": "It's unclear to me, how we actually verify that the checkpointing happened? Above, we have\nstateManager.checkpoint();\nEasyMock.expectLastCall();\n\nbut it only help to verify that we checkpoint a single time, but not which of the three calls does the checkpointing?", "author": "mjsax", "createdAt": "2020-08-07T21:52:28Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java", "diffHunk": "@@ -207,15 +207,25 @@ public void shouldFlushAndCheckpointStateManagerOnCommit() {\n         EasyMock.expect(stateManager.changelogOffsets()).andStubReturn(Collections.emptyMap());\n         stateManager.flush();\n         EasyMock.expectLastCall();\n-        stateManager.checkpoint(EasyMock.eq(Collections.emptyMap()));\n-        EasyMock.expect(stateManager.changelogOffsets()).andReturn(Collections.singletonMap(partition, 50L));\n+        stateManager.checkpoint();\n+        EasyMock.expectLastCall();\n+        EasyMock.expect(stateManager.changelogOffsets())\n+                .andReturn(Collections.singletonMap(partition, 50L))\n+                .andReturn(Collections.singletonMap(partition, 11000L))\n+                .andReturn(Collections.singletonMap(partition, 11000L));\n         EasyMock.expect(stateManager.changelogPartitions()).andReturn(Collections.singleton(partition)).anyTimes();\n         EasyMock.replay(stateManager);\n \n         task = createStandbyTask();\n         task.initializeIfNeeded();\n         task.prepareCommit();\n-        task.postCommit();\n+        task.postCommit(false);  // this should not checkpoint", "originalCommit": "2503a74c5e025ad21835928d690c2fbc22bd3e69", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc5MzE3MA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r468793170", "bodyText": "Yeah I tried to mock the static checkpointNeeded but it seems not possible with easy-mock.", "author": "guozhangwang", "createdAt": "2020-08-11T18:47:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI5NzM5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM1OTY4OA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r467359688", "bodyText": "Should we verify stateManager, too?", "author": "mjsax", "createdAt": "2020-08-08T04:22:21Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -1259,13 +1262,47 @@ public void shouldReInitializeTopologyWhenResuming() throws IOException {\n     }\n \n     @Test\n-    public void shouldCheckpointOffsetsOnCommit() {\n+    public void shouldNotCheckpointOffsetsAgainOnCommitIfSnapshotNotChangedMuch() {\n         final Long offset = 543L;\n \n         EasyMock.expect(recordCollector.offsets()).andReturn(Collections.singletonMap(changelogPartition, offset)).anyTimes();\n-        stateManager.checkpoint(EasyMock.eq(Collections.singletonMap(changelogPartition, offset)));\n+        stateManager.checkpoint();\n+        EasyMock.expectLastCall().once();\n+        EasyMock.expect(stateManager.changelogPartitions()).andReturn(Collections.singleton(changelogPartition));\n+        EasyMock.expect(stateManager.changelogOffsets())\n+                .andReturn(Collections.singletonMap(changelogPartition, 0L))\n+                .andReturn(Collections.singletonMap(changelogPartition, 10L))\n+                .andReturn(Collections.singletonMap(changelogPartition, 20L));\n+        stateManager.registerStore(stateStore, stateStore.stateRestoreCallback);\n         EasyMock.expectLastCall();\n+        EasyMock.replay(stateManager, recordCollector);\n+\n+        task = createStatefulTask(createConfig(false, \"100\"), true);\n+\n+        task.initializeIfNeeded();\n+        task.completeRestoration();\n+\n+        task.prepareCommit();\n+        task.postCommit(true);\n+\n+        task.prepareCommit();\n+        task.postCommit(false);\n+\n+        EasyMock.verify(recordCollector);", "originalCommit": "2503a74c5e025ad21835928d690c2fbc22bd3e69", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc5NjE4Mw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r468796183", "bodyText": "ack", "author": "guozhangwang", "createdAt": "2020-08-11T18:53:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM1OTY4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM1OTc3Ng==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r467359776", "bodyText": "Why do we need to setup an exception? If we don't setup any expected call at all, it should fail automatically?", "author": "mjsax", "createdAt": "2020-08-08T04:23:40Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -1458,50 +1498,91 @@ public void shouldThrowIfPostCommittingOnIllegalState() {\n \n         task.transitionTo(Task.State.SUSPENDED);\n         task.transitionTo(Task.State.CLOSED);\n-        assertThrows(IllegalStateException.class, task::postCommit);\n+        assertThrows(IllegalStateException.class, () -> task.postCommit(true));\n     }\n \n     @Test\n     public void shouldSkipCheckpointingSuspendedCreatedTask() {\n-        stateManager.checkpoint(EasyMock.anyObject());\n+        stateManager.checkpoint();\n         EasyMock.expectLastCall().andThrow(new AssertionError(\"Should not have tried to checkpoint\"));\n         EasyMock.replay(stateManager);\n \n         task = createStatefulTask(createConfig(false, \"100\"), true);\n         task.suspend();\n-        task.postCommit();\n+        task.postCommit(true);\n     }\n \n     @Test\n-    public void shouldCheckpointWithEmptyOffsetsForSuspendedRestoringTask() {\n-        stateManager.checkpoint(emptyMap());\n+    public void shouldCheckpointForSuspendedTask() {\n+        stateManager.checkpoint();\n+        EasyMock.expectLastCall().once();\n+        EasyMock.expect(stateManager.changelogOffsets())\n+                .andReturn(Collections.singletonMap(partition1, 0L))\n+                .andReturn(Collections.singletonMap(partition1, 1L));\n         EasyMock.replay(stateManager);\n \n         task = createStatefulTask(createConfig(false, \"100\"), true);\n         task.initializeIfNeeded();\n         task.suspend();\n-        task.postCommit();\n+        task.postCommit(true);\n         EasyMock.verify(stateManager);\n     }\n \n     @Test\n-    public void shouldCheckpointWithEmptyOffsetsForSuspendedRunningTaskWithNoCommitNeeded() {\n-        stateManager.checkpoint(emptyMap());\n+    public void shouldNotCheckpointForSuspendedRunningTaskWithSmallProgress() {\n+        EasyMock.expect(stateManager.changelogOffsets())\n+                .andReturn(Collections.singletonMap(partition1, 1L))\n+                .andReturn(Collections.singletonMap(partition1, 2L))\n+                .andReturn(Collections.singletonMap(partition1, 3L));\n+        stateManager.checkpoint();\n+        EasyMock.expectLastCall().andThrow(new AssertionError(\"Checkpoint should not be called\")).anyTimes();", "originalCommit": "2503a74c5e025ad21835928d690c2fbc22bd3e69", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc5OTg0Nw==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r468799847", "bodyText": "Actually if I do not setup, it would not fail.", "author": "guozhangwang", "createdAt": "2020-08-11T19:00:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM1OTc3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM1OTgzOQ==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r467359839", "bodyText": "Similar to above: instead of throwing, it should be sufficient to just not register any expected calll?", "author": "mjsax", "createdAt": "2020-08-08T04:24:51Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -1560,42 +1641,13 @@ public void shouldCheckpointWithCreatedStateOnClose() {\n     }\n \n     @Test\n-    public void shouldNotCommitAndThrowOnCloseDirty() {\n-        EasyMock.expect(stateManager.changelogPartitions()).andReturn(Collections.emptySet()).anyTimes();\n-        stateManager.close();\n-        EasyMock.expectLastCall().andThrow(new ProcessorStateException(\"KABOOM!\")).anyTimes();\n-        stateManager.checkpoint(EasyMock.anyObject());\n-        EasyMock.expectLastCall().andThrow(new AssertionError(\"Checkpoint should not be called\")).anyTimes();\n-        EasyMock.expect(recordCollector.offsets()).andReturn(Collections.emptyMap()).anyTimes();\n-        EasyMock.replay(stateManager, recordCollector);\n-\n-        final MetricName metricName = setupCloseTaskMetric();\n-\n-        task = createOptimizedStatefulTask(createConfig(false, \"100\"), consumer);\n-\n-        task.initializeIfNeeded();\n-        task.completeRestoration();\n-\n-        task.suspend();\n-        task.closeDirty();\n-\n-        assertEquals(Task.State.CLOSED, task.state());\n-        assertTrue(source1.initialized);\n-        assertTrue(source1.closed);\n-\n-        final double expectedCloseTaskMetric = 1.0;\n-        verifyCloseTaskMetric(expectedCloseTaskMetric, streamsMetrics, metricName);\n-\n-        EasyMock.verify(stateManager);\n-    }\n-\n-    @Test\n-    public void shouldCheckpointOnCloseRestoring() {\n+    public void shouldNotCheckpointOnCloseRestoringIfNoProgress() {\n         stateManager.flush();\n-        EasyMock.expectLastCall();\n-        stateManager.checkpoint(EasyMock.eq(Collections.emptyMap()));\n-        EasyMock.expectLastCall();\n+        EasyMock.expectLastCall().andThrow(new AssertionError(\"Flush should not be called\")).anyTimes();\n+        stateManager.checkpoint();\n+        EasyMock.expectLastCall().andThrow(new AssertionError(\"Checkpoint should not be called\")).anyTimes();", "originalCommit": "2503a74c5e025ad21835928d690c2fbc22bd3e69", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM1OTkxMA==", "url": "https://github.com/apache/kafka/pull/8964#discussion_r467359910", "bodyText": "as above? (more below... won't add comments each time)", "author": "mjsax", "createdAt": "2020-08-08T04:25:08Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -1638,21 +1689,22 @@ public void shouldCheckpointOffsetsOnPostCommitIfCommitNeeded() {\n \n         task.suspend();\n         task.prepareCommit();\n-        task.postCommit();\n+        task.postCommit(false);\n \n         assertEquals(Task.State.SUSPENDED, task.state());\n \n         EasyMock.verify(stateManager);\n     }\n \n     @Test\n-    public void shouldSwallowExceptionOnCloseCleanError() {\n+    public void shouldThrowExceptionOnCloseCleanError() {\n         final long offset = 543L;\n \n         EasyMock.expect(recordCollector.offsets()).andReturn(emptyMap()).anyTimes();\n-        stateManager.checkpoint(EasyMock.eq(Collections.singletonMap(partition1, offset)));\n-        EasyMock.expectLastCall();\n+        stateManager.checkpoint();\n+        EasyMock.expectLastCall().andThrow(new AssertionError(\"Checkpoint should not be called\")).anyTimes();", "originalCommit": "2503a74c5e025ad21835928d690c2fbc22bd3e69", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c8a0379dc3c720ef06e7e64444b26ce2e1fee650", "url": "https://github.com/apache/kafka/commit/c8a0379dc3c720ef06e7e64444b26ce2e1fee650", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into K9450-decouple-flush-state-from-commit", "committedDate": "2020-08-11T18:21:03Z", "type": "commit"}, {"oid": "a4f2272986357de9c48032d1fd0d9b1482d5a974", "url": "https://github.com/apache/kafka/commit/a4f2272986357de9c48032d1fd0d9b1482d5a974", "message": "github comments", "committedDate": "2020-08-11T21:24:45Z", "type": "commit"}]}