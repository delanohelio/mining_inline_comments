{"pr_number": 8497, "pr_title": "KAFKA-6145: KIP-441 Build state constrained assignment from balanced one", "pr_createdAt": "2020-04-16T05:10:35Z", "pr_url": "https://github.com/apache/kafka/pull/8497", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDM5NjYyNQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410396625", "bodyText": "Do you feel like the source/destination definition has become a bit muddled, or does it still make sense to you? I guess it seems a bit weird to me because in some sense, the source is where we're putting the task now and the destination is where we plan to move it to later. But from another angle, since we now start with the balanced assignment, we are actually moving tasks from the destination to the source.  Does anyone else find that confusing or should I not worry so much", "author": "ableegoldman", "createdAt": "2020-04-17T18:25:14Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -16,30 +16,35 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n+import static org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor.taskIsCaughtUpOnClient;\n+\n+import java.util.Comparator;\n import java.util.List;\n import java.util.Map;\n import java.util.Objects;\n-import java.util.Set;\n import java.util.SortedSet;\n+import java.util.TreeSet;\n import java.util.UUID;\n import org.apache.kafka.streams.processor.TaskId;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n public class TaskMovement {\n-    private static final Logger log = LoggerFactory.getLogger(TaskMovement.class);\n+    private static final UUID UNKNOWN = null;\n \n     final TaskId task;\n-    final UUID source;\n-    final UUID destination;\n+    private UUID source;\n+    private final UUID destination;\n \n-    TaskMovement(final TaskId task, final UUID source, final UUID destination) {\n+    TaskMovement(final TaskId task, final UUID destination) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQyMDkwMQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410420901", "bodyText": "I think the source/destination definition is still sensible (actually still the same), the source is where the task is now, and the destination is where we want it to be.\nHowever, now that the source field is mutable, the hashCode/equals implementations are unsafe. Really, the specific thing that's unsafe is that hashCode cannot be defined in terms of mutable fields, but you only need hashCode if you put the object in a hashed collection. So, if we don't need to use this in sets or as map keys, maybe we can delete the hashCode method and leave equals in place (I assume it's used in testing).\nOr, you can preserve the immutability of this class and do the initial bookkeeping in \"assignMovements\" differently.", "author": "vvcephei", "createdAt": "2020-04-17T19:15:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDM5NjYyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUwMTg1Mg==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410501852", "bodyText": "We don't need the hashCode/equals implementation anymore, as TaskMovement is only ever used in a SortedSet with a custom (non-TaskMovement based) Comparator. I'll remove it.\nThat said, we don't really need the source field at all anymore. Technically we never used it in the previous code either, but we agreed it made sense to keep for clarity if not usefulness.\nBut after this refactoring I think the source only adds to the confusion, and is never exposed/used for testing. I think at this point it makes sense to remove, but lmk if you disagree.", "author": "ableegoldman", "createdAt": "2020-04-17T22:38:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDM5NjYyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUwMjM0MQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410502341", "bodyText": "Sure, if we don't need it, then drop it!", "author": "vvcephei", "createdAt": "2020-04-17T22:40:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDM5NjYyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDM5ODM5MA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410398390", "bodyText": "I just moved this class to its own file from HATA, with one main change: we now just pass in the criteria to consider a client a valid candidate for a task.\nThe original criteria was that the client has no other version of this task already,  but now we are flexible enough to use other validation criteria (eg that the client is caught-up on this task)", "author": "ableegoldman", "createdAt": "2020-04-17T18:28:33Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ValidClientsByTaskLoadQueue.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.function.BiFunction;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+/**\n+ * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n+ */\n+class ValidClientsByTaskLoadQueue {\n+    private final PriorityQueue<UUID> clientsByTaskLoad;\n+    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+\n+    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n+                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0ODg0OA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410748848", "bodyText": "I'd just like to say what an awesome tool for optimization this class is. Kudos to you and @cadonna .", "author": "vvcephei", "createdAt": "2020-04-18T20:48:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDM5ODM5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ3MTE2Nw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411471167", "bodyText": "I do not remember having contributed to this awesomeness. It is all @ableegoldman 's merit.", "author": "cadonna", "createdAt": "2020-04-20T15:27:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDM5ODM5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQyMzAxMQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r412423011", "bodyText": "Ah, sorry about that @ableegoldman ; I wasn't able (or was too lazy) to follow the git praise trail through the class movement. Well, kudos to you, then. :)", "author": "vvcephei", "createdAt": "2020-04-21T19:14:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDM5ODM5MA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQxMjY5NQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410412695", "bodyText": "Can we create a util class and move this to it? There's currently a weird dependency relationship between TaskMovement and HATA involving this method.\nRecommendation:\npackage org.apache.kafka.streams.processor.internals.assignment;\n\nimport org.apache.kafka.streams.processor.TaskId;\n\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.SortedSet;\nimport java.util.UUID;\n\nfinal class AssignmentUtils {\n\n    private AssignmentUtils() {}\n\n    /**\n     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n     */\n    static boolean taskIsCaughtUpOnClient(final TaskId task,\n                                          final UUID client,\n                                          final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients) {\n        final Set<UUID> caughtUpClients = tasksToCaughtUpClients.get(task);\n        return caughtUpClients == null || caughtUpClients.contains(client);\n    }\n}", "author": "vvcephei", "createdAt": "2020-04-17T18:57:42Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -198,52 +171,40 @@ boolean previousAssignmentIsValid() {\n \n             // Verify that this client was caught-up on all stateful active tasks\n             for (final TaskId activeTask : prevActiveTasks) {\n-                if (!taskIsCaughtUpOnClient(activeTask, client)) {\n+                if (!taskIsCaughtUpOnClient(activeTask, client, tasksToCaughtUpClients)) {\n                     return false;\n                 }\n             }\n+            if (!unassignedActiveTasks.containsAll(prevActiveTasks)) {\n+                return false;\n+            }\n             unassignedActiveTasks.removeAll(prevActiveTasks);\n \n-            if (!unassignedStandbyTasks.isEmpty()) {\n-                for (final TaskId task : state.prevStandbyTasks()) {\n-                    final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n-                    if (remainingStandbys != null) {\n-                        if (remainingStandbys == 1) {\n-                            unassignedStandbyTasks.remove(task);\n-                        } else {\n-                            unassignedStandbyTasks.put(task, remainingStandbys - 1);\n-                        }\n+            for (final TaskId task : state.prevStandbyTasks()) {\n+                final Integer remainingStandbys = unassignedStandbyTasks.get(task);\n+                if (remainingStandbys != null) {\n+                    if (remainingStandbys == 1) {\n+                        unassignedStandbyTasks.remove(task);\n+                    } else {\n+                        unassignedStandbyTasks.put(task, remainingStandbys - 1);\n                     }\n+                } else {\n+                    return false;\n                 }\n             }\n+\n         }\n         return unassignedActiveTasks.isEmpty() && unassignedStandbyTasks.isEmpty();\n     }\n \n     /**\n      * @return true if this client is caught-up for this task, or the task has no caught-up clients\n      */\n-    boolean taskIsCaughtUpOnClient(final TaskId task, final UUID client) {\n-        boolean hasNoCaughtUpClients = true;\n-        final SortedSet<RankedClient> rankedClients = statefulTasksToRankedCandidates.get(task);\n-        if (rankedClients == null) {\n-            return true;\n-        }\n-        for (final RankedClient rankedClient : rankedClients) {\n-            if (rankedClient.rank() <= 0L) {\n-                if (rankedClient.clientId().equals(client)) {\n-                    return true;\n-                } else {\n-                    hasNoCaughtUpClients = false;\n-                }\n-            }\n-\n-            // If we haven't found our client yet, it must not be caught-up\n-            if (rankedClient.rank() > 0L) {\n-                break;\n-            }\n-        }\n-        return hasNoCaughtUpClients;\n+    static boolean taskIsCaughtUpOnClient(final TaskId task,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ5NjM5OA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410496398", "bodyText": "Done \ud83d\udc4d", "author": "ableegoldman", "createdAt": "2020-04-17T22:18:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQxMjY5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQyODQ0Mg==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410428442", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final PriorityQueue<UUID> queue = new PriorityQueue<>(\n          \n          \n            \n                        (client, other) -> {\n          \n          \n            \n                            final double clientTaskLoad = clientStates.get(client).taskLoad();\n          \n          \n            \n                            final double otherTaskLoad = clientStates.get(other).taskLoad();\n          \n          \n            \n                            if (clientTaskLoad < otherTaskLoad) {\n          \n          \n            \n                                return -1;\n          \n          \n            \n                            } else if (clientTaskLoad > otherTaskLoad) {\n          \n          \n            \n                                return 1;\n          \n          \n            \n                            } else {\n          \n          \n            \n                                return client.compareTo(other);\n          \n          \n            \n                            }\n          \n          \n            \n                        });\n          \n          \n            \n                    final PriorityQueue<UUID> queue = new PriorityQueue<>(\n          \n          \n            \n                        Comparator.comparingDouble(k -> clientStates.get(k).taskLoad())\n          \n          \n            \n                    );\n          \n      \n    \n    \n  \n\nUpon second reading, this does the same thing, right?", "author": "vvcephei", "createdAt": "2020-04-17T19:31:49Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ValidClientsByTaskLoadQueue.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.function.BiFunction;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+/**\n+ * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n+ */\n+class ValidClientsByTaskLoadQueue {\n+    private final PriorityQueue<UUID> clientsByTaskLoad;\n+    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+\n+    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n+                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n+        clientsByTaskLoad = getClientPriorityQueueByTaskLoad(clientStates);\n+        this.validClientCriteria = validClientCriteria;\n+    }\n+\n+    /**\n+=     * @return the next least loaded client that satisfies the given criteria, or null if none do\n+     */\n+    UUID poll(final TaskId task) {\n+        final List<UUID> validClient = poll(task, 1);\n+        return validClient.isEmpty() ? null : validClient.get(0);\n+    }\n+\n+    /**\n+     * @return the next N <= {@code numClientsPerTask} clients in the underlying priority queue that are valid\n+     * candidates for the given task\n+     */\n+    List<UUID> poll(final TaskId task, final int numClients) {\n+        final List<UUID> nextLeastLoadedValidClients = new LinkedList<>();\n+        final Set<UUID> invalidPolledClients = new HashSet<>();\n+        while (nextLeastLoadedValidClients.size() < numClients) {\n+            UUID candidateClient;\n+            while (true) {\n+                candidateClient = clientsByTaskLoad.poll();\n+                if (candidateClient == null) {\n+                    returnPolledClientsToQueue(invalidPolledClients);\n+                    return nextLeastLoadedValidClients;\n+                }\n+\n+                if (validClientCriteria.apply(candidateClient, task)) {\n+                    nextLeastLoadedValidClients.add(candidateClient);\n+                    break;\n+                } else {\n+                    invalidPolledClients.add(candidateClient);\n+                }\n+            }\n+        }\n+        returnPolledClientsToQueue(invalidPolledClients);\n+        return nextLeastLoadedValidClients;\n+    }\n+\n+    void offerAll(final Collection<UUID> clients) {\n+        returnPolledClientsToQueue(clients);\n+    }\n+\n+    void offer(final UUID client) {\n+        clientsByTaskLoad.offer(client);\n+    }\n+\n+    private void returnPolledClientsToQueue(final Collection<UUID> polledClients) {\n+        for (final UUID client : polledClients) {\n+            clientsByTaskLoad.offer(client);\n+        }\n+    }\n+\n+    static PriorityQueue<UUID> getClientPriorityQueueByTaskLoad(final Map<UUID, ClientState> clientStates) {\n+        final PriorityQueue<UUID> queue = new PriorityQueue<>(\n+            (client, other) -> {\n+                final double clientTaskLoad = clientStates.get(client).taskLoad();\n+                final double otherTaskLoad = clientStates.get(other).taskLoad();\n+                if (clientTaskLoad < otherTaskLoad) {\n+                    return -1;\n+                } else if (clientTaskLoad > otherTaskLoad) {\n+                    return 1;\n+                } else {\n+                    return client.compareTo(other);\n+                }\n+            });", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ5MDU3Mw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410490573", "bodyText": "Almost, we want to fall back to comparing the actual UUIDs if the taskLoad happens to be equal", "author": "ableegoldman", "createdAt": "2020-04-17T22:00:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQyODQ0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUwMjU1MQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410502551", "bodyText": "Ah, thanks.", "author": "vvcephei", "createdAt": "2020-04-17T22:40:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQyODQ0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQzNTgzMw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410435833", "bodyText": "This seems suspicious... It doesn't look like we previously polled this client, but we did add all the clients while initializing the queue. Does this add the client twice in the queue?\nSince we know we want the queue to have a uniqueness property over its elements as well, and since the heap is already encapsulated, we could consider adding a simple hashset of elements alongside the internal heap, and remove before offering when we know the element is already present. Did that make sense?", "author": "vvcephei", "createdAt": "2020-04-17T19:47:59Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -62,82 +67,65 @@ public int hashCode() {\n     }\n \n     /**\n-     * Computes the movement of tasks from the state constrained to the balanced assignment, up to the configured\n-     * {@code max.warmup.replicas}. A movement corresponds to a warmup replica on the destination client, with\n-     * a few exceptional cases:\n-     * <p>\n-     * 1. Tasks whose destination clients are caught-up, or whose source clients are not caught-up, will be moved\n-     * immediately from the source to the destination in the state constrained assignment\n-     * 2. Tasks whose destination client previously had this task as a standby will not be counted towards the total\n-     * {@code max.warmup.replicas}. Instead they will be counted against that task's total {@code num.standby.replicas}.\n-     *\n-     * @param statefulActiveTaskAssignment the initial, state constrained assignment, with the source clients\n-     * @param balancedStatefulActiveTaskAssignment the final, balanced assignment, with the destination clients\n-     * @return list of the task movements from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @return whether any warmup replicas were assigned\n      */\n-    static List<TaskMovement> getMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                           final Map<UUID, List<TaskId>> balancedStatefulActiveTaskAssignment,\n-                                           final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n-                                           final Map<UUID, ClientState> clientStates,\n-                                           final Map<TaskId, Integer> tasksToRemainingStandbys,\n-                                           final int maxWarmupReplicas) {\n-        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n-            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n-        }\n+    static boolean assignTaskMovements(final Map<UUID, ClientState> clientStates,\n+                                    final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n+                                    final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+                                    final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                    final int maxWarmupReplicas) {\n+        boolean warmupReplicasAssigned = false;\n \n-        final Map<TaskId, UUID> taskToDestinationClient = new HashMap<>();\n-        for (final Map.Entry<UUID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n-            final UUID destination = clientEntry.getKey();\n-            for (final TaskId task : clientEntry.getValue()) {\n-                taskToDestinationClient.put(task, destination);\n-            }\n-        }\n+        final ValidClientsByTaskLoadQueue clientsByTaskLoad =\n+            new ValidClientsByTaskLoadQueue(\n+                clientStates,\n+                (client, task) -> taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)\n+            );\n \n-        int remainingAllowedWarmupReplicas = maxWarmupReplicas;\n-        final List<TaskMovement> movements = new LinkedList<>();\n-        for (final Map.Entry<UUID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n-            final UUID source = sourceClientEntry.getKey();\n-\n-            final Iterator<TaskId> sourceClientTasksIterator = sourceClientEntry.getValue().iterator();\n-            while (sourceClientTasksIterator.hasNext()) {\n-                final TaskId task = sourceClientTasksIterator.next();\n-                final UUID destination = taskToDestinationClient.get(task);\n-                if (destination == null) {\n-                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n-                                  \"balanced assignment.\", task, source);\n-                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n-                } else if (!source.equals(destination)) {\n-                    if (destinationClientIsCaughtUp(task, destination, tasksToCaughtUpClients)) {\n-                        sourceClientTasksIterator.remove();\n-                        statefulActiveTaskAssignment.get(destination).add(task);\n-                    } else {\n-                        if (clientStates.get(destination).prevStandbyTasks().contains(task)\n-                                && tasksToRemainingStandbys.get(task) > 0\n-                        ) {\n-                            decrementRemainingStandbys(task, tasksToRemainingStandbys);\n-                        } else {\n-                            --remainingAllowedWarmupReplicas;\n-                        }\n-\n-                        movements.add(new TaskMovement(task, source, destination));\n-                        if (remainingAllowedWarmupReplicas == 0) {\n-                            return movements;\n-                        }\n-                    }\n+        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(Comparator.comparingInt(\n+            movement -> tasksToCaughtUpClients.get(movement.task).size()\n+        ));\n+\n+        for (final Map.Entry<UUID, List<TaskId>> assignmentEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final UUID client = assignmentEntry.getKey();\n+            final ClientState state = clientStates.get(client);\n+            for (final TaskId task : assignmentEntry.getValue()) {\n+                if (taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)) {\n+                    state.assignActive(task);\n+                } else {\n+                    final TaskMovement taskMovement = new TaskMovement(task,  client);\n+                    taskMovements.add(taskMovement);\n                 }\n             }\n+            clientsByTaskLoad.offer(client);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ5MTY5Mg==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410491692", "bodyText": "Good catch, I think we should remove adding the clients from the initialization and force that to be done through offerAll (or addAll). Enforcing uniqueness sounds like a good idea though", "author": "ableegoldman", "createdAt": "2020-04-17T22:03:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQzNTgzMw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ4OTI0NQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410489245", "bodyText": "This test now produces different assignments depending on which task assignor is used. Since the only thing its verifying is the actual assignment, and that's not really the responsibility of the StreamsPartitionAssignor anyway, I thought it made the most sense to just remove", "author": "ableegoldman", "createdAt": "2020-04-17T21:56:38Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "diffHunk": "@@ -1610,79 +1610,6 @@ public void shouldReturnInterleavedAssignmentWithUnrevokedPartitionsRemovedWhenN\n             )));\n     }\n \n-    @Test\n-    public void shouldReturnNormalAssignmentForOldAndFutureInstancesDuringVersionProbing() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc1MDIzMw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410750233", "bodyText": "Should we instead adapt the test to verify that it produces a valid assignment for mixed instances during version probing? Or is that already covered?", "author": "vvcephei", "createdAt": "2020-04-18T21:01:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ4OTI0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1ODU1NQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411858555", "bodyText": "I don't mean to totally cop out on this, but I think we should do this in a followup PR. I'll make a ticket and assign it to myself for later so I can't escape, but I don't even think it's worth marking it @Ignore for now.\nTbh we should have removed it a while ago, rather than changing it over time to become its useless self today. It's a long history, and I'm mostly responsible, but just looking ahead the question now is: what do we even want to validate? The task assignor has no knowledge of version probing, and the partition assignor is not responsible for the task assignment (whereas it used to be with version probing, hence this test). What we should do is validate the inputs are being assembled sensibly during version probing.\nAnyways this will be really difficult to do just based on the final partition assignment, and even harder to distinguish a real failure from an unrelated one. So I'd propose to kick this into the future, when we embed the actual assignor class in the configs instead of this flag, and then pass in a VersionProbingClientStatesValidatingAssignor or whatever...SG?", "author": "ableegoldman", "createdAt": "2020-04-21T04:25:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ4OTI0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1ODgwOA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411858808", "bodyText": "Probably a much longer answer than you ever wanted, but this test has been haunting me over many PRs \ud83d\udc40", "author": "ableegoldman", "createdAt": "2020-04-21T04:26:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ4OTI0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTIyOA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410745228", "bodyText": "Unless I missed something, it's possible for tasksToCaughtUpClients not to contain a task, which would give us an NPE. Can we either add a test and handle the case or assert it here with an IllegalStateException, so we don't have to chase down an NPE later?", "author": "vvcephei", "createdAt": "2020-04-18T20:16:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -16,128 +16,94 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClient;\n+\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n-import java.util.Set;\n import java.util.SortedSet;\n+import java.util.TreeSet;\n import java.util.UUID;\n import org.apache.kafka.streams.processor.TaskId;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n public class TaskMovement {\n-    private static final Logger log = LoggerFactory.getLogger(TaskMovement.class);\n-\n     final TaskId task;\n-    final UUID source;\n-    final UUID destination;\n+    private final UUID destination;\n \n-    TaskMovement(final TaskId task, final UUID source, final UUID destination) {\n+    TaskMovement(final TaskId task, final UUID destination) {\n         this.task = task;\n-        this.source = source;\n         this.destination = destination;\n     }\n \n-    @Override\n-    public boolean equals(final Object o) {\n-        if (this == o) {\n-            return true;\n-        }\n-        if (o == null || getClass() != o.getClass()) {\n-            return false;\n-        }\n-        final TaskMovement movement = (TaskMovement) o;\n-        return Objects.equals(task, movement.task) &&\n-                   Objects.equals(source, movement.source) &&\n-                   Objects.equals(destination, movement.destination);\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-        return Objects.hash(task, source, destination);\n-    }\n-\n     /**\n-     * Computes the movement of tasks from the state constrained to the balanced assignment, up to the configured\n-     * {@code max.warmup.replicas}. A movement corresponds to a warmup replica on the destination client, with\n-     * a few exceptional cases:\n-     * <p>\n-     * 1. Tasks whose destination clients are caught-up, or whose source clients are not caught-up, will be moved\n-     * immediately from the source to the destination in the state constrained assignment\n-     * 2. Tasks whose destination client previously had this task as a standby will not be counted towards the total\n-     * {@code max.warmup.replicas}. Instead they will be counted against that task's total {@code num.standby.replicas}.\n-     *\n-     * @param statefulActiveTaskAssignment the initial, state constrained assignment, with the source clients\n-     * @param balancedStatefulActiveTaskAssignment the final, balanced assignment, with the destination clients\n-     * @return list of the task movements from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @return whether any warmup replicas were assigned\n      */\n-    static List<TaskMovement> getMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                           final Map<UUID, List<TaskId>> balancedStatefulActiveTaskAssignment,\n-                                           final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n-                                           final Map<UUID, ClientState> clientStates,\n-                                           final Map<TaskId, Integer> tasksToRemainingStandbys,\n-                                           final int maxWarmupReplicas) {\n-        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n-            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n-        }\n+    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n+                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+                                       final Map<UUID, ClientState> clientStates,\n+                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                       final int maxWarmupReplicas) {\n+        boolean warmupReplicasAssigned = false;\n+\n+        final ValidClientsByTaskLoadQueue clientsByTaskLoad =\n+            new ValidClientsByTaskLoadQueue(\n+                clientStates,\n+                (client, task) -> taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)\n+            );\n \n-        final Map<TaskId, UUID> taskToDestinationClient = new HashMap<>();\n-        for (final Map.Entry<UUID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n-            final UUID destination = clientEntry.getKey();\n-            for (final TaskId task : clientEntry.getValue()) {\n-                taskToDestinationClient.put(task, destination);\n+        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(\n+            (movement, other) -> {\n+                final int numCaughtUpClients = tasksToCaughtUpClients.get(movement.task).size();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc4NzgyMg==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411787822", "bodyText": "Well, by definition every task in here must have at least one caught-up client. I'll add the IllegalStateException", "author": "ableegoldman", "createdAt": "2020-04-21T00:45:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTIyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgxMzA1Mw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411813053", "bodyText": "I might have dropped the thread of logic here. Why is that by definition? It looks like all we know about the task is that it's not caught up on the destination client. Why do we think it's caught up on some other client?", "author": "vvcephei", "createdAt": "2020-04-21T02:00:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTIyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgxNTcyNA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411815724", "bodyText": "I think we've hit the same source of confusion as in the other thread. But anything in taskMovements, and in  fact any task that has an associated TaskMovement object must have at least one caught-up client. If it didn't, we wouldn't be creating a warmup task for it; that's just a normal standby. A warmup replica always implies there is an active version elsewhere on a caught-up client", "author": "ableegoldman", "createdAt": "2020-04-21T02:09:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTIyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgyNjc4Mg==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411826782", "bodyText": "Yep, you're right. I'm on the same page now. So the only risk is that the code changes elsewhere and breaks your invariant.\nI'll leave it to you whether you want to check the invariant and throw an exception or just let it be an NPE if that happens.", "author": "vvcephei", "createdAt": "2020-04-21T02:42:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTIyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgyNzc3Nw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411827777", "bodyText": "Yeah I am a bit worried about protecting against future changes (very much including those by myself a few months from now). I have a thought about how to enforce things a bit better, let's see where this goes...", "author": "ableegoldman", "createdAt": "2020-04-21T02:45:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTIyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg0MTkwOQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411841909", "bodyText": "Alright I decided to just push the validation into the TaskMovement constructor, and skip the check here", "author": "ableegoldman", "createdAt": "2020-04-21T03:30:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTIyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTQyMQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410745421", "bodyText": "IIRC, this is an anti-pattern for comparators (I think because of underflow). Can we delegate to Integer.compare instead?\nAlso, this line has no test coverage. Not sure if that's a bad sign or not.", "author": "vvcephei", "createdAt": "2020-04-18T20:18:19Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -16,128 +16,94 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClient;\n+\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n-import java.util.Set;\n import java.util.SortedSet;\n+import java.util.TreeSet;\n import java.util.UUID;\n import org.apache.kafka.streams.processor.TaskId;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n public class TaskMovement {\n-    private static final Logger log = LoggerFactory.getLogger(TaskMovement.class);\n-\n     final TaskId task;\n-    final UUID source;\n-    final UUID destination;\n+    private final UUID destination;\n \n-    TaskMovement(final TaskId task, final UUID source, final UUID destination) {\n+    TaskMovement(final TaskId task, final UUID destination) {\n         this.task = task;\n-        this.source = source;\n         this.destination = destination;\n     }\n \n-    @Override\n-    public boolean equals(final Object o) {\n-        if (this == o) {\n-            return true;\n-        }\n-        if (o == null || getClass() != o.getClass()) {\n-            return false;\n-        }\n-        final TaskMovement movement = (TaskMovement) o;\n-        return Objects.equals(task, movement.task) &&\n-                   Objects.equals(source, movement.source) &&\n-                   Objects.equals(destination, movement.destination);\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-        return Objects.hash(task, source, destination);\n-    }\n-\n     /**\n-     * Computes the movement of tasks from the state constrained to the balanced assignment, up to the configured\n-     * {@code max.warmup.replicas}. A movement corresponds to a warmup replica on the destination client, with\n-     * a few exceptional cases:\n-     * <p>\n-     * 1. Tasks whose destination clients are caught-up, or whose source clients are not caught-up, will be moved\n-     * immediately from the source to the destination in the state constrained assignment\n-     * 2. Tasks whose destination client previously had this task as a standby will not be counted towards the total\n-     * {@code max.warmup.replicas}. Instead they will be counted against that task's total {@code num.standby.replicas}.\n-     *\n-     * @param statefulActiveTaskAssignment the initial, state constrained assignment, with the source clients\n-     * @param balancedStatefulActiveTaskAssignment the final, balanced assignment, with the destination clients\n-     * @return list of the task movements from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @return whether any warmup replicas were assigned\n      */\n-    static List<TaskMovement> getMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                           final Map<UUID, List<TaskId>> balancedStatefulActiveTaskAssignment,\n-                                           final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n-                                           final Map<UUID, ClientState> clientStates,\n-                                           final Map<TaskId, Integer> tasksToRemainingStandbys,\n-                                           final int maxWarmupReplicas) {\n-        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n-            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n-        }\n+    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n+                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+                                       final Map<UUID, ClientState> clientStates,\n+                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                       final int maxWarmupReplicas) {\n+        boolean warmupReplicasAssigned = false;\n+\n+        final ValidClientsByTaskLoadQueue clientsByTaskLoad =\n+            new ValidClientsByTaskLoadQueue(\n+                clientStates,\n+                (client, task) -> taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)\n+            );\n \n-        final Map<TaskId, UUID> taskToDestinationClient = new HashMap<>();\n-        for (final Map.Entry<UUID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n-            final UUID destination = clientEntry.getKey();\n-            for (final TaskId task : clientEntry.getValue()) {\n-                taskToDestinationClient.put(task, destination);\n+        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(\n+            (movement, other) -> {\n+                final int numCaughtUpClients = tasksToCaughtUpClients.get(movement.task).size();\n+                final int otherNumCaughtUpClients = tasksToCaughtUpClients.get(other.task).size();\n+                if (numCaughtUpClients != otherNumCaughtUpClients) {\n+                    return numCaughtUpClients - otherNumCaughtUpClients;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTczODk2NQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411738965", "bodyText": "It's just my dumb little heuristic for hopefully producing a less unbalanced assignment, but I'll add test coverage. Gotta make sure it's not too dumb", "author": "ableegoldman", "createdAt": "2020-04-20T22:39:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTQyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTczOTY0NA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411739644", "bodyText": "By the way, my personal take is that we shouldn't worry too much about the intermediate assignment being unbalanced. But, if we do decide this could/should be improved, or get complaints from users, I wrote up a quick brainstorm on a hopefully-less-dumb algorithm on the ticket", "author": "ableegoldman", "createdAt": "2020-04-20T22:40:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTQyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgxMjEyNw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411812127", "bodyText": "Sounds good. I have no concern about the heuristic.\nAbout the anti-pattern, I was specifically talking about subtracting two integers as a Comparator#compare return value, though. The recommended pattern is to return one of -1, 0, and 1.\nAbout the lack of test coverage, the concern isn't that we return a less-balanced assignment, but that we break everything completely (somehow). If it's worth writing the code, it's worth verifying it works as expected.", "author": "vvcephei", "createdAt": "2020-04-21T01:58:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTQyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgxNDQ2Ng==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411814466", "bodyText": "Oh I did fix it to use Integer.compare, forgot to respond to that part of the comment. Will push the latest changes soon", "author": "ableegoldman", "createdAt": "2020-04-21T02:05:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTQyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgxODkxOQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411818919", "bodyText": "Ah, okeydoke.", "author": "vvcephei", "createdAt": "2020-04-21T02:18:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTQyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NTU5MA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410745590", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                final TaskMovement taskMovement = new TaskMovement(task,  client);\n          \n          \n            \n                                final TaskMovement taskMovement = new TaskMovement(task, client);", "author": "vvcephei", "createdAt": "2020-04-18T20:19:41Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -16,128 +16,94 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClient;\n+\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n-import java.util.Set;\n import java.util.SortedSet;\n+import java.util.TreeSet;\n import java.util.UUID;\n import org.apache.kafka.streams.processor.TaskId;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n public class TaskMovement {\n-    private static final Logger log = LoggerFactory.getLogger(TaskMovement.class);\n-\n     final TaskId task;\n-    final UUID source;\n-    final UUID destination;\n+    private final UUID destination;\n \n-    TaskMovement(final TaskId task, final UUID source, final UUID destination) {\n+    TaskMovement(final TaskId task, final UUID destination) {\n         this.task = task;\n-        this.source = source;\n         this.destination = destination;\n     }\n \n-    @Override\n-    public boolean equals(final Object o) {\n-        if (this == o) {\n-            return true;\n-        }\n-        if (o == null || getClass() != o.getClass()) {\n-            return false;\n-        }\n-        final TaskMovement movement = (TaskMovement) o;\n-        return Objects.equals(task, movement.task) &&\n-                   Objects.equals(source, movement.source) &&\n-                   Objects.equals(destination, movement.destination);\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-        return Objects.hash(task, source, destination);\n-    }\n-\n     /**\n-     * Computes the movement of tasks from the state constrained to the balanced assignment, up to the configured\n-     * {@code max.warmup.replicas}. A movement corresponds to a warmup replica on the destination client, with\n-     * a few exceptional cases:\n-     * <p>\n-     * 1. Tasks whose destination clients are caught-up, or whose source clients are not caught-up, will be moved\n-     * immediately from the source to the destination in the state constrained assignment\n-     * 2. Tasks whose destination client previously had this task as a standby will not be counted towards the total\n-     * {@code max.warmup.replicas}. Instead they will be counted against that task's total {@code num.standby.replicas}.\n-     *\n-     * @param statefulActiveTaskAssignment the initial, state constrained assignment, with the source clients\n-     * @param balancedStatefulActiveTaskAssignment the final, balanced assignment, with the destination clients\n-     * @return list of the task movements from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @return whether any warmup replicas were assigned\n      */\n-    static List<TaskMovement> getMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                           final Map<UUID, List<TaskId>> balancedStatefulActiveTaskAssignment,\n-                                           final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n-                                           final Map<UUID, ClientState> clientStates,\n-                                           final Map<TaskId, Integer> tasksToRemainingStandbys,\n-                                           final int maxWarmupReplicas) {\n-        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n-            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n-        }\n+    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n+                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+                                       final Map<UUID, ClientState> clientStates,\n+                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                       final int maxWarmupReplicas) {\n+        boolean warmupReplicasAssigned = false;\n+\n+        final ValidClientsByTaskLoadQueue clientsByTaskLoad =\n+            new ValidClientsByTaskLoadQueue(\n+                clientStates,\n+                (client, task) -> taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)\n+            );\n \n-        final Map<TaskId, UUID> taskToDestinationClient = new HashMap<>();\n-        for (final Map.Entry<UUID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n-            final UUID destination = clientEntry.getKey();\n-            for (final TaskId task : clientEntry.getValue()) {\n-                taskToDestinationClient.put(task, destination);\n+        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(\n+            (movement, other) -> {\n+                final int numCaughtUpClients = tasksToCaughtUpClients.get(movement.task).size();\n+                final int otherNumCaughtUpClients = tasksToCaughtUpClients.get(other.task).size();\n+                if (numCaughtUpClients != otherNumCaughtUpClients) {\n+                    return numCaughtUpClients - otherNumCaughtUpClients;\n+                } else {\n+                    return movement.task.compareTo(other.task);\n+                }\n             }\n+        );\n+\n+        for (final Map.Entry<UUID, List<TaskId>> assignmentEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final UUID client = assignmentEntry.getKey();\n+            final ClientState state = clientStates.get(client);\n+            for (final TaskId task : assignmentEntry.getValue()) {\n+                if (taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)) {\n+                    state.assignActive(task);\n+                } else {\n+                    final TaskMovement taskMovement = new TaskMovement(task,  client);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NjAzNQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410746035", "bodyText": "can be private", "author": "vvcephei", "createdAt": "2020-04-18T20:23:53Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -16,128 +16,94 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClient;\n+\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n-import java.util.Set;\n import java.util.SortedSet;\n+import java.util.TreeSet;\n import java.util.UUID;\n import org.apache.kafka.streams.processor.TaskId;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n public class TaskMovement {\n-    private static final Logger log = LoggerFactory.getLogger(TaskMovement.class);\n-\n     final TaskId task;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzA0NA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410747044", "bodyText": "This also doesn't have any test coverage. It took me a while to figure out why this should be a illegal.\nIf I followed the code paths to this point correctly, it seems like the balanced assignor declared it wants task X to be on client A, and A is not caught up on X, so we don't immediately assign A to X. Instead, we try to find the least loaded valid client to host it for now. However, there was no valid client. At a glance, this means that there's no client that's caught up on the task, but the trick is that taskIsCaughtUpOnClient considers all clients valid if there are no caught-up ones. So, if we did get null here, it would either mean that A is the only caught-up client on X (and we shouldn't be here, since it means A is caught up on X, contradicting the earlier statement), or that there are no clients in the queue (which also shouldn't happen).\nWe can possibly clarify it my making the name of taskIsCaughtUpOnClient more complete: eitherClientIsCaughtUpOnTaskOrNoClientIs. But I wouldn't hesitate to also write a nice letter to future us here as a comment.", "author": "vvcephei", "createdAt": "2020-04-18T20:33:04Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -16,128 +16,94 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClient;\n+\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n-import java.util.Set;\n import java.util.SortedSet;\n+import java.util.TreeSet;\n import java.util.UUID;\n import org.apache.kafka.streams.processor.TaskId;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n public class TaskMovement {\n-    private static final Logger log = LoggerFactory.getLogger(TaskMovement.class);\n-\n     final TaskId task;\n-    final UUID source;\n-    final UUID destination;\n+    private final UUID destination;\n \n-    TaskMovement(final TaskId task, final UUID source, final UUID destination) {\n+    TaskMovement(final TaskId task, final UUID destination) {\n         this.task = task;\n-        this.source = source;\n         this.destination = destination;\n     }\n \n-    @Override\n-    public boolean equals(final Object o) {\n-        if (this == o) {\n-            return true;\n-        }\n-        if (o == null || getClass() != o.getClass()) {\n-            return false;\n-        }\n-        final TaskMovement movement = (TaskMovement) o;\n-        return Objects.equals(task, movement.task) &&\n-                   Objects.equals(source, movement.source) &&\n-                   Objects.equals(destination, movement.destination);\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-        return Objects.hash(task, source, destination);\n-    }\n-\n     /**\n-     * Computes the movement of tasks from the state constrained to the balanced assignment, up to the configured\n-     * {@code max.warmup.replicas}. A movement corresponds to a warmup replica on the destination client, with\n-     * a few exceptional cases:\n-     * <p>\n-     * 1. Tasks whose destination clients are caught-up, or whose source clients are not caught-up, will be moved\n-     * immediately from the source to the destination in the state constrained assignment\n-     * 2. Tasks whose destination client previously had this task as a standby will not be counted towards the total\n-     * {@code max.warmup.replicas}. Instead they will be counted against that task's total {@code num.standby.replicas}.\n-     *\n-     * @param statefulActiveTaskAssignment the initial, state constrained assignment, with the source clients\n-     * @param balancedStatefulActiveTaskAssignment the final, balanced assignment, with the destination clients\n-     * @return list of the task movements from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @return whether any warmup replicas were assigned\n      */\n-    static List<TaskMovement> getMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                           final Map<UUID, List<TaskId>> balancedStatefulActiveTaskAssignment,\n-                                           final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n-                                           final Map<UUID, ClientState> clientStates,\n-                                           final Map<TaskId, Integer> tasksToRemainingStandbys,\n-                                           final int maxWarmupReplicas) {\n-        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n-            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n-        }\n+    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n+                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+                                       final Map<UUID, ClientState> clientStates,\n+                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                       final int maxWarmupReplicas) {\n+        boolean warmupReplicasAssigned = false;\n+\n+        final ValidClientsByTaskLoadQueue clientsByTaskLoad =\n+            new ValidClientsByTaskLoadQueue(\n+                clientStates,\n+                (client, task) -> taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)\n+            );\n \n-        final Map<TaskId, UUID> taskToDestinationClient = new HashMap<>();\n-        for (final Map.Entry<UUID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n-            final UUID destination = clientEntry.getKey();\n-            for (final TaskId task : clientEntry.getValue()) {\n-                taskToDestinationClient.put(task, destination);\n+        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(\n+            (movement, other) -> {\n+                final int numCaughtUpClients = tasksToCaughtUpClients.get(movement.task).size();\n+                final int otherNumCaughtUpClients = tasksToCaughtUpClients.get(other.task).size();\n+                if (numCaughtUpClients != otherNumCaughtUpClients) {\n+                    return numCaughtUpClients - otherNumCaughtUpClients;\n+                } else {\n+                    return movement.task.compareTo(other.task);\n+                }\n             }\n+        );\n+\n+        for (final Map.Entry<UUID, List<TaskId>> assignmentEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final UUID client = assignmentEntry.getKey();\n+            final ClientState state = clientStates.get(client);\n+            for (final TaskId task : assignmentEntry.getValue()) {\n+                if (taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)) {\n+                    state.assignActive(task);\n+                } else {\n+                    final TaskMovement taskMovement = new TaskMovement(task,  client);\n+                    taskMovements.add(taskMovement);\n+                }\n+            }\n+            clientsByTaskLoad.offer(client);\n         }\n \n-        int remainingAllowedWarmupReplicas = maxWarmupReplicas;\n-        final List<TaskMovement> movements = new LinkedList<>();\n-        for (final Map.Entry<UUID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n-            final UUID source = sourceClientEntry.getKey();\n+        int remainingWarmupReplicas = maxWarmupReplicas;\n+        for (final TaskMovement movement : taskMovements) {\n+            final UUID leastLoadedClient = clientsByTaskLoad.poll(movement.task);\n+            if (leastLoadedClient == null) {\n+                throw new IllegalStateException(\"Tried to move task to caught-up client but none exist\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQzODAwOQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411438009", "bodyText": "We can possibly clarify it my making the name of taskIsCaughtUpOnClient more complete: eitherClientIsCaughtUpOnTaskOrNoClientIs.\n\nAgree on that\n\nBut I wouldn't hesitate to also write a nice letter to future us here as a comment.\n\nWhat about writing the nice letter to future us in the exception message instead of a comment?", "author": "cadonna", "createdAt": "2020-04-20T14:46:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY3MzExNQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411673115", "bodyText": "Sure, that sounds even better. We just need to bear in mind the difference in audience. Exception messages need to be worded in a way that will make sense to users. Nothing wrong with that, though.", "author": "vvcephei", "createdAt": "2020-04-20T20:33:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc5NzEzNQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411797135", "bodyText": "I guess the way I thought about it was:\n\nthe clientsByTaskLoad queue returns the (next least loaded) caught-up client for a given task\nthe taskMovements set only contains tasks with caught-up clients\ntherefore, every task in taskMovements has at least one client\n\nIf I'm following this discussion correctly, it seems like point 2. is the root cause of confusion. Is that a fair summary? I'll definitely rename taskIsCaughtUpOnClient, thanks for pointing out that was pretty misleading. Would it help to leave a comment on the actual TaskMovement class clarifying that a task to be moved necessarily has at least one caught-up client (because otherwise why would we try to move it), and/or rename taskMovements (suggestions welcome) ?", "author": "ableegoldman", "createdAt": "2020-04-21T01:13:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc5OTA0NQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411799045", "bodyText": "I'm having trouble thinking of a way to make the exception message more clear without just starting to name variable names and describe the literal code, which doesn't seem appropriate for an exception message. Does anyone have any suggestions? Maybe something like scheduled a task to be moved and assigned to a caught-up client, but no caught up clients were found? That just seems like a more verbose way to say the same thing \ud83d\ude15", "author": "ableegoldman", "createdAt": "2020-04-21T01:19:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgxMDcwMA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411810700", "bodyText": "Actually, I think I was saying something different. There is not a guarantee that any client is caught up, which is why the name taskIsCaughtUpOnClient is misleading. If that name is accurate, then it looked like the IllegalStateException would actually get thrown all the time, whenever there is no caught-up client.\nSo the subtlety is that we do get back a client when there is no caught-up client. In that case, the predicate falls back to giving us all the clients. Hence the name I recommended. I do think this is all fairly esoteric from the user's perspective, which may be why I thought first of leaving a comment.\nI also can't think of what to say in the exception. What I said before references internal variables, which is not appropriate. But there also doesn't really seem to be a way to say \"this is why this happened\" because the whole point of ISE is that we have no idea why it would happen.\nHow about you just take as feedback that this is one of the subtlest things I've seen in a while, and make some attempt to make it less mind-blowing (maybe just renaming the method is good enough).\nWorst-case scenario, if we come back in a year and again get confused, we'll have another chance to clarify the code. This feedback is just about trying to prevent that future, but we can only do our best.", "author": "vvcephei", "createdAt": "2020-04-21T01:53:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgxMzM1MQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411813351", "bodyText": "But we only put a task in the taskMovements set if we know it has a caught-up client (that isn't its current client). The inverse is also true; if a task has no caught-up clients, it won't be in taskMovements. So any task we get while looping through taskMovements necessarily has at least one caught-up client, right?\nI agree here that taskIsCaughtUpOnClient was misleading, and have renamed it (just haven't pushed the latest changes yet).", "author": "ableegoldman", "createdAt": "2020-04-21T02:01:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgxMzgzMg==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411813832", "bodyText": "Let's say we rename it to taskIsCaughtUpOnClientOrNoCaughtUpClientsExist Would you still feel like this is a subtle issue worth leaving a comment for? Or was that the sole source of confusion?", "author": "ableegoldman", "createdAt": "2020-04-21T02:03:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgxODgwNg==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411818806", "bodyText": "I think that would be good enough. Thanks!\nGoing back to your prior comment,\n\nBut we only put a task in the taskMovements set if we know it has a caught-up client (that isn't its current client). The inverse is also true; if a task has no caught-up clients, it won't be in taskMovements. So any task we get while looping through taskMovements necessarily has at least one caught-up client, right?\n\nI feel like I'm just missing something here. It looks to me like the only precondition for adding a task to taskMovements is that it is not caught up on the destination. Why does that imply that we know it is caught up on another?", "author": "vvcephei", "createdAt": "2020-04-21T02:18:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgyMTI2OA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411821268", "bodyText": "if (taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)) {\n    state.assignActive(task);\n} else {\n    final TaskMovement taskMovement = new TaskMovement(task, client);\n    taskMovements.add(taskMovement);\n}\n\nwith the renaming, this block now looks like this. If no caught up clients exist, we just assign it to the ClientState right away; otherwise, if and only if taskIsCaughtUpOnClientOrNoCaughtUpClientsExist returns false, do we add to taskMovements.\nSo by (I think it's called deMorgan's Law?):\n!(caughtUpOnClient | noCaughtUpClientsExist)\n== !caughtUpOnClient & !noCaughtUpClientsExist\n== notCaughtUpOnClient & caughtUpClientsExist\n\u2234 caught-up clients exist\nI also feel like I might be missing your point, but I hope we can get somewhere when the method is more appropriately named \ud83d\ude42", "author": "ableegoldman", "createdAt": "2020-04-21T02:25:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgyNTc1Ng==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411825756", "bodyText": "Haha, ok, now I get it. See? I knew renaming the method was a good idea. I think we can definitely chalk this one up as a win for me ;)\nNice work busting out the \u2234 character.", "author": "vvcephei", "createdAt": "2020-04-21T02:38:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgyNjc4NQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411826785", "bodyText": "Yeah I felt pretty fancy with the \u2234.\nHope that didn't come off as too condescending, it's just been a while since I got to practice my logical proofs and I couldn't resist \ud83d\ude04", "author": "ableegoldman", "createdAt": "2020-04-21T02:42:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgyODU2NQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411828565", "bodyText": "Hah! No, not offended at all. I think the old phrase is \"Flawless Victory.\"", "author": "vvcephei", "createdAt": "2020-04-21T02:47:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzA0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzYwNw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410747607", "bodyText": "Should it be:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if (destinationClientState.prevStandbyTasks().contains(movement.task) && tasksToRemainingStandbys.get(movement.task) > 0) {\n          \n          \n            \n                        if (destinationClientState.previousAssignedTasks().contains(movement.task) && tasksToRemainingStandbys.get(movement.task) > 0) {\n          \n      \n    \n    \n  \n\nIt doesn't seem like we care if the task was previously a standby or active, just that it wasn't caught up, and we have an available standby replica to give it, so we don't have to count it as a warmup.", "author": "vvcephei", "createdAt": "2020-04-18T20:38:06Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -16,128 +16,94 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClient;\n+\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n-import java.util.Set;\n import java.util.SortedSet;\n+import java.util.TreeSet;\n import java.util.UUID;\n import org.apache.kafka.streams.processor.TaskId;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n public class TaskMovement {\n-    private static final Logger log = LoggerFactory.getLogger(TaskMovement.class);\n-\n     final TaskId task;\n-    final UUID source;\n-    final UUID destination;\n+    private final UUID destination;\n \n-    TaskMovement(final TaskId task, final UUID source, final UUID destination) {\n+    TaskMovement(final TaskId task, final UUID destination) {\n         this.task = task;\n-        this.source = source;\n         this.destination = destination;\n     }\n \n-    @Override\n-    public boolean equals(final Object o) {\n-        if (this == o) {\n-            return true;\n-        }\n-        if (o == null || getClass() != o.getClass()) {\n-            return false;\n-        }\n-        final TaskMovement movement = (TaskMovement) o;\n-        return Objects.equals(task, movement.task) &&\n-                   Objects.equals(source, movement.source) &&\n-                   Objects.equals(destination, movement.destination);\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-        return Objects.hash(task, source, destination);\n-    }\n-\n     /**\n-     * Computes the movement of tasks from the state constrained to the balanced assignment, up to the configured\n-     * {@code max.warmup.replicas}. A movement corresponds to a warmup replica on the destination client, with\n-     * a few exceptional cases:\n-     * <p>\n-     * 1. Tasks whose destination clients are caught-up, or whose source clients are not caught-up, will be moved\n-     * immediately from the source to the destination in the state constrained assignment\n-     * 2. Tasks whose destination client previously had this task as a standby will not be counted towards the total\n-     * {@code max.warmup.replicas}. Instead they will be counted against that task's total {@code num.standby.replicas}.\n-     *\n-     * @param statefulActiveTaskAssignment the initial, state constrained assignment, with the source clients\n-     * @param balancedStatefulActiveTaskAssignment the final, balanced assignment, with the destination clients\n-     * @return list of the task movements from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @return whether any warmup replicas were assigned\n      */\n-    static List<TaskMovement> getMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                           final Map<UUID, List<TaskId>> balancedStatefulActiveTaskAssignment,\n-                                           final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n-                                           final Map<UUID, ClientState> clientStates,\n-                                           final Map<TaskId, Integer> tasksToRemainingStandbys,\n-                                           final int maxWarmupReplicas) {\n-        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n-            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n-        }\n+    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n+                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+                                       final Map<UUID, ClientState> clientStates,\n+                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                       final int maxWarmupReplicas) {\n+        boolean warmupReplicasAssigned = false;\n+\n+        final ValidClientsByTaskLoadQueue clientsByTaskLoad =\n+            new ValidClientsByTaskLoadQueue(\n+                clientStates,\n+                (client, task) -> taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)\n+            );\n \n-        final Map<TaskId, UUID> taskToDestinationClient = new HashMap<>();\n-        for (final Map.Entry<UUID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n-            final UUID destination = clientEntry.getKey();\n-            for (final TaskId task : clientEntry.getValue()) {\n-                taskToDestinationClient.put(task, destination);\n+        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(\n+            (movement, other) -> {\n+                final int numCaughtUpClients = tasksToCaughtUpClients.get(movement.task).size();\n+                final int otherNumCaughtUpClients = tasksToCaughtUpClients.get(other.task).size();\n+                if (numCaughtUpClients != otherNumCaughtUpClients) {\n+                    return numCaughtUpClients - otherNumCaughtUpClients;\n+                } else {\n+                    return movement.task.compareTo(other.task);\n+                }\n             }\n+        );\n+\n+        for (final Map.Entry<UUID, List<TaskId>> assignmentEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final UUID client = assignmentEntry.getKey();\n+            final ClientState state = clientStates.get(client);\n+            for (final TaskId task : assignmentEntry.getValue()) {\n+                if (taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)) {\n+                    state.assignActive(task);\n+                } else {\n+                    final TaskMovement taskMovement = new TaskMovement(task,  client);\n+                    taskMovements.add(taskMovement);\n+                }\n+            }\n+            clientsByTaskLoad.offer(client);\n         }\n \n-        int remainingAllowedWarmupReplicas = maxWarmupReplicas;\n-        final List<TaskMovement> movements = new LinkedList<>();\n-        for (final Map.Entry<UUID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n-            final UUID source = sourceClientEntry.getKey();\n+        int remainingWarmupReplicas = maxWarmupReplicas;\n+        for (final TaskMovement movement : taskMovements) {\n+            final UUID leastLoadedClient = clientsByTaskLoad.poll(movement.task);\n+            if (leastLoadedClient == null) {\n+                throw new IllegalStateException(\"Tried to move task to caught-up client but none exist\");\n+            }\n \n-            final Iterator<TaskId> sourceClientTasksIterator = sourceClientEntry.getValue().iterator();\n-            while (sourceClientTasksIterator.hasNext()) {\n-                final TaskId task = sourceClientTasksIterator.next();\n-                final UUID destination = taskToDestinationClient.get(task);\n-                if (destination == null) {\n-                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n-                                  \"balanced assignment.\", task, source);\n-                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n-                } else if (!source.equals(destination)) {\n-                    if (destinationClientIsCaughtUp(task, destination, tasksToCaughtUpClients)) {\n-                        sourceClientTasksIterator.remove();\n-                        statefulActiveTaskAssignment.get(destination).add(task);\n-                    } else {\n-                        if (clientStates.get(destination).prevStandbyTasks().contains(task)\n-                                && tasksToRemainingStandbys.get(task) > 0\n-                        ) {\n-                            decrementRemainingStandbys(task, tasksToRemainingStandbys);\n-                        } else {\n-                            --remainingAllowedWarmupReplicas;\n-                        }\n+            final ClientState sourceClientState = clientStates.get(leastLoadedClient);\n+            sourceClientState.assignActive(movement.task);\n \n-                        movements.add(new TaskMovement(task, source, destination));\n-                        if (remainingAllowedWarmupReplicas == 0) {\n-                            return movements;\n-                        }\n-                    }\n-                }\n+            final ClientState destinationClientState = clientStates.get(movement.destination);\n+            if (destinationClientState.prevStandbyTasks().contains(movement.task) && tasksToRemainingStandbys.get(movement.task) > 0) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTUyODMzOA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411528338", "bodyText": "Q: Why do we even care at all whether the task was running on the client? What if we just assign a real stand-by task if we have a spare one?", "author": "cadonna", "createdAt": "2020-04-20T16:41:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzYwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY3NjM2MQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411676361", "bodyText": "I think we're trying to strike a balance of not harming HA while moving tasks around.\nSo if we have a task T and three nodes A, B[T=active], C[T=standby], and we want to move T to A, then we'll do it by assigning a warmup to A and letting B continue to be active and C continue to be a standby until A catches up.", "author": "vvcephei", "createdAt": "2020-04-20T20:39:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0NzYwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0ODE4MQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410748181", "bodyText": "It feels like there's a missing condition here.\n\nif the task was previously on the destination AND we can consider it one of the configured standbys\nwe can assign the task to the destination as a warm-up\n\nWhat about:\n3. We have an available standby, and the task was not previously assigned to the client, but it also wasn't previously assigned to any client, so we can just choose to assign the standby to the destination client and call it a standby", "author": "vvcephei", "createdAt": "2020-04-18T20:43:22Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -16,128 +16,94 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClient;\n+\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n-import java.util.Set;\n import java.util.SortedSet;\n+import java.util.TreeSet;\n import java.util.UUID;\n import org.apache.kafka.streams.processor.TaskId;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n public class TaskMovement {\n-    private static final Logger log = LoggerFactory.getLogger(TaskMovement.class);\n-\n     final TaskId task;\n-    final UUID source;\n-    final UUID destination;\n+    private final UUID destination;\n \n-    TaskMovement(final TaskId task, final UUID source, final UUID destination) {\n+    TaskMovement(final TaskId task, final UUID destination) {\n         this.task = task;\n-        this.source = source;\n         this.destination = destination;\n     }\n \n-    @Override\n-    public boolean equals(final Object o) {\n-        if (this == o) {\n-            return true;\n-        }\n-        if (o == null || getClass() != o.getClass()) {\n-            return false;\n-        }\n-        final TaskMovement movement = (TaskMovement) o;\n-        return Objects.equals(task, movement.task) &&\n-                   Objects.equals(source, movement.source) &&\n-                   Objects.equals(destination, movement.destination);\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-        return Objects.hash(task, source, destination);\n-    }\n-\n     /**\n-     * Computes the movement of tasks from the state constrained to the balanced assignment, up to the configured\n-     * {@code max.warmup.replicas}. A movement corresponds to a warmup replica on the destination client, with\n-     * a few exceptional cases:\n-     * <p>\n-     * 1. Tasks whose destination clients are caught-up, or whose source clients are not caught-up, will be moved\n-     * immediately from the source to the destination in the state constrained assignment\n-     * 2. Tasks whose destination client previously had this task as a standby will not be counted towards the total\n-     * {@code max.warmup.replicas}. Instead they will be counted against that task's total {@code num.standby.replicas}.\n-     *\n-     * @param statefulActiveTaskAssignment the initial, state constrained assignment, with the source clients\n-     * @param balancedStatefulActiveTaskAssignment the final, balanced assignment, with the destination clients\n-     * @return list of the task movements from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @return whether any warmup replicas were assigned\n      */\n-    static List<TaskMovement> getMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                           final Map<UUID, List<TaskId>> balancedStatefulActiveTaskAssignment,\n-                                           final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n-                                           final Map<UUID, ClientState> clientStates,\n-                                           final Map<TaskId, Integer> tasksToRemainingStandbys,\n-                                           final int maxWarmupReplicas) {\n-        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n-            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n-        }\n+    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n+                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+                                       final Map<UUID, ClientState> clientStates,\n+                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                       final int maxWarmupReplicas) {\n+        boolean warmupReplicasAssigned = false;\n+\n+        final ValidClientsByTaskLoadQueue clientsByTaskLoad =\n+            new ValidClientsByTaskLoadQueue(\n+                clientStates,\n+                (client, task) -> taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)\n+            );\n \n-        final Map<TaskId, UUID> taskToDestinationClient = new HashMap<>();\n-        for (final Map.Entry<UUID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n-            final UUID destination = clientEntry.getKey();\n-            for (final TaskId task : clientEntry.getValue()) {\n-                taskToDestinationClient.put(task, destination);\n+        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(\n+            (movement, other) -> {\n+                final int numCaughtUpClients = tasksToCaughtUpClients.get(movement.task).size();\n+                final int otherNumCaughtUpClients = tasksToCaughtUpClients.get(other.task).size();\n+                if (numCaughtUpClients != otherNumCaughtUpClients) {\n+                    return numCaughtUpClients - otherNumCaughtUpClients;\n+                } else {\n+                    return movement.task.compareTo(other.task);\n+                }\n             }\n+        );\n+\n+        for (final Map.Entry<UUID, List<TaskId>> assignmentEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final UUID client = assignmentEntry.getKey();\n+            final ClientState state = clientStates.get(client);\n+            for (final TaskId task : assignmentEntry.getValue()) {\n+                if (taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)) {\n+                    state.assignActive(task);\n+                } else {\n+                    final TaskMovement taskMovement = new TaskMovement(task,  client);\n+                    taskMovements.add(taskMovement);\n+                }\n+            }\n+            clientsByTaskLoad.offer(client);\n         }\n \n-        int remainingAllowedWarmupReplicas = maxWarmupReplicas;\n-        final List<TaskMovement> movements = new LinkedList<>();\n-        for (final Map.Entry<UUID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n-            final UUID source = sourceClientEntry.getKey();\n+        int remainingWarmupReplicas = maxWarmupReplicas;\n+        for (final TaskMovement movement : taskMovements) {\n+            final UUID leastLoadedClient = clientsByTaskLoad.poll(movement.task);\n+            if (leastLoadedClient == null) {\n+                throw new IllegalStateException(\"Tried to move task to caught-up client but none exist\");\n+            }\n \n-            final Iterator<TaskId> sourceClientTasksIterator = sourceClientEntry.getValue().iterator();\n-            while (sourceClientTasksIterator.hasNext()) {\n-                final TaskId task = sourceClientTasksIterator.next();\n-                final UUID destination = taskToDestinationClient.get(task);\n-                if (destination == null) {\n-                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n-                                  \"balanced assignment.\", task, source);\n-                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n-                } else if (!source.equals(destination)) {\n-                    if (destinationClientIsCaughtUp(task, destination, tasksToCaughtUpClients)) {\n-                        sourceClientTasksIterator.remove();\n-                        statefulActiveTaskAssignment.get(destination).add(task);\n-                    } else {\n-                        if (clientStates.get(destination).prevStandbyTasks().contains(task)\n-                                && tasksToRemainingStandbys.get(task) > 0\n-                        ) {\n-                            decrementRemainingStandbys(task, tasksToRemainingStandbys);\n-                        } else {\n-                            --remainingAllowedWarmupReplicas;\n-                        }\n+            final ClientState sourceClientState = clientStates.get(leastLoadedClient);\n+            sourceClientState.assignActive(movement.task);\n \n-                        movements.add(new TaskMovement(task, source, destination));\n-                        if (remainingAllowedWarmupReplicas == 0) {\n-                            return movements;\n-                        }\n-                    }\n-                }\n+            final ClientState destinationClientState = clientStates.get(movement.destination);\n+            if (destinationClientState.prevStandbyTasks().contains(movement.task) && tasksToRemainingStandbys.get(movement.task) > 0) {\n+                decrementRemainingStandbys(movement.task, tasksToRemainingStandbys);\n+                destinationClientState.assignStandby(movement.task);\n+                warmupReplicasAssigned = true;\n+            } else if (remainingWarmupReplicas > 0) {\n+                --remainingWarmupReplicas;\n+                destinationClientState.assignStandby(movement.task);\n+                warmupReplicasAssigned = true;\n             }", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc3OTM1OA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411779358", "bodyText": "Hm. I'm not sure this was what you meant, but I think we have to interpret \"wasn't previously assigned to any client\" as meaning \"any client other than the source client\". By definition someone had to have had this task previously for it to be involved in a movement: of course this could be due to leftover/old state and not the actual assigned task, but we currently don't and can't distinguish these.\nGiven that, I think I buy this. But should the condition also be generalized to \"the task had fewer than num.standby previous clients\" (after accounting for the above)?\nJust to lay out the general reasoning for my  future self: we basically only want to count something as a warmup replica when counting it against the total standbys for this rebalance(s) would mean temporarily revoking a standby task from some client for the duration of the rebalance(s). In other words, outside of the tasks & clients involved in a movement, we want the assignment during the intermediate rebalances to resemble the final assignment as much as possible.\nSo if we had num.standbys = 5 and 4 previous clients I think we would still want to consider this a standby. Of course, nothing discussed here will really matter unless (until) we make the standby assignment more sticky and/or lag-based", "author": "ableegoldman", "createdAt": "2020-04-21T00:19:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0ODE4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgxNTczOA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411815738", "bodyText": "Hmm, it doesn't seem like there's such a guarantee. But I'm always suspicious I just mised the point.\nWhat if we had lost the node holding the standby? Or we lost the node holding the active, and decided to promote the standby to active? Or if we were running with no standbys and the setting were adjusted to allow standbys? Or something similar... It seems like in these situations, there is no prior owner of the standby, and (because the cluster changed) we might want to move the active to a new node, and there wouldn't be a valid candidate to host the standby, so we might as well let the destination be the standby without using up a \"warmup\" task.", "author": "vvcephei", "createdAt": "2020-04-21T02:09:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0ODE4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTgyODM5Nw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411828397", "bodyText": "I suspect this may also be related to the issue we just resolved, ie we know all tasks here must have caught-up clients. But just to clarify what I meant in the first paragraph, this client may be caught-up due to old leftover state rather than a previous task. Unfortunately(?) we just encode the offset sums for everything, so we can't distinguish these cases.\nI think we're actually in agreement here, but I'd propose a slight relaxation of the constraint: if N is the number of clients who previously had this task as a standby, let we count it as a standby if N < num.standby.replicas. This includes the cases you described, but also extends to ones where we have multiple standbys and lost a single node. WDYT?", "author": "ableegoldman", "createdAt": "2020-04-21T02:47:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0ODE4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQxOTk3Mw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r412419973", "bodyText": "Yeah, something like that sounds right to me. I think what we have here is \"good enough\" for now, though, and we can merge this PR while continuing to refine the standby accounting later.", "author": "vvcephei", "createdAt": "2020-04-21T19:09:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0ODE4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0ODM3Nw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410748377", "bodyText": "We also might have assigned tasks to movement.destination, so we should re-offer it as well, right?", "author": "vvcephei", "createdAt": "2020-04-18T20:44:51Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -16,128 +16,94 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClient;\n+\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n-import java.util.Set;\n import java.util.SortedSet;\n+import java.util.TreeSet;\n import java.util.UUID;\n import org.apache.kafka.streams.processor.TaskId;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n public class TaskMovement {\n-    private static final Logger log = LoggerFactory.getLogger(TaskMovement.class);\n-\n     final TaskId task;\n-    final UUID source;\n-    final UUID destination;\n+    private final UUID destination;\n \n-    TaskMovement(final TaskId task, final UUID source, final UUID destination) {\n+    TaskMovement(final TaskId task, final UUID destination) {\n         this.task = task;\n-        this.source = source;\n         this.destination = destination;\n     }\n \n-    @Override\n-    public boolean equals(final Object o) {\n-        if (this == o) {\n-            return true;\n-        }\n-        if (o == null || getClass() != o.getClass()) {\n-            return false;\n-        }\n-        final TaskMovement movement = (TaskMovement) o;\n-        return Objects.equals(task, movement.task) &&\n-                   Objects.equals(source, movement.source) &&\n-                   Objects.equals(destination, movement.destination);\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-        return Objects.hash(task, source, destination);\n-    }\n-\n     /**\n-     * Computes the movement of tasks from the state constrained to the balanced assignment, up to the configured\n-     * {@code max.warmup.replicas}. A movement corresponds to a warmup replica on the destination client, with\n-     * a few exceptional cases:\n-     * <p>\n-     * 1. Tasks whose destination clients are caught-up, or whose source clients are not caught-up, will be moved\n-     * immediately from the source to the destination in the state constrained assignment\n-     * 2. Tasks whose destination client previously had this task as a standby will not be counted towards the total\n-     * {@code max.warmup.replicas}. Instead they will be counted against that task's total {@code num.standby.replicas}.\n-     *\n-     * @param statefulActiveTaskAssignment the initial, state constrained assignment, with the source clients\n-     * @param balancedStatefulActiveTaskAssignment the final, balanced assignment, with the destination clients\n-     * @return list of the task movements from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @return whether any warmup replicas were assigned\n      */\n-    static List<TaskMovement> getMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                           final Map<UUID, List<TaskId>> balancedStatefulActiveTaskAssignment,\n-                                           final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n-                                           final Map<UUID, ClientState> clientStates,\n-                                           final Map<TaskId, Integer> tasksToRemainingStandbys,\n-                                           final int maxWarmupReplicas) {\n-        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n-            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n-        }\n+    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n+                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+                                       final Map<UUID, ClientState> clientStates,\n+                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                       final int maxWarmupReplicas) {\n+        boolean warmupReplicasAssigned = false;\n+\n+        final ValidClientsByTaskLoadQueue clientsByTaskLoad =\n+            new ValidClientsByTaskLoadQueue(\n+                clientStates,\n+                (client, task) -> taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)\n+            );\n \n-        final Map<TaskId, UUID> taskToDestinationClient = new HashMap<>();\n-        for (final Map.Entry<UUID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n-            final UUID destination = clientEntry.getKey();\n-            for (final TaskId task : clientEntry.getValue()) {\n-                taskToDestinationClient.put(task, destination);\n+        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(\n+            (movement, other) -> {\n+                final int numCaughtUpClients = tasksToCaughtUpClients.get(movement.task).size();\n+                final int otherNumCaughtUpClients = tasksToCaughtUpClients.get(other.task).size();\n+                if (numCaughtUpClients != otherNumCaughtUpClients) {\n+                    return numCaughtUpClients - otherNumCaughtUpClients;\n+                } else {\n+                    return movement.task.compareTo(other.task);\n+                }\n             }\n+        );\n+\n+        for (final Map.Entry<UUID, List<TaskId>> assignmentEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final UUID client = assignmentEntry.getKey();\n+            final ClientState state = clientStates.get(client);\n+            for (final TaskId task : assignmentEntry.getValue()) {\n+                if (taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)) {\n+                    state.assignActive(task);\n+                } else {\n+                    final TaskMovement taskMovement = new TaskMovement(task,  client);\n+                    taskMovements.add(taskMovement);\n+                }\n+            }\n+            clientsByTaskLoad.offer(client);\n         }\n \n-        int remainingAllowedWarmupReplicas = maxWarmupReplicas;\n-        final List<TaskMovement> movements = new LinkedList<>();\n-        for (final Map.Entry<UUID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n-            final UUID source = sourceClientEntry.getKey();\n+        int remainingWarmupReplicas = maxWarmupReplicas;\n+        for (final TaskMovement movement : taskMovements) {\n+            final UUID leastLoadedClient = clientsByTaskLoad.poll(movement.task);\n+            if (leastLoadedClient == null) {\n+                throw new IllegalStateException(\"Tried to move task to caught-up client but none exist\");\n+            }\n \n-            final Iterator<TaskId> sourceClientTasksIterator = sourceClientEntry.getValue().iterator();\n-            while (sourceClientTasksIterator.hasNext()) {\n-                final TaskId task = sourceClientTasksIterator.next();\n-                final UUID destination = taskToDestinationClient.get(task);\n-                if (destination == null) {\n-                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n-                                  \"balanced assignment.\", task, source);\n-                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n-                } else if (!source.equals(destination)) {\n-                    if (destinationClientIsCaughtUp(task, destination, tasksToCaughtUpClients)) {\n-                        sourceClientTasksIterator.remove();\n-                        statefulActiveTaskAssignment.get(destination).add(task);\n-                    } else {\n-                        if (clientStates.get(destination).prevStandbyTasks().contains(task)\n-                                && tasksToRemainingStandbys.get(task) > 0\n-                        ) {\n-                            decrementRemainingStandbys(task, tasksToRemainingStandbys);\n-                        } else {\n-                            --remainingAllowedWarmupReplicas;\n-                        }\n+            final ClientState sourceClientState = clientStates.get(leastLoadedClient);\n+            sourceClientState.assignActive(movement.task);\n \n-                        movements.add(new TaskMovement(task, source, destination));\n-                        if (remainingAllowedWarmupReplicas == 0) {\n-                            return movements;\n-                        }\n-                    }\n-                }\n+            final ClientState destinationClientState = clientStates.get(movement.destination);\n+            if (destinationClientState.prevStandbyTasks().contains(movement.task) && tasksToRemainingStandbys.get(movement.task) > 0) {\n+                decrementRemainingStandbys(movement.task, tasksToRemainingStandbys);\n+                destinationClientState.assignStandby(movement.task);\n+                warmupReplicasAssigned = true;\n+            } else if (remainingWarmupReplicas > 0) {\n+                --remainingWarmupReplicas;\n+                destinationClientState.assignStandby(movement.task);\n+                warmupReplicasAssigned = true;\n             }\n-        }\n-        return movements;\n-    }\n \n-    private static boolean destinationClientIsCaughtUp(final TaskId task,\n-                                                       final UUID destination,\n-                                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients) {\n-        final Set<UUID> caughtUpClients = tasksToCaughtUpClients.get(task);\n-        return caughtUpClients != null && caughtUpClients.contains(destination);\n+            clientsByTaskLoad.offer(leastLoadedClient);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0MzU5Nw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411743597", "bodyText": "Good point", "author": "ableegoldman", "createdAt": "2020-04-20T22:50:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0ODM3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0OTc4Nw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410749787", "bodyText": "I'm wondering if we should use ValidClientsByTaskLoadQueue (with a (c,t)-> true predicate) instead of a plain PriorityQueue here, since our queue now nicely enforces uniqueness of elements.", "author": "vvcephei", "createdAt": "2020-04-18T20:57:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -89,95 +88,72 @@ public boolean assign() {\n             return false;\n         }\n \n-        final Map<UUID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap(sortedClients);\n-        final Map<UUID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap(sortedClients);\n-        final Map<UUID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap(sortedClients);\n+        final Map<TaskId, Integer> tasksToRemainingStandbys =\n+            statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        // ---------------- Stateful Active Tasks ---------------- //\n+        final boolean followupRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n \n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment =\n-            new DefaultStateConstrainedBalancedAssignor().assign(\n-                statefulTasksToRankedCandidates,\n-                configs.balanceFactor,\n-                sortedClients,\n-                clientsToNumberOfThreads,\n-                tasksToCaughtUpClients\n-            );\n+        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+\n+        assignStatelessActiveTasks();\n \n-        // ---------------- Warmup Replica Tasks ---------------- //\n+        return followupRebalanceNeeded;\n+    }\n \n-        final Map<UUID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n+        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment =\n             new DefaultBalancedAssignor().assign(\n                 sortedClients,\n                 statefulTasks,\n                 clientsToNumberOfThreads,\n                 configs.balanceFactor);\n \n-        final Map<TaskId, Integer> tasksToRemainingStandbys =\n-            statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n-\n-        final List<TaskMovement> movements = getMovements(\n+        return assignTaskMovements(\n             statefulActiveTaskAssignment,\n-            balancedStatefulActiveTaskAssignment,\n             tasksToCaughtUpClients,\n             clientStates,\n             tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas);\n-\n-        for (final TaskMovement movement : movements) {\n-            warmupTaskAssignment.get(movement.destination).add(movement.task);\n-        }\n-\n-        // ---------------- Standby Replica Tasks ---------------- //\n-\n-        final List<Map<UUID, List<TaskId>>> allTaskAssignmentMaps = asList(\n-            statefulActiveTaskAssignment,\n-            warmupTaskAssignment,\n-            standbyTaskAssignment,\n-            statelessActiveTaskAssignment\n+            configs.maxWarmupReplicas\n         );\n+    }\n \n-        final ValidClientsByTaskLoadQueue<UUID> clientsByStandbyTaskLoad =\n-            new ValidClientsByTaskLoadQueue<>(\n-                getClientPriorityQueueByTaskLoad(allTaskAssignmentMaps),\n-                allTaskAssignmentMaps\n+    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n+        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad =\n+            new ValidClientsByTaskLoadQueue(\n+                clientStates,\n+                (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n             );\n+        standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n             final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = clientsByStandbyTaskLoad.poll(task, numRemainingStandbys);\n+            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n             for (final UUID client : clients) {\n-                standbyTaskAssignment.get(client).add(task);\n+                clientStates.get(client).assignStandby(task);\n             }\n-            clientsByStandbyTaskLoad.offer(clients);\n+            standbyTaskClientsByTaskLoad.offerAll(clients);\n+\n             final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < configs.numStandbyReplicas) {\n+            if (numStandbysAssigned < numRemainingStandbys) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                    configs.numStandbyReplicas - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n             }\n         }\n+    }\n \n-        // ---------------- Stateless Active Tasks ---------------- //\n-\n-        final PriorityQueue<UUID> statelessActiveTaskClientsQueue = getClientPriorityQueueByTaskLoad(allTaskAssignmentMaps);\n+    private void assignStatelessActiveTasks() {\n+        final PriorityQueue<UUID> statelessActiveTaskClientsQueue = getClientPriorityQueueByTaskLoad(clientStates);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTczNDQ2OA==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411734468", "bodyText": "Definitely", "author": "ableegoldman", "createdAt": "2020-04-20T22:29:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc0OTc4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDc1MDM4Ng==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r410750386", "bodyText": "Yay!", "author": "vvcephei", "createdAt": "2020-04-18T21:03:01Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java", "diffHunk": "@@ -266,7 +265,6 @@ public void assignmentShouldConvergeAfterAddingNode() {\n         verifyValidAssignment(numStandbyReplicas, harness);\n     }\n \n-    @Ignore // Adding this failing test before adding the code that fixes it", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTM2NjExMQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411366111", "bodyText": "I love it when a comment gets killed by a meaningful method name!", "author": "cadonna", "createdAt": "2020-04-20T13:13:44Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -89,95 +88,72 @@ public boolean assign() {\n             return false;\n         }\n \n-        final Map<UUID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap(sortedClients);\n-        final Map<UUID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap(sortedClients);\n-        final Map<UUID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap(sortedClients);\n+        final Map<TaskId, Integer> tasksToRemainingStandbys =\n+            statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        // ---------------- Stateful Active Tasks ---------------- //\n+        final boolean followupRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTM2ODk5MQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411368991", "bodyText": "prop:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final Map<UUID, List<TaskId>> statefulActiveTaskAssignment =\n          \n          \n            \n                        new DefaultBalancedAssignor().assign(\n          \n          \n            \n                            sortedClients,\n          \n          \n            \n                            statefulTasks,\n          \n          \n            \n                            clientsToNumberOfThreads,\n          \n          \n            \n                            configs.balanceFactor);\n          \n          \n            \n                    final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n          \n          \n            \n                        sortedClients,\n          \n          \n            \n                        statefulTasks,\n          \n          \n            \n                        clientsToNumberOfThreads,\n          \n          \n            \n                        configs.balanceFactor\n          \n          \n            \n                    );", "author": "cadonna", "createdAt": "2020-04-20T13:17:49Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -89,95 +88,72 @@ public boolean assign() {\n             return false;\n         }\n \n-        final Map<UUID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap(sortedClients);\n-        final Map<UUID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap(sortedClients);\n-        final Map<UUID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap(sortedClients);\n+        final Map<TaskId, Integer> tasksToRemainingStandbys =\n+            statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        // ---------------- Stateful Active Tasks ---------------- //\n+        final boolean followupRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n \n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment =\n-            new DefaultStateConstrainedBalancedAssignor().assign(\n-                statefulTasksToRankedCandidates,\n-                configs.balanceFactor,\n-                sortedClients,\n-                clientsToNumberOfThreads,\n-                tasksToCaughtUpClients\n-            );\n+        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+\n+        assignStatelessActiveTasks();\n \n-        // ---------------- Warmup Replica Tasks ---------------- //\n+        return followupRebalanceNeeded;\n+    }\n \n-        final Map<UUID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n+        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment =\n             new DefaultBalancedAssignor().assign(\n                 sortedClients,\n                 statefulTasks,\n                 clientsToNumberOfThreads,\n                 configs.balanceFactor);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTM4MzcyNQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411383725", "bodyText": "prop:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final ValidClientsByTaskLoadQueue clientsByTaskLoad =\n          \n          \n            \n                        new ValidClientsByTaskLoadQueue(\n          \n          \n            \n                            clientStates,\n          \n          \n            \n                            (client, task) -> taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)\n          \n          \n            \n                        );\n          \n          \n            \n                    final ValidClientsByTaskLoadQueue clientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n          \n          \n            \n                        clientStates,\n          \n          \n            \n                        (client, task) -> taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)\n          \n          \n            \n                    );", "author": "cadonna", "createdAt": "2020-04-20T13:38:08Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -16,128 +16,94 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClient;\n+\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n-import java.util.Set;\n import java.util.SortedSet;\n+import java.util.TreeSet;\n import java.util.UUID;\n import org.apache.kafka.streams.processor.TaskId;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n public class TaskMovement {\n-    private static final Logger log = LoggerFactory.getLogger(TaskMovement.class);\n-\n     final TaskId task;\n-    final UUID source;\n-    final UUID destination;\n+    private final UUID destination;\n \n-    TaskMovement(final TaskId task, final UUID source, final UUID destination) {\n+    TaskMovement(final TaskId task, final UUID destination) {\n         this.task = task;\n-        this.source = source;\n         this.destination = destination;\n     }\n \n-    @Override\n-    public boolean equals(final Object o) {\n-        if (this == o) {\n-            return true;\n-        }\n-        if (o == null || getClass() != o.getClass()) {\n-            return false;\n-        }\n-        final TaskMovement movement = (TaskMovement) o;\n-        return Objects.equals(task, movement.task) &&\n-                   Objects.equals(source, movement.source) &&\n-                   Objects.equals(destination, movement.destination);\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-        return Objects.hash(task, source, destination);\n-    }\n-\n     /**\n-     * Computes the movement of tasks from the state constrained to the balanced assignment, up to the configured\n-     * {@code max.warmup.replicas}. A movement corresponds to a warmup replica on the destination client, with\n-     * a few exceptional cases:\n-     * <p>\n-     * 1. Tasks whose destination clients are caught-up, or whose source clients are not caught-up, will be moved\n-     * immediately from the source to the destination in the state constrained assignment\n-     * 2. Tasks whose destination client previously had this task as a standby will not be counted towards the total\n-     * {@code max.warmup.replicas}. Instead they will be counted against that task's total {@code num.standby.replicas}.\n-     *\n-     * @param statefulActiveTaskAssignment the initial, state constrained assignment, with the source clients\n-     * @param balancedStatefulActiveTaskAssignment the final, balanced assignment, with the destination clients\n-     * @return list of the task movements from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @return whether any warmup replicas were assigned\n      */\n-    static List<TaskMovement> getMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                           final Map<UUID, List<TaskId>> balancedStatefulActiveTaskAssignment,\n-                                           final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n-                                           final Map<UUID, ClientState> clientStates,\n-                                           final Map<TaskId, Integer> tasksToRemainingStandbys,\n-                                           final int maxWarmupReplicas) {\n-        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n-            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n-        }\n+    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n+                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+                                       final Map<UUID, ClientState> clientStates,\n+                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                       final int maxWarmupReplicas) {\n+        boolean warmupReplicasAssigned = false;\n+\n+        final ValidClientsByTaskLoadQueue clientsByTaskLoad =\n+            new ValidClientsByTaskLoadQueue(\n+                clientStates,\n+                (client, task) -> taskIsCaughtUpOnClient(task, client, tasksToCaughtUpClients)\n+            );", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQwNDM0NQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411404345", "bodyText": "Q: I do not understand why we need uniqueClients here? Would it not suffice to check for clientsByTaskLoad.contains(client)?", "author": "cadonna", "createdAt": "2020-04-20T14:03:43Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ValidClientsByTaskLoadQueue.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.function.BiFunction;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+/**\n+ * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n+ */\n+class ValidClientsByTaskLoadQueue {\n+    private final PriorityQueue<UUID> clientsByTaskLoad;\n+    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+    private final Set<UUID> uniqueClients = new HashSet<>();\n+\n+    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n+                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n+        clientsByTaskLoad = getClientPriorityQueueByTaskLoad(clientStates);\n+        this.validClientCriteria = validClientCriteria;\n+    }\n+\n+    /**\n+=     * @return the next least loaded client that satisfies the given criteria, or null if none do\n+     */\n+    UUID poll(final TaskId task) {\n+        final List<UUID> validClient = poll(task, 1);\n+        return validClient.isEmpty() ? null : validClient.get(0);\n+    }\n+\n+    /**\n+     * @return the next N <= {@code numClientsPerTask} clients in the underlying priority queue that are valid\n+     * candidates for the given task\n+     */\n+    List<UUID> poll(final TaskId task, final int numClients) {\n+        final List<UUID> nextLeastLoadedValidClients = new LinkedList<>();\n+        final Set<UUID> invalidPolledClients = new HashSet<>();\n+        while (nextLeastLoadedValidClients.size() < numClients) {\n+            UUID candidateClient;\n+            while (true) {\n+                candidateClient = clientsByTaskLoad.poll();\n+                if (candidateClient == null) {\n+                    offerAll(invalidPolledClients);\n+                    return nextLeastLoadedValidClients;\n+                }\n+\n+                if (validClientCriteria.apply(candidateClient, task)) {\n+                    nextLeastLoadedValidClients.add(candidateClient);\n+                    break;\n+                } else {\n+                    invalidPolledClients.add(candidateClient);\n+                }\n+            }\n+        }\n+        offerAll(invalidPolledClients);\n+        return nextLeastLoadedValidClients;\n+    }\n+\n+    void offerAll(final Collection<UUID> clients) {\n+        for (final UUID client : clients) {\n+            offer(client);\n+        }\n+    }\n+\n+    void offer(final UUID client) {\n+        if (uniqueClients.contains(client)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY3MjA0MQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411672041", "bodyText": "I think it's just a computer-sciencey matter of principle. clientsByTaskLoad is a linear collection, so every offer would become O(n) if we did a contains call on it every time. Right now, it's only O(n) when we need to remove the prior record for the same client, and O(log(n)) otherwise.\nDoes it really matter? I'm not sure.", "author": "vvcephei", "createdAt": "2020-04-20T20:32:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQwNDM0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjA0NjQyOQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r412046429", "bodyText": "Got it!\nThat means, we get O(n) for all cases where we first poll() and then offer() the same clients because those clients are contained in uniqueClients, i.e.:\n\nHighAvailabilityTaskAssignor@155\nHighAvailabilityTaskAssignor@131\nTaskMovement@94\nValidClientsByTaskLoadQueue@76\nValidClientsByTaskLoadQueue@88\n\nThose are the majority of the calls to offer() and offerAll(). Additionally, the last two occurrences in the list are called in each call to poll(). In poll(), if the top does not satisfy the criteria it is added to invalidPolledClients which then is added with offerAll(). For each element of invalidPolledClients the whole queue clientsByTaskLoad is scanned, since each element is contained in uniqueClients but not in clientsByTaskLoad. This results in O(n^2).\nAFAIU, we need the uniqueness check because of TaskMovement@99.\nIf we update uniqueClients also in poll(), we would avoid O(n^2) for poll() and restrict O(n) to the case at TaskMovement@99.\n\nDoes it really matter?\n\nI'm not also sure. Performance test would be the only way to tell.", "author": "cadonna", "createdAt": "2020-04-21T09:58:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQwNDM0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQyNDQ0NQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r412424445", "bodyText": "Gah! You're right. We should also remove the client from uniqueClients when we poll.", "author": "vvcephei", "createdAt": "2020-04-21T19:16:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQwNDM0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQyNDg2MQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r412424861", "bodyText": "@cadonna you're right, I forgot to remove from uniqueClients in poll. Good catch", "author": "ableegoldman", "createdAt": "2020-04-21T19:17:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQwNDM0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ1NzU3NQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411457575", "bodyText": "prop:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad =\n          \n          \n            \n                        new ValidClientsByTaskLoadQueue(\n          \n          \n            \n                            clientStates,\n          \n          \n            \n                            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n          \n          \n            \n                        );\n          \n          \n            \n                    final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n          \n          \n            \n                        clientStates,\n          \n          \n            \n                        (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n          \n          \n            \n                    );", "author": "cadonna", "createdAt": "2020-04-20T15:10:25Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -89,95 +88,72 @@ public boolean assign() {\n             return false;\n         }\n \n-        final Map<UUID, List<TaskId>> warmupTaskAssignment = initializeEmptyTaskAssignmentMap(sortedClients);\n-        final Map<UUID, List<TaskId>> standbyTaskAssignment = initializeEmptyTaskAssignmentMap(sortedClients);\n-        final Map<UUID, List<TaskId>> statelessActiveTaskAssignment = initializeEmptyTaskAssignmentMap(sortedClients);\n+        final Map<TaskId, Integer> tasksToRemainingStandbys =\n+            statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        // ---------------- Stateful Active Tasks ---------------- //\n+        final boolean followupRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n \n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment =\n-            new DefaultStateConstrainedBalancedAssignor().assign(\n-                statefulTasksToRankedCandidates,\n-                configs.balanceFactor,\n-                sortedClients,\n-                clientsToNumberOfThreads,\n-                tasksToCaughtUpClients\n-            );\n+        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+\n+        assignStatelessActiveTasks();\n \n-        // ---------------- Warmup Replica Tasks ---------------- //\n+        return followupRebalanceNeeded;\n+    }\n \n-        final Map<UUID, List<TaskId>> balancedStatefulActiveTaskAssignment =\n+    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n+        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment =\n             new DefaultBalancedAssignor().assign(\n                 sortedClients,\n                 statefulTasks,\n                 clientsToNumberOfThreads,\n                 configs.balanceFactor);\n \n-        final Map<TaskId, Integer> tasksToRemainingStandbys =\n-            statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n-\n-        final List<TaskMovement> movements = getMovements(\n+        return assignTaskMovements(\n             statefulActiveTaskAssignment,\n-            balancedStatefulActiveTaskAssignment,\n             tasksToCaughtUpClients,\n             clientStates,\n             tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas);\n-\n-        for (final TaskMovement movement : movements) {\n-            warmupTaskAssignment.get(movement.destination).add(movement.task);\n-        }\n-\n-        // ---------------- Standby Replica Tasks ---------------- //\n-\n-        final List<Map<UUID, List<TaskId>>> allTaskAssignmentMaps = asList(\n-            statefulActiveTaskAssignment,\n-            warmupTaskAssignment,\n-            standbyTaskAssignment,\n-            statelessActiveTaskAssignment\n+            configs.maxWarmupReplicas\n         );\n+    }\n \n-        final ValidClientsByTaskLoadQueue<UUID> clientsByStandbyTaskLoad =\n-            new ValidClientsByTaskLoadQueue<>(\n-                getClientPriorityQueueByTaskLoad(allTaskAssignmentMaps),\n-                allTaskAssignmentMaps\n+    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n+        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad =\n+            new ValidClientsByTaskLoadQueue(\n+                clientStates,\n+                (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n             );", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ2ODcyMw==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r411468723", "bodyText": "req: Please add unit tests for this class.", "author": "cadonna", "createdAt": "2020-04-20T15:24:21Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ValidClientsByTaskLoadQueue.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.function.BiFunction;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+/**\n+ * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n+ */\n+class ValidClientsByTaskLoadQueue {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "42b300e7c21d31ac08354ab3419c96181339a37a", "url": "https://github.com/apache/kafka/commit/42b300e7c21d31ac08354ab3419c96181339a37a", "message": "fix bugs in previousAssignmentIsValid", "committedDate": "2020-04-20T22:26:01Z", "type": "commit"}, {"oid": "22a670750c1998b2ada9d732dba3acb4360dcf9e", "url": "https://github.com/apache/kafka/commit/22a670750c1998b2ada9d732dba3acb4360dcf9e", "message": "random convergence test is passing", "committedDate": "2020-04-20T22:26:01Z", "type": "commit"}, {"oid": "957ee75f9fea26907609dc129697be70ecff3a73", "url": "https://github.com/apache/kafka/commit/957ee75f9fea26907609dc129697be70ecff3a73", "message": "remove unused classes", "committedDate": "2020-04-20T22:26:01Z", "type": "commit"}, {"oid": "c2ba04e79c5dd0fe92a92d3d03717c22481a2739", "url": "https://github.com/apache/kafka/commit/c2ba04e79c5dd0fe92a92d3d03717c22481a2739", "message": "clean up TaskMovementTest", "committedDate": "2020-04-20T22:26:01Z", "type": "commit"}, {"oid": "73aa71a13bbd02ab915b0a7996a1fcd0549cf443", "url": "https://github.com/apache/kafka/commit/73aa71a13bbd02ab915b0a7996a1fcd0549cf443", "message": "fix broken HATAT test", "committedDate": "2020-04-20T22:26:01Z", "type": "commit"}, {"oid": "ae3aebf2b2a4b0600d66ad6480023e68410fa564", "url": "https://github.com/apache/kafka/commit/ae3aebf2b2a4b0600d66ad6480023e68410fa564", "message": "fix remaining HATAT tests", "committedDate": "2020-04-20T22:26:01Z", "type": "commit"}, {"oid": "1e5cddb00c1535406a3ac85da19c1b16d66148a7", "url": "https://github.com/apache/kafka/commit/1e5cddb00c1535406a3ac85da19c1b16d66148a7", "message": "checkstyle", "committedDate": "2020-04-20T22:26:01Z", "type": "commit"}, {"oid": "e3271af6a83daded756be70e8026cb92ad701fa3", "url": "https://github.com/apache/kafka/commit/e3271af6a83daded756be70e8026cb92ad701fa3", "message": "fix up SPAT tests", "committedDate": "2020-04-20T22:26:01Z", "type": "commit"}, {"oid": "e0071faf053d48961272d0c8c9d4a1ee2c59d074", "url": "https://github.com/apache/kafka/commit/e0071faf053d48961272d0c8c9d4a1ee2c59d074", "message": "first set of github reviews", "committedDate": "2020-04-20T22:26:01Z", "type": "commit"}, {"oid": "8ebc11ce1bd2e3185b62454598a0a0a6c83503ef", "url": "https://github.com/apache/kafka/commit/8ebc11ce1bd2e3185b62454598a0a0a6c83503ef", "message": "remove ignore annotation from convergence tests", "committedDate": "2020-04-20T22:26:01Z", "type": "commit"}, {"oid": "2709ba0245cddcf4a6c4051050f34923f99cd36a", "url": "https://github.com/apache/kafka/commit/2709ba0245cddcf4a6c4051050f34923f99cd36a", "message": "remove unused ignore import", "committedDate": "2020-04-20T22:26:01Z", "type": "commit"}, {"oid": "864a539e33f5927d00d0914901d918da78662260", "url": "https://github.com/apache/kafka/commit/864a539e33f5927d00d0914901d918da78662260", "message": "first set of github review comments", "committedDate": "2020-04-21T03:30:46Z", "type": "commit"}, {"oid": "864a539e33f5927d00d0914901d918da78662260", "url": "https://github.com/apache/kafka/commit/864a539e33f5927d00d0914901d918da78662260", "message": "first set of github review comments", "committedDate": "2020-04-21T03:30:46Z", "type": "forcePushed"}, {"oid": "67d20fdc83eb780157be6f1aecd20932b94c916b", "url": "https://github.com/apache/kafka/commit/67d20fdc83eb780157be6f1aecd20932b94c916b", "message": "add tests, helper method for ClientState in tests", "committedDate": "2020-04-21T04:12:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjA0NzMzMg==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r412047332", "bodyText": "prop: I would not add the client if it is already contained in the set.", "author": "cadonna", "createdAt": "2020-04-21T09:59:32Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ValidClientsByTaskLoadQueue.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.function.BiFunction;\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+/**\n+ * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n+ */\n+class ValidClientsByTaskLoadQueue {\n+\n+    private final PriorityQueue<UUID> clientsByTaskLoad;\n+    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+    private final Set<UUID> uniqueClients = new HashSet<>();\n+\n+    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n+                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n+        this.validClientCriteria = validClientCriteria;\n+\n+        clientsByTaskLoad = new PriorityQueue<>(\n+            (client, other) -> {\n+                final double clientTaskLoad = clientStates.get(client).taskLoad();\n+                final double otherTaskLoad = clientStates.get(other).taskLoad();\n+                if (clientTaskLoad < otherTaskLoad) {\n+                    return -1;\n+                } else if (clientTaskLoad > otherTaskLoad) {\n+                    return 1;\n+                } else {\n+                    return client.compareTo(other);\n+                }\n+            });\n+    }\n+\n+    /**\n+     * @return the next least loaded client that satisfies the given criteria, or null if none do\n+     */\n+    UUID poll(final TaskId task) {\n+        final List<UUID> validClient = poll(task, 1);\n+        return validClient.isEmpty() ? null : validClient.get(0);\n+    }\n+\n+    /**\n+     * @return the next N <= {@code numClientsPerTask} clients in the underlying priority queue that are valid candidates for the given task\n+     */\n+    List<UUID> poll(final TaskId task, final int numClients) {\n+        final List<UUID> nextLeastLoadedValidClients = new LinkedList<>();\n+        final Set<UUID> invalidPolledClients = new HashSet<>();\n+        while (nextLeastLoadedValidClients.size() < numClients) {\n+            UUID candidateClient;\n+            while (true) {\n+                candidateClient = clientsByTaskLoad.poll();\n+                if (candidateClient == null) {\n+                    offerAll(invalidPolledClients);\n+                    return nextLeastLoadedValidClients;\n+                }\n+\n+                if (validClientCriteria.apply(candidateClient, task)) {\n+                    nextLeastLoadedValidClients.add(candidateClient);\n+                    break;\n+                } else {\n+                    invalidPolledClients.add(candidateClient);\n+                }\n+            }\n+        }\n+        offerAll(invalidPolledClients);\n+        return nextLeastLoadedValidClients;\n+    }\n+\n+    void offerAll(final Collection<UUID> clients) {\n+        for (final UUID client : clients) {\n+            offer(client);\n+        }\n+    }\n+\n+    void offer(final UUID client) {\n+        if (uniqueClients.contains(client)) {\n+            clientsByTaskLoad.remove(client);\n+        }\n+        clientsByTaskLoad.offer(client);\n+        uniqueClients.add(client);", "originalCommit": "67d20fdc83eb780157be6f1aecd20932b94c916b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjA4MTEyNQ==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r412081125", "bodyText": "This question from my previous review went unnoticed (or you did simply not care ;-)).\n\nQ: Why do we even care at all whether the task was running on the client? What if we just assign a real stand-by task if we have a spare one?", "author": "cadonna", "createdAt": "2020-04-21T10:53:14Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -16,128 +16,103 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClientOrNoCaughtUpClientsExist;\n+\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n-import java.util.Set;\n import java.util.SortedSet;\n+import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.concurrent.atomic.AtomicInteger;\n import org.apache.kafka.streams.processor.TaskId;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-public class TaskMovement {\n-    private static final Logger log = LoggerFactory.getLogger(TaskMovement.class);\n \n-    final TaskId task;\n-    final UUID source;\n-    final UUID destination;\n+class TaskMovement {\n+    private final TaskId task;\n+    private final UUID destination;\n+    private final SortedSet<UUID> caughtUpClients;\n \n-    TaskMovement(final TaskId task, final UUID source, final UUID destination) {\n+    private TaskMovement(final TaskId task, final UUID destination, final SortedSet<UUID> caughtUpClients) {\n         this.task = task;\n-        this.source = source;\n         this.destination = destination;\n-    }\n+        this.caughtUpClients = caughtUpClients;\n \n-    @Override\n-    public boolean equals(final Object o) {\n-        if (this == o) {\n-            return true;\n-        }\n-        if (o == null || getClass() != o.getClass()) {\n-            return false;\n+        if (caughtUpClients == null || caughtUpClients.isEmpty()) {\n+            throw new IllegalStateException(\"Should not attempt to move a task if no caught up clients exist\");\n         }\n-        final TaskMovement movement = (TaskMovement) o;\n-        return Objects.equals(task, movement.task) &&\n-                   Objects.equals(source, movement.source) &&\n-                   Objects.equals(destination, movement.destination);\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-        return Objects.hash(task, source, destination);\n     }\n \n     /**\n-     * Computes the movement of tasks from the state constrained to the balanced assignment, up to the configured\n-     * {@code max.warmup.replicas}. A movement corresponds to a warmup replica on the destination client, with\n-     * a few exceptional cases:\n-     * <p>\n-     * 1. Tasks whose destination clients are caught-up, or whose source clients are not caught-up, will be moved\n-     * immediately from the source to the destination in the state constrained assignment\n-     * 2. Tasks whose destination client previously had this task as a standby will not be counted towards the total\n-     * {@code max.warmup.replicas}. Instead they will be counted against that task's total {@code num.standby.replicas}.\n-     *\n-     * @param statefulActiveTaskAssignment the initial, state constrained assignment, with the source clients\n-     * @param balancedStatefulActiveTaskAssignment the final, balanced assignment, with the destination clients\n-     * @return list of the task movements from statefulActiveTaskAssignment to balancedStatefulActiveTaskAssignment\n+     * @return whether any warmup replicas were assigned\n      */\n-    static List<TaskMovement> getMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                           final Map<UUID, List<TaskId>> balancedStatefulActiveTaskAssignment,\n-                                           final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n-                                           final Map<UUID, ClientState> clientStates,\n-                                           final Map<TaskId, Integer> tasksToRemainingStandbys,\n-                                           final int maxWarmupReplicas) {\n-        if (statefulActiveTaskAssignment.size() != balancedStatefulActiveTaskAssignment.size()) {\n-            throw new IllegalStateException(\"Tried to compute movements but assignments differ in size.\");\n-        }\n+    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n+                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+                                       final Map<UUID, ClientState> clientStates,\n+                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                       final int maxWarmupReplicas) {\n+        boolean warmupReplicasAssigned = false;\n+\n+        final ValidClientsByTaskLoadQueue clientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+            clientStates,\n+            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)\n+        );\n \n-        final Map<TaskId, UUID> taskToDestinationClient = new HashMap<>();\n-        for (final Map.Entry<UUID, List<TaskId>> clientEntry : balancedStatefulActiveTaskAssignment.entrySet()) {\n-            final UUID destination = clientEntry.getKey();\n-            for (final TaskId task : clientEntry.getValue()) {\n-                taskToDestinationClient.put(task, destination);\n+        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(\n+            (movement, other) -> {\n+                final int numCaughtUpClients = movement.caughtUpClients.size();\n+                final int otherNumCaughtUpClients = other.caughtUpClients.size();\n+                if (numCaughtUpClients != otherNumCaughtUpClients) {\n+                    return Integer.compare(numCaughtUpClients, otherNumCaughtUpClients);\n+                } else {\n+                    return movement.task.compareTo(other.task);\n+                }\n             }\n+        );\n+\n+        for (final Map.Entry<UUID, List<TaskId>> assignmentEntry : statefulActiveTaskAssignment.entrySet()) {\n+            final UUID client = assignmentEntry.getKey();\n+            final ClientState state = clientStates.get(client);\n+            for (final TaskId task : assignmentEntry.getValue()) {\n+                if (taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)) {\n+                    state.assignActive(task);\n+                } else {\n+                    final TaskMovement taskMovement = new TaskMovement(task, client, tasksToCaughtUpClients.get(task));\n+                    taskMovements.add(taskMovement);\n+                }\n+            }\n+            clientsByTaskLoad.offer(client);\n         }\n \n-        int remainingAllowedWarmupReplicas = maxWarmupReplicas;\n-        final List<TaskMovement> movements = new LinkedList<>();\n-        for (final Map.Entry<UUID, List<TaskId>> sourceClientEntry : statefulActiveTaskAssignment.entrySet()) {\n-            final UUID source = sourceClientEntry.getKey();\n+        final AtomicInteger remainingWarmupReplicas = new AtomicInteger(maxWarmupReplicas);\n+        for (final TaskMovement movement : taskMovements) {\n+            final UUID sourceClient = clientsByTaskLoad.poll(movement.task);\n+            if (sourceClient == null) {\n+                throw new IllegalStateException(\"Tried to move task to caught-up client but none exist\");\n+            }\n \n-            final Iterator<TaskId> sourceClientTasksIterator = sourceClientEntry.getValue().iterator();\n-            while (sourceClientTasksIterator.hasNext()) {\n-                final TaskId task = sourceClientTasksIterator.next();\n-                final UUID destination = taskToDestinationClient.get(task);\n-                if (destination == null) {\n-                    log.error(\"Task {} is assigned to client {} in initial assignment but has no owner in the final \" +\n-                                  \"balanced assignment.\", task, source);\n-                    throw new IllegalStateException(\"Found task in initial assignment that was not assigned in the final.\");\n-                } else if (!source.equals(destination)) {\n-                    if (destinationClientIsCaughtUp(task, destination, tasksToCaughtUpClients)) {\n-                        sourceClientTasksIterator.remove();\n-                        statefulActiveTaskAssignment.get(destination).add(task);\n-                    } else {\n-                        if (clientStates.get(destination).prevStandbyTasks().contains(task)\n-                                && tasksToRemainingStandbys.get(task) > 0\n-                        ) {\n-                            decrementRemainingStandbys(task, tasksToRemainingStandbys);\n-                        } else {\n-                            --remainingAllowedWarmupReplicas;\n-                        }\n+            final ClientState sourceClientState = clientStates.get(sourceClient);\n+            sourceClientState.assignActive(movement.task);\n+            clientsByTaskLoad.offer(sourceClient);\n \n-                        movements.add(new TaskMovement(task, source, destination));\n-                        if (remainingAllowedWarmupReplicas == 0) {\n-                            return movements;\n-                        }\n-                    }\n-                }\n+            final ClientState destinationClientState = clientStates.get(movement.destination);\n+            if (shouldAssignWarmupReplica(movement.task, destinationClientState, remainingWarmupReplicas, tasksToRemainingStandbys)) {\n+                destinationClientState.assignStandby(movement.task);\n+                clientsByTaskLoad.offer(movement.destination);\n+                warmupReplicasAssigned = true;\n             }\n         }\n-        return movements;\n+        return warmupReplicasAssigned;\n     }\n \n-    private static boolean destinationClientIsCaughtUp(final TaskId task,\n-                                                       final UUID destination,\n-                                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients) {\n-        final Set<UUID> caughtUpClients = tasksToCaughtUpClients.get(task);\n-        return caughtUpClients != null && caughtUpClients.contains(destination);\n+    private static boolean shouldAssignWarmupReplica(final TaskId task,\n+                                                     final ClientState destinationClientState,\n+                                                     final AtomicInteger remainingWarmupReplicas,\n+                                                     final Map<TaskId, Integer> tasksToRemainingStandbys) {\n+        if (destinationClientState.previousAssignedTasks().contains(task) && tasksToRemainingStandbys.get(task) > 0) {", "originalCommit": "67d20fdc83eb780157be6f1aecd20932b94c916b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQyMjA4Mg==", "url": "https://github.com/apache/kafka/pull/8497#discussion_r412422082", "bodyText": "I think I answered this already. We're trying not to decrease the overall availability the standbys are providing, which could happen if we drop a caught-up standby in order to warm up an empty node. We can certainly do better than what we do now, which is not very efficient in terms of task movement, but I think it's good enough for this PR.", "author": "vvcephei", "createdAt": "2020-04-21T19:12:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjA4MTEyNQ=="}], "type": "inlineReview"}, {"oid": "2ab532fa1c932b177bc82a08c75374228dabae77", "url": "https://github.com/apache/kafka/commit/2ab532fa1c932b177bc82a08c75374228dabae77", "message": "github review prop", "committedDate": "2020-04-21T19:14:24Z", "type": "commit"}, {"oid": "7500412b14201520525b5027849cdd79585c1751", "url": "https://github.com/apache/kafka/commit/7500412b14201520525b5027849cdd79585c1751", "message": "queue fix", "committedDate": "2020-04-21T19:19:13Z", "type": "commit"}, {"oid": "2a6dc0b002bf6e6f46d9a910d0358cbcf81dd1f7", "url": "https://github.com/apache/kafka/commit/2a6dc0b002bf6e6f46d9a910d0358cbcf81dd1f7", "message": "wrap in method", "committedDate": "2020-04-21T19:20:43Z", "type": "commit"}]}