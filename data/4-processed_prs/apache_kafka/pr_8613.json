{"pr_number": 8613, "pr_title": "KAFKA-6145: Set HighAvailabilityTaskAssignor as default in streams_upgrade_test.py", "pr_createdAt": "2020-05-04T14:44:18Z", "pr_url": "https://github.com/apache/kafka/pull/8613", "timeline": [{"oid": "a6e1f292b82a17bad8c1e8f249d37f869b347894", "url": "https://github.com/apache/kafka/commit/a6e1f292b82a17bad8c1e8f249d37f869b347894", "message": "KAFKA-6145: Set HighAvailabilityTaskAssignor as default in streams_upgrade_test.py\n\nThis PR sets HighAvailabilityTaskAssignor as default task assignor in\nstreams_upgrade_test.py. The verification of the test needed to be\nmodified to because the HighAvailabilityTaskAssignor surfaced a flakiness\nin the test. More precisely, the verifications assume that the last\nclient that is bounced joins the group before the other two clients\nare able to rebalance without the last client. This assumption does not\nalways hold.", "committedDate": "2020-05-04T14:43:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ5NjcwMw==", "url": "https://github.com/apache/kafka/pull/8613#discussion_r419496703", "bodyText": "This verification is only true if the two other processors haven't rebalanced before the processor that bounced last re-joins the group. If the rebalance occurs, the commonly supported version is already at 8 when the last processor joins.\nActually, the test test_version_probing_upgrade is independent of the used task assignor, but this issue was surfaced by the HighAvailabilityTaskAssignor but not by the StickyTaskAssignor. I cannot say for sure why.\nIMO, removing this verification should be OK, since afterwards we check whether the processors have synchronized generations which means that all three processors successfully joined the group in the end. The state that we do not explicitly verify anymore is the transient state where version 7 is currently used, but all processor are able to use version 8.", "author": "cadonna", "createdAt": "2020-05-04T14:52:51Z", "path": "tests/kafkatest/tests/streams/streams_upgrade_test.py", "diffHunk": "@@ -532,20 +532,22 @@ def do_rolling_bounce(self, processor, counter, current_generation):\n                         log_monitor.wait_until(\"Sent a version 8 subscription and got version 7 assignment back (successful version probing). Downgrade subscription metadata to commonly supported version 7 and trigger new rebalance.\",\n                                                timeout_sec=60,\n                                                err_msg=\"Could not detect 'successful version probing' at upgrading node \" + str(node.account))\n-                    else:\n-                        log_monitor.wait_until(\"Sent a version 8 subscription and got version 7 assignment back (successful version probing). Downgrade subscription metadata to commonly supported version 8 and trigger new rebalance.\",", "originalCommit": "a6e1f292b82a17bad8c1e8f249d37f869b347894", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY5MzY0NA==", "url": "https://github.com/apache/kafka/pull/8613#discussion_r419693644", "bodyText": "Thanks @cadonna ; I agree. This test should just be verifying that we first converge on 7, and then that we converge on 8.", "author": "vvcephei", "createdAt": "2020-05-04T20:02:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ5NjcwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY5MzgyNw==", "url": "https://github.com/apache/kafka/pull/8613#discussion_r419693827", "bodyText": "We can actually just delete these lines now.", "author": "vvcephei", "createdAt": "2020-05-04T20:02:48Z", "path": "tests/kafkatest/tests/streams/streams_upgrade_test.py", "diffHunk": "@@ -305,11 +305,11 @@ def test_version_probing_upgrade(self):\n         self.driver.disable_auto_terminate()\n         # TODO KIP-441: consider rewriting the test for HighAvailabilityTaskAssignor\n         self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)\n-        self.processor1.set_config(\"internal.task.assignor.class\", \"org.apache.kafka.streams.processor.internals.assignment.StickyTaskAssignor\")\n+        self.processor1.set_config(\"internal.task.assignor.class\", \"org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor\")", "originalCommit": "a6e1f292b82a17bad8c1e8f249d37f869b347894", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY5NTUyMg==", "url": "https://github.com/apache/kafka/pull/8613#discussion_r419695522", "bodyText": "I know that this check was here in some fashion before, but I'm drawing a blank on why we need to verify this log line. It seems like just checking the version number logs and nothing else would be the key to a long and happy life.", "author": "vvcephei", "createdAt": "2020-05-04T20:05:59Z", "path": "tests/kafkatest/tests/streams/streams_upgrade_test.py", "diffHunk": "@@ -532,20 +532,22 @@ def do_rolling_bounce(self, processor, counter, current_generation):\n                         log_monitor.wait_until(\"Sent a version 8 subscription and got version 7 assignment back (successful version probing). Downgrade subscription metadata to commonly supported version 7 and trigger new rebalance.\",\n                                                timeout_sec=60,\n                                                err_msg=\"Could not detect 'successful version probing' at upgrading node \" + str(node.account))\n-                    else:\n-                        log_monitor.wait_until(\"Sent a version 8 subscription and got version 7 assignment back (successful version probing). Downgrade subscription metadata to commonly supported version 8 and trigger new rebalance.\",\n+                        log_monitor.wait_until(\"Detected that the assignor requested a rebalance. Rejoining the consumer group to trigger a new rebalance.\",", "originalCommit": "a6e1f292b82a17bad8c1e8f249d37f869b347894", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTgyOTAyMA==", "url": "https://github.com/apache/kafka/pull/8613#discussion_r419829020", "bodyText": "I think the idea is to verify that the actual version probing rebalance takes place, ie that the partition assignor actually handles the version probing once it's detected. And that it signals to the stream thread which also handles it correctly in turn. But idk -- I've probably broken and fixed the version probing test 2 or 3 times now due to this one line in particular.\nSo, I'd be happy to see it go. I probably have too much bad history to make an unbiased call here though \ud83d\ude04", "author": "ableegoldman", "createdAt": "2020-05-05T02:11:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY5NTUyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA1Mjg1Mw==", "url": "https://github.com/apache/kafka/pull/8613#discussion_r420052853", "bodyText": "We can leave it because it verifies whether the assignment was triggered in the assignor, which is better than nothing. However, it does not give us any guarantee that the rebalance took actually place.\nI guess what we really would need is a way to check if a group stabilized and if the assignment is valid. We try to do that by verifying that the generations of the processors are synced. However, I ran into cases where all processors had the same generation, but one processor did not have any tasks assigned. So we would actually need to check if they have the highest generation in sync across the processors AND if all processors have at least one task assigned (AND if all tasks were assigned).", "author": "cadonna", "createdAt": "2020-05-05T11:56:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY5NTUyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDMwNjcwMQ==", "url": "https://github.com/apache/kafka/pull/8613#discussion_r420306701", "bodyText": "Thanks, all. This doesn't seem like the best way to verify what we're trying to verify, but it also seems about the same as before. I'm happy to leave this here for now.\nIf/when the test breaks again, I'd prefer for us to put in a more reliable and direct mechanism.", "author": "vvcephei", "createdAt": "2020-05-05T18:07:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY5NTUyMg=="}], "type": "inlineReview"}, {"oid": "b80ef741379b3044715738f16648bc0b35370fed", "url": "https://github.com/apache/kafka/commit/b80ef741379b3044715738f16648bc0b35370fed", "message": "Remove calls to set the task assignor", "committedDate": "2020-05-04T20:10:43Z", "type": "commit"}]}