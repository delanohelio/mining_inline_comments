{"pr_number": 8186, "pr_title": "KAFKA-9618: Directory deletion failure leading to error task RocksDB open", "pr_createdAt": "2020-02-27T19:10:29Z", "pr_url": "https://github.com/apache/kafka/pull/8186", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ4MDQ4OQ==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r385480489", "bodyText": "Why do we now always wipe the state stores (even on clean close)? cc/ @guozhangwang", "author": "ableegoldman", "createdAt": "2020-02-28T02:36:49Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -83,8 +84,8 @@ static void wipeStateStores(final Logger log, final ProcessorStateManager stateM\n         try {\n             Utils.delete(stateMgr.baseDir());\n         } catch (final IOException fatalException) {\n-            // since it is only called under dirty close, we always swallow the exception", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTgxNjUyOQ==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r385816529", "bodyText": "We do not wipe state stores on clean close, but only in dirty close.", "author": "guozhangwang", "createdAt": "2020-02-28T17:12:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ4MDQ4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTgxNjcxMA==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r385816710", "bodyText": "wipeStateStores is only triggered if it is dirty close, under EOS.", "author": "guozhangwang", "createdAt": "2020-02-28T17:13:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ4MDQ4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTg5MjcwNg==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r385892706", "bodyText": "Ah I wasn't looking at the latest trunk, I see you fixed that in a recent PR. In that case we should leave the comment?", "author": "ableegoldman", "createdAt": "2020-02-28T19:53:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ4MDQ4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ4MjE2Mg==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r385482162", "bodyText": "Seems like we should retry, at least if this is a clean close...otherwise this thread still dies, and also kills any other threads that then get assigned this task.\nOr better yet, can we fix the state store to not blindly open non-existent files? If we successfully delete the checkpoint file but fail to delete the stores, Streams shouldn't attempt to open any store files.\nIf we fail at deleting the checkpoint, we're more at risk as the data may be corrupted (otherwise we wouldn't be trying to delete the task state to begin with). In that case it seems like the only safe thing to do would be to retry until we at least delete the checkpoint file", "author": "ableegoldman", "createdAt": "2020-02-28T02:44:26Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -83,8 +84,8 @@ static void wipeStateStores(final Logger log, final ProcessorStateManager stateM\n         try {\n             Utils.delete(stateMgr.baseDir());\n         } catch (final IOException fatalException) {\n-            // since it is only called under dirty close, we always swallow the exception\n-            log.warn(\"Failed to wiping state stores for task {}\", stateMgr.taskId());\n+            log.error(\"Failed to wiping state stores for task {} due to {}\", stateMgr.taskId(), fatalException);\n+            throw new StreamsException(fatalException);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ5MDA0OQ==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r385490049", "bodyText": "My current feeling is that we should infinitely retry to make sure we clean up a directory completely, since in principal if this is not an atomic operation, the handling complexity during normal processing would be 2X IMHO. In this case, I would rather we stuck in here instead of proceed, as we potentially could already corrupt the state and what's even worse is that we don't know it.", "author": "abbccdda", "createdAt": "2020-02-28T03:22:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ4MjE2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ5Mjk2MA==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r385492960", "bodyText": "Well, isn't the whole point of the checkpoint file to act as the source of truth? ie we should feel secure that deletion of the checkpoint file will always invalidate any leftover state, even if we fail to actually wipe it out -- whether the store or even the directory itself is left behind should not matter. We should absolutely retry indefinitely to delete the checkpoint file though. Once that's gone, either the cleanup thread will delete the directory or some thread will get the task re-assigned and delete the leftover store to recreate from scratch", "author": "ableegoldman", "createdAt": "2020-02-28T03:36:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ4MjE2Mg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjY2NTU2Nw==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r386665567", "bodyText": "Is that okay that we wipe out the directory already and then closing state manager, in which we would flush the stores and then close them? Would any exception be thrown?", "author": "guozhangwang", "createdAt": "2020-03-02T21:37:48Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -440,16 +440,16 @@ private void close(final boolean clean) {\n             }\n \n             if (state() == State.CLOSING) {\n-                // first close state manager (which is idempotent) then close the record collector (which could throw),\n-                // if the latter throws and we re-close dirty which would close the state manager again.\n-                StateManagerUtil.closeStateManager(log, logPrefix, clean, stateMgr, stateDirectory);\n-\n                 // if EOS is enabled, we wipe out the whole state store for unclean close\n                 // since they are invalid to use anymore\n                 if (!clean && !eosDisabled) {\n                     StateManagerUtil.wipeStateStores(log, stateMgr);\n                 }\n \n+                // first close state manager (which is idempotent) then close the record collector (which could throw),\n+                // if the latter throws and we re-close dirty which would close the state manager again.\n+                StateManagerUtil.closeStateManager(log, logPrefix, clean, stateMgr, stateDirectory);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI0MDE3NA==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r387240174", "bodyText": "nit: use TaskType.ACTIVE.", "author": "guozhangwang", "createdAt": "2020-03-03T19:22:27Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -440,15 +442,14 @@ private void close(final boolean clean) {\n             }\n \n             if (state() == State.CLOSING) {\n-                // first close state manager (which is idempotent) then close the record collector (which could throw),\n-                // if the latter throws and we re-close dirty which would close the state manager again.\n-                StateManagerUtil.closeStateManager(log, logPrefix, clean, stateMgr, stateDirectory);\n-\n                 // if EOS is enabled, we wipe out the whole state store for unclean close\n                 // since they are invalid to use anymore\n-                if (!clean && !eosDisabled) {\n-                    StateManagerUtil.wipeStateStores(log, stateMgr);\n-                }\n+                final boolean wipeStateStore = !clean && !eosDisabled;\n+\n+                // first close state manager (which is idempotent) then close the record collector (which could throw),\n+                // if the latter throws and we re-close dirty which would close the state manager again.\n+                StateManagerUtil.closeStateManager(log, logPrefix, clean,\n+                    wipeStateStore, stateMgr, stateDirectory, \"active\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI0MDMzNA==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r387240334", "bodyText": "Use TaskType.STANDBY.", "author": "guozhangwang", "createdAt": "2020-03-03T19:22:47Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -179,7 +179,7 @@ private void close(final boolean clean) {\n             }\n \n             if (state() == State.CLOSING) {\n-                StateManagerUtil.closeStateManager(log, logPrefix, clean, stateMgr, stateDirectory);\n+                StateManagerUtil.closeStateManager(log, logPrefix, clean, false, stateMgr, stateDirectory, \"standby\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NzY0MQ==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r387367641", "bodyText": "Why do we want to validate this case, where no exception would be thrown, and hence no handling logic should be triggered either?", "author": "guozhangwang", "createdAt": "2020-03-04T00:02:30Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EOSUncleanShutdownIntegrationTest.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.Serializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.test.IntegrationTest;\n+\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.singletonList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.cleanStateAfterTest;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.cleanStateBeforeTest;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertFalse;\n+\n+/**\n+ * Test the unclean shutdown behavior around state store cleanup.\n+ */\n+@Category(IntegrationTest.class)\n+@RunWith(value = Parameterized.class)\n+public class EOSUncleanShutdownIntegrationTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(3);\n+\n+    @ClassRule\n+    public static final TemporaryFolder TEST_FOLDER = new TemporaryFolder(TestUtils.tempDirectory());\n+\n+    private static final Properties STREAMS_CONFIG = new Properties();\n+    private static final StringSerializer STRING_SERIALIZER = new StringSerializer();\n+    private static final Long COMMIT_INTERVAL = 100L;\n+\n+    private static final int RECORD_TOTAL = 3;\n+\n+    @Parameterized.Parameters(name = \"exception threshold\")\n+    public static Collection<Object[]> data() {\n+        final List<Object[]> values = new ArrayList<>();\n+        for (final int threshold : Arrays.asList(RECORD_TOTAL, RECORD_TOTAL + 1)) {\n+            values.add(new Object[]{threshold});\n+        }\n+        return values;\n+    }\n+\n+    private final int fatalExceptionThreshold;\n+\n+    public EOSUncleanShutdownIntegrationTest(final int fatalExceptionThreshold) {\n+        this.fatalExceptionThreshold = fatalExceptionThreshold;\n+    }\n+\n+    @BeforeClass\n+    public static void setupConfigsAndUtils() {\n+        STREAMS_CONFIG.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        STREAMS_CONFIG.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n+        STREAMS_CONFIG.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n+        STREAMS_CONFIG.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n+        STREAMS_CONFIG.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, COMMIT_INTERVAL);\n+\n+        STREAMS_CONFIG.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, EXACTLY_ONCE);\n+        STREAMS_CONFIG.put(StreamsConfig.STATE_DIR_CONFIG, TEST_FOLDER.getRoot().getPath());\n+    }\n+\n+    @Test\n+    public void shouldWorkWithUncleanShutdownWipeOutStateStore() throws InterruptedException {\n+        final String appId = getClass().getSimpleName().toLowerCase(Locale.getDefault()) + \"-test\";\n+        STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appId);\n+\n+        final String input = \"input-topic\";\n+        cleanStateBeforeTest(CLUSTER, input);\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+\n+        final KStream<String, String> inputStream = builder.stream(input);\n+\n+        final AtomicInteger recordCount = new AtomicInteger(0);\n+\n+        final KTable<String, String> valueCounts = inputStream\n+                                                       .groupByKey()\n+                                                       .aggregate(() -> \"()\", (key, value, aggregate) -> aggregate + \",(\" + key + \": \" + value + \")\", Materialized.as(\"aggregated_value\"));\n+        valueCounts.toStream().peek((key, value) -> {\n+            if (recordCount.incrementAndGet() >= fatalExceptionThreshold) {\n+                throw new IllegalStateException(\"Crash on the \" + fatalExceptionThreshold + \" record\");\n+            }\n+        });\n+\n+        final Properties producerConfig = mkProperties(mkMap(\n+            mkEntry(ProducerConfig.CLIENT_ID_CONFIG, \"anything\"),\n+            mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ((Serializer<String>) STRING_SERIALIZER).getClass().getName()),\n+            mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ((Serializer<String>) STRING_SERIALIZER).getClass().getName()),\n+            mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers())\n+        ));\n+        final KafkaStreams driver = IntegrationTestUtils.getStartedStreams(STREAMS_CONFIG, builder, true);\n+\n+        final File stateDir = new File(\n+            String.join(\"/\", TEST_FOLDER.getRoot().getPath(), appId, \"0_0\"));\n+        try {\n+\n+            IntegrationTestUtils.produceSynchronously(producerConfig, false, input, Optional.empty(),\n+                singletonList(new KeyValueTimestamp<>(\"k1\", \"v1\", 0L)));\n+\n+            TestUtils.waitForCondition(stateDir::exists,\n+                \"Failed awaiting CreateTopics first request failure\");\n+\n+            IntegrationTestUtils.produceSynchronously(producerConfig, false, input, Optional.empty(),\n+                asList(new KeyValueTimestamp<>(\"k2\", \"v2\", 1L),\n+                    new KeyValueTimestamp<>(\"k3\", \"v3\", 2L)));\n+\n+            TestUtils.waitForCondition(() -> recordCount.get() == RECORD_TOTAL,\n+                \"Expected \" + RECORD_TOTAL + \" records processed but only got \" + recordCount.get());\n+        } finally {\n+            driver.close();\n+            if (fatalExceptionThreshold > RECORD_TOTAL) {\n+                assertTrue(stateDir.exists());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM4MzMyMg==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r387383322", "bodyText": "The purpose is just for more comprehensive checking to make sure the current codepath won't wipe out states if we close clean. Let me update the test name to reflect this.", "author": "abbccdda", "createdAt": "2020-03-04T00:53:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NzY0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM4Mzg5Mg==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r387383892", "bodyText": "Actually let me just simplify the test case", "author": "abbccdda", "createdAt": "2020-03-04T00:55:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NzY0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODEyMg==", "url": "https://github.com/apache/kafka/pull/8186#discussion_r387368122", "bodyText": "We can also add a validation that driver KafkaStreams's state is ERROR as we handle the error by shutting down the thread.", "author": "guozhangwang", "createdAt": "2020-03-04T00:04:07Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EOSUncleanShutdownIntegrationTest.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.Serializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValueTimestamp;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.test.IntegrationTest;\n+\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.singletonList;\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.cleanStateAfterTest;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.cleanStateBeforeTest;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertFalse;\n+\n+/**\n+ * Test the unclean shutdown behavior around state store cleanup.\n+ */\n+@Category(IntegrationTest.class)\n+@RunWith(value = Parameterized.class)\n+public class EOSUncleanShutdownIntegrationTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(3);\n+\n+    @ClassRule\n+    public static final TemporaryFolder TEST_FOLDER = new TemporaryFolder(TestUtils.tempDirectory());\n+\n+    private static final Properties STREAMS_CONFIG = new Properties();\n+    private static final StringSerializer STRING_SERIALIZER = new StringSerializer();\n+    private static final Long COMMIT_INTERVAL = 100L;\n+\n+    private static final int RECORD_TOTAL = 3;\n+\n+    @Parameterized.Parameters(name = \"exception threshold\")\n+    public static Collection<Object[]> data() {\n+        final List<Object[]> values = new ArrayList<>();\n+        for (final int threshold : Arrays.asList(RECORD_TOTAL, RECORD_TOTAL + 1)) {\n+            values.add(new Object[]{threshold});\n+        }\n+        return values;\n+    }\n+\n+    private final int fatalExceptionThreshold;\n+\n+    public EOSUncleanShutdownIntegrationTest(final int fatalExceptionThreshold) {\n+        this.fatalExceptionThreshold = fatalExceptionThreshold;\n+    }\n+\n+    @BeforeClass\n+    public static void setupConfigsAndUtils() {\n+        STREAMS_CONFIG.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        STREAMS_CONFIG.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n+        STREAMS_CONFIG.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n+        STREAMS_CONFIG.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n+        STREAMS_CONFIG.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, COMMIT_INTERVAL);\n+\n+        STREAMS_CONFIG.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, EXACTLY_ONCE);\n+        STREAMS_CONFIG.put(StreamsConfig.STATE_DIR_CONFIG, TEST_FOLDER.getRoot().getPath());\n+    }\n+\n+    @Test\n+    public void shouldWorkWithUncleanShutdownWipeOutStateStore() throws InterruptedException {\n+        final String appId = getClass().getSimpleName().toLowerCase(Locale.getDefault()) + \"-test\";\n+        STREAMS_CONFIG.put(StreamsConfig.APPLICATION_ID_CONFIG, appId);\n+\n+        final String input = \"input-topic\";\n+        cleanStateBeforeTest(CLUSTER, input);\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+\n+        final KStream<String, String> inputStream = builder.stream(input);\n+\n+        final AtomicInteger recordCount = new AtomicInteger(0);\n+\n+        final KTable<String, String> valueCounts = inputStream\n+                                                       .groupByKey()\n+                                                       .aggregate(() -> \"()\", (key, value, aggregate) -> aggregate + \",(\" + key + \": \" + value + \")\", Materialized.as(\"aggregated_value\"));\n+        valueCounts.toStream().peek((key, value) -> {\n+            if (recordCount.incrementAndGet() >= fatalExceptionThreshold) {\n+                throw new IllegalStateException(\"Crash on the \" + fatalExceptionThreshold + \" record\");\n+            }\n+        });\n+\n+        final Properties producerConfig = mkProperties(mkMap(\n+            mkEntry(ProducerConfig.CLIENT_ID_CONFIG, \"anything\"),\n+            mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ((Serializer<String>) STRING_SERIALIZER).getClass().getName()),\n+            mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ((Serializer<String>) STRING_SERIALIZER).getClass().getName()),\n+            mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers())\n+        ));\n+        final KafkaStreams driver = IntegrationTestUtils.getStartedStreams(STREAMS_CONFIG, builder, true);\n+\n+        final File stateDir = new File(\n+            String.join(\"/\", TEST_FOLDER.getRoot().getPath(), appId, \"0_0\"));\n+        try {\n+\n+            IntegrationTestUtils.produceSynchronously(producerConfig, false, input, Optional.empty(),\n+                singletonList(new KeyValueTimestamp<>(\"k1\", \"v1\", 0L)));\n+\n+            TestUtils.waitForCondition(stateDir::exists,\n+                \"Failed awaiting CreateTopics first request failure\");\n+\n+            IntegrationTestUtils.produceSynchronously(producerConfig, false, input, Optional.empty(),\n+                asList(new KeyValueTimestamp<>(\"k2\", \"v2\", 1L),\n+                    new KeyValueTimestamp<>(\"k3\", \"v3\", 2L)));\n+\n+            TestUtils.waitForCondition(() -> recordCount.get() == RECORD_TOTAL,\n+                \"Expected \" + RECORD_TOTAL + \" records processed but only got \" + recordCount.get());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "57dd73774114b79fdde271c21a3dae66ef25dc93", "url": "https://github.com/apache/kafka/commit/57dd73774114b79fdde271c21a3dae66ef25dc93", "message": "wipe out state store", "committedDate": "2020-03-04T01:44:59Z", "type": "commit"}, {"oid": "45ffa590cadba452290ca6884059859d98682c77", "url": "https://github.com/apache/kafka/commit/45ffa590cadba452290ca6884059859d98682c77", "message": "fix unit test", "committedDate": "2020-03-04T04:44:18Z", "type": "commit"}, {"oid": "45ffa590cadba452290ca6884059859d98682c77", "url": "https://github.com/apache/kafka/commit/45ffa590cadba452290ca6884059859d98682c77", "message": "fix unit test", "committedDate": "2020-03-04T04:44:18Z", "type": "forcePushed"}]}