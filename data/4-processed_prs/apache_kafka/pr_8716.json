{"pr_number": 8716, "pr_title": "KAFKA-6145: KIP-441: Fix assignor config passthough", "pr_createdAt": "2020-05-22T16:47:44Z", "pr_url": "https://github.com/apache/kafka/pull/8716", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429352272", "bodyText": "This is where we forgot to copy over the configs.", "author": "vvcephei", "createdAt": "2020-05-22T16:48:10Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -1148,6 +1148,9 @@ private void verifyMaxInFlightRequestPerConnection(final Object maxInFlightReque\n         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));\n         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));\n         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));\n+        consumerProps.put(ACCEPTABLE_RECOVERY_LAG_CONFIG, getLong(ACCEPTABLE_RECOVERY_LAG_CONFIG));\n+        consumerProps.put(MAX_WARMUP_REPLICAS_CONFIG, getInt(MAX_WARMUP_REPLICAS_CONFIG));\n+        consumerProps.put(PROBING_REBALANCE_INTERVAL_MS_CONFIG, getLong(PROBING_REBALANCE_INTERVAL_MS_CONFIG));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5MDQ4Mw==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429490483", "bodyText": "Can we add a note to AssignorConfiguration pointing to this for any new assignor-related configs that get added?", "author": "ableegoldman", "createdAt": "2020-05-22T23:42:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5MTk4NQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429491985", "bodyText": "Do we automatically pass through the internal configs? I notice we don't copy over the task assignor class, or the new assignment listener callback I added for the integration tests. But both of them seem to get through", "author": "ableegoldman", "createdAt": "2020-05-22T23:53:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5MjU5MQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429492591", "bodyText": "I know it's deprecated, but I think the PartitionGrouper config is missing as well.", "author": "ableegoldman", "createdAt": "2020-05-22T23:58:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY2ODc4NQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430668785", "bodyText": "Yes, we copy over the other internal configs in org.apache.kafka.streams.processor.internals.StreamThread#create\nI'll add your listener to the config test. I'm not sure about PartitionGrouper.", "author": "vvcephei", "createdAt": "2020-05-26T19:50:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDczMTI4Nw==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430731287", "bodyText": "The answer about your listener is that we automatically pass through any config that isn't registered. The assumption is, I suppose, that users might want to pass through custom configs to their custom plugged-in components.\nThe configs in this PR got filtered out because they are registered configs in the public API.", "author": "vvcephei", "createdAt": "2020-05-26T21:59:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDczNjcwOQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430736709", "bodyText": "Turns out PartitionGrouper does not get copied over. I'll create a jira to track this, so that we don't have to get sidetracked in this PR with the question of what we should do about this deprecated config.", "author": "vvcephei", "createdAt": "2020-05-26T22:13:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDczODk1NQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430738955", "bodyText": "https://issues.apache.org/jira/browse/KAFKA-10046", "author": "vvcephei", "createdAt": "2020-05-26T22:19:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3ODYwMw==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430778603", "bodyText": "we automatically pass through any config that isn't registered\n\nI have to say, this seems totally backwards to me. So basically we just happen to pass in any number of configs that we may or may not need, but will split out specific configs that we do need unless explicitly told to include them? I understand the custom configs motivation, but then why not just pass through all the configs?\nWhat if I wanted to access the value of one of my registered Streams configs in my plugged-in component? I'd have to add the same value a second time, but with an unregistered config name. Huh?", "author": "ableegoldman", "createdAt": "2020-05-27T00:26:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMyMjY4Mg==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r431322682", "bodyText": "Regarding KAFKA-10046, in current trunk we already have some logic that assumes the default partition grouper is always used, so I'd suggest we just bite the bullet and remove it in 2.6.", "author": "guozhangwang", "createdAt": "2020-05-27T17:37:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ0MTY5OA==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r438441698", "bodyText": "Thanks, @guozhangwang\n@ableegoldman , it seems backwards to me also.", "author": "vvcephei", "createdAt": "2020-06-10T22:27:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjI3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjY2MQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429352661", "bodyText": "I added this constraint to mirror the constraint we already apply in StreamConfig. It's not critical, but I was disappointed that I had written a bunch of tests that included a technically invalid configuration.", "author": "vvcephei", "createdAt": "2020-05-22T16:49:03Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -359,6 +359,10 @@ private AssignmentConfigs(final StreamsConfig configs) {\n                           final Integer maxWarmupReplicas,\n                           final Integer numStandbyReplicas,\n                           final Long probingRebalanceIntervalMs) {\n+            if (maxWarmupReplicas < 1) {\n+                throw new IllegalArgumentException(\"must configure at least one warmup replica\");\n+            }\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5MDc1Mw==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429490753", "bodyText": "Should we add the same for all the other configs?", "author": "ableegoldman", "createdAt": "2020-05-22T23:44:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjY2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkxMzk3Mw==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429913973", "bodyText": "IMO, we should check the limits for all configs. However, I am not sure if we should check probingRebalanceIntervalMs to be >= 60 * 1000L (as we do in StreamsConfig) or just `>= 0.", "author": "cadonna", "createdAt": "2020-05-25T12:40:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjY2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY3MDYzMg==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430670632", "bodyText": "We could; to be fair, the config parser already checks the limits, so it's not necessary for production code.\nIt only comes up when we manually create these internal objects for tests. I added this particular bound specifically because I had previously passed in what I thought was a dummy value, which turned out to be a misconfiguration that actually affected the behavior I was testing.\nOffhand, it doesn't seem like the same thing would happen with probingRebalanceIntervalMs, so it doesn't seem like the check here would have the same benefit; WDYT?", "author": "vvcephei", "createdAt": "2020-05-26T19:53:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjY2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1NjY2MA==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430756660", "bodyText": "Nevermind; I refactored the class to use the same config validation for passed-in arguments.", "author": "vvcephei", "createdAt": "2020-05-26T23:11:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MjY2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1Mjc1OQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429352759", "bodyText": "I found this useful while debugging the system test.", "author": "vvcephei", "createdAt": "2020-05-22T16:49:19Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -334,6 +334,7 @@ public String toString() {\n             \") prevStandbyTasks: (\" + prevStandbyTasks +\n             \") prevOwnedPartitionsByConsumerId: (\" + ownedPartitions.keySet() +\n             \") changelogOffsetTotalsByTask: (\" + taskOffsetSums.entrySet() +\n+            \") taskLagTotals: (\" + taskLagTotals.entrySet() +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MzE5Nw==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429353197", "bodyText": "Since we can't have zero warmups, we don't need this condition (that I added in #8696)", "author": "vvcephei", "createdAt": "2020-05-22T16:50:14Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -90,15 +91,12 @@ public boolean assign(final Map<UUID, ClientState> clients,\n \n         assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n-        // We shouldn't plan a probing rebalance if we _needed_ task movements, but couldn't do any\n-        // due to being configured for no warmups.\n-        final boolean probingRebalanceNeeded =\n-            configs.maxWarmupReplicas > 0 && neededActiveTaskMovements + neededStandbyTaskMovements > 0;\n+        final boolean probingRebalanceNeeded = neededActiveTaskMovements + neededStandbyTaskMovements > 0;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1MzcwOQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429353709", "bodyText": "Fixing a double-space we were printing when there was a followup. (It would say with  followup)", "author": "vvcephei", "createdAt": "2020-05-22T16:51:18Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -90,15 +91,12 @@ public boolean assign(final Map<UUID, ClientState> clients,\n \n         assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n-        // We shouldn't plan a probing rebalance if we _needed_ task movements, but couldn't do any\n-        // due to being configured for no warmups.\n-        final boolean probingRebalanceNeeded =\n-            configs.maxWarmupReplicas > 0 && neededActiveTaskMovements + neededStandbyTaskMovements > 0;\n+        final boolean probingRebalanceNeeded = neededActiveTaskMovements + neededStandbyTaskMovements > 0;\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n-                     \" with \" +\n-                     (probingRebalanceNeeded ? \"\" : \"no\") +\n+                     \" with\" +\n+                     (probingRebalanceNeeded ? \"\" : \" no\") +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1NDIzMg==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429354232", "bodyText": "A short-lived, empty TreeSet costs practically nothing, and I found the other logic (with null meaning empty) a bit confusing during debugging.", "author": "vvcephei", "createdAt": "2020-05-22T16:52:23Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -241,16 +239,29 @@ private static void assignStatelessActiveTasks(final TreeMap<UUID, ClientState>\n         final Map<TaskId, SortedSet<UUID>> taskToCaughtUpClients = new HashMap<>();\n \n         for (final TaskId task : statefulTasks) {\n-\n+            final TreeSet<UUID> caughtUpClients = new TreeSet<>();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5MTExOA==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429491118", "bodyText": "Fair enough. I don't think it was meant as a cost saving thing, just to make it easier to understand when something did or did not have caught-up clients. If you find this logic easier to follow, go for it", "author": "ableegoldman", "createdAt": "2020-05-22T23:47:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1NDIzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY3MDg4NQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430670885", "bodyText": "Cool; thanks!", "author": "vvcephei", "createdAt": "2020-05-26T19:54:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1NDIzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1OTI1MQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429359251", "bodyText": "I realized that our condition was actually wrong here. In addition to all the zero-or-greater lags, there are two negative lags, one meaning \"unknown\" (-1), and one meaning \"latest\" (-2). When we said taskLag <= acceptableRecoveryLag, it was unintentionally encompassing the sentinel values as well. Even if we want a sentinel to be considered \"caught up\" (as with \"Latest\"), we should do so explicitly, not by mathematical coincidence.\nI also added a special case when acceptableRecoveryLag is set to MAX_VALUE to indicate that all tasks, regardless of their lag (even if it's a sentinel), are to be considered caught-up.\nI also found the boolean expression with all the conditionals a little hard to read, so I pulled out some semantic methods.", "author": "vvcephei", "createdAt": "2020-05-22T17:03:41Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -241,16 +239,29 @@ private static void assignStatelessActiveTasks(final TreeMap<UUID, ClientState>\n         final Map<TaskId, SortedSet<UUID>> taskToCaughtUpClients = new HashMap<>();\n \n         for (final TaskId task : statefulTasks) {\n-\n+            final TreeSet<UUID> caughtUpClients = new TreeSet<>();\n             for (final Map.Entry<UUID, ClientState> clientEntry : clientStates.entrySet()) {\n                 final UUID client = clientEntry.getKey();\n                 final long taskLag = clientEntry.getValue().lagFor(task);\n-                if (taskLag == Task.LATEST_OFFSET || taskLag <= acceptableRecoveryLag) {\n-                    taskToCaughtUpClients.computeIfAbsent(task, ignored -> new TreeSet<>()).add(client);\n+                if (active(taskLag) || unbounded(acceptableRecoveryLag) || acceptable(acceptableRecoveryLag, taskLag)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5MTUxMw==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429491513", "bodyText": "Nice catch! One nit is that \"active\" alone is not sufficient for being considered caught-up. Can we rename the active condition to running or activeRunning, etc?", "author": "ableegoldman", "createdAt": "2020-05-22T23:50:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1OTI1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkyMzA3MQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429923071", "bodyText": "Nice catch indeed! I agree with @ableegoldman about the renaming. I am in favour of activeRunning or activeAndRunning.", "author": "cadonna", "createdAt": "2020-05-25T13:02:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1OTI1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM1OTk1OQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429359959", "bodyText": "Expanding DeMorgan's law at @cadonna 's request (which I also appreciated).", "author": "vvcephei", "createdAt": "2020-05-22T17:05:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -56,14 +56,17 @@ private int numCaughtUpClients() {\n         return caughtUpClients.size();\n     }\n \n-    /**\n-     * @return true if this client is caught-up for this task, or the task has no caught-up clients\n-     */\n+    private static boolean taskIsNotCaughtUpOnClientAndOtherCaughtUpClientsExist(final TaskId task,\n+                                                                                 final UUID client,\n+                                                                                 final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients) {\n+        return !taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients);\n+    }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM2MjA0MA==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429362040", "bodyText": "The diff is misaligned. I removed shouldBeStickyForActiveAndStandbyTasksEvenIfNoWarmups and added shouldSkipWarmupsWhenAcceptableLagIsMax.", "author": "vvcephei", "createdAt": "2020-05-22T17:10:30Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -81,7 +81,7 @@\n     );\n \n     @Test\n-    public void shouldBeStickyForActiveAndStandbyTasksEvenIfNoWarmups() {\n+    public void shouldBeStickyForActiveAndStandbyTasksWhileWarmingUp() {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM2MjE4OA==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429362188", "bodyText": "All these tests erroneously set \"max warmups\" to zero.", "author": "vvcephei", "createdAt": "2020-05-22T17:10:50Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -150,7 +147,7 @@ public void shouldAssignActiveStatefulTasksEvenlyOverClientsWhereNumberOfClients\n             clientStates,\n             allTaskIds,\n             allTaskIds,\n-            new AssignmentConfigs(0L, 0, 0, 0L)\n+            new AssignmentConfigs(0L, 1, 0, 0L)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM2MjM5MA==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429362390", "bodyText": "It was handy to be able to see the used config file while debugging.", "author": "vvcephei", "createdAt": "2020-05-22T17:11:19Z", "path": "tests/kafkatest/services/streams.py", "diffHunk": "@@ -44,6 +44,9 @@ class StreamsTestBaseService(KafkaPathResolverMixin, JmxMixin, Service):\n     CLEAN_NODE_ENABLED = True\n \n     logs = {\n+        \"streams_config\": {\n+            \"path\": CONFIG_FILE,\n+            \"collect_default\": True},", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM2MjYzOA==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429362638", "bodyText": "Added this configuration to fix the flaky StreamsOptimizedTest.test_upgrade_optimized_topology", "author": "vvcephei", "createdAt": "2020-05-22T17:11:58Z", "path": "tests/kafkatest/services/streams.py", "diffHunk": "@@ -465,6 +468,9 @@ def prop_file(self):\n         properties['reduce.topic'] = self.REDUCE_TOPIC\n         properties['join.topic'] = self.JOIN_TOPIC\n \n+        # Long.MAX_VALUE lets us do the assignment without a warmup\n+        properties['acceptable.recovery.lag'] = \"9223372036854775807\"\n+", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQyMDAxNw==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429420017", "bodyText": "Added this test, and verified that it does indeed fail on trunk for the expected reason that the new configs were ignored and defaults were substituted instead.", "author": "vvcephei", "createdAt": "2020-05-22T19:26:35Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,94 @@\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.processor.internals.StreamThread;\n+import org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.hamcrest.Matchers;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.lang.reflect.Field;\n+import java.util.List;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class TaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void shouldProperlyConfigureTheAssignor() throws NoSuchFieldException, IllegalAccessException {\n+        // This test uses reflection to check and make sure that all the expected configurations really\n+        // make it all the way to configure the task assignor. There's no other use case for being able\n+        // to extract all these fields, so reflection is a good choice until we find that the maintenance\n+        // burden is too high.\n+        //\n+        // Also note that this is an integration test because so many components have to come together to\n+        // ensure these configurations wind up where they belong, and any number of future code changes\n+        // could break this change.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NDIxMw==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429494213", "bodyText": "Should we move the handful of AssignorConfiguration related tests from StreamsPartitionAssignorTest to here?", "author": "ableegoldman", "createdAt": "2020-05-23T00:09:05Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfigurationTest.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.junit.Test;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.junit.Assert.assertThrows;\n+\n+public class AssignorConfigurationTest {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkyNzQ5MA==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429927490", "bodyText": "Yes, please!", "author": "cadonna", "createdAt": "2020-05-25T13:12:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NDIxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1ODUyNg==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430758526", "bodyText": "Ah, unfortunately, that test relies on mocking package-private fields from another package. I'll just leave it alone for now.", "author": "vvcephei", "createdAt": "2020-05-26T23:17:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NDIxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NjYwNQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429496605", "bodyText": "Trying to avoid piling on even more unrelated questions to the thread below, but there's another config that would need to be passed in, the admin client timeout.\nThat said, can we just remove it? It only gets the timeout the admin is configured with anyway", "author": "ableegoldman", "createdAt": "2020-05-23T00:29:57Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -1148,6 +1148,9 @@ private void verifyMaxInFlightRequestPerConnection(final Object maxInFlightReque\n         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));\n         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));\n         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));\n+        consumerProps.put(ACCEPTABLE_RECOVERY_LAG_CONFIG, getLong(ACCEPTABLE_RECOVERY_LAG_CONFIG));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY3MTcxOA==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430671718", "bodyText": "Ack, I'll take a look at it.", "author": "vvcephei", "createdAt": "2020-05-26T19:55:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NjYwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2MDM5NQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430760395", "bodyText": "It does wind up \"surviving\" into the config that we use in the assignor, but I'm not sure if it's on purpose or just lucky.\nI'll postpone any existential questions, and just add it to the regression test.", "author": "vvcephei", "createdAt": "2020-05-26T23:23:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NjYwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2MDY1OQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430760659", "bodyText": "Ah, it was also tested here: org.apache.kafka.streams.StreamsConfigTest#consumerConfigShouldContainAdminClientConfigsForRetriesAndRetryBackOffMsWithAdminPrefix", "author": "vvcephei", "createdAt": "2020-05-26T23:24:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NjYwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMyNzg5OA==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r431327898", "bodyText": "I'll open a separate PR to remove the extra timeout config", "author": "ableegoldman", "createdAt": "2020-05-27T17:46:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NjYwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkwOTUxOQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r429909519", "bodyText": "req: Please add verifications to StreamsConfigTest#consumerConfigMustContainStreamPartitionAssignorConfig()", "author": "cadonna", "createdAt": "2020-05-25T12:29:27Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -1148,6 +1148,9 @@ private void verifyMaxInFlightRequestPerConnection(final Object maxInFlightReque\n         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));\n         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));\n         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));\n+        consumerProps.put(ACCEPTABLE_RECOVERY_LAG_CONFIG, getLong(ACCEPTABLE_RECOVERY_LAG_CONFIG));\n+        consumerProps.put(MAX_WARMUP_REPLICAS_CONFIG, getInt(MAX_WARMUP_REPLICAS_CONFIG));\n+        consumerProps.put(PROBING_REBALANCE_INTERVAL_MS_CONFIG, getLong(PROBING_REBALANCE_INTERVAL_MS_CONFIG));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3OTM3MA==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430779370", "bodyText": "I think this comment got lost in a discussion thread, but can we add a note to AssignorConfiguration pointing out that any Streams configs added here will need to be explicitly passed through? It seems like it's too easy to fall into this same trap again", "author": "ableegoldman", "createdAt": "2020-05-27T00:28:59Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -347,22 +348,28 @@ public AssignmentListener assignmentListener() {\n         public final long probingRebalanceIntervalMs;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyMTY4MA==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r430821680", "bodyText": "Ah, right. Sure thing!", "author": "vvcephei", "createdAt": "2020-05-27T02:23:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3OTM3MA=="}], "type": "inlineReview"}, {"oid": "3c8426c3678dfff53a49327bc7a98be5dff90da5", "url": "https://github.com/apache/kafka/commit/3c8426c3678dfff53a49327bc7a98be5dff90da5", "message": "KAFKA-6145: KIP-441: Fix assignor config passthough", "committedDate": "2020-05-27T14:33:35Z", "type": "commit"}, {"oid": "94194f2f6cc87a202266fbabfe426f8bc4fb09aa", "url": "https://github.com/apache/kafka/commit/94194f2f6cc87a202266fbabfe426f8bc4fb09aa", "message": "fix test", "committedDate": "2020-05-27T14:44:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMyNjM5MA==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r431326390", "bodyText": "\ud83e\udd26\u200d\u2640\ufe0f", "author": "ableegoldman", "createdAt": "2020-05-27T17:43:43Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -874,7 +874,7 @@\n         public static final String TIME = \"__time__\";\n \n         // This is settable in the main Streams config, but it's a private API for testing\n-        public static final String ASSIGNMENT_LISTENER = \"__asignment.listener__\";", "originalCommit": "94194f2f6cc87a202266fbabfe426f8bc4fb09aa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMzMjM4Ng==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r431332386", "bodyText": "Should we also validate that the task assignor gets passed through? We could even pass in a custom assignor and use that to verify the correct assignor configs got passed through.\nOf course, reflection black magic-ry is just more fun \ud83d\ude42", "author": "ableegoldman", "createdAt": "2020-05-27T17:51:43Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.processor.internals.StreamThread;\n+import org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.lang.reflect.Field;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.sameInstance;\n+\n+@Category(IntegrationTest.class)\n+public class TaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void shouldProperlyConfigureTheAssignor() throws NoSuchFieldException, IllegalAccessException {\n+        // This test uses reflection to check and make sure that all the expected configurations really\n+        // make it all the way to configure the task assignor. There's no other use case for being able\n+        // to extract all these fields, so reflection is a good choice until we find that the maintenance\n+        // burden is too high.\n+        //\n+        // Also note that this is an integration test because so many components have to come together to\n+        // ensure these configurations wind up where they belong, and any number of future code changes\n+        // could break this change.\n+\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + testId;\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, \"input\");\n+\n+        // Maybe I'm paranoid, but I don't want the compiler deciding that my lambdas are equal to the identity\n+        // function and defeating my identity check\n+        final AtomicInteger compilerDefeatingReference = new AtomicInteger(0);\n+\n+        // the implementation doesn't matter, we're just going to verify the reference.\n+        final AssignorConfiguration.AssignmentListener configuredAssignmentListener =\n+            stable -> compilerDefeatingReference.incrementAndGet();\n+\n+        final Properties properties = mkObjectProperties(\n+            mkMap(\n+                mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                mkEntry(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, \"5\"),\n+                mkEntry(StreamsConfig.ACCEPTABLE_RECOVERY_LAG_CONFIG, \"6\"),\n+                mkEntry(StreamsConfig.MAX_WARMUP_REPLICAS_CONFIG, \"7\"),\n+                mkEntry(StreamsConfig.PROBING_REBALANCE_INTERVAL_MS_CONFIG, \"480000\"),\n+                mkEntry(StreamsConfig.InternalConfig.ASSIGNMENT_LISTENER, configuredAssignmentListener),", "originalCommit": "94194f2f6cc87a202266fbabfe426f8bc4fb09aa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM0MjAxMQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r431342011", "bodyText": "Good idea; I'm not sure why I didn't think to do this already. With my newfound understanding of this config translation logic, I'm confident that it gets copied over right now, because it's not a registered config, but a regression test would be nice.\nI'll quickly follow up with a separate PR so that I can merge this one.", "author": "vvcephei", "createdAt": "2020-05-27T18:04:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMzMjM4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM0OTkwNQ==", "url": "https://github.com/apache/kafka/pull/8716#discussion_r431349905", "bodyText": "Sounds good. Thanks for the fix!", "author": "ableegoldman", "createdAt": "2020-05-27T18:18:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTMzMjM4Ng=="}], "type": "inlineReview"}]}