{"pr_number": 9232, "pr_title": "KAFKA-9924: Add remaining property-based RocksDB metrics as described in KIP-607", "pr_createdAt": "2020-08-31T11:14:47Z", "pr_url": "https://github.com/apache/kafka/pull/9232", "timeline": [{"oid": "dd647535339a9d0cc6003caa5f297195fd64d615", "url": "https://github.com/apache/kafka/commit/dd647535339a9d0cc6003caa5f297195fd64d615", "message": "Add wrapper around BlockBasedTableConfig to make cache accessible", "committedDate": "2020-07-29T11:34:40Z", "type": "commit"}, {"oid": "634d18b15ff400dced3b1af6b43c98630e115d8d", "url": "https://github.com/apache/kafka/commit/634d18b15ff400dced3b1af6b43c98630e115d8d", "message": "Refactor RocksDBMetricsRecorder and instantiation of RocksDBMetricsRecordingTrigger", "committedDate": "2020-07-29T11:34:40Z", "type": "commit"}, {"oid": "67986c4b3e7c8fa627ec52a10bbdc3138e828b8d", "url": "https://github.com/apache/kafka/commit/67986c4b3e7c8fa627ec52a10bbdc3138e828b8d", "message": "Add unit test when user specifies new table format config", "committedDate": "2020-07-30T08:51:12Z", "type": "commit"}, {"oid": "0afb35d8bba0d76adf9002bd9db14025cd0ca340", "url": "https://github.com/apache/kafka/commit/0afb35d8bba0d76adf9002bd9db14025cd0ca340", "message": "Make RocksDB recording trigger member variable final", "committedDate": "2020-07-30T10:25:54Z", "type": "commit"}, {"oid": "4331821d84ecc8873f814cd691e9bb13b7762fe2", "url": "https://github.com/apache/kafka/commit/4331821d84ecc8873f814cd691e9bb13b7762fe2", "message": "Make warning regarding RocksDB's table configuration clearer", "committedDate": "2020-07-31T13:46:51Z", "type": "commit"}, {"oid": "34dac91f6249d809df5b366eaf82596dde3d5b3b", "url": "https://github.com/apache/kafka/commit/34dac91f6249d809df5b366eaf82596dde3d5b3b", "message": "Remove unused parameter", "committedDate": "2020-07-31T13:50:02Z", "type": "commit"}, {"oid": "91b3430b9db78b0cff834d6197f509f65a639dcd", "url": "https://github.com/apache/kafka/commit/91b3430b9db78b0cff834d6197f509f65a639dcd", "message": "Throw exception instead of log a warning", "committedDate": "2020-08-10T20:15:57Z", "type": "commit"}, {"oid": "ebab7af0bb7ea98af111aa626874f678d9fe3c56", "url": "https://github.com/apache/kafka/commit/ebab7af0bb7ea98af111aa626874f678d9fe3c56", "message": "Allow other table formats than block-based tables", "committedDate": "2020-08-11T19:41:48Z", "type": "commit"}, {"oid": "5c967a48033bcf92438e8fc419a6a8b4835ba665", "url": "https://github.com/apache/kafka/commit/5c967a48033bcf92438e8fc419a6a8b4835ba665", "message": "Improve statistics handling in metrics recorder", "committedDate": "2020-08-12T08:36:07Z", "type": "commit"}, {"oid": "fb15c3a5bf91f9a46d6914739b617ca2291f52da", "url": "https://github.com/apache/kafka/commit/fb15c3a5bf91f9a46d6914739b617ca2291f52da", "message": "Change guard to avoid recording when statistics are null", "committedDate": "2020-08-12T15:27:07Z", "type": "commit"}, {"oid": "cf7c79f9148244d37b502fbdb4e4e86b818ea3ac", "url": "https://github.com/apache/kafka/commit/cf7c79f9148244d37b502fbdb4e4e86b818ea3ac", "message": "Add methods to add gauge metrics on state store level", "committedDate": "2020-08-12T15:31:38Z", "type": "commit"}, {"oid": "86bcf00fc34a840160029178ca3b0ca77d4f9443", "url": "https://github.com/apache/kafka/commit/86bcf00fc34a840160029178ca3b0ca77d4f9443", "message": "Add metric num-entries-active-mem-table", "committedDate": "2020-08-12T15:31:38Z", "type": "commit"}, {"oid": "8d6759aa75bf1ecf75de5114fb6ac8be50259535", "url": "https://github.com/apache/kafka/commit/8d6759aa75bf1ecf75de5114fb6ac8be50259535", "message": "Add methods to expose property-based RocksDB metrics", "committedDate": "2020-08-13T17:22:29Z", "type": "commit"}, {"oid": "fa486862db47294a18ba3c41b0e5bf172fca2cb1", "url": "https://github.com/apache/kafka/commit/fa486862db47294a18ba3c41b0e5bf172fca2cb1", "message": "Add property-based metrics", "committedDate": "2020-08-19T08:44:06Z", "type": "commit"}, {"oid": "eeec8879b2ef9b21a65cfec45c7dbc9397bc27bb", "url": "https://github.com/apache/kafka/commit/eeec8879b2ef9b21a65cfec45c7dbc9397bc27bb", "message": "Add more tests", "committedDate": "2020-08-20T16:49:12Z", "type": "commit"}, {"oid": "bbd7548f9f2a15e5d3ecec8d89e5b4ea5d515bbf", "url": "https://github.com/apache/kafka/commit/bbd7548f9f2a15e5d3ecec8d89e5b4ea5d515bbf", "message": "Merge remote-tracking branch 'upstream/trunk' into AK9924-add-metrics2", "committedDate": "2020-08-31T10:46:48Z", "type": "commit"}, {"oid": "f6802d79b2f9b372060ed32dfd091615443ea9a1", "url": "https://github.com/apache/kafka/commit/f6802d79b2f9b372060ed32dfd091615443ea9a1", "message": "Remove unused parameter", "committedDate": "2020-08-31T11:10:00Z", "type": "commit"}, {"oid": "f9ca85b919216839d9e8d5bdb81003031a2e9462", "url": "https://github.com/apache/kafka/commit/f9ca85b919216839d9e8d5bdb81003031a2e9462", "message": "Improve code", "committedDate": "2020-08-31T11:40:07Z", "type": "commit"}, {"oid": "91d16685e1460ec903956b40f5eedf1fb21e16b9", "url": "https://github.com/apache/kafka/commit/91d16685e1460ec903956b40f5eedf1fb21e16b9", "message": "Clean up code", "committedDate": "2020-08-31T11:45:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3NzYzOQ==", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480477639", "bodyText": "Hmm, why we need the second condition to determine singleCache = false here?", "author": "guozhangwang", "createdAt": "2020-08-31T23:49:12Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -150,14 +176,55 @@ private void verifyStatistics(final String segmentName, final Statistics statist\n                 statistics != null &&\n                 storeToValueProviders.values().stream().anyMatch(valueProviders -> valueProviders.statistics == null))) {\n \n-            throw new IllegalStateException(\"Statistics for store \\\"\" + segmentName + \"\\\" of task \" + taskId +\n-                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another store in this \" +\n+            throw new IllegalStateException(\"Statistics for segment \" + segmentName + \" of task \" + taskId +\n+                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another segment in this \" +\n                 \"metrics recorder is\" + (statistics != null ? \" \" : \" not \") + \"null. \" +\n                 \"This is a bug in Kafka Streams. \" +\n                 \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n         }\n     }\n \n+    private void verifyDbAndCacheAndStatistics(final String segmentName,\n+                                               final RocksDB db,\n+                                               final Cache cache,\n+                                               final Statistics statistics) {\n+        for (final DbAndCacheAndStatistics valueProviders : storeToValueProviders.values()) {\n+            verifyIfSomeAreNull(segmentName, statistics, valueProviders.statistics, \"statistics\");\n+            verifyIfSomeAreNull(segmentName, cache, valueProviders.cache, \"cache\");\n+            if (db == valueProviders.db) {\n+                throw new IllegalStateException(\"DB instance for store \" + segmentName + \" of task \" + taskId +\n+                    \" was already added for another segment as a value provider. This is a bug in Kafka Streams. \" +\n+                    \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n+            }\n+            if (storeToValueProviders.size() == 1 && cache != valueProviders.cache) {", "originalCommit": "91d16685e1460ec903956b40f5eedf1fb21e16b9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDkxNDc4Mg==", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480914782", "bodyText": "When RocksDB uses the memory bounded configuration, all RocksDB instances use the same cache, i.e., the reference to the same cache.\nAt this point in the code the value providers for the given segment with the cache cache has not been added yet to the value providers used in this recorder. To understand if different caches are used for different segments (i.e., singleCache = false), it is not enough to just check if only one single cache has been already added, we also need to check if the already added cache (i.e., valueProviders.cache) is different from the cache to add (cache).", "author": "cadonna", "createdAt": "2020-09-01T07:24:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3NzYzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3ODAyMw==", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480478023", "bodyText": "nit: verifyConsistentSegmentValueProviders?", "author": "guozhangwang", "createdAt": "2020-08-31T23:50:26Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -150,14 +176,55 @@ private void verifyStatistics(final String segmentName, final Statistics statist\n                 statistics != null &&\n                 storeToValueProviders.values().stream().anyMatch(valueProviders -> valueProviders.statistics == null))) {\n \n-            throw new IllegalStateException(\"Statistics for store \\\"\" + segmentName + \"\\\" of task \" + taskId +\n-                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another store in this \" +\n+            throw new IllegalStateException(\"Statistics for segment \" + segmentName + \" of task \" + taskId +\n+                \" is\" + (statistics == null ? \" \" : \" not \") + \"null although the statistics of another segment in this \" +\n                 \"metrics recorder is\" + (statistics != null ? \" \" : \" not \") + \"null. \" +\n                 \"This is a bug in Kafka Streams. \" +\n                 \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n         }\n     }\n \n+    private void verifyDbAndCacheAndStatistics(final String segmentName,\n+                                               final RocksDB db,\n+                                               final Cache cache,\n+                                               final Statistics statistics) {\n+        for (final DbAndCacheAndStatistics valueProviders : storeToValueProviders.values()) {\n+            verifyIfSomeAreNull(segmentName, statistics, valueProviders.statistics, \"statistics\");\n+            verifyIfSomeAreNull(segmentName, cache, valueProviders.cache, \"cache\");\n+            if (db == valueProviders.db) {\n+                throw new IllegalStateException(\"DB instance for store \" + segmentName + \" of task \" + taskId +\n+                    \" was already added for another segment as a value provider. This is a bug in Kafka Streams. \" +\n+                    \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n+            }\n+            if (storeToValueProviders.size() == 1 && cache != valueProviders.cache) {\n+                singleCache = false;\n+            } else if (singleCache && cache != valueProviders.cache || !singleCache && cache == valueProviders.cache) {\n+                throw new IllegalStateException(\"Caches for store \" + storeName + \" of task \" + taskId +\n+                    \" are either not all distinct or do not all refer to the same cache. This is a bug in Kafka Streams. \" +\n+                    \"Please open a bug report under https://issues.apache.org/jira/projects/KAFKA/issues\");\n+            }\n+        }\n+    }\n+\n+    private void verifyIfSomeAreNull(final String segmentName,", "originalCommit": "91d16685e1460ec903956b40f5eedf1fb21e16b9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDkyNDI5Nw==", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480924297", "bodyText": "I will rename it to verifyConsistencyOfValueProvidersAcrossSegments(). WDYT?", "author": "cadonna", "createdAt": "2020-09-01T07:36:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3ODAyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3ODYyMQ==", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480478621", "bodyText": "Is this a piggy-backed fix to wrap RocksDBException here?", "author": "guozhangwang", "createdAt": "2020-08-31T23:52:32Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -174,22 +241,163 @@ private void initSensors(final StreamsMetricsImpl streamsMetrics, final RocksDBM\n         numberOfFileErrorsSensor = RocksDBMetrics.numberOfFileErrorsSensor(streamsMetrics, metricContext);\n     }\n \n-    private void initGauges(final StreamsMetricsImpl streamsMetrics, final RocksDBMetricContext metricContext) {\n-        RocksDBMetrics.addNumEntriesActiveMemTableMetric(streamsMetrics, metricContext, (metricsConfig, now) -> {\n+    private void initGauges(final StreamsMetricsImpl streamsMetrics,\n+                            final RocksDBMetricContext metricContext) {\n+        RocksDBMetrics.addNumImmutableMemTableMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_IMMUTABLE_MEMTABLES)\n+        );\n+        RocksDBMetrics.addCurSizeActiveMemTable(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(CURRENT_SIZE_OF_ACTIVE_MEMTABLE)\n+        );\n+        RocksDBMetrics.addCurSizeAllMemTables(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(CURRENT_SIZE_OF_ALL_MEMTABLES)\n+        );\n+        RocksDBMetrics.addSizeAllMemTables(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(SIZE_OF_ALL_MEMTABLES)\n+        );\n+        RocksDBMetrics.addNumEntriesActiveMemTableMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_ENTRIES_ACTIVE_MEMTABLE)\n+        );\n+        RocksDBMetrics.addNumDeletesActiveMemTableMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_DELETES_ACTIVE_MEMTABLE)\n+        );\n+        RocksDBMetrics.addNumEntriesImmMemTablesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_ENTRIES_IMMUTABLE_MEMTABLES)\n+        );\n+        RocksDBMetrics.addNumDeletesImmMemTablesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_DELETES_IMMUTABLE_MEMTABLES)\n+        );\n+        RocksDBMetrics.addMemTableFlushPending(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(MEMTABLE_FLUSH_PENDING)\n+        );\n+        RocksDBMetrics.addNumRunningFlushesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_RUNNING_FLUSHES)\n+        );\n+        RocksDBMetrics.addCompactionPendingMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(COMPACTION_PENDING)\n+        );\n+        RocksDBMetrics.addNumRunningCompactionsMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_RUNNING_COMPACTIONS)\n+        );\n+        RocksDBMetrics.addEstimatePendingCompactionBytesMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(ESTIMATED_BYTES_OF_PENDING_COMPACTION)\n+        );\n+        RocksDBMetrics.addTotalSstFilesSizeMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(TOTAL_SST_FILES_SIZE)\n+        );\n+        RocksDBMetrics.addLiveSstFilesSizeMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(LIVE_SST_FILES_SIZE)\n+        );\n+        RocksDBMetrics.addNumLiveVersionMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_LIVE_VERSIONS)\n+        );\n+        RocksDBMetrics.addEstimateNumKeysMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(ESTIMATED_NUMBER_OF_KEYS)\n+        );\n+        RocksDBMetrics.addEstimateTableReadersMemMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(ESTIMATED_MEMORY_OF_TABLE_READERS)\n+        );\n+        RocksDBMetrics.addBackgroundErrorsMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeSumOfProperties(NUMBER_OF_BACKGROUND_ERRORS)\n+        );\n+        RocksDBMetrics.addBlockCacheCapacityMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeBlockCacheMetrics(CAPACITY_OF_BLOCK_CACHE)\n+        );\n+        RocksDBMetrics.addBlockCacheUsageMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeBlockCacheMetrics(USAGE_OF_BLOCK_CACHE)\n+        );\n+        RocksDBMetrics.addBlockCachePinnedUsageMetric(\n+            streamsMetrics,\n+            metricContext,\n+            gaugeToComputeBlockCacheMetrics(PINNED_USAGE_OF_BLOCK_CACHE)\n+        );\n+    }\n+\n+    private Gauge<BigInteger> gaugeToComputeSumOfProperties(final String propertyName) {\n+        return (metricsConfig, now) -> {\n             BigInteger result = BigInteger.valueOf(0);\n             for (final DbAndCacheAndStatistics valueProvider : storeToValueProviders.values()) {\n                 try {\n                     // values of RocksDB properties are of type unsigned long in C++, i.e., in Java we need to use\n                     // BigInteger and construct the object from the byte representation of the value\n                     result = result.add(new BigInteger(1, longToBytes(\n-                        valueProvider.db.getAggregatedLongProperty(ROCKSDB_PROPERTIES_PREFIX + NUMBER_OF_ENTRIES_ACTIVE_MEMTABLE))));\n+                        valueProvider.db.getAggregatedLongProperty(ROCKSDB_PROPERTIES_PREFIX + propertyName)\n+                    )));\n+                } catch (final RocksDBException e) {", "originalCommit": "91d16685e1460ec903956b40f5eedf1fb21e16b9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDkyNzAxMw==", "url": "https://github.com/apache/kafka/pull/9232#discussion_r480927013", "bodyText": "Actually, I just followed the pattern found in RocksDBStore.", "author": "cadonna", "createdAt": "2020-09-01T07:39:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDQ3ODYyMQ=="}], "type": "inlineReview"}, {"oid": "fd18a43b71d300f764ce2aeba3d75862990ecc93", "url": "https://github.com/apache/kafka/commit/fd18a43b71d300f764ce2aeba3d75862990ecc93", "message": "Include feedback", "committedDate": "2020-09-01T07:40:22Z", "type": "commit"}]}