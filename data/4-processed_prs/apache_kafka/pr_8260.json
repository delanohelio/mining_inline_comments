{"pr_number": 8260, "pr_title": "KAFKA-9625: Fixing IncrementalAlterConfigs with respect to Broker Configs", "pr_createdAt": "2020-03-10T01:42:46Z", "pr_url": "https://github.com/apache/kafka/pull/8260", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQzNDAzNA==", "url": "https://github.com/apache/kafka/pull/8260#discussion_r390434034", "bodyText": "Hmm... we don't want to re-apply configurations that haven't changed between the new and the old set of configs.  We want to generate a map that includes only the changes (whether they are additions or removals.)", "author": "cmccabe", "createdAt": "2020-03-10T16:12:01Z", "path": "core/src/main/scala/kafka/server/DynamicBrokerConfig.scala", "diffHunk": "@@ -507,7 +507,7 @@ class DynamicBrokerConfig(private val kafkaConfig: KafkaConfig) extends Logging\n \n   private def processReconfiguration(newProps: Map[String, String], validateOnly: Boolean): (KafkaConfig, List[BrokerReconfigurable]) = {\n     val newConfig = new KafkaConfig(newProps.asJava, !validateOnly, None)\n-    val updatedMap = updatedConfigs(newConfig.originalsFromThisConfig, currentConfig.originals)\n+    val updatedMap = newConfig.originalsFromThisConfig.asScala", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQ3NTI2Ng==", "url": "https://github.com/apache/kafka/pull/8260#discussion_r390475266", "bodyText": "In the case of only deletions though, how would this work? For example, as in the test, if we first configured all the configs and then deleted them, my understanding was that we have to update the configs by applying all the configs in the original configs without the deleted configs.", "author": "skaundinya15", "createdAt": "2020-03-10T17:11:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQzNDAzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU1MjY4NA==", "url": "https://github.com/apache/kafka/pull/8260#discussion_r390552684", "bodyText": "After thinking about it some more, I realize that the edge case that the test has is that if there are only delete operations done in a call to incrementalAlterConfigs, then updatedConfigs only returns an empty map, which then doesn't actually delete any of the configs. I've fixed this now by checking in updatedConfigs whether or not the new configs is a subset of the old configs, and if it is, simply return that as the updatedMap, if not, the old logic stands. I also wrote a new test that tests a mixture of operations (delete and set in one call). Please let me know what you think of this approach.", "author": "skaundinya15", "createdAt": "2020-03-10T19:16:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQzNDAzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEyNTA1OQ==", "url": "https://github.com/apache/kafka/pull/8260#discussion_r391125059", "bodyText": "There are other cases that need to be handled here.  For example, maybe some things are deleted and some things are added.  In that case, the approach in the PR now still doesn't work, since it falls back on the original incorrect logic.\nThe fundamental problem is that the updatedMap doesn't have entries for deleted elements, but there are parts of the code that equivocate between the updatedMap, and the set of all change deltas.  Either the `updatedMap needs to represent deleted elements somehow, or we need a separate data structure that does.", "author": "cmccabe", "createdAt": "2020-03-11T17:04:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQzNDAzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE1MDE1OQ==", "url": "https://github.com/apache/kafka/pull/8260#discussion_r391150159", "bodyText": "I believe the problem with the previous logic was that it wasn't handling the edge case of when an incrementalAlterConfig operation has only DELETE operations. I did write a test when it deletes some configs but changes another config (in the same incrementalAlterConfig operation) in this PR: https://github.com/apache/kafka/pull/8260/files#diff-90c82107f654d6ec91c2d22c183b78c5R1728-R1766\nThe old logic just checks if the value of the old configs is not equal to the value of the new config, add that config to the updatedMap. I believe this covers all cases except the case of only DELETE operations, which is why I added a check to see if newConfigs were a subset of currentConfigs. If this is the case we can deduce that configs have only been deleted, so we only care about applying the newConfigs for the broker configuration. By applying the new configuration, we can effectively \"delete\" the configurations by reapplying all the new configs.", "author": "skaundinya15", "createdAt": "2020-03-11T17:41:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQzNDAzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgyOTYzNQ==", "url": "https://github.com/apache/kafka/pull/8260#discussion_r391829635", "bodyText": "Think about the case where some things are added and some things are removed.  In this case, your PR will fall back on the original incorrect logic.  As an example, let's say the old config is (foo -> 123, bar -> 345) and the new config is (bar -> 345, baz -> 678).  The current logic in the PR will determine that the new config is not a subset of the old, and then generate an updated configuration that is (foo -> 123, bar -> 345, baz -> 678).  Notice that foo is included even though it shouldn't be.\nThe reason why foo is included even though it shouldn't be is that needsReconfiguration uses the updatedMap to determine what needs to be reconfigured, and you never put foo in that map.", "author": "cmccabe", "createdAt": "2020-03-12T18:59:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQzNDAzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg2MDUzNQ==", "url": "https://github.com/apache/kafka/pull/8260#discussion_r391860535", "bodyText": "I just modified the test I linked above to be the exact case you described, and it passes with the correct logic in the code. The test is as follows:\n  @Test\n  def testIncrementalAlterConfigsDeleteAndSetBrokerConfigs(): Unit = {\n    client = Admin.create(createConfig())\n    val broker0Resource = new ConfigResource(ConfigResource.Type.BROKER, \"0\")\n// LeaderReplicationThrottledRateProp and FollowerReplicationThrottledRateProp are \n// foo and bar as described in your example\n    client.incrementalAlterConfigs(Map(broker0Resource ->\n      Seq(new AlterConfigOp(new ConfigEntry(DynamicConfig.Broker.LeaderReplicationThrottledRateProp, \"123\"),\n          AlterConfigOp.OpType.SET),\n        new AlterConfigOp(new ConfigEntry(DynamicConfig.Broker.FollowerReplicationThrottledRateProp, \"456\"),\n          AlterConfigOp.OpType.SET)\n      ).asJavaCollection).asJava).all().get()\n    TestUtils.waitUntilTrue(() => {\n      val broker0Configs = client.describeConfigs(Seq(broker0Resource).asJava).\n        all().get().get(broker0Resource).entries().asScala.map {\n        case entry => (entry.name, entry.value)\n      }.toMap\n      (\"123\".equals(broker0Configs.getOrElse(DynamicConfig.Broker.LeaderReplicationThrottledRateProp, \"\")) && \n\n\"456\".equals(broker0Configs.getOrElse(DynamicConfig.Broker.FollowerReplicationThrottledRateProp, \"\")))\n    }, \"Expected to see the broker properties we just set\", pause=25)\n// Now we delete LeaderReplicationThrottledRateProp and change \n// FollowerReplicationThrottledRateProp\n// We also add ReplicaAlterLogDirsIoMaxBytesPerSecondProp, which would be baz as in your \n// example\n    client.incrementalAlterConfigs(Map(broker0Resource ->\n      Seq(new AlterConfigOp(new ConfigEntry(DynamicConfig.Broker.LeaderReplicationThrottledRateProp, \"\"),\n        AlterConfigOp.OpType.DELETE),\n        new AlterConfigOp(new ConfigEntry(DynamicConfig.Broker.FollowerReplicationThrottledRateProp, \"654\"),\n          AlterConfigOp.OpType.SET),\n        new AlterConfigOp(new ConfigEntry(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp, \"987\"),\n          AlterConfigOp.OpType.SET)\n      ).asJavaCollection).asJava).all().get()\n    TestUtils.waitUntilTrue(() => {\n      val broker0Configs = client.describeConfigs(Seq(broker0Resource).asJava).\n        all().get().get(broker0Resource).entries().asScala.map {\n        case entry => (entry.name, entry.value)\n      }.toMap\n      (\"\".equals(broker0Configs.getOrElse(DynamicConfig.Broker.LeaderReplicationThrottledRateProp, \"\")) &&\n        \"654\".equals(broker0Configs.getOrElse(DynamicConfig.Broker.FollowerReplicationThrottledRateProp, \"\")) &&\n        \"987\".equals(broker0Configs.getOrElse(DynamicConfig.Broker.ReplicaAlterLogDirsIoMaxBytesPerSecondProp, \"\")))\n    }, \"Expected to see the broker properties we just modified\", pause=25)\n  }\nThe logic for updatedConfigs isn't incorrect, it was missing out on the edge case of if an incrementalAlterConfigs request had only delete operations - that's why the check for subset is there, as if the new config is a subset of the old configs (where we can deduce the only alterations done were DELETE operations), then only the new configs should apply so that the old configs can be deleted. The old logic in question is here:\n      newProps.asScala.filter {\n        case (k, v) => v != currentProps.get(k)\n      }\nThis logic filters the newProps on whether or not each key in newProps retrieved from currentProps has the same value as in newProps. Thus in the case you described, when the old config is foo -> 123, bar -> 345) and the new config is (bar -> 345, baz -> 678), the logic will determine that the new config is not a subset of the old config, and generate an updated configuration by iterating through the keys of the new config.\nThere are only 2 keys in the new config it goes through, bar and baz. It checks the value of bar in the old config, and finds that the value of bar in the old config is not the same as the value in the new config, therefore this key-value pair is added. Similarly, it checks the value of baz in the old config, and since there is no entry for that key, it returns null, which is also not the same as the value of baz in the new config. Therefore that key-value pair is also added. Thus the updatedMap is returned as (bar -> 345, baz -> 678), which is correct, as that is what we want applied.\nHowever if we were filtering by iterating through currentProps, then we would run into the bug you mentioned.", "author": "skaundinya15", "createdAt": "2020-03-12T20:00:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQzNDAzNA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0NDYyOA==", "url": "https://github.com/apache/kafka/pull/8260#discussion_r391944628", "bodyText": "This isn't quite deleted keys, right?  This is keys that are in the current map, but have a different value (or no value) in the new map.\nDeleted keys would be something like currentProps.asScala.filter(!newProps.contains(_))", "author": "cmccabe", "createdAt": "2020-03-12T22:45:04Z", "path": "core/src/main/scala/kafka/server/DynamicBrokerConfig.scala", "diffHunk": "@@ -465,10 +465,15 @@ class DynamicBrokerConfig(private val kafkaConfig: KafkaConfig) extends Logging\n       reconfigurable.reconfigure(newConfig)\n   }\n \n-  private def updatedConfigs(newProps: java.util.Map[String, _], currentProps: java.util.Map[_, _]): mutable.Map[String, _] = {\n-    newProps.asScala.filter {\n+  private def updatedConfigs(newProps: java.util.Map[String, _], currentProps: java.util.Map[String, _]):\n+  (mutable.Map[String, _], Set[String]) = {\n+    val deletedKeySet = currentProps.asScala.filter {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk2NjIwOA==", "url": "https://github.com/apache/kafka/pull/8260#discussion_r391966208", "bodyText": "Ah yes that's right, my bad. I've fixed this to reflect what deletedKeys should be.", "author": "skaundinya15", "createdAt": "2020-03-13T00:03:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0NDYyOA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY0MjczOQ==", "url": "https://github.com/apache/kafka/pull/8260#discussion_r392642739", "bodyText": "We should have JavaDoc for this function to identify what it returns.  Also, this is a nitpick, but considering that deletedKeySet is returned as the second part of the tuple, why not put the code for calculating it second in the function?", "author": "cmccabe", "createdAt": "2020-03-15T05:23:59Z", "path": "core/src/main/scala/kafka/server/DynamicBrokerConfig.scala", "diffHunk": "@@ -469,10 +469,15 @@ class DynamicBrokerConfig(private val kafkaConfig: KafkaConfig) extends Logging\n       reconfigurable.reconfigure(newConfig)\n   }\n \n-  private def updatedConfigs(newProps: java.util.Map[String, _], currentProps: java.util.Map[_, _]): mutable.Map[String, _] = {\n-    newProps.asScala.filter {\n+  private def updatedConfigs(newProps: java.util.Map[String, _], currentProps: java.util.Map[String, _]):", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY0ODIxMg==", "url": "https://github.com/apache/kafka/pull/8260#discussion_r392648212", "bodyText": "Just pushed a new version that computes deletedKeySet second and has a javadoc for the function.", "author": "skaundinya15", "createdAt": "2020-03-15T07:21:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY0MjczOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY0MjgxNA==", "url": "https://github.com/apache/kafka/pull/8260#discussion_r392642814", "bodyText": "This works, but it looks a little nicer from a scala point of view to have val (changeMap, deletedKeySet) = updatedConfigs(...).  It's a bit of an adjustment coming from Java, I know.", "author": "cmccabe", "createdAt": "2020-03-15T05:25:51Z", "path": "core/src/main/scala/kafka/server/DynamicBrokerConfig.scala", "diffHunk": "@@ -511,23 +516,25 @@ class DynamicBrokerConfig(private val kafkaConfig: KafkaConfig) extends Logging\n \n   private def processReconfiguration(newProps: Map[String, String], validateOnly: Boolean): (KafkaConfig, List[BrokerReconfigurable]) = {\n     val newConfig = new KafkaConfig(newProps.asJava, !validateOnly, None)\n-    val updatedMap = updatedConfigs(newConfig.originalsFromThisConfig, currentConfig.originals)\n-    if (updatedMap.nonEmpty) {\n+    val updatedConfig = updatedConfigs(newConfig.originalsFromThisConfig, currentConfig.originals)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY0ODIyNg==", "url": "https://github.com/apache/kafka/pull/8260#discussion_r392648226", "bodyText": "Ah okay wasn't sure if I could do that - good to know. I have just made the change and pushed.", "author": "skaundinya15", "createdAt": "2020-03-15T07:21:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY0MjgxNA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "b4a8d13917e93227e369b1961b8c24e4318c9957", "url": "https://github.com/apache/kafka/commit/b4a8d13917e93227e369b1961b8c24e4318c9957", "message": "KAFKA-9625: Broker throttles are incorrectly marked as sensitive configurations", "committedDate": "2020-03-17T18:16:04Z", "type": "commit"}, {"oid": "85f82a2c5b71054112ed95fa3652946bae742a9b", "url": "https://github.com/apache/kafka/commit/85f82a2c5b71054112ed95fa3652946bae742a9b", "message": "reverting log4j props", "committedDate": "2020-03-17T18:16:04Z", "type": "commit"}, {"oid": "cf44f4a39be41debf185b356a50a8fb5f53f4e95", "url": "https://github.com/apache/kafka/commit/cf44f4a39be41debf185b356a50a8fb5f53f4e95", "message": "fixing only deletion edge case bug", "committedDate": "2020-03-17T18:16:05Z", "type": "commit"}, {"oid": "c225d49beaa6fe461702e1ea719e5fc721f9f172", "url": "https://github.com/apache/kafka/commit/c225d49beaa6fe461702e1ea719e5fc721f9f172", "message": "modified test a bit", "committedDate": "2020-03-17T18:16:05Z", "type": "commit"}, {"oid": "cfdb74da1295cdaa0d6cb0a720750aecd3d3efac", "url": "https://github.com/apache/kafka/commit/cfdb74da1295cdaa0d6cb0a720750aecd3d3efac", "message": "removing unnecessary log", "committedDate": "2020-03-17T18:16:05Z", "type": "commit"}, {"oid": "dd641cf86a67cec4cbaec91424185ddfdceb7b45", "url": "https://github.com/apache/kafka/commit/dd641cf86a67cec4cbaec91424185ddfdceb7b45", "message": "addressing PR comments", "committedDate": "2020-03-17T18:16:05Z", "type": "commit"}, {"oid": "bdb06be5afd43c4addf3d238ee6a3fb80da5b65d", "url": "https://github.com/apache/kafka/commit/bdb06be5afd43c4addf3d238ee6a3fb80da5b65d", "message": "simplifying logic a bit", "committedDate": "2020-03-17T18:16:05Z", "type": "commit"}, {"oid": "1b74532d73612e793275709152413ff4abd21567", "url": "https://github.com/apache/kafka/commit/1b74532d73612e793275709152413ff4abd21567", "message": "addressing PR comments", "committedDate": "2020-03-17T18:16:05Z", "type": "commit"}, {"oid": "71f2c99ed8e8d09a70213ad60b77f69f0e5415ca", "url": "https://github.com/apache/kafka/commit/71f2c99ed8e8d09a70213ad60b77f69f0e5415ca", "message": "rebasing commits on trunk", "committedDate": "2020-03-17T18:16:05Z", "type": "commit"}, {"oid": "53b78e4b409ed8d1daa62efd226f1f235fb4a81f", "url": "https://github.com/apache/kafka/commit/53b78e4b409ed8d1daa62efd226f1f235fb4a81f", "message": "addressing PR comments", "committedDate": "2020-03-17T18:16:05Z", "type": "commit"}, {"oid": "53b78e4b409ed8d1daa62efd226f1f235fb4a81f", "url": "https://github.com/apache/kafka/commit/53b78e4b409ed8d1daa62efd226f1f235fb4a81f", "message": "addressing PR comments", "committedDate": "2020-03-17T18:16:05Z", "type": "forcePushed"}]}