{"pr_number": 9177, "pr_title": "KAFKA-9924: Add RocksDB metric num-entries-active-mem-table", "pr_createdAt": "2020-08-13T20:33:59Z", "pr_url": "https://github.com/apache/kafka/pull/9177", "timeline": [{"oid": "dd647535339a9d0cc6003caa5f297195fd64d615", "url": "https://github.com/apache/kafka/commit/dd647535339a9d0cc6003caa5f297195fd64d615", "message": "Add wrapper around BlockBasedTableConfig to make cache accessible", "committedDate": "2020-07-29T11:34:40Z", "type": "commit"}, {"oid": "634d18b15ff400dced3b1af6b43c98630e115d8d", "url": "https://github.com/apache/kafka/commit/634d18b15ff400dced3b1af6b43c98630e115d8d", "message": "Refactor RocksDBMetricsRecorder and instantiation of RocksDBMetricsRecordingTrigger", "committedDate": "2020-07-29T11:34:40Z", "type": "commit"}, {"oid": "67986c4b3e7c8fa627ec52a10bbdc3138e828b8d", "url": "https://github.com/apache/kafka/commit/67986c4b3e7c8fa627ec52a10bbdc3138e828b8d", "message": "Add unit test when user specifies new table format config", "committedDate": "2020-07-30T08:51:12Z", "type": "commit"}, {"oid": "0afb35d8bba0d76adf9002bd9db14025cd0ca340", "url": "https://github.com/apache/kafka/commit/0afb35d8bba0d76adf9002bd9db14025cd0ca340", "message": "Make RocksDB recording trigger member variable final", "committedDate": "2020-07-30T10:25:54Z", "type": "commit"}, {"oid": "4331821d84ecc8873f814cd691e9bb13b7762fe2", "url": "https://github.com/apache/kafka/commit/4331821d84ecc8873f814cd691e9bb13b7762fe2", "message": "Make warning regarding RocksDB's table configuration clearer", "committedDate": "2020-07-31T13:46:51Z", "type": "commit"}, {"oid": "34dac91f6249d809df5b366eaf82596dde3d5b3b", "url": "https://github.com/apache/kafka/commit/34dac91f6249d809df5b366eaf82596dde3d5b3b", "message": "Remove unused parameter", "committedDate": "2020-07-31T13:50:02Z", "type": "commit"}, {"oid": "91b3430b9db78b0cff834d6197f509f65a639dcd", "url": "https://github.com/apache/kafka/commit/91b3430b9db78b0cff834d6197f509f65a639dcd", "message": "Throw exception instead of log a warning", "committedDate": "2020-08-10T20:15:57Z", "type": "commit"}, {"oid": "ebab7af0bb7ea98af111aa626874f678d9fe3c56", "url": "https://github.com/apache/kafka/commit/ebab7af0bb7ea98af111aa626874f678d9fe3c56", "message": "Allow other table formats than block-based tables", "committedDate": "2020-08-11T19:41:48Z", "type": "commit"}, {"oid": "5c967a48033bcf92438e8fc419a6a8b4835ba665", "url": "https://github.com/apache/kafka/commit/5c967a48033bcf92438e8fc419a6a8b4835ba665", "message": "Improve statistics handling in metrics recorder", "committedDate": "2020-08-12T08:36:07Z", "type": "commit"}, {"oid": "fb15c3a5bf91f9a46d6914739b617ca2291f52da", "url": "https://github.com/apache/kafka/commit/fb15c3a5bf91f9a46d6914739b617ca2291f52da", "message": "Change guard to avoid recording when statistics are null", "committedDate": "2020-08-12T15:27:07Z", "type": "commit"}, {"oid": "cf7c79f9148244d37b502fbdb4e4e86b818ea3ac", "url": "https://github.com/apache/kafka/commit/cf7c79f9148244d37b502fbdb4e4e86b818ea3ac", "message": "Add methods to add gauge metrics on state store level", "committedDate": "2020-08-12T15:31:38Z", "type": "commit"}, {"oid": "86bcf00fc34a840160029178ca3b0ca77d4f9443", "url": "https://github.com/apache/kafka/commit/86bcf00fc34a840160029178ca3b0ca77d4f9443", "message": "Add metric num-entries-active-mem-table", "committedDate": "2020-08-12T15:31:38Z", "type": "commit"}, {"oid": "a54a5053720a7f692d4b650ee6fa1e02fe436c75", "url": "https://github.com/apache/kafka/commit/a54a5053720a7f692d4b650ee6fa1e02fe436c75", "message": "Merge remote-tracking branch 'upstream/trunk' into AK9924-add-metrics", "committedDate": "2020-08-13T19:55:24Z", "type": "commit"}, {"oid": "1c733ffa84da60c9fb9b93f532c884beb7bd3063", "url": "https://github.com/apache/kafka/commit/1c733ffa84da60c9fb9b93f532c884beb7bd3063", "message": "Fix compile error", "committedDate": "2020-08-13T20:13:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4MjU4MA==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r471582580", "bodyText": "Should we check if (metrics.metric(metricName) == null) again after synchronizing?", "author": "vvcephei", "createdAt": "2020-08-17T16:05:04Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -415,9 +416,40 @@ public final Sensor storeLevelSensor(final String threadId,\n         }\n     }\n \n-    public final void removeAllStoreLevelSensors(final String threadId,\n-                                                 final String taskId,\n-                                                 final String storeName) {\n+    public <T> void addStoreLevelMutableMetric(final String threadId,\n+                                               final String taskId,\n+                                               final String metricsScope,\n+                                               final String storeName,\n+                                               final String name,\n+                                               final String description,\n+                                               final RecordingLevel recordingLevel,\n+                                               final Gauge<T> valueProvider) {\n+        final MetricName metricName = metrics.metricName(\n+            name,\n+            STATE_STORE_LEVEL_GROUP,\n+            description,\n+            storeLevelTagMap(threadId, taskId, metricsScope, storeName)\n+        );\n+        if (metrics.metric(metricName) == null) {\n+            final MetricConfig metricConfig = new MetricConfig().recordLevel(recordingLevel);\n+            final String key = storeSensorPrefix(threadId, taskId, storeName);\n+            synchronized (storeLevelMetrics) {", "originalCommit": "1c733ffa84da60c9fb9b93f532c884beb7bd3063", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjE4MzQ5Mw==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472183493", "bodyText": "I will do that to ensure that the removeAllStoreLevelMetrics() completes before we do the check.", "author": "cadonna", "createdAt": "2020-08-18T13:16:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4MjU4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4NDAyNQ==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r471584025", "bodyText": "Should we make this all one method, and also synchronize both storeLevel collections on a single monitor?", "author": "vvcephei", "createdAt": "2020-08-17T16:07:25Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -415,9 +416,40 @@ public final Sensor storeLevelSensor(final String threadId,\n         }\n     }\n \n-    public final void removeAllStoreLevelSensors(final String threadId,\n-                                                 final String taskId,\n-                                                 final String storeName) {\n+    public <T> void addStoreLevelMutableMetric(final String threadId,\n+                                               final String taskId,\n+                                               final String metricsScope,\n+                                               final String storeName,\n+                                               final String name,\n+                                               final String description,\n+                                               final RecordingLevel recordingLevel,\n+                                               final Gauge<T> valueProvider) {\n+        final MetricName metricName = metrics.metricName(\n+            name,\n+            STATE_STORE_LEVEL_GROUP,\n+            description,\n+            storeLevelTagMap(threadId, taskId, metricsScope, storeName)\n+        );\n+        if (metrics.metric(metricName) == null) {\n+            final MetricConfig metricConfig = new MetricConfig().recordLevel(recordingLevel);\n+            final String key = storeSensorPrefix(threadId, taskId, storeName);\n+            synchronized (storeLevelMetrics) {\n+                metrics.addMetric(metricName, metricConfig, valueProvider);\n+                storeLevelMetrics.computeIfAbsent(key, ignored -> new LinkedList<>()).push(metricName);\n+            }\n+        }\n+    }\n+\n+    public final void removeAllStoreLevelSensorsAndMetrics(final String threadId,\n+                                                           final String taskId,\n+                                                           final String storeName) {\n+        removeAllStoreLevelMetrics(threadId, taskId, storeName);\n+        removeAllStoreLevelSensors(threadId, taskId, storeName);\n+    }", "originalCommit": "1c733ffa84da60c9fb9b93f532c884beb7bd3063", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIwMDM2MA==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472200360", "bodyText": "Do you have performance concerns due to the two monitors? Or what is the main reason for using a single monitor here? By using a single monitor here and in addStoreLevelMutableMetric() and storeLevelSensor(), we do not ensure that no metrics are added to the metrics map during removal of all metrics because each time  Sensor#add() is called a metric is added without synchronizing on the monitor of storeLevelSensors. Single operations on the metrics map are synchronized (through ConcurrentMap), but not multiple operations.", "author": "cadonna", "createdAt": "2020-08-18T13:33:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4NDAyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI1MDYxMg==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472250612", "bodyText": "It just seems oddly granular to synchronize them individually, since we always remove all of both collections together. If it doesn't matter, then do we need to synchronize at all?", "author": "vvcephei", "createdAt": "2020-08-18T14:42:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4NDAyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI3NjYxNQ==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472276615", "bodyText": "Yes, we need to synchronize. At least, we have to ensure that lines 411 and 438 are thread-safe. Then, if we do not want to have duplicates in storeLevelSensors we should ensure to have a lock between line 409 to 411. Between line 434 and 439, we need to ensure that the removal of all store level metrics completed otherwise it could happen that we find a store level metric that would prevent the addition of a metric but then the earlier found metric would be removed during the remainder of the removal process. Similar is true for the store level sensors.\nIt is true that we always remove all of both collections together, but we do not add metric names and sensor names to both collections together.", "author": "cadonna", "createdAt": "2020-08-18T15:17:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4NDAyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQxNjEzOA==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472416138", "bodyText": "This may be a bit paranoid, but when adding them, the order seem to be initSensors first and initGauges, while removing we call removeAllStoreLevelMetrics first and then the other. I know that today there should be not concurrent threads trying to init / removeAll concurrently, but just to be safe maybe we can make the call ordering to be sensors first and then gauges?", "author": "guozhangwang", "createdAt": "2020-08-18T19:02:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4NDAyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4ODA0OQ==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r471588049", "bodyText": "It seems a little risky to use this in a multithreaded context. Why not just create a new short-lived buffer each time for the conversion?", "author": "vvcephei", "createdAt": "2020-08-17T16:14:11Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java", "diffHunk": "@@ -56,6 +62,9 @@ public void maybeCloseStatistics() {\n         }\n     }\n \n+    private static final String ROCKSDB_PROPERTIES_PREFIX = \"rocksdb.\";\n+    private static final ByteBuffer CONVERSION_BUFFER = ByteBuffer.allocate(Long.BYTES);", "originalCommit": "1c733ffa84da60c9fb9b93f532c884beb7bd3063", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjEwMDk1MQ==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472100951", "bodyText": "Good point! I missed that the gauge can be called by multiple metrics reporters concurrently.", "author": "cadonna", "createdAt": "2020-08-18T11:16:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4ODA0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU5NTIxNg==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r471595216", "bodyText": "would it not be exactly 1?", "author": "vvcephei", "createdAt": "2020-08-17T16:26:22Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -609,6 +611,37 @@ public void shouldVerifyThatMetricsGetMeasurementsFromRocksDB() {\n         assertThat((double) bytesWrittenTotal.metricValue(), greaterThan(0d));\n     }\n \n+    @Test\n+    public void shouldVerifyThatMetricsRecordedFromPropertiesGetMeasurementsFromRocksDB() {\n+        final TaskId taskId = new TaskId(0, 0);\n+\n+        final Metrics metrics = new Metrics(new MetricConfig().recordLevel(RecordingLevel.INFO));\n+        final StreamsMetricsImpl streamsMetrics =\n+            new StreamsMetricsImpl(metrics, \"test-application\", StreamsConfig.METRICS_LATEST, time);\n+\n+        context = EasyMock.niceMock(InternalMockProcessorContext.class);\n+        EasyMock.expect(context.metrics()).andStubReturn(streamsMetrics);\n+        EasyMock.expect(context.taskId()).andStubReturn(taskId);\n+        EasyMock.expect(context.appConfigs())\n+                .andStubReturn(new StreamsConfig(StreamsTestUtils.getStreamsConfig()).originals());\n+        EasyMock.expect(context.stateDir()).andStubReturn(dir);\n+        EasyMock.replay(context);\n+\n+        rocksDBStore.init(context, rocksDBStore);\n+        final byte[] key = \"hello\".getBytes();\n+        final byte[] value = \"world\".getBytes();\n+        rocksDBStore.put(Bytes.wrap(key), value);\n+\n+        final Metric numberOfEntriesActiveMemTable = metrics.metric(new MetricName(\n+            \"num-entries-active-mem-table\",\n+            StreamsMetricsImpl.STATE_STORE_LEVEL_GROUP,\n+            \"description is not verified\",\n+            streamsMetrics.storeLevelTagMap(Thread.currentThread().getName(), taskId.toString(), METRICS_SCOPE, DB_NAME)\n+        ));\n+        assertThat(numberOfEntriesActiveMemTable, notNullValue());\n+        assertThat((BigInteger) numberOfEntriesActiveMemTable.metricValue(), greaterThan(BigInteger.valueOf(0)));", "originalCommit": "1c733ffa84da60c9fb9b93f532c884beb7bd3063", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjEwNTUyMA==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472105520", "bodyText": "Yes, but in this test I merely test whether the metric is updated. The correctness of the computation is verified in RocksDBMetricsRecorderGaugesTest. I will improve this test to verify that the metric is zero before the put and greater than zero after the put.", "author": "cadonna", "createdAt": "2020-08-18T11:25:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU5NTIxNg=="}], "type": "inlineReview"}, {"oid": "f7e6c661557d2a91a9a6c736b00cf7dff01c0d89", "url": "https://github.com/apache/kafka/commit/f7e6c661557d2a91a9a6c736b00cf7dff01c0d89", "message": "Include feedback", "committedDate": "2020-08-18T13:34:44Z", "type": "commit"}, {"oid": "f77543b3ff2f51b892fa807d9ddbe0b5b0d0729f", "url": "https://github.com/apache/kafka/commit/f77543b3ff2f51b892fa807d9ddbe0b5b0d0729f", "message": "Small refactoring", "committedDate": "2020-08-18T15:48:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQyOTE0MQ==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r472429141", "bodyText": "nit: extra space.", "author": "guozhangwang", "createdAt": "2020-08-18T19:26:40Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetrics.java", "diffHunk": "@@ -33,7 +35,7 @@\n import static org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.addSumMetricToSensor;\n import static org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.addValueMetricToSensor;\n \n-public class RocksDBMetrics {\n+public class  RocksDBMetrics {", "originalCommit": "f77543b3ff2f51b892fa807d9ddbe0b5b0d0729f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "08299ba8e1e8ca0ee760f09db41395972ddb478a", "url": "https://github.com/apache/kafka/commit/08299ba8e1e8ca0ee760f09db41395972ddb478a", "message": "Include feedback", "committedDate": "2020-08-19T07:42:41Z", "type": "commit"}, {"oid": "4b1909dc3f9167ef3a16408dabd207059f794904", "url": "https://github.com/apache/kafka/commit/4b1909dc3f9167ef3a16408dabd207059f794904", "message": "Remove synchronized during adding and removing store level sensors and metrics", "committedDate": "2020-08-19T09:08:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQyNDU3OA==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r477424578", "bodyText": "I'm still mildly concerned about walking back the synchronization here, but I can't think of a realistic scenario in which we'd get a concurrency bug. Then again, the whole point of defaulting to less granular concurrency controls is that it's hard to imagine all the possible scenarios.\nIn this case, it really doesn't seem like there's a good reason to go for super granular concurrency control. Did we spend a lot of time blocked registering sensors before?\nActually, one condition comes to mind: LinkedList is not threadsafe, and accessing the ConcurrentHashMap value is only either a CAS or volatile read, so it doesn't create a memory barrier as synchronized does. Therefore, different threads will only be looking at their own locally cached list for each value in the map, although they'll all agree on the set of keys in the map.\nIf you want to push the current implementation style, then you should use a ConcurrentLinkedDeque instead of LinkedList, but I'd really prefer to see the synchronized blocks come back unless/until there's a compelling performance reason to drop them.", "author": "vvcephei", "createdAt": "2020-08-26T16:17:47Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java", "diffHunk": "@@ -396,34 +398,65 @@ private String cacheSensorPrefix(final String threadId, final String taskId, fin\n             + SENSOR_PREFIX_DELIMITER + SENSOR_CACHE_LABEL + SENSOR_PREFIX_DELIMITER + cacheName;\n     }\n \n-    public final Sensor storeLevelSensor(final String threadId,\n-                                         final String taskId,\n+    public final Sensor storeLevelSensor(final String taskId,\n                                          final String storeName,\n                                          final String sensorName,\n-                                         final Sensor.RecordingLevel recordingLevel,\n+                                         final RecordingLevel recordingLevel,\n                                          final Sensor... parents) {\n-        final String key = storeSensorPrefix(threadId, taskId, storeName);\n-        synchronized (storeLevelSensors) {\n-            final String fullSensorName = key + SENSOR_NAME_DELIMITER + sensorName;\n-            final Sensor sensor = metrics.getSensor(fullSensorName);\n-            if (sensor == null) {\n+        final String key = storeSensorPrefix(Thread.currentThread().getName(), taskId, storeName);\n+        final String fullSensorName = key + SENSOR_NAME_DELIMITER + sensorName;\n+        return Optional.ofNullable(metrics.getSensor(fullSensorName))\n+            .orElseGet(() -> {\n                 storeLevelSensors.computeIfAbsent(key, ignored -> new LinkedList<>()).push(fullSensorName);\n                 return metrics.sensor(fullSensorName, recordingLevel, parents);\n-            } else {\n-                return sensor;\n-            }\n+            });", "originalCommit": "4b1909dc3f9167ef3a16408dabd207059f794904", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODUwNDUzMg==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r478504532", "bodyText": "After our offline discussion, I added some clarifications in the code.", "author": "cadonna", "createdAt": "2020-08-27T15:26:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQyNDU3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcwOTUyNQ==", "url": "https://github.com/apache/kafka/pull/9177#discussion_r478709525", "bodyText": "Thanks!", "author": "vvcephei", "createdAt": "2020-08-27T21:36:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQyNDU3OA=="}], "type": "inlineReview"}, {"oid": "6773fca37bae28412562b903fdaac4579f3f664a", "url": "https://github.com/apache/kafka/commit/6773fca37bae28412562b903fdaac4579f3f664a", "message": "Add some clarifications", "committedDate": "2020-08-27T15:24:46Z", "type": "commit"}, {"oid": "f2d5ed0aedd5f8af2516d00b1cdbaf83b145e3c0", "url": "https://github.com/apache/kafka/commit/f2d5ed0aedd5f8af2516d00b1cdbaf83b145e3c0", "message": "Merge remote-tracking branch 'apache/trunk' into pull/9177", "committedDate": "2020-08-27T21:48:14Z", "type": "commit"}, {"oid": "8cf214e95b5cc9df49e34e04a56f80ee22e0994b", "url": "https://github.com/apache/kafka/commit/8cf214e95b5cc9df49e34e04a56f80ee22e0994b", "message": "fix conflict with trunk", "committedDate": "2020-08-27T22:03:26Z", "type": "commit"}]}