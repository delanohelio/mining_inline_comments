{"pr_number": 9139, "pr_title": "KAFKA-9929: Support backward iterator on SessionStore", "pr_createdAt": "2020-08-07T16:24:09Z", "pr_url": "https://github.com/apache/kafka/pull/9139", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk5Njg3OQ==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r499996879", "bodyText": "Can you fix the keyFrom == keyTo to use .equals on the side (down on line 370)", "author": "ableegoldman", "createdAt": "2020-10-06T04:18:53Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java", "diffHunk": "@@ -359,7 +431,11 @@ private void getNextSegmentIterator() {\n             setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n \n             current.close();\n-            current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n+            if (forward) {\n+                current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n+            } else {\n+                current = context.cache().reverseRange(cacheName, cacheKeyFrom, cacheKeyTo);\n+            }\n         }\n \n         private void setCacheKeyRange(final long lowerRangeEndTime, final long upperRangeEndTime) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk5ODcyMg==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r499998722", "bodyText": "I think we're going to need some additional changes in this class similar to what we had in CachingWindowStore. Definitely at least in getNextSegmentIterator(). Let's make sure to have some cross-segment test coverage here as well, especially because the iteration logic of session store range queries is the hardest to wrap your head around out of all the stores (at least, it is for me)", "author": "ableegoldman", "createdAt": "2020-10-06T04:27:12Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java", "diffHunk": "@@ -359,7 +431,11 @@ private void getNextSegmentIterator() {\n             setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n \n             current.close();\n-            current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n+            if (forward) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMzNjI1Mw==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r501336253", "bodyText": "One of the reasons these missing pieces are not throwing exceptions seems to be that Caching Store tests are only covering or InMemory or Persistent underlying stores. This means only one path of the caching logic is followed (e.g. persistent() ? iteratorWraper : otherIterator)\nI rename the test classes to be explicit about what backing store is covered. We could probably cover this as another issue?", "author": "jeqo", "createdAt": "2020-10-07T22:04:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk5ODcyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM0Njk0Mw==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r501346943", "bodyText": "Ohh yeah good catch. Kind of weird that we're inconsistent with which underlying store type is used in the existing caching store tests, too. But thanks for adding the in-memory + persistent flavors for the SessionStore, I think it sounds reasonable to file a ticket to follow up on the Window/KV flavors and clean up the caching tests", "author": "ableegoldman", "createdAt": "2020-10-07T22:31:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk5ODcyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk5OTMzMw==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r499999333", "bodyText": "If/else needs brackets", "author": "ableegoldman", "createdAt": "2020-10-06T04:29:42Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java", "diffHunk": "@@ -382,9 +478,20 @@ private boolean setInnerIterators() {\n                 currentKey = nextKeyEntry.getKey();\n \n                 if (latestSessionStartTime == Long.MAX_VALUE) {\n-                    recordIterator = nextKeyEntry.getValue().entrySet().iterator();\n+                    final Set<Entry<Long, byte[]>> entries;\n+                    if (forward) entries = nextKeyEntry.getValue().descendingMap().entrySet();\n+                    else entries = nextKeyEntry.getValue().entrySet();\n+                    recordIterator = entries.iterator();\n                 } else {\n-                    recordIterator = nextKeyEntry.getValue().headMap(latestSessionStartTime, true).entrySet().iterator();\n+                    final Set<Entry<Long, byte[]>> entries;\n+                    if (forward) entries = nextKeyEntry.getValue()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk5OTY3NA==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r499999674", "bodyText": "missing newline", "author": "ableegoldman", "createdAt": "2020-10-06T04:31:09Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java", "diffHunk": "@@ -201,7 +247,26 @@ public void remove(final Windowed<Bytes> sessionKey) {\n \n         removeExpiredSegments();\n \n-        return registerNewIterator(key, key, Long.MAX_VALUE, endTimeMap.entrySet().iterator());\n+        return registerNewIterator(\n+            key,\n+            key,\n+            Long.MAX_VALUE, endTimeMap.entrySet().iterator(),", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY1ODU2MA==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r500658560", "bodyText": "Ok I think that for the reverse case, this should be initialized to cacheFunction.segmentId(maxObservedTimestamp) and lastSegmentId should be initialized to this (segmentId(earliestSessionEndTime)).", "author": "ableegoldman", "createdAt": "2020-10-06T23:50:49Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java", "diffHunk": "@@ -270,25 +335,32 @@ public void close() {\n \n         private CacheIteratorWrapper(final Bytes key,\n                                      final long earliestSessionEndTime,\n-                                     final long latestSessionStartTime) {\n-            this(key, key, earliestSessionEndTime, latestSessionStartTime);\n+                                     final long latestSessionStartTime,\n+                                     final boolean forward) {\n+            this(key, key, earliestSessionEndTime, latestSessionStartTime, forward);\n         }\n \n         private CacheIteratorWrapper(final Bytes keyFrom,\n                                      final Bytes keyTo,\n                                      final long earliestSessionEndTime,\n-                                     final long latestSessionStartTime) {\n+                                     final long latestSessionStartTime,\n+                                     final boolean forward) {\n             this.keyFrom = keyFrom;\n             this.keyTo = keyTo;\n             this.latestSessionStartTime = latestSessionStartTime;\n             this.lastSegmentId = cacheFunction.segmentId(maxObservedTimestamp);\n             this.segmentInterval = cacheFunction.getSegmentInterval();\n+            this.forward = forward;\n \n             this.currentSegmentId = cacheFunction.segmentId(earliestSessionEndTime);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTMzNDg4OA==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r501334888", "bodyText": "right! great catch! forgot to align this with WindowStore.", "author": "jeqo", "createdAt": "2020-10-07T22:01:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY1ODU2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY2NDc1MQ==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r500664751", "bodyText": "We're losing the ordering check by comparing this as a set, let's use a list (or whatever) to verify the actual order", "author": "ableegoldman", "createdAt": "2020-10-07T00:11:32Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/CachingSessionStoreTest.java", "diffHunk": "@@ -301,6 +366,29 @@ public void shouldFetchRangeCorrectlyAcrossSegments() {\n         assertEquals(mkSet(a1, a2, a3, aa1, aa3), keys);\n     }\n \n+    @Test\n+    public void shouldBackwardFetchRangeCorrectlyAcrossSegments() {\n+        final Windowed<Bytes> a1 = new Windowed<>(keyA, new SessionWindow(SEGMENT_INTERVAL * 0, SEGMENT_INTERVAL * 0));\n+        final Windowed<Bytes> aa1 = new Windowed<>(keyAA, new SessionWindow(SEGMENT_INTERVAL * 0, SEGMENT_INTERVAL * 0));\n+        final Windowed<Bytes> a2 = new Windowed<>(keyA, new SessionWindow(SEGMENT_INTERVAL * 1, SEGMENT_INTERVAL * 1));\n+        final Windowed<Bytes> a3 = new Windowed<>(keyA, new SessionWindow(SEGMENT_INTERVAL * 2, SEGMENT_INTERVAL * 2));\n+        final Windowed<Bytes> aa3 = new Windowed<>(keyAA, new SessionWindow(SEGMENT_INTERVAL * 2, SEGMENT_INTERVAL * 2));\n+        cachingStore.put(a1, \"1\".getBytes());\n+        cachingStore.put(aa1, \"1\".getBytes());\n+        cachingStore.put(a2, \"2\".getBytes());\n+        cachingStore.put(a3, \"3\".getBytes());\n+        cachingStore.put(aa3, \"3\".getBytes());\n+\n+        final KeyValueIterator<Windowed<Bytes>, byte[]> rangeResults =\n+            cachingStore.backwardFindSessions(keyA, keyAA, 0, SEGMENT_INTERVAL * 2);\n+        final Set<Windowed<Bytes>> keys = new HashSet<>();\n+        while (rangeResults.hasNext()) {\n+            keys.add(rangeResults.next().key);\n+        }\n+        rangeResults.close();\n+        assertEquals(mkSet(a1, a2, a3, aa1, aa3), keys);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY2NDkyNw==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r500664927", "bodyText": "Can we add a few more records that span multiple segments that don't get flushed as well?", "author": "ableegoldman", "createdAt": "2020-10-07T00:12:05Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/CachingSessionStoreTest.java", "diffHunk": "@@ -278,6 +326,23 @@ public void shouldFetchCorrectlyAcrossSegments() {\n         assertFalse(results.hasNext());\n     }\n \n+    @Test\n+    public void shouldBackwardFetchCorrectlyAcrossSegments() {\n+        final Windowed<Bytes> a1 = new Windowed<>(keyA, new SessionWindow(SEGMENT_INTERVAL * 0, SEGMENT_INTERVAL * 0));\n+        final Windowed<Bytes> a2 = new Windowed<>(keyA, new SessionWindow(SEGMENT_INTERVAL * 1, SEGMENT_INTERVAL * 1));\n+        final Windowed<Bytes> a3 = new Windowed<>(keyA, new SessionWindow(SEGMENT_INTERVAL * 2, SEGMENT_INTERVAL * 2));\n+        cachingStore.put(a1, \"1\".getBytes());\n+        cachingStore.put(a2, \"2\".getBytes());\n+        cachingStore.flush();\n+        cachingStore.put(a3, \"3\".getBytes());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY2NTUwNg==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r500665506", "bodyText": "Technically \"InvalidRangeException\" was just the name of the exception that could get thrown, there is no \"InvalidBackwardRangeException\" that I know of \ud83d\ude1b  But I think the meaning is clear enough lol", "author": "ableegoldman", "createdAt": "2020-10-07T00:14:08Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/CachingSessionStoreTest.java", "diffHunk": "@@ -456,68 +562,88 @@ public void shouldClearNamespaceCacheOnClose() {\n         assertEquals(0, cache.size());\n     }\n \n-    @Test(expected = InvalidStateStoreException.class)\n+    @Test\n     public void shouldThrowIfTryingToFetchFromClosedCachingStore() {\n         cachingStore.close();\n-        cachingStore.fetch(keyA);\n+        assertThrows(InvalidStateStoreException.class, () -> cachingStore.fetch(keyA));\n     }\n \n-    @Test(expected = InvalidStateStoreException.class)\n+    @Test\n     public void shouldThrowIfTryingToFindMergeSessionFromClosedCachingStore() {\n         cachingStore.close();\n-        cachingStore.findSessions(keyA, 0, Long.MAX_VALUE);\n+        assertThrows(InvalidStateStoreException.class, () -> cachingStore.findSessions(keyA, 0, Long.MAX_VALUE));\n     }\n \n-    @Test(expected = InvalidStateStoreException.class)\n+    @Test\n     public void shouldThrowIfTryingToRemoveFromClosedCachingStore() {\n         cachingStore.close();\n-        cachingStore.remove(new Windowed<>(keyA, new SessionWindow(0, 0)));\n+        assertThrows(InvalidStateStoreException.class, () -> cachingStore.remove(new Windowed<>(keyA, new SessionWindow(0, 0))));\n     }\n \n-    @Test(expected = InvalidStateStoreException.class)\n+    @Test\n     public void shouldThrowIfTryingToPutIntoClosedCachingStore() {\n         cachingStore.close();\n-        cachingStore.put(new Windowed<>(keyA, new SessionWindow(0, 0)), \"1\".getBytes());\n+        assertThrows(InvalidStateStoreException.class, () -> cachingStore.put(new Windowed<>(keyA, new SessionWindow(0, 0)), \"1\".getBytes()));\n     }\n \n-    @Test(expected = NullPointerException.class)\n+    @Test\n     public void shouldThrowNullPointerExceptionOnFindSessionsNullKey() {\n-        cachingStore.findSessions(null, 1L, 2L);\n+        assertThrows(NullPointerException.class, () -> cachingStore.findSessions(null, 1L, 2L));\n     }\n \n-    @Test(expected = NullPointerException.class)\n+    @Test\n     public void shouldThrowNullPointerExceptionOnFindSessionsNullFromKey() {\n-        cachingStore.findSessions(null, keyA, 1L, 2L);\n+        assertThrows(NullPointerException.class, () -> cachingStore.findSessions(null, keyA, 1L, 2L));\n     }\n \n-    @Test(expected = NullPointerException.class)\n+    @Test\n     public void shouldThrowNullPointerExceptionOnFindSessionsNullToKey() {\n-        cachingStore.findSessions(keyA, null, 1L, 2L);\n+        assertThrows(NullPointerException.class, () -> cachingStore.findSessions(keyA, null, 1L, 2L));\n     }\n \n-    @Test(expected = NullPointerException.class)\n+    @Test\n     public void shouldThrowNullPointerExceptionOnFetchNullFromKey() {\n-        cachingStore.fetch(null, keyA);\n+        assertThrows(NullPointerException.class, () -> cachingStore.fetch(null, keyA));\n     }\n \n-    @Test(expected = NullPointerException.class)\n+    @Test\n     public void shouldThrowNullPointerExceptionOnFetchNullToKey() {\n-        cachingStore.fetch(keyA, null);\n+        assertThrows(NullPointerException.class, () -> cachingStore.fetch(keyA, null));\n     }\n \n-    @Test(expected = NullPointerException.class)\n+    @Test\n     public void shouldThrowNullPointerExceptionOnFetchNullKey() {\n-        cachingStore.fetch(null);\n+        assertThrows(NullPointerException.class, () -> cachingStore.fetch(null));\n     }\n \n-    @Test(expected = NullPointerException.class)\n+    @Test\n     public void shouldThrowNullPointerExceptionOnRemoveNullKey() {\n-        cachingStore.remove(null);\n+        assertThrows(NullPointerException.class, () -> cachingStore.remove(null));\n     }\n \n-    @Test(expected = NullPointerException.class)\n+    @Test\n     public void shouldThrowNullPointerExceptionOnPutNullKey() {\n-        cachingStore.put(null, \"1\".getBytes());\n+        assertThrows(NullPointerException.class, () -> cachingStore.put(null, \"1\".getBytes()));\n+    }\n+\n+    @Test\n+    public void shouldNotThrowInvalidBackwardRangeExceptionWithNegativeFromKey() {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY2NjgxMw==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r500666813", "bodyText": "I guess it probably doesn't matter since we presumably aren't using these backward methods of the ReadOnlySessionStoreStub, but it seems like it might result in some tricky NPEs to debug if ever someone does try to use it in a test. If you don't feel like implementing it I think it's fine to just throw UnsupportedOperationException and say that you'll have to implement this to use it.\nOr just copy the code from the forward direction and flip it \ud83e\udd37\u200d\u2640\ufe0f  Same goes for all the methods in here that return null", "author": "ableegoldman", "createdAt": "2020-10-07T00:18:41Z", "path": "streams/src/test/java/org/apache/kafka/test/ReadOnlySessionStoreStub.java", "diffHunk": "@@ -82,11 +118,15 @@ public boolean hasNext() {\n                 public KeyValue<Windowed<K>, V> next() {\n                     return it.next();\n                 }\n-\n             }\n         );\n     }\n \n+    @Override\n+    public KeyValueIterator<Windowed<K>, V> backwardFetch(K from, K to) {\n+        return null;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDg4OTI3Ng==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r500889276", "bodyText": "mixed choice here. hope is good enough", "author": "jeqo", "createdAt": "2020-10-07T10:00:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY2NjgxMw=="}], "type": "inlineReview"}, {"oid": "9d4327cbadb408c8134e5ac09592d41719f772e5", "url": "https://github.com/apache/kafka/commit/9d4327cbadb408c8134e5ac09592d41719f772e5", "message": "key/value reverse operation", "committedDate": "2020-10-07T08:21:08Z", "type": "commit"}, {"oid": "6761c71e41040883634c8b664cdac9b3250a778c", "url": "https://github.com/apache/kafka/commit/6761c71e41040883634c8b664cdac9b3250a778c", "message": "window backward operations", "committedDate": "2020-10-07T08:21:08Z", "type": "commit"}, {"oid": "000c7b79009d24f61a97e562d8cdf5fb2bbef637", "url": "https://github.com/apache/kafka/commit/000c7b79009d24f61a97e562d8cdf5fb2bbef637", "message": "session backward operations", "committedDate": "2020-10-07T08:21:09Z", "type": "commit"}, {"oid": "1811b577b027506d6249e1d71690bc82107ef2bf", "url": "https://github.com/apache/kafka/commit/1811b577b027506d6249e1d71690bc82107ef2bf", "message": "improve time range comments", "committedDate": "2020-10-07T08:21:09Z", "type": "commit"}, {"oid": "8c26756dac79c2e5f1574c8057aeef3e163c71ea", "url": "https://github.com/apache/kafka/commit/8c26756dac79c2e5f1574c8057aeef3e163c71ea", "message": "fix bytes range validator not needed", "committedDate": "2020-10-07T08:21:09Z", "type": "commit"}, {"oid": "e0d160bc3ebf65a02fd995ea59e191a8db90b171", "url": "https://github.com/apache/kafka/commit/e0d160bc3ebf65a02fd995ea59e191a8db90b171", "message": "fix cache iterator", "committedDate": "2020-10-07T08:21:09Z", "type": "commit"}, {"oid": "ae7e3edcc426a98d61572686c4f884085f49e80c", "url": "https://github.com/apache/kafka/commit/ae7e3edcc426a98d61572686c4f884085f49e80c", "message": "fix session store to support backward queries", "committedDate": "2020-10-07T08:21:10Z", "type": "commit"}, {"oid": "64a95c01658f1281e6e7b8986b9678dcbf456ffc", "url": "https://github.com/apache/kafka/commit/64a95c01658f1281e6e7b8986b9678dcbf456ffc", "message": "improve format based on feedback", "committedDate": "2020-10-07T08:21:10Z", "type": "commit"}, {"oid": "a875cf1237a75eed6ccf3e00321768623aa8af11", "url": "https://github.com/apache/kafka/commit/a875cf1237a75eed6ccf3e00321768623aa8af11", "message": "test caching store across segments", "committedDate": "2020-10-07T08:21:10Z", "type": "commit"}, {"oid": "a875cf1237a75eed6ccf3e00321768623aa8af11", "url": "https://github.com/apache/kafka/commit/a875cf1237a75eed6ccf3e00321768623aa8af11", "message": "test caching store across segments", "committedDate": "2020-10-07T08:21:10Z", "type": "forcePushed"}, {"oid": "76bb9a27fd7a5e58bcfb74c8eca70c295f573d20", "url": "https://github.com/apache/kafka/commit/76bb9a27fd7a5e58bcfb74c8eca70c295f573d20", "message": "improve testing names and scenarios", "committedDate": "2020-10-07T09:40:10Z", "type": "commit"}, {"oid": "e49f258ad78bb75f23481f030d5e7be1db12ce00", "url": "https://github.com/apache/kafka/commit/e49f258ad78bb75f23481f030d5e7be1db12ce00", "message": "expand caching store tests per backing store", "committedDate": "2020-10-07T22:00:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTEwNzcyMg==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r501107722", "bodyText": "It won't matter to users whether this method was moved from another interface or not. They just need to know why they're getting the exception. I.e., we just need to tell them that the store implementation they selected didn't implement the method.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    throw new UnsupportedOperationException(\"Moved from SessionStore\");\n          \n          \n            \n                    throw new UnsupportedOperationException(\"This API is not supported by this implementation of ReadOnlySessionStore.\");\n          \n      \n    \n    \n  \n\nWe should say the exact same thing in all default implementations. Right now, they're a bit inconsistent.", "author": "vvcephei", "createdAt": "2020-10-07T15:30:42Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/ReadOnlySessionStore.java", "diffHunk": "@@ -24,35 +24,156 @@\n  * Implementations should be thread-safe as concurrent reads and writes\n  * are expected.\n  *\n- * @param <K> the key type\n+ * @param <K>   the key type\n  * @param <AGG> the aggregated value type\n  */\n public interface ReadOnlySessionStore<K, AGG> {\n+\n     /**\n-     * Retrieve all aggregated sessions for the provided key.\n+     * Fetch any sessions with the matching key and the sessions end is &ge; earliestSessionEndTime and the sessions\n+     * start is &le; latestSessionStartTime iterating from earliest to latest.\n+     * <p>\n      * This iterator must be closed after use.\n      *\n+     * @param key                    the key to return sessions for\n+     * @param earliestSessionEndTime the end timestamp of the earliest session to search for, where iteration starts.\n+     * @param latestSessionStartTime the end timestamp of the latest session to search for, where iteration ends.\n+     * @return iterator of sessions with the matching key and aggregated values, from earliest to latest session time.\n+     * @throws NullPointerException If null is used for key.\n+     */\n+    default KeyValueIterator<Windowed<K>, AGG> findSessions(final K key,\n+                                                            final long earliestSessionEndTime,\n+                                                            final long latestSessionStartTime) {\n+        throw new UnsupportedOperationException(\"Moved from SessionStore\");", "originalCommit": "76bb9a27fd7a5e58bcfb74c8eca70c295f573d20", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTEwOTY2Ng==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r501109666", "bodyText": "There are a lot of unnecessary whitespace changes in this PR. You don't need to back them all out right now, but in the future, please clean up the diff before submitting a PR. These extra changes make it harder to review.", "author": "vvcephei", "createdAt": "2020-10-07T15:33:29Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java", "diffHunk": "@@ -150,22 +149,44 @@ public void remove(final Windowed<Bytes> sessionKey) {\n         validateStoreOpen();\n \n         final PeekingKeyValueIterator<Bytes, LRUCacheEntry> cacheIterator = wrapped().persistent() ?\n-            new CacheIteratorWrapper(key, earliestSessionEndTime, latestSessionStartTime) :\n+            new CacheIteratorWrapper(key, earliestSessionEndTime, latestSessionStartTime, true) :\n             context.cache().range(cacheName,\n-                        cacheFunction.cacheKey(keySchema.lowerRangeFixedSize(key, earliestSessionEndTime)),\n-                        cacheFunction.cacheKey(keySchema.upperRangeFixedSize(key, latestSessionStartTime))\n+                cacheFunction.cacheKey(keySchema.lowerRangeFixedSize(key, earliestSessionEndTime)),\n+                cacheFunction.cacheKey(keySchema.upperRangeFixedSize(key, latestSessionStartTime))", "originalCommit": "76bb9a27fd7a5e58bcfb74c8eca70c295f573d20", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM1MTQ1Nw==", "url": "https://github.com/apache/kafka/pull/9139#discussion_r501351457", "bodyText": "The code style discourages inline conditionals. It's more maintainable to always use blocks.", "author": "vvcephei", "createdAt": "2020-10-07T22:43:26Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java", "diffHunk": "@@ -382,9 +479,23 @@ private boolean setInnerIterators() {\n                 currentKey = nextKeyEntry.getKey();\n \n                 if (latestSessionStartTime == Long.MAX_VALUE) {\n-                    recordIterator = nextKeyEntry.getValue().entrySet().iterator();\n+                    final Set<Entry<Long, byte[]>> entries;\n+                    if (forward) entries = nextKeyEntry.getValue().descendingMap().entrySet();\n+                    else entries = nextKeyEntry.getValue().entrySet();", "originalCommit": "e49f258ad78bb75f23481f030d5e7be1db12ce00", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b8562422783d83ad5d334155d1bcc6c56cea47d2", "url": "https://github.com/apache/kafka/commit/b8562422783d83ad5d334155d1bcc6c56cea47d2", "message": "fix formatting", "committedDate": "2020-10-08T03:36:49Z", "type": "commit"}, {"oid": "19324130d62a482b001c0e85b8fe51a0160d10fd", "url": "https://github.com/apache/kafka/commit/19324130d62a482b001c0e85b8fe51a0160d10fd", "message": "fix formatting", "committedDate": "2020-10-08T03:41:58Z", "type": "commit"}, {"oid": "70484e4a1ffc29f5effc822d1aea54c24730b603", "url": "https://github.com/apache/kafka/commit/70484e4a1ffc29f5effc822d1aea54c24730b603", "message": "Merge branch 'trunk' into backward-sessionstore", "committedDate": "2020-10-08T04:12:56Z", "type": "commit"}, {"oid": "14fce521346b290407c971e42bd684f55b512785", "url": "https://github.com/apache/kafka/commit/14fce521346b290407c971e42bd684f55b512785", "message": "fix conflict with trunk", "committedDate": "2020-10-08T04:17:59Z", "type": "commit"}]}