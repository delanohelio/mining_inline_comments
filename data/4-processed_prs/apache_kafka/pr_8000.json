{"pr_number": 8000, "pr_title": "KAFKA-9417: New Integration Test for KIP-447", "pr_createdAt": "2020-01-22T21:55:14Z", "pr_url": "https://github.com/apache/kafka/pull/8000", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM5MzIyOA==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r370393228", "bodyText": "During the test, I realized one scenario which is pretty dangerous. If we have a hard failure and the txn offsets get sticky on the broker side, for next consumer it could take over one minute to back-off and wait for the clearance, thus timing out on the fetch. Shrinking to 10 seconds here is just a remediation, but we should pay attention to general usage as well.", "author": "abbccdda", "createdAt": "2020-01-23T22:40:17Z", "path": "tests/kafkatest/tests/core/transactions_test.py", "diffHunk": "@@ -47,6 +47,7 @@ def __init__(self, test_context):\n         self.num_output_partitions = 3\n         self.num_seed_messages = 100000\n         self.transaction_size = 750\n+        self.transaction_timeout = 10000", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyNjA5OQ==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r373626099", "bodyText": "This timeout was increased from 30 to 60 seconds for the sake of rebalance caused offset wait (if triggered twice, that would be roughly 20~30 seconds)", "author": "abbccdda", "createdAt": "2020-01-31T18:40:41Z", "path": "tests/kafkatest/tests/core/group_mode_transactions_test.py", "diffHunk": "@@ -0,0 +1,312 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from kafkatest.services.zookeeper import ZookeeperService\n+from kafkatest.services.kafka import KafkaService\n+from kafkatest.services.console_consumer import ConsoleConsumer\n+from kafkatest.services.verifiable_producer import VerifiableProducer\n+from kafkatest.services.transactional_message_copier import TransactionalMessageCopier\n+from kafkatest.utils import is_int\n+\n+from ducktape.tests.test import Test\n+from ducktape.mark import matrix\n+from ducktape.mark.resource import cluster\n+from ducktape.utils.util import wait_until\n+\n+\n+class GroupModeTransactionsTest(Test):\n+    \"\"\"This test essentially does the same effort as TransactionsTest by transactionally copying data from a source topic to\n+    a destination topic and killing the copy process as well as the broker randomly through the process.\n+    The major difference is that we choose to work as a collaborated group with same topic subscription\n+    instead of individual consumers.\n+\n+    In the end we verify that the final output\n+    topic contains exactly one committed copy of each message in the input\n+    topic\n+    \"\"\"\n+    def __init__(self, test_context):\n+        \"\"\":type test_context: ducktape.tests.test.TestContext\"\"\"\n+        super(GroupModeTransactionsTest, self).__init__(test_context=test_context)\n+\n+        self.input_topic = \"input-topic\"\n+        self.output_topic = \"output-topic\"\n+\n+        self.num_brokers = 3\n+\n+        # Test parameters\n+        self.num_input_partitions = 9\n+        self.num_output_partitions = 9\n+        self.num_copiers = 3\n+        self.num_seed_messages = 100000\n+        self.transaction_size = 750\n+        self.transaction_timeout = 10000\n+        self.consumer_group = \"grouped-transactions-test-consumer-group\"\n+\n+        self.zk = ZookeeperService(test_context, num_nodes=1)\n+        self.kafka = KafkaService(test_context,\n+                                  num_nodes=self.num_brokers,\n+                                  zk=self.zk)\n+\n+    def setUp(self):\n+        self.zk.start()\n+\n+    def seed_messages(self, topic, num_seed_messages):\n+        seed_timeout_sec = 10000\n+        seed_producer = VerifiableProducer(context=self.test_context,\n+                                           num_nodes=1,\n+                                           kafka=self.kafka,\n+                                           topic=topic,\n+                                           message_validator=is_int,\n+                                           max_messages=num_seed_messages,\n+                                           enable_idempotence=True,\n+                                           repeating_keys=self.num_input_partitions)\n+        seed_producer.start()\n+        wait_until(lambda: seed_producer.num_acked >= num_seed_messages,\n+                   timeout_sec=seed_timeout_sec,\n+                   err_msg=\"Producer failed to produce messages %d in  %ds.\" % \\\n+                           (self.num_seed_messages, seed_timeout_sec))\n+        return seed_producer.acked_by_partition\n+\n+    def get_messages_from_topic(self, topic, num_messages):\n+        consumer = self.start_consumer(topic, group_id=\"verifying_consumer\")\n+        return self.drain_consumer(consumer, num_messages)\n+\n+    def bounce_brokers(self, clean_shutdown):\n+        for node in self.kafka.nodes:\n+            if clean_shutdown:\n+                self.kafka.restart_node(node, clean_shutdown = True)\n+            else:\n+                self.kafka.stop_node(node, clean_shutdown = False)\n+                wait_until(lambda: len(self.kafka.pids(node)) == 0 and not self.kafka.is_registered(node),\n+                           timeout_sec=self.kafka.zk_session_timeout + 5,\n+                           err_msg=\"Failed to see timely deregistration of \\\n+                           hard-killed broker %s\" % str(node.account))\n+                self.kafka.start_node(node)\n+\n+    def create_and_start_message_copier(self, input_topic, output_topic, transactional_id):\n+        message_copier = TransactionalMessageCopier(\n+            context=self.test_context,\n+            num_nodes=1,\n+            kafka=self.kafka,\n+            transactional_id=transactional_id,\n+            consumer_group=self.consumer_group,\n+            input_topic=input_topic,\n+            input_partition=-1,\n+            output_topic=output_topic,\n+            max_messages=-1,\n+            transaction_size=self.transaction_size,\n+            transaction_timeout=self.transaction_timeout,\n+            group_mode=True\n+        )\n+        message_copier.start()\n+        wait_until(lambda: message_copier.alive(message_copier.nodes[0]),\n+                   timeout_sec=10,\n+                   err_msg=\"Message copier failed to start after 10 s\")\n+        return message_copier\n+\n+    def bounce_copiers(self, copiers, clean_shutdown, timeout_sec=60):", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyNzUzMg==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r373627532", "bodyText": "The changes in transactions_test are mainly for compatibility. Nothing should be behaving differently", "author": "abbccdda", "createdAt": "2020-01-31T18:43:51Z", "path": "tests/kafkatest/tests/core/transactions_test.py", "diffHunk": "@@ -218,8 +225,9 @@ def setup_topics(self):\n     @cluster(num_nodes=9)\n     @matrix(failure_mode=[\"hard_bounce\", \"clean_bounce\"],\n             bounce_target=[\"brokers\", \"clients\"],\n-            check_order=[True, False])\n-    def test_transactions(self, failure_mode, bounce_target, check_order):\n+            check_order=[True, False],\n+            use_group_metadata=[True, False])", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyOTA2OA==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r373629068", "bodyText": "Only minor style fixes in this class", "author": "abbccdda", "createdAt": "2020-01-31T18:47:13Z", "path": "tools/src/main/java/org/apache/kafka/tools/VerifiableConsumer.java", "diffHunk": "@@ -159,8 +159,9 @@ private boolean isFinished() {\n                     partitionRecords.size(), minOffset, maxOffset));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1Njk3OA==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374956978", "bodyText": "This line is a bit confusing: the delayed rebalance is true / false, what does this mean?", "author": "guozhangwang", "createdAt": "2020-02-04T22:22:48Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1033,7 +1034,9 @@ class GroupCoordinator(val brokerId: Int,\n     group.transitionTo(PreparingRebalance)\n \n     info(s\"Preparing to rebalance group ${group.groupId} in state ${group.currentState} with old generation \" +\n-      s\"${group.generationId} (${Topic.GROUP_METADATA_TOPIC_NAME}-${partitionFor(group.groupId)}) (reason: $reason)\")\n+      s\"${group.generationId} (${Topic.GROUP_METADATA_TOPIC_NAME}-${partitionFor(group.groupId)}) (reason: $reason), \" +\n+      s\"the remaining instances are ${group.allMembers}, and the delayed rebalance is $startEmpty \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NzIzOA==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374957238", "bodyText": "Is this part of the KIP to change the default value?", "author": "guozhangwang", "createdAt": "2020-02-04T22:23:25Z", "path": "core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala", "diffHunk": "@@ -47,7 +47,7 @@ object TransactionStateManager {\n   // default transaction management config values\n   val DefaultTransactionsMaxTimeoutMs: Int = TimeUnit.MINUTES.toMillis(15).toInt\n   val DefaultTransactionalIdExpirationMs: Int = TimeUnit.DAYS.toMillis(7).toInt\n-  val DefaultAbortTimedOutTransactionsIntervalMs: Int = TimeUnit.MINUTES.toMillis(1).toInt\n+  val DefaultAbortTimedOutTransactionsIntervalMs: Int = TimeUnit.SECONDS.toMillis(10).toInt", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ5MjUxNA==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r375492514", "bodyText": "Do we still want this change, even if Kafka Streams changes slip? Maybe ok, because people might write plain consumer/producer apps using the new fencing. Just double checking.", "author": "mjsax", "createdAt": "2020-02-05T20:33:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NzIzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg3NjQ2NA==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377876464", "bodyText": "The KIP and mailing list just get updated with this minor change FYI", "author": "abbccdda", "createdAt": "2020-02-11T20:21:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NzIzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1ODE0MQ==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374958141", "bodyText": "Where is this value used?", "author": "guozhangwang", "createdAt": "2020-02-04T22:25:39Z", "path": "tests/kafkatest/services/console_consumer.py", "diffHunk": "@@ -105,9 +106,11 @@ def __init__(self, context, num_nodes, kafka, topic, group_id=\"test-consumer-gro\n         self.from_beginning = from_beginning\n         self.message_validator = message_validator\n         self.messages_consumed = {idx: [] for idx in range(1, num_nodes + 1)}\n+        self.messages_consumed_by_partition = {idx: [] for idx in range(1, num_nodes + 1)}", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg3ODM4Mg==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377878382", "bodyText": "Should be removed", "author": "abbccdda", "createdAt": "2020-02-11T20:25:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1ODE0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1OTAwOQ==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374959009", "bodyText": "nit: some = are spaced before / after and some are not.", "author": "guozhangwang", "createdAt": "2020-02-04T22:27:47Z", "path": "tests/kafkatest/services/transactional_message_copier.py", "diffHunk": "@@ -47,12 +47,13 @@ class TransactionalMessageCopier(KafkaPathResolverMixin, BackgroundThreadService\n \n     def __init__(self, context, num_nodes, kafka, transactional_id, consumer_group,\n                  input_topic, input_partition, output_topic, max_messages = -1,\n-                 transaction_size = 1000, enable_random_aborts=True):\n+                 transaction_size = 1000, transaction_timeout = None, enable_random_aborts=True, use_group_metadata=False, group_mode=False):", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg3OTExOA==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377879118", "bodyText": "Nice catch!", "author": "abbccdda", "createdAt": "2020-02-11T20:27:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1OTAwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2MTc4OQ==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374961789", "bodyText": "Why these values need to be atomic?", "author": "guozhangwang", "createdAt": "2020-02-04T22:33:59Z", "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -226,80 +259,133 @@ private static String toJsonString(Map<String, Object> data) {\n         return json;\n     }\n \n-    private static String statusAsJson(long consumed, long remaining, String transactionalId) {\n+    private static String statusAsJson(long totalProcessed, long consumed, long remaining, String transactionalId, String stage) {\n         Map<String, Object> statusData = new HashMap<>();\n         statusData.put(\"progress\", transactionalId);\n+        statusData.put(\"totalProcessed\", totalProcessed);\n         statusData.put(\"consumed\", consumed);\n         statusData.put(\"remaining\", remaining);\n+        statusData.put(\"time\", FORMAT.format(new Date()));\n+        statusData.put(\"stage\", stage);\n         return toJsonString(statusData);\n     }\n \n-    private static String shutDownString(long consumed, long remaining, String transactionalId) {\n+    private static String shutDownString(long totalProcessed, long consumed, long remaining, String transactionalId) {\n         Map<String, Object> shutdownData = new HashMap<>();\n-        shutdownData.put(\"remaining\", remaining);\n-        shutdownData.put(\"consumed\", consumed);\n         shutdownData.put(\"shutdown_complete\", transactionalId);\n+        shutdownData.put(\"totalProcessed\", totalProcessed);\n+        shutdownData.put(\"consumed\", consumed);\n+        shutdownData.put(\"remaining\", remaining);\n+        shutdownData.put(\"time\", FORMAT.format(new Date()));\n         return toJsonString(shutdownData);\n     }\n \n-    public static void main(String[] args) throws IOException {\n+    public static void main(String[] args) {\n         Namespace parsedArgs = argParser().parseArgsOrFail(args);\n         Integer numMessagesPerTransaction = parsedArgs.getInt(\"messagesPerTransaction\");\n         final String transactionalId = parsedArgs.getString(\"transactionalId\");\n         final String outputTopic = parsedArgs.getString(\"outputTopic\");\n \n         String consumerGroup = parsedArgs.getString(\"consumerGroup\");\n-        TopicPartition inputPartition = new TopicPartition(parsedArgs.getString(\"inputTopic\"), parsedArgs.getInt(\"inputPartition\"));\n \n         final KafkaProducer<String, String> producer = createProducer(parsedArgs);\n         final KafkaConsumer<String, String> consumer = createConsumer(parsedArgs);\n \n-        consumer.assign(singleton(inputPartition));\n+        final AtomicLong messageCap = new AtomicLong(\n+            parsedArgs.getInt(\"maxMessages\") == -1 ? Long.MAX_VALUE : parsedArgs.getInt(\"maxMessages\"));\n+\n+        boolean groupMode = parsedArgs.getBoolean(\"groupMode\");\n+        String topicName = parsedArgs.getString(\"inputTopic\");\n+        final AtomicLong remainingMessages = new AtomicLong(messageCap.get());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2MjE2Ng==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374962166", "bodyText": "I had a similar question on the other PR: are we suggesting for manual assignment to use the original overload (hence we would never deprecate it)?", "author": "guozhangwang", "createdAt": "2020-02-04T22:34:52Z", "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -226,80 +259,133 @@ private static String toJsonString(Map<String, Object> data) {\n         return json;\n     }\n \n-    private static String statusAsJson(long consumed, long remaining, String transactionalId) {\n+    private static String statusAsJson(long totalProcessed, long consumed, long remaining, String transactionalId, String stage) {\n         Map<String, Object> statusData = new HashMap<>();\n         statusData.put(\"progress\", transactionalId);\n+        statusData.put(\"totalProcessed\", totalProcessed);\n         statusData.put(\"consumed\", consumed);\n         statusData.put(\"remaining\", remaining);\n+        statusData.put(\"time\", FORMAT.format(new Date()));\n+        statusData.put(\"stage\", stage);\n         return toJsonString(statusData);\n     }\n \n-    private static String shutDownString(long consumed, long remaining, String transactionalId) {\n+    private static String shutDownString(long totalProcessed, long consumed, long remaining, String transactionalId) {\n         Map<String, Object> shutdownData = new HashMap<>();\n-        shutdownData.put(\"remaining\", remaining);\n-        shutdownData.put(\"consumed\", consumed);\n         shutdownData.put(\"shutdown_complete\", transactionalId);\n+        shutdownData.put(\"totalProcessed\", totalProcessed);\n+        shutdownData.put(\"consumed\", consumed);\n+        shutdownData.put(\"remaining\", remaining);\n+        shutdownData.put(\"time\", FORMAT.format(new Date()));\n         return toJsonString(shutdownData);\n     }\n \n-    public static void main(String[] args) throws IOException {\n+    public static void main(String[] args) {\n         Namespace parsedArgs = argParser().parseArgsOrFail(args);\n         Integer numMessagesPerTransaction = parsedArgs.getInt(\"messagesPerTransaction\");\n         final String transactionalId = parsedArgs.getString(\"transactionalId\");\n         final String outputTopic = parsedArgs.getString(\"outputTopic\");\n \n         String consumerGroup = parsedArgs.getString(\"consumerGroup\");\n-        TopicPartition inputPartition = new TopicPartition(parsedArgs.getString(\"inputTopic\"), parsedArgs.getInt(\"inputPartition\"));\n \n         final KafkaProducer<String, String> producer = createProducer(parsedArgs);\n         final KafkaConsumer<String, String> consumer = createConsumer(parsedArgs);\n \n-        consumer.assign(singleton(inputPartition));\n+        final AtomicLong messageCap = new AtomicLong(\n+            parsedArgs.getInt(\"maxMessages\") == -1 ? Long.MAX_VALUE : parsedArgs.getInt(\"maxMessages\"));\n+\n+        boolean groupMode = parsedArgs.getBoolean(\"groupMode\");\n+        String topicName = parsedArgs.getString(\"inputTopic\");\n+        final AtomicLong remainingMessages = new AtomicLong(messageCap.get());\n+        final AtomicLong numMessagesProcessedSinceLastRebalance = new AtomicLong(0);\n+        final AtomicLong totalMessageProcessed = new AtomicLong(0);\n+        if (groupMode) {\n+            consumer.subscribe(Collections.singleton(topicName), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    messageCap.set(partitions.stream()\n+                        .mapToLong(partition -> messagesRemaining(consumer, partition)).sum());\n+                    log.info(\"Message cap set to {} on rebalance complete\", messageCap);\n+                    numMessagesProcessedSinceLastRebalance.set(0);\n+                    // We use message cap for remaining here as the remainingMessages are not set yet.\n+                    System.out.println(statusAsJson(totalMessageProcessed.get(),\n+                        numMessagesProcessedSinceLastRebalance.get(), messageCap.get(), transactionalId, \"RebalanceComplete\"));\n+                }\n+            });\n+        } else {\n+            TopicPartition inputPartition = new TopicPartition(topicName, parsedArgs.getInt(\"inputPartition\"));\n+            consumer.assign(singleton(inputPartition));\n+            messageCap.set(Math.min(messagesRemaining(consumer, inputPartition), messageCap.get()));\n+            remainingMessages.set(messageCap.get());\n+        }\n \n-        long maxMessages = parsedArgs.getInt(\"maxMessages\") == -1 ? Long.MAX_VALUE : parsedArgs.getInt(\"maxMessages\");\n-        maxMessages = Math.min(messagesRemaining(consumer, inputPartition), maxMessages);\n         final boolean enableRandomAborts = parsedArgs.getBoolean(\"enableRandomAborts\");\n \n         producer.initTransactions();\n \n         final AtomicBoolean isShuttingDown = new AtomicBoolean(false);\n-        final AtomicLong remainingMessages = new AtomicLong(maxMessages);\n-        final AtomicLong numMessagesProcessed = new AtomicLong(0);\n+\n         Exit.addShutdownHook(\"transactional-message-copier-shutdown-hook\", () -> {\n             isShuttingDown.set(true);\n             // Flush any remaining messages\n             producer.close();\n             synchronized (consumer) {\n                 consumer.close();\n             }\n-            System.out.println(shutDownString(numMessagesProcessed.get(), remainingMessages.get(), transactionalId));\n+            System.out.println(shutDownString(totalMessageProcessed.get(),\n+                numMessagesProcessedSinceLastRebalance.get(), remainingMessages.get(), transactionalId));\n         });\n \n+        final boolean useGroupMetadata = parsedArgs.getBoolean(\"useGroupMetadata\");\n         try {\n             Random random = new Random();\n-            while (0 < remainingMessages.get()) {\n-                System.out.println(statusAsJson(numMessagesProcessed.get(), remainingMessages.get(), transactionalId));\n+            while (remainingMessages.get() > 0) {\n+                System.out.println(statusAsJson(totalMessageProcessed.get(),\n+                    numMessagesProcessedSinceLastRebalance.get(), remainingMessages.get(), transactionalId, \"ProcessLoop\"));\n                 if (isShuttingDown.get())\n                     break;\n-                int messagesInCurrentTransaction = 0;\n-                long numMessagesForNextTransaction = Math.min(numMessagesPerTransaction, remainingMessages.get());\n+                long messagesSentWithinCurrentTxn = 0L;\n+                long messagesNeededForCurrentTxn = remainingMessages.get();\n \n                 try {\n                     producer.beginTransaction();\n-                    while (messagesInCurrentTransaction < numMessagesForNextTransaction) {\n+                    Map<Integer, Integer> partitionCount = new HashMap<>();\n+                    while (messagesSentWithinCurrentTxn < messagesNeededForCurrentTxn) {\n                         ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(200));\n+                        log.info(\"number of consumer records fetched: {}\", records.count());\n+                        if (messageCap.get() <= 0) {\n+                            // We could see no more message needed for processing after poll\n+                            break;\n+                        }\n                         for (ConsumerRecord<String, String> record : records) {\n                             producer.send(producerRecordFromConsumerRecord(outputTopic, record));\n-                            messagesInCurrentTransaction++;\n+                            messagesSentWithinCurrentTxn++;\n+                            partitionCount.put(record.partition(), partitionCount.getOrDefault(record.partition(), 0) + 1);\n                         }\n+                        messagesNeededForCurrentTxn = Math.min(numMessagesPerTransaction, remainingMessages.get());\n+                    }\n+\n+                    log.info(\"Messages sent in current transaction: {}\", messagesSentWithinCurrentTxn);\n+                    for (Map.Entry<Integer, Integer> entry : partitionCount.entrySet()) {\n+                        log.info(\"Partition {} has contributed {} records\", entry.getKey(), entry.getValue());\n+                    }\n+\n+                    if (useGroupMetadata) {\n+                        producer.sendOffsetsToTransaction(consumerPositions(consumer), consumer.groupMetadata());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ5OTQzNQ==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r375499435", "bodyText": "I think the main reason why we would not deprecate the existing overload is a \"producer only\" application for which there is not even a consumer.", "author": "mjsax", "createdAt": "2020-02-05T20:48:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2MjE2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2MzY5NQ==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374963695", "bodyText": "nit: consumedSinceLastRebalanced.", "author": "guozhangwang", "createdAt": "2020-02-04T22:38:19Z", "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -226,80 +259,133 @@ private static String toJsonString(Map<String, Object> data) {\n         return json;\n     }\n \n-    private static String statusAsJson(long consumed, long remaining, String transactionalId) {\n+    private static String statusAsJson(long totalProcessed, long consumed, long remaining, String transactionalId, String stage) {\n         Map<String, Object> statusData = new HashMap<>();\n         statusData.put(\"progress\", transactionalId);\n+        statusData.put(\"totalProcessed\", totalProcessed);\n         statusData.put(\"consumed\", consumed);\n         statusData.put(\"remaining\", remaining);\n+        statusData.put(\"time\", FORMAT.format(new Date()));\n+        statusData.put(\"stage\", stage);\n         return toJsonString(statusData);\n     }\n \n-    private static String shutDownString(long consumed, long remaining, String transactionalId) {\n+    private static String shutDownString(long totalProcessed, long consumed, long remaining, String transactionalId) {\n         Map<String, Object> shutdownData = new HashMap<>();\n-        shutdownData.put(\"remaining\", remaining);\n-        shutdownData.put(\"consumed\", consumed);\n         shutdownData.put(\"shutdown_complete\", transactionalId);\n+        shutdownData.put(\"totalProcessed\", totalProcessed);\n+        shutdownData.put(\"consumed\", consumed);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ5MTk3NQ==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r375491975", "bodyText": "Is INFO level appropriate? (If yes, the message read a little too code centric IMHO)", "author": "mjsax", "createdAt": "2020-02-05T20:31:52Z", "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala", "diffHunk": "@@ -362,7 +362,10 @@ private[group] class GroupMetadata(val groupId: String, initialState: GroupState\n \n   def notYetRejoinedMembers = members.values.filter(!_.isAwaitingJoin).toList\n \n-  def hasAllMembersJoined = members.size == numMembersAwaitingJoin && pendingMembers.isEmpty\n+  def hasAllMembersJoined = {\n+    info(s\"The join complete condition checking: members => $members, members waiting join count: $numMembersAwaitingJoin, pending members: $pendingMembers\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ5ODcxMw==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r375498713", "bodyText": "If we are in groupMode and enable userGroupMetadata should we use a different transaction.id for each producer instead of the same?", "author": "mjsax", "createdAt": "2020-02-05T20:47:02Z", "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -137,16 +156,28 @@ private static ArgumentParser argParser() {\n                 .dest(\"enableRandomAborts\")\n                 .help(\"Whether or not to enable random transaction aborts (for system testing)\");\n \n+        parser.addArgument(\"--group-mode\")\n+                .action(storeTrue())\n+                .type(Boolean.class)\n+                .metavar(\"GROUP-MODE\")\n+                .dest(\"groupMode\")\n+                .help(\"Whether to let consumer subscribe to the input topic or do manual assign. If we do\" +\n+                          \" subscription based consumption, the input partition shall be ignored\");\n+\n+        parser.addArgument(\"--use-group-metadata\")\n+                .action(storeTrue())\n+                .type(Boolean.class)\n+                .metavar(\"USE-GROUP-METADATA\")\n+                .dest(\"useGroupMetadata\")\n+                .help(\"Whether to use the new transactional commit API with group metadata\");\n+\n         return parser;\n     }\n \n     private static KafkaProducer<String, String> createProducer(Namespace parsedArgs) {\n-        String transactionalId = parsedArgs.getString(\"transactionalId\");\n-        String brokerList = parsedArgs.getString(\"brokerList\");\n-\n         Properties props = new Properties();\n-        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);\n-        props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId);\n+        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, parsedArgs.getString(\"brokerList\"));\n+        props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, parsedArgs.getString(\"transactionalId\"));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg5OTc4OA==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377899788", "bodyText": "Should be guaranteed by the python test suite level", "author": "abbccdda", "createdAt": "2020-02-11T21:09:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ5ODcxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTUwMDI5Mg==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r375500292", "bodyText": "Do we really need to a a logger instead of just using System.out.println()? And if yes, should we update all outputs to use the logger? It's a little unclear why we would mix both approaches?", "author": "mjsax", "createdAt": "2020-02-05T20:50:29Z", "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -54,6 +61,9 @@\n  */\n public class TransactionalMessageCopier {\n \n+    private static final Logger log = LoggerFactory.getLogger(TransactionalMessageCopier.class);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAwNjE1MQ==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r378006151", "bodyText": "Correct, this gets removed.", "author": "abbccdda", "createdAt": "2020-02-12T02:11:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTUwMDI5Mg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "7f6d8ab335b286eef84d11afbac80fd72e5df543", "url": "https://github.com/apache/kafka/commit/7f6d8ab335b286eef84d11afbac80fd72e5df543", "message": "new txn test", "committedDate": "2020-02-11T20:37:34Z", "type": "commit"}, {"oid": "40ed3bd5aa91ad358cccfc76e3ddc922a123c7ce", "url": "https://github.com/apache/kafka/commit/40ed3bd5aa91ad358cccfc76e3ddc922a123c7ce", "message": "tooling class support", "committedDate": "2020-02-11T20:37:34Z", "type": "commit"}, {"oid": "c88e6d0c7f7ffca4db743b533c90126557be321d", "url": "https://github.com/apache/kafka/commit/c88e6d0c7f7ffca4db743b533c90126557be321d", "message": "tune txn timeout for transactions_test", "committedDate": "2020-02-11T20:37:35Z", "type": "commit"}, {"oid": "b2395c9d443c3103f95cfebf1a6d7daed6288237", "url": "https://github.com/apache/kafka/commit/b2395c9d443c3103f95cfebf1a6d7daed6288237", "message": "more fixes on partition output", "committedDate": "2020-02-11T20:37:35Z", "type": "commit"}, {"oid": "466e66805c17a8e85b8cf3575534fd15a7caff32", "url": "https://github.com/apache/kafka/commit/466e66805c17a8e85b8cf3575534fd15a7caff32", "message": "debug log for transaction_test", "committedDate": "2020-02-11T20:37:35Z", "type": "commit"}, {"oid": "2ccb59d6c4daffa13a31009158e9277b392c9699", "url": "https://github.com/apache/kafka/commit/2ccb59d6c4daffa13a31009158e9277b392c9699", "message": "group mode remaining count fix", "committedDate": "2020-02-11T20:37:35Z", "type": "commit"}, {"oid": "18bd2dd00a442804d639549bad0be51dcce7e4e8", "url": "https://github.com/apache/kafka/commit/18bd2dd00a442804d639549bad0be51dcce7e4e8", "message": "adjust result", "committedDate": "2020-02-11T20:37:35Z", "type": "commit"}, {"oid": "6b78e0262a76f81a8cc8bb951e9929db756ca041", "url": "https://github.com/apache/kafka/commit/6b78e0262a76f81a8cc8bb951e9929db756ca041", "message": "debug log", "committedDate": "2020-02-11T20:37:35Z", "type": "commit"}, {"oid": "5aca9b632674c3f67d2cdbd0a8546c4fdf6e4c97", "url": "https://github.com/apache/kafka/commit/5aca9b632674c3f67d2cdbd0a8546c4fdf6e4c97", "message": "less records", "committedDate": "2020-02-11T20:37:35Z", "type": "commit"}, {"oid": "1f824ff4d9365a74dc77eccb9cdee078da6a4167", "url": "https://github.com/apache/kafka/commit/1f824ff4d9365a74dc77eccb9cdee078da6a4167", "message": "revert to only single test", "committedDate": "2020-02-11T20:37:35Z", "type": "commit"}, {"oid": "1e15f3037647e4eb531337a687d38d001aaebe2c", "url": "https://github.com/apache/kafka/commit/1e15f3037647e4eb531337a687d38d001aaebe2c", "message": "debug txn timeout and shorten abort scheduler interval from 1 min to 10 seconds", "committedDate": "2020-02-11T20:37:35Z", "type": "commit"}, {"oid": "2e7d0212c76515de4d32e74746411d4f231ba72c", "url": "https://github.com/apache/kafka/commit/2e7d0212c76515de4d32e74746411d4f231ba72c", "message": "make message cap atomic", "committedDate": "2020-02-11T20:37:35Z", "type": "commit"}, {"oid": "b386012c497beac4de83513e792b273f38854b75", "url": "https://github.com/apache/kafka/commit/b386012c497beac4de83513e792b273f38854b75", "message": "debug group mode", "committedDate": "2020-02-11T20:37:35Z", "type": "commit"}, {"oid": "f7d5783f277a2e1a5d45f18128aef7d2cf3ac438", "url": "https://github.com/apache/kafka/commit/f7d5783f277a2e1a5d45f18128aef7d2cf3ac438", "message": "good for group test", "committedDate": "2020-02-11T20:37:36Z", "type": "commit"}, {"oid": "297c700cabee3fb7619678bc40c31329a58a1b55", "url": "https://github.com/apache/kafka/commit/297c700cabee3fb7619678bc40c31329a58a1b55", "message": "remove logging", "committedDate": "2020-02-11T20:37:36Z", "type": "commit"}, {"oid": "d244b6c60d14ed57d6561addf582879ebd38bcc0", "url": "https://github.com/apache/kafka/commit/d244b6c60d14ed57d6561addf582879ebd38bcc0", "message": "add total message tracking", "committedDate": "2020-02-11T20:37:36Z", "type": "commit"}, {"oid": "653bc2758da6c7df5e4b8ea83707dee9b94209db", "url": "https://github.com/apache/kafka/commit/653bc2758da6c7df5e4b8ea83707dee9b94209db", "message": "more rebalance related debug log", "committedDate": "2020-02-11T20:37:36Z", "type": "commit"}, {"oid": "40cbb2aa2d3eddc4ed2fb5a41ea018c139018710", "url": "https://github.com/apache/kafka/commit/40cbb2aa2d3eddc4ed2fb5a41ea018c139018710", "message": "add notes for local docker build failure", "committedDate": "2020-02-11T20:37:36Z", "type": "commit"}, {"oid": "1b78116ea9ca5bcfde65b9e48fd687936c9eb02c", "url": "https://github.com/apache/kafka/commit/1b78116ea9ca5bcfde65b9e48fd687936c9eb02c", "message": "fix remaining message count change after rebalance", "committedDate": "2020-02-11T20:37:36Z", "type": "commit"}, {"oid": "bf30abf7fcc43ceb8a9d91f81b73cf39d3816fab", "url": "https://github.com/apache/kafka/commit/bf30abf7fcc43ceb8a9d91f81b73cf39d3816fab", "message": "debug logging", "committedDate": "2020-02-11T20:37:36Z", "type": "commit"}, {"oid": "e0800139fadbb213cae31fa645574d1aa15d08fe", "url": "https://github.com/apache/kafka/commit/e0800139fadbb213cae31fa645574d1aa15d08fe", "message": "debug info for double counting and delayed join complete", "committedDate": "2020-02-11T20:37:36Z", "type": "commit"}, {"oid": "54f1227c8c7db4c41ffaf4ca7f28f577ab2545e5", "url": "https://github.com/apache/kafka/commit/54f1227c8c7db4c41ffaf4ca7f28f577ab2545e5", "message": "deprecate misleading message cap", "committedDate": "2020-02-11T20:37:36Z", "type": "commit"}, {"oid": "21b8d54ffc564412e89242879f9fe0727dffc0d6", "url": "https://github.com/apache/kafka/commit/21b8d54ffc564412e89242879f9fe0727dffc0d6", "message": "excessive long timeout for new member join timeout", "committedDate": "2020-02-11T20:37:36Z", "type": "commit"}, {"oid": "a6a07e1e5c8a59a51466887bca678cc1f6132af7", "url": "https://github.com/apache/kafka/commit/a6a07e1e5c8a59a51466887bca678cc1f6132af7", "message": "long expire hb", "committedDate": "2020-02-11T20:37:36Z", "type": "commit"}, {"oid": "34946fc7bd06c9270859094b9c4f10405a386f9b", "url": "https://github.com/apache/kafka/commit/34946fc7bd06c9270859094b9c4f10405a386f9b", "message": "add 0 check", "committedDate": "2020-02-11T20:37:37Z", "type": "commit"}, {"oid": "f5f862a8436474aac517e857a0de3db385a7525f", "url": "https://github.com/apache/kafka/commit/f5f862a8436474aac517e857a0de3db385a7525f", "message": "fix later", "committedDate": "2020-02-11T20:37:37Z", "type": "commit"}, {"oid": "8e651db36378181794f6ea1471641979ad3fd09d", "url": "https://github.com/apache/kafka/commit/8e651db36378181794f6ea1471641979ad3fd09d", "message": "remove record bound", "committedDate": "2020-02-11T20:37:37Z", "type": "commit"}, {"oid": "6902766174f8b6baf44e447f8243712b222c83da", "url": "https://github.com/apache/kafka/commit/6902766174f8b6baf44e447f8243712b222c83da", "message": "smaller max poll timeout", "committedDate": "2020-02-11T20:37:37Z", "type": "commit"}, {"oid": "4b7ffdf9a97368adcc64281f828449efceb17f0e", "url": "https://github.com/apache/kafka/commit/4b7ffdf9a97368adcc64281f828449efceb17f0e", "message": "expose group metadata", "committedDate": "2020-02-11T20:37:37Z", "type": "commit"}, {"oid": "abd921a275099cdbb729c593a4b2023192812248", "url": "https://github.com/apache/kafka/commit/abd921a275099cdbb729c593a4b2023192812248", "message": "add timeout to listOffset and let consumer redo the remaining count after rebalance", "committedDate": "2020-02-11T20:37:37Z", "type": "commit"}, {"oid": "918d57d1f81e271d41f4cae0d7c5f5896fad5bc6", "url": "https://github.com/apache/kafka/commit/918d57d1f81e271d41f4cae0d7c5f5896fad5bc6", "message": "Revert \"add timeout to listOffset and let consumer redo the remaining count after rebalance\"\n\nThis reverts commit bf94b0af68066aa762a169b08b4c09d74ef0ed0a.", "committedDate": "2020-02-11T20:37:37Z", "type": "commit"}, {"oid": "2a8ed5ad92bb64c840b98939d6ed8ea73cc32548", "url": "https://github.com/apache/kafka/commit/2a8ed5ad92bb64c840b98939d6ed8ea73cc32548", "message": "request update of metadata", "committedDate": "2020-02-11T20:37:37Z", "type": "commit"}, {"oid": "20be486525411a877947dc3c74e82e283bb7b14d", "url": "https://github.com/apache/kafka/commit/20be486525411a877947dc3c74e82e283bb7b14d", "message": "formal fix", "committedDate": "2020-02-11T20:37:37Z", "type": "commit"}, {"oid": "51e1809862f52de261ebf961b2b79aa8056b58d7", "url": "https://github.com/apache/kafka/commit/51e1809862f52de261ebf961b2b79aa8056b58d7", "message": "cleanup log messages", "committedDate": "2020-02-11T20:37:37Z", "type": "commit"}, {"oid": "17b1ebb275d0bef7744a0215fc2392c4c97285cc", "url": "https://github.com/apache/kafka/commit/17b1ebb275d0bef7744a0215fc2392c4c97285cc", "message": "add txn timeout to example", "committedDate": "2020-02-11T21:06:44Z", "type": "commit"}, {"oid": "17b1ebb275d0bef7744a0215fc2392c4c97285cc", "url": "https://github.com/apache/kafka/commit/17b1ebb275d0bef7744a0215fc2392c4c97285cc", "message": "add txn timeout to example", "committedDate": "2020-02-11T21:06:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg5ODk2MA==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377898960", "bodyText": "Add a reminder in EOS example to people", "author": "abbccdda", "createdAt": "2020-02-11T21:07:58Z", "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -76,8 +76,11 @@ public ExactlyOnceMessageProcessor(final String mode,\n         this.numInstances = numInstances;\n         this.instanceIdx = instanceIdx;\n         this.transactionalId = \"Processor-\" + instanceIdx;\n+        // If we are using the group mode, it is recommended to have a relatively short txn timeout\n+        // in order to clear pending offsets faster.\n+        final int transactionTimeoutMs = this.mode.equals(\"groupMode\") ? 10000 : -1;", "originalCommit": "17b1ebb275d0bef7744a0215fc2392c4c97285cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkwMDI1Mg==", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377900252", "bodyText": "Need to add synchronized as we are using static date FORMAT which shall triggers spotsbug for concurrent access.", "author": "abbccdda", "createdAt": "2020-02-11T21:10:37Z", "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -226,87 +256,124 @@ private static String toJsonString(Map<String, Object> data) {\n         return json;\n     }\n \n-    private static String statusAsJson(long consumed, long remaining, String transactionalId) {\n+    private static synchronized String statusAsJson(long totalProcessed, long consumedSinceLastRebalanced, long remaining, String transactionalId, String stage) {", "originalCommit": "17b1ebb275d0bef7744a0215fc2392c4c97285cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}