{"pr_number": 8282, "pr_title": "KAFKA-6145: add new assignment configs", "pr_createdAt": "2020-03-11T22:43:50Z", "pr_url": "https://github.com/apache/kafka/pull/8282", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r391313693", "bodyText": "I've been going back and forth on whether to add this as a \"per-cluster\" or \"per-instance\" config, @vvcephei  and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts?", "author": "ableegoldman", "createdAt": "2020-03-11T22:48:52Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -307,6 +307,25 @@\n     public static final String APPLICATION_SERVER_CONFIG = \"application.server\";\n     private static final String APPLICATION_SERVER_DOC = \"A host:port pair pointing to a user-defined endpoint that can be used for state store discovery and interactive queries on this KafkaStreams instance.\";\n \n+    /** {@code assignment.acceptable.recovery.lag} */\n+    public static final String ASSIGNMENT_ACCEPTABLE_RECOVERY_LAG_CONFIG = \"assignment.acceptable.recovery.lag\";\n+    private static final String ASSIGNMENT_ACCEPTABLE_RECOVERY_LAG_DOC = \"The maximum acceptable lag (number of offsets to catch up) for an active task to be assignable to a client. \" +\n+                                                                    \"Should correspond to a recovery time of well under a minute for a given workload. Must be at least 0.\";\n+\n+    /** {@code assignment.balance.factor} */\n+    public static final String ASSIGNMENT_BALANCE_FACTOR_CONFIG = \"assignment.balance.factor\";\n+    private static final String ASSIGNMENT_BALANCE_FACTOR_DOC = \"Target maximum difference in the number of active tasks assigned to the thread with the most tasks and the thread with the least in a steady-state assignment.\";\n+\n+    /** {@code assignment.max.extra.replicas} */\n+    public static final String ASSIGNMENT_MAX_EXTRA_REPLICAS_CONFIG = \"assignment.max.extra.replicas\";\n+    private static final String ASSIGNMENT_MAX_EXTRA_REPLICAS_DOC = \"The maximum number of extra replicas that can be assigned at once for the purpose of keeping the task available on one instance while it is warming up on \" +", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNDM4Mg==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r391314382", "bodyText": "Also pretty significantly modified the original name, I didn't feel \"max.task.migrations\" got to the point and was imagining a lot of users asking us about this. Not sure \"max.extra.replicas\" is the right way to go either, any other suggestions (or arguments in favor of \"max.task.migrations\")?", "author": "ableegoldman", "createdAt": "2020-03-11T22:50:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk3NTU5NA==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r392975594", "bodyText": "I've been going back and forth on whether to add this as a \"per-cluster\" or \"per-instance\" config, @vvcephei and I discussed briefly but @cadonna any opinion? If we do go with cluster-wide I think we should make the default pretty big, but I'm now leaning towards towards making it per-instance. Thoughts?\n\nper cluster:\n\n(+) We would have more freedom in the assignment algorithm, because we do not need to consider the value of this config for each client.\n(-) How should we react to distinct values on clients? How should we even know whether there are distinct values configured? We could just say Streams assumes the values of this config are the same on all clients and whatever is specified on the group leader is the truth.\n\nper client:\n\n(+) Each client host might differ in performance and users might want to adapt the value of this config for each client\n(-) Within the algorithm, how should we know the value of this config for a specific client? Don't we need to adapt the protocol to get all values at the group leader?\n\nSince assignment and restoration (a.k.a. warming-up) is done on stream thread level, it might be better to specify the value per stream thread. For example, 1 would mean that one extra replica per stream thread might be assigned. If a Streams client has three stream threads it can host up to three extra replica. The config would be global (a.k.a. cluster-wide) but users could account for different performances of the hosts by specifying different number of stream threads (which they probably have to do anyways). Here again, the value specified at the group leader is the truth.", "author": "cadonna", "createdAt": "2020-03-16T12:18:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk3OTU5Mw==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r392979593", "bodyText": "I agree that max.task.migration is not precise enough, because a migration of a task from one caught-up client to another caught-up client would not count against this config although it is a task migration. However, max.extra.replicas is also not ideal because extra can mean anything. What about max.warmup.replicas (or max.warm.up.replicas)?", "author": "cadonna", "createdAt": "2020-03-16T12:24:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3MzIyMw==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393373223", "bodyText": "whatever is specified on the group leader is the truth\nYes, and this applies to all the rebalance/assignment related configs.\n\nFor the per-instance option, I wasn't thinking we'd let each client send a different value (although we certainly could do this), just that we'd take whatever value the leader has and apply it to all instances. This way it doesn't complicate the algorithm, and we don't have to add it to the subscription: and my impression is that surely in practice users don't spin up instances with drastically different performance characteristics? But, if we go with the per-instance config and say only the leader's value will be chosen, and it turns out users would like to be able to set it differently on different instances, we can easily bump the protocol version in the next release and tell users they can now set it differently for different instances.\nI think a per-thread max may be a bit weird for users to reason about, but let's first decide if we do/don't want to make it cluster-wide before going on to instance vs. thread. One of the main arguments I still see for making it instance-wide, is that this is supposed to be a throttling mechanism and the most natural way to think of it in that light is probably on a cluster-wide basis, ie, \"don't let this app ever use more than 4 extra tasks of resources\".\nThis of course limits how many instances can be brought up at once while maintaining high availability; on the other hand, if you really are resource constrained (and presumably you wouldn't set this to a low value otherwise) you can't bring up a large number of instances at the same time anyway, without hitting your system/app limits", "author": "ableegoldman", "createdAt": "2020-03-16T23:52:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3NDk3NQ==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393374975", "bodyText": "Now I'm leaning towards making it cluster-wide, but setting it to a very high default and warning users that lowering it creates a tradeoff between high availability and time to scale out. Sorry to drag everyone into my indecisiveness...", "author": "ableegoldman", "createdAt": "2020-03-16T23:59:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzUxODAwNw==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393518007", "bodyText": "I took again a look at the KIP and I agree with you that the main motivation is throttling the usage of cluster resources not the instance resources. If this is still true then cluster-wide makes more sense to me.", "author": "cadonna", "createdAt": "2020-03-17T08:40:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkxNjEwNw==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393916107", "bodyText": "Sounds good -- do you think 2 is a reasonable default for it as a cluster-wide config? I think that's way too low, I'm actually inclined to set it to MAX_VALUE and maybe bump it to \"Medium\" priority so users consider whether to lower it. WDYT @cadonna @vvcephei ?", "author": "ableegoldman", "createdAt": "2020-03-17T19:23:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDIwMjc2MQ==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r394202761", "bodyText": "I would rather be conservative here, since too many warm-up replicas might cost more money in a cloud setting. Might that happen?\nBump the config to medium priority seems reasonable to me in any case.", "author": "cadonna", "createdAt": "2020-03-18T09:19:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDYwNTk5NA==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r394605994", "bodyText": "That's true...but a lower number will essentially make high availability an \"opt-in\" feature, which it's not (and should not be). Also, how do we choose a reasonable number? It doesn't make sense to optimize for a specific case without knowing what the specific restraints. If we set it to \"5\", then how many users will still need to change it because their cluster can actually tolerate 10, or only 1, or 50 (+ all the users who want high availability no matter what). If we set it to MAX_VLUE then all those same users will still have to set it, minus maybe a handful for whom we happened to guess right that \"5\" was the allowable number.\nIt seems better to not set the limit and document very clearly that users with resource constraints should set a limit, than guess wildly at what that limit should be while also restricting the high availability for all by default", "author": "ableegoldman", "createdAt": "2020-03-18T20:01:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDY5MDYxMg==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r394690612", "bodyText": "Thanks for the discussion, all.\nOne thing to note is that this config has \"no\" impact on high availability. Anyone who wants HA would configure num.standby.replicas, and this config applies on top of that one (for this reason, I like the name you picked. It is \"extra\".)\nI think you can make a case for per-node or per-cluster, but if we assume we only want to add one config, I think per-cluster is more valuable, since it lets you protect the brokers from overload, which a per-node config may not.\nRegarding whether we set the default limit low or high, I'd advocate for low. Some clusters are undoubtedly running close to the limits of their disk space or broker capacity, so suddenly letting every node double its traffic would result in serious operational consequences. On the other hand, if we start out low, then the probability of a crash becomes much lower, and the only problem is that the overall balancing process takes a long time. But, since the config is set low, there's a low impact on processing capacity while it's happening, so maybe there's no real impact over that long time.\nIf you buy the argument that a low default is good, then the obvious choice is \"1\", but that would take a really long time to complete balancing. The default of \"2\" is basically a compromise. It lets you balance twice as fast, and it's \"probably\" still low enough to cause a problem for no one. \"5\" also seems fine-ish, but the farther from \"1\" you move, the riskier the choice is.\nOne final thought, when you say that some people would \"need to change\" the config, say from the default (2) to MAX_VALUE... It seems like this population is restricted to the people who really need to make sure that balancing happens absolutely as fast as possible (maybe because their application can't keep up unless the whole cluster is processing, or because they're scaling up to cope with a spike in input data). Hopefully, these people know that they need extra horsepower and are motivated to read the docs and locate the config they need to change. Also, hopefullly, this isn't the common case.", "author": "vvcephei", "createdAt": "2020-03-18T23:10:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDY5NzI1Ng==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r394697256", "bodyText": "Yep, thanks for the clarification, I was misinterpreting how this config would be applied. FWIW I stand by the claim \"2 is 2 low\" :\nyou kind on touch on this in the 4th paragraph (\"Regarding...\"), but I'm just wondering what is likely to be the state of the cluster at the moment of scaling out. In other words, what is the bottleneck driving the need to bring up more instances? It can't be broker traffic, because bringing up another instance will not help with that. More likely it's something like disk, or consumer lag.\nIf the bottleneck is disk, throttling the number of warmup replicas will have absolutely zero impact on the immediate state as the original owner will remain the owner until the new instance is ready. Throttling will only serve to slow this down, and increase the likelihood we run out of disk before migrating enough tasks away*\nIf it's consumer lag, then again we just delay the ability to catch up as the over-constrained instance will have to continue processing active tasks for longer while waiting for a warmup replica to be created.\nSo, I agree now that this has no implications for high availability, but it does seem to have implications for \"smooth scaling out\", which is the literal title of this KIP \ud83d\ude09 . Of course, in the real world you may have two bottlenecks/constraints at the same time, so just because you're scaling out doesn't mean you don't also have to worry about broker traffic. But I would argue that that's something the user should customize if they expect it to be a constraint, and we shouldn't limit the cluster's ability to quickly relax other bottlenecks which, after all, is most likely the reason for scaling out in the first place.\nBut, since I have less (ie, no) experience on the operational side of things I'm happy to trust your intuition. Sorry to keep dragging this out, I'll shut up now", "author": "ableegoldman", "createdAt": "2020-03-18T23:30:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDcwNzkxMg==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r394707912", "bodyText": "Haha, no problem. I think your reasoning makes sense. It's certainly true that if the cluster is at a moment when it direly needs scale-out to happen as fast as possible, then the operator would likely need to un-throttle the task migration. As you alluded to in your comment, I have myself been in this exact situation.\nIt's tricky, though, because you can easily turn one incident into a much worse incident with this kind of emergency activity. For example, your app is not keeping up, so you double its size, and all the extra migration load now overwhelms the broker, causing a company-wide outage. Not saying this would happen, but it's the basic kind of thought that's behind my desire to err on the side of \"slow and steady\" with the default.\nAt least, if that did happen, it would be the operator's fault for de-throttling too much, not ours for selecting a dangerous default. I'm not just passing the buck with this statement. The operator is right there, in the moment. When the broker craps out, they know exactly what they've done and are in a good position to react immediately. If the fault were our bad default, then it might be much longer before anyone even reads the docs and learns that there is a throttling config, and that they can set it lower.\nNot sure if this really adds much to the conversation, since you already yielded the point, but it felt like a little more elaboration might be nice.", "author": "vvcephei", "createdAt": "2020-03-19T00:05:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDcyMzExMQ==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r394723111", "bodyText": "Fair enough. Just wanted to put it all out there -- good chat \ud83d\ude04", "author": "ableegoldman", "createdAt": "2020-03-19T00:35:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMzY5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNTMxNw==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r391315317", "bodyText": "Put this one as \"medium\" importance and the rest as low, because this seems like the most likely config that a typical user might need to tweak for their specific app/workload", "author": "ableegoldman", "createdAt": "2020-03-11T22:53:16Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -544,6 +563,11 @@\n \n             // MEDIUM\n \n+            .define(ASSIGNMENT_ACCEPTABLE_RECOVERY_LAG_CONFIG,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNTUwOQ==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r391315509", "bodyText": "@vvcephei suggested this be 10,000 which seemed high to me. But I'm willing to be convinced \ud83d\ude42", "author": "ableegoldman", "createdAt": "2020-03-11T22:53:47Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -544,6 +563,11 @@\n \n             // MEDIUM\n \n+            .define(ASSIGNMENT_ACCEPTABLE_RECOVERY_LAG_CONFIG,\n+                    Type.LONG,\n+                    1_000L,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAyNjk0Mw==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393026943", "bodyText": "Looking at the stateheavy benchmark that we run, 10000 does not seem too unreasonable. In the benchmarks 10 state stores are filled and the benchmark reports a throughput of around 7000 records per second. But I am also fine with 5000 as default. 1000 seems a bit low to me.", "author": "cadonna", "createdAt": "2020-03-16T13:36:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNTUwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3ODExOQ==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393378119", "bodyText": "Alright, let's just go with 10,000 then", "author": "ableegoldman", "createdAt": "2020-03-17T00:11:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNTUwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzOTg1MA==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r392939850", "bodyText": "prop:\nThe maximum acceptable lag (number of offsets to catch up) of a client to be considered caught-up for an active task.", "author": "cadonna", "createdAt": "2020-03-16T11:03:48Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -307,6 +307,25 @@\n     public static final String APPLICATION_SERVER_CONFIG = \"application.server\";\n     private static final String APPLICATION_SERVER_DOC = \"A host:port pair pointing to a user-defined endpoint that can be used for state store discovery and interactive queries on this KafkaStreams instance.\";\n \n+    /** {@code assignment.acceptable.recovery.lag} */\n+    public static final String ASSIGNMENT_ACCEPTABLE_RECOVERY_LAG_CONFIG = \"assignment.acceptable.recovery.lag\";\n+    private static final String ASSIGNMENT_ACCEPTABLE_RECOVERY_LAG_DOC = \"The maximum acceptable lag (number of offsets to catch up) for an active task to be assignable to a client. \" +\n+                                                                    \"Should correspond to a recovery time of well under a minute for a given workload. Must be at least 0.\";", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk4MzU3Mw==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r392983573", "bodyText": "prop: I find the assignment. prefix a bit clumsy and I think we do not really need it. Other configs do also not have its context prepended like for instance built.in.metrics.version.", "author": "cadonna", "createdAt": "2020-03-16T12:29:44Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -307,6 +307,25 @@\n     public static final String APPLICATION_SERVER_CONFIG = \"application.server\";\n     private static final String APPLICATION_SERVER_DOC = \"A host:port pair pointing to a user-defined endpoint that can be used for state store discovery and interactive queries on this KafkaStreams instance.\";\n \n+    /** {@code assignment.acceptable.recovery.lag} */", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM1MDc0Nw==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393350747", "bodyText": "Yeah...I agree it seems a bit awkward, the motivation was to keep all assignment/rebalance related configs together, both conceptually and physically -- other configs aren't really part of a \"single feature\" so it wouldn't make sense to give them a common prefix?.\nWe have so many configs, controlling so many unrelated things, but maybe I'm overthinking it. I suppose we could always add a \"assignment configs\" subsection to the docs...", "author": "ableegoldman", "createdAt": "2020-03-16T22:44:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk4MzU3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzUwODM4NA==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393508384", "bodyText": "Metrics configs have a common context but not a consistent prefix, but that might be for historical reasons. I just find the name of the config a bit long and as you said we could always cluster them in the docs. That was just a proposal and will not fight for it.", "author": "cadonna", "createdAt": "2020-03-17T08:21:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk4MzU3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwNjg3OQ==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393906879", "bodyText": "I'll remove it", "author": "ableegoldman", "createdAt": "2020-03-17T19:05:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk4MzU3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk4NDc3NA==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r392984774", "bodyText": "prop:\n\nRemove Target\nthread -> stream thread", "author": "cadonna", "createdAt": "2020-03-16T12:31:09Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -307,6 +307,25 @@\n     public static final String APPLICATION_SERVER_CONFIG = \"application.server\";\n     private static final String APPLICATION_SERVER_DOC = \"A host:port pair pointing to a user-defined endpoint that can be used for state store discovery and interactive queries on this KafkaStreams instance.\";\n \n+    /** {@code assignment.acceptable.recovery.lag} */\n+    public static final String ASSIGNMENT_ACCEPTABLE_RECOVERY_LAG_CONFIG = \"assignment.acceptable.recovery.lag\";\n+    private static final String ASSIGNMENT_ACCEPTABLE_RECOVERY_LAG_DOC = \"The maximum acceptable lag (number of offsets to catch up) for an active task to be assignable to a client. \" +\n+                                                                    \"Should correspond to a recovery time of well under a minute for a given workload. Must be at least 0.\";\n+\n+    /** {@code assignment.balance.factor} */\n+    public static final String ASSIGNMENT_BALANCE_FACTOR_CONFIG = \"assignment.balance.factor\";\n+    private static final String ASSIGNMENT_BALANCE_FACTOR_DOC = \"Target maximum difference in the number of active tasks assigned to the thread with the most tasks and the thread with the least in a steady-state assignment. Must be at least 1.\";", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk4Nzg1Ng==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r392987856", "bodyText": "req: We should agree on terminology. Here we talk about standbys and above about extra replica. This might be confusing.", "author": "cadonna", "createdAt": "2020-03-16T12:34:48Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -307,6 +307,25 @@\n     public static final String APPLICATION_SERVER_CONFIG = \"application.server\";\n     private static final String APPLICATION_SERVER_DOC = \"A host:port pair pointing to a user-defined endpoint that can be used for state store discovery and interactive queries on this KafkaStreams instance.\";\n \n+    /** {@code assignment.acceptable.recovery.lag} */\n+    public static final String ASSIGNMENT_ACCEPTABLE_RECOVERY_LAG_CONFIG = \"assignment.acceptable.recovery.lag\";\n+    private static final String ASSIGNMENT_ACCEPTABLE_RECOVERY_LAG_DOC = \"The maximum acceptable lag (number of offsets to catch up) for an active task to be assignable to a client. \" +\n+                                                                    \"Should correspond to a recovery time of well under a minute for a given workload. Must be at least 0.\";\n+\n+    /** {@code assignment.balance.factor} */\n+    public static final String ASSIGNMENT_BALANCE_FACTOR_CONFIG = \"assignment.balance.factor\";\n+    private static final String ASSIGNMENT_BALANCE_FACTOR_DOC = \"Target maximum difference in the number of active tasks assigned to the thread with the most tasks and the thread with the least in a steady-state assignment. Must be at least 1.\";\n+\n+    /** {@code assignment.max.extra.replicas} */\n+    public static final String ASSIGNMENT_MAX_EXTRA_REPLICAS_CONFIG = \"assignment.max.extra.replicas\";\n+    private static final String ASSIGNMENT_MAX_EXTRA_REPLICAS_DOC = \"The maximum number of extra replicas that can be assigned at once for the purpose of keeping the task available on one instance while it is warming up on \" +\n+                                                                         \"another instance it has been reassigned to. Used to throttle how much extra broker traffic and cluster state can be used for high availability. Must be at least 1.\";\n+\n+    /** {@code assignment.probing.rebalance.interval.ms} */\n+    public static final String ASSIGNMENT_PROBING_REBALANCE_INTERVAL_MS_CONFIG = \"assignment.probing.rebalance.interval.ms\";\n+    private static final String ASSIGNMENT_PROBING_REBALANCE_INTERVAL_MS_DOC = \"The maximum time to wait before triggering a rebalance to probe for standbys that have finished warming up and are ready to become active. Probing rebalances \" +\n+                                                                                \"will continue to be triggered until the assignment is balanced according to the \" + ASSIGNMENT_BALANCE_FACTOR_CONFIG;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAzNDU5OA==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393034598", "bodyText": "req: Please add unit tests for the definitions.", "author": "cadonna", "createdAt": "2020-03-16T13:48:52Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -620,6 +644,21 @@\n                     \"\",\n                     Importance.LOW,\n                     APPLICATION_SERVER_DOC)\n+            .define(ASSIGNMENT_BALANCE_FACTOR_CONFIG,\n+                    Type.INT,\n+                    1,\n+                    Importance.LOW,\n+                    ASSIGNMENT_BALANCE_FACTOR_DOC)\n+            .define(ASSIGNMENT_MAX_EXTRA_REPLICAS_CONFIG,\n+                    Type.INT,\n+                    5,\n+                    Importance.LOW,\n+                ASSIGNMENT_MAX_EXTRA_REPLICAS_DOC)\n+            .define(ASSIGNMENT_PROBING_REBALANCE_INTERVAL_MS_CONFIG,\n+                    Type.LONG,\n+                    10 * 60 * 1000L,\n+                    Importance.LOW,\n+                    ASSIGNMENT_PROBING_REBALANCE_INTERVAL_MS_DOC)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4MDIxMg==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393380212", "bodyText": "What do you mean by tests \"for the definitions\"?", "author": "ableegoldman", "createdAt": "2020-03-17T00:20:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAzNDU5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzUwOTYzMw==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393509633", "bodyText": "Have a look into StreamsConfigTest, e.g., shouldSetDefaultBuiltInMetricsVersionIfNoneIsSpecified().", "author": "cadonna", "createdAt": "2020-03-17T08:24:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAzNDU5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkxNDgxOA==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393914818", "bodyText": "I'm not sure I see the value in testing the default for every new config, isn't that just testing the code that generates/sets the defaults (ConfigDef#define)? We don't test eg Consumer#subscribe inside StreamThreadTest", "author": "ableegoldman", "createdAt": "2020-03-17T19:21:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAzNDU5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDE5NjUyMA==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r394196520", "bodyText": "The value is that if anybody changes the default value or the allowed values by mistake, maybe during a refactoring, there is a test that shows the mistake. Of course it is not a 100% protection, but at least it won't go totally unnoticed.", "author": "cadonna", "createdAt": "2020-03-18T09:08:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAzNDU5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDYwODQzNw==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r394608437", "bodyText": "Alright, I'll add the tests. We're missing tests for every other config then \ud83d\ude42", "author": "ableegoldman", "createdAt": "2020-03-18T20:06:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAzNDU5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAzOTU2Mg==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393039562", "bodyText": "req: Please add unit tests for this method (I am aware that there aren't any unit tests for this class).", "author": "cadonna", "createdAt": "2020-03-16T13:56:11Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -261,4 +257,24 @@ public InternalTopicManager getInternalTopicManager() {\n     public CopartitionedTopicsEnforcer getCopartitionedTopicsEnforcer() {\n         return copartitionedTopicsEnforcer;\n     }\n+\n+    public AssignmentConfigs getAssignmentConfigs() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4NTg1MQ==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393385851", "bodyText": "ack", "author": "ableegoldman", "createdAt": "2020-03-17T00:43:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAzOTU2Mg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkyNDM1Mw==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r393924353", "bodyText": "@cadonna Does RankedClient sound good, or would you prefer ClientAndRank or...?", "author": "ableegoldman", "createdAt": "2020-03-17T19:39:37Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -152,6 +153,36 @@ public String toString() {\n                 '}';\n         }\n     }\n+    \n+    public static class RankedClient<ID extends Comparable<? super ID>> implements Comparable<RankedClient<ID>> {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDE5NzE5MA==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r394197190", "bodyText": "I actually like RankedClient more. Thank you!", "author": "cadonna", "createdAt": "2020-03-18T09:09:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkyNDM1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDYxNjA4NA==", "url": "https://github.com/apache/kafka/pull/8282#discussion_r394616084", "bodyText": "req: Please add atLeast(1) validator here and the corresponding unit test. Similar applies also to the other configs.", "author": "cadonna", "createdAt": "2020-03-18T20:21:17Z", "path": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java", "diffHunk": "@@ -620,6 +650,11 @@\n                     \"\",\n                     Importance.LOW,\n                     APPLICATION_SERVER_DOC)\n+            .define(BALANCE_FACTOR_CONFIG,\n+                    Type.INT,\n+                    1,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "ddc5cfa0fe78cd363ffaf21cb97ac6a368967506", "url": "https://github.com/apache/kafka/commit/ddc5cfa0fe78cd363ffaf21cb97ac6a368967506", "message": "add configs; squashed for rebase", "committedDate": "2020-03-19T01:50:32Z", "type": "commit"}, {"oid": "ddc5cfa0fe78cd363ffaf21cb97ac6a368967506", "url": "https://github.com/apache/kafka/commit/ddc5cfa0fe78cd363ffaf21cb97ac6a368967506", "message": "add configs; squashed for rebase", "committedDate": "2020-03-19T01:50:32Z", "type": "forcePushed"}]}