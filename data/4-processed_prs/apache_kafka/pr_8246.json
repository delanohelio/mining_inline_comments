{"pr_number": 8246, "pr_title": "KAFKA-6145: Pt 2. Include offset sums in subscription", "pr_createdAt": "2020-03-06T23:55:44Z", "pr_url": "https://github.com/apache/kafka/pull/8246", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUwMTcwMA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390501700", "bodyText": "Let's get the lock first, and then check if the checkpoint file exists.", "author": "vvcephei", "createdAt": "2020-03-10T17:51:57Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU1NTQxMQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390555411", "bodyText": "We do \ud83d\ude42", "author": "ableegoldman", "createdAt": "2020-03-10T19:21:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUwMTcwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUwMjU3NQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390502575", "bodyText": "Let's add this before we try to read the checkpoint file. In case we do get an IOException, we shouldn't forget that we got the lock.", "author": "vvcephei", "createdAt": "2020-03-10T17:53:21Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU1NTc0Ng==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390555746", "bodyText": "Ack", "author": "ableegoldman", "createdAt": "2020-03-10T19:22:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUwMjU3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxNzcxNA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390517714", "bodyText": "what should we do if the offset is negative?", "author": "vvcephei", "createdAt": "2020-03-10T18:17:23Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on\n+                        }\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // we should just ignore any unknown files that sit in the same directory\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        return taskOffsetSums;\n+    }\n+\n+    private long computeOffsetSum(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0;\n+        for (final long offset : changelogOffsets.values()) {\n+            offsetSum += offset;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUyMDg1NA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390520854", "bodyText": "Oh, also, should we detect overflow and pin to MAX_VALUE in that case?", "author": "vvcephei", "createdAt": "2020-03-10T18:22:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxNzcxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU2MzE2NQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390563165", "bodyText": "Q: Would it be too strict to specify offset >= 0 as an invariant and throw an IllegalStateException?\nRegarding the overflow, when computing the lag, the sum of the last committed offsets will always be larger or equal to the sum of the offset of the states of a task except for the case where the sum of the last committed offsets has already overflown but the sum of the offset of the states has not. So, if both have overflown then the difference should not be affected by the overflows. If only the sum of the last committed offsets has overflown we need to compute the difference differently, but we are able to recognize this case. All of this assumes that overflows are well defined in Java as MIN_VALUE comes after MAX_VALUE.", "author": "cadonna", "createdAt": "2020-03-10T19:36:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxNzcxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYwNjI5Ng==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390606296", "bodyText": "Seems like the only negative offset we can get is -1, which indicates the offset is unknown in which case we should skip it. Will add a check for overflow too", "author": "ableegoldman", "createdAt": "2020-03-10T20:57:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxNzcxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYyODA5NQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390628095", "bodyText": "Hmm. Skipping would count the \"current position\" on that store as 0. Should we assume \"unknown offset\" equates to \"fully caught up\", or is it safer to set it to MAX_VALUE, or something else?", "author": "vvcephei", "createdAt": "2020-03-10T21:41:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxNzcxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY1NDQzMA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390654430", "bodyText": "-1 means that recordMetadata.offset == ProduceResponse.INVALID_OFFSET, which sounds to me like either the topic or producer isn't initialized yet or is corrupted in some way. Both of those make sense (to me) to interpret as an offset of 0. But, I'm no expert in the Producer client and various offset meanings.\nw.r.t the matter of overflow, I think we should aim to keep things simple and just pin to MAX_VALUE in the event of overflow, if we think that's likely to be a rare event. Obviously we should at the least make sure we don't crash or seriously harm the operation or results of the app -- pinning to MAX_VALUE will at worst make us potentially switch over to an active task before it's completely caught up.", "author": "ableegoldman", "createdAt": "2020-03-10T22:45:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxNzcxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxODM1NA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390518354", "bodyText": "Should we try to unlock the rest? Also, we should always include the causing exception.", "author": "vvcephei", "createdAt": "2020-03-10T18:18:29Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on\n+                        }\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // we should just ignore any unknown files that sit in the same directory\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        return taskOffsetSums;\n+    }\n+\n+    private long computeOffsetSum(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0;\n+        for (final long offset : changelogOffsets.values()) {\n+            offsetSum += offset;\n+        }\n+        return offsetSum;\n+    }\n+\n+    private void releaseTemporarilyLockedTaskDirectories() {\n+        for (final TaskId id : lockedUnassignedTaskDirectories) {\n+            try {\n+                stateDirectory.unlock(id);\n+            } catch (final IOException e) {\n+                log.error(\"Failed to release the state directory lock for task {}.\", id);\n+                throw new StreamsException(\"Unable to unlock task directory after rebalance.\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxODU0MA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390518540", "bodyText": "we should log the exception, too.", "author": "vvcephei", "createdAt": "2020-03-10T18:18:46Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on\n+                        }\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // we should just ignore any unknown files that sit in the same directory\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        return taskOffsetSums;\n+    }\n+\n+    private long computeOffsetSum(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0;\n+        for (final long offset : changelogOffsets.values()) {\n+            offsetSum += offset;\n+        }\n+        return offsetSum;\n+    }\n+\n+    private void releaseTemporarilyLockedTaskDirectories() {\n+        for (final TaskId id : lockedUnassignedTaskDirectories) {\n+            try {\n+                stateDirectory.unlock(id);\n+            } catch (final IOException e) {\n+                log.error(\"Failed to release the state directory lock for task {}.\", id);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUyMDYzNw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390520637", "bodyText": "This now looks a little suspicious... In the absence of a value, should we assume standbys are caught up (0L), or that they are not (MAX_VALUE)?", "author": "vvcephei", "createdAt": "2020-03-10T18:22:10Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -200,10 +201,19 @@ public UUID processId() {\n     public Map<TaskId, Long> taskOffsetSums() {\n         if (taskOffsetSumsCache == null) {\n             taskOffsetSumsCache = new HashMap<>();\n-            for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n-                for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n-                    taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n-                                            partitionOffsetSum.offsetSum());\n+            if (data.version() >= MIN_VERSION_OFFSET_SUM_SUBSCRIPTION) {\n+                for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n+                    for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n+                        taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n+                            partitionOffsetSum.offsetSum());\n+                    }\n+                }\n+            } else {\n+                for (final TaskId task : prevTasks()) {\n+                    taskOffsetSumsCache.put(task, Task.LATEST_OFFSET);\n+                }\n+                for (final TaskId task : standbyTasks()) {\n+                    taskOffsetSumsCache.put(task, 0L);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYxOTgxOQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390619819", "bodyText": "Hm, I guess if we do that then the assignment algorithm should hopefully reduce to the original one during an upgrade while we're pinned to the lower subscription version.\nThe assignment will be a little screwy on the first VP rebalance while there are mixed subscription versions, but maybe that's not worth worrying about. Probably not worth adding another sentinel value either..", "author": "ableegoldman", "createdAt": "2020-03-10T21:23:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUyMDYzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY1ODM0OA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390658348", "bodyText": "Actually I reverse, think we should add a (negative) OFFSET_SUM_UNKNOWN sentinel for this case. Either way we have to do some special handling, this way we can still enforce the endOffsetSum >= taskOffsetSum invariant and also don't share this pseudo-sentinel with the overflow case", "author": "ableegoldman", "createdAt": "2020-03-10T22:57:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUyMDYzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUyNjE1Mg==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390526152", "bodyText": "Can we move 01 to before 02?", "author": "vvcephei", "createdAt": "2020-03-10T18:31:13Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        final Map<TaskId, Set<TopicPartition>> activeTaskAssignment = new HashMap<>(taskId00Assignment);\n+        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n+            .andReturn(singletonList(task00)).once();\n+        final StateMachineTask task02 = new StateMachineTask(taskId02, taskId02Partitions, false);\n+        task02.setChangelogOffsets(task02ChangelogOffsets);\n+        expect(standbyTaskCreator.createTasks(eq(taskId02Assignment)))\n+            .andReturn(singletonList(task02)).once();\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU1NTIzMw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390555233", "bodyText": "Not taking a hard stance on this spelling, just aiming for consistency across the code base", "author": "ableegoldman", "createdAt": "2020-03-10T19:21:08Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -664,16 +664,16 @@ private void initializeTaskTime(final Map<TopicPartition, OffsetAndMetadata> off\n     }\n \n     @Override\n-    public Map<TopicPartition, Long> purgableOffsets() {\n-        final Map<TopicPartition, Long> purgableConsumedOffsets = new HashMap<>();\n+    public Map<TopicPartition, Long> purgeableOffsets() {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDIzOTA5NQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390239095", "bodyText": "Q: Wouldn't a log message on DEBUG-level make sense here?", "author": "cadonna", "createdAt": "2020-03-10T11:04:33Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY0NzgyNA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390647824", "bodyText": "Ack", "author": "ableegoldman", "createdAt": "2020-03-10T22:27:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDIzOTA5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY0ODI4OA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390648288", "bodyText": "Actually, what about warn?", "author": "ableegoldman", "createdAt": "2020-03-10T22:28:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDIzOTA5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI0NzUzMA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390247530", "bodyText": "prop:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    long offsetSum = 0;\n          \n          \n            \n                    for (final long offset : changelogOffsets.values()) {\n          \n          \n            \n                        offsetSum += offset;\n          \n          \n            \n                    }\n          \n          \n            \n                    return offsetSum;\n          \n          \n            \n                    return changelogOffsets.values().stream().reduce(0L, Long::sum);", "author": "cadonna", "createdAt": "2020-03-10T11:23:11Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on\n+                        }\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // we should just ignore any unknown files that sit in the same directory\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        return taskOffsetSums;\n+    }\n+\n+    private long computeOffsetSum(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0;\n+        for (final long offset : changelogOffsets.values()) {\n+            offsetSum += offset;\n+        }\n+        return offsetSum;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI0OTA4OQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390249089", "bodyText": "prop: rename to sumUpChangelogOffsets", "author": "cadonna", "createdAt": "2020-03-10T11:26:34Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on\n+                        }\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // we should just ignore any unknown files that sit in the same directory\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        return taskOffsetSums;\n+    }\n+\n+    private long computeOffsetSum(final Map<TopicPartition, Long> changelogOffsets) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY0NzUyMw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390647523", "bodyText": "How about sumOfChangelogOffsets?", "author": "ableegoldman", "createdAt": "2020-03-10T22:26:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI0OTA4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI1MzkwNw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390253907", "bodyText": "Q: This question is unrelated to your change. Why is firstException an atomic variable? It is a local variable and shutdown() is only called from StreamThread which should be single-threaded. \\cc @guozhangwang", "author": "cadonna", "createdAt": "2020-03-10T11:36:55Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -480,6 +503,12 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n+        try {\n+            releaseTemporarilyLockedTaskDirectories();\n+        } catch (final RuntimeException e) {\n+            firstException.compareAndSet(null, e);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYyOTc5Ng==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390629796", "bodyText": "I think it's just more convenient than the conditional block to check if it's null.", "author": "vvcephei", "createdAt": "2020-03-10T21:45:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI1MzkwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDc5NDA5OA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390794098", "bodyText": "Fair enough if the performance is similar.", "author": "cadonna", "createdAt": "2020-03-11T07:54:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI1MzkwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI3NzAxMg==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390277012", "bodyText": "prop:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            for (final TaskId task : prevTasks()) {\n          \n          \n            \n                                taskOffsetSumsCache.put(task, Task.LATEST_OFFSET);\n          \n          \n            \n                            }\n          \n          \n            \n                            for (final TaskId task : standbyTasks()) {\n          \n          \n            \n                                taskOffsetSumsCache.put(task, 0L);\n          \n          \n            \n                            }\n          \n          \n            \n                            prevTasks().forEach((taskId) -> taskOffsetSumsCache.put(taskId, Task.LATEST_OFFSET));\n          \n          \n            \n                            prevTasks().forEach((taskId) -> taskOffsetSumsCache.put(taskId, 0L));", "author": "cadonna", "createdAt": "2020-03-10T12:25:52Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -200,10 +201,19 @@ public UUID processId() {\n     public Map<TaskId, Long> taskOffsetSums() {\n         if (taskOffsetSumsCache == null) {\n             taskOffsetSumsCache = new HashMap<>();\n-            for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n-                for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n-                    taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n-                                            partitionOffsetSum.offsetSum());\n+            if (data.version() >= MIN_VERSION_OFFSET_SUM_SUBSCRIPTION) {\n+                for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n+                    for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n+                        taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n+                            partitionOffsetSum.offsetSum());\n+                    }\n+                }\n+            } else {\n+                for (final TaskId task : prevTasks()) {\n+                    taskOffsetSumsCache.put(task, Task.LATEST_OFFSET);\n+                }\n+                for (final TaskId task : standbyTasks()) {\n+                    taskOffsetSumsCache.put(task, 0L);\n                 }", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYzMDIxOQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390630219", "bodyText": "Not a big fan of this suggestion. Is there something wrong with loops?", "author": "vvcephei", "createdAt": "2020-03-10T21:46:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI3NzAxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY1NTUxNQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390655515", "bodyText": "I'll let you two fight this one out \ud83d\ude04", "author": "ableegoldman", "createdAt": "2020-03-10T22:48:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI3NzAxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDc5NzA2NQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390797065", "bodyText": "Nothing wrong with loops as forEach() is also a loop. If I can write a loop more concisely and I still easily get what they do, I would go for it. This is a proposal, so if @ableegoldman wants to follow it fine, if not I am also fine with it (after a bit of crying).", "author": "cadonna", "createdAt": "2020-03-11T08:01:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI3NzAxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI3Nzg3NQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390277875", "bodyText": "req:\nLOG.info(\n    \"Unable to decode subscription data: used version: {}; latest supported version: {}\",\n    version, \n    latestSupportedVersion\n);\n\nor\nLOG.info(\"Unable to decode subscription data: used version: {}; latest supported version: {}\",\n    version, \n    latestSupportedVersion\n);", "author": "cadonna", "createdAt": "2020-03-10T12:27:38Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -266,7 +276,7 @@ public static SubscriptionInfo decode(final ByteBuffer data) {\n             subscriptionInfoData.setVersion(version);\n             subscriptionInfoData.setLatestSupportedVersion(latestSupportedVersion);\n             LOG.info(\"Unable to decode subscription data: used version: {}; latest supported version: {}\",\n-                     version, latestSupportedVersion);\n+                version, latestSupportedVersion);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI3ODg1NA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390278854", "bodyText": "req:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n          \n          \n            \n                                        partitionOffsetSum.offsetSum());\n          \n          \n            \n                                    taskOffsetSumsCache.put(\n          \n          \n            \n                                        new TaskId(topicGroup.topicGroupId(), \n          \n          \n            \n                                        partitionOffsetSum.partition()),\n          \n          \n            \n                                        partitionOffsetSum.offsetSum()\n          \n          \n            \n                                    );", "author": "cadonna", "createdAt": "2020-03-10T12:29:48Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -200,10 +201,19 @@ public UUID processId() {\n     public Map<TaskId, Long> taskOffsetSums() {\n         if (taskOffsetSumsCache == null) {\n             taskOffsetSumsCache = new HashMap<>();\n-            for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n-                for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n-                    taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n-                                            partitionOffsetSum.offsetSum());\n+            if (data.version() >= MIN_VERSION_OFFSET_SUM_SUBSCRIPTION) {\n+                for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n+                    for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n+                        taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n+                            partitionOffsetSum.offsetSum());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI4NTExNg==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390285116", "bodyText": "prop:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        topicGroupIdToPartitionOffsetSum.putIfAbsent(task.topicGroupId, new ArrayList<>());\n          \n          \n            \n                        topicGroupIdToPartitionOffsetSum.get(task.topicGroupId).add(\n          \n          \n            \n                        topicGroupIdToPartitionOffsetSum.computeIfAbsent(task.topicGroupId, new ArrayList<>()).add(", "author": "cadonna", "createdAt": "2020-03-10T12:42:38Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -115,8 +116,8 @@ private void setTaskOffsetSumDataFromTaskOffsetSumMap(final Map<TaskId, Long> ta\n             topicGroupIdToPartitionOffsetSum.putIfAbsent(task.topicGroupId, new ArrayList<>());\n             topicGroupIdToPartitionOffsetSum.get(task.topicGroupId).add(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU2Nzg5Mw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390567893", "bodyText": "req: Could you add unit tests for this method?", "author": "cadonna", "createdAt": "2020-03-10T19:45:03Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -200,10 +201,19 @@ public UUID processId() {\n     public Map<TaskId, Long> taskOffsetSums() {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU3MzE0OQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390573149", "bodyText": "req: Could you compute those from the above maps? It might make maintenance easier.", "author": "cadonna", "createdAt": "2020-03-10T19:55:05Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY0MDcyNQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390640725", "bodyText": "I've gotten conflicting reviews on computing the expected value by code vs by hand. I think in this case it's more appropriate to compute it \"by hand\" rather than write a small method to compute it from the map, since computing offset sums from a changelog offset map is exactly what we're testing here (TaskManager#computeOffsetSum)", "author": "ableegoldman", "createdAt": "2020-03-10T22:10:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU3MzE0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU4MDczMw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390580733", "bodyText": "req: Are those verifications required? It seems to me they test the File object. Could you remove them?", "author": "cadonna", "createdAt": "2020-03-10T20:09:09Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        final Map<TaskId, Set<TopicPartition>> activeTaskAssignment = new HashMap<>(taskId00Assignment);\n+        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n+            .andReturn(singletonList(task00)).once();\n+        final StateMachineTask task02 = new StateMachineTask(taskId02, taskId02Partitions, false);\n+        task02.setChangelogOffsets(task02ChangelogOffsets);\n+        expect(standbyTaskCreator.createTasks(eq(taskId02Assignment)))\n+            .andReturn(singletonList(task02)).once();\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        task01.setChangelogOffsets(task01ChangelogOffsets);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId01Assignment)))\n+            .andReturn(singletonList(task01)).once();\n \n         expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n \n-        replay(activeTaskCreator, stateDirectory);\n+        expect(stateDirectory.lock(taskId10)).andReturn(true).once();\n+        expect(stateDirectory.lock(taskId12)).andReturn(false).once();\n+\n+\n+        final File task10CheckpointFile = new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        final File task12CheckpointFile = new File(taskFolders[5], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        assertThat(task10CheckpointFile.createNewFile(), is(true));\n+        assertThat(task12CheckpointFile.createNewFile(), is(true));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYzNTgzMw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390635833", "bodyText": "The point isn't to test anything about File, it's to create the actual file and verify that the creation was successful", "author": "ableegoldman", "createdAt": "2020-03-10T21:59:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU4MDczMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU5MTk5NQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390591995", "bodyText": "req: I assume you do not want to test handleAssignment() here, so you should not specify behaviour verification on the mock. You could simply write\nexpect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n    .andStubReturn(singletonList(task00));\n\n.andStubReturn() is behavior that is not verified in the verify() method. Using it were no behavior verification is needed makes the test more robust to changes in the productive code that should not affect this test.\nSame applies to other similar locations in this test.", "author": "cadonna", "createdAt": "2020-03-10T20:30:10Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        final Map<TaskId, Set<TopicPartition>> activeTaskAssignment = new HashMap<>(taskId00Assignment);\n+        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n+            .andReturn(singletonList(task00)).once();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYzODk3Mw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390638973", "bodyText": "Ah, thanks, that's a good suggestion. Will do", "author": "ableegoldman", "createdAt": "2020-03-10T22:07:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU5MTk5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU5ODIwOQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390598209", "bodyText": "req: Why do you verify the setup code here?", "author": "cadonna", "createdAt": "2020-03-10T20:42:25Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        final Map<TaskId, Set<TopicPartition>> activeTaskAssignment = new HashMap<>(taskId00Assignment);\n+        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n+            .andReturn(singletonList(task00)).once();\n+        final StateMachineTask task02 = new StateMachineTask(taskId02, taskId02Partitions, false);\n+        task02.setChangelogOffsets(task02ChangelogOffsets);\n+        expect(standbyTaskCreator.createTasks(eq(taskId02Assignment)))\n+            .andReturn(singletonList(task02)).once();\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        task01.setChangelogOffsets(task01ChangelogOffsets);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId01Assignment)))\n+            .andReturn(singletonList(task01)).once();\n \n         expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n \n-        replay(activeTaskCreator, stateDirectory);\n+        expect(stateDirectory.lock(taskId10)).andReturn(true).once();\n+        expect(stateDirectory.lock(taskId12)).andReturn(false).once();\n+\n+\n+        final File task10CheckpointFile = new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        final File task12CheckpointFile = new File(taskFolders[5], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        assertThat(task10CheckpointFile.createNewFile(), is(true));\n+        assertThat(task12CheckpointFile.createNewFile(), is(true));\n+        new OffsetCheckpoint(task10CheckpointFile).write(task10ChangelogOffsets);\n+\n+        replay(activeTaskCreator, standbyTaskCreator, consumer, changeLogReader, stateDirectory);\n+\n+        taskManager.handleAssignment(activeTaskAssignment, taskId02Assignment);\n+        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n+\n+        activeTaskAssignment.putAll(taskId01Assignment);\n+        taskManager.handleAssignment(activeTaskAssignment, taskId02Assignment);\n+\n+        assertThat(task00.state(), is(Task.State.RUNNING));\n+        assertThat(task01.state(), not(Task.State.RUNNING));\n+        assertThat(task02.state(), is(Task.State.RUNNING));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYzMjE4NA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390632184", "bodyText": "I bet she copied the idiom from all of my tests. I did it because it makes the tests easier to read... I.e., you can visually see what state everything is in. Otherwise you'd have to reason about what state it would be in, given all the mocks above.", "author": "vvcephei", "createdAt": "2020-03-10T21:50:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU5ODIwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYzNzA1NQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390637055", "bodyText": "To verify that we're actually testing what we think we're testing, ie if due to some bug in TaskManager 0_0 did not actually reach the RUNNING state, we should fail fast. Otherwise when the test fails, it's not clear that it's due to an unrelated bug rather than a bug in getTaskOffsetSums", "author": "ableegoldman", "createdAt": "2020-03-10T22:02:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU5ODIwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY1NTEyNQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390655125", "bodyText": "Also what @vvcephei said \ud83d\ude42 . It's kind of hard to reason about the states based only on calls to TaskManager in this test class", "author": "ableegoldman", "createdAt": "2020-03-10T22:47:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU5ODIwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgwNzUxMg==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390807512", "bodyText": "Fair enough given the complexity of the setup. I guess what disturbs me most is the fact that the setup is so complex.", "author": "cadonna", "createdAt": "2020-03-11T08:26:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU5ODIwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYwMDU4Mg==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390600582", "bodyText": "prop: Please use assertThat() since it makes verifications a bit better readable.", "author": "cadonna", "createdAt": "2020-03-10T20:46:48Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        final Map<TaskId, Set<TopicPartition>> activeTaskAssignment = new HashMap<>(taskId00Assignment);\n+        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n+            .andReturn(singletonList(task00)).once();\n+        final StateMachineTask task02 = new StateMachineTask(taskId02, taskId02Partitions, false);\n+        task02.setChangelogOffsets(task02ChangelogOffsets);\n+        expect(standbyTaskCreator.createTasks(eq(taskId02Assignment)))\n+            .andReturn(singletonList(task02)).once();\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        task01.setChangelogOffsets(task01ChangelogOffsets);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId01Assignment)))\n+            .andReturn(singletonList(task01)).once();\n \n         expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n \n-        replay(activeTaskCreator, stateDirectory);\n+        expect(stateDirectory.lock(taskId10)).andReturn(true).once();\n+        expect(stateDirectory.lock(taskId12)).andReturn(false).once();\n+\n+\n+        final File task10CheckpointFile = new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        final File task12CheckpointFile = new File(taskFolders[5], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        assertThat(task10CheckpointFile.createNewFile(), is(true));\n+        assertThat(task12CheckpointFile.createNewFile(), is(true));\n+        new OffsetCheckpoint(task10CheckpointFile).write(task10ChangelogOffsets);\n+\n+        replay(activeTaskCreator, standbyTaskCreator, consumer, changeLogReader, stateDirectory);\n+\n+        taskManager.handleAssignment(activeTaskAssignment, taskId02Assignment);\n+        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n+\n+        activeTaskAssignment.putAll(taskId01Assignment);\n+        taskManager.handleAssignment(activeTaskAssignment, taskId02Assignment);\n+\n+        assertThat(task00.state(), is(Task.State.RUNNING));\n+        assertThat(task01.state(), not(Task.State.RUNNING));\n+        assertThat(task02.state(), is(Task.State.RUNNING));\n \n         final Map<TaskId, Long> taskOffsetSums = taskManager.getTaskOffsetSums();\n \n         verify(activeTaskCreator, stateDirectory);\n-\n-        assertThat(taskOffsetSums.keySet(), equalTo(mkSet(taskId01, taskId02, new TaskId(1, 1))));\n+        assertThat(taskOffsetSums.keySet(), equalTo(mkSet(taskId00, taskId01, taskId02, taskId10)));\n+        assertEquals(task00OffsetSum, taskOffsetSums.get(taskId00));\n+        assertEquals(task01OffsetSum, taskOffsetSums.get(taskId01));\n+        assertEquals(task02OffsetSum, taskOffsetSums.get(taskId02));\n+        assertEquals(task10OffsetSum, taskOffsetSums.get(taskId10));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYzNzUyOQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390637529", "bodyText": "Ack", "author": "ableegoldman", "createdAt": "2020-03-10T22:03:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYwMDU4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYwMzQzNg==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390603436", "bodyText": "req: You do not need to verify the activeTaskCreator here, since you are not testing handleAssignment().", "author": "cadonna", "createdAt": "2020-03-10T20:52:06Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        final Map<TaskId, Set<TopicPartition>> activeTaskAssignment = new HashMap<>(taskId00Assignment);\n+        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n+            .andReturn(singletonList(task00)).once();\n+        final StateMachineTask task02 = new StateMachineTask(taskId02, taskId02Partitions, false);\n+        task02.setChangelogOffsets(task02ChangelogOffsets);\n+        expect(standbyTaskCreator.createTasks(eq(taskId02Assignment)))\n+            .andReturn(singletonList(task02)).once();\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        task01.setChangelogOffsets(task01ChangelogOffsets);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId01Assignment)))\n+            .andReturn(singletonList(task01)).once();\n \n         expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n \n-        replay(activeTaskCreator, stateDirectory);\n+        expect(stateDirectory.lock(taskId10)).andReturn(true).once();\n+        expect(stateDirectory.lock(taskId12)).andReturn(false).once();\n+\n+\n+        final File task10CheckpointFile = new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        final File task12CheckpointFile = new File(taskFolders[5], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        assertThat(task10CheckpointFile.createNewFile(), is(true));\n+        assertThat(task12CheckpointFile.createNewFile(), is(true));\n+        new OffsetCheckpoint(task10CheckpointFile).write(task10ChangelogOffsets);\n+\n+        replay(activeTaskCreator, standbyTaskCreator, consumer, changeLogReader, stateDirectory);\n+\n+        taskManager.handleAssignment(activeTaskAssignment, taskId02Assignment);\n+        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n+\n+        activeTaskAssignment.putAll(taskId01Assignment);\n+        taskManager.handleAssignment(activeTaskAssignment, taskId02Assignment);\n+\n+        assertThat(task00.state(), is(Task.State.RUNNING));\n+        assertThat(task01.state(), not(Task.State.RUNNING));\n+        assertThat(task02.state(), is(Task.State.RUNNING));\n \n         final Map<TaskId, Long> taskOffsetSums = taskManager.getTaskOffsetSums();\n \n         verify(activeTaskCreator, stateDirectory);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYwNjA1NA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390606054", "bodyText": "req: Could you use not the same topic partitions for the changelog topic partition as for the assigned topic partitions? It had a hard time to understand that those topic partitions are just there for convenience. At least give them new variable names with a more realistic naming. Maybe you could also vary the number of topic partitions in the maps from 1 to 3.", "author": "cadonna", "createdAt": "2020-03-10T20:56:59Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY0Mjg1OA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390642858", "bodyText": "Yeah, I was just reusing them to be lazy and because they don't matter. But better to avoid confusion with separate variables/names.\nAgreed on varying the partition count,  will also add some \"special cases\" like -1", "author": "ableegoldman", "createdAt": "2020-03-10T22:14:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYwNjA1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYxMzgxMA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390613810", "bodyText": "req: In general I think this unit test is really large. For the sake of readability and modularization, you should split it into multiple tests. Maybe for each case two unit tests: one with a single case. Then one unit test for the composite scenario with all cases and different occurrences of the different cases. If you extract and parametrize the setup, it should not be too much code duplication. Additionally, a test where stateDirectory.listTaskDirectories() returns an empty array and a test with a stateless task are missing.", "author": "cadonna", "createdAt": "2020-03-10T21:10:39Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY0NDM2MA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390644360", "bodyText": "To clarify, you're suggesting to add smaller tests for each case (and edge cases) but also leave the composite test in as well?", "author": "ableegoldman", "createdAt": "2020-03-10T22:18:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYxMzgxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgwMjI1NQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390802255", "bodyText": "Basically yes. I would extend the composite test with multiple occurrences of each case so that we also cover the scenario where we have -- for example -- n active running task, m active non-running tasks, k standby tasks etc.\nIf this makes the test too clumsy, you could cover each of n active running task, m active non-running tasks, k standby tasks etc in its own test and make one composite test with one occurrence of each case. Choose what is better readable.\nMy point is, that the current test does not cover multiple occurrences of the same case.", "author": "cadonna", "createdAt": "2020-03-11T08:14:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYxMzgxMA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzMjY0OQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390932649", "bodyText": "prop: Rename to shouldNotLockAnythingIfStateDirIsEmpty().", "author": "cadonna", "createdAt": "2020-03-11T12:22:25Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzMzgzOQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390933839", "bodyText": "req: Please add a similar unit test as the one above but where stateDirectory.listTaskDirectories() returns null instead of an empty array.", "author": "cadonna", "createdAt": "2020-03-11T12:24:45Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE2Nzk5Mw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391167993", "bodyText": "I think we should actually just make listTaskDirectories() always return an empty File[] instead of null in some cases and new File[0] in others, as we treat both cases the same. WDYT?", "author": "ableegoldman", "createdAt": "2020-03-11T18:11:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzMzgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTQ1NTIzNg==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391455236", "bodyText": "Yeah, I agree with you. I checked the code and it should be OK to always return an empty array. However, could you open a second PR for it that we merge before this one to keep this PR focussed.", "author": "cadonna", "createdAt": "2020-03-12T08:08:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzMzgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg5MTc1Ng==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391891756", "bodyText": "I'm not sure it's really worth doing as a separate PR, it's about 10 lines of code and is only really motivated by the work in this KIP?", "author": "ableegoldman", "createdAt": "2020-03-12T21:01:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzMzgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjI0ODQ1MQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r392248451", "bodyText": "Fair enough", "author": "cadonna", "createdAt": "2020-03-13T14:06:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzMzgzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzNjQxMQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390936411", "bodyText": "prop: Rename to tryToLockAllTaskDirectories()", "author": "cadonna", "createdAt": "2020-03-11T12:30:08Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,50 +378,134 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryLockForAllTaskDirectories()}\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+        for (final TaskId id : lockedTaskDirectories) {\n+            final Task task = tasks.get(id);\n+            if (task != null) {\n+                if (task.isActive() && task.state() == RUNNING) {\n+                    taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                } else {\n+                    taskOffsetSums.put(id, sumOfChangelogOffsets(task.changelogOffsets()));\n+                }\n             } else {\n-                taskOffsetSums.put(id, 0L);\n+                final File checkpointFile = stateDirectory.checkpointFileFor(id);\n+                try {\n+                    // If we can't read the checkpoint file or it doesn't exist, release the task directory\n+                    // so the background cleaner thread can do its thing\n+                    if (checkpointFile.exists()) {\n+                        taskOffsetSums.put(id, sumOfChangelogOffsets(new OffsetCheckpoint(checkpointFile).read()));\n+                    } else {\n+                        releaseTaskDirLock(id);\n+                    }\n+                } catch (final IOException e) {\n+                    log.warn(String.format(\"Exception caught while trying to read checkpoint for task %s:\", id), e);\n+                    releaseTaskDirLock(id);\n+                }\n             }\n         }\n+\n         return taskOffsetSums;\n     }\n \n     /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n+     * Makes a weak attempt to lock all task directories in the state dir. We are responsible for computing and\n+     * reporting the offset sum for any unassigned tasks we obtain the lock for in the upcoming rebalance. Tasks\n+     * that we locked but didn't own will be released at the end of the rebalance (unless of course we were\n+     * assigned the task as a result of the rebalance). This method should be idempotent.\n      */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n+    private void tryLockForAllTaskDirectories() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE2ODMxOQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391168319", "bodyText": "SG", "author": "ableegoldman", "createdAt": "2020-03-11T18:11:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzNjQxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODM1Mw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390948353", "bodyText": "req: If the release of the lock throws, we log an error message in releaseTaskDirLock(id) but swallow the exception here. This seems to me a false alarm. Imagine you analyse the log files and find an error that actually isn't one. I think, we should suppress the log message in this case.", "author": "cadonna", "createdAt": "2020-03-11T12:53:00Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,50 +378,134 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryLockForAllTaskDirectories()}\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+        for (final TaskId id : lockedTaskDirectories) {\n+            final Task task = tasks.get(id);\n+            if (task != null) {\n+                if (task.isActive() && task.state() == RUNNING) {\n+                    taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                } else {\n+                    taskOffsetSums.put(id, sumOfChangelogOffsets(task.changelogOffsets()));\n+                }\n             } else {\n-                taskOffsetSums.put(id, 0L);\n+                final File checkpointFile = stateDirectory.checkpointFileFor(id);\n+                try {\n+                    // If we can't read the checkpoint file or it doesn't exist, release the task directory\n+                    // so the background cleaner thread can do its thing\n+                    if (checkpointFile.exists()) {\n+                        taskOffsetSums.put(id, sumOfChangelogOffsets(new OffsetCheckpoint(checkpointFile).read()));\n+                    } else {\n+                        releaseTaskDirLock(id);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTIxMzY2OQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391213669", "bodyText": "How about just a warning in releaseTaskDirLock, and then log as an error if it's actually fatal?", "author": "ableegoldman", "createdAt": "2020-03-11T19:34:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODM1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTQ2MTc2MA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391461760", "bodyText": "I would prefer to not log anything in releaseTaskDirLock() and check the return value of releaseTaskDirLock() at caller side. That would avoid double log messages due to the same event.\nOn a different note, do we need to return the exception from releaseTaskDirLock(). We could just throw it and catch it where we call releaseTaskDirLock(). Am I missing something?", "author": "cadonna", "createdAt": "2020-03-12T08:24:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODM1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTcxNzEyNQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391717125", "bodyText": "new question: why bother with this complexity? The background cleaner can do its thing when we're not rebalancing, right?", "author": "vvcephei", "createdAt": "2020-03-12T15:52:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODM1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg5MTE0OA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391891148", "bodyText": "We have/are making some changes to the directory cleanup (unrelated to 441) that seem like we could end up with a lot of empty directories that have to wait for the cleanup thread to be removed. Since every thread has to go through every task directory and try to lock it, I was thinking we should try to avoid blocking the cleanup thread as much as possible.\nNote that currently, the cleanup thread runs every 10 min (by default). If we also choose 10 min as the default for the probing rebalance interval, and leave empty directories locked during a rebalance, we might never delete them (until the probing reblances end of course).\nMaybe a better approach is to let the cleanup thread run slightly more frequently to remove empty directories only, and skip anything with remaining state & valid checkpoint. WDYT? If that sounds preferable I can make a ticket to follow up later", "author": "ableegoldman", "createdAt": "2020-03-12T21:00:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODM1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk1MTk5MA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391951990", "bodyText": "Nevermind the above -- assuming we get this PR into 2.6 as well we can just leverage the new listNonEmptyTaskDirectories and the problem becomes moot. Removed the unlocking attempt from this method for now", "author": "ableegoldman", "createdAt": "2020-03-12T23:09:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODM1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MzA5MQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390953091", "bodyText": "prop: Could you please use a else branch here?", "author": "cadonna", "createdAt": "2020-03-11T13:01:59Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,50 +378,134 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryLockForAllTaskDirectories()}\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+        for (final TaskId id : lockedTaskDirectories) {\n+            final Task task = tasks.get(id);\n+            if (task != null) {\n+                if (task.isActive() && task.state() == RUNNING) {\n+                    taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                } else {\n+                    taskOffsetSums.put(id, sumOfChangelogOffsets(task.changelogOffsets()));\n+                }\n             } else {\n-                taskOffsetSums.put(id, 0L);\n+                final File checkpointFile = stateDirectory.checkpointFileFor(id);\n+                try {\n+                    // If we can't read the checkpoint file or it doesn't exist, release the task directory\n+                    // so the background cleaner thread can do its thing\n+                    if (checkpointFile.exists()) {\n+                        taskOffsetSums.put(id, sumOfChangelogOffsets(new OffsetCheckpoint(checkpointFile).read()));\n+                    } else {\n+                        releaseTaskDirLock(id);\n+                    }\n+                } catch (final IOException e) {\n+                    log.warn(String.format(\"Exception caught while trying to read checkpoint for task %s:\", id), e);\n+                    releaseTaskDirLock(id);\n+                }\n             }\n         }\n+\n         return taskOffsetSums;\n     }\n \n     /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n+     * Makes a weak attempt to lock all task directories in the state dir. We are responsible for computing and\n+     * reporting the offset sum for any unassigned tasks we obtain the lock for in the upcoming rebalance. Tasks\n+     * that we locked but didn't own will be released at the end of the rebalance (unless of course we were\n+     * assigned the task as a result of the rebalance). This method should be idempotent.\n      */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n+    private void tryLockForAllTaskDirectories() {\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    try {\n+                        if (stateDirectory.lock(id)) {\n+                            lockedTaskDirectories.add(id);\n+                            if (!tasks.containsKey(id)) {\n+                                log.debug(\"Temporarily locked unassigned task {} for the upcoming rebalance\", id);\n+                            }\n+                        }\n+                    } catch (final IOException e) {\n+                        // if for any reason we can't lock this task dir, just move on\n+                        log.warn(String.format(\"Exception caught while attempting to lock task %s:\", id), e);\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // ignore any unknown files that sit in the same directory\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempts to release the lock for the passed in task's directory. It does not remove the task from\n+     * {@code lockedTaskDirectories} so it's safe to call during iteration, and should be idempotent.\n+     */\n+    private RuntimeException releaseTaskDirLock(final TaskId taskId) {\n+        try {\n+            stateDirectory.unlock(taskId);\n+        } catch (final IOException e) {\n+            log.error(\"Failed to release the lock for task directory {}.\", taskId);\n+            return new StreamsException(String.format(\"Unable to unlock task directory %s\", taskId), e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * We must release the lock for any unassigned tasks that we temporarily locked in preparation for a\n+     * rebalance in {@link #tryLockForAllTaskDirectories()}.\n+     */\n+    private void releaseLockedUnassignedTaskDirectories() {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        final Iterator<TaskId> taskIdIterator = lockedTaskDirectories.iterator();\n+        while (taskIdIterator.hasNext()) {\n+            final TaskId id = taskIdIterator.next();\n+\n+            if (!tasks.containsKey(id)) {\n+                final RuntimeException unlockException = releaseTaskDirLock(id);\n+                if (unlockException == null) {\n+                    taskIdIterator.remove();\n+                } else {\n+                    firstException.compareAndSet(null, unlockException);\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        final RuntimeException fatalException = firstException.get();\n+        if (fatalException != null) {\n+            throw fatalException;\n+        }\n+    }\n+\n+    private long sumOfChangelogOffsets(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0L;\n+        for (final Map.Entry<TopicPartition, Long> changelogEntry : changelogOffsets.entrySet()) {\n+            final TopicPartition changelog = changelogEntry.getKey();\n+            final long offset = changelogEntry.getValue();\n+\n+            if (offset < 0L) {\n+                if (offset == -1L) {\n+                    log.debug(\"Skipping unknown offset for changelog {}\", changelog);\n+                } else {\n+                    log.warn(\"Unexpected negative offset {} for changelog {}\", offset, changelog);\n+                }\n+                continue;\n+            }\n+\n+            offsetSum += offset;\n+            if (offsetSum < 0) {\n+                return Long.MAX_VALUE;\n+            }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2NDc0Mg==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390964742", "bodyText": "A last comment on overflows (after this I will shut up). Probably overflows are a rare event. However, if we pin the offset sum to MAX_VALUE we are stuck there. All following assignments will have the same (probably wrong) behavior. My proposal will actually make this code simpler, because we would not do the check for negative sum. The code on the assignor would need to handle the cases I pointed out in my previous comment. The only transient mistake that we would do is because of Task.LATEST_OFFSET since this value of the sum has a special meaning. As I have already pointed out, my proposal only works if Java guarantees that MAX_VALUE is followed by MIN_VALUE.", "author": "cadonna", "createdAt": "2020-03-11T13:22:03Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,50 +378,134 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryLockForAllTaskDirectories()}\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+        for (final TaskId id : lockedTaskDirectories) {\n+            final Task task = tasks.get(id);\n+            if (task != null) {\n+                if (task.isActive() && task.state() == RUNNING) {\n+                    taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                } else {\n+                    taskOffsetSums.put(id, sumOfChangelogOffsets(task.changelogOffsets()));\n+                }\n             } else {\n-                taskOffsetSums.put(id, 0L);\n+                final File checkpointFile = stateDirectory.checkpointFileFor(id);\n+                try {\n+                    // If we can't read the checkpoint file or it doesn't exist, release the task directory\n+                    // so the background cleaner thread can do its thing\n+                    if (checkpointFile.exists()) {\n+                        taskOffsetSums.put(id, sumOfChangelogOffsets(new OffsetCheckpoint(checkpointFile).read()));\n+                    } else {\n+                        releaseTaskDirLock(id);\n+                    }\n+                } catch (final IOException e) {\n+                    log.warn(String.format(\"Exception caught while trying to read checkpoint for task %s:\", id), e);\n+                    releaseTaskDirLock(id);\n+                }\n             }\n         }\n+\n         return taskOffsetSums;\n     }\n \n     /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n+     * Makes a weak attempt to lock all task directories in the state dir. We are responsible for computing and\n+     * reporting the offset sum for any unassigned tasks we obtain the lock for in the upcoming rebalance. Tasks\n+     * that we locked but didn't own will be released at the end of the rebalance (unless of course we were\n+     * assigned the task as a result of the rebalance). This method should be idempotent.\n      */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n+    private void tryLockForAllTaskDirectories() {\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    try {\n+                        if (stateDirectory.lock(id)) {\n+                            lockedTaskDirectories.add(id);\n+                            if (!tasks.containsKey(id)) {\n+                                log.debug(\"Temporarily locked unassigned task {} for the upcoming rebalance\", id);\n+                            }\n+                        }\n+                    } catch (final IOException e) {\n+                        // if for any reason we can't lock this task dir, just move on\n+                        log.warn(String.format(\"Exception caught while attempting to lock task %s:\", id), e);\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // ignore any unknown files that sit in the same directory\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempts to release the lock for the passed in task's directory. It does not remove the task from\n+     * {@code lockedTaskDirectories} so it's safe to call during iteration, and should be idempotent.\n+     */\n+    private RuntimeException releaseTaskDirLock(final TaskId taskId) {\n+        try {\n+            stateDirectory.unlock(taskId);\n+        } catch (final IOException e) {\n+            log.error(\"Failed to release the lock for task directory {}.\", taskId);\n+            return new StreamsException(String.format(\"Unable to unlock task directory %s\", taskId), e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * We must release the lock for any unassigned tasks that we temporarily locked in preparation for a\n+     * rebalance in {@link #tryLockForAllTaskDirectories()}.\n+     */\n+    private void releaseLockedUnassignedTaskDirectories() {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        final Iterator<TaskId> taskIdIterator = lockedTaskDirectories.iterator();\n+        while (taskIdIterator.hasNext()) {\n+            final TaskId id = taskIdIterator.next();\n+\n+            if (!tasks.containsKey(id)) {\n+                final RuntimeException unlockException = releaseTaskDirLock(id);\n+                if (unlockException == null) {\n+                    taskIdIterator.remove();\n+                } else {\n+                    firstException.compareAndSet(null, unlockException);\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        final RuntimeException fatalException = firstException.get();\n+        if (fatalException != null) {\n+            throw fatalException;\n+        }\n+    }\n+\n+    private long sumOfChangelogOffsets(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0L;\n+        for (final Map.Entry<TopicPartition, Long> changelogEntry : changelogOffsets.entrySet()) {\n+            final TopicPartition changelog = changelogEntry.getKey();\n+            final long offset = changelogEntry.getValue();\n+\n+            if (offset < 0L) {\n+                if (offset == -1L) {\n+                    log.debug(\"Skipping unknown offset for changelog {}\", changelog);\n+                } else {\n+                    log.warn(\"Unexpected negative offset {} for changelog {}\", offset, changelog);\n+                }\n+                continue;\n+            }\n+\n+            offsetSum += offset;\n+            if (offsetSum < 0) {\n+                return Long.MAX_VALUE;\n+            }", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE5NzMyMA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391197320", "bodyText": "That's fair, and I'm pretty  sure Java does guarantee that. However I still have two concerns:\n\nIf it can overflow once, it can overflow twice. If it wraps back around to positive offsets we won't be able to distinguish an instance that's caught up to extremely large offsets from one that's just started\nWe'd have to protect Task.LATEST_OFFSET, which would be awkward (though admittedly not terribly so). But, what if we need to introduce new sentinels?", "author": "ableegoldman", "createdAt": "2020-03-11T19:02:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2NDc0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTIwNjYxNg==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391206616", "bodyText": "This is hard to reason about in a vacuum, so here's some rough numbers to give things context. I found this quote that gives perspective on what you can do with one partition whose offsets are stored as long:\n\nIf you write 1TB a day, you can keep going for about 4 million days\n\nThis corresponds to roughly 11.5MB/s for 11,000 years. Of course a Streams app will have more than one store per partition. Most topologies probably cap out at around 5-10 stores/partition, but let's take an extreme example and say you have 1000 stores/partition.\nWith this setup you could process 11.5MB/s for 11 years, assuming you hit no other limits -- and I'm pretty skeptical an app with 1000+ stores per instance could easily reach 11.5MB/s to begin with.\nSo with that in mind, do we think a long is sufficient? cc/ @vvcephei", "author": "ableegoldman", "createdAt": "2020-03-11T19:20:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2NDc0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxMTM5MQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391311391", "bodyText": "I was looking over the history of this discussion to see what Bruno's proposal actually was, and I couldn't find it. This? #8246\nBut I didn't see anything about not using a long. I think that comment was that you can just let the overflow happen, and then when you compute lag, the end-offsets would also have overflown, and they'll bear a predictable relationship to each other, which is true.\nThen again, Sophie makes a good point that overflowing Long with offsets, even with multiple stores per task, is pretty hard in practice. In that case, it would still be nice to detect it and just throw an exception, rather than allowing nonsense to propagate through the application logic. I think 1000 stores in a single task is probably on the high side, but who knows... I think 11 years is kind of a long time, but then again, I've personally worked with databases that had been running continuously for longer than that.", "author": "vvcephei", "createdAt": "2020-03-11T22:42:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2NDc0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM2MDAzNQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391360035", "bodyText": "@vvcephei Not 100% what you're getting at in the first paragraph, but I don't think Bruno was suggesting to not use a long, just to let it overflow. My concern was that if we really think it's possible to overflow past Long.MAX_VALUE then we should also be concerned about it doing so again and wrapping back to positive offsets again. This would cause more serious degradation to the assignment algorithm since really-caught-up clients would be indistinguishable from just-started ones.\nOn the other hand, pinning overflow to Long.MAX_VALUE will at worst cause the algorithm to reduce to the current one as all clients will appear caught-up.", "author": "ableegoldman", "createdAt": "2020-03-12T01:41:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2NDc0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM2MTUxNw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391361517", "bodyText": "Not to mention, unless they do have a truly unreasonable number of stores per partition AND the app manages to keep up with the input topics, it's pretty likely they would be doomed to hit the MAX_VALUE on at least one partition's offset not long after hitting this on the offset sum (if not before). And there's nothing we can do about that.", "author": "ableegoldman", "createdAt": "2020-03-12T01:49:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2NDc0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTQ3MjU2MQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391472561", "bodyText": "@ableegoldman Good point about not be able to distinguish a new instance from a caught-up instance when the offset sums get positive again (I do not want to imagine how long restoration would take in that case ;-)). Let's pin the offset at MAX_VALUE in the rare case of an overflow and issue a WARN log message. OK?", "author": "cadonna", "createdAt": "2020-03-12T08:46:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2NDc0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTY2NDkyMQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391664921", "bodyText": "One thing to get out of the way, you can't overflow back to positive again with addition: MAX_VALUE + MAX_VALUE = -2, so as long as you check after each addition operation, you're 100% covered. If you really want to be paranoid that maybe some future version of Java would change the relationship among max, min, and zero such that max*2 is > 0 (which I think is unlikely), you could instead check that the result of adding two numbers is greater than both operands.\nI'm ok with a saturation strategy, and just pinning at MAX_VALUE. We should do the same when computing end offsets as when computing current position. Although, I guess that means that by the time a bunch of tasks saturate their positions (gets pinned to MAX_VALUE), the assignment would just throw up its hands and say they're all caught up, since they all have the same value as the end-offset (which also would have saturated). This seems like a bummer, but it's either that or crash. Since the result isn't incorrect processing, just a potentially suboptimal assignment, I think saturation is a fine approach in conjunction with a warning.", "author": "vvcephei", "createdAt": "2020-03-12T14:36:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2NDc0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg5MzQxNw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391893417", "bodyText": "Note that we can always bump the protocol version in 10 years, and start encoding the offset sums as a byte[], BigInt, etc.", "author": "ableegoldman", "createdAt": "2020-03-12T21:05:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2NDc0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg5NTk5OA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391895998", "bodyText": "I'll add the warning for now -- presumably if someone actually hits this on a task they'll ask about the warning and/or submit a ticket.\nAnd that's how we'll know it's time to bump the protocol \ud83d\ude09", "author": "ableegoldman", "createdAt": "2020-03-12T21:11:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2NDc0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2NzQ3Mw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390967473", "bodyText": "Nice! I always forget about making fields unmodifiable when they become visible to the outside.", "author": "cadonna", "createdAt": "2020-03-11T13:26:22Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -676,4 +773,8 @@ public String toString(final String indent) {\n     Set<String> producerClientIds() {\n         return activeTaskCreator.producerClientIds();\n     }\n+\n+    Set<TaskId> lockedTaskDirectories() {\n+        return Collections.unmodifiableSet(lockedTaskDirectories);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk4MzE0Mw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390983143", "bodyText": "prop: Rename topicGroup to something more appropriate.", "author": "cadonna", "createdAt": "2020-03-11T13:50:13Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -200,11 +201,19 @@ public UUID processId() {\n     public Map<TaskId, Long> taskOffsetSums() {\n         if (taskOffsetSumsCache == null) {\n             taskOffsetSumsCache = new HashMap<>();\n-            for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n-                for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n-                    taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n-                                            partitionOffsetSum.offsetSum());\n+            if (data.version() >= MIN_VERSION_OFFSET_SUM_SUBSCRIPTION) {\n+                for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk4NzI5Mw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390987293", "bodyText": "req: I am missing a unit test for version >= 7.", "author": "cadonna", "createdAt": "2020-03-11T13:56:13Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java", "diffHunk": "@@ -294,13 +295,36 @@ public void shouldEncodeAndDecodeVersion7() {\n     }\n \n     @Test\n-    public void shouldConvertTaskOffsetSumMapToTaskSetsForOlderVersion() {\n+    public void shouldConvertTaskOffsetSumMapToTaskSets() {\n         final SubscriptionInfo info =\n             new SubscriptionInfo(7, LATEST_SUPPORTED_VERSION, processId, \"localhost:80\", TASK_OFFSET_SUMS);\n         assertThat(info.prevTasks(), is(ACTIVE_TASKS));\n         assertThat(info.standbyTasks(), is(STANDBY_TASKS));\n     }\n \n+    @Test\n+    public void shouldConvertTaskSetsToTaskOffsetSumMapWithOlderSubscription() {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk5MzMwMw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390993303", "bodyText": "prop: I would use taskId01.toString() here, since you are not testing the taskId01.toString() method. Our assumption is that the folder has a name that is equal to the result of taskId01.toString() and not 0_1.", "author": "cadonna", "createdAt": "2020-03-11T14:04:49Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldTryToLockValidTaskDirsAtRebalanceStart() throws IOException {\n+        expectLockObtainedFor(taskId01);\n+        expectLockFailedFor(taskId10);\n+\n+        makeTaskFolders(\n+            \"0_1\",", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE3Mzk4NQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391173985", "bodyText": "Good point, will do", "author": "ableegoldman", "createdAt": "2020-03-11T18:20:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk5MzMwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAyMDk1MA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391020950", "bodyText": "req: Specify stateDirectory as\n    @Mock(type = MockType.DEFAULT)\n    private StateDirectory stateDirectory;\n\non line 122.\nThen you can omit the expectation for stateDirectory.lock(anyObject()).\nNote that with a default mock, the tests shouldNotReportOffsetSumsForUnassignedTaskWithoutCheckpoint() and shouldReleaseLockForUnassignedTasksAfterRebalance() fail, because you do not set the expectation that the state dir unlocks some task directories. You should actually set that expectations, right?", "author": "cadonna", "createdAt": "2020-03-11T14:41:44Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE3OTQxNA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391179414", "bodyText": "Ack, will add the unlock expectation where appropriate", "author": "ableegoldman", "createdAt": "2020-03-11T18:30:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAyMDk1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAyODMxMw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391028313", "bodyText": "req: Please also verify stateDirectory.unlock(\"0_2\"). Only verifying lockedTaskDirectories() seems too weak to me.", "author": "cadonna", "createdAt": "2020-03-11T14:51:23Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldTryToLockValidTaskDirsAtRebalanceStart() throws IOException {\n+        expectLockObtainedFor(taskId01);\n+        expectLockFailedFor(taskId10);\n+\n+        makeTaskFolders(\n+            \"0_1\",\n+            \"1_0\",\n+            \"dummy\"\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        verify(stateDirectory);\n+        assertThat(taskManager.lockedTaskDirectories(), is(singleton(taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReleaseLockForUnassignedTasksAfterRebalance() throws IOException {\n+        expectLockObtainedFor(taskId00, taskId01, taskId02);\n+\n+        makeTaskFolders(\n+                \"0_0\",  // active task\n+                \"0_1\",  // standby task\n+                \"0_2\"   // unassigned but able to lock\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01, taskId02)));\n+\n+        handleAssignment(taskId00Assignment, taskId01Assignment, emptyMap());\n+        reset(consumer);\n+        expectConsumerAssignmentPaused(consumer);\n+        replay(consumer);\n+\n+        taskManager.handleRebalanceComplete();\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01)));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAzMjQ0MA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391032440", "bodyText": "prop: You can remove this line. You only need expectLastCall() if you need to do some further expectation settings on a call that returns void, e.g., expectLastCall().times(3)", "author": "cadonna", "createdAt": "2020-03-11T14:56:49Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -1411,6 +1587,74 @@ public void shouldTransmitProducerMetrics() {\n         assertThat(taskManager.producerMetrics(), is(dummyProducerMetrics));\n     }\n \n+    private Map<TaskId, StateMachineTask> handleAssignment(final Map<TaskId, Set<TopicPartition>> runningActiveAssignment,\n+                                                           final Map<TaskId, Set<TopicPartition>> standbyAssignment,\n+                                                           final Map<TaskId, Set<TopicPartition>> restoringActiveAssignment) {\n+        final Set<Task> runningTasks = runningActiveAssignment.entrySet().stream()\n+                                           .map(t -> new StateMachineTask(t.getKey(), t.getValue(), true))\n+                                           .collect(Collectors.toSet());\n+        final Set<Task> standbyTasks = standbyAssignment.entrySet().stream()\n+                                           .map(t -> new StateMachineTask(t.getKey(), t.getValue(), false))\n+                                           .collect(Collectors.toSet());\n+        final Set<Task> restoringTasks = restoringActiveAssignment.entrySet().stream()\n+                                             .map(t -> new StateMachineTask(t.getKey(), t.getValue(), true))\n+                                             .collect(Collectors.toSet());\n+\n+        // Initially assign only the active tasks we want to complete restoration\n+        final Map<TaskId, Set<TopicPartition>> allActiveTasksAssignment = new HashMap<>(runningActiveAssignment);\n+        allActiveTasksAssignment.putAll(restoringActiveAssignment);\n+        final Set<Task> allActiveTasks = new HashSet<>(runningTasks);\n+        allActiveTasks.addAll(restoringTasks);\n+\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(runningActiveAssignment))).andStubReturn(runningTasks);\n+        expect(standbyTaskCreator.createTasks(eq(standbyAssignment))).andStubReturn(standbyTasks);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(allActiveTasksAssignment))).andStubReturn(allActiveTasks);\n+\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        replay(activeTaskCreator, standbyTaskCreator, consumer, changeLogReader);\n+\n+        taskManager.handleAssignment(runningActiveAssignment, standbyAssignment);\n+        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n+\n+        taskManager.handleAssignment(allActiveTasksAssignment, standbyAssignment);\n+\n+        final Map<TaskId, StateMachineTask> allTasks = new HashMap<>();\n+\n+        // Just make sure all tasks ended up in the expected state\n+        for (final Task task : runningTasks) {\n+            assertThat(task.state(), is(Task.State.RUNNING));\n+            allTasks.put(task.id(), (StateMachineTask) task);\n+        }\n+        for (final Task task : restoringTasks) {\n+            assertThat(task.state(), not(Task.State.RUNNING));\n+            allTasks.put(task.id(), (StateMachineTask) task);\n+        }\n+        for (final Task task : standbyTasks) {\n+            assertThat(task.state(), is(Task.State.RUNNING));\n+            allTasks.put(task.id(), (StateMachineTask) task);\n+        }\n+        return allTasks;\n+    }\n+\n+    private void expectLockObtainedFor(final TaskId... tasks) throws IOException {\n+        for (final TaskId task : tasks) {\n+            expect(stateDirectory.lock(task)).andReturn(true).once();\n+        }\n+    }\n+\n+    private void expectLockFailedFor(final TaskId... tasks) throws IOException {\n+        for (final TaskId task : tasks) {\n+            expect(stateDirectory.lock(task)).andReturn(false).once();\n+        }\n+    }\n+\n+    private static void expectConsumerAssignmentPaused(final Consumer<byte[], byte[]> consumer) {\n+        final Set<TopicPartition> assignment = singleton(new TopicPartition(\"assignment\", 0));\n+        expect(consumer.assignment()).andReturn(assignment);\n+        consumer.pause(assignment);\n+        expectLastCall();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAzNTIyNA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391035224", "bodyText": "prop: Rename to shouldReportLatestOffsetAsOffsetSumForRunningTask", "author": "cadonna", "createdAt": "2020-03-11T15:00:25Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldTryToLockValidTaskDirsAtRebalanceStart() throws IOException {\n+        expectLockObtainedFor(taskId01);\n+        expectLockFailedFor(taskId10);\n+\n+        makeTaskFolders(\n+            \"0_1\",\n+            \"1_0\",\n+            \"dummy\"\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        verify(stateDirectory);\n+        assertThat(taskManager.lockedTaskDirectories(), is(singleton(taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReleaseLockForUnassignedTasksAfterRebalance() throws IOException {\n+        expectLockObtainedFor(taskId00, taskId01, taskId02);\n+\n+        makeTaskFolders(\n+                \"0_0\",  // active task\n+                \"0_1\",  // standby task\n+                \"0_2\"   // unassigned but able to lock\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01, taskId02)));\n+\n+        handleAssignment(taskId00Assignment, taskId01Assignment, emptyMap());\n+        reset(consumer);\n+        expectConsumerAssignmentPaused(consumer);\n+        replay(consumer);\n+\n+        taskManager.handleRebalanceComplete();\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReportLatestOffsetForRunningTask() throws IOException {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA0MDEwMw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391040103", "bodyText": "req: Could you add a test for taskManager.getTaskOffsetSums() when there are no locked task directories, i.e., lockedTaskDirectories is empty?", "author": "cadonna", "createdAt": "2020-03-11T15:07:03Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldTryToLockValidTaskDirsAtRebalanceStart() throws IOException {\n+        expectLockObtainedFor(taskId01);\n+        expectLockFailedFor(taskId10);\n+\n+        makeTaskFolders(\n+            \"0_1\",\n+            \"1_0\",\n+            \"dummy\"\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        verify(stateDirectory);\n+        assertThat(taskManager.lockedTaskDirectories(), is(singleton(taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReleaseLockForUnassignedTasksAfterRebalance() throws IOException {\n+        expectLockObtainedFor(taskId00, taskId01, taskId02);\n+\n+        makeTaskFolders(\n+                \"0_0\",  // active task\n+                \"0_1\",  // standby task\n+                \"0_2\"   // unassigned but able to lock\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01, taskId02)));\n+\n+        handleAssignment(taskId00Assignment, taskId01Assignment, emptyMap());\n+        reset(consumer);\n+        expectConsumerAssignmentPaused(consumer);\n+        replay(consumer);\n+\n+        taskManager.handleRebalanceComplete();\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReportLatestOffsetForRunningTask() throws IOException {\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, Task.LATEST_OFFSET));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        handleAssignment(taskId00Assignment, emptyMap(), emptyMap());\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForNonRunningActiveTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        final StateMachineTask restoringTask = handleAssignment(\n+            emptyMap(),\n+            emptyMap(),\n+            taskId00Assignment\n+        ).get(taskId00);\n+        restoringTask.setChangelogOffsets(changelogOffsets);\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForStandbyTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        final StateMachineTask restoringTask = handleAssignment(\n+            emptyMap(),\n+            taskId00Assignment,\n+            emptyMap()\n+        ).get(taskId00);\n+        restoringTask.setChangelogOffsets(changelogOffsets);\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForUnassignedLockableTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        writeCheckpointFile(taskId00, changelogOffsets);\n+\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldNotReportOffsetSumsForUnlockableTask() throws IOException {\n+        expectLockFailedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertTrue(taskManager.getTaskOffsetSums().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldNotReportOffsetSumsForUnassignedTaskWithoutCheckpoint() throws IOException {\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        expect(stateDirectory.checkpointFileFor(taskId00)).andReturn(getCheckpointFile(taskId00));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertTrue(taskManager.getTaskOffsetSums().isEmpty());\n+    }\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE4NTg3Ng==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391185876", "bodyText": "I believe that's the same as what shouldNotReportOffsetSumsForTaskWeCantLock is testing, or did you have something different in mind?", "author": "ableegoldman", "createdAt": "2020-03-11T18:41:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA0MDEwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTQ3ODM5MQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391478391", "bodyText": "Oh, I guess I missed shouldNotReportOffsetSumsForTaskWeCantLock(). No, I did not have something different in mind.", "author": "cadonna", "createdAt": "2020-03-12T08:58:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA0MDEwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA0MTk1Nw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391041957", "bodyText": "prop: Rename to shouldPinOffsetToLongMaxValueInCaseOfOverflow", "author": "cadonna", "createdAt": "2020-03-11T15:09:30Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldTryToLockValidTaskDirsAtRebalanceStart() throws IOException {\n+        expectLockObtainedFor(taskId01);\n+        expectLockFailedFor(taskId10);\n+\n+        makeTaskFolders(\n+            \"0_1\",\n+            \"1_0\",\n+            \"dummy\"\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        verify(stateDirectory);\n+        assertThat(taskManager.lockedTaskDirectories(), is(singleton(taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReleaseLockForUnassignedTasksAfterRebalance() throws IOException {\n+        expectLockObtainedFor(taskId00, taskId01, taskId02);\n+\n+        makeTaskFolders(\n+                \"0_0\",  // active task\n+                \"0_1\",  // standby task\n+                \"0_2\"   // unassigned but able to lock\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01, taskId02)));\n+\n+        handleAssignment(taskId00Assignment, taskId01Assignment, emptyMap());\n+        reset(consumer);\n+        expectConsumerAssignmentPaused(consumer);\n+        replay(consumer);\n+\n+        taskManager.handleRebalanceComplete();\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReportLatestOffsetForRunningTask() throws IOException {\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, Task.LATEST_OFFSET));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        handleAssignment(taskId00Assignment, emptyMap(), emptyMap());\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForNonRunningActiveTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        final StateMachineTask restoringTask = handleAssignment(\n+            emptyMap(),\n+            emptyMap(),\n+            taskId00Assignment\n+        ).get(taskId00);\n+        restoringTask.setChangelogOffsets(changelogOffsets);\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForStandbyTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        final StateMachineTask restoringTask = handleAssignment(\n+            emptyMap(),\n+            taskId00Assignment,\n+            emptyMap()\n+        ).get(taskId00);\n+        restoringTask.setChangelogOffsets(changelogOffsets);\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForUnassignedLockableTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        writeCheckpointFile(taskId00, changelogOffsets);\n+\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldNotReportOffsetSumsForUnlockableTask() throws IOException {\n+        expectLockFailedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertTrue(taskManager.getTaskOffsetSums().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldNotReportOffsetSumsForUnassignedTaskWithoutCheckpoint() throws IOException {\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        expect(stateDirectory.checkpointFileFor(taskId00)).andReturn(getCheckpointFile(taskId00));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertTrue(taskManager.getTaskOffsetSums().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldSkipInvalidOffsetsWhenComputingOffsetSum() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 1), -1L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 0L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        writeCheckpointFile(taskId00, changelogOffsets);\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        replay(activeTaskCreator, stateDirectory);\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n \n-        final Map<TaskId, Long> taskOffsetSums = taskManager.getTaskOffsetSums();\n+    @Test\n+    public void shouldOffsetSumOverflowToLongMaxValue() throws IOException {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE4NjMyMA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391186320", "bodyText": "Wow yeah that original test name made no sense, thanks for the prop", "author": "ableegoldman", "createdAt": "2020-03-11T18:42:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA0MTk1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA0MzczNg==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391043736", "bodyText": "req: Could add a test for an offset that is negative but not -1? And another test with multiple offsets of different types, i.e., negative, unknown, etc. The latter could be the composite case, we already talked about.", "author": "cadonna", "createdAt": "2020-03-11T15:11:57Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldTryToLockValidTaskDirsAtRebalanceStart() throws IOException {\n+        expectLockObtainedFor(taskId01);\n+        expectLockFailedFor(taskId10);\n+\n+        makeTaskFolders(\n+            \"0_1\",\n+            \"1_0\",\n+            \"dummy\"\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        verify(stateDirectory);\n+        assertThat(taskManager.lockedTaskDirectories(), is(singleton(taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReleaseLockForUnassignedTasksAfterRebalance() throws IOException {\n+        expectLockObtainedFor(taskId00, taskId01, taskId02);\n+\n+        makeTaskFolders(\n+                \"0_0\",  // active task\n+                \"0_1\",  // standby task\n+                \"0_2\"   // unassigned but able to lock\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01, taskId02)));\n+\n+        handleAssignment(taskId00Assignment, taskId01Assignment, emptyMap());\n+        reset(consumer);\n+        expectConsumerAssignmentPaused(consumer);\n+        replay(consumer);\n+\n+        taskManager.handleRebalanceComplete();\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReportLatestOffsetForRunningTask() throws IOException {\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, Task.LATEST_OFFSET));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        handleAssignment(taskId00Assignment, emptyMap(), emptyMap());\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForNonRunningActiveTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        final StateMachineTask restoringTask = handleAssignment(\n+            emptyMap(),\n+            emptyMap(),\n+            taskId00Assignment\n+        ).get(taskId00);\n+        restoringTask.setChangelogOffsets(changelogOffsets);\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForStandbyTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        final StateMachineTask restoringTask = handleAssignment(\n+            emptyMap(),\n+            taskId00Assignment,\n+            emptyMap()\n+        ).get(taskId00);\n+        restoringTask.setChangelogOffsets(changelogOffsets);\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForUnassignedLockableTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        writeCheckpointFile(taskId00, changelogOffsets);\n+\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldNotReportOffsetSumsForUnlockableTask() throws IOException {\n+        expectLockFailedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertTrue(taskManager.getTaskOffsetSums().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldNotReportOffsetSumsForUnassignedTaskWithoutCheckpoint() throws IOException {\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        expect(stateDirectory.checkpointFileFor(taskId00)).andReturn(getCheckpointFile(taskId00));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertTrue(taskManager.getTaskOffsetSums().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldSkipInvalidOffsetsWhenComputingOffsetSum() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 1), -1L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 0L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        writeCheckpointFile(taskId00, changelogOffsets);\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        replay(activeTaskCreator, stateDirectory);\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n \n-        final Map<TaskId, Long> taskOffsetSums = taskManager.getTaskOffsetSums();\n+    @Test\n+    public void shouldOffsetSumOverflowToLongMaxValue() throws IOException {\n+        final long largeOffset = Long.MAX_VALUE / 2;\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 1), largeOffset),\n+            mkEntry(new TopicPartition(\"changelog\", 2), largeOffset),\n+            mkEntry(new TopicPartition(\"changelog\", 3), largeOffset)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, Long.MAX_VALUE));\n \n-        verify(activeTaskCreator, stateDirectory);\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        writeCheckpointFile(taskId00, changelogOffsets);\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        assertThat(taskOffsetSums.keySet(), equalTo(mkSet(taskId01, taskId02, new TaskId(1, 1))));\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n     }\n ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE4ODE2Ng==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391188166", "bodyText": "Sure, I'll add one based on the current code and can update it if we decide to allow overflow", "author": "ableegoldman", "createdAt": "2020-03-11T18:46:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA0MzczNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgyOTYxNw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391829617", "bodyText": "Still unsure if this is the right logic. What if we just return an \"unknown sum\" sentinel here? Then, if any store's offset is unknown, then the task's offset sum would also be reported as \"unknown\", which would let the assignor treat it as \"not caught up\".", "author": "vvcephei", "createdAt": "2020-03-12T18:59:38Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,50 +378,132 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryToLockAllTaskDirectories()}\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+        for (final TaskId id : lockedTaskDirectories) {\n+            final Task task = tasks.get(id);\n+            if (task != null) {\n+                if (task.isActive() && task.state() == RUNNING) {\n+                    taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                } else {\n+                    taskOffsetSums.put(id, sumOfChangelogOffsets(task.changelogOffsets()));\n+                }\n             } else {\n-                taskOffsetSums.put(id, 0L);\n+                final File checkpointFile = stateDirectory.checkpointFileFor(id);\n+                try {\n+                    // If we can't read the checkpoint file or it doesn't exist, release the task directory\n+                    // so the background cleaner thread can do its thing\n+                    if (checkpointFile.exists()) {\n+                        taskOffsetSums.put(id, sumOfChangelogOffsets(new OffsetCheckpoint(checkpointFile).read()));\n+                    } else {\n+                        releaseTaskDirLock(id);\n+                    }\n+                } catch (final IOException e) {\n+                    log.warn(String.format(\"Exception caught while trying to read checkpoint for task %s:\", id), e);\n+                    releaseTaskDirLock(id);\n+                }\n             }\n         }\n+\n         return taskOffsetSums;\n     }\n \n     /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n+     * Makes a weak attempt to lock all task directories in the state dir. We are responsible for computing and\n+     * reporting the offset sum for any unassigned tasks we obtain the lock for in the upcoming rebalance. Tasks\n+     * that we locked but didn't own will be released at the end of the rebalance (unless of course we were\n+     * assigned the task as a result of the rebalance). This method should be idempotent.\n      */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n-        final File[] stateDirs = stateDirectory.listTaskDirectories();\n-        if (stateDirs != null) {\n-            for (final File dir : stateDirs) {\n+    private void tryToLockAllTaskDirectories() {\n+        for (final File dir : stateDirectory.listTaskDirectories()) {\n+            try {\n+                final TaskId id = TaskId.parse(dir.getName());\n                 try {\n-                    final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    if (stateDirectory.lock(id)) {\n+                        lockedTaskDirectories.add(id);\n+                        if (!tasks.containsKey(id)) {\n+                            log.debug(\"Temporarily locked unassigned task {} for the upcoming rebalance\", id);\n+                        }\n                     }\n-                } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                } catch (final IOException e) {\n+                    // if for any reason we can't lock this task dir, just move on\n+                    log.warn(String.format(\"Exception caught while attempting to lock task %s:\", id), e);\n+                }\n+            } catch (final TaskIdFormatException e) {\n+                // ignore any unknown files that sit in the same directory\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempts to release the lock for the passed in task's directory. It does not remove the task from\n+     * {@code lockedTaskDirectories} so it's safe to call during iteration, and should be idempotent.\n+     */\n+    private RuntimeException releaseTaskDirLock(final TaskId taskId) {\n+        try {\n+            stateDirectory.unlock(taskId);\n+        } catch (final IOException e) {\n+            log.warn(String.format(\"Caught the following exception while trying to unlock task %s\", taskId), e);\n+            return new StreamsException(String.format(\"Unable to unlock task directory %s\", taskId), e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * We must release the lock for any unassigned tasks that we temporarily locked in preparation for a\n+     * rebalance in {@link #tryToLockAllTaskDirectories()}.\n+     */\n+    private void releaseLockedUnassignedTaskDirectories() {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        final Iterator<TaskId> taskIdIterator = lockedTaskDirectories.iterator();\n+        while (taskIdIterator.hasNext()) {\n+            final TaskId id = taskIdIterator.next();\n+\n+            if (!tasks.containsKey(id)) {\n+                final RuntimeException unlockException = releaseTaskDirLock(id);\n+                if (unlockException == null) {\n+                    taskIdIterator.remove();\n+                } else {\n+                    log.error(\"Failed to release the lock for task directory {}.\", id);\n+                    firstException.compareAndSet(null, unlockException);\n+                }\n+            }\n+        }\n+\n+        final RuntimeException fatalException = firstException.get();\n+        if (fatalException != null) {\n+            throw fatalException;\n+        }\n+    }\n+\n+    private long sumOfChangelogOffsets(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0L;\n+        for (final Map.Entry<TopicPartition, Long> changelogEntry : changelogOffsets.entrySet()) {\n+            final TopicPartition changelog = changelogEntry.getKey();\n+            final long offset = changelogEntry.getValue();\n+\n+            if (offset < 0L) {\n+                if (offset == -1L) {\n+                    log.debug(\"Skipping unknown offset for changelog {}\", changelog);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkwODgzNw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391908837", "bodyText": "In what cases might one store's offsets being invalid mean that every other store with valid offsets should not be taken into account?\nThat's not rhetorical, I really am asking. It's not clear to me exactly when you'd get this invalid offset response -- but if only one partition was having issues (whatever those may be) and the others al had valid, positive offsets, would we have to wipe out the entire state? Do we even check for negative offsets elsewhere in Streams? It's not clear to me that/if we do (in fact several places assume they are always positive and I believe would actually crash if not)", "author": "ableegoldman", "createdAt": "2020-03-12T21:32:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgyOTYxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjI5NDQ5MA==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r392294490", "bodyText": "As far as I can see from the code, an offset of -1 is returned in error case. Additionally, offset -1 is used during initialization of the producer response. I guess in the error case we do not write any offset into the checkpoint file and the offset map. So I suppose that it cannot happen that the offset becomes -1 in this code. So, my proposal would be to double-check my observation. In case it is correct we can specify an invariant that the offset must be >= 0 and throw an IllegalStateException if the invariant is not satisfied.", "author": "cadonna", "createdAt": "2020-03-13T15:20:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgyOTYxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM4MTU2Nw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r392381567", "bodyText": "We skip adding offsets to the map in the error case, but I asked Jason and apparently there may be a non-error case where -1 is returned with an idempotent producer. Just cc'ed you on the thread", "author": "ableegoldman", "createdAt": "2020-03-13T17:49:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgyOTYxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgzMDAwNQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391830005", "bodyText": "you could avoid computing the addition twice by checking after this line if offsetSum < 0", "author": "vvcephei", "createdAt": "2020-03-12T19:00:29Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,50 +378,132 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryToLockAllTaskDirectories()}\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+        for (final TaskId id : lockedTaskDirectories) {\n+            final Task task = tasks.get(id);\n+            if (task != null) {\n+                if (task.isActive() && task.state() == RUNNING) {\n+                    taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                } else {\n+                    taskOffsetSums.put(id, sumOfChangelogOffsets(task.changelogOffsets()));\n+                }\n             } else {\n-                taskOffsetSums.put(id, 0L);\n+                final File checkpointFile = stateDirectory.checkpointFileFor(id);\n+                try {\n+                    // If we can't read the checkpoint file or it doesn't exist, release the task directory\n+                    // so the background cleaner thread can do its thing\n+                    if (checkpointFile.exists()) {\n+                        taskOffsetSums.put(id, sumOfChangelogOffsets(new OffsetCheckpoint(checkpointFile).read()));\n+                    } else {\n+                        releaseTaskDirLock(id);\n+                    }\n+                } catch (final IOException e) {\n+                    log.warn(String.format(\"Exception caught while trying to read checkpoint for task %s:\", id), e);\n+                    releaseTaskDirLock(id);\n+                }\n             }\n         }\n+\n         return taskOffsetSums;\n     }\n \n     /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n+     * Makes a weak attempt to lock all task directories in the state dir. We are responsible for computing and\n+     * reporting the offset sum for any unassigned tasks we obtain the lock for in the upcoming rebalance. Tasks\n+     * that we locked but didn't own will be released at the end of the rebalance (unless of course we were\n+     * assigned the task as a result of the rebalance). This method should be idempotent.\n      */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n-        final File[] stateDirs = stateDirectory.listTaskDirectories();\n-        if (stateDirs != null) {\n-            for (final File dir : stateDirs) {\n+    private void tryToLockAllTaskDirectories() {\n+        for (final File dir : stateDirectory.listTaskDirectories()) {\n+            try {\n+                final TaskId id = TaskId.parse(dir.getName());\n                 try {\n-                    final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    if (stateDirectory.lock(id)) {\n+                        lockedTaskDirectories.add(id);\n+                        if (!tasks.containsKey(id)) {\n+                            log.debug(\"Temporarily locked unassigned task {} for the upcoming rebalance\", id);\n+                        }\n                     }\n-                } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                } catch (final IOException e) {\n+                    // if for any reason we can't lock this task dir, just move on\n+                    log.warn(String.format(\"Exception caught while attempting to lock task %s:\", id), e);\n+                }\n+            } catch (final TaskIdFormatException e) {\n+                // ignore any unknown files that sit in the same directory\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempts to release the lock for the passed in task's directory. It does not remove the task from\n+     * {@code lockedTaskDirectories} so it's safe to call during iteration, and should be idempotent.\n+     */\n+    private RuntimeException releaseTaskDirLock(final TaskId taskId) {\n+        try {\n+            stateDirectory.unlock(taskId);\n+        } catch (final IOException e) {\n+            log.warn(String.format(\"Caught the following exception while trying to unlock task %s\", taskId), e);\n+            return new StreamsException(String.format(\"Unable to unlock task directory %s\", taskId), e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * We must release the lock for any unassigned tasks that we temporarily locked in preparation for a\n+     * rebalance in {@link #tryToLockAllTaskDirectories()}.\n+     */\n+    private void releaseLockedUnassignedTaskDirectories() {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        final Iterator<TaskId> taskIdIterator = lockedTaskDirectories.iterator();\n+        while (taskIdIterator.hasNext()) {\n+            final TaskId id = taskIdIterator.next();\n+\n+            if (!tasks.containsKey(id)) {\n+                final RuntimeException unlockException = releaseTaskDirLock(id);\n+                if (unlockException == null) {\n+                    taskIdIterator.remove();\n+                } else {\n+                    log.error(\"Failed to release the lock for task directory {}.\", id);\n+                    firstException.compareAndSet(null, unlockException);\n+                }\n+            }\n+        }\n+\n+        final RuntimeException fatalException = firstException.get();\n+        if (fatalException != null) {\n+            throw fatalException;\n+        }\n+    }\n+\n+    private long sumOfChangelogOffsets(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0L;\n+        for (final Map.Entry<TopicPartition, Long> changelogEntry : changelogOffsets.entrySet()) {\n+            final TopicPartition changelog = changelogEntry.getKey();\n+            final long offset = changelogEntry.getValue();\n+\n+            if (offset < 0L) {\n+                if (offset == -1L) {\n+                    log.debug(\"Skipping unknown offset for changelog {}\", changelog);\n+                } else {\n+                    log.warn(\"Unexpected negative offset {} for changelog {}\", offset, changelog);\n                 }\n+                continue;\n+            } else if (offsetSum + offset < 0) {\n+                return Long.MAX_VALUE;\n             }\n+\n+            offsetSum += offset;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg5NzI5OQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391897299", "bodyText": "That's what I did originally, Bruno suggested changing it to use else -- but, maybe I misunderstood his actual proposal...I'll set it back", "author": "ableegoldman", "createdAt": "2020-03-12T21:14:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgzMDAwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjI2ODczOQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r392268739", "bodyText": "My original proposal was\n            if (offset < 0L) {\n                if (offset == -1L) {\n                    log.debug(\"Skipping unknown offset for changelog {}\", changelog);\n                } else {\n                    log.warn(\"Unexpected negative offset {} for changelog {}\", offset, changelog);\n                }\n            } else {\n                offsetSum += offset;\n\n                if (offsetSum < 0) {\n                    log.warn(\"Sum of changelog offsets for task {} overflowed, pinning to Long.MAX_VALUE\", id);\n                    return Long.MAX_VALUE;\n                }\n            }\n\nI find this easier to read.", "author": "cadonna", "createdAt": "2020-03-13T14:39:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgzMDAwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM4MzAyMQ==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r392383021", "bodyText": "I see, I think I find it easier to read without the else as it makes it clear that we are just adding the offset, except in these two potential edge cases (overflow and negative). But we still need to come to a consensus about how to handle the negative case anyway", "author": "ableegoldman", "createdAt": "2020-03-13T17:52:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgzMDAwNQ=="}], "type": "inlineReview"}, {"oid": "dd0366a1866541b4ee2b1a15c68aad7d5c222095", "url": "https://github.com/apache/kafka/commit/dd0366a1866541b4ee2b1a15c68aad7d5c222095", "message": "report offset sums", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "9f995fd93ee45180494892fb607617c920a7e1e0", "url": "https://github.com/apache/kafka/commit/9f995fd93ee45180494892fb607617c920a7e1e0", "message": "add test case", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "9ff3dfce8cea9e8c29e8f3136f848b852b2e05d7", "url": "https://github.com/apache/kafka/commit/9ff3dfce8cea9e8c29e8f3136f848b852b2e05d7", "message": "write checkpoint for unowned task", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "fc9b9433a200330cc0ab7231da78d35dba5e987f", "url": "https://github.com/apache/kafka/commit/fc9b9433a200330cc0ab7231da78d35dba5e987f", "message": "checkstyle", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "b10999a8b5ea381a6e4d20348c01407d0e4d7f12", "url": "https://github.com/apache/kafka/commit/b10999a8b5ea381a6e4d20348c01407d0e4d7f12", "message": "fix spelling", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "d96d2b9059100c5d953566dbe63f4a03ddfc38b5", "url": "https://github.com/apache/kafka/commit/d96d2b9059100c5d953566dbe63f4a03ddfc38b5", "message": "decode older subscriptions to dummy offset sums", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "e96442d1fd0767d2b84cc4da434515a3c20d0bca", "url": "https://github.com/apache/kafka/commit/e96442d1fd0767d2b84cc4da434515a3c20d0bca", "message": "checkstyle", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "146cff7cb4afa190d8a9feebe36acd955182affc", "url": "https://github.com/apache/kafka/commit/146cff7cb4afa190d8a9feebe36acd955182affc", "message": "github comments", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "801dfa740d664f12bec33f25ea08044e149fab3d", "url": "https://github.com/apache/kafka/commit/801dfa740d664f12bec33f25ea08044e149fab3d", "message": "more changes from github review", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "f3854dbfffd4eb9f0a5b09ef310a0de93dbda14e", "url": "https://github.com/apache/kafka/commit/f3854dbfffd4eb9f0a5b09ef310a0de93dbda14e", "message": "split out lock acquisition andcheckpoint reading/offset sum computation", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "0cf37a4b0f8db98a0c7e6b330a9b7b48ce309eb7", "url": "https://github.com/apache/kafka/commit/0cf37a4b0f8db98a0c7e6b330a9b7b48ce309eb7", "message": "add helper to StateDirectory", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "a6cad4328e56be2d79d2c8386309d36fe073401e", "url": "https://github.com/apache/kafka/commit/a6cad4328e56be2d79d2c8386309d36fe073401e", "message": "annoying but better tests", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "d1adbee9c794e9f728c8bf1a6a7e60f33212912b", "url": "https://github.com/apache/kafka/commit/d1adbee9c794e9f728c8bf1a6a7e60f33212912b", "message": "remove original monolith test", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "1eb7c0c57a6a1d71b6668dc32c6bba5afb9f6ca4", "url": "https://github.com/apache/kafka/commit/1eb7c0c57a6a1d71b6668dc32c6bba5afb9f6ca4", "message": "fix NPE", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "2fa42cb082fe3cd7bbb660036bdba8f9a184c07f", "url": "https://github.com/apache/kafka/commit/2fa42cb082fe3cd7bbb660036bdba8f9a184c07f", "message": "fix no-valid-checkpoint test", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "042b12b2a38b5c5ec2130f2ba983165ffaf6a2a5", "url": "https://github.com/apache/kafka/commit/042b12b2a38b5c5ec2130f2ba983165ffaf6a2a5", "message": "githubreview", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "d4b30920c00f675e862e1fc4ce510372d4334842", "url": "https://github.com/apache/kafka/commit/d4b30920c00f675e862e1fc4ce510372d4334842", "message": "checkstyle", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "cf64c338e070f42779751344f3d34800fafe9b96", "url": "https://github.com/apache/kafka/commit/cf64c338e070f42779751344f3d34800fafe9b96", "message": "log warning on overflow", "committedDate": "2020-03-12T21:16:34Z", "type": "commit"}, {"oid": "35675a7e494cdc1b06ad756d63be1262903ec444", "url": "https://github.com/apache/kafka/commit/35675a7e494cdc1b06ad756d63be1262903ec444", "message": "fix active task creator test", "committedDate": "2020-03-12T21:22:35Z", "type": "commit"}, {"oid": "35675a7e494cdc1b06ad756d63be1262903ec444", "url": "https://github.com/apache/kafka/commit/35675a7e494cdc1b06ad756d63be1262903ec444", "message": "fix active task creator test", "committedDate": "2020-03-12T21:22:35Z", "type": "forcePushed"}, {"oid": "298f1e23a3116e537f235f7ccb5bf0904c816f32", "url": "https://github.com/apache/kafka/commit/298f1e23a3116e537f235f7ccb5bf0904c816f32", "message": "don't attempt unlock for empty or checkpoint-less task dirs", "committedDate": "2020-03-12T22:24:27Z", "type": "commit"}, {"oid": "3135d653f8c7f1148381e25cc9dc423be195c7b4", "url": "https://github.com/apache/kafka/commit/3135d653f8c7f1148381e25cc9dc423be195c7b4", "message": "remove negative offset handling to enforce in separate PR", "committedDate": "2020-03-13T19:32:31Z", "type": "commit"}, {"oid": "a757918bbbf7c4b27aa29720e540a5603f890b1e", "url": "https://github.com/apache/kafka/commit/a757918bbbf7c4b27aa29720e540a5603f890b1e", "message": "fix tests", "committedDate": "2020-03-13T22:09:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY5NjEwNw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r392696107", "bodyText": "One last thing: Could you open another PR to add unit tests that check that the array is empty for the two edge cases?", "author": "cadonna", "createdAt": "2020-03-15T17:46:04Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java", "diffHunk": "@@ -347,8 +349,14 @@ private synchronized void cleanRemovedTasks(final long cleanupDelayMs,\n      * @return The list of all the existing local directories for stream tasks\n      */\n     File[] listTaskDirectories() {", "originalCommit": "a757918bbbf7c4b27aa29720e540a5603f890b1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzI0NzI5Ng==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r393247296", "bodyText": "Ack, added the tests to the Pt 2.5 PR", "author": "ableegoldman", "createdAt": "2020-03-16T18:58:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY5NjEwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzMyNzAzMw==", "url": "https://github.com/apache/kafka/pull/8246#discussion_r393327033", "bodyText": "Actually, ended up doing some additional cleanup on the side so I split it out into a  small PR; please give this a quick review.\n#8304", "author": "ableegoldman", "createdAt": "2020-03-16T21:44:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY5NjEwNw=="}], "type": "inlineReview"}]}