{"pr_number": 8060, "pr_title": "KAFKA-9274: Gracefully handle timeout exception", "pr_createdAt": "2020-02-07T18:32:39Z", "pr_url": "https://github.com/apache/kafka/pull/8060", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjgzOTI1Mg==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r376839252", "bodyText": "If we change the semantic, we should update the class JavaDocs accordingly -- also, do we need to state that \"It means all tasks belonging to this thread have been migrated\" ?", "author": "mjsax", "createdAt": "2020-02-10T01:39:49Z", "path": "streams/src/main/java/org/apache/kafka/streams/errors/TaskMigratedException.java", "diffHunk": "@@ -32,43 +29,7 @@\n \n     private final static long serialVersionUID = 1L;\n \n-    private final TaskId taskId;\n-\n-    public TaskMigratedException(final TaskId taskId,\n-                                 final TopicPartition topicPartition,\n-                                 final long endOffset,\n-                                 final long pos) {\n-        this(taskId, String.format(\"Log end offset of %s should not change while restoring: old end offset %d, current offset %d\",\n-            topicPartition,\n-            endOffset,\n-            pos), null);\n-    }\n-\n-    public TaskMigratedException(final TaskId taskId) {\n-        this(taskId, String.format(\"Task %s is unexpectedly closed during processing\", taskId), null);\n-    }\n-\n-    public TaskMigratedException(final TaskId taskId,\n-                                 final Throwable throwable) {\n-        this(taskId, String.format(\"Client request for task %s has been fenced due to a rebalance\", taskId), throwable);\n-    }\n-\n-    public TaskMigratedException(final TaskId taskId,\n-                                 final String message,\n-                                 final Throwable throwable) {\n-        super(message, throwable);\n-        this.taskId = taskId;\n-    }\n-\n     public TaskMigratedException(final String message, final Throwable throwable) {\n-        this(null, message + \" It means all tasks belonging to this thread have been migrated\", throwable);\n-    }\n-\n-    public TaskId migratedTaskId() {\n-        return taskId;\n-    }\n-\n-    public TaskMigratedException() {\n-        this(null, \"A task has been migrated unexpectedly\", null);\n+        super(message + \"; It means all tasks belonging to this thread have been migrated\", throwable);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjgzOTQ0NQ==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r376839445", "bodyText": "Why do we need to clear the store now but not before?", "author": "mjsax", "createdAt": "2020-02-10T01:41:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -415,6 +415,8 @@ public void close() throws ProcessorStateException {\n                     log.error(\"Failed to close state store {}: \", store.name(), exception);\n                 }\n             }\n+\n+            stores.clear();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk5NzAzNw==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r377997037", "bodyText": "Because before we never \"revive\" a task: once it's closed it's dead; but now it is possible and we may re-initialize the stores and if we do not clear it would cause an illegal-state exception.", "author": "guozhangwang", "createdAt": "2020-02-12T01:34:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjgzOTQ0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MDE3OQ==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r376840179", "bodyText": "Well -- we could also \"buffer\" the record and try to send it later? In the mean time we would need to pause the corresponding task though to not process more input records (or course, we would need to let the task finish processing the current input record what might lead to more output records that we would need to buffer, too). -- This is just a wild thought and we could also handle this case later if required.", "author": "mjsax", "createdAt": "2020-02-10T01:47:14Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -244,9 +252,16 @@ private void recordSendError(final String topic, final Exception exception, fina\n                             final StreamPartitioner<? super K, ? super V> partitioner) {\n         final Integer partition;\n \n-        // TODO K9113: we need to decide how to handle exceptions from partitionsFor\n         if (partitioner != null) {\n-            final List<PartitionInfo> partitions = producer.partitionsFor(topic);\n+            final List<PartitionInfo> partitions;\n+            try {\n+                partitions = producer.partitionsFor(topic);\n+            } catch (final KafkaException e) {\n+                // here we cannot drop the message on the floor even if it is a transient timeout exception,\n+                // so we treat everything the same as a fatal exception", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk5Nzk3NA==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r377997974", "bodyText": "I thought about the buffering mechanism here, but decided it may not worth since we've not seen timeout from partitionsFor -- it should be quite rare because in most cases the producer already got the partition metadata cached locally. If we found this call timing out become an issue we can revisit the buffering, wdyt?", "author": "guozhangwang", "createdAt": "2020-02-12T01:38:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MDE3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MDQ5OA==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r376840498", "bodyText": "Does the consumer not log this already within assign(...) ?", "author": "mjsax", "createdAt": "2020-02-10T01:49:23Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -679,6 +691,8 @@ private void addChangelogsToRestoreConsumer(final Set<TopicPartition> partitions\n         }\n         assignment.addAll(partitions);\n         restoreConsumer.assign(assignment);\n+\n+        log.debug(\"Added partitions {} to the restore consumer, current assignment is {}\", partitions, assignment);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk5ODIyOA==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r377998228", "bodyText": "Yes but it does not log the \"delta\" :) Joking aside, I found that logging the added / removed partitions are important and making the debugging easier.", "author": "guozhangwang", "createdAt": "2020-02-12T01:39:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MDQ5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MDUxOQ==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r376840519", "bodyText": "Same question as above (this question comes up for more log statements below -- don't add a comment each time)", "author": "mjsax", "createdAt": "2020-02-10T01:49:35Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -690,18 +704,18 @@ private void pauseChangelogsFromRestoreConsumer(final Collection<TopicPartition>\n                 \"does not contain some of the partitions \" + partitions + \" for pausing.\");\n         }\n         restoreConsumer.pause(partitions);\n+\n+        log.debug(\"Paused partitions {} from the restore consumer\", partitions);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk5ODQyNw==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r377998427", "bodyText": "Hmm for pause / resume I think I buy your argument -- I can remove these two from the changelog reader.", "author": "guozhangwang", "createdAt": "2020-02-12T01:39:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MDUxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODU0NDE1OQ==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r378544159", "bodyText": "After some more thoughts I feel it's better to still keep it since in many cases (e.g. in unit test trouble shooting) we usually only enable debugging on sub-packages like o.a.k.streams instead of everything, so we cannot always rely on the embedded client's log4j entries.", "author": "guozhangwang", "createdAt": "2020-02-12T22:14:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MDUxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MTUyMw==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r376841523", "bodyText": "nit: we should use ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG instead of a hard-coded string -- in case we ever deprecate the config we would figure out that we might want to update this log statement", "author": "mjsax", "createdAt": "2020-02-10T01:55:44Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -597,6 +600,11 @@ private void initializeMetadata() {\n                 .filter(e -> e.getValue() != null)\n                 .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n             initializeTaskTime(offsetsAndMetadata);\n+        } catch (final TimeoutException e) {\n+            log.warn(\"Encountered {} while trying to fetch committed offsets, will retry initializing the metadata in the next loop.\" +\n+                \"\\nConsider overwriting consumer config `default.api.timeout.ms` to a larger value to avoid timeout errors\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk5ODQ5OA==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r377998498", "bodyText": "Sounds good!", "author": "guozhangwang", "createdAt": "2020-02-12T01:40:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MTUyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MTY1OA==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r376841658", "bodyText": "Why do we throw TimeoutException directly but not wrap it?", "author": "mjsax", "createdAt": "2020-02-10T01:56:41Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -597,6 +600,11 @@ private void initializeMetadata() {\n                 .filter(e -> e.getValue() != null)\n                 .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n             initializeTaskTime(offsetsAndMetadata);\n+        } catch (final TimeoutException e) {\n+            log.warn(\"Encountered {} while trying to fetch committed offsets, will retry initializing the metadata in the next loop.\" +\n+                \"\\nConsider overwriting consumer config `default.api.timeout.ms` to a larger value to avoid timeout errors\");\n+\n+            throw e;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk5OTMyMA==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r377999320", "bodyText": "Because on the caller TaskManager we would swallow TimeoutException anyways.", "author": "guozhangwang", "createdAt": "2020-02-12T01:43:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MTY1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MTg1Mw==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r376841853", "bodyText": "To what extent do we skip? Seems the method just executes as always?", "author": "mjsax", "createdAt": "2020-02-10T01:58:04Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -692,9 +700,10 @@ private void closeRecordCollector(final boolean clean) {\n     @Override\n     public void addRecords(final TopicPartition partition, final Iterable<ConsumerRecord<byte[], byte[]>> records) {\n         if (state() == State.CLOSED || state() == State.CLOSING) {\n-            log.info(\"Stream task {} is already closed, probably because it got unexpectedly migrated to another thread already. \" +\n-                         \"Notifying the thread to trigger a new rebalance immediately.\", id());\n-            throw new TaskMigratedException(id());\n+            // a task is only closing / closed when 1) task manager is closing, 2) a rebalance is undergoing;\n+            // in either case we can just log it and move on without notifying the thread since the consumer\n+            // would soon be updated to not return any records for this task anymore.\n+            log.info(\"Stream task {} is already in {} state, skip adding records to it.\", id(), state());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk5OTUwMQ==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r377999501", "bodyText": "We skip adding records to it. After reviewed your PR I think I would refactor this logic a bit more, stay tuned.", "author": "guozhangwang", "createdAt": "2020-02-12T01:44:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MTg1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODU0NTUzOA==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r378545538", "bodyText": "I moved this logic from addRecords to isProcessible so we would still add records for closing tasks but would skip processing them.", "author": "guozhangwang", "createdAt": "2020-02-12T22:18:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MTg1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MjA0Mg==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r376842042", "bodyText": "Do we need to distinguish StreamsException and KafkaException (StreamsException is a KafkaException and both are fatal)`?\nActually similar question about KafkaException and Exception? The different error messages don't seems to provide much value?", "author": "mjsax", "createdAt": "2020-02-10T01:59:17Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -718,14 +719,20 @@ public void run() {\n         try {\n             runLoop();\n             cleanRun = true;\n+        } catch (final StreamsException e) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAwMDI1OQ==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r378000259", "bodyText": "Yeah that's a good question.. My original plan is that we should not expect KafkaException to be thrown here since we already wrap all of them as StreamsException so if there's any thrown it may be a bug. But since either case we re-throw it anyways now I'm not feeling so strong (I've also thinking if we just do not re-throw exception if StreamsException since it is expected, but that is a behavior change since user's registered handler would not trigger then). I can just collapse all of them into one capture and just log the actual exception as well, wdyt?", "author": "guozhangwang", "createdAt": "2020-02-12T01:47:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MjA0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MjQ3MQ==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r376842471", "bodyText": "Should we keep this check and add final else that throws an IllegalStateException instead?", "author": "mjsax", "createdAt": "2020-02-10T02:01:38Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -936,7 +950,7 @@ private void resetInvalidOffsets(final InvalidOffsetException e) {\n \n                 if (originalReset.equals(\"earliest\")) {\n                     addToResetList(partition, seekToBeginning, \"No custom setting defined for topic '{}' using original config '{}' for offset reset\", \"earliest\", loggedTopics);\n-                } else if (originalReset.equals(\"latest\")) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAwMDU2Mw==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r378000563", "bodyText": "Yeah that sounds better :)", "author": "guozhangwang", "createdAt": "2020-02-12T01:48:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MjQ3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MzA0MA==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r376843040", "bodyText": "Why do we only remove the task if it was active now?", "author": "mjsax", "createdAt": "2020-02-10T02:06:06Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -285,18 +306,13 @@ void handleLostAll() {\n         final Iterator<Task> iterator = tasks.values().iterator();\n         while (iterator.hasNext()) {\n             final Task task = iterator.next();\n-            final Set<TopicPartition> inputPartitions = task.inputPartitions();\n             // Even though we've apparently dropped out of the group, we can continue safely to maintain our\n             // standby tasks while we rejoin.\n             if (task.isActive()) {\n+                cleanupTask(task);\n                 task.closeDirty();\n-                changelogReader.remove(task.changelogPartitions());\n-            }\n-\n-            for (final TopicPartition inputPartition : inputPartitions) {\n-                partitionToTask.remove(inputPartition);\n+                iterator.remove();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAwMTA5Mw==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r378001093", "bodyText": "We previously also only remove the task (#8058 (comment)), the original motivation to only remove the task is that standby tasks are likely to go back and even if they do not they can still be closed in the next rebalance's onAssignment call, so we just keep them a bit longer hoping it worth the \"wait\" :)", "author": "guozhangwang", "createdAt": "2020-02-12T01:50:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0MzA0MA=="}], "type": "inlineReview"}, {"oid": "74cfbf1a779b7dd3fb3a1ee9f4277338cd9a66d3", "url": "https://github.com/apache/kafka/commit/74cfbf1a779b7dd3fb3a1ee9f4277338cd9a66d3", "message": "timeout for committed and initTxn", "committedDate": "2020-02-14T21:30:16Z", "type": "commit"}, {"oid": "74cfbf1a779b7dd3fb3a1ee9f4277338cd9a66d3", "url": "https://github.com/apache/kafka/commit/74cfbf1a779b7dd3fb3a1ee9f4277338cd9a66d3", "message": "timeout for committed and initTxn", "committedDate": "2020-02-14T21:30:16Z", "type": "forcePushed"}, {"oid": "19332579cacd46146a8309938215845c16448a8d", "url": "https://github.com/apache/kafka/commit/19332579cacd46146a8309938215845c16448a8d", "message": "improve producer unit test", "committedDate": "2020-02-14T22:46:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTY3OTE2OQ==", "url": "https://github.com/apache/kafka/pull/8060#discussion_r379679169", "bodyText": "This is just to verify that initTransactions can indeed be retried. cc @hachikuji", "author": "guozhangwang", "createdAt": "2020-02-14T22:49:14Z", "path": "clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java", "diffHunk": "@@ -761,7 +762,18 @@ public void testInitTransactionTimeout() {\n \n         try (Producer<String, String> producer = new KafkaProducer<>(configs, new StringSerializer(),\n                 new StringSerializer(), metadata, client, null, time)) {\n+            client.prepareResponse(\n+                request -> request instanceof FindCoordinatorRequest &&\n+                    ((FindCoordinatorRequest) request).data().keyType() == FindCoordinatorRequest.CoordinatorType.TRANSACTION.id(),\n+                FindCoordinatorResponse.prepareResponse(Errors.NONE, host1));\n+\n             assertThrows(TimeoutException.class, producer::initTransactions);\n+\n+            client.respond(FindCoordinatorResponse.prepareResponse(Errors.NONE, host1));\n+            client.prepareResponse(initProducerIdResponse(1L, (short) 5, Errors.NONE));\n+\n+            // retry initialization should work\n+            producer.initTransactions();", "originalCommit": "19332579cacd46146a8309938215845c16448a8d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}