{"pr_number": 8829, "pr_title": "KAFKA-10115: Incorporate errors.tolerance with the Errant Record Reporter", "pr_createdAt": "2020-06-07T22:57:58Z", "pr_url": "https://github.com/apache/kafka/pull/8829", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQxMTIzNQ==", "url": "https://github.com/apache/kafka/pull/8829#discussion_r436411235", "bodyText": "This marks the failure when we get a new error but the previous error already put us over the limit. IOW, if this is the first error that is reported, then totalFailures will be 0 when this method is called and thus the withinToleranceLimits() will return true (i.e., we haven't recorded any errors yet) and we will not enter the if block due to the negation.\nSeems like we should actually do this check after we record the error. That would be something like:\n        markAsFailed();\n        context.consumerRecord(consumerRecord);\n        context.currentContext(stage, executingClass);\n        context.error(error);\n        errorHandlingMetrics.recordError();\n        if (!withinToleranceLimits()) {\n            errorHandlingMetrics.recordFailure();\n            throw new ConnectException(\"Tolerance exceeded in error handler\", error);\n        }\n        return context.report();\n\nNote that I added the markAsFailed() call since that's what increments the totalFailures field (and calls errorHandlingMetrics.recordErrorTimestamp()).\nBTW, I'm not sure whether we should call errorHandlingMetrics.recordError() or errorHandlingMetrics.recordFailure() or both.\nIIUC, then when we get to the if-block on the first error being reported, the markAsFailed() method will have incremented the totalFailures (we were not doing that in this method before this PR), and if errors.tolerance=NONE is used we will fail on the first error -- which is what we want.\nI also think that if we add other error tolerance policies in the future, this logic will work correctly, as long as withinToleranceLimits() is implemented to return false when we should fail rather than report.\nAlso, it'd be great to have unit tests that verify this behavior.", "author": "rhauch", "createdAt": "2020-06-07T23:16:37Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java", "diffHunk": "@@ -87,6 +87,12 @@ public RetryWithToleranceOperator(long errorRetryTimeout, long errorMaxDelayInMi\n     public Future<Void> executeFailed(Stage stage, Class<?> executingClass,\n                                       ConsumerRecord<byte[], byte[]> consumerRecord,\n                                       Throwable error) {\n+        if (!withinToleranceLimits()) {\n+            errorHandlingMetrics.recordFailure();\n+            markAsFailed();\n+            throw new ConnectException(\"Tolerance exceeded in the errant record reporter\", error);\n+        }\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQzNzk4OQ==", "url": "https://github.com/apache/kafka/pull/8829#discussion_r436437989", "bodyText": "Thanks @rhauch for the comments, I agree with what you're saying. I think we should call both errorHandlingMetrics.recordError() and errorHandlingMetrics.recordFailure(). It seems like recordError() and recordFailure() represent basically the same thing (failed operations) and they're both called at some point when execute(...) is called by the RetryWithToleranceOperator, albeit at different times.\nAdditionally, it looks as if in other circumstances, the error is reported to the various reporters even if the error tolerance has been exceeded, so I'll adjust the order of operations accordingly.", "author": "aakashnshah", "createdAt": "2020-06-08T03:08:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQxMTIzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjg0MzcxMw==", "url": "https://github.com/apache/kafka/pull/8829#discussion_r436843713", "bodyText": "if we attempt an operation and it fails, recordFailure will be incremented, but recordError only tracks the cases where the when we encounter a problem that the framework cannot retry or skip. In the first case, we may still be able to retry or skip the record. In the executeFailed scenario, we should recordFailure() every time, and only recordError only when we have to fail the task.", "author": "wicknicks", "createdAt": "2020-06-08T16:39:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQxMTIzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjg0NTA4Ng==", "url": "https://github.com/apache/kafka/pull/8829#discussion_r436845086", "bodyText": "Since this is called from the task(), is it enough to just raise an exception? that may be swallowed by the task, and could continue processing.", "author": "wicknicks", "createdAt": "2020-06-08T16:41:49Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java", "diffHunk": "@@ -87,6 +87,12 @@ public RetryWithToleranceOperator(long errorRetryTimeout, long errorMaxDelayInMi\n     public Future<Void> executeFailed(Stage stage, Class<?> executingClass,\n                                       ConsumerRecord<byte[], byte[]> consumerRecord,\n                                       Throwable error) {\n+        if (!withinToleranceLimits()) {\n+            errorHandlingMetrics.recordFailure();\n+            markAsFailed();\n+            throw new ConnectException(\"Tolerance exceeded in the errant record reporter\", error);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c2867f70bc491f87329736487aaf68663c50f4a5", "url": "https://github.com/apache/kafka/commit/c2867f70bc491f87329736487aaf68663c50f4a5", "message": "KAFKA-10115: Incorporate errors.tolerance with the Errant Record Reporter", "committedDate": "2020-06-08T21:14:11Z", "type": "commit"}, {"oid": "0ce4daf669d95320c9ab096b7dbc207786aaeeb8", "url": "https://github.com/apache/kafka/commit/0ce4daf669d95320c9ab096b7dbc207786aaeeb8", "message": "addressed comments", "committedDate": "2020-06-10T22:21:21Z", "type": "commit"}, {"oid": "0ce4daf669d95320c9ab096b7dbc207786aaeeb8", "url": "https://github.com/apache/kafka/commit/0ce4daf669d95320c9ab096b7dbc207786aaeeb8", "message": "addressed comments", "committedDate": "2020-06-10T22:21:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ1MjE1MQ==", "url": "https://github.com/apache/kafka/pull/8829#discussion_r438452151", "bodyText": "instead, you can just check:\n        if (retryWithToleranceOperator.failed()) {\n            throw retryWithToleranceOperator.error();\n        }\n\nbecause we are already storing the error in the processing context. you can expose that through the operator.", "author": "wicknicks", "createdAt": "2020-06-10T22:58:33Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -556,6 +556,9 @@ private void deliverMessages() {\n             log.trace(\"{} Delivering batch of {} messages to task\", this, messageBatch.size());\n             long start = time.milliseconds();\n             task.put(new ArrayList<>(messageBatch));\n+            if (workerErrantRecordReporter != null && workerErrantRecordReporter.mustThrowException()) {\n+                throw workerErrantRecordReporter.getExceptionToThrow();\n+            }", "originalCommit": "0ce4daf669d95320c9ab096b7dbc207786aaeeb8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ1MjY5Ng==", "url": "https://github.com/apache/kafka/pull/8829#discussion_r438452696", "bodyText": "we don't need these vars, the errors are already stored in the ProcessingContext. look at comment above.", "author": "wicknicks", "createdAt": "2020-06-10T23:00:11Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -99,8 +102,15 @@ public WorkerErrantRecordReporter(\n                 valLength, key, value, headers);\n         }\n \n-        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n-            SinkTask.class, consumerRecord, error);\n+        Future<Void> future;\n+        try {\n+            future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n+                SinkTask.class, consumerRecord, error);\n+        } catch (ConnectException e) {\n+            mustThrowException = true;\n+            exceptionToThrow = e;\n+            throw e;\n+        }", "originalCommit": "0ce4daf669d95320c9ab096b7dbc207786aaeeb8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "603217f1ed0e68295b5a6cfd0bc2fd09ba9f681d", "url": "https://github.com/apache/kafka/commit/603217f1ed0e68295b5a6cfd0bc2fd09ba9f681d", "message": "addressed comments", "committedDate": "2020-06-10T23:55:34Z", "type": "commit"}, {"oid": "aefe73ffc3e6173a2427bd3dfb957d776d4088fd", "url": "https://github.com/apache/kafka/commit/aefe73ffc3e6173a2427bd3dfb957d776d4088fd", "message": "moved errant record reporter test", "committedDate": "2020-06-11T00:20:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ3NTM1NQ==", "url": "https://github.com/apache/kafka/pull/8829#discussion_r438475355", "bodyText": "minor: we should move this test to ErrorHandlingIntegrationTest. this class was meant to be an example of how to do integration tests.", "author": "wicknicks", "createdAt": "2020-06-11T00:17:22Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java", "diffHunk": "@@ -237,6 +239,7 @@ public void testErrantRecordReporter() throws Exception {\n         props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n         props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n         props.put(DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n+        props.put(ERRORS_TOLERANCE_CONFIG, ToleranceType.ALL.value());", "originalCommit": "603217f1ed0e68295b5a6cfd0bc2fd09ba9f681d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ3NjcyMg==", "url": "https://github.com/apache/kafka/pull/8829#discussion_r438476722", "bodyText": "let's add a small comment saying why we need to do this: specifically, that if the errors raised from the operator were swallowed by the task implementation, then here we need to kill the task, and if they were not swallowed, we would not get here.", "author": "wicknicks", "createdAt": "2020-06-11T00:22:32Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -556,6 +556,10 @@ private void deliverMessages() {\n             log.trace(\"{} Delivering batch of {} messages to task\", this, messageBatch.size());\n             long start = time.milliseconds();\n             task.put(new ArrayList<>(messageBatch));\n+            if (retryWithToleranceOperator.failed() && !retryWithToleranceOperator.withinToleranceLimits()) {", "originalCommit": "603217f1ed0e68295b5a6cfd0bc2fd09ba9f681d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c3324cce90f8379c69c8538b594edd29e83f7fe1", "url": "https://github.com/apache/kafka/commit/c3324cce90f8379c69c8538b594edd29e83f7fe1", "message": "more comments", "committedDate": "2020-06-11T00:44:15Z", "type": "commit"}]}