{"pr_number": 8088, "pr_title": "KAFKA-9535: Update metadata upon retrying partitions for ListOffset", "pr_createdAt": "2020-02-11T06:51:28Z", "pr_url": "https://github.com/apache/kafka/pull/8088", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzY1MjA4Mw==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377652083", "bodyText": "Nit: would a switch be more reasonable than the huge if/else?", "author": "ijuma", "createdAt": "2020-02-11T14:02:53Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -1018,15 +1018,16 @@ private void handleListOffsetResponse(Map<TopicPartition, ListOffsetRequest.Part\n                        error == Errors.REPLICA_NOT_AVAILABLE ||\n                        error == Errors.KAFKA_STORAGE_ERROR ||\n                        error == Errors.OFFSET_NOT_AVAILABLE ||\n-                       error == Errors.LEADER_NOT_AVAILABLE) {\n-                log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n-                        topicPartition, error);\n-                partitionsToRetry.add(topicPartition);\n-            } else if (error == Errors.FENCED_LEADER_EPOCH ||\n+                       error == Errors.LEADER_NOT_AVAILABLE ||\n                        error == Errors.UNKNOWN_LEADER_EPOCH) {\n                 log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n                         topicPartition, error);\n                 partitionsToRetry.add(topicPartition);\n+            } else if (error == Errors.FENCED_LEADER_EPOCH) {\n+                log.debug(\"Attempt to fetch offsets for partition {} failed due to fenced leader epoch, refresh \" +\n+                              \"the metadata and retrying.\", topicPartition);\n+                metadata.requestUpdate();\n+                partitionsToRetry.add(topicPartition);\n             } else if (error == Errors.UNKNOWN_TOPIC_OR_PARTITION) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzczNjMzOQ==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377736339", "bodyText": "Haven't considered that, but these huge if/else are sort of everywhere, probably make sense to refactor them all in one diff if people agree.", "author": "abbccdda", "createdAt": "2020-02-11T16:10:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzY1MjA4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc4MTUxMg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377781512", "bodyText": "There are other cases where we would need to refetch metadata. For example, LEADER_NOT_AVAILABLE. Another way we could handle this is to request an update any time we have partitions to retry.", "author": "hachikuji", "createdAt": "2020-02-11T17:21:45Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -1018,15 +1018,16 @@ private void handleListOffsetResponse(Map<TopicPartition, ListOffsetRequest.Part\n                        error == Errors.REPLICA_NOT_AVAILABLE ||\n                        error == Errors.KAFKA_STORAGE_ERROR ||\n                        error == Errors.OFFSET_NOT_AVAILABLE ||\n-                       error == Errors.LEADER_NOT_AVAILABLE) {\n-                log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n-                        topicPartition, error);\n-                partitionsToRetry.add(topicPartition);\n-            } else if (error == Errors.FENCED_LEADER_EPOCH ||\n+                       error == Errors.LEADER_NOT_AVAILABLE ||\n                        error == Errors.UNKNOWN_LEADER_EPOCH) {\n                 log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n                         topicPartition, error);\n                 partitionsToRetry.add(topicPartition);\n+            } else if (error == Errors.FENCED_LEADER_EPOCH) {\n+                log.debug(\"Attempt to fetch offsets for partition {} failed due to fenced leader epoch, refresh \" +\n+                              \"the metadata and retrying.\", topicPartition);\n+                metadata.requestUpdate();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzgwOTkwNw==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377809907", "bodyText": "I have to follow @ijuma suggestion to convert the check into a case switch, as the checkstyle won't let me merge 7 boolean statements XD", "author": "abbccdda", "createdAt": "2020-02-11T18:14:49Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -979,62 +979,65 @@ private void handleListOffsetResponse(Map<TopicPartition, ListOffsetRequest.Part\n             TopicPartition topicPartition = entry.getKey();\n             ListOffsetResponse.PartitionData partitionData = listOffsetResponse.responseData().get(topicPartition);\n             Errors error = partitionData.error;\n-            if (error == Errors.NONE) {\n-                if (partitionData.offsets != null) {\n-                    // Handle v0 response\n-                    long offset;\n-                    if (partitionData.offsets.size() > 1) {\n-                        future.raise(new IllegalStateException(\"Unexpected partitionData response of length \" +\n-                                partitionData.offsets.size()));\n-                        return;\n-                    } else if (partitionData.offsets.isEmpty()) {\n-                        offset = ListOffsetResponse.UNKNOWN_OFFSET;\n-                    } else {\n-                        offset = partitionData.offsets.get(0);\n-                    }\n-                    log.debug(\"Handling v0 ListOffsetResponse response for {}. Fetched offset {}\",\n+            switch (error) {\n+                case NONE:\n+                    if (partitionData.offsets != null) {\n+                        // Handle v0 response\n+                        long offset;\n+                        if (partitionData.offsets.size() > 1) {\n+                            future.raise(new IllegalStateException(\"Unexpected partitionData response of length \" +\n+                                                                       partitionData.offsets.size()));\n+                            return;\n+                        } else if (partitionData.offsets.isEmpty()) {\n+                            offset = ListOffsetResponse.UNKNOWN_OFFSET;\n+                        } else {\n+                            offset = partitionData.offsets.get(0);\n+                        }\n+                        log.debug(\"Handling v0 ListOffsetResponse response for {}. Fetched offset {}\",\n                             topicPartition, offset);\n-                    if (offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n-                        ListOffsetData offsetData = new ListOffsetData(offset, null, Optional.empty());\n-                        fetchedOffsets.put(topicPartition, offsetData);\n-                    }\n-                } else {\n-                    // Handle v1 and later response\n-                    log.debug(\"Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}\",\n+                        if (offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n+                            ListOffsetData offsetData = new ListOffsetData(offset, null, Optional.empty());\n+                            fetchedOffsets.put(topicPartition, offsetData);\n+                        }\n+                    } else {\n+                        // Handle v1 and later response\n+                        log.debug(\"Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}\",\n                             topicPartition, partitionData.offset, partitionData.timestamp);\n-                    if (partitionData.offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n-                        ListOffsetData offsetData = new ListOffsetData(partitionData.offset, partitionData.timestamp,\n+                        if (partitionData.offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n+                            ListOffsetData offsetData = new ListOffsetData(partitionData.offset, partitionData.timestamp,\n                                 partitionData.leaderEpoch);\n-                        fetchedOffsets.put(topicPartition, offsetData);\n+                            fetchedOffsets.put(topicPartition, offsetData);\n+                        }\n                     }\n-                }\n-            } else if (error == Errors.UNSUPPORTED_FOR_MESSAGE_FORMAT) {\n-                // The message format on the broker side is before 0.10.0, which means it does not\n-                // support timestamps. We treat this case the same as if we weren't able to find an\n-                // offset corresponding to the requested timestamp and leave it out of the result.\n-                log.debug(\"Cannot search by timestamp for partition {} because the message format version \" +\n-                        \"is before 0.10.0\", topicPartition);\n-            } else if (error == Errors.NOT_LEADER_FOR_PARTITION ||\n-                       error == Errors.REPLICA_NOT_AVAILABLE ||\n-                       error == Errors.KAFKA_STORAGE_ERROR ||\n-                       error == Errors.OFFSET_NOT_AVAILABLE ||\n-                       error == Errors.LEADER_NOT_AVAILABLE) {\n-                log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n+                    break;\n+                case UNSUPPORTED_FOR_MESSAGE_FORMAT:\n+                    // The message format on the broker side is before 0.10.0, which means it does not\n+                    // support timestamps. We treat this case the same as if we weren't able to find an\n+                    // offset corresponding to the requested timestamp and leave it out of the result.\n+                    log.debug(\"Cannot search by timestamp for partition {} because the message format version \" +\n+                                  \"is before 0.10.0\", topicPartition);\n+                    break;\n+                case NOT_LEADER_FOR_PARTITION:", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzgyMzQ0Nw==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377823447", "bodyText": "This is the new test being added, other changes are just side cleanup.", "author": "abbccdda", "createdAt": "2020-02-11T18:40:13Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2414,28 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg5ODU3MA==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377898570", "bodyText": "We can consolidate this with the logic below. Something like this:\nif (remainingToSearch.isEmpty()) {\n  return result;\n} else {\n  client.awaitMetadataUpdate(timer);\n}", "author": "hachikuji", "createdAt": "2020-02-11T21:07:09Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -536,18 +536,18 @@ private ListOffsetResult fetchOffsetsByTimes(Map<TopicPartition, Long> timestamp\n             RequestFuture<ListOffsetResult> future = sendListOffsetsRequests(remainingToSearch, requireTimestamps);\n             client.poll(future, timer);\n \n-            if (!future.isDone())\n+            if (!future.isDone()) {\n                 break;\n-\n-            if (future.succeeded()) {\n+            } else if (future.succeeded()) {\n                 ListOffsetResult value = future.value();\n                 result.fetchedOffsets.putAll(value.fetchedOffsets);\n-                if (value.partitionsToRetry.isEmpty())\n-                    return result;\n-\n                 remainingToSearch.keySet().retainAll(value.partitionsToRetry);\n             } else if (!future.isRetriable()) {\n                 throw future.exception();\n+            }\n+\n+            if (remainingToSearch.isEmpty()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkyNDk5NQ==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377924995", "bodyText": "Since we're refactoring from if/else to a switch/case as well as changing the logic, it would be useful to include the before/after of errors that cause retry in the PR description", "author": "mumrah", "createdAt": "2020-02-11T21:59:07Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -979,62 +974,66 @@ private void handleListOffsetResponse(Map<TopicPartition, ListOffsetRequest.Part\n             TopicPartition topicPartition = entry.getKey();\n             ListOffsetResponse.PartitionData partitionData = listOffsetResponse.responseData().get(topicPartition);\n             Errors error = partitionData.error;\n-            if (error == Errors.NONE) {\n-                if (partitionData.offsets != null) {\n-                    // Handle v0 response\n-                    long offset;\n-                    if (partitionData.offsets.size() > 1) {\n-                        future.raise(new IllegalStateException(\"Unexpected partitionData response of length \" +\n-                                partitionData.offsets.size()));\n-                        return;\n-                    } else if (partitionData.offsets.isEmpty()) {\n-                        offset = ListOffsetResponse.UNKNOWN_OFFSET;\n-                    } else {\n-                        offset = partitionData.offsets.get(0);\n-                    }\n-                    log.debug(\"Handling v0 ListOffsetResponse response for {}. Fetched offset {}\",\n+            switch (error) {\n+                case NONE:\n+                    if (partitionData.offsets != null) {\n+                        // Handle v0 response\n+                        long offset;\n+                        if (partitionData.offsets.size() > 1) {\n+                            future.raise(new IllegalStateException(\"Unexpected partitionData response of length \" +\n+                                                                       partitionData.offsets.size()));\n+                            return;\n+                        } else if (partitionData.offsets.isEmpty()) {\n+                            offset = ListOffsetResponse.UNKNOWN_OFFSET;\n+                        } else {\n+                            offset = partitionData.offsets.get(0);\n+                        }\n+                        log.debug(\"Handling v0 ListOffsetResponse response for {}. Fetched offset {}\",\n                             topicPartition, offset);\n-                    if (offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n-                        ListOffsetData offsetData = new ListOffsetData(offset, null, Optional.empty());\n-                        fetchedOffsets.put(topicPartition, offsetData);\n-                    }\n-                } else {\n-                    // Handle v1 and later response\n-                    log.debug(\"Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}\",\n+                        if (offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n+                            ListOffsetData offsetData = new ListOffsetData(offset, null, Optional.empty());\n+                            fetchedOffsets.put(topicPartition, offsetData);\n+                        }\n+                    } else {\n+                        // Handle v1 and later response\n+                        log.debug(\"Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}\",\n                             topicPartition, partitionData.offset, partitionData.timestamp);\n-                    if (partitionData.offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n-                        ListOffsetData offsetData = new ListOffsetData(partitionData.offset, partitionData.timestamp,\n+                        if (partitionData.offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n+                            ListOffsetData offsetData = new ListOffsetData(partitionData.offset, partitionData.timestamp,\n                                 partitionData.leaderEpoch);\n-                        fetchedOffsets.put(topicPartition, offsetData);\n+                            fetchedOffsets.put(topicPartition, offsetData);\n+                        }\n                     }\n-                }\n-            } else if (error == Errors.UNSUPPORTED_FOR_MESSAGE_FORMAT) {\n-                // The message format on the broker side is before 0.10.0, which means it does not\n-                // support timestamps. We treat this case the same as if we weren't able to find an\n-                // offset corresponding to the requested timestamp and leave it out of the result.\n-                log.debug(\"Cannot search by timestamp for partition {} because the message format version \" +\n-                        \"is before 0.10.0\", topicPartition);\n-            } else if (error == Errors.NOT_LEADER_FOR_PARTITION ||\n-                       error == Errors.REPLICA_NOT_AVAILABLE ||\n-                       error == Errors.KAFKA_STORAGE_ERROR ||\n-                       error == Errors.OFFSET_NOT_AVAILABLE ||\n-                       error == Errors.LEADER_NOT_AVAILABLE) {\n-                log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n-                        topicPartition, error);\n-                partitionsToRetry.add(topicPartition);\n-            } else if (error == Errors.FENCED_LEADER_EPOCH ||\n-                       error == Errors.UNKNOWN_LEADER_EPOCH) {\n-                log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n+                    break;\n+                case UNSUPPORTED_FOR_MESSAGE_FORMAT:\n+                    // The message format on the broker side is before 0.10.0, which means it does not\n+                    // support timestamps. We treat this case the same as if we weren't able to find an\n+                    // offset corresponding to the requested timestamp and leave it out of the result.\n+                    log.debug(\"Cannot search by timestamp for partition {} because the message format version \" +\n+                                  \"is before 0.10.0\", topicPartition);\n+                    break;\n+                case NOT_LEADER_FOR_PARTITION:\n+                case REPLICA_NOT_AVAILABLE:\n+                case KAFKA_STORAGE_ERROR:\n+                case OFFSET_NOT_AVAILABLE:\n+                case LEADER_NOT_AVAILABLE:\n+                case FENCED_LEADER_EPOCH:\n+                case UNKNOWN_LEADER_EPOCH:", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk0MzgwMg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r377943802", "bodyText": "Sure, done!", "author": "abbccdda", "createdAt": "2020-02-11T22:42:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkyNDk5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODQxMDg3MQ==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378410871", "bodyText": "This an awkward way to verify that the metadata gets updated. Nothing in the test actually involves the use of the new metadata. A more realistic scenario would be a leader change. Could we let the metadata update indicate a leader change and then use prepareResponseFrom to ensure that the retry goes to a particular node?", "author": "hachikuji", "createdAt": "2020-02-12T17:45:17Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,34 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        buildFetcher();\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            subscriptions.assignFromUser(singleton(tp0));\n+            client.updateMetadata(initialUpdateResponse);\n+            assertEquals(1, metadata.fetch().nodes().size());\n+\n+            Map<String, Integer> partitionNumByTopic = new HashMap<>();\n+            partitionNumByTopic.put(topicName, 1);\n+            final int updatedNodeSize = 3;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "cc84e6f2a996a743e74d4e904cda4c1b86c4915a", "url": "https://github.com/apache/kafka/commit/cc84e6f2a996a743e74d4e904cda4c1b86c4915a", "message": "update metadata upon FENCED_LEADER_EPOCH", "committedDate": "2020-02-12T21:39:01Z", "type": "commit"}, {"oid": "32553454bb54d9db69030dcce992989ddcf2780c", "url": "https://github.com/apache/kafka/commit/32553454bb54d9db69030dcce992989ddcf2780c", "message": "blindly trigger metadata update for retry partitions", "committedDate": "2020-02-12T21:39:01Z", "type": "commit"}, {"oid": "d731718803825bd8a65833594a26e3b49531ea68", "url": "https://github.com/apache/kafka/commit/d731718803825bd8a65833594a26e3b49531ea68", "message": "side cleanup", "committedDate": "2020-02-12T21:39:01Z", "type": "commit"}, {"oid": "100bfd51ede28e59d3b675a7cf463d39e811f686", "url": "https://github.com/apache/kafka/commit/100bfd51ede28e59d3b675a7cf463d39e811f686", "message": "expand test", "committedDate": "2020-02-12T21:39:01Z", "type": "commit"}, {"oid": "616c245c474ec83acb72e2f5eb92e9e49174c91a", "url": "https://github.com/apache/kafka/commit/616c245c474ec83acb72e2f5eb92e9e49174c91a", "message": "consolidate", "committedDate": "2020-02-12T21:39:01Z", "type": "commit"}, {"oid": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "url": "https://github.com/apache/kafka/commit/7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "message": "add leader change", "committedDate": "2020-02-12T21:39:01Z", "type": "commit"}, {"oid": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "url": "https://github.com/apache/kafka/commit/7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "message": "add leader change", "committedDate": "2020-02-12T21:39:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODczMw==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378688733", "bodyText": "This test seems to pass with the original logic. I'm wondering if we need to let the offset request take two partitions. One of them can succeed and the other can fail due to the provided error so that we are handling the partial failure case.", "author": "hachikuji", "createdAt": "2020-02-13T07:26:18Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,38 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {", "originalCommit": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk5NzU4MA==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378997580", "bodyText": "I double checked, the reason was because the second try will not match the newLeader so that it actually disconnects and ask for a metadata update. I have injected a fatal error if the client reconnects to the original leader.", "author": "abbccdda", "createdAt": "2020-02-13T17:07:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODczMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA2ODcyOA==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379068728", "bodyText": "Ok, maybe we need a separate test for the partial failure case? I am interested in verifying 1) that metadata update gets triggered after a partial failure, and 2) the retry does not request partitions that were fetched successfully.", "author": "hachikuji", "createdAt": "2020-02-13T19:23:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODczMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTEyMTU5NA==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379121594", "bodyText": "For 1), as long as it's partition level error, it's a partial failure. For 2) I could check to see if there is a way.", "author": "abbccdda", "createdAt": "2020-02-13T21:13:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODczMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODk1MQ==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378688951", "bodyText": "Not really sure this has value if the test case expects the leader change correctly.", "author": "hachikuji", "createdAt": "2020-02-13T07:26:55Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,38 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        buildFetcher();\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            subscriptions.assignFromUser(singleton(tp1));\n+            client.updateMetadata(initialUpdateResponse);\n+\n+            Node originalLeader = metadata.fetch().leaderFor(tp1);\n+\n+            final int updatedNodeSize = 3;\n+            MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3, Collections.emptyMap(), singletonMap(topicName, 4), tp -> 3);\n+\n+            client.prepareMetadataUpdate(updatedMetadata);\n+            client.prepareResponseFrom(listOffsetResponse(tp1, retriableError, ListOffsetRequest.LATEST_TIMESTAMP, -1L), originalLeader);\n+\n+            final long timestamp = 1L;\n+            Node newLeader = new Node(1, \"localhost\", 1970);\n+            assertNotEquals(originalLeader, newLeader);\n+\n+            client.prepareResponseFrom(listOffsetResponse(tp1, Errors.NONE, timestamp, 5L), newLeader);\n+            Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestampMap =\n+                fetcher.offsetsForTimes(Collections.singletonMap(tp1, timestamp), time.timer(Integer.MAX_VALUE));\n+\n+            assertEquals(Collections.singletonMap(tp1, new OffsetAndTimestamp(5L, timestamp)), offsetAndTimestampMap);\n+            assertEquals(updatedNodeSize, metadata.fetch().nodes().size());", "originalCommit": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk5ODgxMg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378998812", "bodyText": "Removed", "author": "abbccdda", "createdAt": "2020-02-13T17:09:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODk1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4OTUzMg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378689532", "bodyText": "Why construct this directly? Shouldn't we get it from the metadata update?", "author": "hachikuji", "createdAt": "2020-02-13T07:28:47Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,38 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        buildFetcher();\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            subscriptions.assignFromUser(singleton(tp1));\n+            client.updateMetadata(initialUpdateResponse);\n+\n+            Node originalLeader = metadata.fetch().leaderFor(tp1);\n+\n+            final int updatedNodeSize = 3;\n+            MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3, Collections.emptyMap(), singletonMap(topicName, 4), tp -> 3);\n+\n+            client.prepareMetadataUpdate(updatedMetadata);\n+            client.prepareResponseFrom(listOffsetResponse(tp1, retriableError, ListOffsetRequest.LATEST_TIMESTAMP, -1L), originalLeader);\n+\n+            final long timestamp = 1L;\n+            Node newLeader = new Node(1, \"localhost\", 1970);", "originalCommit": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk5Nzk1Mg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378997952", "bodyText": "Unfortunately the metadata update happens within the offsetsForTimes call, so we could not capture it beforehand.", "author": "abbccdda", "createdAt": "2020-02-13T17:08:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4OTUzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA2NzU1OQ==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379067559", "bodyText": "What I'm saying is that the metadata update contains the node we are looking for. So why do we need to build the Node object directly? e.g. where does 1970 come from?", "author": "hachikuji", "createdAt": "2020-02-13T19:21:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4OTUzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY5MDk0OA==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378690948", "bodyText": "Can we add an assertion like the following to ensure that both requests were sent?\n            assertFalse(client.hasPendingResponses());", "author": "hachikuji", "createdAt": "2020-02-13T07:32:23Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,38 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        buildFetcher();\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            subscriptions.assignFromUser(singleton(tp1));\n+            client.updateMetadata(initialUpdateResponse);\n+\n+            Node originalLeader = metadata.fetch().leaderFor(tp1);\n+\n+            final int updatedNodeSize = 3;\n+            MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3, Collections.emptyMap(), singletonMap(topicName, 4), tp -> 3);\n+\n+            client.prepareMetadataUpdate(updatedMetadata);\n+            client.prepareResponseFrom(listOffsetResponse(tp1, retriableError, ListOffsetRequest.LATEST_TIMESTAMP, -1L), originalLeader);\n+\n+            final long timestamp = 1L;\n+            Node newLeader = new Node(1, \"localhost\", 1970);\n+            assertNotEquals(originalLeader, newLeader);\n+\n+            client.prepareResponseFrom(listOffsetResponse(tp1, Errors.NONE, timestamp, 5L), newLeader);\n+            Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestampMap =\n+                fetcher.offsetsForTimes(Collections.singletonMap(tp1, timestamp), time.timer(Integer.MAX_VALUE));\n+\n+            assertEquals(Collections.singletonMap(tp1, new OffsetAndTimestamp(5L, timestamp)), offsetAndTimestampMap);\n+            assertEquals(updatedNodeSize, metadata.fetch().nodes().size());\n+        }", "originalCommit": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTAwMzIxNg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379003216", "bodyText": "Add check of pending responses.", "author": "abbccdda", "createdAt": "2020-02-13T17:17:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY5MDk0OA=="}], "type": "inlineReview"}, {"oid": "772b1a9c7971b03bdc02df20bc18ff8f07b7f95d", "url": "https://github.com/apache/kafka/commit/772b1a9c7971b03bdc02df20bc18ff8f07b7f95d", "message": "improve test", "committedDate": "2020-02-13T17:20:53Z", "type": "commit"}, {"oid": "772b1a9c7971b03bdc02df20bc18ff8f07b7f95d", "url": "https://github.com/apache/kafka/commit/772b1a9c7971b03bdc02df20bc18ff8f07b7f95d", "message": "improve test", "committedDate": "2020-02-13T17:20:53Z", "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "9fdf4b794c06e8788905bd79b72d3818306f68c9", "url": "https://github.com/apache/kafka/commit/9fdf4b794c06e8788905bd79b72d3818306f68c9", "message": "request matcher", "committedDate": "2020-02-13T22:44:26Z", "type": "commit"}, {"oid": "9fdf4b794c06e8788905bd79b72d3818306f68c9", "url": "https://github.com/apache/kafka/commit/9fdf4b794c06e8788905bd79b72d3818306f68c9", "message": "request matcher", "committedDate": "2020-02-13T22:44:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2Mzc2Mg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379263762", "bodyText": "This is an interesting idea, but it seems good enough to verify the fetched offsets. The only way we could get 5L is fetching against the new leader.", "author": "hachikuji", "createdAt": "2020-02-14T06:06:26Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,93 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        final int newLeaderEpoch = 3;\n+        MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3,\n+            singletonMap(topicName, Errors.NONE), singletonMap(topicName, 4), tp -> newLeaderEpoch);\n+        LogContext dummyContext = new LogContext();\n+        ConsumerMetadata dummyMetadata = new ConsumerMetadata(0, Long.MAX_VALUE, false, false,\n+            new SubscriptionState(dummyContext, OffsetResetStrategy.EARLIEST),\n+            dummyContext, new ClusterResourceListeners());\n+        dummyMetadata.updateWithCurrentRequestVersion(updatedMetadata, false, 0L);\n+\n+        Node newLeader = dummyMetadata.fetch().leaderFor(tp1);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            buildFetcher();\n+\n+            subscriptions.assignFromUser(Utils.mkSet(tp0, tp1));\n+            client.updateMetadata(initialUpdateResponse);\n+\n+            Node originalLeader = metadata.fetch().leaderFor(tp1);\n+            assertNotEquals(originalLeader, newLeader);\n+\n+            final long fetchTimestamp = 10L;\n+            Map<TopicPartition, ListOffsetResponse.PartitionData> allPartitionData = new HashMap<>();\n+            allPartitionData.put(tp0, new ListOffsetResponse.PartitionData(\n+                Errors.NONE, fetchTimestamp, 4L, Optional.empty()));\n+            allPartitionData.put(tp1, new ListOffsetResponse.PartitionData(\n+                retriableError, ListOffsetRequest.LATEST_TIMESTAMP, -1L, Optional.empty()));\n+\n+            client.prepareResponseFrom(body -> {\n+                boolean isListOffsetRequest = body instanceof ListOffsetRequest;\n+                if (isListOffsetRequest) {\n+                    ListOffsetRequest request = (ListOffsetRequest) body;\n+                    Map<TopicPartition, ListOffsetRequest.PartitionData> expectedTopicPartitions = new HashMap<>();\n+                    expectedTopicPartitions.put(tp0, new ListOffsetRequest.PartitionData(\n+                        fetchTimestamp, Optional.empty()));\n+                    expectedTopicPartitions.put(tp1, new ListOffsetRequest.PartitionData(\n+                        fetchTimestamp, Optional.empty()));\n+\n+                    return request.partitionTimestamps().equals(expectedTopicPartitions);\n+                } else {\n+                    return false;\n+                }\n+            }, new ListOffsetResponse(allPartitionData), originalLeader);\n+\n+            client.prepareMetadataUpdate(updatedMetadata);\n+\n+            // If the metadata wasn't updated before retrying, the fetcher would consult the original leader and hit a fatal exception.", "originalCommit": "9fdf4b794c06e8788905bd79b72d3818306f68c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5Njc2Mg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379296762", "bodyText": "In fact, it isn't. We shall hit a DisconnectedException first, and the fetcher would refresh the metadata before a third attempt. So a fatal exception is needed.", "author": "abbccdda", "createdAt": "2020-02-14T08:12:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2Mzc2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU3NDgyMg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379574822", "bodyText": "Ok. How about we use the NOT_LEADER error since that is the case we are trying to simulate?", "author": "hachikuji", "createdAt": "2020-02-14T18:17:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2Mzc2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU4NzAwOQ==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379587009", "bodyText": "Just did", "author": "abbccdda", "createdAt": "2020-02-14T18:45:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2Mzc2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2NzU1Mg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379267552", "bodyText": "MetadataResponse allows us to get the Cluster directly, so we can do something simpler:\n        Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1);\n        Node newLeader = updatedMetadata.cluster().leaderFor(tp1);\n        assertNotEquals(oldLeader, newLeader);\n\nSince the metadata doesn't change, we can just do this check once.", "author": "hachikuji", "createdAt": "2020-02-14T06:24:33Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,93 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        final int newLeaderEpoch = 3;\n+        MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3,\n+            singletonMap(topicName, Errors.NONE), singletonMap(topicName, 4), tp -> newLeaderEpoch);\n+        LogContext dummyContext = new LogContext();\n+        ConsumerMetadata dummyMetadata = new ConsumerMetadata(0, Long.MAX_VALUE, false, false,", "originalCommit": "9fdf4b794c06e8788905bd79b72d3818306f68c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NjAwNg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379296006", "bodyText": "Sounds good", "author": "abbccdda", "createdAt": "2020-02-14T08:10:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2NzU1Mg=="}], "type": "inlineReview"}, {"oid": "1500e239dc91b80a9dd653c321d40fda812a4a25", "url": "https://github.com/apache/kafka/commit/1500e239dc91b80a9dd653c321d40fda812a4a25", "message": "old/new leader put out", "committedDate": "2020-02-14T08:08:41Z", "type": "commit"}, {"oid": "78820ca0b3d6bde44409634dae6a78900838870f", "url": "https://github.com/apache/kafka/commit/78820ca0b3d6bde44409634dae6a78900838870f", "message": "not leader", "committedDate": "2020-02-14T18:32:16Z", "type": "commit"}]}