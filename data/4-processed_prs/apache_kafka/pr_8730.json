{"pr_number": 8730, "pr_title": "KAFKA-10048: Possible data gap for a consumer after a failover when u\u2026", "pr_createdAt": "2020-05-27T06:58:08Z", "pr_url": "https://github.com/apache/kafka/pull/8730", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk1MjEyMw==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r431952123", "bodyText": "Can we add a deadline to this function? Something like \n  \n    \n      kafka/connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/MirrorClient.java\n    \n    \n         Line 164\n      in\n      1c4eb1a\n    \n    \n    \n    \n\n        \n          \n           while (System.currentTimeMillis() < deadline && !endOfStream(consumer, checkpointAssignment)) {", "author": "ryannedolan", "createdAt": "2020-05-28T16:05:11Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -316,6 +354,23 @@ public void testReplication() throws InterruptedException {\n             backup.kafka().consume(NUM_RECORDS_PRODUCED, 2 * RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-2\").count());\n     }\n \n+    private Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> consumeAllMessages(Consumer<byte[], byte[]> consumer) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4NTYxMw==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r432585613", "bodyText": "Good point. Will add that.", "author": "asdaraujo", "createdAt": "2020-05-29T16:03:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk1MjEyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzMjExMw==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r447832113", "bodyText": "This has been added.", "author": "asdaraujo", "createdAt": "2020-06-30T16:48:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk1MjEyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk1NDgzOQ==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r431954839", "bodyText": "I don't see why we need the cast, ceil, and coercion here? Integer math?", "author": "ryannedolan", "createdAt": "2020-05-28T16:09:36Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -272,28 +303,35 @@ public void testReplication() throws InterruptedException {\n         waitForCondition(() -> {\n             try {\n                 return primaryClient.remoteConsumerOffsets(\"consumer-group-1\", \"backup\",\n-                    Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"test-topic-1\", 0));\n+                    Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"test-topic-1\", 1));\n             } catch (Throwable e) {\n                 return false;\n             }\n         }, CHECKPOINT_DURATION_MS, \"Offsets not translated upstream to primary cluster.\");\n \n         Map<TopicPartition, OffsetAndMetadata> primaryOffsets = primaryClient.remoteConsumerOffsets(\"consumer-group-1\", \"backup\",\n                 Duration.ofMillis(CHECKPOINT_DURATION_MS));\n- \n+\n         // Failback consumer group to primary cluster\n-        Consumer<byte[], byte[]> consumer2 = primary.kafka().createConsumer(Collections.singletonMap(\"group.id\", \"consumer-group-1\"));\n-        consumer2.assign(primaryOffsets.keySet());\n+        Consumer<byte[], byte[]> consumer2 = primary.kafka().createConsumer(consumerProps);\n+        List<TopicPartition> primaryPartitions = IntStream.range(0, NUM_PARTITIONS)\n+                .boxed()\n+                .flatMap(p -> Stream.of(new TopicPartition(\"test-topic-1\", p), new TopicPartition(\"backup.test-topic-1\", p)))\n+                .collect(Collectors.toList());\n+        consumer2.assign(primaryPartitions);\n         primaryOffsets.forEach(consumer2::seek);\n-        consumer2.poll(Duration.ofMillis(500));\n \n         assertTrue(\"Consumer failedback to zero upstream offset.\", consumer2.position(new TopicPartition(\"test-topic-1\", 0)) > 0);\n         assertTrue(\"Consumer failedback to zero downstream offset.\", consumer2.position(new TopicPartition(\"backup.test-topic-1\", 0)) > 0);\n         assertTrue(\"Consumer failedback beyond expected upstream offset.\", consumer2.position(\n-            new TopicPartition(\"test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+            new TopicPartition(\"test-topic-1\", 0)) <= Math.ceil((float) NUM_RECORDS_PRODUCED / (NUM_PARTITIONS - 1)) + Math.ceil((float) NUM_RECORDS_PRODUCED / NUM_PARTITIONS));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4NzExOA==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r432587118", "bodyText": "Yep. I need to cast to float to ensure that the result of the division is a float, with a non-zero decimal value, so that ceil will round it up to the nearest integer.", "author": "asdaraujo", "createdAt": "2020-05-29T16:06:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk1NDgzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk1Nzk2OA==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r431957968", "bodyText": "There is a lot of magic here -- why the special case and magic numbers? Can we restructure this to test the two cases separately maybe?", "author": "ryannedolan", "createdAt": "2020-05-28T16:13:29Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -244,26 +251,50 @@ public void testReplication() throws InterruptedException {\n \n         assertTrue(\"Offsets not translated downstream to backup cluster. Found: \" + backupOffsets, backupOffsets.containsKey(\n             new TopicPartition(\"primary.test-topic-1\", 0)));\n+        assertTrue(\"Offset of empty partition not translated downstream to backup cluster. Found: \" + backupOffsets, backupOffsets.containsKey(\n+            new TopicPartition(\"primary.test-topic-1\", NUM_PARTITIONS - 1)));\n+\n+        // Produce additional messages.\n+        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {\n+            // produce to all partitions this time\n+            primary.kafka().produce(\"test-topic-1\", i % NUM_PARTITIONS, \"key\", \"message-2-\" + i);\n+            backup.kafka().produce(\"test-topic-1\", i % NUM_PARTITIONS, \"key\", \"message-2-\" + i);\n+        }\n \n         // Failover consumer group to backup cluster.\n-        Consumer<byte[], byte[]> consumer1 = backup.kafka().createConsumer(Collections.singletonMap(\"group.id\", \"consumer-group-1\"));\n-        consumer1.assign(backupOffsets.keySet());\n+        Map<String, Object> consumerProps = new HashMap<String, Object>() {{\n+                put(\"group.id\", \"consumer-group-1\");\n+                put(\"auto.offset.reset\", \"latest\");\n+            }};\n+        Consumer<byte[], byte[]> consumer1 = backup.kafka().createConsumer(consumerProps);\n+        List<TopicPartition> backupPartitions = IntStream.range(0, NUM_PARTITIONS)\n+                .boxed()\n+                .flatMap(p -> Stream.of(new TopicPartition(\"test-topic-1\", p), new TopicPartition(\"primary.test-topic-1\", p)))\n+                .collect(Collectors.toList());\n+        consumer1.assign(backupPartitions);\n         backupOffsets.forEach(consumer1::seek);\n-        consumer1.poll(Duration.ofMillis(500));\n-        consumer1.commitSync();\n \n         assertTrue(\"Consumer failedover to zero offset.\", consumer1.position(new TopicPartition(\"primary.test-topic-1\", 0)) > 0);\n         assertTrue(\"Consumer failedover beyond expected offset.\", consumer1.position(\n-            new TopicPartition(\"primary.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+            new TopicPartition(\"primary.test-topic-1\", 0)) <= Math.ceil((float) NUM_RECORDS_PRODUCED / (NUM_PARTITIONS - 1)));\n+        assertEquals(\"Consumer failedover to non-zero offset on last partition.\", 0,\n+            consumer1.position(new TopicPartition(\"primary.test-topic-1\", NUM_PARTITIONS - 1)));\n         assertTrue(\"Checkpoints were not emitted upstream to primary cluster.\", primary.kafka().consume(1,\n             CHECKPOINT_DURATION_MS, \"backup.checkpoints.internal\").count() > 0);\n \n+        Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> messages1 = consumeAllMessages(consumer1);\n+        System.out.println(messages1);\n+        for (TopicPartition tp : backupPartitions) {\n+            assertNotNull(\"No data consumed from partition \" + tp + \".\", messages1.get(tp));\n+            int expectedMessageCount = tp.toString().equals(\"test-topic-1-0\") ? 22 : 10;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc0OTIwNg==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r432749206", "bodyText": "You're right. This looks magic indeed! I had to think about it again to understand my own reasoning :)\nI added comments to explain what's going on here and why we expect 22 for one partition and 10 for the others.\nI left the tests as they were. I think the comment makes the test logic clear.", "author": "asdaraujo", "createdAt": "2020-05-29T21:42:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk1Nzk2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMjY1MA==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r463132650", "bodyText": "I'm actually surprized we only see positions 22 and 10. Why do we only get test-topic-1-0 here and not the other 9 partitions?", "author": "mimaison", "createdAt": "2020-07-30T16:46:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk1Nzk2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk1ODI3MQ==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r431958271", "bodyText": "errant println", "author": "ryannedolan", "createdAt": "2020-05-28T16:13:48Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -244,26 +251,50 @@ public void testReplication() throws InterruptedException {\n \n         assertTrue(\"Offsets not translated downstream to backup cluster. Found: \" + backupOffsets, backupOffsets.containsKey(\n             new TopicPartition(\"primary.test-topic-1\", 0)));\n+        assertTrue(\"Offset of empty partition not translated downstream to backup cluster. Found: \" + backupOffsets, backupOffsets.containsKey(\n+            new TopicPartition(\"primary.test-topic-1\", NUM_PARTITIONS - 1)));\n+\n+        // Produce additional messages.\n+        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {\n+            // produce to all partitions this time\n+            primary.kafka().produce(\"test-topic-1\", i % NUM_PARTITIONS, \"key\", \"message-2-\" + i);\n+            backup.kafka().produce(\"test-topic-1\", i % NUM_PARTITIONS, \"key\", \"message-2-\" + i);\n+        }\n \n         // Failover consumer group to backup cluster.\n-        Consumer<byte[], byte[]> consumer1 = backup.kafka().createConsumer(Collections.singletonMap(\"group.id\", \"consumer-group-1\"));\n-        consumer1.assign(backupOffsets.keySet());\n+        Map<String, Object> consumerProps = new HashMap<String, Object>() {{\n+                put(\"group.id\", \"consumer-group-1\");\n+                put(\"auto.offset.reset\", \"latest\");\n+            }};\n+        Consumer<byte[], byte[]> consumer1 = backup.kafka().createConsumer(consumerProps);\n+        List<TopicPartition> backupPartitions = IntStream.range(0, NUM_PARTITIONS)\n+                .boxed()\n+                .flatMap(p -> Stream.of(new TopicPartition(\"test-topic-1\", p), new TopicPartition(\"primary.test-topic-1\", p)))\n+                .collect(Collectors.toList());\n+        consumer1.assign(backupPartitions);\n         backupOffsets.forEach(consumer1::seek);\n-        consumer1.poll(Duration.ofMillis(500));\n-        consumer1.commitSync();\n \n         assertTrue(\"Consumer failedover to zero offset.\", consumer1.position(new TopicPartition(\"primary.test-topic-1\", 0)) > 0);\n         assertTrue(\"Consumer failedover beyond expected offset.\", consumer1.position(\n-            new TopicPartition(\"primary.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+            new TopicPartition(\"primary.test-topic-1\", 0)) <= Math.ceil((float) NUM_RECORDS_PRODUCED / (NUM_PARTITIONS - 1)));\n+        assertEquals(\"Consumer failedover to non-zero offset on last partition.\", 0,\n+            consumer1.position(new TopicPartition(\"primary.test-topic-1\", NUM_PARTITIONS - 1)));\n         assertTrue(\"Checkpoints were not emitted upstream to primary cluster.\", primary.kafka().consume(1,\n             CHECKPOINT_DURATION_MS, \"backup.checkpoints.internal\").count() > 0);\n \n+        Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> messages1 = consumeAllMessages(consumer1);\n+        System.out.println(messages1);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4OTMwNQ==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r432589305", "bodyText": "Oops... it escaped my final review. Removing it.", "author": "asdaraujo", "createdAt": "2020-05-29T16:10:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk1ODI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgzMjM0OQ==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r447832349", "bodyText": "Done", "author": "asdaraujo", "createdAt": "2020-06-30T16:48:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk1ODI3MQ=="}], "type": "inlineReview"}, {"oid": "17f67504bd3a6773d2c266f1755c6c4993222f1d", "url": "https://github.com/apache/kafka/commit/17f67504bd3a6773d2c266f1755c6c4993222f1d", "message": "KAFKA-10048: Possible data gap for a consumer after a failover when using MM2\n\nEnsure that the MM2 checkpoint mirror task replicates consumer offsets even when they are\nzero to avoid issues with consumers after failovers.\n\nAuthor: Andre Araujo <asdaraujo@gmail.com>", "committedDate": "2020-05-29T21:55:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyNDExMQ==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r435424111", "bodyText": "Can downstream offset be negative?", "author": "heritamas", "createdAt": "2020-06-04T17:24:07Z", "path": "connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorCheckpointTask.java", "diffHunk": "@@ -132,7 +132,7 @@ public String version() {\n             return listConsumerGroupOffsets(group).entrySet().stream()\n                 .filter(x -> shouldCheckpointTopic(x.getKey().topic()))\n                 .map(x -> checkpoint(group, x.getKey(), x.getValue()))\n-                .filter(x -> x.downstreamOffset() > 0)  // ignore offsets we cannot translate accurately\n+                .filter(x -> x.downstreamOffset() >= 0)  // ignore offsets we cannot translate accurately", "originalCommit": "17f67504bd3a6773d2c266f1755c6c4993222f1d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2OTMxMQ==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r447769311", "bodyText": "nope, that's not possible", "author": "ryannedolan", "createdAt": "2020-06-30T15:21:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyNDExMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQ0MDUxMQ==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r449440511", "bodyText": "Then, this filter matches everything, doesn't it?", "author": "heritamas", "createdAt": "2020-07-03T08:07:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyNDExMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTUzNjMxOA==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r459536318", "bodyText": "@ryannedolan @heritamas If you guys think this is safe enough I can remove that filter. But it doesn't hurt to leave it there just in case a rogue negative offset comes through for whatever reason/bug... Please let me know what you think.", "author": "asdaraujo", "createdAt": "2020-07-23T15:27:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyNDExMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5NzAwNw==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r460197007", "bodyText": "Yes, does not hurt to leave it. Just for sure.", "author": "heritamas", "createdAt": "2020-07-24T17:40:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyNDExMQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA5NzkxOA==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r463097918", "bodyText": "This comment needs updating", "author": "mimaison", "createdAt": "2020-07-30T15:52:21Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -367,14 +406,37 @@ public void testOneWayReplicationWithAutorOffsetSync1() throws InterruptedExcept\n         time.sleep(5000);\n \n         // create a consumer at backup cluster with same consumer group Id to consume old and new topic\n-        consumer = backup.kafka().createConsumerAndSubscribeTo(Collections.singletonMap(\n-            \"group.id\", \"consumer-group-1\"), \"primary.test-topic-1\", \"primary.test-topic-2\");\n+        consumer = backup.kafka().createConsumerAndSubscribeTo(consumerProps, \"primary.test-topic-1\", \"primary.test-topic-2\");\n \n         records = consumer.poll(Duration.ofMillis(500));\n         // similar reasoning as above, no more records to consume by the same consumer group at backup cluster\n         assertEquals(\"consumer record size is not zero\", 0, records.count());\n         consumer.close();\n+    }\n+\n+    private void produceMessages(EmbeddedConnectCluster cluster, String topicName, int partitions, String msgPrefix) {\n+        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {\n+            // produce to all partitions but the last one", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQ4NTEwNg==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r477485106", "bodyText": "Good catch. Updating.", "author": "asdaraujo", "createdAt": "2020-08-26T17:57:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA5NzkxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI2NTQ1MQ==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r480265451", "bodyText": "Updated.", "author": "asdaraujo", "createdAt": "2020-08-31T17:04:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA5NzkxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEyODQxNQ==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r463128415", "bodyText": "Would it be better using a separate topic in order to keep a partition without any records? By changing this topic it affects existing checks in all tests", "author": "mimaison", "createdAt": "2020-07-30T16:39:59Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -128,10 +136,23 @@ public void setup() throws InterruptedException {\n         backup.kafka().createTopic(\"primary.test-topic-1\", 1);\n         backup.kafka().createTopic(\"heartbeats\", 1);\n \n-        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {\n-            primary.kafka().produce(\"test-topic-1\", i % NUM_PARTITIONS, \"key\", \"message-1-\" + i);\n-            backup.kafka().produce(\"test-topic-1\", i % NUM_PARTITIONS, \"key\", \"message-2-\" + i);\n-        }\n+        // produce to all partitions but the last one", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQ4NTkxNg==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r477485916", "bodyText": "Good point. I'll look into this. Besides not affecting the other tests, it should make it simpler to reason about", "author": "asdaraujo", "createdAt": "2020-08-26T17:58:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEyODQxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI2NjQ1OQ==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r480266459", "bodyText": "I've separated the test from the existing ones, also using a different topic. Some of the logic on those tests is complex and may be hard to follow so I thought it would be better to have the tests totally separate and simpler to interpret.\nI think, and hope, it's easier to understand now than it was before.", "author": "asdaraujo", "createdAt": "2020-08-31T17:06:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEyODQxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEyODk2OA==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r463128968", "bodyText": "As this does not change, I wonder if we could direct initialize consumerProps when it's declared", "author": "mimaison", "createdAt": "2020-07-30T16:40:56Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -128,10 +136,23 @@ public void setup() throws InterruptedException {\n         backup.kafka().createTopic(\"primary.test-topic-1\", 1);\n         backup.kafka().createTopic(\"heartbeats\", 1);\n \n-        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {\n-            primary.kafka().produce(\"test-topic-1\", i % NUM_PARTITIONS, \"key\", \"message-1-\" + i);\n-            backup.kafka().produce(\"test-topic-1\", i % NUM_PARTITIONS, \"key\", \"message-2-\" + i);\n-        }\n+        // produce to all partitions but the last one\n+        produceMessages(primary, \"test-topic-1\", NUM_PARTITIONS - 1, \"message-1-\");\n+        produceMessages(backup, \"test-topic-1\", NUM_PARTITIONS - 1, \"message-2-\");\n+\n+        consumerProps = new HashMap<String, Object>() {{", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI2NzEyMg==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r480267122", "bodyText": "I changed the way this is done, setting it at the test-level, with test-specific CGs.", "author": "asdaraujo", "createdAt": "2020-08-31T17:07:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEyODk2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEyOTU5MA==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r463129590", "bodyText": "Do we still need these 2 blocks? In setup() we already consumed all messages", "author": "mimaison", "createdAt": "2020-07-30T16:42:00Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -190,24 +211,19 @@ public void close() {\n     public void testReplication() throws InterruptedException {\n \n         // create consumers before starting the connectors so we don't need to wait for discovery\n-        Consumer<byte[], byte[]> consumer1 = primary.kafka().createConsumerAndSubscribeTo(Collections.singletonMap(\n-            \"group.id\", \"consumer-group-1\"), \"test-topic-1\", \"backup.test-topic-1\");\n+        Consumer<byte[], byte[]> consumer1 = primary.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\", \"backup.test-topic-1\");\n         consumer1.poll(Duration.ofMillis(500));\n         consumer1.commitSync();\n         consumer1.close();\n \n-        Consumer<byte[], byte[]> consumer2 = backup.kafka().createConsumerAndSubscribeTo(Collections.singletonMap(\n-            \"group.id\", \"consumer-group-1\"), \"test-topic-1\", \"primary.test-topic-1\");\n+        Consumer<byte[], byte[]> consumer2 = backup.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\", \"primary.test-topic-1\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI2NzM0Nw==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r480267347", "bodyText": "Good catch. Also changed this and we're now consuming it only once.", "author": "asdaraujo", "createdAt": "2020-08-31T17:07:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEyOTU5MA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMzNTA5Ng==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r490335096", "bodyText": "this overwrite of mm2config should go in the setup method, IMHO", "author": "edoardocomar", "createdAt": "2020-09-17T15:16:55Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -283,49 +296,140 @@ public void testReplication() throws InterruptedException {\n \n         waitForCondition(() -> {\n             try {\n-                return primaryClient.remoteConsumerOffsets(\"consumer-group-1\", \"backup\",\n+                return primaryClient.remoteConsumerOffsets(consumerGroupName, \"backup\",\n                     Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"test-topic-1\", 0));\n             } catch (Throwable e) {\n                 return false;\n             }\n         }, CHECKPOINT_DURATION_MS, \"Offsets not translated upstream to primary cluster.\");\n \n-        Map<TopicPartition, OffsetAndMetadata> primaryOffsets = primaryClient.remoteConsumerOffsets(\"consumer-group-1\", \"backup\",\n+        Map<TopicPartition, OffsetAndMetadata> primaryOffsets = primaryClient.remoteConsumerOffsets(consumerGroupName, \"backup\",\n                 Duration.ofMillis(CHECKPOINT_DURATION_MS));\n- \n+\n         // Failback consumer group to primary cluster\n-        consumer2 = primary.kafka().createConsumer(Collections.singletonMap(\"group.id\", \"consumer-group-1\"));\n-        consumer2.assign(primaryOffsets.keySet());\n-        primaryOffsets.forEach(consumer2::seek);\n-        consumer2.poll(Duration.ofMillis(500));\n-\n-        assertTrue(\"Consumer failedback to zero upstream offset.\", consumer2.position(new TopicPartition(\"test-topic-1\", 0)) > 0);\n-        assertTrue(\"Consumer failedback to zero downstream offset.\", consumer2.position(new TopicPartition(\"backup.test-topic-1\", 0)) > 0);\n-        assertTrue(\"Consumer failedback beyond expected upstream offset.\", consumer2.position(\n-            new TopicPartition(\"test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n-        assertTrue(\"Consumer failedback beyond expected downstream offset.\", consumer2.position(\n-            new TopicPartition(\"backup.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n-        \n-        consumer2.close();\n-      \n+        primaryConsumer = primary.kafka().createConsumer(consumerProps);\n+        primaryConsumer.assign(allPartitions(\"test-topic-1\", \"backup.test-topic-1\"));\n+        seek(primaryConsumer, primaryOffsets);\n+        consumeAllMessages(primaryConsumer, 0);\n+\n+        assertTrue(\"Consumer failedback to zero upstream offset.\", primaryConsumer.position(new TopicPartition(\"test-topic-1\", 0)) > 0);\n+        assertTrue(\"Consumer failedback to zero downstream offset.\", primaryConsumer.position(new TopicPartition(\"backup.test-topic-1\", 0)) > 0);\n+        assertTrue(\"Consumer failedback beyond expected upstream offset.\", primaryConsumer.position(\n+            new TopicPartition(\"test-topic-1\", 0)) <= NUM_RECORDS_PER_PARTITION);\n+        assertTrue(\"Consumer failedback beyond expected downstream offset.\", primaryConsumer.position(\n+            new TopicPartition(\"backup.test-topic-1\", 0)) <= NUM_RECORDS_PER_PARTITION);\n+\n+        Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> messages2 = consumeAllMessages(primaryConsumer, 0);\n+        // If offset translation was successful we expect no messages to be consumed after failback\n+        assertEquals(\"Data was consumed from partitions: \" + messages2.keySet() + \".\", 0, messages2.size());\n+        primaryConsumer.close();\n+\n         // create more matching topics\n         primary.kafka().createTopic(\"test-topic-2\", NUM_PARTITIONS);\n         backup.kafka().createTopic(\"test-topic-3\", NUM_PARTITIONS);\n \n-        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {\n-            primary.kafka().produce(\"test-topic-2\", 0, \"key\", \"message-2-\" + i);\n-            backup.kafka().produce(\"test-topic-3\", 0, \"key\", \"message-3-\" + i);\n+        produceMessages(primary, \"test-topic-2\", \"message-3-\", 1);\n+        produceMessages(backup, \"test-topic-3\", \"message-4-\", 1);\n+\n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-2\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-3\").count());\n+\n+        assertEquals(\"New topic was not replicated to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-3\").count());\n+        assertEquals(\"New topic was not replicated to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-2\").count());\n+    }\n+\n+    @Test\n+    public void testReplicationWithEmptyPartition() throws InterruptedException {\n+        String consumerGroupName = \"consumer-group-testReplicationWithEmptyPartition\";\n+        Map<String, Object> consumerProps  = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"latest\");\n+            }};\n+\n+        // create topics\n+        String topic = \"test-topic-empty\";\n+        String primaryTopicReplica = \"primary.\" + topic;\n+        String backupTopicReplica = \"backup.\" + topic;\n+        primary.kafka().createTopic(topic, NUM_PARTITIONS);\n+        primary.kafka().createTopic(backupTopicReplica, 1);\n+        backup.kafka().createTopic(topic, NUM_PARTITIONS);\n+        backup.kafka().createTopic(primaryTopicReplica, 1);\n+\n+        // Consume, from the primary cluster, before starting the connectors so we don't need to wait for discovery\n+        Consumer<byte[], byte[]> consumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, topic, backupTopicReplica);\n+        consumeAllMessages(consumer, 0);\n+\n+        // produce to all test-topic-empty's partitions *but the last one*, on the primary cluster\n+        produceMessages(primary, topic, \"message-1-\", NUM_PARTITIONS - 1);\n+\n+        // Consume all messages\n+        consumeAllMessages(consumer, NUM_RECORDS_PER_PARTITION * (NUM_PARTITIONS - 1));\n+        consumer.close();\n+\n+        // Consumer group offsets after consumption: topic's last partition doesn't yet has data, so\n+        // the committed offset is 0. All other topic's partition should have offset equal to NUM_RECORDS_PER_PARTITION.\n+        // backupTopicReplica still has a single empty partition, since MM2 is not yet started, and its record offset is 0.\n+\n+        mm2Config = new MirrorMakerConfig(mm2Props);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMwNjI5OA==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r496306298", "bodyText": "Thanks, @edoardocomar . I've addressed this.", "author": "asdaraujo", "createdAt": "2020-09-29T00:24:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMzNTA5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMzODU5MQ==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r490338591", "bodyText": "This test is overly complicated. I think it could:\n\nCreate a topic\nProduce messages to all partitions but one\nConsume all messages\nStart a single MirrorMaker2 instance primary->backup\nUse RemoteClusterUtils.translateOffsets() to retrieve offsets\nAssert offset for the last partition is 0\n\nFor example, something along these lines (this cuts a few corners so you'd need to improve it)\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void testReplicationWithEmptyPartition() throws InterruptedException {\n          \n          \n            \n                @Test\n          \n          \n            \n                public void testReplicationWithEmptyPartition() throws Exception {\n          \n          \n            \n                    String consumerGroupName = \"consumer-group-testReplicationWithEmptyPartition\";\n          \n          \n            \n                    Map<String, Object> consumerProps  = new HashMap<String, Object>() {{\n          \n          \n            \n                        put(\"group.id\", consumerGroupName);\n          \n          \n            \n                        put(\"auto.offset.reset\", \"earliest\");\n          \n          \n            \n                    }};\n          \n          \n            \n            \n          \n          \n            \n                    String topic = \"test-topic-empty\";\n          \n          \n            \n                    primary.kafka().createTopic(topic, NUM_PARTITIONS);\n          \n          \n            \n                    mm2Config = new MirrorMakerConfig(mm2Props);\n          \n          \n            \n                    // produce to all test-topic-empty's partitions *but the last one*, on the primary cluster\n          \n          \n            \n                    produceMessages(primary, topic, \"message-1-\", NUM_PARTITIONS - 1);\n          \n          \n            \n            \n          \n          \n            \n                    // Consume, from the primary cluster, before starting the connectors so we don't need to wait for discovery\n          \n          \n            \n                    Consumer<byte[], byte[]> consumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, topic);\n          \n          \n            \n                    consumeAllMessages(consumer, NUM_RECORDS_PER_PARTITION * (NUM_PARTITIONS - 1));\n          \n          \n            \n                    consumer.close();\n          \n          \n            \n            \n          \n          \n            \n                    waitUntilMirrorMakerIsRunning(backup, mm2Config, \"primary\", \"backup\");\n          \n          \n            \n            \n          \n          \n            \n                    Map<TopicPartition, OffsetAndMetadata> backupOffsets = RemoteClusterUtils.translateOffsets(\n          \n          \n            \n                            mm2Config.clientConfig(\"backup\").adminConfig(),\n          \n          \n            \n                            \"primary\",\n          \n          \n            \n                            consumerGroupName,\n          \n          \n            \n                            Duration.ofMillis(CHECKPOINT_DURATION_MS));\n          \n          \n            \n            \n          \n          \n            \n                    OffsetAndMetadata oam = backupOffsets.get(new TopicPartition(\"primary.\" + topic, NUM_PARTITIONS - 1));\n          \n          \n            \n                    assertNotNull(oam);\n          \n          \n            \n                    assertEquals(0, oam.offset());\n          \n          \n            \n                }", "author": "mimaison", "createdAt": "2020-09-17T15:21:47Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -283,49 +296,140 @@ public void testReplication() throws InterruptedException {\n \n         waitForCondition(() -> {\n             try {\n-                return primaryClient.remoteConsumerOffsets(\"consumer-group-1\", \"backup\",\n+                return primaryClient.remoteConsumerOffsets(consumerGroupName, \"backup\",\n                     Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"test-topic-1\", 0));\n             } catch (Throwable e) {\n                 return false;\n             }\n         }, CHECKPOINT_DURATION_MS, \"Offsets not translated upstream to primary cluster.\");\n \n-        Map<TopicPartition, OffsetAndMetadata> primaryOffsets = primaryClient.remoteConsumerOffsets(\"consumer-group-1\", \"backup\",\n+        Map<TopicPartition, OffsetAndMetadata> primaryOffsets = primaryClient.remoteConsumerOffsets(consumerGroupName, \"backup\",\n                 Duration.ofMillis(CHECKPOINT_DURATION_MS));\n- \n+\n         // Failback consumer group to primary cluster\n-        consumer2 = primary.kafka().createConsumer(Collections.singletonMap(\"group.id\", \"consumer-group-1\"));\n-        consumer2.assign(primaryOffsets.keySet());\n-        primaryOffsets.forEach(consumer2::seek);\n-        consumer2.poll(Duration.ofMillis(500));\n-\n-        assertTrue(\"Consumer failedback to zero upstream offset.\", consumer2.position(new TopicPartition(\"test-topic-1\", 0)) > 0);\n-        assertTrue(\"Consumer failedback to zero downstream offset.\", consumer2.position(new TopicPartition(\"backup.test-topic-1\", 0)) > 0);\n-        assertTrue(\"Consumer failedback beyond expected upstream offset.\", consumer2.position(\n-            new TopicPartition(\"test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n-        assertTrue(\"Consumer failedback beyond expected downstream offset.\", consumer2.position(\n-            new TopicPartition(\"backup.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n-        \n-        consumer2.close();\n-      \n+        primaryConsumer = primary.kafka().createConsumer(consumerProps);\n+        primaryConsumer.assign(allPartitions(\"test-topic-1\", \"backup.test-topic-1\"));\n+        seek(primaryConsumer, primaryOffsets);\n+        consumeAllMessages(primaryConsumer, 0);\n+\n+        assertTrue(\"Consumer failedback to zero upstream offset.\", primaryConsumer.position(new TopicPartition(\"test-topic-1\", 0)) > 0);\n+        assertTrue(\"Consumer failedback to zero downstream offset.\", primaryConsumer.position(new TopicPartition(\"backup.test-topic-1\", 0)) > 0);\n+        assertTrue(\"Consumer failedback beyond expected upstream offset.\", primaryConsumer.position(\n+            new TopicPartition(\"test-topic-1\", 0)) <= NUM_RECORDS_PER_PARTITION);\n+        assertTrue(\"Consumer failedback beyond expected downstream offset.\", primaryConsumer.position(\n+            new TopicPartition(\"backup.test-topic-1\", 0)) <= NUM_RECORDS_PER_PARTITION);\n+\n+        Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> messages2 = consumeAllMessages(primaryConsumer, 0);\n+        // If offset translation was successful we expect no messages to be consumed after failback\n+        assertEquals(\"Data was consumed from partitions: \" + messages2.keySet() + \".\", 0, messages2.size());\n+        primaryConsumer.close();\n+\n         // create more matching topics\n         primary.kafka().createTopic(\"test-topic-2\", NUM_PARTITIONS);\n         backup.kafka().createTopic(\"test-topic-3\", NUM_PARTITIONS);\n \n-        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {\n-            primary.kafka().produce(\"test-topic-2\", 0, \"key\", \"message-2-\" + i);\n-            backup.kafka().produce(\"test-topic-3\", 0, \"key\", \"message-3-\" + i);\n+        produceMessages(primary, \"test-topic-2\", \"message-3-\", 1);\n+        produceMessages(backup, \"test-topic-3\", \"message-4-\", 1);\n+\n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-2\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-3\").count());\n+\n+        assertEquals(\"New topic was not replicated to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-3\").count());\n+        assertEquals(\"New topic was not replicated to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-2\").count());\n+    }\n+\n+    @Test\n+    public void testReplicationWithEmptyPartition() throws InterruptedException {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMwNjc3Nw==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r496306777", "bodyText": "Thanks, @mimaison . I've changed the test to simplify it as per your suggestion.", "author": "asdaraujo", "createdAt": "2020-09-29T00:26:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMzODU5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM0MzYxNA==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r490343614", "bodyText": "We can use Collections.singletonMap() here", "author": "mimaison", "createdAt": "2020-09-17T15:28:36Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -136,10 +141,19 @@ public void setup() throws InterruptedException {\n         backup.kafka().createTopic(\"primary.test-topic-1\", 1);\n         backup.kafka().createTopic(\"heartbeats\", 1);\n \n-        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {\n-            primary.kafka().produce(\"test-topic-1\", i % NUM_PARTITIONS, \"key\", \"message-1-\" + i);\n-            backup.kafka().produce(\"test-topic-1\", i % NUM_PARTITIONS, \"key\", \"message-2-\" + i);\n-        }\n+        // produce to all partitions of test-topic-1\n+        produceMessages(primary, \"test-topic-1\", \"message-1-\");\n+        produceMessages(backup, \"test-topic-1\", \"message-2-\");\n+\n+        // Generate some consumer activity on both clusters to ensure the checkpoint connector always starts promptly\n+        Map<String, Object> dummyProps = new HashMap<String, Object>();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMwNjM5OA==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r496306398", "bodyText": "Fixed.", "author": "asdaraujo", "createdAt": "2020-09-29T00:25:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM0MzYxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM0NDQwNw==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r490344407", "bodyText": "latest is the default, why are we setting it?", "author": "mimaison", "createdAt": "2020-09-17T15:29:36Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -201,26 +213,24 @@ public void close() {\n \n     @Test\n     public void testReplication() throws InterruptedException {\n+        String consumerGroupName = \"consumer-group-testReplication\";\n+        Map<String, Object> consumerProps  = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"latest\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMyNjk4OA==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r496326988", "bodyText": "Yep, unnecessary. I'll remove this.", "author": "asdaraujo", "createdAt": "2020-09-29T01:44:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM0NDQwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMzNTU1Nw==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r496335557", "bodyText": "Oops, spoke too soon.\nEven though latest is Kafka's default, EmbeddedKafkaCluster::createConsumer defaults it to earliest", "author": "asdaraujo", "createdAt": "2020-09-29T02:04:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDM0NDQwNw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODEyMTc0OQ==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r498121749", "bodyText": "Is this left over from debugging?", "author": "mimaison", "createdAt": "2020-10-01T09:52:24Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -381,45 +440,46 @@ public void testOneWayReplicationWithAutoOffsetSync() throws InterruptedExceptio\n \n         waitUntilMirrorMakerIsRunning(backup, mm2Config, \"primary\", \"backup\");\n \n-        // create a consumer at backup cluster with same consumer group Id to consume 1 topic\n-        Consumer<byte[], byte[]> consumer = backup.kafka().createConsumerAndSubscribeTo(\n-            Collections.singletonMap(\"group.id\", \"consumer-group-1\"), \"primary.test-topic-1\");\n+        // Map<TopicPartition, OffsetAndMetadata> offsets =", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODEyMzMzMw==", "url": "https://github.com/apache/kafka/pull/8730#discussion_r498123333", "bodyText": "I wonder if we should pass NUM_PARTITIONS instead of null for the last argument. Then numPartitions can be an int in the other produceMessages() method. WDYT?", "author": "mimaison", "createdAt": "2020-10-01T09:54:58Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -429,4 +489,69 @@ private void deleteAllTopics(EmbeddedKafkaCluster cluster) {\n         } catch (Throwable e) {\n         }\n     }\n+\n+    private void produceMessages(EmbeddedConnectCluster cluster, String topicName, String msgPrefix) {\n+        produceMessages(cluster, topicName, msgPrefix, null);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9e73f16151a7901e7af853b698cb3b7df67ab4f5", "url": "https://github.com/apache/kafka/commit/9e73f16151a7901e7af853b698cb3b7df67ab4f5", "message": "KAFKA-10048: Possible data gap for a consumer after a failover when using MM2\n\nEnsure that the MM2 checkpoint mirror task replicates consumer offsets even when they are\nzero to avoid issues with consumers after failovers.\n\nAuthor: Andre Araujo <asdaraujo@gmail.com>", "committedDate": "2020-10-01T17:18:55Z", "type": "commit"}, {"oid": "9e73f16151a7901e7af853b698cb3b7df67ab4f5", "url": "https://github.com/apache/kafka/commit/9e73f16151a7901e7af853b698cb3b7df67ab4f5", "message": "KAFKA-10048: Possible data gap for a consumer after a failover when using MM2\n\nEnsure that the MM2 checkpoint mirror task replicates consumer offsets even when they are\nzero to avoid issues with consumers after failovers.\n\nAuthor: Andre Araujo <asdaraujo@gmail.com>", "committedDate": "2020-10-01T17:18:55Z", "type": "forcePushed"}]}