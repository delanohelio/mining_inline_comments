{"pr_number": 8311, "pr_title": "KAFKA-9434: automated protocol for alterReplicaLogDirs", "pr_createdAt": "2020-03-18T11:13:14Z", "pr_url": "https://github.com/apache/kafka/pull/8311", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjIyOTI2Nw==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r406229267", "bodyText": "Leaving this comment here because there is not other place. While we are refactoring them, could we ensure that both the request and the response are covered in org.apache.kafka.common.requests.RequestResponseTest? I had a very quick look and it seems that they are not.", "author": "dajac", "createdAt": "2020-04-09T14:05:49Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java", "diffHunk": "@@ -18,140 +18,87 @@\n package org.apache.kafka.common.requests;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI0MjMwMw==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r406242303", "bodyText": "The flatMap seems wasteful here. Couldn't we use nested forEachs and add the TopicPartition and the path to the HashMap?", "author": "dajac", "createdAt": "2020-04-09T14:24:12Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java", "diffHunk": "@@ -18,140 +18,87 @@\n package org.apache.kafka.common.requests;\n \n import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n import org.apache.kafka.common.protocol.ApiKeys;\n import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n \n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Map;\n-\n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-import static org.apache.kafka.common.protocol.types.Type.INT32;\n-import static org.apache.kafka.common.protocol.types.Type.STRING;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n \n public class AlterReplicaLogDirsRequest extends AbstractRequest {\n \n-    // request level key names\n-    private static final String LOG_DIRS_KEY_NAME = \"log_dirs\";\n-\n-    // log dir level key names\n-    private static final String LOG_DIR_KEY_NAME = \"log_dir\";\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n-\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V0 = new Schema(\n-            new Field(\"log_dirs\", new ArrayOf(new Schema(\n-                    new Field(\"log_dir\", STRING, \"The absolute log directory path.\"),\n-                    new Field(\"topics\", new ArrayOf(new Schema(\n-                            TOPIC_NAME,\n-                            new Field(\"partitions\", new ArrayOf(INT32), \"List of partition ids of the topic.\"))))))));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V1 = ALTER_REPLICA_LOG_DIRS_REQUEST_V0;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{ALTER_REPLICA_LOG_DIRS_REQUEST_V0, ALTER_REPLICA_LOG_DIRS_REQUEST_V1};\n-    }\n-\n-    private final Map<TopicPartition, String> partitionDirs;\n+    private final AlterReplicaLogDirsRequestData data;\n \n     public static class Builder extends AbstractRequest.Builder<AlterReplicaLogDirsRequest> {\n-        private final Map<TopicPartition, String> partitionDirs;\n+        private final AlterReplicaLogDirsRequestData data;\n \n-        public Builder(Map<TopicPartition, String> partitionDirs) {\n+        public Builder(AlterReplicaLogDirsRequestData data) {\n             super(ApiKeys.ALTER_REPLICA_LOG_DIRS);\n-            this.partitionDirs = partitionDirs;\n+            this.data = data;\n         }\n \n         @Override\n         public AlterReplicaLogDirsRequest build(short version) {\n-            return new AlterReplicaLogDirsRequest(partitionDirs, version);\n+            return new AlterReplicaLogDirsRequest(data, version);\n         }\n \n         @Override\n         public String toString() {\n-            StringBuilder builder = new StringBuilder();\n-            builder.append(\"(type=AlterReplicaLogDirsRequest\")\n-                .append(\", partitionDirs=\")\n-                .append(partitionDirs)\n-                .append(\")\");\n-            return builder.toString();\n+            return data.toString();\n         }\n     }\n \n     public AlterReplicaLogDirsRequest(Struct struct, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        partitionDirs = new HashMap<>();\n-        for (Object logDirStructObj : struct.getArray(LOG_DIRS_KEY_NAME)) {\n-            Struct logDirStruct = (Struct) logDirStructObj;\n-            String logDir = logDirStruct.getString(LOG_DIR_KEY_NAME);\n-            for (Object topicStructObj : logDirStruct.getArray(TOPICS_KEY_NAME)) {\n-                Struct topicStruct = (Struct) topicStructObj;\n-                String topic = topicStruct.get(TOPIC_NAME);\n-                for (Object partitionObj : topicStruct.getArray(PARTITIONS_KEY_NAME)) {\n-                    int partition = (Integer) partitionObj;\n-                    partitionDirs.put(new TopicPartition(topic, partition), logDir);\n-                }\n-            }\n-        }\n+        this.data = new AlterReplicaLogDirsRequestData(struct, version);\n     }\n \n-    public AlterReplicaLogDirsRequest(Map<TopicPartition, String> partitionDirs, short version) {\n+    public AlterReplicaLogDirsRequest(AlterReplicaLogDirsRequestData data, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        this.partitionDirs = partitionDirs;\n+        this.data = data;\n     }\n \n     @Override\n     protected Struct toStruct() {\n-        Map<String, List<TopicPartition>> dirPartitions = new HashMap<>();\n-        for (Map.Entry<TopicPartition, String> entry: partitionDirs.entrySet()) {\n-            if (!dirPartitions.containsKey(entry.getValue()))\n-                dirPartitions.put(entry.getValue(), new ArrayList<>());\n-            dirPartitions.get(entry.getValue()).add(entry.getKey());\n-        }\n-\n-        Struct struct = new Struct(ApiKeys.ALTER_REPLICA_LOG_DIRS.requestSchema(version()));\n-        List<Struct> logDirStructArray = new ArrayList<>();\n-        for (Map.Entry<String, List<TopicPartition>> logDirEntry: dirPartitions.entrySet()) {\n-            Struct logDirStruct = struct.instance(LOG_DIRS_KEY_NAME);\n-            logDirStruct.set(LOG_DIR_KEY_NAME, logDirEntry.getKey());\n-\n-            List<Struct> topicStructArray = new ArrayList<>();\n-            for (Map.Entry<String, List<Integer>> topicEntry: CollectionUtils.groupPartitionsByTopic(logDirEntry.getValue()).entrySet()) {\n-                Struct topicStruct = logDirStruct.instance(TOPICS_KEY_NAME);\n-                topicStruct.set(TOPIC_NAME, topicEntry.getKey());\n-                topicStruct.set(PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());\n-                topicStructArray.add(topicStruct);\n-            }\n-            logDirStruct.set(TOPICS_KEY_NAME, topicStructArray.toArray());\n-            logDirStructArray.add(logDirStruct);\n-        }\n-        struct.set(LOG_DIRS_KEY_NAME, logDirStructArray.toArray());\n-        return struct;\n+        return data.toStruct(version());\n     }\n \n     @Override\n-    public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n-        Map<TopicPartition, Errors> responseMap = new HashMap<>();\n-        for (Map.Entry<TopicPartition, String> entry : partitionDirs.entrySet()) {\n-            responseMap.put(entry.getKey(), Errors.forException(e));\n-        }\n-        return new AlterReplicaLogDirsResponse(throttleTimeMs, responseMap);\n+    public AlterReplicaLogDirsResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n+        AlterReplicaLogDirsResponseData data = new AlterReplicaLogDirsResponseData();\n+\n+        data.setResults(this.data.dirs().stream().flatMap(x -> {\n+            Stream<AlterReplicaLogDirTopicResult> y = x.topics().stream().map(topic -> {\n+                return new AlterReplicaLogDirTopicResult()\n+                        .setTopicName(topic.name())\n+                        .setPartitions(\n+                                topic.partitions().stream().map(partitionId -> {\n+                                    return new AlterReplicaLogDirsResponseData.AlterReplicaLogDirPartitionResult()\n+                                            .setErrorCode(Errors.forException(e).code())\n+                                            .setPartitionIndex(partitionId);\n+                                }).collect(Collectors.toList()));\n+            });\n+            return y;\n+        }).collect(Collectors.toList()));\n+        return new AlterReplicaLogDirsResponse(data.setThrottleTimeMs(throttleTimeMs));\n     }\n \n     public Map<TopicPartition, String> partitionDirs() {\n-        return partitionDirs;\n+        Map<TopicPartition, String> result = new HashMap<>();\n+        for (AlterReplicaLogDirsRequestData.AlterReplicaLogDir alter : data.dirs()) {\n+            alter.topics().stream().flatMap(t -> {\n+                Stream<TopicPartition> objectStream = t.partitions().stream().map(\n+                    p -> new TopicPartition(t.name(), p.intValue()));\n+                return objectStream;\n+            }).forEach(tp -> result.put(tp, alter.path()));\n+        }\n+        return result;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI0NDM2Mg==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r406244362", "bodyText": "Could we please use better names than x and y?", "author": "dajac", "createdAt": "2020-04-09T14:26:58Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java", "diffHunk": "@@ -18,140 +18,87 @@\n package org.apache.kafka.common.requests;\n \n import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n import org.apache.kafka.common.protocol.ApiKeys;\n import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n \n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Map;\n-\n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-import static org.apache.kafka.common.protocol.types.Type.INT32;\n-import static org.apache.kafka.common.protocol.types.Type.STRING;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n \n public class AlterReplicaLogDirsRequest extends AbstractRequest {\n \n-    // request level key names\n-    private static final String LOG_DIRS_KEY_NAME = \"log_dirs\";\n-\n-    // log dir level key names\n-    private static final String LOG_DIR_KEY_NAME = \"log_dir\";\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n-\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V0 = new Schema(\n-            new Field(\"log_dirs\", new ArrayOf(new Schema(\n-                    new Field(\"log_dir\", STRING, \"The absolute log directory path.\"),\n-                    new Field(\"topics\", new ArrayOf(new Schema(\n-                            TOPIC_NAME,\n-                            new Field(\"partitions\", new ArrayOf(INT32), \"List of partition ids of the topic.\"))))))));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V1 = ALTER_REPLICA_LOG_DIRS_REQUEST_V0;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{ALTER_REPLICA_LOG_DIRS_REQUEST_V0, ALTER_REPLICA_LOG_DIRS_REQUEST_V1};\n-    }\n-\n-    private final Map<TopicPartition, String> partitionDirs;\n+    private final AlterReplicaLogDirsRequestData data;\n \n     public static class Builder extends AbstractRequest.Builder<AlterReplicaLogDirsRequest> {\n-        private final Map<TopicPartition, String> partitionDirs;\n+        private final AlterReplicaLogDirsRequestData data;\n \n-        public Builder(Map<TopicPartition, String> partitionDirs) {\n+        public Builder(AlterReplicaLogDirsRequestData data) {\n             super(ApiKeys.ALTER_REPLICA_LOG_DIRS);\n-            this.partitionDirs = partitionDirs;\n+            this.data = data;\n         }\n \n         @Override\n         public AlterReplicaLogDirsRequest build(short version) {\n-            return new AlterReplicaLogDirsRequest(partitionDirs, version);\n+            return new AlterReplicaLogDirsRequest(data, version);\n         }\n \n         @Override\n         public String toString() {\n-            StringBuilder builder = new StringBuilder();\n-            builder.append(\"(type=AlterReplicaLogDirsRequest\")\n-                .append(\", partitionDirs=\")\n-                .append(partitionDirs)\n-                .append(\")\");\n-            return builder.toString();\n+            return data.toString();\n         }\n     }\n \n     public AlterReplicaLogDirsRequest(Struct struct, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        partitionDirs = new HashMap<>();\n-        for (Object logDirStructObj : struct.getArray(LOG_DIRS_KEY_NAME)) {\n-            Struct logDirStruct = (Struct) logDirStructObj;\n-            String logDir = logDirStruct.getString(LOG_DIR_KEY_NAME);\n-            for (Object topicStructObj : logDirStruct.getArray(TOPICS_KEY_NAME)) {\n-                Struct topicStruct = (Struct) topicStructObj;\n-                String topic = topicStruct.get(TOPIC_NAME);\n-                for (Object partitionObj : topicStruct.getArray(PARTITIONS_KEY_NAME)) {\n-                    int partition = (Integer) partitionObj;\n-                    partitionDirs.put(new TopicPartition(topic, partition), logDir);\n-                }\n-            }\n-        }\n+        this.data = new AlterReplicaLogDirsRequestData(struct, version);\n     }\n \n-    public AlterReplicaLogDirsRequest(Map<TopicPartition, String> partitionDirs, short version) {\n+    public AlterReplicaLogDirsRequest(AlterReplicaLogDirsRequestData data, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        this.partitionDirs = partitionDirs;\n+        this.data = data;\n     }\n \n     @Override\n     protected Struct toStruct() {\n-        Map<String, List<TopicPartition>> dirPartitions = new HashMap<>();\n-        for (Map.Entry<TopicPartition, String> entry: partitionDirs.entrySet()) {\n-            if (!dirPartitions.containsKey(entry.getValue()))\n-                dirPartitions.put(entry.getValue(), new ArrayList<>());\n-            dirPartitions.get(entry.getValue()).add(entry.getKey());\n-        }\n-\n-        Struct struct = new Struct(ApiKeys.ALTER_REPLICA_LOG_DIRS.requestSchema(version()));\n-        List<Struct> logDirStructArray = new ArrayList<>();\n-        for (Map.Entry<String, List<TopicPartition>> logDirEntry: dirPartitions.entrySet()) {\n-            Struct logDirStruct = struct.instance(LOG_DIRS_KEY_NAME);\n-            logDirStruct.set(LOG_DIR_KEY_NAME, logDirEntry.getKey());\n-\n-            List<Struct> topicStructArray = new ArrayList<>();\n-            for (Map.Entry<String, List<Integer>> topicEntry: CollectionUtils.groupPartitionsByTopic(logDirEntry.getValue()).entrySet()) {\n-                Struct topicStruct = logDirStruct.instance(TOPICS_KEY_NAME);\n-                topicStruct.set(TOPIC_NAME, topicEntry.getKey());\n-                topicStruct.set(PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());\n-                topicStructArray.add(topicStruct);\n-            }\n-            logDirStruct.set(TOPICS_KEY_NAME, topicStructArray.toArray());\n-            logDirStructArray.add(logDirStruct);\n-        }\n-        struct.set(LOG_DIRS_KEY_NAME, logDirStructArray.toArray());\n-        return struct;\n+        return data.toStruct(version());\n     }\n \n     @Override\n-    public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n-        Map<TopicPartition, Errors> responseMap = new HashMap<>();\n-        for (Map.Entry<TopicPartition, String> entry : partitionDirs.entrySet()) {\n-            responseMap.put(entry.getKey(), Errors.forException(e));\n-        }\n-        return new AlterReplicaLogDirsResponse(throttleTimeMs, responseMap);\n+    public AlterReplicaLogDirsResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n+        AlterReplicaLogDirsResponseData data = new AlterReplicaLogDirsResponseData();\n+\n+        data.setResults(this.data.dirs().stream().flatMap(x -> {\n+            Stream<AlterReplicaLogDirTopicResult> y = x.topics().stream().map(topic -> {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI0NTg2NQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r406245865", "bodyText": "nit: What about putting this return at L77 directly?", "author": "dajac", "createdAt": "2020-04-09T14:29:04Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java", "diffHunk": "@@ -18,140 +18,87 @@\n package org.apache.kafka.common.requests;\n \n import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n import org.apache.kafka.common.protocol.ApiKeys;\n import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n \n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Map;\n-\n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-import static org.apache.kafka.common.protocol.types.Type.INT32;\n-import static org.apache.kafka.common.protocol.types.Type.STRING;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n \n public class AlterReplicaLogDirsRequest extends AbstractRequest {\n \n-    // request level key names\n-    private static final String LOG_DIRS_KEY_NAME = \"log_dirs\";\n-\n-    // log dir level key names\n-    private static final String LOG_DIR_KEY_NAME = \"log_dir\";\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n-\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V0 = new Schema(\n-            new Field(\"log_dirs\", new ArrayOf(new Schema(\n-                    new Field(\"log_dir\", STRING, \"The absolute log directory path.\"),\n-                    new Field(\"topics\", new ArrayOf(new Schema(\n-                            TOPIC_NAME,\n-                            new Field(\"partitions\", new ArrayOf(INT32), \"List of partition ids of the topic.\"))))))));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V1 = ALTER_REPLICA_LOG_DIRS_REQUEST_V0;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{ALTER_REPLICA_LOG_DIRS_REQUEST_V0, ALTER_REPLICA_LOG_DIRS_REQUEST_V1};\n-    }\n-\n-    private final Map<TopicPartition, String> partitionDirs;\n+    private final AlterReplicaLogDirsRequestData data;\n \n     public static class Builder extends AbstractRequest.Builder<AlterReplicaLogDirsRequest> {\n-        private final Map<TopicPartition, String> partitionDirs;\n+        private final AlterReplicaLogDirsRequestData data;\n \n-        public Builder(Map<TopicPartition, String> partitionDirs) {\n+        public Builder(AlterReplicaLogDirsRequestData data) {\n             super(ApiKeys.ALTER_REPLICA_LOG_DIRS);\n-            this.partitionDirs = partitionDirs;\n+            this.data = data;\n         }\n \n         @Override\n         public AlterReplicaLogDirsRequest build(short version) {\n-            return new AlterReplicaLogDirsRequest(partitionDirs, version);\n+            return new AlterReplicaLogDirsRequest(data, version);\n         }\n \n         @Override\n         public String toString() {\n-            StringBuilder builder = new StringBuilder();\n-            builder.append(\"(type=AlterReplicaLogDirsRequest\")\n-                .append(\", partitionDirs=\")\n-                .append(partitionDirs)\n-                .append(\")\");\n-            return builder.toString();\n+            return data.toString();\n         }\n     }\n \n     public AlterReplicaLogDirsRequest(Struct struct, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        partitionDirs = new HashMap<>();\n-        for (Object logDirStructObj : struct.getArray(LOG_DIRS_KEY_NAME)) {\n-            Struct logDirStruct = (Struct) logDirStructObj;\n-            String logDir = logDirStruct.getString(LOG_DIR_KEY_NAME);\n-            for (Object topicStructObj : logDirStruct.getArray(TOPICS_KEY_NAME)) {\n-                Struct topicStruct = (Struct) topicStructObj;\n-                String topic = topicStruct.get(TOPIC_NAME);\n-                for (Object partitionObj : topicStruct.getArray(PARTITIONS_KEY_NAME)) {\n-                    int partition = (Integer) partitionObj;\n-                    partitionDirs.put(new TopicPartition(topic, partition), logDir);\n-                }\n-            }\n-        }\n+        this.data = new AlterReplicaLogDirsRequestData(struct, version);\n     }\n \n-    public AlterReplicaLogDirsRequest(Map<TopicPartition, String> partitionDirs, short version) {\n+    public AlterReplicaLogDirsRequest(AlterReplicaLogDirsRequestData data, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        this.partitionDirs = partitionDirs;\n+        this.data = data;\n     }\n \n     @Override\n     protected Struct toStruct() {\n-        Map<String, List<TopicPartition>> dirPartitions = new HashMap<>();\n-        for (Map.Entry<TopicPartition, String> entry: partitionDirs.entrySet()) {\n-            if (!dirPartitions.containsKey(entry.getValue()))\n-                dirPartitions.put(entry.getValue(), new ArrayList<>());\n-            dirPartitions.get(entry.getValue()).add(entry.getKey());\n-        }\n-\n-        Struct struct = new Struct(ApiKeys.ALTER_REPLICA_LOG_DIRS.requestSchema(version()));\n-        List<Struct> logDirStructArray = new ArrayList<>();\n-        for (Map.Entry<String, List<TopicPartition>> logDirEntry: dirPartitions.entrySet()) {\n-            Struct logDirStruct = struct.instance(LOG_DIRS_KEY_NAME);\n-            logDirStruct.set(LOG_DIR_KEY_NAME, logDirEntry.getKey());\n-\n-            List<Struct> topicStructArray = new ArrayList<>();\n-            for (Map.Entry<String, List<Integer>> topicEntry: CollectionUtils.groupPartitionsByTopic(logDirEntry.getValue()).entrySet()) {\n-                Struct topicStruct = logDirStruct.instance(TOPICS_KEY_NAME);\n-                topicStruct.set(TOPIC_NAME, topicEntry.getKey());\n-                topicStruct.set(PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());\n-                topicStructArray.add(topicStruct);\n-            }\n-            logDirStruct.set(TOPICS_KEY_NAME, topicStructArray.toArray());\n-            logDirStructArray.add(logDirStruct);\n-        }\n-        struct.set(LOG_DIRS_KEY_NAME, logDirStructArray.toArray());\n-        return struct;\n+        return data.toStruct(version());\n     }\n \n     @Override\n-    public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n-        Map<TopicPartition, Errors> responseMap = new HashMap<>();\n-        for (Map.Entry<TopicPartition, String> entry : partitionDirs.entrySet()) {\n-            responseMap.put(entry.getKey(), Errors.forException(e));\n-        }\n-        return new AlterReplicaLogDirsResponse(throttleTimeMs, responseMap);\n+    public AlterReplicaLogDirsResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n+        AlterReplicaLogDirsResponseData data = new AlterReplicaLogDirsResponseData();\n+\n+        data.setResults(this.data.dirs().stream().flatMap(x -> {\n+            Stream<AlterReplicaLogDirTopicResult> y = x.topics().stream().map(topic -> {\n+                return new AlterReplicaLogDirTopicResult()\n+                        .setTopicName(topic.name())\n+                        .setPartitions(\n+                                topic.partitions().stream().map(partitionId -> {\n+                                    return new AlterReplicaLogDirsResponseData.AlterReplicaLogDirPartitionResult()\n+                                            .setErrorCode(Errors.forException(e).code())\n+                                            .setPartitionIndex(partitionId);\n+                                }).collect(Collectors.toList()));\n+            });\n+            return y;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI1MDQxMQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r406250411", "bodyText": "nit: { } and return are not necessary in one-line lambda bodies. I would remove them here and in the other places to be consistent with rest of the code base.", "author": "dajac", "createdAt": "2020-04-09T14:35:16Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java", "diffHunk": "@@ -18,140 +18,87 @@\n package org.apache.kafka.common.requests;\n \n import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n import org.apache.kafka.common.protocol.ApiKeys;\n import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n \n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Map;\n-\n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-import static org.apache.kafka.common.protocol.types.Type.INT32;\n-import static org.apache.kafka.common.protocol.types.Type.STRING;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n \n public class AlterReplicaLogDirsRequest extends AbstractRequest {\n \n-    // request level key names\n-    private static final String LOG_DIRS_KEY_NAME = \"log_dirs\";\n-\n-    // log dir level key names\n-    private static final String LOG_DIR_KEY_NAME = \"log_dir\";\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n-\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V0 = new Schema(\n-            new Field(\"log_dirs\", new ArrayOf(new Schema(\n-                    new Field(\"log_dir\", STRING, \"The absolute log directory path.\"),\n-                    new Field(\"topics\", new ArrayOf(new Schema(\n-                            TOPIC_NAME,\n-                            new Field(\"partitions\", new ArrayOf(INT32), \"List of partition ids of the topic.\"))))))));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V1 = ALTER_REPLICA_LOG_DIRS_REQUEST_V0;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{ALTER_REPLICA_LOG_DIRS_REQUEST_V0, ALTER_REPLICA_LOG_DIRS_REQUEST_V1};\n-    }\n-\n-    private final Map<TopicPartition, String> partitionDirs;\n+    private final AlterReplicaLogDirsRequestData data;\n \n     public static class Builder extends AbstractRequest.Builder<AlterReplicaLogDirsRequest> {\n-        private final Map<TopicPartition, String> partitionDirs;\n+        private final AlterReplicaLogDirsRequestData data;\n \n-        public Builder(Map<TopicPartition, String> partitionDirs) {\n+        public Builder(AlterReplicaLogDirsRequestData data) {\n             super(ApiKeys.ALTER_REPLICA_LOG_DIRS);\n-            this.partitionDirs = partitionDirs;\n+            this.data = data;\n         }\n \n         @Override\n         public AlterReplicaLogDirsRequest build(short version) {\n-            return new AlterReplicaLogDirsRequest(partitionDirs, version);\n+            return new AlterReplicaLogDirsRequest(data, version);\n         }\n \n         @Override\n         public String toString() {\n-            StringBuilder builder = new StringBuilder();\n-            builder.append(\"(type=AlterReplicaLogDirsRequest\")\n-                .append(\", partitionDirs=\")\n-                .append(partitionDirs)\n-                .append(\")\");\n-            return builder.toString();\n+            return data.toString();\n         }\n     }\n \n     public AlterReplicaLogDirsRequest(Struct struct, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        partitionDirs = new HashMap<>();\n-        for (Object logDirStructObj : struct.getArray(LOG_DIRS_KEY_NAME)) {\n-            Struct logDirStruct = (Struct) logDirStructObj;\n-            String logDir = logDirStruct.getString(LOG_DIR_KEY_NAME);\n-            for (Object topicStructObj : logDirStruct.getArray(TOPICS_KEY_NAME)) {\n-                Struct topicStruct = (Struct) topicStructObj;\n-                String topic = topicStruct.get(TOPIC_NAME);\n-                for (Object partitionObj : topicStruct.getArray(PARTITIONS_KEY_NAME)) {\n-                    int partition = (Integer) partitionObj;\n-                    partitionDirs.put(new TopicPartition(topic, partition), logDir);\n-                }\n-            }\n-        }\n+        this.data = new AlterReplicaLogDirsRequestData(struct, version);\n     }\n \n-    public AlterReplicaLogDirsRequest(Map<TopicPartition, String> partitionDirs, short version) {\n+    public AlterReplicaLogDirsRequest(AlterReplicaLogDirsRequestData data, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        this.partitionDirs = partitionDirs;\n+        this.data = data;\n     }\n \n     @Override\n     protected Struct toStruct() {\n-        Map<String, List<TopicPartition>> dirPartitions = new HashMap<>();\n-        for (Map.Entry<TopicPartition, String> entry: partitionDirs.entrySet()) {\n-            if (!dirPartitions.containsKey(entry.getValue()))\n-                dirPartitions.put(entry.getValue(), new ArrayList<>());\n-            dirPartitions.get(entry.getValue()).add(entry.getKey());\n-        }\n-\n-        Struct struct = new Struct(ApiKeys.ALTER_REPLICA_LOG_DIRS.requestSchema(version()));\n-        List<Struct> logDirStructArray = new ArrayList<>();\n-        for (Map.Entry<String, List<TopicPartition>> logDirEntry: dirPartitions.entrySet()) {\n-            Struct logDirStruct = struct.instance(LOG_DIRS_KEY_NAME);\n-            logDirStruct.set(LOG_DIR_KEY_NAME, logDirEntry.getKey());\n-\n-            List<Struct> topicStructArray = new ArrayList<>();\n-            for (Map.Entry<String, List<Integer>> topicEntry: CollectionUtils.groupPartitionsByTopic(logDirEntry.getValue()).entrySet()) {\n-                Struct topicStruct = logDirStruct.instance(TOPICS_KEY_NAME);\n-                topicStruct.set(TOPIC_NAME, topicEntry.getKey());\n-                topicStruct.set(PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());\n-                topicStructArray.add(topicStruct);\n-            }\n-            logDirStruct.set(TOPICS_KEY_NAME, topicStructArray.toArray());\n-            logDirStructArray.add(logDirStruct);\n-        }\n-        struct.set(LOG_DIRS_KEY_NAME, logDirStructArray.toArray());\n-        return struct;\n+        return data.toStruct(version());\n     }\n \n     @Override\n-    public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n-        Map<TopicPartition, Errors> responseMap = new HashMap<>();\n-        for (Map.Entry<TopicPartition, String> entry : partitionDirs.entrySet()) {\n-            responseMap.put(entry.getKey(), Errors.forException(e));\n-        }\n-        return new AlterReplicaLogDirsResponse(throttleTimeMs, responseMap);\n+    public AlterReplicaLogDirsResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n+        AlterReplicaLogDirsResponseData data = new AlterReplicaLogDirsResponseData();\n+\n+        data.setResults(this.data.dirs().stream().flatMap(x -> {\n+            Stream<AlterReplicaLogDirTopicResult> y = x.topics().stream().map(topic -> {\n+                return new AlterReplicaLogDirTopicResult()\n+                        .setTopicName(topic.name())\n+                        .setPartitions(\n+                                topic.partitions().stream().map(partitionId -> {\n+                                    return new AlterReplicaLogDirsResponseData.AlterReplicaLogDirPartitionResult()\n+                                            .setErrorCode(Errors.forException(e).code())\n+                                            .setPartitionIndex(partitionId);\n+                                }).collect(Collectors.toList()));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI1MTUwNA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r406251504", "bodyText": "nit: Could we rename x and y here as well?", "author": "dajac", "createdAt": "2020-04-09T14:36:48Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsResponse.java", "diffHunk": "@@ -17,122 +17,59 @@\n \n package org.apache.kafka.common.requests;\n \n-import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n import org.apache.kafka.common.protocol.ApiKeys;\n import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n \n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Map;\n \n-import static org.apache.kafka.common.protocol.CommonFields.ERROR_CODE;\n-import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;\n-import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;\n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-\n \n public class AlterReplicaLogDirsResponse extends AbstractResponse {\n \n-    // request level key names\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n-\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_RESPONSE_V0 = new Schema(\n-            THROTTLE_TIME_MS,\n-            new Field(TOPICS_KEY_NAME, new ArrayOf(new Schema(\n-                    TOPIC_NAME,\n-                    new Field(PARTITIONS_KEY_NAME, new ArrayOf(new Schema(\n-                            PARTITION_ID,\n-                            ERROR_CODE)))))));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_RESPONSE_V1 = ALTER_REPLICA_LOG_DIRS_RESPONSE_V0;\n+    private final AlterReplicaLogDirsResponseData data;\n \n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{ALTER_REPLICA_LOG_DIRS_RESPONSE_V0, ALTER_REPLICA_LOG_DIRS_RESPONSE_V1};\n+    public AlterReplicaLogDirsResponse(Struct struct) {\n+        this(struct, ApiKeys.ALTER_REPLICA_LOG_DIRS.latestVersion());\n     }\n \n-    /**\n-     * Possible error code:\n-     *\n-     * LOG_DIR_NOT_FOUND (57)\n-     * KAFKA_STORAGE_ERROR (56)\n-     * REPLICA_NOT_AVAILABLE (9)\n-     * UNKNOWN (-1)\n-     */\n-    private final Map<TopicPartition, Errors> responses;\n-    private final int throttleTimeMs;\n-\n-    public AlterReplicaLogDirsResponse(Struct struct) {\n-        throttleTimeMs = struct.get(THROTTLE_TIME_MS);\n-        responses = new HashMap<>();\n-        for (Object topicStructObj : struct.getArray(TOPICS_KEY_NAME)) {\n-            Struct topicStruct = (Struct) topicStructObj;\n-            String topic = topicStruct.get(TOPIC_NAME);\n-            for (Object partitionStructObj : topicStruct.getArray(PARTITIONS_KEY_NAME)) {\n-                Struct partitionStruct = (Struct) partitionStructObj;\n-                int partition = partitionStruct.get(PARTITION_ID);\n-                Errors error = Errors.forCode(partitionStruct.get(ERROR_CODE));\n-                responses.put(new TopicPartition(topic, partition), error);\n-            }\n-        }\n+    public AlterReplicaLogDirsResponse(Struct struct, short version) {\n+        this.data = new AlterReplicaLogDirsResponseData(struct, version);\n     }\n \n     /**\n      * Constructor for version 0.\n      */\n-    public AlterReplicaLogDirsResponse(int throttleTimeMs, Map<TopicPartition, Errors> responses) {\n-        this.throttleTimeMs = throttleTimeMs;\n-        this.responses = responses;\n+    public AlterReplicaLogDirsResponse(AlterReplicaLogDirsResponseData data) {\n+        this.data = data;\n+    }\n+\n+    public AlterReplicaLogDirsResponseData data() {\n+        return data;\n     }\n \n     @Override\n     protected Struct toStruct(short version) {\n-        Struct struct = new Struct(ApiKeys.ALTER_REPLICA_LOG_DIRS.responseSchema(version));\n-        struct.set(THROTTLE_TIME_MS, throttleTimeMs);\n-        Map<String, Map<Integer, Errors>> responsesByTopic = CollectionUtils.groupPartitionDataByTopic(responses);\n-        List<Struct> topicStructArray = new ArrayList<>();\n-        for (Map.Entry<String, Map<Integer, Errors>> responsesByTopicEntry : responsesByTopic.entrySet()) {\n-            Struct topicStruct = struct.instance(TOPICS_KEY_NAME);\n-            topicStruct.set(TOPIC_NAME, responsesByTopicEntry.getKey());\n-            List<Struct> partitionStructArray = new ArrayList<>();\n-            for (Map.Entry<Integer, Errors> responsesByPartitionEntry : responsesByTopicEntry.getValue().entrySet()) {\n-                Struct partitionStruct = topicStruct.instance(PARTITIONS_KEY_NAME);\n-                Errors response = responsesByPartitionEntry.getValue();\n-                partitionStruct.set(PARTITION_ID, responsesByPartitionEntry.getKey());\n-                partitionStruct.set(ERROR_CODE, response.code());\n-                partitionStructArray.add(partitionStruct);\n-            }\n-            topicStruct.set(PARTITIONS_KEY_NAME, partitionStructArray.toArray());\n-            topicStructArray.add(topicStruct);\n-        }\n-        struct.set(TOPICS_KEY_NAME, topicStructArray.toArray());\n-        return struct;\n+        return data.toStruct(version);\n     }\n \n     @Override\n     public int throttleTimeMs() {\n-        return throttleTimeMs;\n-    }\n-\n-    public Map<TopicPartition, Errors> responses() {\n-        return this.responses;\n+        return data.throttleTimeMs();\n     }\n \n     @Override\n     public Map<Errors, Integer> errorCounts() {\n-        return errorCounts(responses.values());\n+        Map<Errors, Integer> errorCounts = new HashMap<>();\n+        for (AlterReplicaLogDirTopicResult x : data.results()) {\n+            for (AlterReplicaLogDirsResponseData.AlterReplicaLogDirPartitionResult y : x.partitions()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI2MjMyMQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r406262321", "bodyText": "I would prefer if we could keep the previous structure where authorize and replicaManager.alterReplicaLogDirs were called before the sendResponseMaybeThrottle. Having everything in the callback, coupled with the response construction, makes the code hard to read.", "author": "dajac", "createdAt": "2020-04-09T14:51:27Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2555,13 +2555,26 @@ class KafkaApis(val requestChannel: RequestChannel,\n \n   def handleAlterReplicaLogDirsRequest(request: RequestChannel.Request): Unit = {\n     val alterReplicaDirsRequest = request.body[AlterReplicaLogDirsRequest]\n-    val responseMap = {\n+    def sendResponseCallback(requestThrottleMs: Int): AlterReplicaLogDirsResponse = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMyNjk5NQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r406326995", "bodyText": "I changed it because both branches of the if authorize() ... require the requestThrottleMs to set on the AlterReplicaLogDirsResponseData. I can set the throttle outside the if with something like\nsendResponseMaybeThrottle(request, requestThrottleMs => {\n      response.data().setThrottleTimeMs(requestThrottleMs)\n      response\n    })\nIt would then be more natural to call the alterReplicaDirsRequest.getErrorResponse(Errors.CLUSTER_AUTHORIZATION_FAILED.exception) which doesn't take a throttle (since we'll be setting it later), but that requires overriding that method so that the return type is not AbstractResponse.\nIt's a bit unsatisfying, but did you have a better idea?", "author": "tombentley", "createdAt": "2020-04-09T16:28:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI2MjMyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE4MzEwMQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r408183101", "bodyText": "I was thinking about something like this:\n  def handleAlterReplicaLogDirsRequest(request: RequestChannel.Request): Unit = {\n    val alterReplicaDirsRequest = request.body[AlterReplicaLogDirsRequest]\n    if (authorize(request.context, ALTER, CLUSTER, CLUSTER_NAME)) {\n      val result = replicaManager.alterReplicaLogDirs(alterReplicaDirsRequest.partitionDirs.asScala)\n      sendResponseMaybeThrottle(request, requestThrottleMs =>\n        new AlterReplicaLogDirsResponse(new AlterReplicaLogDirsResponseData()\n          .setResults(result.groupBy(_._1.topic).map {\n            case (topic, errors) => new AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult()\n              .setTopicName(topic)\n              .setPartitions(errors.map { \n                case (tp, error) => new AlterReplicaLogDirsResponseData.AlterReplicaLogDirPartitionResult()\n                  .setPartitionIndex(tp.partition)\n                  .setErrorCode(error.code)\n              }.toList.asJava)\n            }.toList.asJava)\n          .setThrottleTimeMs(requestThrottleMs))\n    } else {\n      sendResponseMaybeThrottle(request, requestThrottleMs =>\n        alterReplicaDirsRequest.getErrorResponse(requestThrottleMs, Errors.CLUSTER_AUTHORIZATION_FAILED.exception))\n    }\n  }\n\nI find it a bit easier to read and it is also a bit more consistent with the implementation of the other handlers.", "author": "dajac", "createdAt": "2020-04-14T14:31:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI2MjMyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI2MzQ3Nw==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r406263477", "bodyText": "While we are refactoring this, could we add a unit test in KafkaApisTest?", "author": "dajac", "createdAt": "2020-04-09T14:52:57Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2555,13 +2555,26 @@ class KafkaApis(val requestChannel: RequestChannel,\n \n   def handleAlterReplicaLogDirsRequest(request: RequestChannel.Request): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI2NjczMQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r406266731", "bodyText": "nit: x", "author": "dajac", "createdAt": "2020-04-09T14:57:11Z", "path": "core/src/test/scala/unit/kafka/server/AlterReplicaLogDirsRequestTest.scala", "diffHunk": "@@ -104,13 +112,26 @@ class AlterReplicaLogDirsRequestTest extends BaseRequestTest {\n     partitionDirs3.put(new TopicPartition(topic, 1), validDir3)\n     partitionDirs3.put(new TopicPartition(topic, 2), offlineDir)\n     val alterReplicaDirResponse3 = sendAlterReplicaLogDirsRequest(partitionDirs3.toMap)\n-    assertEquals(Errors.LOG_DIR_NOT_FOUND, alterReplicaDirResponse3.responses().get(new TopicPartition(topic, 0)))\n-    assertEquals(Errors.KAFKA_STORAGE_ERROR, alterReplicaDirResponse3.responses().get(new TopicPartition(topic, 1)))\n-    assertEquals(Errors.KAFKA_STORAGE_ERROR, alterReplicaDirResponse3.responses().get(new TopicPartition(topic, 2)))\n+    assertEquals(Errors.LOG_DIR_NOT_FOUND, findErrorForPartition(alterReplicaDirResponse3, new TopicPartition(topic, 0)))\n+    assertEquals(Errors.KAFKA_STORAGE_ERROR, findErrorForPartition(alterReplicaDirResponse3, new TopicPartition(topic, 1)))\n+    assertEquals(Errors.KAFKA_STORAGE_ERROR, findErrorForPartition(alterReplicaDirResponse3, new TopicPartition(topic, 2)))\n   }\n \n   private def sendAlterReplicaLogDirsRequest(partitionDirs: Map[TopicPartition, String]): AlterReplicaLogDirsResponse = {\n-    val request = new AlterReplicaLogDirsRequest.Builder(partitionDirs.asJava).build()\n+    val x = partitionDirs.groupBy{case (tp, dir) => dir}.map{ case(dir, tps) =>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI3MTcxNg==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r406271716", "bodyText": "Would it make sense to add unit tests in KafkaAdminClientTest to cover those changes?", "author": "dajac", "createdAt": "2020-04-09T15:04:13Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -69,6 +69,11 @@\n import org.apache.kafka.common.internals.KafkaFutureImpl;\n import org.apache.kafka.common.message.AlterPartitionReassignmentsRequestData;\n import org.apache.kafka.common.message.AlterPartitionReassignmentsRequestData.ReassignableTopic;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI3MjE5OA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r406272198", "bodyText": "nit: Alignment is not consistent with the others.", "author": "dajac", "createdAt": "2020-04-09T15:04:59Z", "path": "clients/src/main/java/org/apache/kafka/common/protocol/ApiKeys.java", "diffHunk": "@@ -188,8 +188,8 @@ public Struct parseResponse(short version, ByteBuffer buffer) {\n             DescribeConfigsResponse.schemaVersions()),\n     ALTER_CONFIGS(33, \"AlterConfigs\", AlterConfigsRequestData.SCHEMAS,\n             AlterConfigsResponseData.SCHEMAS),\n-    ALTER_REPLICA_LOG_DIRS(34, \"AlterReplicaLogDirs\", AlterReplicaLogDirsRequest.schemaVersions(),\n-            AlterReplicaLogDirsResponse.schemaVersions()),\n+    ALTER_REPLICA_LOG_DIRS(34, \"AlterReplicaLogDirs\", AlterReplicaLogDirsRequestData.SCHEMAS,\n+                           AlterReplicaLogDirsResponseData.SCHEMAS),", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjI3NTE5Nw==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r406275197", "bodyText": "I think that we can removed this comment.", "author": "dajac", "createdAt": "2020-04-09T15:09:22Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsResponse.java", "diffHunk": "@@ -17,122 +17,59 @@\n \n package org.apache.kafka.common.requests;\n \n-import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n import org.apache.kafka.common.protocol.ApiKeys;\n import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n \n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Map;\n \n-import static org.apache.kafka.common.protocol.CommonFields.ERROR_CODE;\n-import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;\n-import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;\n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-\n \n public class AlterReplicaLogDirsResponse extends AbstractResponse {\n \n-    // request level key names\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n-\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_RESPONSE_V0 = new Schema(\n-            THROTTLE_TIME_MS,\n-            new Field(TOPICS_KEY_NAME, new ArrayOf(new Schema(\n-                    TOPIC_NAME,\n-                    new Field(PARTITIONS_KEY_NAME, new ArrayOf(new Schema(\n-                            PARTITION_ID,\n-                            ERROR_CODE)))))));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_RESPONSE_V1 = ALTER_REPLICA_LOG_DIRS_RESPONSE_V0;\n+    private final AlterReplicaLogDirsResponseData data;\n \n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{ALTER_REPLICA_LOG_DIRS_RESPONSE_V0, ALTER_REPLICA_LOG_DIRS_RESPONSE_V1};\n+    public AlterReplicaLogDirsResponse(Struct struct) {\n+        this(struct, ApiKeys.ALTER_REPLICA_LOG_DIRS.latestVersion());\n     }\n \n-    /**\n-     * Possible error code:\n-     *\n-     * LOG_DIR_NOT_FOUND (57)\n-     * KAFKA_STORAGE_ERROR (56)\n-     * REPLICA_NOT_AVAILABLE (9)\n-     * UNKNOWN (-1)\n-     */\n-    private final Map<TopicPartition, Errors> responses;\n-    private final int throttleTimeMs;\n-\n-    public AlterReplicaLogDirsResponse(Struct struct) {\n-        throttleTimeMs = struct.get(THROTTLE_TIME_MS);\n-        responses = new HashMap<>();\n-        for (Object topicStructObj : struct.getArray(TOPICS_KEY_NAME)) {\n-            Struct topicStruct = (Struct) topicStructObj;\n-            String topic = topicStruct.get(TOPIC_NAME);\n-            for (Object partitionStructObj : topicStruct.getArray(PARTITIONS_KEY_NAME)) {\n-                Struct partitionStruct = (Struct) partitionStructObj;\n-                int partition = partitionStruct.get(PARTITION_ID);\n-                Errors error = Errors.forCode(partitionStruct.get(ERROR_CODE));\n-                responses.put(new TopicPartition(topic, partition), error);\n-            }\n-        }\n+    public AlterReplicaLogDirsResponse(Struct struct, short version) {\n+        this.data = new AlterReplicaLogDirsResponseData(struct, version);\n     }\n \n     /**\n      * Constructor for version 0.\n      */", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjI4NzU1NA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412287554", "bodyText": "topicPartition is not used except for getting the topic and the partition above. replica.topic() and replica.partition() could be directly used instead.", "author": "dajac", "createdAt": "2020-04-21T15:29:38Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2207,21 +2212,33 @@ public AlterReplicaLogDirsResult alterReplicaLogDirs(Map<TopicPartitionReplica,\n         for (TopicPartitionReplica replica : replicaAssignment.keySet())\n             futures.put(replica, new KafkaFutureImpl<>());\n \n-        Map<Integer, Map<TopicPartition, String>> replicaAssignmentByBroker = new HashMap<>();\n+        Map<Integer, AlterReplicaLogDirsRequestData> replicaAssignmentByBroker = new HashMap<>();\n         for (Map.Entry<TopicPartitionReplica, String> entry: replicaAssignment.entrySet()) {\n             TopicPartitionReplica replica = entry.getKey();\n             String logDir = entry.getValue();\n             int brokerId = replica.brokerId();\n             TopicPartition topicPartition = new TopicPartition(replica.topic(), replica.partition());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjI4ODU5MQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412288591", "bodyText": "nit: Could we rename value to something like alterReplicaLogDirs?", "author": "dajac", "createdAt": "2020-04-21T15:31:04Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2207,21 +2212,33 @@ public AlterReplicaLogDirsResult alterReplicaLogDirs(Map<TopicPartitionReplica,\n         for (TopicPartitionReplica replica : replicaAssignment.keySet())\n             futures.put(replica, new KafkaFutureImpl<>());\n \n-        Map<Integer, Map<TopicPartition, String>> replicaAssignmentByBroker = new HashMap<>();\n+        Map<Integer, AlterReplicaLogDirsRequestData> replicaAssignmentByBroker = new HashMap<>();\n         for (Map.Entry<TopicPartitionReplica, String> entry: replicaAssignment.entrySet()) {\n             TopicPartitionReplica replica = entry.getKey();\n             String logDir = entry.getValue();\n             int brokerId = replica.brokerId();\n             TopicPartition topicPartition = new TopicPartition(replica.topic(), replica.partition());\n-            if (!replicaAssignmentByBroker.containsKey(brokerId))\n-                replicaAssignmentByBroker.put(brokerId, new HashMap<>());\n-            replicaAssignmentByBroker.get(brokerId).put(topicPartition, logDir);\n+            AlterReplicaLogDirsRequestData value = replicaAssignmentByBroker.computeIfAbsent(brokerId,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjMxNzE5OA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412317198", "bodyText": "setName could be done only once within the if statement.", "author": "dajac", "createdAt": "2020-04-21T16:44:24Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2207,21 +2212,33 @@ public AlterReplicaLogDirsResult alterReplicaLogDirs(Map<TopicPartitionReplica,\n         for (TopicPartitionReplica replica : replicaAssignment.keySet())\n             futures.put(replica, new KafkaFutureImpl<>());\n \n-        Map<Integer, Map<TopicPartition, String>> replicaAssignmentByBroker = new HashMap<>();\n+        Map<Integer, AlterReplicaLogDirsRequestData> replicaAssignmentByBroker = new HashMap<>();\n         for (Map.Entry<TopicPartitionReplica, String> entry: replicaAssignment.entrySet()) {\n             TopicPartitionReplica replica = entry.getKey();\n             String logDir = entry.getValue();\n             int brokerId = replica.brokerId();\n             TopicPartition topicPartition = new TopicPartition(replica.topic(), replica.partition());\n-            if (!replicaAssignmentByBroker.containsKey(brokerId))\n-                replicaAssignmentByBroker.put(brokerId, new HashMap<>());\n-            replicaAssignmentByBroker.get(brokerId).put(topicPartition, logDir);\n+            AlterReplicaLogDirsRequestData value = replicaAssignmentByBroker.computeIfAbsent(brokerId,\n+                key -> new AlterReplicaLogDirsRequestData());\n+            AlterReplicaLogDir alterReplicaLogDir = value.dirs().find(logDir);\n+            if (alterReplicaLogDir == null) {\n+                alterReplicaLogDir = new AlterReplicaLogDir();\n+                alterReplicaLogDir.setPath(logDir);\n+                value.dirs().add(alterReplicaLogDir);\n+            }\n+            AlterReplicaLogDirTopic alterReplicaLogDirTopic = alterReplicaLogDir.topics().find(topicPartition.topic());\n+            if (alterReplicaLogDirTopic == null) {\n+                alterReplicaLogDirTopic = new AlterReplicaLogDirTopic();\n+                alterReplicaLogDir.topics().add(alterReplicaLogDirTopic);\n+            }\n+            alterReplicaLogDirTopic.setName(topicPartition.topic())", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjMyMTIyMA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412321220", "bodyText": "Could we keep those and add them in the javadoc of AlterReplicaLogDirsResponse like we did here?", "author": "dajac", "createdAt": "2020-04-21T16:49:27Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsResponse.java", "diffHunk": "@@ -17,122 +17,53 @@\n \n package org.apache.kafka.common.requests;\n \n-import org.apache.kafka.common.TopicPartition;\n-import org.apache.kafka.common.protocol.ApiKeys;\n-import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n-import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n-\n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Map;\n \n-import static org.apache.kafka.common.protocol.CommonFields.ERROR_CODE;\n-import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;\n-import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;\n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n \n \n public class AlterReplicaLogDirsResponse extends AbstractResponse {\n \n-    // request level key names\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n+    private final AlterReplicaLogDirsResponseData data;\n \n-    private static final Schema ALTER_REPLICA_LOG_DIRS_RESPONSE_V0 = new Schema(\n-            THROTTLE_TIME_MS,\n-            new Field(TOPICS_KEY_NAME, new ArrayOf(new Schema(\n-                    TOPIC_NAME,\n-                    new Field(PARTITIONS_KEY_NAME, new ArrayOf(new Schema(\n-                            PARTITION_ID,\n-                            ERROR_CODE)))))));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_RESPONSE_V1 = ALTER_REPLICA_LOG_DIRS_RESPONSE_V0;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{ALTER_REPLICA_LOG_DIRS_RESPONSE_V0, ALTER_REPLICA_LOG_DIRS_RESPONSE_V1};\n+    public AlterReplicaLogDirsResponse(Struct struct) {\n+        this(struct, ApiKeys.ALTER_REPLICA_LOG_DIRS.latestVersion());\n     }\n \n-    /**\n-     * Possible error code:\n-     *\n-     * LOG_DIR_NOT_FOUND (57)\n-     * KAFKA_STORAGE_ERROR (56)\n-     * REPLICA_NOT_AVAILABLE (9)\n-     * UNKNOWN (-1)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjczMDY1OQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412730659", "bodyText": "nit: The mocked environment creates 3 nodes (by default) that you can use so you don't have to create them. You can get them with env.getCluster().nodeById(..).", "author": "dajac", "createdAt": "2020-04-22T07:15:57Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3382,6 +3388,78 @@ public void testAlterClientQuotas() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testAlterReplicaLogDirsSuccess() throws Exception {\n+        Node node0 = new Node(0, \"localhost\", 8120);\n+        Node node1 = new Node(1, \"localhost\", 8121);\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjczMTI1OA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412731258", "bodyText": "nit: partitions?", "author": "dajac", "createdAt": "2020-04-22T07:16:49Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3382,6 +3388,78 @@ public void testAlterClientQuotas() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testAlterReplicaLogDirsSuccess() throws Exception {\n+        Node node0 = new Node(0, \"localhost\", 8120);\n+        Node node1 = new Node(1, \"localhost\", 8121);\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, node0, Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, node1, Errors.NONE, 0);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr0, \"/data0\");\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            result.values().get(tpr0).get();\n+            result.values().get(tpr1).get();\n+        }\n+    }\n+\n+    @Test\n+    public void testAlterReplicaLogDirsLogDirNotFound() throws Exception {\n+        Node node0 = new Node(0, \"localhost\", 8120);\n+        Node node1 = new Node(1, \"localhost\", 8121);\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, node0, Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, node1, Errors.LOG_DIR_NOT_FOUND, 0);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr0, \"/data0\");\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            result.values().get(tpr0).get();\n+            TestUtils.assertFutureError(result.values().get(tpr1), LogDirNotFoundException.class);\n+        }\n+    }\n+\n+    @Test\n+    public void testAlterReplicaLogDirsUnrequested() throws Exception {\n+        Node node0 = new Node(0, \"localhost\", 8120);\n+        Node node1 = new Node(1, \"localhost\", 8121);\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, node0, Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, node1, Errors.NONE, 1, 2);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr0, \"/data0\");\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            // alterReplicaLogDirs() error handling fails all futures, but some of them may already be completed\n+            // so we can't make a reliable assertion about result.values().get(tpr0)\n+            TestUtils.assertFutureError(result.values().get(tpr1), IllegalStateException.class);\n+        }\n+    }\n+\n+    private void createAlterLogDirsResponse(AdminClientUnitTestEnv env, Node node, Errors error, int... partition) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjczMjQ3NQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412732475", "bodyText": "nit: We could use assertNull here and in the other tests when the future returns null.", "author": "dajac", "createdAt": "2020-04-22T07:18:48Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3382,6 +3388,78 @@ public void testAlterClientQuotas() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testAlterReplicaLogDirsSuccess() throws Exception {\n+        Node node0 = new Node(0, \"localhost\", 8120);\n+        Node node1 = new Node(1, \"localhost\", 8121);\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, node0, Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, node1, Errors.NONE, 0);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr0, \"/data0\");\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            result.values().get(tpr0).get();\n+            result.values().get(tpr1).get();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc0NjE0NQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412746145", "bodyText": "Do you intentionally use a TopicPartitionReplica which is not provided in the response here? I guess that it is to ensure that its future is not completed, isn't it?", "author": "dajac", "createdAt": "2020-04-22T07:40:00Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3382,6 +3388,78 @@ public void testAlterClientQuotas() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testAlterReplicaLogDirsSuccess() throws Exception {\n+        Node node0 = new Node(0, \"localhost\", 8120);\n+        Node node1 = new Node(1, \"localhost\", 8121);\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, node0, Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, node1, Errors.NONE, 0);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr0, \"/data0\");\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            result.values().get(tpr0).get();\n+            result.values().get(tpr1).get();\n+        }\n+    }\n+\n+    @Test\n+    public void testAlterReplicaLogDirsLogDirNotFound() throws Exception {\n+        Node node0 = new Node(0, \"localhost\", 8120);\n+        Node node1 = new Node(1, \"localhost\", 8121);\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, node0, Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, node1, Errors.LOG_DIR_NOT_FOUND, 0);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr0, \"/data0\");\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            result.values().get(tpr0).get();\n+            TestUtils.assertFutureError(result.values().get(tpr1), LogDirNotFoundException.class);\n+        }\n+    }\n+\n+    @Test\n+    public void testAlterReplicaLogDirsUnrequested() throws Exception {\n+        Node node0 = new Node(0, \"localhost\", 8120);\n+        Node node1 = new Node(1, \"localhost\", 8121);\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, node0, Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, node1, Errors.NONE, 1, 2);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc0NzU2Ng==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412747566", "bodyText": "Couldn't we just remove tpr0 from the test then? It does not seem to bring much.", "author": "dajac", "createdAt": "2020-04-22T07:42:03Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3382,6 +3388,78 @@ public void testAlterClientQuotas() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testAlterReplicaLogDirsSuccess() throws Exception {\n+        Node node0 = new Node(0, \"localhost\", 8120);\n+        Node node1 = new Node(1, \"localhost\", 8121);\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, node0, Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, node1, Errors.NONE, 0);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr0, \"/data0\");\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            result.values().get(tpr0).get();\n+            result.values().get(tpr1).get();\n+        }\n+    }\n+\n+    @Test\n+    public void testAlterReplicaLogDirsLogDirNotFound() throws Exception {\n+        Node node0 = new Node(0, \"localhost\", 8120);\n+        Node node1 = new Node(1, \"localhost\", 8121);\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, node0, Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, node1, Errors.LOG_DIR_NOT_FOUND, 0);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr0, \"/data0\");\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            result.values().get(tpr0).get();\n+            TestUtils.assertFutureError(result.values().get(tpr1), LogDirNotFoundException.class);\n+        }\n+    }\n+\n+    @Test\n+    public void testAlterReplicaLogDirsUnrequested() throws Exception {\n+        Node node0 = new Node(0, \"localhost\", 8120);\n+        Node node1 = new Node(1, \"localhost\", 8121);\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, node0, Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, node1, Errors.NONE, 1, 2);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr0, \"/data0\");\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            // alterReplicaLogDirs() error handling fails all futures, but some of them may already be completed", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc1MTA2Ng==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412751066", "bodyText": "nit: Could we break these long lines like you did bellow already?", "author": "dajac", "createdAt": "2020-04-22T07:47:23Z", "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -2198,4 +2203,31 @@ private OffsetDeleteResponse createOffsetDeleteResponse() {\n         return new OffsetDeleteResponse(data);\n     }\n \n+    private AlterReplicaLogDirsRequest createAlterReplicaLogDirsRequest() {\n+        AlterReplicaLogDirsRequestData data = new AlterReplicaLogDirsRequestData();\n+        data.dirs().add(\n+                new AlterReplicaLogDirsRequestData.AlterReplicaLogDir().setPath(\"/data0\").setTopics(\n+                        new AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopicCollection(Collections.singletonList(\n+                                new AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopic().setPartitions(singletonList(0)).setName(\"topic\")\n+                        ).iterator())\n+                )", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc1MTUzOA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412751538", "bodyText": "nit: this extra empty line can be removed.", "author": "dajac", "createdAt": "2020-04-22T07:48:05Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2589,15 +2589,27 @@ class KafkaApis(val requestChannel: RequestChannel,\n       new DescribeConfigsResponse(requestThrottleMs, (authorizedConfigs ++ unauthorizedConfigs).asJava))\n   }\n \n+", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc1Mjg0Ng==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412752846", "bodyText": "nit: This one is not used if not mistaken. Let's remove it.", "author": "dajac", "createdAt": "2020-04-22T07:50:00Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java", "diffHunk": "@@ -17,141 +17,84 @@\n \n package org.apache.kafka.common.requests;\n \n-import org.apache.kafka.common.TopicPartition;\n-import org.apache.kafka.common.protocol.ApiKeys;\n-import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n-import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n-\n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Map;\n+import java.util.stream.Collectors;\n \n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-import static org.apache.kafka.common.protocol.types.Type.INT32;\n-import static org.apache.kafka.common.protocol.types.Type.STRING;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n \n public class AlterReplicaLogDirsRequest extends AbstractRequest {\n \n-    // request level key names\n-    private static final String LOG_DIRS_KEY_NAME = \"log_dirs\";\n-\n-    // log dir level key names\n-    private static final String LOG_DIR_KEY_NAME = \"log_dir\";\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n-\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V0 = new Schema(\n-            new Field(\"log_dirs\", new ArrayOf(new Schema(\n-                    new Field(\"log_dir\", STRING, \"The absolute log directory path.\"),\n-                    new Field(\"topics\", new ArrayOf(new Schema(\n-                            TOPIC_NAME,\n-                            new Field(\"partitions\", new ArrayOf(INT32), \"List of partition ids of the topic.\"))))))));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V1 = ALTER_REPLICA_LOG_DIRS_REQUEST_V0;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{ALTER_REPLICA_LOG_DIRS_REQUEST_V0, ALTER_REPLICA_LOG_DIRS_REQUEST_V1};\n-    }\n-\n-    private final Map<TopicPartition, String> partitionDirs;\n+    private final AlterReplicaLogDirsRequestData data;\n \n     public static class Builder extends AbstractRequest.Builder<AlterReplicaLogDirsRequest> {\n-        private final Map<TopicPartition, String> partitionDirs;\n+        private final AlterReplicaLogDirsRequestData data;\n \n-        public Builder(Map<TopicPartition, String> partitionDirs) {\n+        public Builder(AlterReplicaLogDirsRequestData data) {\n             super(ApiKeys.ALTER_REPLICA_LOG_DIRS);\n-            this.partitionDirs = partitionDirs;\n+            this.data = data;\n         }\n \n         @Override\n         public AlterReplicaLogDirsRequest build(short version) {\n-            return new AlterReplicaLogDirsRequest(partitionDirs, version);\n+            return new AlterReplicaLogDirsRequest(data, version);\n         }\n \n         @Override\n         public String toString() {\n-            StringBuilder builder = new StringBuilder();\n-            builder.append(\"(type=AlterReplicaLogDirsRequest\")\n-                .append(\", partitionDirs=\")\n-                .append(partitionDirs)\n-                .append(\")\");\n-            return builder.toString();\n+            return data.toString();\n         }\n     }\n \n     public AlterReplicaLogDirsRequest(Struct struct, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        partitionDirs = new HashMap<>();\n-        for (Object logDirStructObj : struct.getArray(LOG_DIRS_KEY_NAME)) {\n-            Struct logDirStruct = (Struct) logDirStructObj;\n-            String logDir = logDirStruct.getString(LOG_DIR_KEY_NAME);\n-            for (Object topicStructObj : logDirStruct.getArray(TOPICS_KEY_NAME)) {\n-                Struct topicStruct = (Struct) topicStructObj;\n-                String topic = topicStruct.get(TOPIC_NAME);\n-                for (Object partitionObj : topicStruct.getArray(PARTITIONS_KEY_NAME)) {\n-                    int partition = (Integer) partitionObj;\n-                    partitionDirs.put(new TopicPartition(topic, partition), logDir);\n-                }\n-            }\n-        }\n+        this.data = new AlterReplicaLogDirsRequestData(struct, version);\n     }\n \n-    public AlterReplicaLogDirsRequest(Map<TopicPartition, String> partitionDirs, short version) {\n+    public AlterReplicaLogDirsRequest(AlterReplicaLogDirsRequestData data, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        this.partitionDirs = partitionDirs;\n+        this.data = data;\n     }\n \n     @Override\n     protected Struct toStruct() {\n-        Map<String, List<TopicPartition>> dirPartitions = new HashMap<>();\n-        for (Map.Entry<TopicPartition, String> entry: partitionDirs.entrySet()) {\n-            if (!dirPartitions.containsKey(entry.getValue()))\n-                dirPartitions.put(entry.getValue(), new ArrayList<>());\n-            dirPartitions.get(entry.getValue()).add(entry.getKey());\n-        }\n+        return data.toStruct(version());\n+    }\n \n-        Struct struct = new Struct(ApiKeys.ALTER_REPLICA_LOG_DIRS.requestSchema(version()));\n-        List<Struct> logDirStructArray = new ArrayList<>();\n-        for (Map.Entry<String, List<TopicPartition>> logDirEntry: dirPartitions.entrySet()) {\n-            Struct logDirStruct = struct.instance(LOG_DIRS_KEY_NAME);\n-            logDirStruct.set(LOG_DIR_KEY_NAME, logDirEntry.getKey());\n-\n-            List<Struct> topicStructArray = new ArrayList<>();\n-            for (Map.Entry<String, List<Integer>> topicEntry: CollectionUtils.groupPartitionsByTopic(logDirEntry.getValue()).entrySet()) {\n-                Struct topicStruct = logDirStruct.instance(TOPICS_KEY_NAME);\n-                topicStruct.set(TOPIC_NAME, topicEntry.getKey());\n-                topicStruct.set(PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());\n-                topicStructArray.add(topicStruct);\n-            }\n-            logDirStruct.set(TOPICS_KEY_NAME, topicStructArray.toArray());\n-            logDirStructArray.add(logDirStruct);\n-        }\n-        struct.set(LOG_DIRS_KEY_NAME, logDirStructArray.toArray());\n-        return struct;\n+    @Override\n+    public AlterReplicaLogDirsResponse getErrorResponse(Throwable e) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc1NjU3MA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412756570", "bodyText": "nit: Indentation looks wrong here.", "author": "dajac", "createdAt": "2020-04-22T07:55:20Z", "path": "core/src/test/scala/unit/kafka/server/AlterReplicaLogDirsRequestTest.scala", "diffHunk": "@@ -104,13 +112,26 @@ class AlterReplicaLogDirsRequestTest extends BaseRequestTest {\n     partitionDirs3.put(new TopicPartition(topic, 1), validDir3)\n     partitionDirs3.put(new TopicPartition(topic, 2), offlineDir)\n     val alterReplicaDirResponse3 = sendAlterReplicaLogDirsRequest(partitionDirs3.toMap)\n-    assertEquals(Errors.LOG_DIR_NOT_FOUND, alterReplicaDirResponse3.responses().get(new TopicPartition(topic, 0)))\n-    assertEquals(Errors.KAFKA_STORAGE_ERROR, alterReplicaDirResponse3.responses().get(new TopicPartition(topic, 1)))\n-    assertEquals(Errors.KAFKA_STORAGE_ERROR, alterReplicaDirResponse3.responses().get(new TopicPartition(topic, 2)))\n+    assertEquals(Errors.LOG_DIR_NOT_FOUND, findErrorForPartition(alterReplicaDirResponse3, new TopicPartition(topic, 0)))\n+    assertEquals(Errors.KAFKA_STORAGE_ERROR, findErrorForPartition(alterReplicaDirResponse3, new TopicPartition(topic, 1)))\n+    assertEquals(Errors.KAFKA_STORAGE_ERROR, findErrorForPartition(alterReplicaDirResponse3, new TopicPartition(topic, 2)))\n   }\n \n   private def sendAlterReplicaLogDirsRequest(partitionDirs: Map[TopicPartition, String]): AlterReplicaLogDirsResponse = {\n-    val request = new AlterReplicaLogDirsRequest.Builder(partitionDirs.asJava).build()\n+    val logDirs = partitionDirs.groupBy{case (_, dir) => dir}.map{ case(dir, tps) =>\n+      new AlterReplicaLogDirsRequestData.AlterReplicaLogDir()\n+        .setPath(dir)\n+        .setTopics(new AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopicCollection(\n+          tps.groupBy { case (tp, _) => tp.topic }\n+            .map { case (topic, tpPartitions) =>\n+          new AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopic()\n+            .setName(topic)\n+            .setPartitions(tpPartitions.map{case (tp, _) => tp.partition.asInstanceOf[Integer]}.toList.asJava)\n+        }.toList.asJava.iterator))", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc2MjI1Nw==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412762257", "bodyText": "nit: Indentation seems inconsistent.", "author": "dajac", "createdAt": "2020-04-22T08:03:31Z", "path": "core/src/test/scala/unit/kafka/server/KafkaApisTest.scala", "diffHunk": "@@ -1707,4 +1707,38 @@ class KafkaApisTest {\n       0, 0, partitionStates.asJava, Seq(broker).asJava).build()\n     metadataCache.updateMetadata(correlationId = 0, updateMetadataRequest)\n   }\n+\n+  @Test\n+  def testAlterReplicaLogDirs(): Unit = {\n+    val data = new AlterReplicaLogDirsRequestData()\n+    val dir = new AlterReplicaLogDirsRequestData.AlterReplicaLogDir()\n+      .setPath(\"/foo\")\n+    dir.topics().add(new AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopic().setName(\"t0\").setPartitions(asList(0, 1, 2)))\n+    data.dirs().add(dir)\n+    val alterReplicaLogDirsRequest = new AlterReplicaLogDirsRequest.Builder(\n+      data\n+    ).build()\n+    val request = buildRequest(alterReplicaLogDirsRequest)\n+\n+    EasyMock.reset(replicaManager, clientRequestQuotaManager, requestChannel)\n+\n+    val capturedResponse = expectNoThrottling()\n+    EasyMock.expect(replicaManager.alterReplicaLogDirs(EasyMock.eq(Map(\n+      new TopicPartition(\"t0\", 0) -> \"/foo\",\n+      new TopicPartition(\"t0\", 1) -> \"/foo\",\n+      new TopicPartition(\"t0\", 2) -> \"/foo\"))))\n+      .andReturn(Map(\n+        new TopicPartition(\"t0\", 0) -> Errors.NONE,\n+        new TopicPartition(\"t0\", 1) -> Errors.LOG_DIR_NOT_FOUND,\n+        new TopicPartition(\"t0\", 2) -> Errors.INVALID_TOPIC_EXCEPTION))", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc2NTg1Mg==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412765852", "bodyText": "I find the implementation that you did in AuthorizerIntegrationTest at L195 easier to read. Could we use it here as well to stay consistent?", "author": "dajac", "createdAt": "2020-04-22T08:08:52Z", "path": "core/src/test/scala/unit/kafka/server/AlterReplicaLogDirsRequestTest.scala", "diffHunk": "@@ -36,6 +37,13 @@ class AlterReplicaLogDirsRequestTest extends BaseRequestTest {\n \n   val topic = \"topic\"\n \n+  private def findErrorForPartition(response: AlterReplicaLogDirsResponse, tp: TopicPartition): Errors = {\n+    Errors.forCode(response.data.results.asScala.groupBy(x => x.topicName).get(tp.topic).get", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc3MjA4Nw==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412772087", "bodyText": "It is good to validate the errorCounts but could we also validate the overall mapping from TopicPartition to Errors?", "author": "dajac", "createdAt": "2020-04-22T08:18:00Z", "path": "core/src/test/scala/unit/kafka/server/KafkaApisTest.scala", "diffHunk": "@@ -1707,4 +1707,38 @@ class KafkaApisTest {\n       0, 0, partitionStates.asJava, Seq(broker).asJava).build()\n     metadataCache.updateMetadata(correlationId = 0, updateMetadataRequest)\n   }\n+\n+  @Test\n+  def testAlterReplicaLogDirs(): Unit = {\n+    val data = new AlterReplicaLogDirsRequestData()\n+    val dir = new AlterReplicaLogDirsRequestData.AlterReplicaLogDir()\n+      .setPath(\"/foo\")\n+    dir.topics().add(new AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopic().setName(\"t0\").setPartitions(asList(0, 1, 2)))\n+    data.dirs().add(dir)\n+    val alterReplicaLogDirsRequest = new AlterReplicaLogDirsRequest.Builder(\n+      data\n+    ).build()\n+    val request = buildRequest(alterReplicaLogDirsRequest)\n+\n+    EasyMock.reset(replicaManager, clientRequestQuotaManager, requestChannel)\n+\n+    val capturedResponse = expectNoThrottling()\n+    EasyMock.expect(replicaManager.alterReplicaLogDirs(EasyMock.eq(Map(\n+      new TopicPartition(\"t0\", 0) -> \"/foo\",\n+      new TopicPartition(\"t0\", 1) -> \"/foo\",\n+      new TopicPartition(\"t0\", 2) -> \"/foo\"))))\n+      .andReturn(Map(\n+        new TopicPartition(\"t0\", 0) -> Errors.NONE,\n+        new TopicPartition(\"t0\", 1) -> Errors.LOG_DIR_NOT_FOUND,\n+        new TopicPartition(\"t0\", 2) -> Errors.INVALID_TOPIC_EXCEPTION))\n+    EasyMock.replay(replicaManager, clientQuotaManager, clientRequestQuotaManager, requestChannel)\n+\n+    createKafkaApis().handleAlterReplicaLogDirsRequest(request)\n+\n+    val response = readResponse(ApiKeys.ALTER_REPLICA_LOG_DIRS, alterReplicaLogDirsRequest, capturedResponse)\n+      .asInstanceOf[AlterReplicaLogDirsResponse]\n+    assertEquals(Map(Errors.NONE -> 1,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc3NjE5Ng==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412776196", "bodyText": "Could we add a unit test for this one and the one bellow?", "author": "dajac", "createdAt": "2020-04-22T08:23:36Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java", "diffHunk": "@@ -17,141 +17,84 @@\n \n package org.apache.kafka.common.requests;\n \n-import org.apache.kafka.common.TopicPartition;\n-import org.apache.kafka.common.protocol.ApiKeys;\n-import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n-import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n-\n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Map;\n+import java.util.stream.Collectors;\n \n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-import static org.apache.kafka.common.protocol.types.Type.INT32;\n-import static org.apache.kafka.common.protocol.types.Type.STRING;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n \n public class AlterReplicaLogDirsRequest extends AbstractRequest {\n \n-    // request level key names\n-    private static final String LOG_DIRS_KEY_NAME = \"log_dirs\";\n-\n-    // log dir level key names\n-    private static final String LOG_DIR_KEY_NAME = \"log_dir\";\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n-\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V0 = new Schema(\n-            new Field(\"log_dirs\", new ArrayOf(new Schema(\n-                    new Field(\"log_dir\", STRING, \"The absolute log directory path.\"),\n-                    new Field(\"topics\", new ArrayOf(new Schema(\n-                            TOPIC_NAME,\n-                            new Field(\"partitions\", new ArrayOf(INT32), \"List of partition ids of the topic.\"))))))));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V1 = ALTER_REPLICA_LOG_DIRS_REQUEST_V0;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{ALTER_REPLICA_LOG_DIRS_REQUEST_V0, ALTER_REPLICA_LOG_DIRS_REQUEST_V1};\n-    }\n-\n-    private final Map<TopicPartition, String> partitionDirs;\n+    private final AlterReplicaLogDirsRequestData data;\n \n     public static class Builder extends AbstractRequest.Builder<AlterReplicaLogDirsRequest> {\n-        private final Map<TopicPartition, String> partitionDirs;\n+        private final AlterReplicaLogDirsRequestData data;\n \n-        public Builder(Map<TopicPartition, String> partitionDirs) {\n+        public Builder(AlterReplicaLogDirsRequestData data) {\n             super(ApiKeys.ALTER_REPLICA_LOG_DIRS);\n-            this.partitionDirs = partitionDirs;\n+            this.data = data;\n         }\n \n         @Override\n         public AlterReplicaLogDirsRequest build(short version) {\n-            return new AlterReplicaLogDirsRequest(partitionDirs, version);\n+            return new AlterReplicaLogDirsRequest(data, version);\n         }\n \n         @Override\n         public String toString() {\n-            StringBuilder builder = new StringBuilder();\n-            builder.append(\"(type=AlterReplicaLogDirsRequest\")\n-                .append(\", partitionDirs=\")\n-                .append(partitionDirs)\n-                .append(\")\");\n-            return builder.toString();\n+            return data.toString();\n         }\n     }\n \n     public AlterReplicaLogDirsRequest(Struct struct, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        partitionDirs = new HashMap<>();\n-        for (Object logDirStructObj : struct.getArray(LOG_DIRS_KEY_NAME)) {\n-            Struct logDirStruct = (Struct) logDirStructObj;\n-            String logDir = logDirStruct.getString(LOG_DIR_KEY_NAME);\n-            for (Object topicStructObj : logDirStruct.getArray(TOPICS_KEY_NAME)) {\n-                Struct topicStruct = (Struct) topicStructObj;\n-                String topic = topicStruct.get(TOPIC_NAME);\n-                for (Object partitionObj : topicStruct.getArray(PARTITIONS_KEY_NAME)) {\n-                    int partition = (Integer) partitionObj;\n-                    partitionDirs.put(new TopicPartition(topic, partition), logDir);\n-                }\n-            }\n-        }\n+        this.data = new AlterReplicaLogDirsRequestData(struct, version);\n     }\n \n-    public AlterReplicaLogDirsRequest(Map<TopicPartition, String> partitionDirs, short version) {\n+    public AlterReplicaLogDirsRequest(AlterReplicaLogDirsRequestData data, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        this.partitionDirs = partitionDirs;\n+        this.data = data;\n     }\n \n     @Override\n     protected Struct toStruct() {\n-        Map<String, List<TopicPartition>> dirPartitions = new HashMap<>();\n-        for (Map.Entry<TopicPartition, String> entry: partitionDirs.entrySet()) {\n-            if (!dirPartitions.containsKey(entry.getValue()))\n-                dirPartitions.put(entry.getValue(), new ArrayList<>());\n-            dirPartitions.get(entry.getValue()).add(entry.getKey());\n-        }\n+        return data.toStruct(version());\n+    }\n \n-        Struct struct = new Struct(ApiKeys.ALTER_REPLICA_LOG_DIRS.requestSchema(version()));\n-        List<Struct> logDirStructArray = new ArrayList<>();\n-        for (Map.Entry<String, List<TopicPartition>> logDirEntry: dirPartitions.entrySet()) {\n-            Struct logDirStruct = struct.instance(LOG_DIRS_KEY_NAME);\n-            logDirStruct.set(LOG_DIR_KEY_NAME, logDirEntry.getKey());\n-\n-            List<Struct> topicStructArray = new ArrayList<>();\n-            for (Map.Entry<String, List<Integer>> topicEntry: CollectionUtils.groupPartitionsByTopic(logDirEntry.getValue()).entrySet()) {\n-                Struct topicStruct = logDirStruct.instance(TOPICS_KEY_NAME);\n-                topicStruct.set(TOPIC_NAME, topicEntry.getKey());\n-                topicStruct.set(PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());\n-                topicStructArray.add(topicStruct);\n-            }\n-            logDirStruct.set(TOPICS_KEY_NAME, topicStructArray.toArray());\n-            logDirStructArray.add(logDirStruct);\n-        }\n-        struct.set(LOG_DIRS_KEY_NAME, logDirStructArray.toArray());\n-        return struct;\n+    @Override\n+    public AlterReplicaLogDirsResponse getErrorResponse(Throwable e) {\n+        return (AlterReplicaLogDirsResponse) super.getErrorResponse(e);\n     }\n \n     @Override\n-    public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n-        Map<TopicPartition, Errors> responseMap = new HashMap<>();\n-        for (Map.Entry<TopicPartition, String> entry : partitionDirs.entrySet()) {\n-            responseMap.put(entry.getKey(), Errors.forException(e));\n-        }\n-        return new AlterReplicaLogDirsResponse(throttleTimeMs, responseMap);\n+    public AlterReplicaLogDirsResponse getErrorResponse(int throttleTimeMs, Throwable e) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc3NjkzOQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r412776939", "bodyText": "Could we add a unit test for this one?", "author": "dajac", "createdAt": "2020-04-22T08:24:42Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsResponse.java", "diffHunk": "@@ -17,122 +17,53 @@\n \n package org.apache.kafka.common.requests;\n \n-import org.apache.kafka.common.TopicPartition;\n-import org.apache.kafka.common.protocol.ApiKeys;\n-import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n-import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n-\n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Map;\n \n-import static org.apache.kafka.common.protocol.CommonFields.ERROR_CODE;\n-import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;\n-import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;\n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n \n \n public class AlterReplicaLogDirsResponse extends AbstractResponse {\n \n-    // request level key names\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n+    private final AlterReplicaLogDirsResponseData data;\n \n-    private static final Schema ALTER_REPLICA_LOG_DIRS_RESPONSE_V0 = new Schema(\n-            THROTTLE_TIME_MS,\n-            new Field(TOPICS_KEY_NAME, new ArrayOf(new Schema(\n-                    TOPIC_NAME,\n-                    new Field(PARTITIONS_KEY_NAME, new ArrayOf(new Schema(\n-                            PARTITION_ID,\n-                            ERROR_CODE)))))));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_RESPONSE_V1 = ALTER_REPLICA_LOG_DIRS_RESPONSE_V0;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{ALTER_REPLICA_LOG_DIRS_RESPONSE_V0, ALTER_REPLICA_LOG_DIRS_RESPONSE_V1};\n+    public AlterReplicaLogDirsResponse(Struct struct) {\n+        this(struct, ApiKeys.ALTER_REPLICA_LOG_DIRS.latestVersion());\n     }\n \n-    /**\n-     * Possible error code:\n-     *\n-     * LOG_DIR_NOT_FOUND (57)\n-     * KAFKA_STORAGE_ERROR (56)\n-     * REPLICA_NOT_AVAILABLE (9)\n-     * UNKNOWN (-1)\n-     */\n-    private final Map<TopicPartition, Errors> responses;\n-    private final int throttleTimeMs;\n+    public AlterReplicaLogDirsResponse(Struct struct, short version) {\n+        this.data = new AlterReplicaLogDirsResponseData(struct, version);\n+    }\n \n-    public AlterReplicaLogDirsResponse(Struct struct) {\n-        throttleTimeMs = struct.get(THROTTLE_TIME_MS);\n-        responses = new HashMap<>();\n-        for (Object topicStructObj : struct.getArray(TOPICS_KEY_NAME)) {\n-            Struct topicStruct = (Struct) topicStructObj;\n-            String topic = topicStruct.get(TOPIC_NAME);\n-            for (Object partitionStructObj : topicStruct.getArray(PARTITIONS_KEY_NAME)) {\n-                Struct partitionStruct = (Struct) partitionStructObj;\n-                int partition = partitionStruct.get(PARTITION_ID);\n-                Errors error = Errors.forCode(partitionStruct.get(ERROR_CODE));\n-                responses.put(new TopicPartition(topic, partition), error);\n-            }\n-        }\n+    public AlterReplicaLogDirsResponse(AlterReplicaLogDirsResponseData data) {\n+        this.data = data;\n     }\n \n-    /**\n-     * Constructor for version 0.\n-     */\n-    public AlterReplicaLogDirsResponse(int throttleTimeMs, Map<TopicPartition, Errors> responses) {\n-        this.throttleTimeMs = throttleTimeMs;\n-        this.responses = responses;\n+    public AlterReplicaLogDirsResponseData data() {\n+        return data;\n     }\n \n     @Override\n     protected Struct toStruct(short version) {\n-        Struct struct = new Struct(ApiKeys.ALTER_REPLICA_LOG_DIRS.responseSchema(version));\n-        struct.set(THROTTLE_TIME_MS, throttleTimeMs);\n-        Map<String, Map<Integer, Errors>> responsesByTopic = CollectionUtils.groupPartitionDataByTopic(responses);\n-        List<Struct> topicStructArray = new ArrayList<>();\n-        for (Map.Entry<String, Map<Integer, Errors>> responsesByTopicEntry : responsesByTopic.entrySet()) {\n-            Struct topicStruct = struct.instance(TOPICS_KEY_NAME);\n-            topicStruct.set(TOPIC_NAME, responsesByTopicEntry.getKey());\n-            List<Struct> partitionStructArray = new ArrayList<>();\n-            for (Map.Entry<Integer, Errors> responsesByPartitionEntry : responsesByTopicEntry.getValue().entrySet()) {\n-                Struct partitionStruct = topicStruct.instance(PARTITIONS_KEY_NAME);\n-                Errors response = responsesByPartitionEntry.getValue();\n-                partitionStruct.set(PARTITION_ID, responsesByPartitionEntry.getKey());\n-                partitionStruct.set(ERROR_CODE, response.code());\n-                partitionStructArray.add(partitionStruct);\n-            }\n-            topicStruct.set(PARTITIONS_KEY_NAME, partitionStructArray.toArray());\n-            topicStructArray.add(topicStruct);\n-        }\n-        struct.set(TOPICS_KEY_NAME, topicStructArray.toArray());\n-        return struct;\n+        return data.toStruct(version);\n     }\n \n     @Override\n     public int throttleTimeMs() {\n-        return throttleTimeMs;\n-    }\n-\n-    public Map<TopicPartition, Errors> responses() {\n-        return this.responses;\n+        return data.throttleTimeMs();\n     }\n \n     @Override\n     public Map<Errors, Integer> errorCounts() {\n-        return errorCounts(responses.values());\n+        Map<Errors, Integer> errorCounts = new HashMap<>();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzU0Njc2MA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r413546760", "bodyText": "I suppose that this should be named testErrorCounts.", "author": "dajac", "createdAt": "2020-04-23T06:36:24Z", "path": "clients/src/test/java/org/apache/kafka/common/requests/AlterReplicaLogDirsResponseTest.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.util.Map;\n+\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirPartitionResult;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.junit.Test;\n+\n+import static java.util.Arrays.asList;\n+import static org.junit.Assert.assertEquals;\n+\n+public class AlterReplicaLogDirsResponseTest {\n+\n+    @Test\n+    public void testErrorResponse() {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzU0NzQ4Ng==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r413547486", "bodyText": "assertNulls shouldn't be here but few lines bellow.", "author": "dajac", "createdAt": "2020-04-23T06:37:53Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3382,6 +3388,70 @@ public void testAlterClientQuotas() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testAlterReplicaLogDirsSuccess() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(0), Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(1), Errors.NONE, 0);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            assertNull(logDirs.put(tpr0, \"/data0\"));\n+            assertNull(logDirs.put(tpr1, \"/data1\"));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTYxODYwNw==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r415618607", "bodyText": "Good catch! Sorry about that.", "author": "tombentley", "createdAt": "2020-04-27T08:34:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzU0NzQ4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzU1NTEyMQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r413555121", "bodyText": "@tombentley This one bugs me. I know that it was already like this in the previous implementation but I wonder if we could improve this while we are here. I am also fine keeping it as it is and improving this latter on.\nI find odd that we fail all the non-realised yet futures when we discover an unknown replica in the response. From a client perspective, the behaviour is unpredictable and we leave it with potentially a partial view of the results event though the brokers may have sent back all the requested ones. Wouldn't it be better to just ignore unexpected replica and log a warning message? If a broker would ever do this, the client would continue to work as expected which is probably better. I think that it is a better behavior.\nSimilarly, we may want to ensure that all futures are done in case a replica would be missing in the response from the broker.\nWhat do you think?", "author": "dajac", "createdAt": "2020-04-23T06:50:32Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2234,18 +2249,18 @@ public AlterReplicaLogDirsResult alterReplicaLogDirs(Map<TopicPartitionReplica,\n                 @Override\n                 public void handleResponse(AbstractResponse abstractResponse) {\n                     AlterReplicaLogDirsResponse response = (AlterReplicaLogDirsResponse) abstractResponse;\n-                    for (Map.Entry<TopicPartition, Errors> responseEntry: response.responses().entrySet()) {\n-                        TopicPartition tp = responseEntry.getKey();\n-                        Errors error = responseEntry.getValue();\n-                        TopicPartitionReplica replica = new TopicPartitionReplica(tp.topic(), tp.partition(), brokerId);\n-                        KafkaFutureImpl<Void> future = futures.get(replica);\n-                        if (future == null) {\n-                            handleFailure(new IllegalStateException(\n-                                \"The partition \" + tp + \" in the response from broker \" + brokerId + \" is not in the request\"));\n-                        } else if (error == Errors.NONE) {\n-                            future.complete(null);\n-                        } else {\n-                            future.completeExceptionally(error.exception());\n+                    for (AlterReplicaLogDirTopicResult topicResult: response.data().results()) {\n+                        for (AlterReplicaLogDirPartitionResult partitionResult: topicResult.partitions()) {\n+                            TopicPartitionReplica replica = new TopicPartitionReplica(topicResult.topicName(), partitionResult.partitionIndex(), brokerId);\n+                            KafkaFutureImpl<Void> future = futures.get(replica);\n+                            if (future == null) {\n+                                handleFailure(new IllegalStateException(\n+                                        \"The partition \" + new TopicPartition(topicResult.topicName(), partitionResult.partitionIndex()) + \" in the response from broker \" + brokerId + \" is not in the request\"));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzU1ODI4MA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r413558280", "bodyText": "nit: () can be removed for all the accessors here.", "author": "dajac", "createdAt": "2020-04-23T06:55:23Z", "path": "core/src/test/scala/unit/kafka/server/KafkaApisTest.scala", "diffHunk": "@@ -1707,4 +1707,47 @@ class KafkaApisTest {\n       0, 0, partitionStates.asJava, Seq(broker).asJava).build()\n     metadataCache.updateMetadata(correlationId = 0, updateMetadataRequest)\n   }\n+\n+  @Test\n+  def testAlterReplicaLogDirs(): Unit = {\n+    val data = new AlterReplicaLogDirsRequestData()\n+    val dir = new AlterReplicaLogDirsRequestData.AlterReplicaLogDir()\n+      .setPath(\"/foo\")\n+    dir.topics().add(new AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopic().setName(\"t0\").setPartitions(asList(0, 1, 2)))\n+    data.dirs().add(dir)\n+    val alterReplicaLogDirsRequest = new AlterReplicaLogDirsRequest.Builder(\n+      data\n+    ).build()\n+    val request = buildRequest(alterReplicaLogDirsRequest)\n+\n+    EasyMock.reset(replicaManager, clientRequestQuotaManager, requestChannel)\n+\n+    val capturedResponse = expectNoThrottling()\n+    val t0p0 = new TopicPartition(\"t0\", 0)\n+    val t0p1 = new TopicPartition(\"t0\", 1)\n+    val t0p2 = new TopicPartition(\"t0\", 2)\n+    val partitionResults = Map(\n+      t0p0 -> Errors.NONE,\n+      t0p1 -> Errors.LOG_DIR_NOT_FOUND,\n+      t0p2 -> Errors.INVALID_TOPIC_EXCEPTION)\n+    EasyMock.expect(replicaManager.alterReplicaLogDirs(EasyMock.eq(Map(\n+      t0p0 -> \"/foo\",\n+      t0p1 -> \"/foo\",\n+      t0p2 -> \"/foo\"))))\n+    .andReturn(partitionResults)\n+    EasyMock.replay(replicaManager, clientQuotaManager, clientRequestQuotaManager, requestChannel)\n+\n+    createKafkaApis().handleAlterReplicaLogDirsRequest(request)\n+\n+    val response = readResponse(ApiKeys.ALTER_REPLICA_LOG_DIRS, alterReplicaLogDirsRequest, capturedResponse)\n+      .asInstanceOf[AlterReplicaLogDirsResponse]\n+    assertEquals(partitionResults, response.data.results.asScala.flatMap { tr =>\n+      tr.partitions().asScala.map { pr =>\n+        new TopicPartition(tr.topicName(), pr.partitionIndex()) -> Errors.forCode(pr.errorCode())", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzU1OTI2Mg==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r413559262", "bodyText": "Do you mind adding a unit test for this one in AlterReplicaLogDirsRequestTest for the sake of completeness?", "author": "dajac", "createdAt": "2020-04-23T06:57:17Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java", "diffHunk": "@@ -17,141 +17,79 @@\n \n package org.apache.kafka.common.requests;\n \n-import org.apache.kafka.common.TopicPartition;\n-import org.apache.kafka.common.protocol.ApiKeys;\n-import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n-import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n-\n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Map;\n+import java.util.stream.Collectors;\n \n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-import static org.apache.kafka.common.protocol.types.Type.INT32;\n-import static org.apache.kafka.common.protocol.types.Type.STRING;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n \n public class AlterReplicaLogDirsRequest extends AbstractRequest {\n \n-    // request level key names\n-    private static final String LOG_DIRS_KEY_NAME = \"log_dirs\";\n-\n-    // log dir level key names\n-    private static final String LOG_DIR_KEY_NAME = \"log_dir\";\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n-\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V0 = new Schema(\n-            new Field(\"log_dirs\", new ArrayOf(new Schema(\n-                    new Field(\"log_dir\", STRING, \"The absolute log directory path.\"),\n-                    new Field(\"topics\", new ArrayOf(new Schema(\n-                            TOPIC_NAME,\n-                            new Field(\"partitions\", new ArrayOf(INT32), \"List of partition ids of the topic.\"))))))));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema ALTER_REPLICA_LOG_DIRS_REQUEST_V1 = ALTER_REPLICA_LOG_DIRS_REQUEST_V0;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{ALTER_REPLICA_LOG_DIRS_REQUEST_V0, ALTER_REPLICA_LOG_DIRS_REQUEST_V1};\n-    }\n-\n-    private final Map<TopicPartition, String> partitionDirs;\n+    private final AlterReplicaLogDirsRequestData data;\n \n     public static class Builder extends AbstractRequest.Builder<AlterReplicaLogDirsRequest> {\n-        private final Map<TopicPartition, String> partitionDirs;\n+        private final AlterReplicaLogDirsRequestData data;\n \n-        public Builder(Map<TopicPartition, String> partitionDirs) {\n+        public Builder(AlterReplicaLogDirsRequestData data) {\n             super(ApiKeys.ALTER_REPLICA_LOG_DIRS);\n-            this.partitionDirs = partitionDirs;\n+            this.data = data;\n         }\n \n         @Override\n         public AlterReplicaLogDirsRequest build(short version) {\n-            return new AlterReplicaLogDirsRequest(partitionDirs, version);\n+            return new AlterReplicaLogDirsRequest(data, version);\n         }\n \n         @Override\n         public String toString() {\n-            StringBuilder builder = new StringBuilder();\n-            builder.append(\"(type=AlterReplicaLogDirsRequest\")\n-                .append(\", partitionDirs=\")\n-                .append(partitionDirs)\n-                .append(\")\");\n-            return builder.toString();\n+            return data.toString();\n         }\n     }\n \n     public AlterReplicaLogDirsRequest(Struct struct, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        partitionDirs = new HashMap<>();\n-        for (Object logDirStructObj : struct.getArray(LOG_DIRS_KEY_NAME)) {\n-            Struct logDirStruct = (Struct) logDirStructObj;\n-            String logDir = logDirStruct.getString(LOG_DIR_KEY_NAME);\n-            for (Object topicStructObj : logDirStruct.getArray(TOPICS_KEY_NAME)) {\n-                Struct topicStruct = (Struct) topicStructObj;\n-                String topic = topicStruct.get(TOPIC_NAME);\n-                for (Object partitionObj : topicStruct.getArray(PARTITIONS_KEY_NAME)) {\n-                    int partition = (Integer) partitionObj;\n-                    partitionDirs.put(new TopicPartition(topic, partition), logDir);\n-                }\n-            }\n-        }\n+        this.data = new AlterReplicaLogDirsRequestData(struct, version);\n     }\n \n-    public AlterReplicaLogDirsRequest(Map<TopicPartition, String> partitionDirs, short version) {\n+    public AlterReplicaLogDirsRequest(AlterReplicaLogDirsRequestData data, short version) {\n         super(ApiKeys.ALTER_REPLICA_LOG_DIRS, version);\n-        this.partitionDirs = partitionDirs;\n+        this.data = data;\n     }\n \n     @Override\n     protected Struct toStruct() {\n-        Map<String, List<TopicPartition>> dirPartitions = new HashMap<>();\n-        for (Map.Entry<TopicPartition, String> entry: partitionDirs.entrySet()) {\n-            if (!dirPartitions.containsKey(entry.getValue()))\n-                dirPartitions.put(entry.getValue(), new ArrayList<>());\n-            dirPartitions.get(entry.getValue()).add(entry.getKey());\n-        }\n-\n-        Struct struct = new Struct(ApiKeys.ALTER_REPLICA_LOG_DIRS.requestSchema(version()));\n-        List<Struct> logDirStructArray = new ArrayList<>();\n-        for (Map.Entry<String, List<TopicPartition>> logDirEntry: dirPartitions.entrySet()) {\n-            Struct logDirStruct = struct.instance(LOG_DIRS_KEY_NAME);\n-            logDirStruct.set(LOG_DIR_KEY_NAME, logDirEntry.getKey());\n-\n-            List<Struct> topicStructArray = new ArrayList<>();\n-            for (Map.Entry<String, List<Integer>> topicEntry: CollectionUtils.groupPartitionsByTopic(logDirEntry.getValue()).entrySet()) {\n-                Struct topicStruct = logDirStruct.instance(TOPICS_KEY_NAME);\n-                topicStruct.set(TOPIC_NAME, topicEntry.getKey());\n-                topicStruct.set(PARTITIONS_KEY_NAME, topicEntry.getValue().toArray());\n-                topicStructArray.add(topicStruct);\n-            }\n-            logDirStruct.set(TOPICS_KEY_NAME, topicStructArray.toArray());\n-            logDirStructArray.add(logDirStruct);\n-        }\n-        struct.set(LOG_DIRS_KEY_NAME, logDirStructArray.toArray());\n-        return struct;\n+        return data.toStruct(version());\n     }\n \n     @Override\n-    public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n-        Map<TopicPartition, Errors> responseMap = new HashMap<>();\n-        for (Map.Entry<TopicPartition, String> entry : partitionDirs.entrySet()) {\n-            responseMap.put(entry.getKey(), Errors.forException(e));\n-        }\n-        return new AlterReplicaLogDirsResponse(throttleTimeMs, responseMap);\n+    public AlterReplicaLogDirsResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n+        AlterReplicaLogDirsResponseData data = new AlterReplicaLogDirsResponseData();\n+        data.setResults(this.data.dirs().stream().flatMap(alterDir ->\n+            alterDir.topics().stream().map(topic ->\n+                new AlterReplicaLogDirTopicResult()\n+                    .setTopicName(topic.name())\n+                    .setPartitions(topic.partitions().stream().map(partitionId ->\n+                        new AlterReplicaLogDirsResponseData.AlterReplicaLogDirPartitionResult()\n+                            .setErrorCode(Errors.forException(e).code())\n+                            .setPartitionIndex(partitionId)).collect(Collectors.toList())))).collect(Collectors.toList()));\n+        return new AlterReplicaLogDirsResponse(data.setThrottleTimeMs(throttleTimeMs));\n     }\n \n     public Map<TopicPartition, String> partitionDirs() {\n-        return partitionDirs;\n+        Map<TopicPartition, String> result = new HashMap<>();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c2183171394262974cc77a39980c666da0970fdc", "url": "https://github.com/apache/kafka/commit/c2183171394262974cc77a39980c666da0970fdc", "message": "wip, need to fix KafkaApis/ReplicaManager", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "9a338902805a8a50a4edcc33e2ff984cf6b0920d", "url": "https://github.com/apache/kafka/commit/9a338902805a8a50a4edcc33e2ff984cf6b0920d", "message": "wip", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "21f7375d0f429e0ea9d8991c0b2820a296164ed6", "url": "https://github.com/apache/kafka/commit/21f7375d0f429e0ea9d8991c0b2820a296164ed6", "message": "Minor improvements", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "a42774804167c66163288431283465f0c8aeb67e", "url": "https://github.com/apache/kafka/commit/a42774804167c66163288431283465f0c8aeb67e", "message": "Review comments", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "f9069aa905f76d7efbce12126eed71b7d51d815c", "url": "https://github.com/apache/kafka/commit/f9069aa905f76d7efbce12126eed71b7d51d815c", "message": "review comments", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "d3fd28c7b5aeb837488fd5e7670a2d30d354875b", "url": "https://github.com/apache/kafka/commit/d3fd28c7b5aeb837488fd5e7670a2d30d354875b", "message": "Add test for KafkaAdminClient.alterReplicaLogDirs", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "ff5e4576fc5265de0aab24c6205981c94286ec11", "url": "https://github.com/apache/kafka/commit/ff5e4576fc5265de0aab24c6205981c94286ec11", "message": "Add test for KafkaAdminClient.alterReplicaLogDirs", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "a4bcdc04a580271a6db46afa4d077b9551e2f1f2", "url": "https://github.com/apache/kafka/commit/a4bcdc04a580271a6db46afa4d077b9551e2f1f2", "message": "Add test for AlterLogDirs RPCs to RequestResponseTests", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "e8893789d5d64419b3c55eefc0561af543b0de29", "url": "https://github.com/apache/kafka/commit/e8893789d5d64419b3c55eefc0561af543b0de29", "message": "Suggested code by @dajac", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "01a2980254d8b5ffccceb35e25ef40fe0b09f86f", "url": "https://github.com/apache/kafka/commit/01a2980254d8b5ffccceb35e25ef40fe0b09f86f", "message": "Add KafkaApisTest.testAlterReplicaLogDirs()", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "eaf80dc6b2fcc218da0d7c6b2933089b0aa5a6d1", "url": "https://github.com/apache/kafka/commit/eaf80dc6b2fcc218da0d7c6b2933089b0aa5a6d1", "message": "Fix test", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "0fc3bc37ad3406190015ce3f66673c4bf372a6f3", "url": "https://github.com/apache/kafka/commit/0fc3bc37ad3406190015ce3f66673c4bf372a6f3", "message": "review comments", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "1dffaa9fef1346c056fd57f7d427a81aead15d3e", "url": "https://github.com/apache/kafka/commit/1dffaa9fef1346c056fd57f7d427a81aead15d3e", "message": "Review comments; change handling of unexpected responses", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "1dffaa9fef1346c056fd57f7d427a81aead15d3e", "url": "https://github.com/apache/kafka/commit/1dffaa9fef1346c056fd57f7d427a81aead15d3e", "message": "Review comments; change handling of unexpected responses", "committedDate": "2020-04-27T13:08:54Z", "type": "forcePushed"}, {"oid": "c82455ffe5b1f97c548ab9cb966ff78300f33b07", "url": "https://github.com/apache/kafka/commit/c82455ffe5b1f97c548ab9cb966ff78300f33b07", "message": "Review comments 2", "committedDate": "2020-04-27T13:47:54Z", "type": "commit"}, {"oid": "4c5d1e69c288cbf6aecb9864c0f859502e2ca301", "url": "https://github.com/apache/kafka/commit/4c5d1e69c288cbf6aecb9864c0f859502e2ca301", "message": "Exception on partial response", "committedDate": "2020-05-04T17:13:25Z", "type": "commit"}, {"oid": "e0fc9bf5e68896bd034ea8e1bc863db313ce7e98", "url": "https://github.com/apache/kafka/commit/e0fc9bf5e68896bd034ea8e1bc863db313ce7e98", "message": "Add extra test", "committedDate": "2020-05-05T16:09:37Z", "type": "commit"}, {"oid": "103a123ff12524aed9c45235a851c9df882ceb32", "url": "https://github.com/apache/kafka/commit/103a123ff12524aed9c45235a851c9df882ceb32", "message": "Two more tests", "committedDate": "2020-05-05T17:11:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYxNTkzOA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420615938", "bodyText": "nit: Can we align it with the new above?", "author": "dajac", "createdAt": "2020-05-06T08:13:07Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -477,6 +484,27 @@ public void testCreateTopics() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testCreateTopicsPartialResponse() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponse(body -> body instanceof CreateTopicsRequest,\n+                    prepareCreateTopicsResponse(\"myTopic\", Errors.NONE));\n+            CreateTopicsResult topicsResult = env.adminClient().createTopics(\n+                    asList(new NewTopic(\"myTopic\", Collections.singletonMap(0, asList(0, 1, 2))),\n+                            new NewTopic(\"myTopic2\", Collections.singletonMap(0, asList(0, 1, 2)))),", "originalCommit": "103a123ff12524aed9c45235a851c9df882ceb32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYxNzQwOQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420617409", "bodyText": "nit: indentation is a bit inconsistent here.", "author": "dajac", "createdAt": "2020-05-06T08:15:47Z", "path": "clients/src/test/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequestTest.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.LogDirNotFoundException;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDir;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDirCollection;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopic;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopicCollection;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.junit.Test;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.singletonList;\n+import static org.junit.Assert.assertEquals;\n+\n+public class AlterReplicaLogDirsRequestTest {\n+\n+    @Test\n+    public void testErrorResponse() {\n+        AlterReplicaLogDirsRequestData data = new AlterReplicaLogDirsRequestData()\n+                .setDirs(new AlterReplicaLogDirCollection(\n+                        singletonList(new AlterReplicaLogDir()\n+                                .setPath(\"/data0\")\n+                                .setTopics(new AlterReplicaLogDirTopicCollection(\n+                                        singletonList(new AlterReplicaLogDirTopic()\n+                                                .setName(\"topic\")\n+                                                .setPartitions(asList(0, 1, 2))).iterator()))).iterator()));\n+        AlterReplicaLogDirsResponse errorResponse = new AlterReplicaLogDirsRequest.Builder(data).build()\n+                .getErrorResponse(123, new LogDirNotFoundException(\"/data0\"));\n+        assertEquals(1, errorResponse.data().results().size());\n+        AlterReplicaLogDirTopicResult topicResponse = errorResponse.data().results().get(0);\n+        assertEquals(\"topic\", topicResponse.topicName());\n+        assertEquals(3, topicResponse.partitions().size());\n+        for (int i = 0; i < 3; i++) {\n+            assertEquals(i, topicResponse.partitions().get(i).partitionIndex());\n+            assertEquals(Errors.LOG_DIR_NOT_FOUND.code(), topicResponse.partitions().get(i).errorCode());\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionDir() {\n+        AlterReplicaLogDirsRequestData data = new AlterReplicaLogDirsRequestData()\n+                .setDirs(new AlterReplicaLogDirCollection(\n+                        asList(new AlterReplicaLogDir()\n+                                .setPath(\"/data0\")\n+                                .setTopics(new AlterReplicaLogDirTopicCollection(\n+                                        asList(new AlterReplicaLogDirTopic()\n+                                                .setName(\"topic\")\n+                                                .setPartitions(asList(0, 1)),\n+                                                new AlterReplicaLogDirTopic()\n+                                                        .setName(\"topic2\")\n+                                                        .setPartitions(asList(7))).iterator())),", "originalCommit": "103a123ff12524aed9c45235a851c9df882ceb32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYxNzk5Ng==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420617996", "bodyText": "nit: Indentation of these two lines look weird.", "author": "dajac", "createdAt": "2020-05-06T08:16:54Z", "path": "clients/src/test/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequestTest.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.LogDirNotFoundException;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDir;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDirCollection;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopic;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopicCollection;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.junit.Test;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.singletonList;\n+import static org.junit.Assert.assertEquals;\n+\n+public class AlterReplicaLogDirsRequestTest {\n+\n+    @Test\n+    public void testErrorResponse() {\n+        AlterReplicaLogDirsRequestData data = new AlterReplicaLogDirsRequestData()\n+                .setDirs(new AlterReplicaLogDirCollection(\n+                        singletonList(new AlterReplicaLogDir()\n+                                .setPath(\"/data0\")\n+                                .setTopics(new AlterReplicaLogDirTopicCollection(\n+                                        singletonList(new AlterReplicaLogDirTopic()\n+                                                .setName(\"topic\")\n+                                                .setPartitions(asList(0, 1, 2))).iterator()))).iterator()));\n+        AlterReplicaLogDirsResponse errorResponse = new AlterReplicaLogDirsRequest.Builder(data).build()\n+                .getErrorResponse(123, new LogDirNotFoundException(\"/data0\"));\n+        assertEquals(1, errorResponse.data().results().size());\n+        AlterReplicaLogDirTopicResult topicResponse = errorResponse.data().results().get(0);\n+        assertEquals(\"topic\", topicResponse.topicName());\n+        assertEquals(3, topicResponse.partitions().size());\n+        for (int i = 0; i < 3; i++) {\n+            assertEquals(i, topicResponse.partitions().get(i).partitionIndex());\n+            assertEquals(Errors.LOG_DIR_NOT_FOUND.code(), topicResponse.partitions().get(i).errorCode());\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionDir() {\n+        AlterReplicaLogDirsRequestData data = new AlterReplicaLogDirsRequestData()\n+                .setDirs(new AlterReplicaLogDirCollection(\n+                        asList(new AlterReplicaLogDir()\n+                                .setPath(\"/data0\")\n+                                .setTopics(new AlterReplicaLogDirTopicCollection(\n+                                        asList(new AlterReplicaLogDirTopic()\n+                                                .setName(\"topic\")\n+                                                .setPartitions(asList(0, 1)),\n+                                                new AlterReplicaLogDirTopic()\n+                                                        .setName(\"topic2\")\n+                                                        .setPartitions(asList(7))).iterator())),\n+                                new AlterReplicaLogDir()\n+                                        .setPath(\"/data1\")\n+                                        .setTopics(new AlterReplicaLogDirTopicCollection(\n+                                                asList(new AlterReplicaLogDirTopic()\n+                                                                .setName(\"topic3\")\n+                                                                .setPartitions(asList(12))).iterator()))).iterator()));", "originalCommit": "103a123ff12524aed9c45235a851c9df882ceb32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYxODY0MA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420618640", "bodyText": "nit: i would bring the second part on the first line.", "author": "dajac", "createdAt": "2020-05-06T08:18:14Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2234,18 +2258,29 @@ public AlterReplicaLogDirsResult alterReplicaLogDirs(Map<TopicPartitionReplica,\n                 @Override\n                 public void handleResponse(AbstractResponse abstractResponse) {\n                     AlterReplicaLogDirsResponse response = (AlterReplicaLogDirsResponse) abstractResponse;\n-                    for (Map.Entry<TopicPartition, Errors> responseEntry: response.responses().entrySet()) {\n-                        TopicPartition tp = responseEntry.getKey();\n-                        Errors error = responseEntry.getValue();\n-                        TopicPartitionReplica replica = new TopicPartitionReplica(tp.topic(), tp.partition(), brokerId);\n-                        KafkaFutureImpl<Void> future = futures.get(replica);\n-                        if (future == null) {\n-                            handleFailure(new IllegalStateException(\n-                                \"The partition \" + tp + \" in the response from broker \" + brokerId + \" is not in the request\"));\n-                        } else if (error == Errors.NONE) {\n-                            future.complete(null);\n-                        } else {\n-                            future.completeExceptionally(error.exception());\n+                    for (AlterReplicaLogDirTopicResult topicResult: response.data().results()) {\n+                        for (AlterReplicaLogDirPartitionResult partitionResult: topicResult.partitions()) {\n+                            TopicPartitionReplica replica = new TopicPartitionReplica(topicResult.topicName(), partitionResult.partitionIndex(), brokerId);\n+                            KafkaFutureImpl<Void> future = futures.get(replica);\n+                            if (future == null) {\n+                                log.warn(\"The partition {} in the response from broker {}} is not in the request\",\n+                                        new TopicPartition(topicResult.topicName(), partitionResult.partitionIndex()),\n+                                        brokerId);\n+                            } else if (partitionResult.errorCode() == Errors.NONE.code()) {\n+                                future.complete(null);\n+                            } else {\n+                                future.completeExceptionally(Errors.forCode(partitionResult.errorCode()).exception());\n+                            }\n+                        }\n+                    }\n+                    // The server should send back a response for every replica. But do a sanity check anyway.\n+                    for (Map.Entry<TopicPartitionReplica, KafkaFutureImpl<Void>> entry : futures.entrySet()) {\n+                        TopicPartitionReplica replica = entry.getKey();\n+                        KafkaFutureImpl<Void> future = entry.getValue();\n+                        if (!future.isDone()\n+                                && replica.brokerId() == brokerId) {", "originalCommit": "103a123ff12524aed9c45235a851c9df882ceb32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYyMTYwNA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420621604", "bodyText": "nit: . at the end to stay consistent with previous line.", "author": "dajac", "createdAt": "2020-05-06T08:23:43Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -1402,6 +1407,15 @@ int numPendingCalls() {\n         return runnable.pendingCalls.size();\n     }\n \n+    /**\n+     * Fail the given future when a response handler expected a result for an entity but no result was present.\n+     * @param future The future to fail.\n+     * @param message The message to fail the future with", "originalCommit": "103a123ff12524aed9c45235a851c9df882ceb32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYyNDI4MQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420624281", "bodyText": "I am not entirely convinced by the name. At the end, the method complete a future with the given message. There is nothing specific to handling partial responses. I would rename it to something more generic or simply remove it as we don't gain much with it.", "author": "dajac", "createdAt": "2020-05-06T08:28:31Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -1402,6 +1407,15 @@ int numPendingCalls() {\n         return runnable.pendingCalls.size();\n     }\n \n+    /**\n+     * Fail the given future when a response handler expected a result for an entity but no result was present.\n+     * @param future The future to fail.\n+     * @param message The message to fail the future with\n+     */\n+    private void partialResponse(KafkaFutureImpl<?> future, String message) {", "originalCommit": "103a123ff12524aed9c45235a851c9df882ceb32", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDY0NDc5NQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420644795", "bodyText": "I agree it's not doing much. What value it has is in trying to handle these cases in a consistent way, and being able to more easily discover/reason about the call sites. Maybe something like invalidBrokerResponse() would be a better name? But if you don't like that I'm happy to remove it.", "author": "tombentley", "createdAt": "2020-05-06T09:04:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYyNDI4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDY2NDMzMQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420664331", "bodyText": "I see your point. I am definitely for reusing logic as much as possible for those cases. What about going an extra step and do something like this then (code not tested)? Just an idea...\npublic static <K, V> void completeUnrealizedFutures(\n            Map<K, KafkaFutureImpl<V>> futures,\n            Function<K, Boolean> filter,\n            Function<K, String> messageFormatter) {\n        for (Map.Entry<K, KafkaFutureImpl<V>> entry : futures.entrySet()) {\n            K key = entry.getKey();\n            KafkaFutureImpl<V> future = entry.getValue();\n            if (!future.isDone() && filter.apply(key)) {\n                future.completeExceptionally(new ApiException(messageFormatter.apply(key)));\n            }\n        }\n    }\n\n    public static  <K, V> void completeUnrealizedFutures(\n            Map<K, KafkaFutureImpl<V>> futures,\n            unction<K, String> messageFormatter) {\n        completeUnrealizedFutures(futures, (key) -> true, messageFormatter);\n    }\n\nIt would allow the following:\ncompleteUnrealizedFutures(topicFutures,\n     replica -> replica.brokerId() == brokerId,\n     topic -> \"The response from broker \" + brokerId + \" did not contain a result for replica \" + replica);\n\ncompleteUnrealizedFutures(topicFutures,\n     topic -> \"The server response did not contain a reference to node \" + topic);", "author": "dajac", "createdAt": "2020-05-06T09:39:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYyNDI4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDg2MTg2NA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420861864", "bodyText": "That works for me, especially if this is something you're going to make more use of for the other response handlers in the admin client.", "author": "tombentley", "createdAt": "2020-05-06T15:00:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYyNDI4MQ=="}], "type": "inlineReview"}, {"oid": "0ebdc39ffae3c6cb1cea27cfcbc95f41c989746d", "url": "https://github.com/apache/kafka/commit/0ebdc39ffae3c6cb1cea27cfcbc95f41c989746d", "message": "Code review", "committedDate": "2020-05-06T09:02:00Z", "type": "commit"}, {"oid": "3d1aa65a267132bc7bdd00244525a288dd863dd4", "url": "https://github.com/apache/kafka/commit/3d1aa65a267132bc7bdd00244525a288dd863dd4", "message": "completeUnrealizedFutures", "committedDate": "2020-05-06T13:51:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5MDA2Mg==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r422090062", "bodyText": "nit: we can bring this back on the first line.", "author": "dajac", "createdAt": "2020-05-08T11:25:00Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -1479,13 +1512,9 @@ public void handleResponse(AbstractResponse abstractResponse) {\n                     }\n                 }\n                 // The server should send back a response for every topic. But do a sanity check anyway.\n-                for (Map.Entry<String, KafkaFutureImpl<TopicMetadataAndConfig>> entry : topicFutures.entrySet()) {\n-                    KafkaFutureImpl<TopicMetadataAndConfig> future = entry.getValue();\n-                    if (!future.isDone()) {\n-                        future.completeExceptionally(new ApiException(\"The server response did not \" +\n-                            \"contain a reference to node \" + entry.getKey()));\n-                    }\n-                }\n+                completeUnrealizedFutures(topicFutures,\n+                    topic -> \"The controller response \" +\n+                            \"did not contain a result for topic \" + topic);", "originalCommit": "3d1aa65a267132bc7bdd00244525a288dd863dd4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5MDA4NA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r422090084", "bodyText": "nit: we can bring this back on the first line.", "author": "dajac", "createdAt": "2020-05-08T11:25:05Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -1552,13 +1581,9 @@ void handleResponse(AbstractResponse abstractResponse) {\n                     }\n                 }\n                 // The server should send back a response for every topic. But do a sanity check anyway.\n-                for (Map.Entry<String, KafkaFutureImpl<Void>> entry : topicFutures.entrySet()) {\n-                    KafkaFutureImpl<Void> future = entry.getValue();\n-                    if (!future.isDone()) {\n-                        future.completeExceptionally(new ApiException(\"The server response did not \" +\n-                            \"contain a reference to node \" + entry.getKey()));\n-                    }\n-                }\n+                completeUnrealizedFutures(topicFutures,\n+                    topic -> \"The controller response \" +\n+                            \"did not contain a result for topic \" + topic);", "originalCommit": "3d1aa65a267132bc7bdd00244525a288dd863dd4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5MDM1Ng==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r422090356", "bodyText": "nit: We could break this long line.", "author": "dajac", "createdAt": "2020-05-08T11:25:48Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2234,20 +2269,26 @@ public AlterReplicaLogDirsResult alterReplicaLogDirs(Map<TopicPartitionReplica,\n                 @Override\n                 public void handleResponse(AbstractResponse abstractResponse) {\n                     AlterReplicaLogDirsResponse response = (AlterReplicaLogDirsResponse) abstractResponse;\n-                    for (Map.Entry<TopicPartition, Errors> responseEntry: response.responses().entrySet()) {\n-                        TopicPartition tp = responseEntry.getKey();\n-                        Errors error = responseEntry.getValue();\n-                        TopicPartitionReplica replica = new TopicPartitionReplica(tp.topic(), tp.partition(), brokerId);\n-                        KafkaFutureImpl<Void> future = futures.get(replica);\n-                        if (future == null) {\n-                            handleFailure(new IllegalStateException(\n-                                \"The partition \" + tp + \" in the response from broker \" + brokerId + \" is not in the request\"));\n-                        } else if (error == Errors.NONE) {\n-                            future.complete(null);\n-                        } else {\n-                            future.completeExceptionally(error.exception());\n+                    for (AlterReplicaLogDirTopicResult topicResult: response.data().results()) {\n+                        for (AlterReplicaLogDirPartitionResult partitionResult: topicResult.partitions()) {\n+                            TopicPartitionReplica replica = new TopicPartitionReplica(topicResult.topicName(), partitionResult.partitionIndex(), brokerId);", "originalCommit": "3d1aa65a267132bc7bdd00244525a288dd863dd4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6a794a656ad42f65dfff083a45a616513be72cd3", "url": "https://github.com/apache/kafka/commit/6a794a656ad42f65dfff083a45a616513be72cd3", "message": "Review comments + improvement", "committedDate": "2020-05-11T09:50:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTExOTMxNQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435119315", "bodyText": "We can use assertThrows() here. Same below", "author": "mimaison", "createdAt": "2020-06-04T09:31:03Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -477,6 +484,27 @@ public void testCreateTopics() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testCreateTopicsPartialResponse() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponse(body -> body instanceof CreateTopicsRequest,\n+                    prepareCreateTopicsResponse(\"myTopic\", Errors.NONE));\n+            CreateTopicsResult topicsResult = env.adminClient().createTopics(\n+                    asList(new NewTopic(\"myTopic\", Collections.singletonMap(0, asList(0, 1, 2))),\n+                           new NewTopic(\"myTopic2\", Collections.singletonMap(0, asList(0, 1, 2)))),\n+                    new CreateTopicsOptions().timeoutMs(10000));\n+            topicsResult.values().get(\"myTopic\").get();\n+            try {\n+                topicsResult.values().get(\"myTopic2\").get();\n+                fail(\"Expected an exception.\");\n+            } catch (ExecutionException e) {", "originalCommit": "6a794a656ad42f65dfff083a45a616513be72cd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTE2MjE3NA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435162174", "bodyText": "I don't think we can, at least not if we want to check that topicsResult.values().get(\"myTopic2\").get() throws an ExecutionException wrapping an ApiException. assertThrows() would only let us assert the outer exception type.", "author": "tombentley", "createdAt": "2020-06-04T10:46:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTExOTMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTIxODU2MA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435218560", "bodyText": "Sorry, I meant TestUtils.assertFutureThrows()", "author": "mimaison", "createdAt": "2020-06-04T12:35:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTExOTMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTIyMjIwNA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435222204", "bodyText": "Ah, thanks! Now fixed.", "author": "tombentley", "createdAt": "2020-06-04T12:42:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTExOTMxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTEyMjg2Nw==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435122867", "bodyText": "There's an extra }", "author": "mimaison", "createdAt": "2020-06-04T09:37:03Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2234,20 +2251,27 @@ public AlterReplicaLogDirsResult alterReplicaLogDirs(Map<TopicPartitionReplica,\n                 @Override\n                 public void handleResponse(AbstractResponse abstractResponse) {\n                     AlterReplicaLogDirsResponse response = (AlterReplicaLogDirsResponse) abstractResponse;\n-                    for (Map.Entry<TopicPartition, Errors> responseEntry: response.responses().entrySet()) {\n-                        TopicPartition tp = responseEntry.getKey();\n-                        Errors error = responseEntry.getValue();\n-                        TopicPartitionReplica replica = new TopicPartitionReplica(tp.topic(), tp.partition(), brokerId);\n-                        KafkaFutureImpl<Void> future = futures.get(replica);\n-                        if (future == null) {\n-                            handleFailure(new IllegalStateException(\n-                                \"The partition \" + tp + \" in the response from broker \" + brokerId + \" is not in the request\"));\n-                        } else if (error == Errors.NONE) {\n-                            future.complete(null);\n-                        } else {\n-                            future.completeExceptionally(error.exception());\n+                    for (AlterReplicaLogDirTopicResult topicResult: response.data().results()) {\n+                        for (AlterReplicaLogDirPartitionResult partitionResult: topicResult.partitions()) {\n+                            TopicPartitionReplica replica = new TopicPartitionReplica(\n+                                    topicResult.topicName(), partitionResult.partitionIndex(), brokerId);\n+                            KafkaFutureImpl<Void> future = futures.get(replica);\n+                            if (future == null) {\n+                                log.warn(\"The partition {} in the response from broker {}} is not in the request\",", "originalCommit": "6a794a656ad42f65dfff083a45a616513be72cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7b6f3c01f88055ac453f8a1288e9f3daf5c666dd", "url": "https://github.com/apache/kafka/commit/7b6f3c01f88055ac453f8a1288e9f3daf5c666dd", "message": "review comment", "committedDate": "2020-06-04T10:45:44Z", "type": "commit"}, {"oid": "66f796c147536847db2b90d88da648277c2e9767", "url": "https://github.com/apache/kafka/commit/66f796c147536847db2b90d88da648277c2e9767", "message": "TestUtils.assertFutureThrows", "committedDate": "2020-06-04T12:41:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0MzI4Nw==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435243287", "bodyText": "We can use TestUtils.assertFutureThrows() here too", "author": "mimaison", "createdAt": "2020-06-04T13:15:11Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -570,6 +592,27 @@ public void testDeleteTopics() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testDeleteTopicsPartialResponse() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+\n+            env.kafkaClient().prepareResponse(body -> body instanceof DeleteTopicsRequest,\n+                    prepareDeleteTopicsResponse(\"myTopic\", Errors.NONE));\n+            Map<String, KafkaFuture<Void>> values = env.adminClient().deleteTopics(asList(\"myTopic\", \"myOtherTopic\"),\n+                    new DeleteTopicsOptions()).values();\n+            values.get(\"myTopic\").get();\n+\n+            try {", "originalCommit": "66f796c147536847db2b90d88da648277c2e9767", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0MzQ2NQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435243465", "bodyText": "we can use TestUtils.assertFutureThrows() here too", "author": "mimaison", "createdAt": "2020-06-04T13:15:26Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3382,6 +3425,90 @@ public void testAlterClientQuotas() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testAlterReplicaLogDirsSuccess() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(0), Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(1), Errors.NONE, 0);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr0, \"/data0\");\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            assertNull(result.values().get(tpr0).get());\n+            assertNull(result.values().get(tpr1).get());\n+        }\n+    }\n+\n+    @Test\n+    public void testAlterReplicaLogDirsLogDirNotFound() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(0), Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(1), Errors.LOG_DIR_NOT_FOUND, 0);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr0, \"/data0\");\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            assertNull(result.values().get(tpr0).get());\n+            TestUtils.assertFutureError(result.values().get(tpr1), LogDirNotFoundException.class);\n+        }\n+    }\n+\n+    @Test\n+    public void testAlterReplicaLogDirsUnrequested() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(0), Errors.NONE, 1, 2);\n+\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 1, 0);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            assertNull(result.values().get(tpr1).get());\n+        }\n+    }\n+\n+    @Test\n+    public void testAlterReplicaLogDirsPartialResponse() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(0), Errors.NONE, 1);\n+\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 1, 0);\n+            TopicPartitionReplica tpr2 = new TopicPartitionReplica(\"topic\", 2, 0);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr1, \"/data1\");\n+            logDirs.put(tpr2, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            assertNull(result.values().get(tpr1).get());\n+            try {", "originalCommit": "66f796c147536847db2b90d88da648277c2e9767", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2af415b22d9671ee7aff320f104d975b084ea450", "url": "https://github.com/apache/kafka/commit/2af415b22d9671ee7aff320f104d975b084ea450", "message": "More TestUtils.assertFutureThrows", "committedDate": "2020-06-04T13:26:12Z", "type": "commit"}]}