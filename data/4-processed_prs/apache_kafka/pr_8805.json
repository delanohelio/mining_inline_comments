{"pr_number": 8805, "pr_title": "KAFKA-9848: Avoid triggering scheduled rebalance delay when task assignment fails but Connect workers remain in the group", "pr_createdAt": "2020-06-05T06:58:28Z", "pr_url": "https://github.com/apache/kafka/pull/8805", "timeline": [{"oid": "43cd70c4852a3a009d5f326904eba1dd8ddb0886", "url": "https://github.com/apache/kafka/commit/43cd70c4852a3a009d5f326904eba1dd8ddb0886", "message": "KAFKA-9848: Avoid triggering scheduled rebalance delay when task assignment fails but Connect workers remain in the group", "committedDate": "2020-06-06T00:26:21Z", "type": "commit"}, {"oid": "43cd70c4852a3a009d5f326904eba1dd8ddb0886", "url": "https://github.com/apache/kafka/commit/43cd70c4852a3a009d5f326904eba1dd8ddb0886", "message": "KAFKA-9848: Avoid triggering scheduled rebalance delay when task assignment fails but Connect workers remain in the group", "committedDate": "2020-06-06T00:26:21Z", "type": "forcePushed"}, {"oid": "2cfd4d8e8ef672894e7c4cdf6cf8fe3722c7c992", "url": "https://github.com/apache/kafka/commit/2cfd4d8e8ef672894e7c4cdf6cf8fe3722c7c992", "message": "KAFKA-9848: Treat lost assignments as new only if a delay is not already in effect", "committedDate": "2020-06-07T01:57:06Z", "type": "commit"}, {"oid": "3a32fba0b9f51c137567480a53d01c08464df5f9", "url": "https://github.com/apache/kafka/commit/3a32fba0b9f51c137567480a53d01c08464df5f9", "message": "KAFKA-9848: Fix unit tests for IncrementalCooperativeAssignor", "committedDate": "2020-06-07T01:57:52Z", "type": "commit"}, {"oid": "5ba43141dc4c991401c71d62ffda62dab7f30fae", "url": "https://github.com/apache/kafka/commit/5ba43141dc4c991401c71d62ffda62dab7f30fae", "message": "KAFKA-9848: Test assignment failure outside the assignor", "committedDate": "2020-06-07T04:35:09Z", "type": "commit"}, {"oid": "a0332e53309181c37b56f3a58da1434dee99945a", "url": "https://github.com/apache/kafka/commit/a0332e53309181c37b56f3a58da1434dee99945a", "message": "KAFKA-9848: Fix additional comments", "committedDate": "2020-06-08T02:01:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjkyNTMxNQ==", "url": "https://github.com/apache/kafka/pull/8805#discussion_r436925315", "bodyText": "Can we make this a little easier to understand for most users? I think it might be sufficient to add some combination of:\n\nwhat this means (e.g., the worker was partitioned and missed at least one rebalance rounds, likely due to a network issue), and\nwhat resulted (e.g., the workers gave up its tasks in case the cluster had reassigned them to another worker).\n\nAnd, should this be debug or info or warn? Warn seems wrong, since the user shouldn't do anything, but excessive #s of these could signal the need for additional tuning. WDYT?", "author": "rhauch", "createdAt": "2020-06-08T18:59:19Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeAssignor.java", "diffHunk": "@@ -159,6 +163,13 @@ private Long ensureLeaderConfig(long maxOffset, WorkerCoordinator coordinator) {\n         // Base set: The previous assignment of connectors-and-tasks is a standalone snapshot that\n         // can be used to calculate derived sets\n         log.debug(\"Previous assignments: {}\", previousAssignment);\n+        int lastCompletedGenerationId = coordinator.lastCompletedGenerationId();\n+        if (previousGenerationId != lastCompletedGenerationId) {\n+            log.debug(\"Emptying previous assignments due to generation mismatch between previous \"\n+                    + \"generation ID {} and last completed generation ID {} since the last assignment: {}\",\n+                    previousGenerationId, lastCompletedGenerationId, previousAssignment);", "originalCommit": "a0332e53309181c37b56f3a58da1434dee99945a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjk1MDMwNQ==", "url": "https://github.com/apache/kafka/pull/8805#discussion_r436950305", "bodyText": "This is related to the leader's internal bookkeeping when it calculates a new assignment. It's not related to the tasks that a worker (even the leader) is actually running.\nEmptying/clearing the previous assignment might result in some tasks shuffling around, because the leader will calculate an assignment from scratch, but it doesn't affect running tasks. The new computed assignment will send assignment and/or revocations as needed based on a) what tasks each worker has reported running in this round and which tasks are configured in the config topic. Another way to say this is that the leader won't bother detecting lost tasks in this round. Every unassigned task will be treated as a new task.\nYou are right on the log message not conveying that meaning exactly. How about:\nlog.debug(\"Clearing the view of previous assignments due to generation mismatch between \"\n                    + \"previous generation ID {} and last completed generation ID {}. \",\n                    previousGenerationId, lastCompletedGenerationId);\nlog.debug(\"This can happen if the leader fails to sync the assignment within a \" \n                    + \"rebalancing round. The following view of previous assignments might be \"\n                    + \"outdated and will be ignored by the leader in the current computation of \" \n                    + \"new assignments. Possibly outdated previous assignments: {}\", previousAssignment);", "author": "kkonstantine", "createdAt": "2020-06-08T19:29:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjkyNTMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjk1MTQ3NQ==", "url": "https://github.com/apache/kafka/pull/8805#discussion_r436951475", "bodyText": "Also, given that the previous assignments are printed in debug, I think it makes sense to keep these log messages in debug as well.", "author": "kkonstantine", "createdAt": "2020-06-08T19:31:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjkyNTMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjk2MTEyMA==", "url": "https://github.com/apache/kafka/pull/8805#discussion_r436961120", "bodyText": "Sounds good, though it'd be better to have a single (long) log message to prevent them from being separated by other log messages from other threads.", "author": "rhauch", "createdAt": "2020-06-08T19:50:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjkyNTMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjk2MzU3Mg==", "url": "https://github.com/apache/kafka/pull/8805#discussion_r436963572", "bodyText": "I agree. The length will be very similar anyways.\nI'm pushing a commit to address your comments.", "author": "kkonstantine", "createdAt": "2020-06-08T19:55:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjkyNTMxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjkyNjQ3Mg==", "url": "https://github.com/apache/kafka/pull/8805#discussion_r436926472", "bodyText": "Similar comment to that above.", "author": "rhauch", "createdAt": "2020-06-08T19:00:59Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeAssignor.java", "diffHunk": "@@ -361,6 +369,14 @@ protected void handleLostAssignments(ConnectorsAndTasks lostAssignments,\n         log.debug(\"Found the following connectors and tasks missing from previous assignments: \"\n                 + lostAssignments);\n \n+        if (previousMembers.size() == memberConfigs.size() && scheduledRebalance <= 0) {\n+            log.debug(\"Group size is same between rebalances. Lost assignments are probably due to lost SyncGroup \"\n+                    + \"responses. Treating lost tasks as new tasks\");", "originalCommit": "a0332e53309181c37b56f3a58da1434dee99945a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjk1Mzc5NQ==", "url": "https://github.com/apache/kafka/pull/8805#discussion_r436953795", "bodyText": "How about:\nlog.debug(\"The number of workers remained the same between rebalances. The missing \" \n                    + \"assignments that the leader is detecting are probably due to some workers \" \n                    + \"failing to receive the new assignments in the previous rebalance. Will \"\n                    + \"reassign missing tasks as new tasks\");", "author": "kkonstantine", "createdAt": "2020-06-08T19:35:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjkyNjQ3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjk2MTM2NA==", "url": "https://github.com/apache/kafka/pull/8805#discussion_r436961364", "bodyText": "Sounds good.", "author": "rhauch", "createdAt": "2020-06-08T19:50:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjkyNjQ3Mg=="}], "type": "inlineReview"}, {"oid": "6de42a49e746e520493cd5ac08fc5b3e61f43b11", "url": "https://github.com/apache/kafka/commit/6de42a49e746e520493cd5ac08fc5b3e61f43b11", "message": "KAFKA-9848: Improve log messages during task assignment", "committedDate": "2020-06-08T20:25:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjk5ODM5Ng==", "url": "https://github.com/apache/kafka/pull/8805#discussion_r436998396", "bodyText": "Is it enough to trust that the # of workers has not changed, or should we compare the members, via something like:\nif (previousMembers.equals(memberConfigs.keySet()) && scheduledRebalance <= 0) {\n\nIOW, what happens if one worker disappeared about the same time that an operator added a new worker?\nIIUC from the integration tests, this logic actually doesn't care which of these is the case -- all of the task assignments that were lost will be reassigned anyway, so it doesn't matter if the worker that gets those new assignments is the old worker that came back or a new worker. Is that right?", "author": "rhauch", "createdAt": "2020-06-08T21:01:19Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeAssignor.java", "diffHunk": "@@ -361,6 +373,16 @@ protected void handleLostAssignments(ConnectorsAndTasks lostAssignments,\n         log.debug(\"Found the following connectors and tasks missing from previous assignments: \"\n                 + lostAssignments);\n \n+        if (previousMembers.size() == memberConfigs.size() && scheduledRebalance <= 0) {", "originalCommit": "6de42a49e746e520493cd5ac08fc5b3e61f43b11", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzAwNzYxMQ==", "url": "https://github.com/apache/kafka/pull/8805#discussion_r437007611", "bodyText": "I like the idea of checking the member configs. Could probably allow us to avoid duplicate tasks, even in this rare scenario of replacement of a departing node within the rebalance round itself.", "author": "kkonstantine", "createdAt": "2020-06-08T21:19:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjk5ODM5Ng=="}], "type": "inlineReview"}, {"oid": "f5dfaf435e3f06293953b2b1f73e68b8bab414de", "url": "https://github.com/apache/kafka/commit/f5dfaf435e3f06293953b2b1f73e68b8bab414de", "message": "KAFKA-9848: Compare actual member IDs when deciding to reassign missing tasks", "committedDate": "2020-06-08T21:21:56Z", "type": "commit"}, {"oid": "46567beb2515d632471441685af29f1e684565c2", "url": "https://github.com/apache/kafka/commit/46567beb2515d632471441685af29f1e684565c2", "message": "KAFKA-9848: Confirm that all the previous workers remain in the group", "committedDate": "2020-06-08T21:49:39Z", "type": "commit"}, {"oid": "8d8a6619ea3bce0221b5efebee76f9810a8a5a7b", "url": "https://github.com/apache/kafka/commit/8d8a6619ea3bce0221b5efebee76f9810a8a5a7b", "message": "KAFKA-9848: Add a MethodLength checkstyle suppression", "committedDate": "2020-06-09T01:43:41Z", "type": "commit"}, {"oid": "1210d0b2e404f620f16295ca60a104df96e3007b", "url": "https://github.com/apache/kafka/commit/1210d0b2e404f620f16295ca60a104df96e3007b", "message": "KAFKA-9848: Extend tests on lost assignments to consider previous members", "committedDate": "2020-06-09T06:00:39Z", "type": "commit"}]}