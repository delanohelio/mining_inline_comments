{"pr_number": 9178, "pr_title": "KAFKA-8362: fix the old checkpoint won't be removed after alter log dir", "pr_createdAt": "2020-08-14T01:46:11Z", "pr_url": "https://github.com/apache/kafka/pull/9178", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "bb1ad9cb612218bb793dd294b552721bcb89dd02", "url": "https://github.com/apache/kafka/commit/bb1ad9cb612218bb793dd294b552721bcb89dd02", "message": "KAFKA-8362: fix the old checkpoint won't be removed after alter log dir and add tests", "committedDate": "2020-08-14T02:33:48Z", "type": "commit"}, {"oid": "bb1ad9cb612218bb793dd294b552721bcb89dd02", "url": "https://github.com/apache/kafka/commit/bb1ad9cb612218bb793dd294b552721bcb89dd02", "message": "KAFKA-8362: fix the old checkpoint won't be removed after alter log dir and add tests", "committedDate": "2020-08-14T02:33:48Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk1ODQzOA==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485958438", "bodyText": "typo absooute", "author": "junrao", "createdAt": "2020-09-09T22:33:42Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1698,8 +1698,12 @@ class ReplicaManager(val config: KafkaConfig,\n     Partition.removeMetrics(tp)\n   }\n \n-  // logDir should be an absolute path\n-  // sendZkNotification is needed for unit test\n+  /**\n+   * The log directory failure handler for the replica\n+   *\n+   * @param dir                     the absooute path of the log directory", "originalCommit": "bb1ad9cb612218bb793dd294b552721bcb89dd02", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk1ODYyOA==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485958628", "bodyText": "typo diretory", "author": "junrao", "createdAt": "2020-09-09T22:34:12Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -184,7 +184,11 @@ class LogManager(logDirs: Seq[File],\n     numRecoveryThreadsPerDataDir = newSize\n   }\n \n-  // dir should be an absolute path\n+  /**\n+   * The log diretory failure handler. It'll remove all the checkpoint files located in the directory", "originalCommit": "bb1ad9cb612218bb793dd294b552721bcb89dd02", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2MzE3Mg==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485963172", "bodyText": "Could we make topicPartitionToBeRemoved as Option[TopicPartition]?", "author": "junrao", "createdAt": "2020-09-09T22:47:33Z", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -354,12 +354,24 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)]): Unit = {\n+  /**\n+   * Update checkpoint file, or removing topics and partitions that no longer exist\n+   *\n+   * @param dataDir                       The File object to be updated\n+   * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n+   * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n+   */\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: TopicPartition = null): Unit = {", "originalCommit": "bb1ad9cb612218bb793dd294b552721bcb89dd02", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjI2NjU0Nw==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486266547", "bodyText": "Updated. Thanks.", "author": "showuon", "createdAt": "2020-09-10T11:35:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2MzE3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2NDEzOA==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485964138", "bodyText": "updatedOffset is not being used.", "author": "junrao", "createdAt": "2020-09-09T22:50:15Z", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -354,12 +354,24 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)]): Unit = {\n+  /**\n+   * Update checkpoint file, or removing topics and partitions that no longer exist\n+   *\n+   * @param dataDir                       The File object to be updated\n+   * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n+   * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n+   */\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: TopicPartition = null): Unit = {\n     inLock(lock) {\n       val checkpoint = checkpoints(dataDir)\n       if (checkpoint != null) {\n         try {\n-          val existing = checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) } ++ update\n+          val existing = update match {\n+            case Some(updatedOffset) =>", "originalCommit": "bb1ad9cb612218bb793dd294b552721bcb89dd02", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjI2NjcwMQ==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486266701", "bodyText": "Nice catch. Thanks.", "author": "showuon", "createdAt": "2020-09-10T11:35:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2NDEzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2NDMyOQ==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485964329", "bodyText": "typo direcotory", "author": "junrao", "createdAt": "2020-09-09T22:50:52Z", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -369,13 +381,21 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n+  /**\n+   * alter the checkpoint directory for the topicPartition, to remove the data in sourceLogDir, and add the data in destLogDir\n+   */\n   def alterCheckpointDir(topicPartition: TopicPartition, sourceLogDir: File, destLogDir: File): Unit = {\n     inLock(lock) {\n       try {\n         checkpoints.get(sourceLogDir).flatMap(_.read().get(topicPartition)) match {\n           case Some(offset) =>\n-            // Remove this partition from the checkpoint file in the source log directory\n-            updateCheckpoints(sourceLogDir, None)\n+            debug(s\"Removing the partition offset data in checkpoint file for '${topicPartition}' \" +\n+              s\"from ${sourceLogDir.getAbsoluteFile} direcotory.\")", "originalCommit": "bb1ad9cb612218bb793dd294b552721bcb89dd02", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2NDcwNw==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485964707", "bodyText": "removing => remove", "author": "junrao", "createdAt": "2020-09-09T22:51:59Z", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -354,12 +354,24 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)]): Unit = {\n+  /**\n+   * Update checkpoint file, or removing topics and partitions that no longer exist", "originalCommit": "bb1ad9cb612218bb793dd294b552721bcb89dd02", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2NTYyNg==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485965626", "bodyText": "Stop the cleaning logs => Stop cleaning logs", "author": "junrao", "createdAt": "2020-09-09T22:54:53Z", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -393,13 +413,21 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n+  /**\n+   * Stop the cleaning logs in the provided directory", "originalCommit": "bb1ad9cb612218bb793dd294b552721bcb89dd02", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2Njc2MQ==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485966761", "bodyText": "We now have 2 different formats for checkpoint files, one for OffsetCheckpointFile and another for LeaderEpochCheckpointFile. Perhaps we can add the above comment to the appropriate class.", "author": "junrao", "createdAt": "2020-09-09T22:58:25Z", "path": "core/src/main/scala/kafka/server/checkpoints/CheckpointFile.scala", "diffHunk": "@@ -75,6 +75,17 @@ class CheckpointReadBuffer[T](location: String,\n   }\n }\n \n+/**\n+ * This class interacts with the checkpoint file to read or write [TopicPartition, Offset] entries\n+ *\n+ * The format in the checkpoint file is like this:\n+ *  -----checkpoint file content------\n+ *  0                <- OffsetCheckpointFile.currentVersion\n+ *  2                <- following entries size\n+ *  tp1  par1  1     <- the format is: TOPIC  PARTITION  OFFSET\n+ *  tp1  par2  2\n+ *  -----checkpoint file end----------\n+ */", "originalCommit": "bb1ad9cb612218bb793dd294b552721bcb89dd02", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjI2Nzc0NA==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486267744", "bodyText": "Thanks for reminding! I've moved my comments to OffsetCheckpointFile. And add some format comments in LeaderEpochCheckpointFile. Thanks.", "author": "showuon", "createdAt": "2020-09-10T11:37:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2Njc2MQ=="}], "type": "inlineReview"}, {"oid": "9005372c674fedd5332fc93f8f4ce47718138f1f", "url": "https://github.com/apache/kafka/commit/9005372c674fedd5332fc93f8f4ce47718138f1f", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KAFKA-8362", "committedDate": "2020-09-10T09:23:08Z", "type": "commit"}, {"oid": "5320318eec5cdb938cafd385f45525472370d359", "url": "https://github.com/apache/kafka/commit/5320318eec5cdb938cafd385f45525472370d359", "message": "KAFKA-8362: address reviewer's comments to refactor the codes", "committedDate": "2020-09-10T11:29:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ1NDY5Mw==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486454693", "bodyText": "To be consistent, perhaps change update to partitionToUpdateOrAdd and topicPartitionToBeRemoved to partitionToRemove?", "author": "junrao", "createdAt": "2020-09-10T15:55:58Z", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -354,12 +354,30 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)]): Unit = {\n+  /**\n+   * Update checkpoint file, or remove topics and partitions that no longer exist\n+   *\n+   * @param dataDir                       The File object to be updated\n+   * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n+   * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n+   */\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc0NzA3Nw==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486747077", "bodyText": "OK", "author": "showuon", "createdAt": "2020-09-11T03:08:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ1NDY5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4MTg4Mg==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486481882", "bodyText": "This method assumes that only one of update and topicPartitionToBeRemoved will be set. Perhaps we could just handle the more general case that both could be set?", "author": "junrao", "createdAt": "2020-09-10T16:35:58Z", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -355,22 +355,28 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n   }\n \n   /**\n-   * Update checkpoint file, or removing topics and partitions that no longer exist\n+   * Update checkpoint file, or remove topics and partitions that no longer exist\n    *\n    * @param dataDir                       The File object to be updated\n    * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n    * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n    */\n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: TopicPartition = null): Unit = {\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {\n     inLock(lock) {\n       val checkpoint = checkpoints(dataDir)\n       if (checkpoint != null) {\n         try {\n           val existing = update match {\n             case Some(updatedOffset) =>\n-              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap ++ update\n+              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap + updatedOffset\n             case None =>\n-              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap - topicPartitionToBeRemoved\n+              topicPartitionToBeRemoved match {", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc0NzE1OQ==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486747159", "bodyText": "Good suggestion! Updated.", "author": "showuon", "createdAt": "2020-09-11T03:08:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4MTg4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4MjExOQ==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486482119", "bodyText": "Do we need to log this?", "author": "junrao", "createdAt": "2020-09-10T16:36:19Z", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -355,22 +355,28 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n   }\n \n   /**\n-   * Update checkpoint file, or removing topics and partitions that no longer exist\n+   * Update checkpoint file, or remove topics and partitions that no longer exist\n    *\n    * @param dataDir                       The File object to be updated\n    * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n    * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n    */\n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: TopicPartition = null): Unit = {\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {\n     inLock(lock) {\n       val checkpoint = checkpoints(dataDir)\n       if (checkpoint != null) {\n         try {\n           val existing = update match {\n             case Some(updatedOffset) =>\n-              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap ++ update\n+              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap + updatedOffset\n             case None =>\n-              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap - topicPartitionToBeRemoved\n+              topicPartitionToBeRemoved match {\n+                case Some(topicPartion) =>\n+                  checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap - topicPartion\n+                case None =>\n+                  info(s\"Nothing added or removed for ${dataDir.getAbsoluteFile} directory in updateCheckpoints.\")", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc0NzIwNA==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486747204", "bodyText": "Removed. Thanks.", "author": "showuon", "createdAt": "2020-09-11T03:08:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4MjExOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4Mjk0Mg==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486482942", "bodyText": "Could we use the named param for update in updateCheckpoints() below and other places to make it clear?", "author": "junrao", "createdAt": "2020-09-10T16:37:41Z", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -390,9 +396,9 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n         checkpoints.get(sourceLogDir).flatMap(_.read().get(topicPartition)) match {\n           case Some(offset) =>\n             debug(s\"Removing the partition offset data in checkpoint file for '${topicPartition}' \" +\n-              s\"from ${sourceLogDir.getAbsoluteFile} direcotory.\")\n+              s\"from ${sourceLogDir.getAbsoluteFile} directory.\")\n             // Remove this partition data from the checkpoint file in the source log directory\n-            updateCheckpoints(sourceLogDir, None, topicPartitionToBeRemoved = topicPartition)\n+            updateCheckpoints(sourceLogDir, None, topicPartitionToBeRemoved = Some(topicPartition))", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc0NzU5Mg==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486747592", "bodyText": "Good suggestion. I make the default value of 2nd parameter partitionToUpdateOrAdd to be None, so here, I can just call with 2 params: updateCheckpoints(sourceLogDir, topicPartitionToBeRemoved = Some(topicPartition)), and other places as well. Thanks.", "author": "showuon", "createdAt": "2020-09-11T03:10:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4Mjk0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4NjAyMg==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486486022", "bodyText": "Truncate the checkpoint file for the given partition => Truncate the checkpointed offset for the given partition", "author": "junrao", "createdAt": "2020-09-10T16:42:36Z", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -393,13 +419,21 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n+  /**\n+   * Stop cleaning logs in the provided directory\n+   *\n+   * @param dir     the absolute path of the log dir\n+   */\n   def handleLogDirFailure(dir: String): Unit = {\n     warn(s\"Stopping cleaning logs in dir $dir\")\n     inLock(lock) {\n       checkpoints = checkpoints.filter { case (k, _) => k.getAbsolutePath != dir }\n     }\n   }\n \n+  /**\n+   * Truncate the checkpoint file for the given partition if its checkpointed offset is larger than the given offset", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4NzE3OA==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486487178", "bodyText": "The map stores the first offset in each epoch.", "author": "junrao", "createdAt": "2020-09-10T16:44:34Z", "path": "core/src/main/scala/kafka/server/checkpoints/LeaderEpochCheckpointFile.scala", "diffHunk": "@@ -52,8 +52,16 @@ object LeaderEpochCheckpointFile {\n }\n \n /**\n-  * This class persists a map of (LeaderEpoch => Offsets) to a file (for a certain replica)\n-  */\n+ * This class persists a map of (LeaderEpoch => Offsets) to a file (for a certain replica)\n+ *\n+ * The format in the LeaderEpoch checkpoint file is like this:\n+ * -----checkpoint file begin------\n+ * 0                <- LeaderEpochCheckpointFile.currentVersion\n+ * 2                <- following entries size\n+ * 0  1     <- the format is: leader_epoch(int32) end_offset(int64)", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc0OTYxOA==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486749618", "bodyText": "You are right. I referenced the KIP-101 to document it. After your reminding, I found the KIP is wrong. In the description, it said it's \"Start offset\", but in the table below, it becomes \"end offset\". I confirmed this is typo. I also updated the KIP as well. Thank you.", "author": "showuon", "createdAt": "2020-09-11T03:18:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4NzE3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4ODAzMg==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486488032", "bodyText": "Could we used named param for topicPartitionToBeRemoved?", "author": "junrao", "createdAt": "2020-09-10T16:45:53Z", "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "diffHunk": "@@ -203,16 +203,24 @@ class LogCleaner(initialConfig: CleanerConfig,\n   }\n \n   /**\n-   * Update checkpoint file, removing topics and partitions that no longer exist\n+   * Update checkpoint file to remove topics and partitions that no longer exist\n    */\n-  def updateCheckpoints(dataDir: File): Unit = {\n-    cleanerManager.updateCheckpoints(dataDir, update=None)\n+  def updateCheckpoints(dataDir: File, topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {\n+    cleanerManager.updateCheckpoints(dataDir, update=None, topicPartitionToBeRemoved)", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc1MDYxMg==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486750612", "bodyText": "Sure. I also removed the 2nd param update=None", "author": "showuon", "createdAt": "2020-09-11T03:22:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4ODAzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4OTQwMw==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486489403", "bodyText": "Perhaps tweaks the comment to \"Update checkpoint file, adding or removing partitions if necessary.\"?", "author": "junrao", "createdAt": "2020-09-10T16:48:15Z", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -354,12 +354,30 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)]): Unit = {\n+  /**\n+   * Update checkpoint file, or remove topics and partitions that no longer exist", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5MTY1NA==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486491654", "bodyText": "It'll remove all the checkpoint files located in the directory  => It will stop log cleaning in that directory.", "author": "junrao", "createdAt": "2020-09-10T16:52:05Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -184,7 +184,11 @@ class LogManager(logDirs: Seq[File],\n     numRecoveryThreadsPerDataDir = newSize\n   }\n \n-  // dir should be an absolute path\n+  /**\n+   * The log directory failure handler. It'll remove all the checkpoint files located in the directory", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5Mjc0OQ==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486492749", "bodyText": "typo notificiation", "author": "junrao", "createdAt": "2020-09-10T16:53:51Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1729,8 +1729,12 @@ class ReplicaManager(val config: KafkaConfig,\n     Partition.removeMetrics(tp)\n   }\n \n-  // logDir should be an absolute path\n-  // sendZkNotification is needed for unit test\n+  /**\n+   * The log directory failure handler for the replica\n+   *\n+   * @param dir                     the absolute path of the log directory\n+   * @param sendZkNotification      check if we need to send notificiation to zookeeper node (needed for unit test)", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5Mzk2Mg==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486493962", "bodyText": "Should we handle topicPartitionToBeRemoved or assert it is None?", "author": "junrao", "createdAt": "2020-09-10T16:55:46Z", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -55,7 +59,8 @@ class LogCleanerManagerTest extends Logging {\n       cleanerCheckpoints.toMap\n     }\n \n-    override def updateCheckpoints(dataDir: File, update: Option[(TopicPartition,Long)]): Unit = {\n+    override def updateCheckpoints(dataDir: File, update: Option[(TopicPartition,Long)],\n+                                   topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc1MTA0Ng==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486751046", "bodyText": "I assert it. Thanks for reminding.", "author": "showuon", "createdAt": "2020-09-11T03:24:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5Mzk2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NTgwNw==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486495807", "bodyText": "Could we used named param for update? Ditto below.", "author": "junrao", "createdAt": "2020-09-10T16:58:42Z", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -361,6 +366,93 @@ class LogCleanerManagerTest extends Logging {\n     assertEquals(\"should have 1 logs ready to be deleted\", 1, deletableLog3.size)\n   }\n \n+  @Test\n+  def testUpdateCheckpointsShouldAddOffsetToPartition(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // expect the checkpoint offset is not the expectedOffset before doing updateCheckpoints\n+    assertNotEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).getOrElse(0))\n+\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NjA1NA==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486496054", "bodyText": "expectedOffset => expected offset", "author": "junrao", "createdAt": "2020-09-10T16:59:07Z", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -361,6 +366,93 @@ class LogCleanerManagerTest extends Logging {\n     assertEquals(\"should have 1 logs ready to be deleted\", 1, deletableLog3.size)\n   }\n \n+  @Test\n+  def testUpdateCheckpointsShouldAddOffsetToPartition(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // expect the checkpoint offset is not the expectedOffset before doing updateCheckpoints\n+    assertNotEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).getOrElse(0))\n+\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    // expect the checkpoint offset is now updated to the expectedOffset after doing updateCheckpoints", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NjM1MA==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486496350", "bodyText": "allCleanerCheckpoints.get(topicPartition).get) can just be allCleanerCheckpoints(topicPartition). Ditto below.", "author": "junrao", "createdAt": "2020-09-10T16:59:34Z", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -361,6 +366,93 @@ class LogCleanerManagerTest extends Logging {\n     assertEquals(\"should have 1 logs ready to be deleted\", 1, deletableLog3.size)\n   }\n \n+  @Test\n+  def testUpdateCheckpointsShouldAddOffsetToPartition(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // expect the checkpoint offset is not the expectedOffset before doing updateCheckpoints\n+    assertNotEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).getOrElse(0))\n+\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    // expect the checkpoint offset is now updated to the expectedOffset after doing updateCheckpoints\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc1MTEzNA==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486751134", "bodyText": "Nice refactor! Thanks.", "author": "showuon", "createdAt": "2020-09-11T03:24:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NjM1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5ODA1Mg==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486498052", "bodyText": "It seems the < here should be > and the > two lines below should be <?", "author": "junrao", "createdAt": "2020-09-10T17:02:22Z", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -361,6 +366,93 @@ class LogCleanerManagerTest extends Logging {\n     assertEquals(\"should have 1 logs ready to be deleted\", 1, deletableLog3.size)\n   }\n \n+  @Test\n+  def testUpdateCheckpointsShouldAddOffsetToPartition(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // expect the checkpoint offset is not the expectedOffset before doing updateCheckpoints\n+    assertNotEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).getOrElse(0))\n+\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    // expect the checkpoint offset is now updated to the expectedOffset after doing updateCheckpoints\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)\n+  }\n+\n+  @Test\n+  def testUpdateCheckpointsShouldRemovePartitionData(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // write some data into the cleaner-offset-checkpoint file\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)\n+\n+    // updateCheckpoints should remove the topicPartition data in the logDir\n+    cleanerManager.updateCheckpoints(logDir, None, topicPartitionToBeRemoved = Some(topicPartition))\n+    assertTrue(cleanerManager.allCleanerCheckpoints.get(topicPartition).isEmpty)\n+  }\n+\n+  @Test\n+  def testHandleLogDirFailureShouldRemoveDirAndData(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // write some data into the cleaner-offset-checkpoint file in logDir and logDir2\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    cleanerManager.updateCheckpoints(logDir2, Option(topicPartition2, offset))\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition2).get)\n+\n+    cleanerManager.handleLogDirFailure(logDir.getAbsolutePath)\n+    // verify the partition data in logDir is gone, and data in logDir2 is still there\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition2).get)\n+    assertTrue(cleanerManager.allCleanerCheckpoints.get(topicPartition).isEmpty)\n+  }\n+\n+  @Test\n+  def testMaybeTruncateCheckpointShouldTruncateData(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+    val lowerOffset = 1L\n+    val higherOffset = 1000L\n+\n+    // write some data into the cleaner-offset-checkpoint file in logDir\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)\n+\n+    // we should not truncate the checkpoint data for checkpointed offset < the given offset (higherOffset)", "originalCommit": "5320318eec5cdb938cafd385f45525472370d359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc1MTczOA==", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486751738", "bodyText": "I checked again and I think I was right. The truncate Checkpoint file will happen only when the provided offset smaller than the one the the checkpoint file. So the comment is correct. I just added an equal sign (<=) to make it more accurate. Thanks.", "author": "showuon", "createdAt": "2020-09-11T03:27:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5ODA1Mg=="}], "type": "inlineReview"}, {"oid": "c7f436292209f1778f9455421f67fd81eea81f21", "url": "https://github.com/apache/kafka/commit/c7f436292209f1778f9455421f67fd81eea81f21", "message": "KAFKA-8362: address reviewer's comment to make the updateCheckpoints method more general and refactor codes", "committedDate": "2020-09-11T03:45:46Z", "type": "commit"}, {"oid": "c7f436292209f1778f9455421f67fd81eea81f21", "url": "https://github.com/apache/kafka/commit/c7f436292209f1778f9455421f67fd81eea81f21", "message": "KAFKA-8362: address reviewer's comment to make the updateCheckpoints method more general and refactor codes", "committedDate": "2020-09-11T03:45:46Z", "type": "forcePushed"}]}