{"pr_number": 8320, "pr_title": "KAFKA-8470: State change logs should not be in TRACE level", "pr_createdAt": "2020-03-20T14:56:43Z", "pr_url": "https://github.com/apache/kafka/pull/8320", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NTc0NA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395695744", "bodyText": "Why is this trace?", "author": "ijuma", "createdAt": "2020-03-20T15:01:32Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -266,9 +266,10 @@ class RequestSendThread(val controllerId: Int,\n \n         val response = clientResponse.responseBody\n \n-        stateChangeLogger.withControllerEpoch(controllerContext.epoch).trace(s\"Received response \" +\n-          s\"${response.toString(requestHeader.apiVersion)} for request $api with correlation id \" +\n-          s\"${requestHeader.correlationId} sent to broker $brokerNode\")\n+        if (stateChangeLogger.isTraceEnabled)\n+          stateChangeLogger.withControllerEpoch(controllerContext.epoch).trace(s\"Received response \" +\n+            s\"${response.toString(requestHeader.apiVersion)} for request $api with correlation id \" +\n+            s\"${requestHeader.correlationId} sent to broker $brokerNode\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc2NjA4OA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395766088", "bodyText": "The main goal is to avoid partition level logging for UpdateMetadata since it includes all partitions in the cluster. So we can probably log in info, but for UpdataMetadata, only log partition count.", "author": "junrao", "createdAt": "2020-03-20T16:51:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NTc0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTg3NDAzMQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395874031", "bodyText": "There was an earlier comment on the PR suggesting we were more selective with what we log at INFO. I had considered reverting it from INFO here (#6878 (comment)) and received no comments against it, so I thought it was OK.\nThis isn't a per-partition log though, so I agree we don't need to keep it at trace", "author": "stanislavkozlovski", "createdAt": "2020-03-20T20:27:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NTc0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NTg5NQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395695895", "bodyText": "Shouldn't this one be debug at least?", "author": "ijuma", "createdAt": "2020-03-20T15:01:47Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -447,13 +448,11 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n \n     leaderAndIsrRequestMap.filterKeys(controllerContext.liveOrShuttingDownBrokerIds.contains).foreach {\n       case (broker, leaderAndIsrPartitionStates) =>\n-        if (stateChangeLog.isTraceEnabled) {\n-          leaderAndIsrPartitionStates.foreach { case (topicPartition, state) =>\n-            val typeOfRequest =\n-              if (broker == state.leader) \"become-leader\"\n-              else \"become-follower\"\n-            stateChangeLog.trace(s\"Sending $typeOfRequest LeaderAndIsr request $state to broker $broker for partition $topicPartition\")\n-          }\n+        leaderAndIsrPartitionStates.foreach { case (topicPartition, state) =>\n+          val typeOfRequest =\n+            if (broker == state.leader) \"become-leader\"\n+            else \"become-follower\"\n+          stateChangeLog.info(s\"Sending $typeOfRequest LeaderAndIsr request $state to broker $broker for partition $topicPartition\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTg3NTI4NQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395875285", "bodyText": "I'm fine with reducing INFO level logs. My preference would be to group these and then log an info at the end sent $typeOfRequest for partitions X. WDYT we have both the debug and the aggregated log?", "author": "stanislavkozlovski", "createdAt": "2020-03-20T20:30:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NTg5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkyNDQ3MA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395924470", "bodyText": "We can probably keep this as trace.", "author": "junrao", "createdAt": "2020-03-20T22:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NTg5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NjA4Mw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395696083", "bodyText": "Why is this trace?", "author": "ijuma", "createdAt": "2020-03-20T15:02:05Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -473,10 +472,11 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n   }\n \n   private def sendUpdateMetadataRequests(controllerEpoch: Int, stateChangeLog: StateChangeLogger): Unit = {\n-    updateMetadataRequestPartitionInfoMap.foreach { case (tp, partitionState) =>\n-      stateChangeLog.trace(s\"Sending UpdateMetadata request $partitionState to brokers $updateMetadataRequestBrokerSet \" +\n-        s\"for partition $tp\")\n-    }\n+    if (stateChangeLog.isTraceEnabled)\n+      updateMetadataRequestPartitionInfoMap.foreach { case (tp, partitionState) =>\n+        stateChangeLog.trace(s\"Sending UpdateMetadata request $partitionState to brokers $updateMetadataRequestBrokerSet \" +\n+          s\"for partition $tp\")\n+      }", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc2NDkxNg==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395764916", "bodyText": "We probably can just do an info level logging but with just the partition count.", "author": "junrao", "createdAt": "2020-03-20T16:49:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NjA4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NjYwOQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395696609", "bodyText": "Instead of calling this once per partition, I'd do it once before the loop.", "author": "ijuma", "createdAt": "2020-03-20T15:02:51Z", "path": "core/src/main/scala/kafka/server/MetadataCache.scala", "diffHunk": "@@ -302,22 +302,33 @@ class MetadataCache(brokerId: Int) extends Logging {\n           copy ++= oldPartitionStates\n           partitionStates += (topic -> copy)\n         }\n-        updateMetadataRequest.partitionStates.asScala.foreach { info =>\n-          val controllerId = updateMetadataRequest.controllerId\n-          val controllerEpoch = updateMetadataRequest.controllerEpoch\n-          val tp = new TopicPartition(info.topicName, info.partitionIndex)\n-          if (info.leader == LeaderAndIsr.LeaderDuringDelete) {\n+\n+        val controllerId = updateMetadataRequest.controllerId\n+        val controllerEpoch = updateMetadataRequest.controllerEpoch\n+        val cachedPartitions = new mutable.ArrayBuffer[TopicPartition]\n+        updateMetadataRequest.partitionStates.asScala.foreach { state =>\n+          // per-partition logging here proves to be very expensive due to holding up the partition lock and the request thread\n+          val tp = new TopicPartition(state.topicName, state.partitionIndex)\n+          if (state.leader == LeaderAndIsr.LeaderDuringDelete) {\n             removePartitionInfo(partitionStates, tp.topic, tp.partition)\n-            stateChangeLogger.trace(s\"Deleted partition $tp from metadata cache in response to UpdateMetadata \" +\n-              s\"request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n+            if (stateChangeLogger.isTraceEnabled)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTg3NTgzMA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395875830", "bodyText": "Come to think of it, I agree. It's better to not let users shoot themselves in the foot if they enable TRACE state change logging", "author": "stanislavkozlovski", "createdAt": "2020-03-20T20:31:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NjYwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTg3NjQ3OA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395876478", "bodyText": "What did you mean before the loop? Get the number of partitions who are scheduled for deleting and print we're about to delete N of those?", "author": "stanislavkozlovski", "createdAt": "2020-03-20T20:32:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NjYwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTg5OTk4Ng==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395899986", "bodyText": "I meant the isTraceEnabled call since this require a bit of work in the logger to figure out if it's enabled.", "author": "ijuma", "createdAt": "2020-03-20T21:33:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NjYwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAxODI4Nw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396018287", "bodyText": "I see. Do you agree with my suggestion of not having the logs at all, even in trace?", "author": "stanislavkozlovski", "createdAt": "2020-03-21T18:50:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NjYwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NzExMA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395697110", "bodyText": "Maybe we don't include the actual partitions in these two logs? Just a count or something?", "author": "ijuma", "createdAt": "2020-03-20T15:03:33Z", "path": "core/src/main/scala/kafka/server/MetadataCache.scala", "diffHunk": "@@ -302,22 +302,33 @@ class MetadataCache(brokerId: Int) extends Logging {\n           copy ++= oldPartitionStates\n           partitionStates += (topic -> copy)\n         }\n-        updateMetadataRequest.partitionStates.asScala.foreach { info =>\n-          val controllerId = updateMetadataRequest.controllerId\n-          val controllerEpoch = updateMetadataRequest.controllerEpoch\n-          val tp = new TopicPartition(info.topicName, info.partitionIndex)\n-          if (info.leader == LeaderAndIsr.LeaderDuringDelete) {\n+\n+        val controllerId = updateMetadataRequest.controllerId\n+        val controllerEpoch = updateMetadataRequest.controllerEpoch\n+        val cachedPartitions = new mutable.ArrayBuffer[TopicPartition]\n+        updateMetadataRequest.partitionStates.asScala.foreach { state =>\n+          // per-partition logging here proves to be very expensive due to holding up the partition lock and the request thread\n+          val tp = new TopicPartition(state.topicName, state.partitionIndex)\n+          if (state.leader == LeaderAndIsr.LeaderDuringDelete) {\n             removePartitionInfo(partitionStates, tp.topic, tp.partition)\n-            stateChangeLogger.trace(s\"Deleted partition $tp from metadata cache in response to UpdateMetadata \" +\n-              s\"request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n+            if (stateChangeLogger.isTraceEnabled)\n+              stateChangeLogger.trace(s\"Deleted partition $tp from metadata cache in response to UpdateMetadata \" +\n+                s\"request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n             deletedPartitions += tp\n           } else {\n-            addOrUpdatePartitionInfo(partitionStates, tp.topic, tp.partition, info)\n-            stateChangeLogger.trace(s\"Cached leader info $info for partition $tp in response to \" +\n-              s\"UpdateMetadata request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n+            addOrUpdatePartitionInfo(partitionStates, tp.topic, tp.partition, state)\n+            if (stateChangeLogger.isTraceEnabled)\n+              stateChangeLogger.trace(s\"Cached leader info $state for partition $tp in response to \" +\n+                s\"UpdateMetadata request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n+            cachedPartitions += tp\n           }\n         }\n-        metadataSnapshot = MetadataSnapshot(partitionStates, controllerId, aliveBrokers, aliveNodes)\n+        stateChangeLogger.info(s\"Deleted ${deletedPartitions.size} partitions from metadata cache in response to UpdateMetadata \" +\n+          s\"request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId (partitions: $deletedPartitions)\")\n+        stateChangeLogger.info(s\"Cached leader info for ${cachedPartitions.size} partitions in response to UpdateMetadata \" +\n+          s\"request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId (partitions: $cachedPartitions)\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTczOTAxMw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395739013", "bodyText": "Since UpdateMetadata includes all partitions for the cluster, at the info level, we could probably just log a count instead of the actual partition list.", "author": "junrao", "createdAt": "2020-03-20T16:07:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NzExMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NzQ4MQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395697481", "bodyText": "Why not increment instead of decrement?", "author": "ijuma", "createdAt": "2020-03-20T15:04:02Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -386,8 +389,10 @@ class ReplicaManager(val config: KafkaConfig,\n               stateChangeLogger.error(s\"Ignoring stop replica (delete=${stopReplicaRequest.deletePartitions}) for \" +\n                 s\"partition $topicPartition due to storage exception\", e)\n               responseMap.put(topicPartition, Errors.KAFKA_STORAGE_ERROR)\n+              stopReplicaCount -= 1", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTg3NzYwNA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395877604", "bodyText": "Oh, because I already had the variable present due to the log where we print out how many partitions we're about to stop:\n        stateChangeLogger.info(s\"Handling stop replica (delete=${stopReplicaRequest.deletePartitions}) for ${partitions.size} partitions (${partitions.mkString(\", \")})\")", "author": "stanislavkozlovski", "createdAt": "2020-03-20T20:35:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NzQ4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTg3NzY4Nw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395877687", "bodyText": "Is it weird to decrement?", "author": "stanislavkozlovski", "createdAt": "2020-03-20T20:36:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NzQ4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkwMDA2Ng==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395900066", "bodyText": "Yeah, it's a bit less conventional.", "author": "ijuma", "createdAt": "2020-03-20T21:34:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NzQ4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTczNTQ1MA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395735450", "bodyText": "UpdateMetadata doesn't hold partition level lock. It just holds the ReplicaStateLock. We can probably say that it's expensive since it includes all partitions in the cluster.", "author": "junrao", "createdAt": "2020-03-20T16:01:27Z", "path": "core/src/main/scala/kafka/server/MetadataCache.scala", "diffHunk": "@@ -302,22 +302,33 @@ class MetadataCache(brokerId: Int) extends Logging {\n           copy ++= oldPartitionStates\n           partitionStates += (topic -> copy)\n         }\n-        updateMetadataRequest.partitionStates.asScala.foreach { info =>\n-          val controllerId = updateMetadataRequest.controllerId\n-          val controllerEpoch = updateMetadataRequest.controllerEpoch\n-          val tp = new TopicPartition(info.topicName, info.partitionIndex)\n-          if (info.leader == LeaderAndIsr.LeaderDuringDelete) {\n+\n+        val controllerId = updateMetadataRequest.controllerId\n+        val controllerEpoch = updateMetadataRequest.controllerEpoch\n+        val cachedPartitions = new mutable.ArrayBuffer[TopicPartition]\n+        updateMetadataRequest.partitionStates.asScala.foreach { state =>\n+          // per-partition logging here proves to be very expensive due to holding up the partition lock and the request thread", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkwMDkzOA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395900938", "bodyText": "@junrao It also holds the partitionMetadataLock, so if you have multiple UpdateMetadataRequest calls, they each wait behind each other.", "author": "ijuma", "createdAt": "2020-03-20T21:37:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTczNTQ1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0NDY3OA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395744678", "bodyText": "stopReplicaCount => SuccessfulStopReplicaCount ?", "author": "junrao", "createdAt": "2020-03-20T16:16:17Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -377,7 +377,10 @@ class ReplicaManager(val config: KafkaConfig,\n         // First stop fetchers for all partitions, then stop the corresponding replicas\n         replicaFetcherManager.removeFetcherForPartitions(partitions)\n         replicaAlterLogDirsManager.removeFetcherForPartitions(partitions)\n-        for (topicPartition <- partitions){\n+\n+        var stopReplicaCount = partitions.size", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0ODcwMg==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395748702", "bodyText": "In other places, we log a partition in a separate line. So, we should be consistent.", "author": "junrao", "createdAt": "2020-03-20T16:22:45Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -377,7 +377,10 @@ class ReplicaManager(val config: KafkaConfig,\n         // First stop fetchers for all partitions, then stop the corresponding replicas\n         replicaFetcherManager.removeFetcherForPartitions(partitions)\n         replicaAlterLogDirsManager.removeFetcherForPartitions(partitions)\n-        for (topicPartition <- partitions){\n+\n+        var stopReplicaCount = partitions.size\n+        stateChangeLogger.info(s\"Handling stop replica (delete=${stopReplicaRequest.deletePartitions}) for ${stopReplicaCount} partitions (${partitions.mkString(\", \")})\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTg3ODI5MQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395878291", "bodyText": "What do you think this should look like? The same Handling stop replica... message iterated N times for each partition and logged as one multi-line string?\nIs it better to just not print the partitions?", "author": "stanislavkozlovski", "createdAt": "2020-03-20T20:37:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0ODcwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1NTAzNw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395755037", "bodyText": "Here, we could just do an info level logging with the partition count, instead of actual partition list.", "author": "junrao", "createdAt": "2020-03-20T16:32:35Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1468,7 +1471,7 @@ class ReplicaManager(val config: KafkaConfig,\n \n       replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))\n       partitionsToMakeFollower.foreach { partition =>\n-        stateChangeLogger.trace(s\"Stopped fetchers as part of become-follower request from controller $controllerId \" +\n+        stateChangeLogger.info(s\"Stopped fetchers as part of become-follower request from controller $controllerId \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1NTU2MQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395755561", "bodyText": "This seems to be useless now since we moved the truncation logic inside replica fetcher. So, we can just remove this logging.", "author": "junrao", "createdAt": "2020-03-20T16:33:23Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1478,14 +1481,14 @@ class ReplicaManager(val config: KafkaConfig,\n       }\n \n       partitionsToMakeFollower.foreach { partition =>\n-        stateChangeLogger.trace(s\"Truncated logs and checkpointed recovery boundaries for partition \" +\n+        stateChangeLogger.info(s\"Truncated logs and checkpointed recovery boundaries for partition \" +", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTg3ODY4Mg==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395878682", "bodyText": "Nice, I didn't know that.", "author": "stanislavkozlovski", "createdAt": "2020-03-20T20:38:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc1NTU2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc3MDA4OA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395770088", "bodyText": "This logging seems mis-placed. We probably just want to move this to after replicaFetcherManager.removeFetcherForPartitions() with just the partition count. We already have info logging inside Partition.makeLeader() and we can change that logging to stateChangeLog.", "author": "junrao", "createdAt": "2020-03-20T16:58:01Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1353,7 +1356,7 @@ class ReplicaManager(val config: KafkaConfig,\n         try {\n           if (partition.makeLeader(controllerId, partitionState, correlationId, highWatermarkCheckpoints)) {\n             partitionsToMakeLeaders += partition\n-            stateChangeLogger.trace(s\"Stopped fetchers as part of become-leader request from \" +\n+            stateChangeLogger.info(s\"Stopped fetchers as part of become-leader request from \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkxNzM5Nw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395917397", "bodyText": "This comment is no longer needed.", "author": "junrao", "createdAt": "2020-03-20T22:30:41Z", "path": "core/src/main/scala/kafka/server/MetadataCache.scala", "diffHunk": "@@ -302,22 +302,27 @@ class MetadataCache(brokerId: Int) extends Logging {\n           copy ++= oldPartitionStates\n           partitionStates += (topic -> copy)\n         }\n-        updateMetadataRequest.partitionStates.asScala.foreach { info =>\n-          val controllerId = updateMetadataRequest.controllerId\n-          val controllerEpoch = updateMetadataRequest.controllerEpoch\n-          val tp = new TopicPartition(info.topicName, info.partitionIndex)\n-          if (info.leader == LeaderAndIsr.LeaderDuringDelete) {\n+\n+        val controllerId = updateMetadataRequest.controllerId\n+        val controllerEpoch = updateMetadataRequest.controllerEpoch\n+        val cachedPartitions = new mutable.ArrayBuffer[TopicPartition]\n+        updateMetadataRequest.partitionStates.asScala.foreach { state =>\n+          // per-partition logging here proves to be expensive because it includes all partitions in the cluster", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkxNzU3Mw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395917573", "bodyText": "We just need the count. So cachedPartitions could be an int.", "author": "junrao", "createdAt": "2020-03-20T22:31:18Z", "path": "core/src/main/scala/kafka/server/MetadataCache.scala", "diffHunk": "@@ -302,22 +302,27 @@ class MetadataCache(brokerId: Int) extends Logging {\n           copy ++= oldPartitionStates\n           partitionStates += (topic -> copy)\n         }\n-        updateMetadataRequest.partitionStates.asScala.foreach { info =>\n-          val controllerId = updateMetadataRequest.controllerId\n-          val controllerEpoch = updateMetadataRequest.controllerEpoch\n-          val tp = new TopicPartition(info.topicName, info.partitionIndex)\n-          if (info.leader == LeaderAndIsr.LeaderDuringDelete) {\n+\n+        val controllerId = updateMetadataRequest.controllerId\n+        val controllerEpoch = updateMetadataRequest.controllerEpoch\n+        val cachedPartitions = new mutable.ArrayBuffer[TopicPartition]", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAxODM4OQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396018389", "bodyText": "Ah sorry, left over from the past change", "author": "stanislavkozlovski", "createdAt": "2020-03-21T18:51:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkxNzU3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkxOTkwMQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395919901", "bodyText": "Let's keep this as TRACE logging here.", "author": "junrao", "createdAt": "2020-03-20T22:40:01Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1334,12 +1337,6 @@ class ReplicaManager(val config: KafkaConfig,\n                           correlationId: Int,\n                           responseMap: mutable.Map[TopicPartition, Errors],\n                           highWatermarkCheckpoints: OffsetCheckpoints): Set[Partition] = {\n-    partitionStates.keys.foreach { partition =>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkyMDE2Mw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395920163", "bodyText": "Change the logging to \"Stopped fetchers as part of become-leader request from ...\".", "author": "junrao", "createdAt": "2020-03-20T22:41:06Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1348,12 +1345,15 @@ class ReplicaManager(val config: KafkaConfig,\n     try {\n       // First stop fetchers for all the partitions\n       replicaFetcherManager.removeFetcherForPartitions(partitionStates.keySet.map(_.topicPartition))\n+      stateChangeLogger.info(s\"Handling LeaderAndIsr request correlationId $correlationId from \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkyMDI4OA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395920288", "bodyText": "We can remove this logging since partition.makeLeader() logs to stateChangeLog now.", "author": "junrao", "createdAt": "2020-03-20T22:41:36Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1348,12 +1345,15 @@ class ReplicaManager(val config: KafkaConfig,\n     try {\n       // First stop fetchers for all the partitions\n       replicaFetcherManager.removeFetcherForPartitions(partitionStates.keySet.map(_.topicPartition))\n+      stateChangeLogger.info(s\"Handling LeaderAndIsr request correlationId $correlationId from \" +\n+        s\"controller $controllerId epoch $controllerEpoch starting the become-leader transition for \" +\n+        s\"${partitionStates.size} partitions\")\n       // Update the partition information to be the leader\n       partitionStates.foreach { case (partition, partitionState) =>\n         try {\n           if (partition.makeLeader(controllerId, partitionState, correlationId, highWatermarkCheckpoints)) {\n             partitionsToMakeLeaders += partition\n-            stateChangeLogger.trace(s\"Stopped fetchers as part of become-leader request from \" +\n+            stateChangeLogger.info(s\"Stopped fetchers as part of become-leader request from \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkyMTEwMg==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395921102", "bodyText": "Let's keep this as TRACE logging here.", "author": "junrao", "createdAt": "2020-03-20T22:44:40Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1384,7 +1384,7 @@ class ReplicaManager(val config: KafkaConfig,\n     }\n \n     partitionStates.keys.foreach { partition =>\n-      stateChangeLogger.trace(s\"Completed LeaderAndIsr request correlationId $correlationId from controller $controllerId \" +\n+      stateChangeLogger.info(s\"Completed LeaderAndIsr request correlationId $correlationId from controller $controllerId \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkyMTE4NA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395921184", "bodyText": "Let's keep this as TRACE logging here. But add an info level state change log in Partition.makeFollower().", "author": "junrao", "createdAt": "2020-03-20T22:45:00Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1416,7 +1416,7 @@ class ReplicaManager(val config: KafkaConfig,\n                             responseMap: mutable.Map[TopicPartition, Errors],\n                             highWatermarkCheckpoints: OffsetCheckpoints) : Set[Partition] = {\n     partitionStates.foreach { case (partition, partitionState) =>\n-      stateChangeLogger.trace(s\"Handling LeaderAndIsr request correlationId $correlationId from controller $controllerId \" +\n+      stateChangeLogger.info(s\"Handling LeaderAndIsr request correlationId $correlationId from controller $controllerId \" +", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAxOTgwNw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396019807", "bodyText": "I'll also add a single INFO log to mention the # of partitions we're about to handle. This should make it easy to follow the sequence of operations when Partition#makeFollower starts logging", "author": "stanislavkozlovski", "createdAt": "2020-03-21T19:06:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkyMTE4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkyMTU5Mw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395921593", "bodyText": "Let's keep this as TRACE logging here.", "author": "junrao", "createdAt": "2020-03-20T22:46:38Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1467,25 +1467,16 @@ class ReplicaManager(val config: KafkaConfig,\n       }\n \n       replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))\n-      partitionsToMakeFollower.foreach { partition =>\n-        stateChangeLogger.trace(s\"Stopped fetchers as part of become-follower request from controller $controllerId \" +\n-          s\"epoch $controllerEpoch with correlation id $correlationId for partition ${partition.topicPartition} with leader \" +\n-          s\"${partitionStates(partition).leader}\")\n-      }\n+      stateChangeLogger.info(s\"Stopped fetchers as part of become-follower request from controller $controllerId \" +\n+        s\"epoch $controllerEpoch with correlation id $correlationId for ${partitionsToMakeFollower.size} partitions\")\n \n       partitionsToMakeFollower.foreach { partition =>\n         completeDelayedFetchOrProduceRequests(partition.topicPartition)\n       }\n \n-      partitionsToMakeFollower.foreach { partition =>\n-        stateChangeLogger.trace(s\"Truncated logs and checkpointed recovery boundaries for partition \" +\n-          s\"${partition.topicPartition} as part of become-follower request with correlation id $correlationId from \" +\n-          s\"controller $controllerId epoch $controllerEpoch with leader ${partitionStates(partition).leader}\")\n-      }\n-\n       if (isShuttingDown.get()) {\n         partitionsToMakeFollower.foreach { partition =>\n-          stateChangeLogger.trace(s\"Skipped the adding-fetcher step of the become-follower state \" +\n+          stateChangeLogger.info(s\"Skipped the adding-fetcher step of the become-follower state \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkyMjA2MA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395922060", "bodyText": "We already have info level logging in replicaFetcherManager.addFetcherForPartitions(). So we can just remove this logging.", "author": "junrao", "createdAt": "2020-03-20T22:48:40Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1501,7 +1492,7 @@ class ReplicaManager(val config: KafkaConfig,\n \n         replicaFetcherManager.addFetcherForPartitions(partitionsToMakeFollowerWithLeaderAndOffset)\n         partitionsToMakeFollowerWithLeaderAndOffset.foreach { case (partition, initialFetchState) =>\n-          stateChangeLogger.trace(s\"Started fetcher to new leader as part of become-follower \" +\n+          stateChangeLogger.info(s\"Started fetcher to new leader as part of become-follower \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkyMjA5OA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395922098", "bodyText": "Let's keep this as TRACE logging here.", "author": "junrao", "createdAt": "2020-03-20T22:48:48Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1515,7 +1506,7 @@ class ReplicaManager(val config: KafkaConfig,\n     }\n \n     partitionStates.keys.foreach { partition =>\n-      stateChangeLogger.trace(s\"Completed LeaderAndIsr request correlationId $correlationId from controller $controllerId \" +\n+      stateChangeLogger.info(s\"Completed LeaderAndIsr request correlationId $correlationId from controller $controllerId \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkyMzAxMw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395923013", "bodyText": "We can probably keep this as trace. While you are at this, could we also add a trace logging to sendStopReplicaRequest()?", "author": "junrao", "createdAt": "2020-03-20T22:52:45Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -473,10 +471,8 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n   }\n \n   private def sendUpdateMetadataRequests(controllerEpoch: Int, stateChangeLog: StateChangeLogger): Unit = {\n-    updateMetadataRequestPartitionInfoMap.foreach { case (tp, partitionState) =>\n-      stateChangeLog.trace(s\"Sending UpdateMetadata request $partitionState to brokers $updateMetadataRequestBrokerSet \" +\n-        s\"for partition $tp\")\n-    }\n+    stateChangeLog.trace(s\"Sending UpdateMetadata request to brokers $updateMetadataRequestBrokerSet \" +", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAyMDM2OQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396020369", "bodyText": "In sendStopReplicaRequest we have some debug logs:\n        debug(s\"The stop replica request (delete = true) sent to broker $brokerId is ${stopReplicaWithDelete.mkString(\",\")}\")\n\nis this the type of log that we'd prefer the stateChangeLogger to have?", "author": "stanislavkozlovski", "createdAt": "2020-03-21T19:13:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkyMzAxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkyNjA2NQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395926065", "bodyText": "We can probably keep this as trace.", "author": "junrao", "createdAt": "2020-03-20T23:04:45Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -266,7 +266,7 @@ class RequestSendThread(val controllerId: Int,\n \n         val response = clientResponse.responseBody\n \n-        stateChangeLogger.withControllerEpoch(controllerContext.epoch).trace(s\"Received response \" +\n+        stateChangeLogger.withControllerEpoch(controllerContext.epoch).info(s\"Received response \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkzNjM3NQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r395936375", "bodyText": "Since we already have logging in replicaFetcherManager.removeFetcherForPartitions(), we probably don't need any new logging in stopReplicas().", "author": "junrao", "createdAt": "2020-03-20T23:56:25Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -377,17 +377,22 @@ class ReplicaManager(val config: KafkaConfig,\n         // First stop fetchers for all partitions, then stop the corresponding replicas\n         replicaFetcherManager.removeFetcherForPartitions(partitions)\n         replicaAlterLogDirsManager.removeFetcherForPartitions(partitions)\n-        for (topicPartition <- partitions){\n+\n+        var successfulStopReplicaCount = 0\n+        stateChangeLogger.info(s\"Handling stop replica (delete=${stopReplicaRequest.deletePartitions}) for ${partitions.size} partitions (${partitions.mkString(\", \")})\")\n+        for (topicPartition <- partitions) {\n           try {\n             stopReplica(topicPartition, stopReplicaRequest.deletePartitions)\n             responseMap.put(topicPartition, Errors.NONE)\n+            successfulStopReplicaCount += 1\n           } catch {\n             case e: KafkaStorageException =>\n               stateChangeLogger.error(s\"Ignoring stop replica (delete=${stopReplicaRequest.deletePartitions}) for \" +\n                 s\"partition $topicPartition due to storage exception\", e)\n               responseMap.put(topicPartition, Errors.KAFKA_STORAGE_ERROR)\n           }\n         }\n+        stateChangeLogger.info(s\"Successfully handled stop replica (delete=${stopReplicaRequest.deletePartitions}) for ${successfulStopReplicaCount} partitions\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAyMTA0MQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396021041", "bodyText": "I was wondering if the debug() logs a couple of lines below are better suited for the stateChangeLogger", "author": "stanislavkozlovski", "createdAt": "2020-03-21T19:20:43Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -550,6 +547,7 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n         brokerEpoch, deletePartitions, partitions)\n     }\n \n+    stateChangeLog.trace(s\"Sending StopReplica request (version $stopReplicaRequestVersion) to brokers ${controllerContext.liveOrShuttingDownBrokerIds}\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgyOTQ1Mg==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396829452", "bodyText": "This is covered by the logging below. So, we can just remove this.", "author": "junrao", "createdAt": "2020-03-23T23:59:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAyMTA0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAyMTExMw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396021113", "bodyText": "I think this made it clearer and easy to separate from the makeFollower() log, even though both have a log line indicating that we're handling a become-leader or become-follower request", "author": "stanislavkozlovski", "createdAt": "2020-03-21T19:21:42Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -497,7 +498,7 @@ class Partition(val topicPartition: TopicPartition,\n \n       val leaderLog = localLogOrException\n       val leaderEpochStartOffset = leaderLog.logEndOffset\n-      info(s\"$topicPartition starts at leader epoch ${partitionState.leaderEpoch} from \" +\n+      stateChangeLogger.info(s\"Leader $topicPartition starts at leader epoch ${partitionState.leaderEpoch} from \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjU1Mzg1OQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396553859", "bodyText": "Yes, we could just change the following debug logging to  stateChangeLog.trace. Also, in the logging, we could just log StopReplicaRequestInfo.replica since StopReplicaRequestInfo.deletePartition is already extracted out.", "author": "junrao", "createdAt": "2020-03-23T15:48:54Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -550,6 +547,7 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n         brokerEpoch, deletePartitions, partitions)\n     }\n \n+    stateChangeLog.trace(s\"Sending StopReplica request (version $stopReplicaRequestVersion) to brokers ${controllerContext.liveOrShuttingDownBrokerIds}\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjU1NzY5Nw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396557697", "bodyText": "leaderEpochStartOffset => logEndOffset ?", "author": "junrao", "createdAt": "2020-03-23T15:53:45Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -568,6 +569,12 @@ class Partition(val topicPartition: TopicPartition,\n       )\n       createLogIfNotExists(localBrokerId, partitionState.isNew, isFutureReplica = false, highWatermarkCheckpoints)\n \n+      val followerLog = localLogOrException\n+      val leaderEpochStartOffset = followerLog.logEndOffset", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjU2MDEzNQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396560135", "bodyText": "This is no longer needed?", "author": "junrao", "createdAt": "2020-03-23T15:56:51Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -377,10 +377,14 @@ class ReplicaManager(val config: KafkaConfig,\n         // First stop fetchers for all partitions, then stop the corresponding replicas\n         replicaFetcherManager.removeFetcherForPartitions(partitions)\n         replicaAlterLogDirsManager.removeFetcherForPartitions(partitions)\n-        for (topicPartition <- partitions){\n+\n+        var successfulStopReplicaCount = 0", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjU2MDQ5NQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396560495", "bodyText": "We can just log a count at info level.", "author": "junrao", "createdAt": "2020-03-23T15:57:16Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -377,10 +377,14 @@ class ReplicaManager(val config: KafkaConfig,\n         // First stop fetchers for all partitions, then stop the corresponding replicas\n         replicaFetcherManager.removeFetcherForPartitions(partitions)\n         replicaAlterLogDirsManager.removeFetcherForPartitions(partitions)\n-        for (topicPartition <- partitions){\n+\n+        var successfulStopReplicaCount = 0\n+        stateChangeLogger.info(s\"Handling stop replica (delete=${stopReplicaRequest.deletePartitions}) for ${partitions.size} partitions (${partitions.mkString(\", \")})\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjU2MTE5Mg==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396561192", "bodyText": "Let's keep this as trace.", "author": "junrao", "createdAt": "2020-03-23T15:58:08Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1158,12 +1162,10 @@ class ReplicaManager(val config: KafkaConfig,\n   def becomeLeaderOrFollower(correlationId: Int,\n                              leaderAndIsrRequest: LeaderAndIsrRequest,\n                              onLeadershipChange: (Iterable[Partition], Iterable[Partition]) => Unit): LeaderAndIsrResponse = {\n-    if (stateChangeLogger.isTraceEnabled) {\n-      leaderAndIsrRequest.partitionStates.asScala.foreach { partitionState =>\n-        stateChangeLogger.trace(s\"Received LeaderAndIsr request $partitionState \" +\n-          s\"correlation id $correlationId from controller ${leaderAndIsrRequest.controllerId} \" +\n-          s\"epoch ${leaderAndIsrRequest.controllerEpoch}\")\n-      }\n+    leaderAndIsrRequest.partitionStates.asScala.foreach { partitionState =>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjU2MjQ2MA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396562460", "bodyText": "We can simplify this a bit to avoid mentioning become-leader twice.", "author": "junrao", "createdAt": "2020-03-23T15:59:44Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1334,29 +1336,29 @@ class ReplicaManager(val config: KafkaConfig,\n                           correlationId: Int,\n                           responseMap: mutable.Map[TopicPartition, Errors],\n                           highWatermarkCheckpoints: OffsetCheckpoints): Set[Partition] = {\n+    val traceEnabled = stateChangeLogger.isTraceEnabled\n     partitionStates.keys.foreach { partition =>\n-      stateChangeLogger.trace(s\"Handling LeaderAndIsr request correlationId $correlationId from \" +\n-        s\"controller $controllerId epoch $controllerEpoch starting the become-leader transition for \" +\n-        s\"partition ${partition.topicPartition}\")\n-    }\n-\n-    for (partition <- partitionStates.keys)\n+      if (traceEnabled)\n+        stateChangeLogger.trace(s\"Handling LeaderAndIsr request correlationId $correlationId from \" +\n+          s\"controller $controllerId epoch $controllerEpoch starting the become-leader transition for \" +\n+          s\"partition ${partition.topicPartition}\")\n       responseMap.put(partition.topicPartition, Errors.NONE)\n+    }\n \n     val partitionsToMakeLeaders = mutable.Set[Partition]()\n \n     try {\n       // First stop fetchers for all the partitions\n       replicaFetcherManager.removeFetcherForPartitions(partitionStates.keySet.map(_.topicPartition))\n+      stateChangeLogger.info(s\"Stopped fetchers as part of become-leader LeaderAndIsr request correlationId $correlationId from \" +\n+        s\"controller $controllerId epoch $controllerEpoch starting the become-leader transition for \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjY2NDc0MQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396664741", "bodyText": "Hmm, this seems incorrect. If trace is disabled and shutdown is true, we don't want to add the fetchers in the else clause. So the trace testing needs to be inside if (isShuttingDown.get().", "author": "junrao", "createdAt": "2020-03-23T18:23:56Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1467,23 +1472,14 @@ class ReplicaManager(val config: KafkaConfig,\n       }\n \n       replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))\n-      partitionsToMakeFollower.foreach { partition =>\n-        stateChangeLogger.trace(s\"Stopped fetchers as part of become-follower request from controller $controllerId \" +\n-          s\"epoch $controllerEpoch with correlation id $correlationId for partition ${partition.topicPartition} with leader \" +\n-          s\"${partitionStates(partition).leader}\")\n-      }\n+      stateChangeLogger.info(s\"Stopped fetchers as part of become-follower request from controller $controllerId \" +\n+        s\"epoch $controllerEpoch with correlation id $correlationId for ${partitionsToMakeFollower.size} partitions\")\n \n       partitionsToMakeFollower.foreach { partition =>\n         completeDelayedFetchOrProduceRequests(partition.topicPartition)\n       }\n \n-      partitionsToMakeFollower.foreach { partition =>\n-        stateChangeLogger.trace(s\"Truncated logs and checkpointed recovery boundaries for partition \" +\n-          s\"${partition.topicPartition} as part of become-follower request with correlation id $correlationId from \" +\n-          s\"controller $controllerId epoch $controllerEpoch with leader ${partitionStates(partition).leader}\")\n-      }\n-\n-      if (isShuttingDown.get()) {\n+      if (isShuttingDown.get() && traceLoggingEnabled) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjcxNzQ3MA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396717470", "bodyText": "Oh my, good catch", "author": "stanislavkozlovski", "createdAt": "2020-03-23T19:51:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjY2NDc0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgyNjA2MQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396826061", "bodyText": "Could we include StopReplicaRequest.deletePartitions() in the logging?", "author": "junrao", "createdAt": "2020-03-23T23:48:14Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -377,7 +377,9 @@ class ReplicaManager(val config: KafkaConfig,\n         // First stop fetchers for all partitions, then stop the corresponding replicas\n         replicaFetcherManager.removeFetcherForPartitions(partitions)\n         replicaAlterLogDirsManager.removeFetcherForPartitions(partitions)\n-        for (topicPartition <- partitions){\n+\n+        stateChangeLogger.info(s\"Handling stop replica for ${partitions.size} partitions\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgyNjgxNg==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396826816", "bodyText": "starting the become-leader transition for => as part of the become-leader transition for", "author": "junrao", "createdAt": "2020-03-23T23:50:43Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1334,29 +1336,29 @@ class ReplicaManager(val config: KafkaConfig,\n                           correlationId: Int,\n                           responseMap: mutable.Map[TopicPartition, Errors],\n                           highWatermarkCheckpoints: OffsetCheckpoints): Set[Partition] = {\n+    val traceEnabled = stateChangeLogger.isTraceEnabled\n     partitionStates.keys.foreach { partition =>\n-      stateChangeLogger.trace(s\"Handling LeaderAndIsr request correlationId $correlationId from \" +\n-        s\"controller $controllerId epoch $controllerEpoch starting the become-leader transition for \" +\n-        s\"partition ${partition.topicPartition}\")\n-    }\n-\n-    for (partition <- partitionStates.keys)\n+      if (traceEnabled)\n+        stateChangeLogger.trace(s\"Handling LeaderAndIsr request correlationId $correlationId from \" +\n+          s\"controller $controllerId epoch $controllerEpoch starting the become-leader transition for \" +\n+          s\"partition ${partition.topicPartition}\")\n       responseMap.put(partition.topicPartition, Errors.NONE)\n+    }\n \n     val partitionsToMakeLeaders = mutable.Set[Partition]()\n \n     try {\n       // First stop fetchers for all the partitions\n       replicaFetcherManager.removeFetcherForPartitions(partitionStates.keySet.map(_.topicPartition))\n+      stateChangeLogger.info(s\"Stopped fetchers as part of LeaderAndIsr request correlationId $correlationId from \" +\n+        s\"controller $controllerId epoch $controllerEpoch starting the become-leader transition for \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgyODUxNw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396828517", "bodyText": "Should we add if (stateChangeLog.isTraceEnabled)?", "author": "junrao", "createdAt": "2020-03-23T23:56:12Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -473,10 +472,8 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n   }\n \n   private def sendUpdateMetadataRequests(controllerEpoch: Int, stateChangeLog: StateChangeLogger): Unit = {\n-    updateMetadataRequestPartitionInfoMap.foreach { case (tp, partitionState) =>\n-      stateChangeLog.trace(s\"Sending UpdateMetadata request $partitionState to brokers $updateMetadataRequestBrokerSet \" +\n-        s\"for partition $tp\")\n-    }\n+    stateChangeLog.trace(s\"Sending UpdateMetadata request to brokers $updateMetadataRequestBrokerSet \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgyOTU5Mg==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396829592", "bodyText": "$brokerId is  => $brokerId contains", "author": "junrao", "createdAt": "2020-03-23T23:59:56Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -550,19 +547,23 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n         brokerEpoch, deletePartitions, partitions)\n     }\n \n+    stateChangeLog.trace(s\"Sending StopReplica request (version $stopReplicaRequestVersion) to brokers ${controllerContext.liveOrShuttingDownBrokerIds}\")\n+    val traceEnabled = stateChangeLog.isTraceEnabled\n     stopReplicaRequestMap.filterKeys(controllerContext.liveOrShuttingDownBrokerIds.contains).foreach { case (brokerId, replicaInfoList) =>\n       val (stopReplicaWithDelete, stopReplicaWithoutDelete) = replicaInfoList.partition(r => r.deletePartition)\n       val brokerEpoch = controllerContext.liveBrokerIdAndEpochs(brokerId)\n \n       if (stopReplicaWithDelete.nonEmpty) {\n-        debug(s\"The stop replica request (delete = true) sent to broker $brokerId is ${stopReplicaWithDelete.mkString(\",\")}\")\n+        if (traceEnabled)\n+          stateChangeLog.trace(s\"The stop replica request (delete = true) sent to broker $brokerId is ${stopReplicaWithDelete.map(_.replica).mkString(\",\")}\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgyOTY2NA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396829664", "bodyText": "$brokerId is => $brokerId contains", "author": "junrao", "createdAt": "2020-03-24T00:00:11Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -550,19 +547,23 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n         brokerEpoch, deletePartitions, partitions)\n     }\n \n+    stateChangeLog.trace(s\"Sending StopReplica request (version $stopReplicaRequestVersion) to brokers ${controllerContext.liveOrShuttingDownBrokerIds}\")\n+    val traceEnabled = stateChangeLog.isTraceEnabled\n     stopReplicaRequestMap.filterKeys(controllerContext.liveOrShuttingDownBrokerIds.contains).foreach { case (brokerId, replicaInfoList) =>\n       val (stopReplicaWithDelete, stopReplicaWithoutDelete) = replicaInfoList.partition(r => r.deletePartition)\n       val brokerEpoch = controllerContext.liveBrokerIdAndEpochs(brokerId)\n \n       if (stopReplicaWithDelete.nonEmpty) {\n-        debug(s\"The stop replica request (delete = true) sent to broker $brokerId is ${stopReplicaWithDelete.mkString(\",\")}\")\n+        if (traceEnabled)\n+          stateChangeLog.trace(s\"The stop replica request (delete = true) sent to broker $brokerId is ${stopReplicaWithDelete.map(_.replica).mkString(\",\")}\")\n         val stopReplicaRequest = createStopReplicaRequest(brokerEpoch, stopReplicaWithDelete, deletePartitions = true)\n         val callback = stopReplicaPartitionDeleteResponseCallback(brokerId) _\n         sendRequest(brokerId, stopReplicaRequest, callback)\n       }\n \n       if (stopReplicaWithoutDelete.nonEmpty) {\n-        debug(s\"The stop replica request (delete = false) sent to broker $brokerId is ${stopReplicaWithoutDelete.mkString(\",\")}\")\n+        if (traceEnabled)\n+          stateChangeLog.trace(s\"The stop replica request (delete = false) sent to broker $brokerId is ${stopReplicaWithoutDelete.map(_.replica).mkString(\",\")}\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgzMDA4Nw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396830087", "bodyText": "Do we need to pass stateChangeLog in? It seems that we can just reference it directly.", "author": "junrao", "createdAt": "2020-03-24T00:01:38Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -575,7 +576,7 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n       val stateChangeLog = stateChangeLogger.withControllerEpoch(controllerEpoch)\n       sendLeaderAndIsrRequest(controllerEpoch, stateChangeLog)\n       sendUpdateMetadataRequests(controllerEpoch, stateChangeLog)\n-      sendStopReplicaRequests(controllerEpoch)\n+      sendStopReplicaRequests(controllerEpoch, stateChangeLog)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzAzMjU4MQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r397032581", "bodyText": "I was following the convention as used by the other methods. I've removed the passed in log from all methods now and have them instantiate themselves", "author": "stanislavkozlovski", "createdAt": "2020-03-24T10:02:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgzMDA4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzMyMDg0NA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r397320844", "bodyText": "StateChangeLogger has some state. So passing it around is better than duplicating that state in every method. It also avoids allocations.", "author": "ijuma", "createdAt": "2020-03-24T17:08:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgzMDA4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgzNTc0NQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396835745", "bodyText": "To be consistent, could we add the same info logging at the beginning of makeLeaders()?", "author": "junrao", "createdAt": "2020-03-24T00:21:45Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1415,14 +1418,16 @@ class ReplicaManager(val config: KafkaConfig,\n                             correlationId: Int,\n                             responseMap: mutable.Map[TopicPartition, Errors],\n                             highWatermarkCheckpoints: OffsetCheckpoints) : Set[Partition] = {\n+    val traceLoggingEnabled = stateChangeLogger.isTraceEnabled\n+    stateChangeLogger.info(s\"Handling LeaderAndIsr request correlationId $correlationId from controller $controllerId \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njg4OTcyNA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396889724", "bodyText": "This is actually not very useful. So we can leave this in trace. What's more useful is to add an info logging for the leaderAndIsr after the removeReplicasFromIsr() call in line 221.", "author": "junrao", "createdAt": "2020-03-24T03:56:52Z", "path": "core/src/main/scala/kafka/controller/ReplicaStateMachine.scala", "diffHunk": "@@ -422,7 +422,7 @@ class ZkReplicaStateMachine(config: KafkaConfig,\n \n   private def logSuccessfulTransition(replicaId: Int, partition: TopicPartition, currState: ReplicaState, targetState: ReplicaState): Unit = {\n     stateChangeLogger.withControllerEpoch(controllerContext.epoch)\n-      .trace(s\"Changed state of replica $replicaId for partition $partition from $currState to $targetState\")\n+      .info(s\"Changed state of replica $replicaId for partition $partition from $currState to $targetState\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzA0MDE0OA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r397040148", "bodyText": "Went with\nstateLogger.info(s\"Removed replica $replicaId from the ISR of ${updatedLeaderIsrAndControllerEpochs.size} partitions as part of transition to $OfflineReplica\")", "author": "stanislavkozlovski", "createdAt": "2020-03-24T10:15:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njg4OTcyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njg5Mjk1Ng==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r396892956", "bodyText": "It would be useful to log isr, addingReplicas and removingReplicas too.", "author": "junrao", "createdAt": "2020-03-24T04:12:03Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -497,7 +498,7 @@ class Partition(val topicPartition: TopicPartition,\n \n       val leaderLog = localLogOrException\n       val leaderEpochStartOffset = leaderLog.logEndOffset\n-      info(s\"$topicPartition starts at leader epoch ${partitionState.leaderEpoch} from \" +\n+      stateChangeLogger.info(s\"Leader $topicPartition starts at leader epoch ${partitionState.leaderEpoch} from \" +", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzA0MzUxMA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r397043510", "bodyText": "I was considering adding a toString method to the AssignmentState class to make this easier but went without it for now", "author": "stanislavkozlovski", "createdAt": "2020-03-24T10:21:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njg5Mjk1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzI4MjAzNw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r397282037", "bodyText": "For this one, it's useful to log for each partition.", "author": "junrao", "createdAt": "2020-03-24T16:17:46Z", "path": "core/src/main/scala/kafka/controller/ReplicaStateMachine.scala", "diffHunk": "@@ -219,6 +224,7 @@ class ZkReplicaStateMachine(config: KafkaConfig,\n           controllerContext.partitionLeadershipInfo.contains(replica.topicPartition)\n         }\n         val updatedLeaderIsrAndControllerEpochs = removeReplicasFromIsr(replicaId, replicasWithLeadershipInfo.map(_.topicPartition))\n+        stateLogger.info(s\"Removed replica $replicaId from the ISR of ${updatedLeaderIsrAndControllerEpochs.size} partitions as part of transition to $OfflineReplica\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzI4OTM0Nw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r397289347", "bodyText": "Let's keep this trace.", "author": "junrao", "createdAt": "2020-03-24T16:26:51Z", "path": "core/src/main/scala/kafka/controller/PartitionStateMachine.scala", "diffHunk": "@@ -252,13 +252,13 @@ class ZkPartitionStateMachine(config: KafkaConfig,\n         }\n       case OfflinePartition =>\n         validPartitions.foreach { partition =>\n-          stateChangeLog.trace(s\"Changed partition $partition state from ${partitionState(partition)} to $targetState\")\n+          stateChangeLog.info(s\"Changed partition $partition state from ${partitionState(partition)} to $targetState\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODExMDIxNw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398110217", "bodyText": "What's the reasoning for keeping it in trace? Isn't it useful to know when a replica is set to offline?\nWhen do we enter this state - is it only during topic deletion and broker failure? If so, sounds good to keep in TRACE.\nI guess in the cases of broker failure it may be useful to log the total number of offline partitions in KafkaController#onReplicasBecomeOffline", "author": "stanislavkozlovski", "createdAt": "2020-03-25T19:21:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzI4OTM0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE4NTY3NA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398185674", "bodyText": "Yes, this is called on broker offline. But it only sets the in-memory state. The real work is done when changing ReplicaState to OfflineReplica case, which we have info level logging.", "author": "junrao", "createdAt": "2020-03-25T21:37:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzI4OTM0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzI4OTQxOQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r397289419", "bodyText": "Let's keep this trace.", "author": "junrao", "createdAt": "2020-03-24T16:26:57Z", "path": "core/src/main/scala/kafka/controller/PartitionStateMachine.scala", "diffHunk": "@@ -252,13 +252,13 @@ class ZkPartitionStateMachine(config: KafkaConfig,\n         }\n       case OfflinePartition =>\n         validPartitions.foreach { partition =>\n-          stateChangeLog.trace(s\"Changed partition $partition state from ${partitionState(partition)} to $targetState\")\n+          stateChangeLog.info(s\"Changed partition $partition state from ${partitionState(partition)} to $targetState\")\n           controllerContext.putPartitionState(partition, OfflinePartition)\n         }\n         Map.empty\n       case NonExistentPartition =>\n         validPartitions.foreach { partition =>\n-          stateChangeLog.trace(s\"Changed partition $partition state from ${partitionState(partition)} to $targetState\")\n+          stateChangeLog.info(s\"Changed partition $partition state from ${partitionState(partition)} to $targetState\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODEwODcyMg==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398108722", "bodyText": "We enter this state during topic deletion only, right? Makes sense to me", "author": "stanislavkozlovski", "createdAt": "2020-03-25T19:18:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzI4OTQxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzI5NjUxMg==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r397296512", "bodyText": "Could we just combine these 2 lines into a single line?", "author": "junrao", "createdAt": "2020-03-24T16:36:18Z", "path": "core/src/main/scala/kafka/server/MetadataCache.scala", "diffHunk": "@@ -302,22 +302,26 @@ class MetadataCache(brokerId: Int) extends Logging {\n           copy ++= oldPartitionStates\n           partitionStates += (topic -> copy)\n         }\n-        updateMetadataRequest.partitionStates.asScala.foreach { info =>\n-          val controllerId = updateMetadataRequest.controllerId\n-          val controllerEpoch = updateMetadataRequest.controllerEpoch\n-          val tp = new TopicPartition(info.topicName, info.partitionIndex)\n-          if (info.leader == LeaderAndIsr.LeaderDuringDelete) {\n+\n+        val controllerId = updateMetadataRequest.controllerId\n+        val controllerEpoch = updateMetadataRequest.controllerEpoch\n+        val newStates = updateMetadataRequest.partitionStates.asScala\n+        newStates.foreach { state =>\n+          val tp = new TopicPartition(state.topicName, state.partitionIndex)\n+          if (state.leader == LeaderAndIsr.LeaderDuringDelete) {\n             removePartitionInfo(partitionStates, tp.topic, tp.partition)\n-            stateChangeLogger.trace(s\"Deleted partition $tp from metadata cache in response to UpdateMetadata \" +\n-              s\"request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n             deletedPartitions += tp\n           } else {\n-            addOrUpdatePartitionInfo(partitionStates, tp.topic, tp.partition, info)\n-            stateChangeLogger.trace(s\"Cached leader info $info for partition $tp in response to \" +\n-              s\"UpdateMetadata request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n+            addOrUpdatePartitionInfo(partitionStates, tp.topic, tp.partition, state)\n           }\n         }\n-        metadataSnapshot = MetadataSnapshot(partitionStates, controllerId, aliveBrokers, aliveNodes)\n+        val cachedPartitionsCount = newStates.size - deletedPartitions.size\n+        stateChangeLogger.info(s\"Deleted ${deletedPartitions.size} partitions from metadata cache in response to UpdateMetadata \" +\n+          s\"request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n+        stateChangeLogger.info(s\"Cached leader info for $cachedPartitionsCount partitions in response to UpdateMetadata \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzMwNDc4OQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r397304789", "bodyText": "Didn't realize that we have to construct a new stateChangeLog object. So, the existing approach of constructing it once and pass it along seems better.", "author": "junrao", "createdAt": "2020-03-24T16:47:05Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -572,9 +576,8 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n \n   def sendRequestsToBrokers(controllerEpoch: Int): Unit = {\n     try {\n-      val stateChangeLog = stateChangeLogger.withControllerEpoch(controllerEpoch)\n-      sendLeaderAndIsrRequest(controllerEpoch, stateChangeLog)\n-      sendUpdateMetadataRequests(controllerEpoch, stateChangeLog)\n+      sendLeaderAndIsrRequest(controllerEpoch)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzMwNzQ4NA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r397307484", "bodyText": "Could we add an info level logging with the become-leader/become-follower counts for each broker?", "author": "junrao", "createdAt": "2020-03-24T16:50:32Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -447,14 +448,13 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n \n     leaderAndIsrRequestMap.filterKeys(controllerContext.liveOrShuttingDownBrokerIds.contains).foreach {\n       case (broker, leaderAndIsrPartitionStates) =>\n-        if (stateChangeLog.isTraceEnabled) {\n+        if (stateChangeLog.isTraceEnabled)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzMwODQ5OA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r397308498", "bodyText": "Since this just has the count, we can make this info level.", "author": "junrao", "createdAt": "2020-03-24T16:51:52Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -472,11 +472,11 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n     leaderAndIsrRequestMap.clear()\n   }\n \n-  private def sendUpdateMetadataRequests(controllerEpoch: Int, stateChangeLog: StateChangeLogger): Unit = {\n-    updateMetadataRequestPartitionInfoMap.foreach { case (tp, partitionState) =>\n-      stateChangeLog.trace(s\"Sending UpdateMetadata request $partitionState to brokers $updateMetadataRequestBrokerSet \" +\n-        s\"for partition $tp\")\n-    }\n+  private def sendUpdateMetadataRequests(controllerEpoch: Int): Unit = {\n+    val stateChangeLog = stateChangeLogger.withControllerEpoch(controllerEpoch)\n+    if (stateChangeLog.isTraceEnabled)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzMxMTA2OA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r397311068", "bodyText": "To be consistent with the above changes, let's add an info level logging per broker with just the partition count.", "author": "junrao", "createdAt": "2020-03-24T16:55:15Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -550,19 +551,22 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n         brokerEpoch, deletePartitions, partitions)\n     }\n \n+    val traceEnabled = stateChangeLog.isTraceEnabled\n     stopReplicaRequestMap.filterKeys(controllerContext.liveOrShuttingDownBrokerIds.contains).foreach { case (brokerId, replicaInfoList) =>\n       val (stopReplicaWithDelete, stopReplicaWithoutDelete) = replicaInfoList.partition(r => r.deletePartition)\n       val brokerEpoch = controllerContext.liveBrokerIdAndEpochs(brokerId)\n \n       if (stopReplicaWithDelete.nonEmpty) {\n-        debug(s\"The stop replica request (delete = true) sent to broker $brokerId is ${stopReplicaWithDelete.mkString(\",\")}\")\n+        if (traceEnabled)\n+          stateChangeLog.trace(s\"The stop replica request (delete = true) sent to broker $brokerId contains ${stopReplicaWithDelete.map(_.replica).mkString(\",\")}\")\n         val stopReplicaRequest = createStopReplicaRequest(brokerEpoch, stopReplicaWithDelete, deletePartitions = true)\n         val callback = stopReplicaPartitionDeleteResponseCallback(brokerId) _\n         sendRequest(brokerId, stopReplicaRequest, callback)\n       }\n \n       if (stopReplicaWithoutDelete.nonEmpty) {\n-        debug(s\"The stop replica request (delete = false) sent to broker $brokerId is ${stopReplicaWithoutDelete.mkString(\",\")}\")\n+        if (traceEnabled)\n+          stateChangeLog.trace(s\"The stop replica request (delete = false) sent to broker $brokerId contains ${stopReplicaWithoutDelete.map(_.replica).mkString(\",\")}\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE4MTE0Ng==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398181146", "bodyText": "To make it clearer, \"Partition xxx state changed to $leaderIsrAndControllerEpoch after removing replica ...\".", "author": "junrao", "createdAt": "2020-03-25T21:28:29Z", "path": "core/src/main/scala/kafka/controller/ReplicaStateMachine.scala", "diffHunk": "@@ -220,6 +225,7 @@ class ZkReplicaStateMachine(config: KafkaConfig,\n         }\n         val updatedLeaderIsrAndControllerEpochs = removeReplicasFromIsr(replicaId, replicasWithLeadershipInfo.map(_.topicPartition))\n         updatedLeaderIsrAndControllerEpochs.foreach { case (partition, leaderIsrAndControllerEpoch) =>\n+          stateLogger.info(s\"Removed replica $replicaId from the ISR of partition $partition $leaderIsrAndControllerEpoch as part of transition to $OfflineReplica\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI2NjkyMw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398266923", "bodyText": "It's probably useful to leave the per partition logging in trace, under the if (isTraceEnabled) flag.", "author": "junrao", "createdAt": "2020-03-26T01:31:02Z", "path": "core/src/main/scala/kafka/server/MetadataCache.scala", "diffHunk": "@@ -302,22 +302,25 @@ class MetadataCache(brokerId: Int) extends Logging {\n           copy ++= oldPartitionStates\n           partitionStates += (topic -> copy)\n         }\n-        updateMetadataRequest.partitionStates.asScala.foreach { info =>\n-          val controllerId = updateMetadataRequest.controllerId\n-          val controllerEpoch = updateMetadataRequest.controllerEpoch\n-          val tp = new TopicPartition(info.topicName, info.partitionIndex)\n-          if (info.leader == LeaderAndIsr.LeaderDuringDelete) {\n+\n+        val controllerId = updateMetadataRequest.controllerId\n+        val controllerEpoch = updateMetadataRequest.controllerEpoch\n+        val newStates = updateMetadataRequest.partitionStates.asScala\n+        newStates.foreach { state =>\n+          val tp = new TopicPartition(state.topicName, state.partitionIndex)\n+          if (state.leader == LeaderAndIsr.LeaderDuringDelete) {\n             removePartitionInfo(partitionStates, tp.topic, tp.partition)\n-            stateChangeLogger.trace(s\"Deleted partition $tp from metadata cache in response to UpdateMetadata \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI2Njk4NA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398266984", "bodyText": "It's probably useful to leave the per partition logging in trace.", "author": "junrao", "createdAt": "2020-03-26T01:31:20Z", "path": "core/src/main/scala/kafka/server/MetadataCache.scala", "diffHunk": "@@ -302,22 +302,25 @@ class MetadataCache(brokerId: Int) extends Logging {\n           copy ++= oldPartitionStates\n           partitionStates += (topic -> copy)\n         }\n-        updateMetadataRequest.partitionStates.asScala.foreach { info =>\n-          val controllerId = updateMetadataRequest.controllerId\n-          val controllerEpoch = updateMetadataRequest.controllerEpoch\n-          val tp = new TopicPartition(info.topicName, info.partitionIndex)\n-          if (info.leader == LeaderAndIsr.LeaderDuringDelete) {\n+\n+        val controllerId = updateMetadataRequest.controllerId\n+        val controllerEpoch = updateMetadataRequest.controllerEpoch\n+        val newStates = updateMetadataRequest.partitionStates.asScala\n+        newStates.foreach { state =>\n+          val tp = new TopicPartition(state.topicName, state.partitionIndex)\n+          if (state.leader == LeaderAndIsr.LeaderDuringDelete) {\n             removePartitionInfo(partitionStates, tp.topic, tp.partition)\n-            stateChangeLogger.trace(s\"Deleted partition $tp from metadata cache in response to UpdateMetadata \" +\n-              s\"request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n             deletedPartitions += tp\n           } else {\n-            addOrUpdatePartitionInfo(partitionStates, tp.topic, tp.partition, info)\n-            stateChangeLogger.trace(s\"Cached leader info $info for partition $tp in response to \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI3MjkxNQ==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398272915", "bodyText": "Could we change the following logging in line 1245 to info?\n              stateChangeLogger.debug(s\"Ignoring LeaderAndIsr request from \"", "author": "junrao", "createdAt": "2020-03-26T01:54:11Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1334,29 +1336,31 @@ class ReplicaManager(val config: KafkaConfig,\n                           correlationId: Int,\n                           responseMap: mutable.Map[TopicPartition, Errors],\n                           highWatermarkCheckpoints: OffsetCheckpoints): Set[Partition] = {\n+    val traceEnabled = stateChangeLogger.isTraceEnabled", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODU1OTc1Nw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398559757", "bodyText": "Sounds good, done in a346912", "author": "stanislavkozlovski", "createdAt": "2020-03-26T13:12:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI3MjkxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI3NDY5MA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398274690", "bodyText": "Deleted 0 partitions from metadata cache and cached leader info for 2 partitions in response => Deleted 0 partitions and add 2 partitions in metadata cache in response", "author": "junrao", "createdAt": "2020-03-26T02:00:27Z", "path": "core/src/main/scala/kafka/server/MetadataCache.scala", "diffHunk": "@@ -302,22 +302,25 @@ class MetadataCache(brokerId: Int) extends Logging {\n           copy ++= oldPartitionStates\n           partitionStates += (topic -> copy)\n         }\n-        updateMetadataRequest.partitionStates.asScala.foreach { info =>\n-          val controllerId = updateMetadataRequest.controllerId\n-          val controllerEpoch = updateMetadataRequest.controllerEpoch\n-          val tp = new TopicPartition(info.topicName, info.partitionIndex)\n-          if (info.leader == LeaderAndIsr.LeaderDuringDelete) {\n+\n+        val controllerId = updateMetadataRequest.controllerId\n+        val controllerEpoch = updateMetadataRequest.controllerEpoch\n+        val newStates = updateMetadataRequest.partitionStates.asScala\n+        newStates.foreach { state =>\n+          val tp = new TopicPartition(state.topicName, state.partitionIndex)\n+          if (state.leader == LeaderAndIsr.LeaderDuringDelete) {\n             removePartitionInfo(partitionStates, tp.topic, tp.partition)\n-            stateChangeLogger.trace(s\"Deleted partition $tp from metadata cache in response to UpdateMetadata \" +\n-              s\"request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n             deletedPartitions += tp\n           } else {\n-            addOrUpdatePartitionInfo(partitionStates, tp.topic, tp.partition, info)\n-            stateChangeLogger.trace(s\"Cached leader info $info for partition $tp in response to \" +\n-              s\"UpdateMetadata request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n+            addOrUpdatePartitionInfo(partitionStates, tp.topic, tp.partition, state)\n           }\n         }\n-        metadataSnapshot = MetadataSnapshot(partitionStates, controllerId, aliveBrokers, aliveNodes)\n+        val cachedPartitionsCount = newStates.size - deletedPartitions.size\n+        stateChangeLogger.info(s\"Deleted ${deletedPartitions.size} partitions from metadata cache and \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODI3NjIyMA==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398276220", "bodyText": "Let's just add an info level logging with partition count in the caller becomeLeaderOrFollower() instead of at the beginning of makeLeader() and makeFollowers(). This is because sometimes we ignore partitions in becomeLeaderOrFollower().", "author": "junrao", "createdAt": "2020-03-26T02:06:42Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1415,14 +1420,16 @@ class ReplicaManager(val config: KafkaConfig,\n                             correlationId: Int,\n                             responseMap: mutable.Map[TopicPartition, Errors],\n                             highWatermarkCheckpoints: OffsetCheckpoints) : Set[Partition] = {\n+    val traceLoggingEnabled = stateChangeLogger.isTraceEnabled\n+    stateChangeLogger.info(s\"Handling LeaderAndIsr request correlationId $correlationId from controller $controllerId \" +", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODU1OTQ5Ng==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398559496", "bodyText": "Should we add a comment that these trace logs are expensive?", "author": "stanislavkozlovski", "createdAt": "2020-03-26T13:11:48Z", "path": "core/src/main/scala/kafka/server/MetadataCache.scala", "diffHunk": "@@ -302,22 +302,32 @@ class MetadataCache(brokerId: Int) extends Logging {\n           copy ++= oldPartitionStates\n           partitionStates += (topic -> copy)\n         }\n-        updateMetadataRequest.partitionStates.asScala.foreach { info =>\n-          val controllerId = updateMetadataRequest.controllerId\n-          val controllerEpoch = updateMetadataRequest.controllerEpoch\n-          val tp = new TopicPartition(info.topicName, info.partitionIndex)\n-          if (info.leader == LeaderAndIsr.LeaderDuringDelete) {\n+\n+        val traceEnabled = stateChangeLogger.isTraceEnabled\n+        val controllerId = updateMetadataRequest.controllerId\n+        val controllerEpoch = updateMetadataRequest.controllerEpoch\n+        val newStates = updateMetadataRequest.partitionStates.asScala\n+        newStates.foreach { state =>\n+          val tp = new TopicPartition(state.topicName, state.partitionIndex)\n+          if (state.leader == LeaderAndIsr.LeaderDuringDelete) {\n             removePartitionInfo(partitionStates, tp.topic, tp.partition)\n-            stateChangeLogger.trace(s\"Deleted partition $tp from metadata cache in response to UpdateMetadata \" +\n-              s\"request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n+            if (traceEnabled)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODczNDE2Ng==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398734166", "bodyText": "Yes, we can add a comment.", "author": "junrao", "createdAt": "2020-03-26T16:57:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODU1OTQ5Ng=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "f3aebc0c26e3ff8c9e31c91bfc1fa22506e4e6fe", "url": "https://github.com/apache/kafka/commit/f3aebc0c26e3ff8c9e31c91bfc1fa22506e4e6fe", "message": "KAFKA-8470-state-change-logger\n\nSelect logs should stay in trace\n\nAddress comments\n\nAdd state change logger to Partition#makeLeader()\nRemove per-partition logging in channelManager#sendUpdateMetadataRequest\nRemove per-partition logging in MetadataCache#updateMetadata\nReduce per-partition logging in ReplicaManager\n\nChange logging in Partition#makeLeader and Partition#makeFollower\n\nControllerChannelManager - Add TRACE logging in sendStopReplicaRequests and change some LAIR sending  logging to TRACE\n\nClear unnecessary logging in ReplicaManager and switch to TRACE\n\nWe reduce the number of for loops in become-leader/become-follower LAIR handling, log in trace (because Partition#makeFollower/makeLeader() already log per partition) and add some introductory single info-level logs\n\nRemove unnecessary log in stopReplicas()\n\nAddress PR comments\n\nAddress PR comments\n\nLog number of become-leader/become-follower partition states\n\nDo not re-instantiate StateChangeLogger when sending lair/updateMetadata/stopReplica requests to brokers\n\nAdd per-broker info logs on stop replica requests\n\nLog in trace for NonExistentPartition state switch\n\nLog OfflineReplica state switch per partition in INFO\n\nLog sendUpdateMetadataRequest in INFO\n\nLog OfflinePartition in TRACE for PartitionStateMachine\n\nReturn per-partition TRACE logging in metadata update\n\nRevise ReplicaManager log statement placement, make LeaderAndIsr ignore log INFO and reword remove replica log", "committedDate": "2020-03-26T13:19:19Z", "type": "commit"}, {"oid": "f3aebc0c26e3ff8c9e31c91bfc1fa22506e4e6fe", "url": "https://github.com/apache/kafka/commit/f3aebc0c26e3ff8c9e31c91bfc1fa22506e4e6fe", "message": "KAFKA-8470-state-change-logger\n\nSelect logs should stay in trace\n\nAddress comments\n\nAdd state change logger to Partition#makeLeader()\nRemove per-partition logging in channelManager#sendUpdateMetadataRequest\nRemove per-partition logging in MetadataCache#updateMetadata\nReduce per-partition logging in ReplicaManager\n\nChange logging in Partition#makeLeader and Partition#makeFollower\n\nControllerChannelManager - Add TRACE logging in sendStopReplicaRequests and change some LAIR sending  logging to TRACE\n\nClear unnecessary logging in ReplicaManager and switch to TRACE\n\nWe reduce the number of for loops in become-leader/become-follower LAIR handling, log in trace (because Partition#makeFollower/makeLeader() already log per partition) and add some introductory single info-level logs\n\nRemove unnecessary log in stopReplicas()\n\nAddress PR comments\n\nAddress PR comments\n\nLog number of become-leader/become-follower partition states\n\nDo not re-instantiate StateChangeLogger when sending lair/updateMetadata/stopReplica requests to brokers\n\nAdd per-broker info logs on stop replica requests\n\nLog in trace for NonExistentPartition state switch\n\nLog OfflineReplica state switch per partition in INFO\n\nLog sendUpdateMetadataRequest in INFO\n\nLog OfflinePartition in TRACE for PartitionStateMachine\n\nReturn per-partition TRACE logging in metadata update\n\nRevise ReplicaManager log statement placement, make LeaderAndIsr ignore log INFO and reword remove replica log", "committedDate": "2020-03-26T13:19:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODczMjA3Nw==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398732077", "bodyText": "I was thinking of logging the partition count at the beginning of becomeLeaderOrFollower(). Otherwise, it's kind of weird to see the request logging after the logging for \"Ignoring\".", "author": "junrao", "createdAt": "2020-03-26T16:54:23Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1255,15 +1257,19 @@ class ReplicaManager(val config: KafkaConfig,\n         val partitionsToBeFollower = partitionStates -- partitionsTobeLeader.keys\n \n         val highWatermarkCheckpoints = new LazyOffsetCheckpoints(this.highWatermarkCheckpoints)\n-        val partitionsBecomeLeader = if (partitionsTobeLeader.nonEmpty)\n+        val partitionsBecomeLeader = if (partitionsTobeLeader.nonEmpty) {\n+          stateChangeLogger.info(s\"Handling LeaderAndIsr request correlationId $correlationId from controller $controllerId \" +", "originalCommit": "f3aebc0c26e3ff8c9e31c91bfc1fa22506e4e6fe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODczMjY1Mg==", "url": "https://github.com/apache/kafka/pull/8320#discussion_r398732652", "bodyText": "Could we put \"add $cachedPartitionsCount partitions\" before \"Deleted ${deletedPartitions.size} partitions\" ?", "author": "junrao", "createdAt": "2020-03-26T16:55:11Z", "path": "core/src/main/scala/kafka/server/MetadataCache.scala", "diffHunk": "@@ -321,22 +321,32 @@ class MetadataCache(brokerId: Int) extends Logging {\n           copy ++= oldPartitionStates\n           partitionStates += (topic -> copy)\n         }\n-        updateMetadataRequest.partitionStates.asScala.foreach { info =>\n-          val controllerId = updateMetadataRequest.controllerId\n-          val controllerEpoch = updateMetadataRequest.controllerEpoch\n-          val tp = new TopicPartition(info.topicName, info.partitionIndex)\n-          if (info.leader == LeaderAndIsr.LeaderDuringDelete) {\n+\n+        val traceEnabled = stateChangeLogger.isTraceEnabled\n+        val controllerId = updateMetadataRequest.controllerId\n+        val controllerEpoch = updateMetadataRequest.controllerEpoch\n+        val newStates = updateMetadataRequest.partitionStates.asScala\n+        newStates.foreach { state =>\n+          val tp = new TopicPartition(state.topicName, state.partitionIndex)\n+          if (state.leader == LeaderAndIsr.LeaderDuringDelete) {\n             removePartitionInfo(partitionStates, tp.topic, tp.partition)\n-            stateChangeLogger.trace(s\"Deleted partition $tp from metadata cache in response to UpdateMetadata \" +\n-              s\"request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n+            if (traceEnabled)\n+              stateChangeLogger.trace(s\"Deleted partition $tp from metadata cache in response to UpdateMetadata \" +\n+                s\"request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n             deletedPartitions += tp\n           } else {\n-            addOrUpdatePartitionInfo(partitionStates, tp.topic, tp.partition, info)\n-            stateChangeLogger.trace(s\"Cached leader info $info for partition $tp in response to \" +\n-              s\"UpdateMetadata request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n+            addOrUpdatePartitionInfo(partitionStates, tp.topic, tp.partition, state)\n+            if (traceEnabled)\n+              stateChangeLogger.trace(s\"Cached leader info $state for partition $tp in response to \" +\n+                s\"UpdateMetadata request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId\")\n           }\n         }\n-        metadataSnapshot = MetadataSnapshot(partitionStates, controllerId, aliveBrokers, aliveNodes)\n+        val cachedPartitionsCount = newStates.size - deletedPartitions.size\n+        stateChangeLogger.info(s\"Deleted ${deletedPartitions.size} partitions and \" +", "originalCommit": "f3aebc0c26e3ff8c9e31c91bfc1fa22506e4e6fe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1fc70ecb4e4eac9b68ad9762d55275cd26e5b738", "url": "https://github.com/apache/kafka/commit/1fc70ecb4e4eac9b68ad9762d55275cd26e5b738", "message": "Move logging in becomeLeaderOrFollower", "committedDate": "2020-03-26T18:04:58Z", "type": "commit"}, {"oid": "8fc7ca7de7ffd00ed32fcd3ced1cc4774ad503c0", "url": "https://github.com/apache/kafka/commit/8fc7ca7de7ffd00ed32fcd3ced1cc4774ad503c0", "message": "Tweak logging in MetadataCache", "committedDate": "2020-03-26T18:05:11Z", "type": "commit"}]}