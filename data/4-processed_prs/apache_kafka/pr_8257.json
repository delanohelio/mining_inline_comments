{"pr_number": 8257, "pr_title": "KAFKA-9539; Add leader epoch in StopReplicaRequest (KIP-570)", "pr_createdAt": "2020-03-09T14:20:17Z", "pr_url": "https://github.com/apache/kafka/pull/8257", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyMTc1MA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r389721750", "bodyText": "\ud83d\udc40 During the implementation, I have realized that the schema which has been proposed in the KIP does not work (my mistake). I propose to use the following schema instead. This schema will allow us to add more fields in the future. If the reviewer agree with this, I will update the KIP and and the VOTE thread.", "author": "dajac", "createdAt": "2020-03-09T14:23:26Z", "path": "clients/src/main/resources/common/message/StopReplicaRequest.json", "diffHunk": "@@ -19,7 +19,11 @@\n   \"name\": \"StopReplicaRequest\",", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM0MTA3NQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393341075", "bodyText": "Yes, it makes sense to me. This is what I had assumed was in the KIP, but I guess I didn't look carefully enough.", "author": "hachikuji", "createdAt": "2020-03-16T22:20:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyMTc1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzU5NDgxMw==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393594813", "bodyText": "Perfect, thanks. I will update the KIP accordingly.", "author": "dajac", "createdAt": "2020-03-17T10:56:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyMTc1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyMzY3NA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r389723674", "bodyText": "\ud83d\udc40 I have added a new IBP version to gate the new API version. I was wondering if the was really necessary as the new version of the API is backward compatible.", "author": "dajac", "createdAt": "2020-03-09T14:26:16Z", "path": "core/src/main/scala/kafka/api/ApiVersion.scala", "diffHunk": "@@ -96,7 +96,9 @@ object ApiVersion {\n     // Flexible version support in inter-broker APIs\n     KAFKA_2_4_IV1,\n     // No new APIs, equivalent to 2.4-IV1\n-    KAFKA_2_5_IV0\n+    KAFKA_2_5_IV0,\n+    // Introduced StopReplicaRequest V3 containing the leader epoch for each partition (KIP-570)\n+    KAFKA_2_6_IV0", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM0MjYzNQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393342635", "bodyText": "Yes, we bump the IBP every time one of the inter- broker protocol versions gets bumped. Hopefully soon brokers will use ApiVersions instead.", "author": "hachikuji", "createdAt": "2020-03-16T22:25:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyMzY3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyNDcxOQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r389724719", "bodyText": "\ud83d\udc40 This is the logic which handle the sentinel. -2 is used when a topic is deleted and also when the partition doesn't have leadership information yet.", "author": "dajac", "createdAt": "2020-03-09T14:27:43Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -397,8 +398,25 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n                                       topicPartition: TopicPartition,\n                                       deletePartition: Boolean): Unit = {\n     brokerIds.filter(_ >= 0).foreach { brokerId =>\n-      val stopReplicaInfos = stopReplicaRequestMap.getOrElseUpdate(brokerId, ListBuffer.empty[StopReplicaRequestInfo])\n-      stopReplicaInfos.append(StopReplicaRequestInfo(PartitionAndReplica(topicPartition, brokerId), deletePartition))\n+      val result = if (deletePartition)\n+        stopReplicaRequestMapWithDelete.getOrElseUpdate(brokerId, mutable.Map.empty)\n+      else\n+        stopReplicaRequestMapWithoutDelete.getOrElseUpdate(brokerId, mutable.Map.empty)\n+\n+      // A sentinel (-2) is used as an epoch if the topic is queued for deletion or\n+      // does not have a leader yet. This sentinel overrides any existing epoch.\n+      val leaderEpoch = if (controllerContext.isTopicQueuedUpForDeletion(topicPartition.topic)) {\n+        LeaderAndIsr.EpochDuringDelete\n+      } else {\n+        controllerContext.partitionLeadershipInfo.get(topicPartition)\n+          .map(_.leaderAndIsr.leaderEpoch)\n+          .getOrElse(LeaderAndIsr.EpochDuringDelete)\n+      }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3MzM3Ng==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393373376", "bodyText": "Wonder if it makes sense to add this method to StopReplicaRequest. Often the code expects to work with TopicPartition.", "author": "hachikuji", "createdAt": "2020-03-16T23:53:10Z", "path": "clients/src/test/java/org/apache/kafka/common/requests/StopReplicaRequestTest.java", "diffHunk": "@@ -44,22 +52,144 @@ public void testUnsupportedVersion() {\n \n     @Test\n     public void testGetErrorResponse() {\n+        List<StopReplicaTopicState> topicStates = topicStates();\n+\n+        Set<StopReplicaPartitionError> expectedPartitions = new HashSet<>();\n+        for (StopReplicaTopicState topicState : topicStates) {\n+            for (StopReplicaPartitionState partitionState: topicState.partitionStates()) {\n+                expectedPartitions.add(new StopReplicaPartitionError()\n+                    .setTopicName(topicState.topicName())\n+                    .setPartitionIndex(partitionState.partitionIndex())\n+                    .setErrorCode(Errors.CLUSTER_AUTHORIZATION_FAILED.code()));\n+            }\n+        }\n+\n         for (short version = STOP_REPLICA.oldestVersion(); version < STOP_REPLICA.latestVersion(); version++) {\n             StopReplicaRequest.Builder builder = new StopReplicaRequest.Builder(version,\n-                    0, 0, 0L, false, Collections.emptyList());\n+                    0, 0, 0L, false, topicStates);\n             StopReplicaRequest request = builder.build();\n             StopReplicaResponse response = request.getErrorResponse(0,\n                     new ClusterAuthorizationException(\"Not authorized\"));\n             assertEquals(Errors.CLUSTER_AUTHORIZATION_FAILED, response.error());\n+            assertEquals(expectedPartitions, new HashSet<>(response.partitionErrors()));\n+        }\n+    }\n+\n+    @Test\n+    public void testBuilderNormalization() {\n+        List<StopReplicaTopicState> topicStates = topicStates();\n+\n+        Set<TopicPartition> expectedPartitions = new HashSet<>();\n+        for (StopReplicaTopicState topicState : topicStates) {\n+            for (StopReplicaPartitionState partitionState: topicState.partitionStates()) {\n+                expectedPartitions.add(\n+                    new TopicPartition(topicState.topicName(), partitionState.partitionIndex()));\n+            }\n+        }\n+\n+        Map<TopicPartition, StopReplicaPartitionState> expectedPartitionStates = new HashMap<>();\n+        for (StopReplicaTopicState topicState : topicStates) {\n+            for (StopReplicaPartitionState partitionState: topicState.partitionStates()) {\n+                expectedPartitionStates.put(\n+                    new TopicPartition(topicState.topicName(), partitionState.partitionIndex()),\n+                    partitionState);\n+            }\n+        }\n+\n+        for (short version = STOP_REPLICA.oldestVersion(); version < STOP_REPLICA.latestVersion(); version++) {\n+            StopReplicaRequest request = new StopReplicaRequest.Builder(version, 0, 1, 0,\n+                true, topicStates).build(version);\n+            StopReplicaRequestData data = request.data();\n+\n+            if (version < 1) {\n+                Set<TopicPartition> partitions = new HashSet<>();\n+                for (StopReplicaPartitionV0 partition : data.ungroupedPartitions()) {\n+                    partitions.add(new TopicPartition(partition.topicName(), partition.partitionIndex()));\n+                }\n+                assertEquals(expectedPartitions, partitions);\n+            } else if (version < 3) {\n+                Set<TopicPartition> partitions = new HashSet<>();\n+                for (StopReplicaTopicV1 topic : data.topics()) {\n+                    for (Integer partition : topic.partitionIndexes()) {\n+                        partitions.add(new TopicPartition(topic.name(), partition));\n+                    }\n+                }\n+                assertEquals(expectedPartitions, partitions);\n+            } else {\n+                Map<TopicPartition, StopReplicaPartitionState> partitionStates = new HashMap<>();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzU3Mjc5NQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393572795", "bodyText": "I have considered this as well but I haven't done it because it is only used in tests so far. I think that the downside is that using this in core is not optimal thus I am a bit reluctant to provide it. I mean, allocating and populating the Map is not necessary, especially when the controller and the brokers use the latest version of the API.", "author": "dajac", "createdAt": "2020-03-17T10:17:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3MzM3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3MzgxNA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393373814", "bodyText": "Might be worth adding one case where the epoch is -2.", "author": "hachikuji", "createdAt": "2020-03-16T23:54:50Z", "path": "clients/src/test/java/org/apache/kafka/common/requests/StopReplicaRequestTest.java", "diffHunk": "@@ -44,22 +52,144 @@ public void testUnsupportedVersion() {\n \n     @Test\n     public void testGetErrorResponse() {\n+        List<StopReplicaTopicState> topicStates = topicStates();\n+\n+        Set<StopReplicaPartitionError> expectedPartitions = new HashSet<>();\n+        for (StopReplicaTopicState topicState : topicStates) {\n+            for (StopReplicaPartitionState partitionState: topicState.partitionStates()) {\n+                expectedPartitions.add(new StopReplicaPartitionError()\n+                    .setTopicName(topicState.topicName())\n+                    .setPartitionIndex(partitionState.partitionIndex())\n+                    .setErrorCode(Errors.CLUSTER_AUTHORIZATION_FAILED.code()));\n+            }\n+        }\n+\n         for (short version = STOP_REPLICA.oldestVersion(); version < STOP_REPLICA.latestVersion(); version++) {\n             StopReplicaRequest.Builder builder = new StopReplicaRequest.Builder(version,\n-                    0, 0, 0L, false, Collections.emptyList());\n+                    0, 0, 0L, false, topicStates);\n             StopReplicaRequest request = builder.build();\n             StopReplicaResponse response = request.getErrorResponse(0,\n                     new ClusterAuthorizationException(\"Not authorized\"));\n             assertEquals(Errors.CLUSTER_AUTHORIZATION_FAILED, response.error());\n+            assertEquals(expectedPartitions, new HashSet<>(response.partitionErrors()));\n+        }\n+    }\n+\n+    @Test\n+    public void testBuilderNormalization() {\n+        List<StopReplicaTopicState> topicStates = topicStates();\n+\n+        Set<TopicPartition> expectedPartitions = new HashSet<>();\n+        for (StopReplicaTopicState topicState : topicStates) {\n+            for (StopReplicaPartitionState partitionState: topicState.partitionStates()) {\n+                expectedPartitions.add(\n+                    new TopicPartition(topicState.topicName(), partitionState.partitionIndex()));\n+            }\n+        }\n+\n+        Map<TopicPartition, StopReplicaPartitionState> expectedPartitionStates = new HashMap<>();\n+        for (StopReplicaTopicState topicState : topicStates) {\n+            for (StopReplicaPartitionState partitionState: topicState.partitionStates()) {\n+                expectedPartitionStates.put(\n+                    new TopicPartition(topicState.topicName(), partitionState.partitionIndex()),\n+                    partitionState);\n+            }\n+        }\n+\n+        for (short version = STOP_REPLICA.oldestVersion(); version < STOP_REPLICA.latestVersion(); version++) {\n+            StopReplicaRequest request = new StopReplicaRequest.Builder(version, 0, 1, 0,\n+                true, topicStates).build(version);\n+            StopReplicaRequestData data = request.data();\n+\n+            if (version < 1) {\n+                Set<TopicPartition> partitions = new HashSet<>();\n+                for (StopReplicaPartitionV0 partition : data.ungroupedPartitions()) {\n+                    partitions.add(new TopicPartition(partition.topicName(), partition.partitionIndex()));\n+                }\n+                assertEquals(expectedPartitions, partitions);\n+            } else if (version < 3) {\n+                Set<TopicPartition> partitions = new HashSet<>();\n+                for (StopReplicaTopicV1 topic : data.topics()) {\n+                    for (Integer partition : topic.partitionIndexes()) {\n+                        partitions.add(new TopicPartition(topic.name(), partition));\n+                    }\n+                }\n+                assertEquals(expectedPartitions, partitions);\n+            } else {\n+                Map<TopicPartition, StopReplicaPartitionState> partitionStates = new HashMap<>();\n+                for (StopReplicaTopicState topicState : topicStates) {\n+                    for (StopReplicaPartitionState partitionState: topicState.partitionStates()) {\n+                        partitionStates.put(\n+                            new TopicPartition(topicState.topicName(), partitionState.partitionIndex()),\n+                            partitionState);\n+                    }\n+                }\n+                assertEquals(expectedPartitionStates, partitionStates);\n+            }\n         }\n     }\n \n     @Test\n-    public void testStopReplicaRequestNormalization() {\n-        Set<TopicPartition> tps = TestUtils.generateRandomTopicPartitions(10, 10);\n-        StopReplicaRequest.Builder builder = new StopReplicaRequest.Builder((short) 5, 0, 0, 0, false, tps);\n-        assertTrue(MessageTestUtil.messageSize(builder.build((short) 1).data(), (short) 1) <\n-            MessageTestUtil.messageSize(builder.build((short) 0).data(), (short) 0));\n+    public void testTopicStatesNormalization() {\n+        List<StopReplicaTopicState> topicStates = topicStates();\n+\n+        for (short version = STOP_REPLICA.oldestVersion(); version < STOP_REPLICA.latestVersion(); version++) {\n+            // Create a request for version to get its struct\n+            StopReplicaRequest baseRequest = new StopReplicaRequest.Builder(version, 0, 1, 0,\n+                true, topicStates).build(version);\n+\n+            // Construct the request from the struct\n+            StopReplicaRequest request = new StopReplicaRequest(baseRequest.toStruct(), version);\n+\n+            Map<TopicPartition, StopReplicaPartitionState> partitionStates = new HashMap<>();\n+            for (StopReplicaTopicState topicState : request.topicStates()) {\n+                for (StopReplicaPartitionState partitionState: topicState.partitionStates()) {\n+                    partitionStates.put(\n+                        new TopicPartition(topicState.topicName(), partitionState.partitionIndex()),\n+                        partitionState);\n+                }\n+            }\n+\n+            assertEquals(4, partitionStates.size());\n+\n+            for (StopReplicaTopicState expectedTopicState : topicStates) {\n+                for (StopReplicaPartitionState expectedPartitionState: expectedTopicState.partitionStates()) {\n+                    TopicPartition tp = new TopicPartition(expectedTopicState.topicName(),\n+                        expectedPartitionState.partitionIndex());\n+                    StopReplicaPartitionState partitionState = partitionStates.get(tp);\n+\n+                    assertEquals(expectedPartitionState.partitionIndex(), partitionState.partitionIndex());\n+\n+                    if (version >= 3) {\n+                        assertEquals(expectedPartitionState.leaderEpoch(), partitionState.leaderEpoch());\n+                    } else {\n+                        assertEquals(-1, partitionState.leaderEpoch());\n+                    }\n+                }\n+            }\n+        }\n     }\n \n+    private List<StopReplicaTopicState> topicStates() {\n+        List<StopReplicaTopicState> topicStates = new ArrayList<>();\n+        StopReplicaTopicState topic0 = new StopReplicaTopicState()\n+            .setTopicName(\"topic0\");\n+        topic0.partitionStates().add(new StopReplicaPartitionState()\n+            .setPartitionIndex(0)\n+            .setLeaderEpoch(0));\n+        topic0.partitionStates().add(new StopReplicaPartitionState()\n+            .setPartitionIndex(1)\n+            .setLeaderEpoch(1));\n+        topicStates.add(topic0);\n+        StopReplicaTopicState topic1 = new StopReplicaTopicState()\n+            .setTopicName(\"topic1\");\n+        topic1.partitionStates().add(new StopReplicaPartitionState()\n+            .setPartitionIndex(2)\n+            .setLeaderEpoch(2));\n+        topic1.partitionStates().add(new StopReplicaPartitionState()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzU3MzkzNA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393573934", "bodyText": "Sure, done.", "author": "dajac", "createdAt": "2020-03-17T10:19:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3MzgxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3NjkxMQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393376911", "bodyText": "It's a bit annoying that the DeletePartitions flag is set at the top level. It would be nice to move that into the partition state. A KIP for another day I guess...", "author": "hachikuji", "createdAt": "2020-03-17T00:06:35Z", "path": "clients/src/main/resources/common/message/StopReplicaRequest.json", "diffHunk": "@@ -37,12 +41,24 @@\n       { \"name\": \"PartitionIndex\", \"type\": \"int32\", \"versions\": \"0\",\n         \"about\": \"The partition index.\" }\n     ]},\n-    { \"name\": \"Topics\", \"type\": \"[]StopReplicaTopic\", \"versions\": \"1+\",\n+    { \"name\": \"Topics\", \"type\": \"[]StopReplicaTopicV1\", \"versions\": \"1-2\",\n       \"about\": \"The topics to stop.\", \"fields\": [\n-      { \"name\": \"Name\", \"type\": \"string\", \"versions\": \"1+\", \"entityType\": \"topicName\",\n+      { \"name\": \"Name\", \"type\": \"string\", \"versions\": \"1-2\", \"entityType\": \"topicName\",\n         \"about\": \"The topic name.\" },\n-      { \"name\": \"PartitionIndexes\", \"type\": \"[]int32\", \"versions\": \"1+\",\n+      { \"name\": \"PartitionIndexes\", \"type\": \"[]int32\", \"versions\": \"1-2\",\n         \"about\": \"The partition indexes.\" }\n+    ]},\n+    { \"name\": \"TopicStates\", \"type\": \"[]StopReplicaTopicState\", \"versions\": \"3+\",\n+      \"about\": \"Each topic.\", \"fields\": [\n+      { \"name\": \"TopicName\", \"type\": \"string\", \"versions\": \"3+\", \"entityType\": \"topicName\",\n+        \"about\": \"The topic name.\" },\n+      { \"name\": \"PartitionStates\", \"type\": \"[]StopReplicaPartitionState\", \"versions\": \"3+\",", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzU3NTI4MA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393575280", "bodyText": "Yes. I totally agree with you. It would avoid us to always send out two StopReplicaRequests. I almost did it directly but I thought that it would stretch the KIP too far perhaps.  What do you think? Shall we do it directly? Anyway, we should do it before 2.6 is released in order to avoid bumping the API twice. I am fine with both ways.", "author": "dajac", "createdAt": "2020-03-17T10:21:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3NjkxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU1MzQ4OQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r397553489", "bodyText": "I'd suggest sending a message to the vote thread and ask what others think about the change. It seems like a no-brainer improvement to me, so maybe we don't need the overhead of doing another KIP.", "author": "hachikuji", "createdAt": "2020-03-25T01:09:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3NjkxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODAxNzE2OA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r398017168", "bodyText": "Done. I have also implemented it directly ;)", "author": "dajac", "createdAt": "2020-03-25T17:00:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM3NjkxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4NTU1Mg==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393385552", "bodyText": "The iff was probably intentional, if a bit pedantic. I'm fine with the change though.", "author": "hachikuji", "createdAt": "2020-03-17T00:42:05Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -707,7 +707,7 @@ class LogManager(logDirs: Seq[File],\n    * @param topicPartition The partition whose log needs to be returned or created\n    * @param config The configuration of the log that should be applied for log creation\n    * @param isNew Whether the replica should have existed on the broker or not\n-   * @param isFuture True iff the future log of the specified partition should be returned or created\n+   * @param isFuture True if the future log of the specified partition should be returned or created", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzU3NjY4OA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393576688", "bodyText": "Oh.. I did not know that iff was actually valid. I thought that it was a typo :)", "author": "dajac", "createdAt": "2020-03-17T10:24:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4NTU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg3MDI0Mg==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393870242", "bodyText": "Yeah, it's a mathematical short-hand for \"if and only if.\" But the \"only if\" part is implicit here.", "author": "hachikuji", "createdAt": "2020-03-17T18:02:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4NTU1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4NzQ0OQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393387449", "bodyText": "Hmm.. This error seems a little inaccurate. Could we use FENCED_LEADER_EPOCH?", "author": "hachikuji", "createdAt": "2020-03-17T00:50:11Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -364,30 +368,98 @@ class ReplicaManager(val config: KafkaConfig,\n     delayedFetchPurgatory.checkAndComplete(topicPartitionOperationKey)\n   }\n \n-  def stopReplicas(stopReplicaRequest: StopReplicaRequest): (mutable.Map[TopicPartition, Errors], Errors) = {\n+  def stopReplicas(correlationId: Int, stopReplicaRequest: StopReplicaRequest): (mutable.Map[TopicPartition, Errors], Errors) = {\n     replicaStateChangeLock synchronized {\n+      val controllerId = stopReplicaRequest.controllerId\n+      val deletePartition = stopReplicaRequest.deletePartitions\n       val responseMap = new collection.mutable.HashMap[TopicPartition, Errors]\n       if (stopReplicaRequest.controllerEpoch() < controllerEpoch) {\n-        stateChangeLogger.warn(\"Received stop replica request from an old controller epoch \" +\n-          s\"${stopReplicaRequest.controllerEpoch}. Latest known controller epoch is $controllerEpoch\")\n+        stateChangeLogger.warn(s\"Ignoring StopReplica request (delete=$deletePartition) from \" +\n+          s\"controller $controllerId with correlation id $correlationId \" +\n+          s\"since its controller epoch ${stopReplicaRequest.controllerEpoch} is old. \" +\n+          s\"Latest known controller epoch is $controllerEpoch\")\n         (responseMap, Errors.STALE_CONTROLLER_EPOCH)\n       } else {\n-        val partitions = stopReplicaRequest.partitions.asScala.toSet\n+        val partitions = mutable.Set.empty[TopicPartition]\n         controllerEpoch = stopReplicaRequest.controllerEpoch\n+\n+        stopReplicaRequest.topicStates.asScala.foreach { topicState =>\n+          topicState.partitionStates.asScala.foreach { partitionState =>\n+            val topicPartition = new TopicPartition(topicState.topicName, partitionState.partitionIndex)\n+\n+            getPartition(topicPartition) match {\n+              case HostedPartition.Offline =>\n+                stateChangeLogger.warn(s\"Ignoring StopReplica request (delete=$deletePartition) from \" +\n+                  s\"controller $controllerId with correlation id $correlationId \" +\n+                  s\"epoch $controllerEpoch for partition $topicPartition as the local replica for the \" +\n+                  \"partition is in an offline log directory\")\n+                responseMap.put(topicPartition, Errors.KAFKA_STORAGE_ERROR)\n+\n+              case HostedPartition.Online(partition) =>\n+                val currentLeaderEpoch = partition.getLeaderEpoch\n+                val requestLeaderEpoch = partitionState.leaderEpoch\n+                // When a topic is deleted, the leader epoch is not incremented. To circumvent this,\n+                // a sentinel value (EpochDuringDelete) overwriting any previous epoch is used.\n+                // When an older version of the StopReplica request which does not contain the leader\n+                // epoch, a sentinel value (NoEpoch) is used and bypass the epoch validation.\n+                if (requestLeaderEpoch == LeaderAndIsr.EpochDuringDelete ||\n+                    requestLeaderEpoch == LeaderAndIsr.NoEpoch ||\n+                    requestLeaderEpoch > currentLeaderEpoch) {\n+                  partitions += topicPartition\n+                } else if (requestLeaderEpoch < currentLeaderEpoch) {\n+                  stateChangeLogger.warn(s\"Ignoring StopReplica request (delete=$deletePartition) from \" +\n+                    s\"controller $controllerId with correlation id $correlationId \" +\n+                    s\"epoch $controllerEpoch for partition $topicPartition since its associated \" +\n+                    s\"leader epoch $requestLeaderEpoch is smaller than the current \" +\n+                    s\"leader epoch $currentLeaderEpoch\")\n+                  responseMap.put(topicPartition, Errors.STALE_CONTROLLER_EPOCH)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzU4OTMzNQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393589335", "bodyText": "Good point. I completely forgot to raise it...\nI use STALE_CONTROLLER_EPOCH here to stay inline with the LeaderAndIsr API which uses is as well when the leader epoch is stale. See here: https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L1227\nIt was introduced in an old refactoring: a9ff3f2#diff-4f99f5a41c14e2a8523c03ce4ae23987L630. It seems that back in the days, we had StaleLeaderEpochCode but it got replaced by STALE_CONTROLLER_EPOCH .\nI was actually wondering if we should stay inline with the current behavior of the LeaderAndIsr or just use FENCED_LEADER_EPOCH.  If there a not other reasons besides the historical one to use STALE_CONTROLLER_EPOCH, FENCED_LEADER_EPOCH seems indeed more appropriate.", "author": "dajac", "createdAt": "2020-03-17T10:46:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4NzQ0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA0MjUzMQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r395042531", "bodyText": "I have changed it to use FENCED_LEADER_EPOCH.", "author": "dajac", "createdAt": "2020-03-19T13:55:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4NzQ0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4ODQ2OA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393388468", "bodyText": "This changes the order of the operations. Previously we would have stopped fetchers before attempting to delete the log directory. Are we sure this is safe?", "author": "hachikuji", "createdAt": "2020-03-17T00:54:18Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -364,30 +368,98 @@ class ReplicaManager(val config: KafkaConfig,\n     delayedFetchPurgatory.checkAndComplete(topicPartitionOperationKey)\n   }\n \n-  def stopReplicas(stopReplicaRequest: StopReplicaRequest): (mutable.Map[TopicPartition, Errors], Errors) = {\n+  def stopReplicas(correlationId: Int, stopReplicaRequest: StopReplicaRequest): (mutable.Map[TopicPartition, Errors], Errors) = {\n     replicaStateChangeLock synchronized {\n+      val controllerId = stopReplicaRequest.controllerId\n+      val deletePartition = stopReplicaRequest.deletePartitions\n       val responseMap = new collection.mutable.HashMap[TopicPartition, Errors]\n       if (stopReplicaRequest.controllerEpoch() < controllerEpoch) {\n-        stateChangeLogger.warn(\"Received stop replica request from an old controller epoch \" +\n-          s\"${stopReplicaRequest.controllerEpoch}. Latest known controller epoch is $controllerEpoch\")\n+        stateChangeLogger.warn(s\"Ignoring StopReplica request (delete=$deletePartition) from \" +\n+          s\"controller $controllerId with correlation id $correlationId \" +\n+          s\"since its controller epoch ${stopReplicaRequest.controllerEpoch} is old. \" +\n+          s\"Latest known controller epoch is $controllerEpoch\")\n         (responseMap, Errors.STALE_CONTROLLER_EPOCH)\n       } else {\n-        val partitions = stopReplicaRequest.partitions.asScala.toSet\n+        val partitions = mutable.Set.empty[TopicPartition]\n         controllerEpoch = stopReplicaRequest.controllerEpoch\n+\n+        stopReplicaRequest.topicStates.asScala.foreach { topicState =>\n+          topicState.partitionStates.asScala.foreach { partitionState =>\n+            val topicPartition = new TopicPartition(topicState.topicName, partitionState.partitionIndex)\n+\n+            getPartition(topicPartition) match {\n+              case HostedPartition.Offline =>\n+                stateChangeLogger.warn(s\"Ignoring StopReplica request (delete=$deletePartition) from \" +\n+                  s\"controller $controllerId with correlation id $correlationId \" +\n+                  s\"epoch $controllerEpoch for partition $topicPartition as the local replica for the \" +\n+                  \"partition is in an offline log directory\")\n+                responseMap.put(topicPartition, Errors.KAFKA_STORAGE_ERROR)\n+\n+              case HostedPartition.Online(partition) =>\n+                val currentLeaderEpoch = partition.getLeaderEpoch\n+                val requestLeaderEpoch = partitionState.leaderEpoch\n+                // When a topic is deleted, the leader epoch is not incremented. To circumvent this,\n+                // a sentinel value (EpochDuringDelete) overwriting any previous epoch is used.\n+                // When an older version of the StopReplica request which does not contain the leader\n+                // epoch, a sentinel value (NoEpoch) is used and bypass the epoch validation.\n+                if (requestLeaderEpoch == LeaderAndIsr.EpochDuringDelete ||\n+                    requestLeaderEpoch == LeaderAndIsr.NoEpoch ||\n+                    requestLeaderEpoch > currentLeaderEpoch) {\n+                  partitions += topicPartition\n+                } else if (requestLeaderEpoch < currentLeaderEpoch) {\n+                  stateChangeLogger.warn(s\"Ignoring StopReplica request (delete=$deletePartition) from \" +\n+                    s\"controller $controllerId with correlation id $correlationId \" +\n+                    s\"epoch $controllerEpoch for partition $topicPartition since its associated \" +\n+                    s\"leader epoch $requestLeaderEpoch is smaller than the current \" +\n+                    s\"leader epoch $currentLeaderEpoch\")\n+                  responseMap.put(topicPartition, Errors.STALE_CONTROLLER_EPOCH)\n+                } else {\n+                  stateChangeLogger.debug(s\"Ignoring StopReplica request (delete=$deletePartition) from \" +\n+                    s\"controller $controllerId with correlation id $correlationId \" +\n+                    s\"epoch $controllerEpoch for partition $topicPartition since its associated \" +\n+                    s\"leader epoch $requestLeaderEpoch matches the current leader epoch\")\n+                  responseMap.put(topicPartition, Errors.STALE_CONTROLLER_EPOCH)\n+                }\n+\n+              case HostedPartition.None =>\n+                try {\n+                  // Delete log and corresponding folders in case replica manager doesn't hold them anymore.\n+                  // This could happen when topic is being deleted while broker is down and recovers.\n+                  maybeCleanReplica(topicPartition, deletePartition)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzU5NDI2MA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r393594260", "bodyText": "I need to take another look at it.", "author": "dajac", "createdAt": "2020-03-17T10:55:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4ODQ2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTUyNDQwNg==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r395524406", "bodyText": "I had a second look at this one and I think that the change is safe. Let me explain.\nWe can end up in this situation where the replica is not know any more by broker in two ways:\n\n\nThe broker receives a StopReplica request without having received a LeaderAndIsr request prior to it. In this case, the partition is not created and the fetchers haven't been started so we don't need to stop them for the concerned partitions. We have one test which simulate such scenario: https://github.com/apache/kafka/blob/trunk/core/src/test/scala/unit/kafka/admin/DeleteTopicTest.scala#L432. It fails if I comment the cleaning logic.\n\n\nThe handling of the StopReplica request fails with a storage exception when it deletes the log directory. The delete happens after the partition is effectively removed from the allPartitions map in the ReplicaManager. Note that the fetchers for the concerned partitions are already stopped at this point as they are stopped before removing the partition from the Map. If the request is retried somehow, the partition won't be there so the cleaning would take place.\n\n\nAll together, fetchers are always started after the partition is added in the allPartitions Map and always stopped before removing the partition from the Map. If it is not in the Map, fetchers can't be started. Thus, this seems safe to me based on my current knowledge.\nThe only benefit of putting it there is that the logging is better in my opinion. When the replica does not exist, we don't get the handling StopReplica... and completed StopReplica... but only Ignoring StopReplica... cause replica does not exist.\nI would be fine reverting back to the prior behavior as it is only a cosmetic change at the end. It may be safer to do so.", "author": "dajac", "createdAt": "2020-03-20T09:37:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4ODQ2OA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTI0MjgwNA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r399242804", "bodyText": "@hachikuji It looks like that we could improve this now that we do have the leader epoch. I am not familiar at all with transactions. Can I just pass the epoch when provided here?", "author": "dajac", "createdAt": "2020-03-27T12:54:19Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -236,22 +236,22 @@ class KafkaApis(val requestChannel: RequestChannel,\n     if (isBrokerEpochStale(stopReplicaRequest.brokerEpoch)) {\n       // When the broker restarts very quickly, it is possible for this broker to receive request intended\n       // for its previous generation so the broker should skip the stale request.\n-      info(\"Received stop replica request with broker epoch \" +\n+      info(\"Received StopReplica request with broker epoch \" +\n         s\"${stopReplicaRequest.brokerEpoch} smaller than the current broker epoch ${controller.brokerEpoch}\")\n       sendResponseExemptThrottle(request, new StopReplicaResponse(new StopReplicaResponseData().setErrorCode(Errors.STALE_BROKER_EPOCH.code)))\n     } else {\n-      val (result, error) = replicaManager.stopReplicas(stopReplicaRequest)\n+      val (result, error) = replicaManager.stopReplicas(request.context.correlationId, stopReplicaRequest)\n       // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n       // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n-      result.foreach { case (topicPartition, error) =>\n-        if (error == Errors.NONE && stopReplicaRequest.deletePartitions) {\n-          if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME) {\n-            groupCoordinator.onResignation(topicPartition.partition)\n-          } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME) {\n-            // The StopReplica API does not pass through the leader epoch\n-            txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = None)\n-          }\n-        }\n+      result.foreach {\n+        case (topicPartition, Left(true)) if topicPartition.topic == GROUP_METADATA_TOPIC_NAME =>\n+          groupCoordinator.onResignation(topicPartition.partition)\n+\n+        case (topicPartition, Left(true)) if topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME =>\n+          // The StopReplica API does not pass through the leader epoch\n+          txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = None)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNjYyMw==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r400426623", "bodyText": "It seems safe to pass through when defined.", "author": "hachikuji", "createdAt": "2020-03-30T19:06:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTI0MjgwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNTY4Ng==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r400425686", "bodyText": "Might not be too big of a problem, but it would be nice to avoid this pass through all the partitions. As it is we have 1) first pass to split partitions, 2) second pass to validate split, 3) third pass to convert to the needed type. Seems like we should be able to save some work here, like perhaps moving the conversion to the caller (even though it's annoying).", "author": "hachikuji", "createdAt": "2020-03-30T19:04:21Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/StopReplicaRequest.java", "diffHunk": "@@ -17,75 +17,93 @@\n package org.apache.kafka.common.requests;\n \n import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.UnsupportedVersionException;\n import org.apache.kafka.common.message.StopReplicaRequestData;\n+import org.apache.kafka.common.message.StopReplicaRequestData.StopReplicaPartitionState;\n import org.apache.kafka.common.message.StopReplicaRequestData.StopReplicaPartitionV0;\n-import org.apache.kafka.common.message.StopReplicaRequestData.StopReplicaTopic;\n+import org.apache.kafka.common.message.StopReplicaRequestData.StopReplicaTopicV1;\n+import org.apache.kafka.common.message.StopReplicaRequestData.StopReplicaTopicState;\n import org.apache.kafka.common.message.StopReplicaResponseData;\n import org.apache.kafka.common.message.StopReplicaResponseData.StopReplicaPartitionError;\n import org.apache.kafka.common.protocol.ApiKeys;\n import org.apache.kafka.common.protocol.Errors;\n import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n-import org.apache.kafka.common.utils.FlattenedIterator;\n import org.apache.kafka.common.utils.MappedIterator;\n import org.apache.kafka.common.utils.Utils;\n \n import java.nio.ByteBuffer;\n import java.util.ArrayList;\n-import java.util.Collection;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n \n public class StopReplicaRequest extends AbstractControlRequest {\n \n     public static class Builder extends AbstractControlRequest.Builder<StopReplicaRequest> {\n-        private final boolean deletePartitions;\n-        private final Collection<TopicPartition> partitions;\n+        private final List<StopReplicaTopicState> topicStates;\n \n-        public Builder(short version, int controllerId, int controllerEpoch, long brokerEpoch, boolean deletePartitions,\n-                       Collection<TopicPartition> partitions) {\n+        public Builder(short version, int controllerId, int controllerEpoch, long brokerEpoch,\n+                       List<StopReplicaTopicState> topicStates) {\n             super(ApiKeys.STOP_REPLICA, version, controllerId, controllerEpoch, brokerEpoch);\n-            this.deletePartitions = deletePartitions;\n-            this.partitions = partitions;\n+            this.topicStates = topicStates;\n         }\n \n         public StopReplicaRequest build(short version) {\n             StopReplicaRequestData data = new StopReplicaRequestData()\n                 .setControllerId(controllerId)\n                 .setControllerEpoch(controllerEpoch)\n-                .setBrokerEpoch(brokerEpoch)\n-                .setDeletePartitions(deletePartitions);\n-\n-            if (version >= 1) {\n-                Map<String, List<Integer>> topicPartitionsMap = CollectionUtils.groupPartitionsByTopic(partitions);\n-                List<StopReplicaTopic> topics = topicPartitionsMap.entrySet().stream().map(entry ->\n-                    new StopReplicaTopic()\n-                        .setName(entry.getKey())\n-                        .setPartitionIndexes(entry.getValue())\n-                ).collect(Collectors.toList());\n+                .setBrokerEpoch(brokerEpoch);\n+\n+            if (version >= 3) {\n+                data.setTopicStates(topicStates);\n+            } else if (version >= 1) {\n+                data.setDeletePartitions(deletePartitions());\n+                List<StopReplicaTopicV1> topics = topicStates.stream().map(topic ->\n+                    new StopReplicaTopicV1()\n+                        .setName(topic.topicName())\n+                        .setPartitionIndexes(topic.partitionStates().stream()\n+                            .map(StopReplicaPartitionState::partitionIndex)\n+                            .collect(Collectors.toList())))\n+                    .collect(Collectors.toList());\n                 data.setTopics(topics);\n             } else {\n-                List<StopReplicaPartitionV0> requestPartitions = partitions.stream().map(tp ->\n-                    new StopReplicaPartitionV0()\n-                        .setTopicName(tp.topic())\n-                        .setPartitionIndex(tp.partition())\n-                ).collect(Collectors.toList());\n-                data.setUngroupedPartitions(requestPartitions);\n+                data.setDeletePartitions(deletePartitions());\n+                List<StopReplicaPartitionV0> partitions = topicStates.stream().flatMap(topic ->\n+                    topic.partitionStates().stream().map(partition ->\n+                        new StopReplicaPartitionV0()\n+                            .setTopicName(topic.topicName())\n+                            .setPartitionIndex(partition.partitionIndex())))\n+                    .collect(Collectors.toList());\n+                data.setUngroupedPartitions(partitions);\n             }\n \n             return new StopReplicaRequest(data, version);\n         }\n \n+        private boolean deletePartitions() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTAyNjA0NA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r401026044", "bodyText": "You're right. I did not realise this. I think that the best way is to move everything to the caller. I will do this.", "author": "dajac", "createdAt": "2020-03-31T15:54:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNTY4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg4NTEzNA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r404885134", "bodyText": "I have refactored this and pushed all the conversion to the caller.", "author": "dajac", "createdAt": "2020-04-07T15:08:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNTY4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQzMTgzNg==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r400431836", "bodyText": "The return type is a bit awkward. As far as I can tell, the left side is just returning the deletion status, which is taken from the request. Was this an optimization in order to avoid another traversal of the request?", "author": "hachikuji", "createdAt": "2020-03-30T19:15:26Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -236,22 +236,22 @@ class KafkaApis(val requestChannel: RequestChannel,\n     if (isBrokerEpochStale(stopReplicaRequest.brokerEpoch)) {\n       // When the broker restarts very quickly, it is possible for this broker to receive request intended\n       // for its previous generation so the broker should skip the stale request.\n-      info(\"Received stop replica request with broker epoch \" +\n+      info(\"Received StopReplica request with broker epoch \" +\n         s\"${stopReplicaRequest.brokerEpoch} smaller than the current broker epoch ${controller.brokerEpoch}\")\n       sendResponseExemptThrottle(request, new StopReplicaResponse(new StopReplicaResponseData().setErrorCode(Errors.STALE_BROKER_EPOCH.code)))\n     } else {\n-      val (result, error) = replicaManager.stopReplicas(stopReplicaRequest)\n+      val (result, error) = replicaManager.stopReplicas(request.context.correlationId, stopReplicaRequest)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTAyNzk5Mg==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r401027992", "bodyText": "That's correct. Now, we also need the leaderEpoch to pass it to the txnCoordinator so I will refactor this.", "author": "dajac", "createdAt": "2020-03-31T15:56:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQzMTgzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg4NTQzOA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r404885438", "bodyText": "I have refactored this as well. It is much better now.", "author": "dajac", "createdAt": "2020-04-07T15:08:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQzMTgzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQ0MzY0MA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r400443640", "bodyText": "I am not sure I followed your response to my previous question. My concern was actually the happy path when the partition exists locally. If we delete first before stopping replica fetchers, then would the fetcher thread handle that gracefully? By removing the fetchers first, we are guaranteed that we couldn't have a write in progress at the time of deletion.", "author": "hachikuji", "createdAt": "2020-03-30T19:34:02Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -364,32 +358,107 @@ class ReplicaManager(val config: KafkaConfig,\n     delayedFetchPurgatory.checkAndComplete(topicPartitionOperationKey)\n   }\n \n-  def stopReplicas(stopReplicaRequest: StopReplicaRequest): (mutable.Map[TopicPartition, Errors], Errors) = {\n+  def stopReplicas(correlationId: Int, stopReplicaRequest: StopReplicaRequest): (mutable.Map[TopicPartition, Either[Boolean, Errors]], Errors) = {\n+    val controllerId = stopReplicaRequest.controllerId\n+    val requestPartitionStates = stopReplicaRequest.partitionStates.asScala\n+    stateChangeLogger.info(s\"Handling StopReplica request correlationId $correlationId from controller \" +\n+      s\"$controllerId for ${requestPartitionStates.size} partitions\")\n+    if (stateChangeLogger.isTraceEnabled)\n+      requestPartitionStates.foreach { case (topicPartition, partitionState) =>\n+        stateChangeLogger.trace(s\"Received StopReplica request $partitionState \" +\n+          s\"correlation id $correlationId from controller $controllerId \" +\n+          s\"epoch ${stopReplicaRequest.controllerEpoch} for partition $topicPartition\")\n+      }\n+\n     replicaStateChangeLock synchronized {\n-      val responseMap = new collection.mutable.HashMap[TopicPartition, Errors]\n-      if (stopReplicaRequest.controllerEpoch() < controllerEpoch) {\n-        stateChangeLogger.warn(\"Received stop replica request from an old controller epoch \" +\n-          s\"${stopReplicaRequest.controllerEpoch}. Latest known controller epoch is $controllerEpoch\")\n+      val responseMap = new collection.mutable.HashMap[TopicPartition, Either[Boolean, Errors]]\n+      if (stopReplicaRequest.controllerEpoch < controllerEpoch) {\n+        stateChangeLogger.warn(s\"Ignoring StopReplica request from \" +\n+          s\"controller $controllerId with correlation id $correlationId \" +\n+          s\"since its controller epoch ${stopReplicaRequest.controllerEpoch} is old. \" +\n+          s\"Latest known controller epoch is $controllerEpoch\")\n         (responseMap, Errors.STALE_CONTROLLER_EPOCH)\n       } else {\n-        val partitions = stopReplicaRequest.partitions.asScala.toSet\n+        val stoppedPartitions = mutable.Map.empty[TopicPartition, StopReplicaPartitionState]\n         controllerEpoch = stopReplicaRequest.controllerEpoch\n+\n+        requestPartitionStates.foreach { case (topicPartition, partitionState) =>\n+          val deletePartition = partitionState.deletePartition\n+\n+          getPartition(topicPartition) match {\n+            case HostedPartition.Offline =>\n+              stateChangeLogger.warn(s\"Ignoring StopReplica request (delete=$deletePartition) from \" +\n+                s\"controller $controllerId with correlation id $correlationId \" +\n+                s\"epoch $controllerEpoch for partition $topicPartition as the local replica for the \" +\n+                \"partition is in an offline log directory\")\n+              responseMap.put(topicPartition, Right(Errors.KAFKA_STORAGE_ERROR))\n+\n+            case HostedPartition.Online(partition) =>\n+              val currentLeaderEpoch = partition.getLeaderEpoch\n+              val requestLeaderEpoch = partitionState.leaderEpoch\n+              // When a topic is deleted, the leader epoch is not incremented. To circumvent this,\n+              // a sentinel value (EpochDuringDelete) overwriting any previous epoch is used.\n+              // When an older version of the StopReplica request which does not contain the leader\n+              // epoch, a sentinel value (NoEpoch) is used and bypass the epoch validation.\n+              if (requestLeaderEpoch == LeaderAndIsr.EpochDuringDelete ||\n+                  requestLeaderEpoch == LeaderAndIsr.NoEpoch ||\n+                  requestLeaderEpoch > currentLeaderEpoch) {\n+                stoppedPartitions += topicPartition -> partitionState\n+              } else if (requestLeaderEpoch < currentLeaderEpoch) {\n+                stateChangeLogger.warn(s\"Ignoring StopReplica request (delete=$deletePartition) from \" +\n+                  s\"controller $controllerId with correlation id $correlationId \" +\n+                  s\"epoch $controllerEpoch for partition $topicPartition since its associated \" +\n+                  s\"leader epoch $requestLeaderEpoch is smaller than the current \" +\n+                  s\"leader epoch $currentLeaderEpoch\")\n+                responseMap.put(topicPartition, Right(Errors.FENCED_LEADER_EPOCH))\n+              } else {\n+                stateChangeLogger.info(s\"Ignoring StopReplica request (delete=$deletePartition) from \" +\n+                  s\"controller $controllerId with correlation id $correlationId \" +\n+                  s\"epoch $controllerEpoch for partition $topicPartition since its associated \" +\n+                  s\"leader epoch $requestLeaderEpoch matches the current leader epoch\")\n+                responseMap.put(topicPartition, Right(Errors.FENCED_LEADER_EPOCH))\n+              }\n+\n+            case HostedPartition.None =>\n+              try {\n+                // Delete log and corresponding folders in case replica manager doesn't hold them anymore.\n+                // This could happen when topic is being deleted while broker is down and recovers.\n+                maybeCleanReplica(topicPartition, deletePartition)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTAzNTUwNg==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r401035506", "bodyText": "Sorry if my comment was not clear. I was trying to argue that the fetcher thread can't be running for a given partition if the partition is not known by the ReplicaManager (case HostedPartition.None) because the fetcher thread is started after the partition is added to allPartitions Map in the ReplicaManager by the LeaderAndIsrRequest and stopped before the partition is removed from allPartitions by the StopReplicaRequest. This is based on my current understanding of the ReplicaManager but, as it is fairly new, I may have missed something though. Did I?\nIt is probably better to keep the previous behavior to be 100% safe.", "author": "dajac", "createdAt": "2020-03-31T16:07:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQ0MzY0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg4NTc0OQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r404885749", "bodyText": "I have reverted to the previous behavior to be 100% safe.", "author": "dajac", "createdAt": "2020-04-07T15:08:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQ0MzY0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA0ODQyMQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r405048421", "bodyText": "I think I get what you were saying now. You were probably right.", "author": "hachikuji", "createdAt": "2020-04-07T19:10:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQ0MzY0MA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "cdff56b9a8b65d52a109098ed56d548cd95b2da8", "url": "https://github.com/apache/kafka/commit/cdff56b9a8b65d52a109098ed56d548cd95b2da8", "message": "KAFKA-9539; Add leader epoch in StopReplicaRequest (KIP-570)", "committedDate": "2020-04-07T14:36:33Z", "type": "commit"}, {"oid": "cdff56b9a8b65d52a109098ed56d548cd95b2da8", "url": "https://github.com/apache/kafka/commit/cdff56b9a8b65d52a109098ed56d548cd95b2da8", "message": "KAFKA-9539; Add leader epoch in StopReplicaRequest (KIP-570)", "committedDate": "2020-04-07T14:36:33Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjYxNjU5Ng==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r406616596", "bodyText": "AND not OR. could be replaced by >= 0 as well.", "author": "dajac", "createdAt": "2020-04-10T06:05:00Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -236,20 +237,34 @@ class KafkaApis(val requestChannel: RequestChannel,\n     if (isBrokerEpochStale(stopReplicaRequest.brokerEpoch)) {\n       // When the broker restarts very quickly, it is possible for this broker to receive request intended\n       // for its previous generation so the broker should skip the stale request.\n-      info(\"Received stop replica request with broker epoch \" +\n+      info(\"Received StopReplica request with broker epoch \" +\n         s\"${stopReplicaRequest.brokerEpoch} smaller than the current broker epoch ${controller.brokerEpoch}\")\n-      sendResponseExemptThrottle(request, new StopReplicaResponse(new StopReplicaResponseData().setErrorCode(Errors.STALE_BROKER_EPOCH.code)))\n+      sendResponseExemptThrottle(request, new StopReplicaResponse(\n+        new StopReplicaResponseData().setErrorCode(Errors.STALE_BROKER_EPOCH.code)))\n     } else {\n-      val (result, error) = replicaManager.stopReplicas(stopReplicaRequest)\n+      val partitionStates = stopReplicaRequest.partitionStates().asScala\n+      val (result, error) = replicaManager.stopReplicas(\n+        request.context.correlationId,\n+        stopReplicaRequest.controllerId,\n+        stopReplicaRequest.controllerEpoch,\n+        stopReplicaRequest.brokerEpoch,\n+        partitionStates)\n       // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n       // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n       result.foreach { case (topicPartition, error) =>\n-        if (error == Errors.NONE && stopReplicaRequest.deletePartitions) {\n-          if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME) {\n+        if (error == Errors.NONE) {\n+          if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n+              && partitionStates(topicPartition).deletePartition) {\n             groupCoordinator.onResignation(topicPartition.partition)\n-          } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME) {\n-            // The StopReplica API does not pass through the leader epoch\n-            txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = None)\n+          } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n+                     && partitionStates(topicPartition).deletePartition) {\n+            val partitionState = partitionStates(topicPartition)\n+            val leaderEpoch = if (partitionState.leaderEpoch != LeaderAndIsr.EpochDuringDelete ||", "originalCommit": "cdff56b9a8b65d52a109098ed56d548cd95b2da8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0edeeeb600dc1978d23f44e6a8630fa07aaaade3", "url": "https://github.com/apache/kafka/commit/0edeeeb600dc1978d23f44e6a8630fa07aaaade3", "message": "fixup", "committedDate": "2020-04-13T15:48:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgwOTg5MA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r407809890", "bodyText": "The first part of this definitely makes sense, but what is the motivation for the second part? Why not use LeaderAndIsr.NoEpoch? Though I can't really think of what would cause this case to be hit.", "author": "hachikuji", "createdAt": "2020-04-14T01:20:08Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -396,9 +395,23 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n   def addStopReplicaRequestForBrokers(brokerIds: Seq[Int],\n                                       topicPartition: TopicPartition,\n                                       deletePartition: Boolean): Unit = {\n+    // A sentinel (-2) is used as an epoch if the topic is queued for deletion or\n+    // does not have a leader yet. This sentinel overrides any existing epoch.", "originalCommit": "0edeeeb600dc1978d23f44e6a8630fa07aaaade3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODExNDIxOQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r408114219", "bodyText": "Indeed, LeaderAndIsr.NoEpoch makes more sense as a default value here. I did a mistake here. You're right. I don't think this should ever happen but I went on the defensive path with a default value in case.", "author": "dajac", "createdAt": "2020-04-14T12:56:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgwOTg5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODEyNzI1NQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r408127255", "bodyText": "I have updated the PR.", "author": "dajac", "createdAt": "2020-04-14T13:16:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgwOTg5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxMDUzNw==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r407810537", "bodyText": "No need to fix here, but do you know why we do this filtering?", "author": "hachikuji", "createdAt": "2020-04-14T01:22:29Z", "path": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "diffHunk": "@@ -396,9 +395,23 @@ abstract class AbstractControllerBrokerRequestBatch(config: KafkaConfig,\n   def addStopReplicaRequestForBrokers(brokerIds: Seq[Int],\n                                       topicPartition: TopicPartition,\n                                       deletePartition: Boolean): Unit = {\n+    // A sentinel (-2) is used as an epoch if the topic is queued for deletion or\n+    // does not have a leader yet. This sentinel overrides any existing epoch.\n+    val leaderEpoch = if (controllerContext.isTopicQueuedUpForDeletion(topicPartition.topic)) {\n+      LeaderAndIsr.EpochDuringDelete\n+    } else {\n+      controllerContext.partitionLeadershipInfo.get(topicPartition)\n+        .map(_.leaderAndIsr.leaderEpoch)\n+        .getOrElse(LeaderAndIsr.EpochDuringDelete)\n+    }\n+\n     brokerIds.filter(_ >= 0).foreach { brokerId =>", "originalCommit": "0edeeeb600dc1978d23f44e6a8630fa07aaaade3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODEyMjg1OQ==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r408122859", "bodyText": "I've asked myself the same question but I couldn't find a reason. I believe that brokerId is always >= 0 in the controller.", "author": "dajac", "createdAt": "2020-04-14T13:10:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxMDUzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxNDk0Nw==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r407814947", "bodyText": "Could potentially use nonOfflinePartition(topicPartition).foreach", "author": "hachikuji", "createdAt": "2020-04-14T01:38:14Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -323,24 +324,17 @@ class ReplicaManager(val config: KafkaConfig,\n       brokerTopicStats.removeMetrics(topic)\n   }\n \n-  def stopReplica(topicPartition: TopicPartition, deletePartition: Boolean)  = {\n-    stateChangeLogger.trace(s\"Handling stop replica (delete=$deletePartition) for partition $topicPartition\")\n-\n+  def stopReplica(topicPartition: TopicPartition, deletePartition: Boolean): Unit  = {\n     if (deletePartition) {\n       getPartition(topicPartition) match {", "originalCommit": "0edeeeb600dc1978d23f44e6a8630fa07aaaade3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODEyNjU4MA==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r408126580", "bodyText": "Actually, it does not work because we need both the reference to the hosted partition and the partition bellow: hostedPartition and removedPartition. nonOfflinePartition only provides the latter.", "author": "dajac", "createdAt": "2020-04-14T13:15:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxNDk0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxNTU4Mg==", "url": "https://github.com/apache/kafka/pull/8257#discussion_r407815582", "bodyText": "Good call updating this.", "author": "hachikuji", "createdAt": "2020-04-14T01:40:30Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -364,32 +356,97 @@ class ReplicaManager(val config: KafkaConfig,\n     delayedFetchPurgatory.checkAndComplete(topicPartitionOperationKey)\n   }\n \n-  def stopReplicas(stopReplicaRequest: StopReplicaRequest): (mutable.Map[TopicPartition, Errors], Errors) = {\n+  def stopReplicas(correlationId: Int,\n+                   controllerId: Int,\n+                   controllerEpoch: Int,\n+                   brokerEpoch: Long,\n+                   partitionStates: Map[TopicPartition, StopReplicaPartitionState]\n+                  ): (mutable.Map[TopicPartition, Errors], Errors) = {\n+    stateChangeLogger.info(s\"Handling StopReplica request correlationId $correlationId from controller \" +\n+      s\"$controllerId for ${partitionStates.size} partitions\")\n+    if (stateChangeLogger.isTraceEnabled)\n+      partitionStates.foreach { case (topicPartition, partitionState) =>\n+        stateChangeLogger.trace(s\"Received StopReplica request $partitionState \" +\n+          s\"correlation id $correlationId from controller $controllerId \" +\n+          s\"epoch $controllerEpoch for partition $topicPartition\")\n+      }\n+\n     replicaStateChangeLock synchronized {\n       val responseMap = new collection.mutable.HashMap[TopicPartition, Errors]\n-      if (stopReplicaRequest.controllerEpoch() < controllerEpoch) {\n-        stateChangeLogger.warn(\"Received stop replica request from an old controller epoch \" +\n-          s\"${stopReplicaRequest.controllerEpoch}. Latest known controller epoch is $controllerEpoch\")\n+      if (controllerEpoch < this.controllerEpoch) {\n+        stateChangeLogger.warn(s\"Ignoring StopReplica request from \" +\n+          s\"controller $controllerId with correlation id $correlationId \" +\n+          s\"since its controller epoch $controllerEpoch is old. \" +\n+          s\"Latest known controller epoch is ${this.controllerEpoch}\")\n         (responseMap, Errors.STALE_CONTROLLER_EPOCH)\n       } else {\n-        val partitions = stopReplicaRequest.partitions.asScala.toSet\n-        controllerEpoch = stopReplicaRequest.controllerEpoch\n+        this.controllerEpoch = controllerEpoch", "originalCommit": "0edeeeb600dc1978d23f44e6a8630fa07aaaade3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "49360d6d46bf292c7b9a774be923073fd049c19d", "url": "https://github.com/apache/kafka/commit/49360d6d46bf292c7b9a774be923073fd049c19d", "message": "Address review", "committedDate": "2020-04-14T13:15:53Z", "type": "commit"}]}