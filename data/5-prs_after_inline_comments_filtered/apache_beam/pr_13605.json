{"pr_number": 13605, "pr_title": "[BEAM-11508] Set sdkHarnessContainerImages in workerpool configuratio\u2026", "pr_createdAt": "2020-12-23T07:47:11Z", "pr_url": "https://github.com/apache/beam/pull/13605", "timeline": [{"oid": "c9f868adfe3ebec23b59a4401694826e7e714bec", "url": "https://github.com/apache/beam/commit/c9f868adfe3ebec23b59a4401694826e7e714bec", "message": "[BEAM-11508] Set sdkHarnessContainerImages in workerpool configuration for Dataflow runner v2", "committedDate": "2020-12-23T10:32:01Z", "type": "commit"}, {"oid": "c9f868adfe3ebec23b59a4401694826e7e714bec", "url": "https://github.com/apache/beam/commit/c9f868adfe3ebec23b59a4401694826e7e714bec", "message": "[BEAM-11508] Set sdkHarnessContainerImages in workerpool configuration for Dataflow runner v2", "committedDate": "2020-12-23T10:32:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjI5MA==", "url": "https://github.com/apache/beam/pull/13605#discussion_r550936290", "bodyText": "Is there a util function that we can depend on (similar to [1] for Python) introduce of checking these properties separately for runner v2 which could lead to inconsistencies ?\n[1] https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py#L1039", "author": "chamikaramj", "createdAt": "2021-01-03T00:15:56Z", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1186,6 +1190,45 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n     return dataflowPipelineJob;\n   }\n \n+  static void configureSdkHarnessContainerImages(\n+      DataflowPipelineOptions options,\n+      RunnerApi.Pipeline pipelineProto,\n+      Job newJob,\n+      String workerHarnessContainerImage) {\n+    if (hasExperiment(options, \"use_runner_v2\") || hasExperiment(options, \"use_unified_worker\")) {", "originalCommit": "c9f868adfe3ebec23b59a4401694826e7e714bec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI5MjE1NA==", "url": "https://github.com/apache/beam/pull/13605#discussion_r552292154", "bodyText": "add utility function to check runner v2 flags", "author": "ihji", "createdAt": "2021-01-06T00:44:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjI5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjU2OA==", "url": "https://github.com/apache/beam/pull/13605#discussion_r550936568", "bodyText": "It might make sense to de-dupe here in-case same Docker URL gets added twice (we can generalize this when Dataflow supports multiple environments with the same Docker URL hence please add a TODO).", "author": "chamikaramj", "createdAt": "2021-01-03T00:19:14Z", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1186,6 +1190,45 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n     return dataflowPipelineJob;\n   }\n \n+  static void configureSdkHarnessContainerImages(\n+      DataflowPipelineOptions options,\n+      RunnerApi.Pipeline pipelineProto,\n+      Job newJob,\n+      String workerHarnessContainerImage) {\n+    if (hasExperiment(options, \"use_runner_v2\") || hasExperiment(options, \"use_unified_worker\")) {\n+      ImmutableSet.Builder<String> sdkContainerUrlSetBuilder = ImmutableSet.builder();\n+      sdkContainerUrlSetBuilder.add(workerHarnessContainerImage);\n+      for (Map.Entry<String, RunnerApi.Environment> entry :\n+          pipelineProto.getComponents().getEnvironmentsMap().entrySet()) {\n+        if (!BeamUrns.getUrn(RunnerApi.StandardEnvironments.Environments.DOCKER)\n+            .equals(entry.getValue().getUrn())) {\n+          throw new RuntimeException(\n+              \"Dataflow can only execute pipeline steps in Docker environments: \"\n+                  + entry.getValue().getUrn());\n+        }\n+        RunnerApi.DockerPayload dockerPayload;\n+        try {\n+          dockerPayload = RunnerApi.DockerPayload.parseFrom(entry.getValue().getPayload());\n+        } catch (InvalidProtocolBufferException e) {\n+          throw new RuntimeException(\"Error parsing docker payload.\", e);\n+        }\n+        sdkContainerUrlSetBuilder.add(dockerPayload.getContainerImage());", "originalCommit": "c9f868adfe3ebec23b59a4401694826e7e714bec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI5MjkxNA==", "url": "https://github.com/apache/beam/pull/13605#discussion_r552292914", "bodyText": "We are collecting the urls in an immutable set so there should be only one entry with the same container url.", "author": "ihji", "createdAt": "2021-01-06T00:47:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjU2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjY1Mw==", "url": "https://github.com/apache/beam/pull/13605#discussion_r550936653", "bodyText": "Do we need any special handling for pipeline SDK similar to following for Python ?\nhttps://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py#L285", "author": "chamikaramj", "createdAt": "2021-01-03T00:20:26Z", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1186,6 +1190,45 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n     return dataflowPipelineJob;\n   }\n \n+  static void configureSdkHarnessContainerImages(\n+      DataflowPipelineOptions options,\n+      RunnerApi.Pipeline pipelineProto,\n+      Job newJob,\n+      String workerHarnessContainerImage) {\n+    if (hasExperiment(options, \"use_runner_v2\") || hasExperiment(options, \"use_unified_worker\")) {\n+      ImmutableSet.Builder<String> sdkContainerUrlSetBuilder = ImmutableSet.builder();\n+      sdkContainerUrlSetBuilder.add(workerHarnessContainerImage);\n+      for (Map.Entry<String, RunnerApi.Environment> entry :\n+          pipelineProto.getComponents().getEnvironmentsMap().entrySet()) {\n+        if (!BeamUrns.getUrn(RunnerApi.StandardEnvironments.Environments.DOCKER)\n+            .equals(entry.getValue().getUrn())) {\n+          throw new RuntimeException(\n+              \"Dataflow can only execute pipeline steps in Docker environments: \"\n+                  + entry.getValue().getUrn());\n+        }\n+        RunnerApi.DockerPayload dockerPayload;\n+        try {\n+          dockerPayload = RunnerApi.DockerPayload.parseFrom(entry.getValue().getPayload());\n+        } catch (InvalidProtocolBufferException e) {\n+          throw new RuntimeException(\"Error parsing docker payload.\", e);\n+        }\n+        sdkContainerUrlSetBuilder.add(dockerPayload.getContainerImage());\n+      }\n+      List<SdkHarnessContainerImage> sdkContainerList =", "originalCommit": "c9f868adfe3ebec23b59a4401694826e7e714bec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI5MjYzOQ==", "url": "https://github.com/apache/beam/pull/13605#discussion_r552292639", "bodyText": "Pipeline SDK container is passed as a parameter and to be the first entry added to the container url set (as same as the python code).", "author": "ihji", "createdAt": "2021-01-06T00:46:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjY1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjY5OQ==", "url": "https://github.com/apache/beam/pull/13605#discussion_r550936699", "bodyText": "For any Python environments, we should set 'useSingleCorePerContainer' to true for efficiency.", "author": "chamikaramj", "createdAt": "2021-01-03T00:21:08Z", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1186,6 +1190,45 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n     return dataflowPipelineJob;\n   }\n \n+  static void configureSdkHarnessContainerImages(\n+      DataflowPipelineOptions options,\n+      RunnerApi.Pipeline pipelineProto,\n+      Job newJob,\n+      String workerHarnessContainerImage) {\n+    if (hasExperiment(options, \"use_runner_v2\") || hasExperiment(options, \"use_unified_worker\")) {\n+      ImmutableSet.Builder<String> sdkContainerUrlSetBuilder = ImmutableSet.builder();\n+      sdkContainerUrlSetBuilder.add(workerHarnessContainerImage);\n+      for (Map.Entry<String, RunnerApi.Environment> entry :\n+          pipelineProto.getComponents().getEnvironmentsMap().entrySet()) {\n+        if (!BeamUrns.getUrn(RunnerApi.StandardEnvironments.Environments.DOCKER)\n+            .equals(entry.getValue().getUrn())) {\n+          throw new RuntimeException(\n+              \"Dataflow can only execute pipeline steps in Docker environments: \"\n+                  + entry.getValue().getUrn());\n+        }\n+        RunnerApi.DockerPayload dockerPayload;\n+        try {\n+          dockerPayload = RunnerApi.DockerPayload.parseFrom(entry.getValue().getPayload());\n+        } catch (InvalidProtocolBufferException e) {\n+          throw new RuntimeException(\"Error parsing docker payload.\", e);\n+        }\n+        sdkContainerUrlSetBuilder.add(dockerPayload.getContainerImage());\n+      }\n+      List<SdkHarnessContainerImage> sdkContainerList =\n+          sdkContainerUrlSetBuilder.build().stream()\n+              .map(\n+                  (String url) -> {\n+                    SdkHarnessContainerImage image = new SdkHarnessContainerImage();\n+                    image.setContainerImage(url);\n+                    return image;\n+                  })\n+              .collect(Collectors.toList());\n+      for (WorkerPool workerPool : newJob.getEnvironment().getWorkerPools()) {\n+        workerPool.setSdkHarnessContainerImages(sdkContainerList);", "originalCommit": "c9f868adfe3ebec23b59a4401694826e7e714bec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI5MzIxOQ==", "url": "https://github.com/apache/beam/pull/13605#discussion_r552293219", "bodyText": "Done.", "author": "ihji", "createdAt": "2021-01-06T00:48:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjY5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjc0Ng==", "url": "https://github.com/apache/beam/pull/13605#discussion_r550936746", "bodyText": "Please also try out a Dataflow GCE job (or existing ITs).", "author": "chamikaramj", "createdAt": "2021-01-03T00:21:47Z", "path": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowRunnerTest.java", "diffHunk": "@@ -1342,6 +1347,57 @@ public void testTransformTranslator() throws IOException {\n     assertTrue(transform.translated);\n   }\n \n+  @Test\n+  public void testSdkHarnessConfiguration() throws IOException {", "originalCommit": "c9f868adfe3ebec23b59a4401694826e7e714bec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI5NTYwNw==", "url": "https://github.com/apache/beam/pull/13605#discussion_r552295607", "bodyText": "The change applies to all existing ITs with use_runner_v2 flag (for example, runner v2 validate runner tests should fail if something goes wrong with this commit). Do you think we need a separate test only for this change?", "author": "ihji", "createdAt": "2021-01-06T00:51:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjc0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcxOTAyNQ==", "url": "https://github.com/apache/beam/pull/13605#discussion_r553719025", "bodyText": "No existing Runner v2 ITs should be adequate. Thanks.", "author": "chamikaramj", "createdAt": "2021-01-08T03:19:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjc0Ng=="}], "type": "inlineReview"}, {"oid": "ac71556990f8c3a6f05de9b1448449234a91248a", "url": "https://github.com/apache/beam/commit/ac71556990f8c3a6f05de9b1448449234a91248a", "message": "add utility function to check runner v2, set single-core flag for python", "committedDate": "2021-01-06T00:42:57Z", "type": "commit"}]}