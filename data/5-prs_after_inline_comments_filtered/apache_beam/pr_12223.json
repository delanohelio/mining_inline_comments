{"pr_number": 12223, "pr_title": "[Beam-4379] Make ParquetIO read splittable", "pr_createdAt": "2020-07-10T19:45:55Z", "pr_url": "https://github.com/apache/beam/pull/12223", "timeline": [{"oid": "8abd03fd29d12b4ae80b39186be99315d31cdba0", "url": "https://github.com/apache/beam/commit/8abd03fd29d12b4ae80b39186be99315d31cdba0", "message": "first approach", "committedDate": "2020-07-09T23:55:46Z", "type": "commit"}, {"oid": "f08f5092d2a417ef479a619409f7ee739af1f164", "url": "https://github.com/apache/beam/commit/f08f5092d2a417ef479a619409f7ee739af1f164", "message": "splittable dofn", "committedDate": "2020-07-10T19:42:58Z", "type": "commit"}, {"oid": "3a72ed67f45b15d78cc35a2cfe90b730eff2c662", "url": "https://github.com/apache/beam/commit/3a72ed67f45b15d78cc35a2cfe90b730eff2c662", "message": "Merge branch 'master' into BEAM-4379", "committedDate": "2020-07-10T19:48:12Z", "type": "commit"}, {"oid": "528c43d4267d7a14ec32bb297914895b35f1ae6b", "url": "https://github.com/apache/beam/commit/528c43d4267d7a14ec32bb297914895b35f1ae6b", "message": "spotless", "committedDate": "2020-07-10T20:08:39Z", "type": "commit"}, {"oid": "589c1b5ace602ad02811ef6a2973671627cccc9f", "url": "https://github.com/apache/beam/commit/589c1b5ace602ad02811ef6a2973671627cccc9f", "message": "git change", "committedDate": "2020-07-10T20:21:59Z", "type": "commit"}, {"oid": "a627b54bd6c3c0e9d4d1d9641ae55d0f3250c367", "url": "https://github.com/apache/beam/commit/a627b54bd6c3c0e9d4d1d9641ae55d0f3250c367", "message": "change function name", "committedDate": "2020-07-10T21:33:15Z", "type": "commit"}, {"oid": "01f5b8b81a72218da0a389a7174fd91fdc5a4342", "url": "https://github.com/apache/beam/commit/01f5b8b81a72218da0a389a7174fd91fdc5a4342", "message": "fixing spittable dofn part,stoll has problem in function", "committedDate": "2020-07-13T21:19:35Z", "type": "commit"}, {"oid": "515dbb3338559e15b937a87c32fcfccbf7e87e69", "url": "https://github.com/apache/beam/commit/515dbb3338559e15b937a87c32fcfccbf7e87e69", "message": "bug fixed", "committedDate": "2020-07-16T19:20:52Z", "type": "commit"}, {"oid": "8d3a0f3efa853d10c6918d2eca40ad4c426ee72a", "url": "https://github.com/apache/beam/commit/8d3a0f3efa853d10c6918d2eca40ad4c426ee72a", "message": "add build option and unit test", "committedDate": "2020-07-16T20:05:38Z", "type": "commit"}, {"oid": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4", "url": "https://github.com/apache/beam/commit/ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4", "message": "avro change", "committedDate": "2020-07-16T21:47:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYzMjUxMQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456632511", "bodyText": "getSplit() can be False. You can mark getSplit() as not null with False as default value.", "author": "boyuanzz", "createdAt": "2020-07-17T19:24:56Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -190,12 +227,19 @@ public Read withAvroDataModel(GenericData model) {\n     @Override\n     public PCollection<GenericRecord> expand(PBegin input) {\n       checkNotNull(getFilepattern(), \"Filepattern cannot be null.\");\n-\n-      return input\n-          .apply(\"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n-          .apply(FileIO.matchAll())\n-          .apply(FileIO.readMatches())\n-          .apply(readFiles(getSchema()).withAvroDataModel(getAvroDataModel()));\n+      if (getSplit() != null) {", "originalCommit": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYzMjc5Mw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456632793", "bodyText": "Same above.", "author": "boyuanzz", "createdAt": "2020-07-17T19:25:36Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +284,151 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (getSplit() != null) {", "originalCommit": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0MTIwNw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456641207", "bodyText": "L231-L234 and L237-L240 are the same. We can make them as common part and set readFiles differently.", "author": "boyuanzz", "createdAt": "2020-07-17T19:45:22Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -190,12 +227,19 @@ public Read withAvroDataModel(GenericData model) {\n     @Override\n     public PCollection<GenericRecord> expand(PBegin input) {\n       checkNotNull(getFilepattern(), \"Filepattern cannot be null.\");\n-\n-      return input\n-          .apply(\"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n-          .apply(FileIO.matchAll())\n-          .apply(FileIO.readMatches())\n-          .apply(readFiles(getSchema()).withAvroDataModel(getAvroDataModel()));\n+      if (getSplit() != null) {\n+        return input", "originalCommit": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0MjcxMQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456642711", "bodyText": "you can replace filename  with file.getMetadata().resourceId() directly instead of creating a local var.", "author": "boyuanzz", "createdAt": "2020-07-17T19:48:40Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +284,151 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (getSplit() != null) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          ResourceId filename = file.getMetadata().resourceId();\n+          throw new RuntimeException(String.format(\"File has to be seekable: %s\", filename));", "originalCommit": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NTEyNw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456645127", "bodyText": "Are you sure you want to split the OffsetRange initially into one offset per range? It seems too much to me. Also one offset per range means no more further split could happen.", "author": "boyuanzz", "createdAt": "2020-07-17T19:54:00Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +284,151 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (getSplit() != null) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          ResourceId filename = file.getMetadata().resourceId();\n+          throw new RuntimeException(String.format(\"File has to be seekable: %s\", filename));\n+        }\n+\n+        SeekableByteChannel seekableByteChannel = file.openSeekable();\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = new BeamParquetInputFile(seekableByteChannel);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        for (int i = 0; i < reader.getRowGroups().size(); i++) {\n+          if (i < tracker.currentRestriction().getFrom()) {\n+            reader.skipNextRowGroup();\n+            continue;\n+          }\n+          if (tracker.tryClaim((long) i)) {\n+            PageReadStore pages = reader.readNextRowGroup();\n+            i += 1;\n+            RecordReader<GenericRecord> recordReader =\n+                columnIO.getRecordReader(\n+                    pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+            GenericRecord read;\n+            long current = 0;\n+            long totalRows = pages.getRowCount();\n+            while (current < totalRows) {\n+              read = recordReader.read();\n+              outputReceiver.output(read);\n+              current += 1;\n+            }\n+          } else {\n+            break;\n+          }\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          ResourceId filename = file.getMetadata().resourceId();\n+          throw new RuntimeException(String.format(\"File has to be seekable: %s\", filename));\n+        }\n+        SeekableByteChannel seekableByteChannel = file.openSeekable();\n+        InputFile inputFile = new BeamParquetInputFile(seekableByteChannel);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(@Restriction OffsetRange restriction, OutputReceiver<OffsetRange> out) {", "originalCommit": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY3MzM4NA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456673384", "bodyText": "I will change it into dividing the range into half each time.", "author": "danielxjd", "createdAt": "2020-07-17T21:03:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY5ODM0Mg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456698342", "bodyText": "The split  here is for initial split, not for dynamic split. For example, if you have one file(2 row groups in total) as input, after initial split you will get (file, [group1]), (file, [group2]) as the input of your process fn. Usually, we expose an API from IO to let the end user to set desired initial splits.", "author": "boyuanzz", "createdAt": "2020-07-17T22:22:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjgyMTM3MA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456821370", "bodyText": "I think Split into half as the initial splits should be fine as our purpose for making it into splittable doFn is for the read process to be faster. User might not need to put in an actual split info as they might not know exact information about the file before they read them.", "author": "danielxjd", "createdAt": "2020-07-18T19:41:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcwOTQ5OA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r457709498", "bodyText": "I feel like it may not feasible to hardcode the initial split. @chamikaramj What do you think?", "author": "boyuanzz", "createdAt": "2020-07-20T21:45:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTE5MDg4MQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r461190881", "bodyText": "Usual pattern for files is to initial split into 64MB blocks and dynamic split further as needed. I believe this is the tried and tested method for existing file-based sources. This might require changing to a tracker that use byte positions (0 to size of file) instead of a tracker that goes from 0 to number of blocks.", "author": "chamikaramj", "createdAt": "2020-07-27T21:49:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTE5MzE3OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r461193179", "bodyText": "@boyuanzz do you think we should add an abstract block-based SDF implementation/utility that will be shared between multiple source implementations (for example, Parquet, Avro) similar to [1] for bounded sources ?\n[1] https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/BlockBasedSource.java", "author": "chamikaramj", "createdAt": "2020-07-27T21:55:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIwMDUyMA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r461200520", "bodyText": "May be we should add a BlockBasedTracker that extends OffsetRangeTracker that goes from 0 to size of file and can split at block boundaries but can provide better progress signals (based on element estimates).", "author": "chamikaramj", "createdAt": "2020-07-27T22:12:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTc5Njk1Mw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r461796953", "bodyText": "It seems like what Jiadai is trying to do is taking a file as element and the number of row group as restriction. When SDF.process() is called, it will read all row groups defined in the restriction for one file. If using BlockBasedTracker, the restriction will be the number of blocks. I think the key point here is to define what should be the element and restriction for this source.", "author": "boyuanzz", "createdAt": "2020-07-28T18:47:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTkyODc0MQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r461928741", "bodyText": "I have tried to Implement a block tracker which still provide the information of progress in bytes. So the restriction is still block but the progress method built inside the range tracker tracks the estimate progress.", "author": "danielxjd", "createdAt": "2020-07-28T22:26:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk1MzQ4OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r461953489", "bodyText": "Yes, I think we can build a BlockBasedTracker that uses number of blocks for restriction but aware of byte positions. This will allow it to better perform initial splitting and provide a better progress signal based on estimated element positions (by overriding getProgress()).\nWe do something like this for progress in existing block-based source: https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/BlockBasedSource.java#L241", "author": "chamikaramj", "createdAt": "2020-07-28T23:37:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NTEyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1MzkzMA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456653930", "bodyText": "I would get the start position by tracker.currentRestriction().getFrom() and have a util function to move the cursor of reader to such index. Then you can do a while loop like while(tracker.tryClaim(i)).", "author": "boyuanzz", "createdAt": "2020-07-17T20:14:47Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +284,151 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (getSplit() != null) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          ResourceId filename = file.getMetadata().resourceId();\n+          throw new RuntimeException(String.format(\"File has to be seekable: %s\", filename));\n+        }\n+\n+        SeekableByteChannel seekableByteChannel = file.openSeekable();\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = new BeamParquetInputFile(seekableByteChannel);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        for (int i = 0; i < reader.getRowGroups().size(); i++) {", "originalCommit": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY3MzE2NQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456673165", "bodyText": "But I think I need to skipped the blocks before the starting position because there is no method to get the position of the blocks directly like what we usually did with the offset.", "author": "danielxjd", "createdAt": "2020-07-17T21:03:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1MzkzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njc0MzE2NQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456743165", "bodyText": "Yeah I understand. You could do something like:\ncurrent = tracker.currentRestriction().getFrom();\nmoveReadToCurrent(reader, current);\nwhile(tracker.tryClaim(current)) {\n  doSomething;\n  current += 1;\n}", "author": "boyuanzz", "createdAt": "2020-07-18T03:33:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1MzkzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjgyMTM5NQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456821395", "bodyText": "Ok, I have made the relevant change.", "author": "danielxjd", "createdAt": "2020-07-18T19:42:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1MzkzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1NDI5Mw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456654293", "bodyText": "You could do  outputReceiver.output(recordReader.read()) to get rid of local var.", "author": "boyuanzz", "createdAt": "2020-07-17T20:15:39Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +284,151 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (getSplit() != null) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          ResourceId filename = file.getMetadata().resourceId();\n+          throw new RuntimeException(String.format(\"File has to be seekable: %s\", filename));\n+        }\n+\n+        SeekableByteChannel seekableByteChannel = file.openSeekable();\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = new BeamParquetInputFile(seekableByteChannel);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        for (int i = 0; i < reader.getRowGroups().size(); i++) {\n+          if (i < tracker.currentRestriction().getFrom()) {\n+            reader.skipNextRowGroup();\n+            continue;\n+          }\n+          if (tracker.tryClaim((long) i)) {\n+            PageReadStore pages = reader.readNextRowGroup();\n+            i += 1;\n+            RecordReader<GenericRecord> recordReader =\n+                columnIO.getRecordReader(\n+                    pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+            GenericRecord read;\n+            long current = 0;\n+            long totalRows = pages.getRowCount();\n+            while (current < totalRows) {\n+              read = recordReader.read();\n+              outputReceiver.output(read);", "originalCommit": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0c164c34ce06e6b8450357d3c6f7c16553c0cd11", "url": "https://github.com/apache/beam/commit/0c164c34ce06e6b8450357d3c6f7c16553c0cd11", "message": "remove replicate codes", "committedDate": "2020-07-17T21:13:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcwNzY5Nw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456707697", "bodyText": "You will also need to implement @ GetRestrictionCoder which returns a OffsetRange.Coder. Also you may want to consider implementing @GetSize to give a better sizing information.\nFor more references:\nhttps://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFn.java#L661-L698\nhttps://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFn.java#L1006\nhttps://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFn.java#L995", "author": "boyuanzz", "createdAt": "2020-07-17T23:00:46Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +281,147 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {", "originalCommit": "0c164c34ce06e6b8450357d3c6f7c16553c0cd11", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjgyMDY4Mw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456820683", "bodyText": "I am not sure if GetRestrictionCoder is needed as the default one should work well. I have implement the GetSize to use the record count as the sizing information.", "author": "danielxjd", "createdAt": "2020-07-18T19:33:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcwNzY5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcwOTQ2Mg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r457709462", "bodyText": "I would prefer make it explicitly.", "author": "boyuanzz", "createdAt": "2020-07-20T21:45:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcwNzY5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI5Mzg1MQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r458293851", "bodyText": "Method added.", "author": "danielxjd", "createdAt": "2020-07-21T18:11:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcwNzY5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcxMzM5MA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456713390", "bodyText": "It's possible that model is null. Is it expected?", "author": "boyuanzz", "createdAt": "2020-07-17T23:26:53Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +281,147 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);", "originalCommit": "0c164c34ce06e6b8450357d3c6f7c16553c0cd11", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjgxODUyOA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r456818528", "bodyText": "if it is null, then the readSupport will create a new AvroReadSupport with null as its parameter.", "author": "danielxjd", "createdAt": "2020-07-18T19:05:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcxMzM5MA=="}], "type": "inlineReview"}, {"oid": "2d529b85b19ffc263c2285fcfacc7dfb4446497e", "url": "https://github.com/apache/beam/commit/2d529b85b19ffc263c2285fcfacc7dfb4446497e", "message": "add getSize", "committedDate": "2020-07-18T19:45:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxMDEzMA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r457710130", "bodyText": "If the restriction is null, it should means there is no more work.", "author": "boyuanzz", "createdAt": "2020-07-20T21:46:47Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +283,164 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          GenericRecord read;\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            outputReceiver.output(recordReader.read());\n+            currentRow += 1;\n+          }\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(@Restriction OffsetRange restriction, OutputReceiver<OffsetRange> out) {\n+        for (OffsetRange range : restriction.split(1, 0)) {\n+          out.output(range);\n+        }\n+      }\n+\n+      @NewTracker\n+      public OffsetRangeTracker newTracker(@Restriction OffsetRange restriction) {\n+        return new OffsetRangeTracker(restriction);\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return reader.getRecordCount();", "originalCommit": "2d529b85b19ffc263c2285fcfacc7dfb4446497e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxMjYwNg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r457712606", "bodyText": "If it's feasible, it would be nice to track an estimated avg row size like KafkaIO. Then the size will be avgSize * rowCount. @chamikaramj Do you think it's necessary to do so?", "author": "boyuanzz", "createdAt": "2020-07-20T21:52:28Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +283,164 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          GenericRecord read;\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            outputReceiver.output(recordReader.read());\n+            currentRow += 1;\n+          }\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(@Restriction OffsetRange restriction, OutputReceiver<OffsetRange> out) {\n+        for (OffsetRange range : restriction.split(1, 0)) {\n+          out.output(range);\n+        }\n+      }\n+\n+      @NewTracker\n+      public OffsetRangeTracker newTracker(@Restriction OffsetRange restriction) {\n+        return new OffsetRangeTracker(restriction);\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return reader.getRecordCount();\n+        } else {\n+          long start = restriction.getFrom();\n+          long end = restriction.getTo();\n+          List<BlockMetaData> blocks = reader.getRowGroups();\n+          double size = 0;\n+          for (long i = start; i < end; i++) {\n+            size += blocks.get((int) i).getRowCount();\n+          }\n+          return size;", "originalCommit": "2d529b85b19ffc263c2285fcfacc7dfb4446497e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI5NzQ0Nw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r458297447", "bodyText": "I think get size is used to get an idea of the current process and for the purpose of optimized split. Since the size of each row is similar for a given file, the number of row is a fairly accurate representation of the work, multiply with a constant does not change too much when we compare. The previous methods of printing the progress also used number of rows(records) in their progress report.", "author": "danielxjd", "createdAt": "2020-07-21T18:17:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxMjYwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTE5MTcxNw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r461191717", "bodyText": "If you use byte sizes for tracking implementation here will end up being trivial.", "author": "chamikaramj", "createdAt": "2020-07-27T21:51:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxMjYwNg=="}], "type": "inlineReview"}, {"oid": "d9f7709307f5456fd286c524ace4688255d19fbe", "url": "https://github.com/apache/beam/commit/d9f7709307f5456fd286c524ace4688255d19fbe", "message": "set coder", "committedDate": "2020-07-21T18:10:40Z", "type": "commit"}, {"oid": "f507a33b506eb7c0aac011b4b021eb22dfffb277", "url": "https://github.com/apache/beam/commit/f507a33b506eb7c0aac011b4b021eb22dfffb277", "message": "change", "committedDate": "2020-07-27T21:06:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTE5MjE1Ng==", "url": "https://github.com/apache/beam/pull/12223#discussion_r461192156", "bodyText": "Does this mean that we will be incrementing progress (obtained by the runner through restriction tracker) only when a new block  is read ? This might be too coarse grained.", "author": "chamikaramj", "createdAt": "2020-07-27T21:52:47Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +277,195 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+                  columnIO.getRecordReader(\n+                          pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+              throw new ParquetDecodingException(format(\"Can not read value at %d in block %d in file %s\", currentRow, currentBlock, file.toString()), e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+\n+\n+\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());", "originalCommit": "f507a33b506eb7c0aac011b4b021eb22dfffb277", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzI1OTg1OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r463259859", "bodyText": "I have change the initial split to be either 64MB or single row group(if the row group size is larger than 64mb).", "author": "danielxjd", "createdAt": "2020-07-30T20:43:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTE5MjE1Ng=="}], "type": "inlineReview"}, {"oid": "7b93829dab22a33226dc313d1231001c9c334416", "url": "https://github.com/apache/beam/commit/7b93829dab22a33226dc313d1231001c9c334416", "message": "Merge branch 'master' into BEAM-4379", "committedDate": "2020-07-27T22:51:55Z", "type": "commit"}, {"oid": "a44061cf10d23309e8bc418147c04e54993ac6e3", "url": "https://github.com/apache/beam/commit/a44061cf10d23309e8bc418147c04e54993ac6e3", "message": "spotless", "committedDate": "2020-07-28T16:42:28Z", "type": "commit"}, {"oid": "03ba0c3e9056eb25d66f61554bc13c7605593737", "url": "https://github.com/apache/beam/commit/03ba0c3e9056eb25d66f61554bc13c7605593737", "message": "Merge branch 'BEAM-4379' of https://github.com/danielxjd/beam into BEAM-4379", "committedDate": "2020-07-28T16:43:09Z", "type": "commit"}, {"oid": "55c17ead8439b058b5418c3332abdc88572db4ab", "url": "https://github.com/apache/beam/commit/55c17ead8439b058b5418c3332abdc88572db4ab", "message": "spotless", "committedDate": "2020-07-28T16:43:39Z", "type": "commit"}, {"oid": "62b6c6362a62ae1d5f54899111761db2b7484268", "url": "https://github.com/apache/beam/commit/62b6c6362a62ae1d5f54899111761db2b7484268", "message": "spotless", "committedDate": "2020-07-28T16:44:24Z", "type": "commit"}, {"oid": "5641c96a41a7de8f6088a9de3b385b51adee66b1", "url": "https://github.com/apache/beam/commit/5641c96a41a7de8f6088a9de3b385b51adee66b1", "message": "spotless", "committedDate": "2020-07-28T16:49:04Z", "type": "commit"}, {"oid": "032715d1f30f2038bcada16c1402f0385d31b6ed", "url": "https://github.com/apache/beam/commit/032715d1f30f2038bcada16c1402f0385d31b6ed", "message": "add block tracker", "committedDate": "2020-07-28T18:08:52Z", "type": "commit"}, {"oid": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "url": "https://github.com/apache/beam/commit/99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "message": "change split function", "committedDate": "2020-07-30T20:41:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY1OTQxNQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464659415", "bodyText": "Why do we need this option ? We should be able to safely always enable splitting for runners that support SDF without an explicit flag from the user.", "author": "chamikaramj", "createdAt": "2020-08-03T21:00:07Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -177,6 +214,10 @@ public Read from(String filepattern) {\n       return from(ValueProvider.StaticValueProvider.of(filepattern));\n     }\n \n+    public Read withSplit() {", "originalCommit": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwMDY3OA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464700678", "bodyText": "I think Splittable dofn is not available for all runners yet so maybe we should still keep both function.", "author": "danielxjd", "createdAt": "2020-08-03T22:45:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY1OTQxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY1OTYzNw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464659637", "bodyText": "We should be able to get rid of this fork by using a single DoFn.", "author": "chamikaramj", "createdAt": "2020-08-03T21:00:32Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {", "originalCommit": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2MDcwOQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464660709", "bodyText": "Why do we need a separate DoFn ? We should be able to use the same DoFn but implement SDF specific methods as well (which will also allow us to better reuse code).", "author": "chamikaramj", "createdAt": "2020-08-03T21:02:55Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {", "originalCommit": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2NDQ1MA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464664450", "bodyText": "How expensive will this be ? Note that input file can be arbitrary long (for example, petabytes) and we don't want the newTracker() call to hang forever.  BTW I think we can safely assume that records are of equal size. So if we can more efficiently determine the number of records per block/row group that should be adequate (for example, we do this for Avro).", "author": "chamikaramj", "createdAt": "2020-08-03T21:11:34Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        long start = restriction.getFrom();\n+        long end = restriction.getFrom();\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        long totalSize = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          totalSize += rowGroups.get((int) i).getTotalByteSize();\n+          end += 1;\n+          if (totalSize > SPLIT_LIMIT) {\n+            start = end;\n+            totalSize = 0;\n+            out.output(new OffsetRange(start, end));\n+          }\n+        }\n+        if (totalSize != 0) {\n+          out.output(new OffsetRange(start, end));\n+        }\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          long recordCount = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {", "originalCommit": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwNTYxNQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464705615", "bodyText": "Row counts are stored in each BlockMetaData so the complexity should be O(n) given that the range has n blocks.", "author": "danielxjd", "createdAt": "2020-08-03T23:01:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2NDQ1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2NTY0Ng==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464665646", "bodyText": "Nice!", "author": "chamikaramj", "createdAt": "2020-08-03T21:14:19Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();", "originalCommit": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2NzMyOQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464667329", "bodyText": "Let's call this approximateRecordSize", "author": "chamikaramj", "createdAt": "2020-08-03T21:17:57Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        long start = restriction.getFrom();\n+        long end = restriction.getFrom();\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        long totalSize = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          totalSize += rowGroups.get((int) i).getTotalByteSize();\n+          end += 1;\n+          if (totalSize > SPLIT_LIMIT) {\n+            start = end;\n+            totalSize = 0;\n+            out.output(new OffsetRange(start, end));\n+          }\n+        }\n+        if (totalSize != 0) {\n+          out.output(new OffsetRange(start, end));\n+        }\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          long recordCount = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+          }\n+          return recordCount;\n+        }\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          double size = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+          }\n+          return size;\n+        }\n+      }\n+    }\n+\n+    private static class BlockTracker extends OffsetRangeTracker {\n+      private long totalWork;\n+      private long progress;\n+      private long recordSize;\n+\n+      public BlockTracker(OffsetRange range, long totalWork, long recordCount) {\n+        super(range);\n+        this.recordSize = totalWork / recordCount;", "originalCommit": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwMTkzMw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464701933", "bodyText": "Sure!", "author": "danielxjd", "createdAt": "2020-08-03T22:49:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2NzMyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2NzcyMg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464667722", "bodyText": "this.totalWork = totalWork", "author": "chamikaramj", "createdAt": "2020-08-03T21:18:56Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        long start = restriction.getFrom();\n+        long end = restriction.getFrom();\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        long totalSize = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          totalSize += rowGroups.get((int) i).getTotalByteSize();\n+          end += 1;\n+          if (totalSize > SPLIT_LIMIT) {\n+            start = end;\n+            totalSize = 0;\n+            out.output(new OffsetRange(start, end));\n+          }\n+        }\n+        if (totalSize != 0) {\n+          out.output(new OffsetRange(start, end));\n+        }\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          long recordCount = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+          }\n+          return recordCount;\n+        }\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          double size = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+          }\n+          return size;\n+        }\n+      }\n+    }\n+\n+    private static class BlockTracker extends OffsetRangeTracker {\n+      private long totalWork;\n+      private long progress;\n+      private long recordSize;\n+\n+      public BlockTracker(OffsetRange range, long totalWork, long recordCount) {\n+        super(range);\n+        this.recordSize = totalWork / recordCount;\n+        this.totalWork = recordSize * recordCount;", "originalCommit": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwMTg2OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464701869", "bodyText": "I am not sure if there will be a round-off for the record size and the totalWork will be different from the product.", "author": "danielxjd", "createdAt": "2020-08-03T22:49:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2NzcyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2ODQ5NA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464668494", "bodyText": "If restriction is null that means we'll be reading the whole file. So what we return here should be the size of the file I believe.", "author": "chamikaramj", "createdAt": "2020-08-03T21:20:44Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        long start = restriction.getFrom();\n+        long end = restriction.getFrom();\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        long totalSize = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          totalSize += rowGroups.get((int) i).getTotalByteSize();\n+          end += 1;\n+          if (totalSize > SPLIT_LIMIT) {\n+            start = end;\n+            totalSize = 0;\n+            out.output(new OffsetRange(start, end));\n+          }\n+        }\n+        if (totalSize != 0) {\n+          out.output(new OffsetRange(start, end));\n+        }\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          long recordCount = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+          }\n+          return recordCount;\n+        }\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;", "originalCommit": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2ODg2MQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464668861", "bodyText": "How expensive is this operation ?", "author": "chamikaramj", "createdAt": "2020-08-03T21:21:39Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        long start = restriction.getFrom();\n+        long end = restriction.getFrom();\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        long totalSize = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          totalSize += rowGroups.get((int) i).getTotalByteSize();\n+          end += 1;\n+          if (totalSize > SPLIT_LIMIT) {\n+            start = end;\n+            totalSize = 0;\n+            out.output(new OffsetRange(start, end));\n+          }\n+        }\n+        if (totalSize != 0) {\n+          out.output(new OffsetRange(start, end));\n+        }\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          long recordCount = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+          }\n+          return recordCount;\n+        }\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          double size = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            size += reader.getRowGroups().get((int) i).getTotalByteSize();", "originalCommit": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwMzk2OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464703969", "bodyText": "It should be a O(N) operation while N is the number of blocks.", "author": "danielxjd", "createdAt": "2020-08-03T22:56:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2ODg2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2OTY1Mw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r464669653", "bodyText": "Please add a comprehensive set of unit tests for ParquetIO and BlockTracker splitting and progress tracking. You can use following for inspiration.\nhttps://github.com/apache/beam/blob/master/sdks/java/core/src/test/java/org/apache/beam/sdk/io/AvroSourceTest.java", "author": "chamikaramj", "createdAt": "2020-08-03T21:23:38Z", "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -84,11 +85,31 @@ public void testWriteAndRead() {\n     PCollection<GenericRecord> readBack =\n         readPipeline.apply(\n             ParquetIO.read(SCHEMA).from(temporaryFolder.getRoot().getAbsolutePath() + \"/*\"));\n-\n     PAssert.that(readBack).containsInAnyOrder(records);\n     readPipeline.run().waitUntilFinish();\n   }\n \n+  @Test\n+  public void testWriteAndReadWithSplit() {\n+    List<GenericRecord> records = generateGenericRecords(1000);", "originalCommit": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b3ec41b3bac0a1b37a537880aabc29faba8d7ccb", "url": "https://github.com/apache/beam/commit/b3ec41b3bac0a1b37a537880aabc29faba8d7ccb", "message": "unit test added", "committedDate": "2020-08-04T19:38:08Z", "type": "commit"}, {"oid": "b48b28cade88ab3e40ff355edb07d85051678dc0", "url": "https://github.com/apache/beam/commit/b48b28cade88ab3e40ff355edb07d85051678dc0", "message": "[Beam-4379] add exception handling", "committedDate": "2020-08-04T22:49:54Z", "type": "commit"}, {"oid": "a42d3ee599da1c0b8ba9e2703a3dc83e4feaedfd", "url": "https://github.com/apache/beam/commit/a42d3ee599da1c0b8ba9e2703a3dc83e4feaedfd", "message": "[Beam-4379]Add unit test", "committedDate": "2020-08-05T21:28:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyMDA0OA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466020048", "bodyText": "Should it be if (getSplit()) ?", "author": "boyuanzz", "createdAt": "2020-08-05T21:40:18Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -187,12 +229,18 @@ public Read withAvroDataModel(GenericData model) {\n     @Override\n     public PCollection<GenericRecord> expand(PBegin input) {\n       checkNotNull(getFilepattern(), \"Filepattern cannot be null.\");\n-\n-      return input\n-          .apply(\"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n-          .apply(FileIO.matchAll())\n-          .apply(FileIO.readMatches())\n-          .apply(readFiles(getSchema()).withAvroDataModel(getAvroDataModel()));\n+      PCollection<FileIO.ReadableFile> inputFiles =\n+          input\n+              .apply(\n+                  \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches());\n+      if (!getSplit()) {", "originalCommit": "b48b28cade88ab3e40ff355edb07d85051678dc0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwMDUxMA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467200510", "bodyText": "changed.", "author": "danielxjd", "createdAt": "2020-08-07T18:23:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyMDA0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyMDQ0Ng==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466020446", "bodyText": "isSplittable?", "author": "boyuanzz", "createdAt": "2020-08-05T21:41:19Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -154,10 +187,15 @@ public static ReadFiles readFiles(Schema schema) {\n \n     abstract @Nullable GenericData getAvroDataModel();\n \n+    abstract boolean getSplit();", "originalCommit": "b48b28cade88ab3e40ff355edb07d85051678dc0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjczOTY5OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466739699", "bodyText": "+1\nalso setSplittable instead of setSplit", "author": "ihji", "createdAt": "2020-08-06T23:31:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyMDQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwMjkwMw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467202903", "bodyText": "Changed.", "author": "danielxjd", "createdAt": "2020-08-07T18:28:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyMDQ0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyMTE2MQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466021161", "bodyText": "We can drop this redundant else.", "author": "boyuanzz", "createdAt": "2020-08-05T21:42:55Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -187,12 +229,18 @@ public Read withAvroDataModel(GenericData model) {\n     @Override\n     public PCollection<GenericRecord> expand(PBegin input) {\n       checkNotNull(getFilepattern(), \"Filepattern cannot be null.\");\n-\n-      return input\n-          .apply(\"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n-          .apply(FileIO.matchAll())\n-          .apply(FileIO.readMatches())\n-          .apply(readFiles(getSchema()).withAvroDataModel(getAvroDataModel()));\n+      PCollection<FileIO.ReadableFile> inputFiles =\n+          input\n+              .apply(\n+                  \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches());\n+      if (!getSplit()) {\n+        return inputFiles.apply(\n+            readFiles(getSchema()).withSplit().withAvroDataModel(getAvroDataModel()));\n+      } else {", "originalCommit": "b48b28cade88ab3e40ff355edb07d85051678dc0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwMDM4OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467200389", "bodyText": "removed.", "author": "danielxjd", "createdAt": "2020-08-07T18:23:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyMTE2MQ=="}], "type": "inlineReview"}, {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "url": "https://github.com/apache/beam/commit/8b8194f52cc0971ba5f2befcf91aba01a5556485", "message": "delete else", "committedDate": "2020-08-06T22:23:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0MTY2OA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466741668", "bodyText": "What's the unit of SPLIT_LIMIT? Please put a comment that how to interpret the number.", "author": "ihji", "createdAt": "2020-08-06T23:38:25Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwMDMwOA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467200308", "bodyText": "Comment added. the unit is bytes.", "author": "danielxjd", "createdAt": "2020-08-07T18:22:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0MTY2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0NTQyMA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466745420", "bodyText": "one liner: Maps.transformValues(map, ImmutableSet::of)", "author": "ihji", "createdAt": "2020-08-06T23:51:47Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwMDE0MA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467200140", "bodyText": "Changed.", "author": "danielxjd", "createdAt": "2020-08-07T18:22:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0NTQyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0NjEwMQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466746101", "bodyText": "one liner: new BeamParquetInputFile(file.openSeekable())\nisReadSeekEfficient check is not necessary.", "author": "ihji", "createdAt": "2020-08-06T23:54:23Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwMDEwMw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467200103", "bodyText": "Changed.", "author": "danielxjd", "createdAt": "2020-08-07T18:22:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0NjEwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1NDAwMA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466754000", "bodyText": "I think this method needs better name that explains what it exactly does e.g. getConfWithDefaultAvroCompatibility", "author": "ihji", "createdAt": "2020-08-07T00:23:16Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwMDA2MQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467200061", "bodyText": "Changed.", "author": "danielxjd", "createdAt": "2020-08-07T18:22:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1NDAwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1NTM2MQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466755361", "bodyText": "Should we create a new instance of Configuration  and ParquetReadOptions multiple times in each method? Is it possible to make options an instance variable?", "author": "ihji", "createdAt": "2020-08-07T00:28:39Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE5NDQ4Ng==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467194486", "bodyText": "I find that options and Configuration does not implements Serializable so they can be passed as instance.", "author": "danielxjd", "createdAt": "2020-08-07T18:10:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1NTM2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1NTg2Mw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466755863", "bodyText": "please avoid reassignment. use conf from above or create a new variable with a meaningful name.", "author": "ihji", "createdAt": "2020-08-07T00:30:30Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE5OTYzMQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467199631", "bodyText": "Changed.", "author": "danielxjd", "createdAt": "2020-08-07T18:21:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1NTg2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1OTM1OA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466759358", "bodyText": "The rhs expression could be embedded since it's only used onetime.", "author": "ihji", "createdAt": "2020-08-07T00:43:58Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE5OTY3OA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467199678", "bodyText": "Changed.", "author": "danielxjd", "createdAt": "2020-08-07T18:21:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1OTM1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1OTQzMA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466759430", "bodyText": "ditto.", "author": "ihji", "createdAt": "2020-08-07T00:44:16Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE5OTc0OA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467199748", "bodyText": "Changed.", "author": "danielxjd", "createdAt": "2020-08-07T18:21:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1OTQzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc3ODA1Mg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466778052", "bodyText": "How many logs will this print? How important is this log? Why not debug?", "author": "ihji", "createdAt": "2020-08-07T01:56:37Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2MzYxMg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467163612", "bodyText": "It will print once for each block. Should be an important info to tell. Maybe I should move it to after finishing the reading   and merge it with the number of record read.", "author": "danielxjd", "createdAt": "2020-08-07T17:07:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc3ODA1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc3ODg5NA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466778894", "bodyText": "Why debug? It seems more important than debug since it's a failure.", "author": "ihji", "createdAt": "2020-08-07T01:59:48Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2ODg0Mg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467168842", "bodyText": "The LOG info and debug for reading record followed the original code.", "author": "danielxjd", "createdAt": "2020-08-07T17:17:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc3ODg5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MTU1MQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r481451551", "bodyText": "I think this should be warn not debug. WDYT @chamikaramj ?\nAlso, it would be great if we print out some additional info from the exception like e.getMessage().", "author": "ihji", "createdAt": "2020-09-01T21:49:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc3ODg5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MDQ5OA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466780498", "bodyText": "This log doesn't deliver enough information for debugging. Probably there will be too many of them also.", "author": "ihji", "createdAt": "2020-08-07T02:06:24Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MzMwMg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r481453302", "bodyText": "ditto.", "author": "ihji", "createdAt": "2020-09-01T21:53:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MDQ5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MTA0MQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466781041", "bodyText": "not so much helpful. better to be something like \"finished reading # rows in row group #\"", "author": "ihji", "createdAt": "2020-08-07T02:08:37Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MzEyNg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r481453126", "bodyText": "We need to print out the block number with the log message at least. Otherwise it would be hard to debug with this string constant message.", "author": "ihji", "createdAt": "2020-09-01T21:53:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MTA0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjQ0MzEwNA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r482443104", "bodyText": "There is a debug on L371 before this line that will print out the row group number and a debug after this L408 to give the information of how many lines are read.", "author": "danielxjd", "createdAt": "2020-09-02T20:46:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MTA0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MTkxNw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466781917", "bodyText": "why 3?", "author": "ihji", "createdAt": "2020-08-07T02:11:59Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE1ODg2OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467158869", "bodyText": "sorry this is a change for testing that was forgot to get removed.", "author": "danielxjd", "createdAt": "2020-08-07T16:57:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MTkxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MjU1Mg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466782552", "bodyText": "Isn't rangeEnd == i+1?", "author": "ihji", "createdAt": "2020-08-07T02:14:11Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE1OTA4MA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467159080", "bodyText": "they should have the same value.", "author": "danielxjd", "createdAt": "2020-08-07T16:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MjU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MTE5OA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r481451198", "bodyText": "How about renaming i to rangeEnd and removing the separate rangeEnd variable?", "author": "ihji", "createdAt": "2020-09-01T21:48:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MjU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjQ1NzQxMQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r482457411", "bodyText": "Changed.", "author": "danielxjd", "createdAt": "2020-09-02T20:56:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MjU1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4NDkyOA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466784928", "bodyText": "These four lines\nInputFile inputFile = getInputFile(file);\nConfiguration conf = setConf();\nParquetReadOptions options = HadoopReadOptions.builder(conf).build();\nParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n\nare repeated in many methods. Could we extract them into a new method?", "author": "ihji", "createdAt": "2020-08-07T02:23:48Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2OTE3NQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467169175", "bodyText": "Yes, I think we should extract them into a new method.", "author": "danielxjd", "createdAt": "2020-08-07T17:18:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4NDkyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4NTczMg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466785732", "bodyText": "for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {", "author": "ihji", "createdAt": "2020-08-07T02:27:03Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4NjA3Nw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466786077", "bodyText": "Is this downcasting safe?", "author": "ihji", "createdAt": "2020-08-07T02:28:19Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2MDM5MQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467160391", "bodyText": "The number of row group should be able to be represented by int safely. Since the range tracker we use extends the offset tracker, they use long to track offset.", "author": "danielxjd", "createdAt": "2020-08-07T17:00:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4NjA3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4NjY2MA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466786660", "bodyText": "We should merge this method with getRecordCount. Don't need to iterate twice.", "author": "ihji", "createdAt": "2020-08-07T02:30:36Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+        }\n+        return recordCount;\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE5OTk0MA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467199940", "bodyText": "merged.", "author": "danielxjd", "createdAt": "2020-08-07T18:22:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4NjY2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4ODAwMQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466788001", "bodyText": "work is too ambiguous. Please rename it to totalBytesSize", "author": "ihji", "createdAt": "2020-08-07T02:36:00Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+        }\n+        return recordCount;\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        double size = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+        }\n+        return size;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private static final Logger LOG = LoggerFactory.getLogger(BlockTracker.class);\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long work, long recordCount) {", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE5OTU2MQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467199561", "bodyText": "Changed.", "author": "danielxjd", "createdAt": "2020-08-07T18:21:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4ODAwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4ODY4OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466788689", "bodyText": "Why not just use work? What's difference between work and totalWork?", "author": "ihji", "createdAt": "2020-08-07T02:38:41Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+        }\n+        return recordCount;\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        double size = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+        }\n+        return size;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private static final Logger LOG = LoggerFactory.getLogger(BlockTracker.class);\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long work, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = work / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2MTAxNQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467161015", "bodyText": "want to make sure that totalwork==approximateRecordSize * recordCount. Since work / recordCount can be rounded to the nearest integer.", "author": "danielxjd", "createdAt": "2020-08-07T17:01:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4ODY4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0OTk2OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r481449969", "bodyText": "Please put a comment then.", "author": "ihji", "createdAt": "2020-09-01T21:45:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4ODY4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjQ1Mjc1OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r482452759", "bodyText": "Comment added", "author": "danielxjd", "createdAt": "2020-09-02T20:53:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4ODY4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5NTM4OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466795389", "bodyText": "Do we need BigDecimal conversion? I think Progress.from(0, this.totalWork) would just work.", "author": "ihji", "createdAt": "2020-08-07T03:06:32Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+        }\n+        return recordCount;\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        double size = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+        }\n+        return size;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private static final Logger LOG = LoggerFactory.getLogger(BlockTracker.class);\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long work, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = work / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;\n+          this.progress = 0;\n+        }\n+      }\n+\n+      public void makeProgress() throws Exception {\n+        progress += approximateRecordSize;\n+        if (progress > totalWork) {\n+          throw new IOException(\"Making progress out of range\");\n+        }\n+      }\n+\n+      @Override\n+      public Progress getProgress() {\n+        if (this.lastAttemptedOffset == null) {\n+          return Progress.from(0.0D, BigDecimal.valueOf(this.totalWork).doubleValue());", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE5OTMzMA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467199330", "bodyText": "Yes, I think so.", "author": "danielxjd", "createdAt": "2020-08-07T18:20:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5NTM4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5NTk3NQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466795975", "bodyText": "How many logs will this print? Why not debug?", "author": "ihji", "createdAt": "2020-08-07T03:08:49Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+        }\n+        return recordCount;\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        double size = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+        }\n+        return size;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private static final Logger LOG = LoggerFactory.getLogger(BlockTracker.class);\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long work, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = work / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;\n+          this.progress = 0;\n+        }\n+      }\n+\n+      public void makeProgress() throws Exception {\n+        progress += approximateRecordSize;\n+        if (progress > totalWork) {\n+          throw new IOException(\"Making progress out of range\");\n+        }\n+      }\n+\n+      @Override\n+      public Progress getProgress() {\n+        if (this.lastAttemptedOffset == null) {\n+          return Progress.from(0.0D, BigDecimal.valueOf(this.totalWork).doubleValue());\n+        } else {\n+          BigDecimal workRemaining =\n+              BigDecimal.valueOf(this.totalWork)\n+                  .subtract(BigDecimal.valueOf(this.progress), MathContext.DECIMAL128)\n+                  .max(BigDecimal.ZERO);\n+          BigDecimal work = BigDecimal.valueOf(this.totalWork);\n+          LOG.info(\"total work: \" + work + \" work remaining: \" + workRemaining);", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE5OTQ4Ng==", "url": "https://github.com/apache/beam/pull/12223#discussion_r467199486", "bodyText": "This is only for test purpose. It is now removed.", "author": "danielxjd", "createdAt": "2020-08-07T18:21:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5NTk3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5Njk2OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r466796969", "bodyText": "I think we could just write Progress.from(this.progress, this.totalWork - this.progress).", "author": "ihji", "createdAt": "2020-08-07T03:13:05Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+        }\n+        return recordCount;\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        double size = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+        }\n+        return size;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private static final Logger LOG = LoggerFactory.getLogger(BlockTracker.class);\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long work, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = work / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;\n+          this.progress = 0;\n+        }\n+      }\n+\n+      public void makeProgress() throws Exception {\n+        progress += approximateRecordSize;\n+        if (progress > totalWork) {\n+          throw new IOException(\"Making progress out of range\");\n+        }\n+      }\n+\n+      @Override\n+      public Progress getProgress() {\n+        if (this.lastAttemptedOffset == null) {\n+          return Progress.from(0.0D, BigDecimal.valueOf(this.totalWork).doubleValue());\n+        } else {\n+          BigDecimal workRemaining =\n+              BigDecimal.valueOf(this.totalWork)\n+                  .subtract(BigDecimal.valueOf(this.progress), MathContext.DECIMAL128)\n+                  .max(BigDecimal.ZERO);\n+          BigDecimal work = BigDecimal.valueOf(this.totalWork);\n+          LOG.info(\"total work: \" + work + \" work remaining: \" + workRemaining);\n+          return Progress.from(\n+              work.subtract(workRemaining, MathContext.DECIMAL128).doubleValue(),", "originalCommit": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6c0c72a299265070079e1417d63d51946c13197c", "url": "https://github.com/apache/beam/commit/6c0c72a299265070079e1417d63d51946c13197c", "message": "extracting methods", "committedDate": "2020-08-07T18:07:21Z", "type": "commit"}, {"oid": "63f8efa249828964ff68009b406b8bb89e8e4fa3", "url": "https://github.com/apache/beam/commit/63f8efa249828964ff68009b406b8bb89e8e4fa3", "message": "change method names", "committedDate": "2020-08-07T18:27:20Z", "type": "commit"}, {"oid": "b554bd1079f7fada6db96b073736438691f183d3", "url": "https://github.com/apache/beam/commit/b554bd1079f7fada6db96b073736438691f183d3", "message": "[BEAM-4379]additional unit test", "committedDate": "2020-08-07T21:54:08Z", "type": "commit"}, {"oid": "a5fcb46d2c14dfd2aea8bfa48b172902916093c8", "url": "https://github.com/apache/beam/commit/a5fcb46d2c14dfd2aea8bfa48b172902916093c8", "message": "[BEAM-4379] update logger", "committedDate": "2020-08-12T22:59:56Z", "type": "commit"}, {"oid": "b16196328f72d3812f0da9cfd6eb3d8711e7fb17", "url": "https://github.com/apache/beam/commit/b16196328f72d3812f0da9cfd6eb3d8711e7fb17", "message": "[BEAM-4379] change info", "committedDate": "2020-08-26T18:16:21Z", "type": "commit"}, {"oid": "efda4310ad3006b37997de227de3d2aaa0ea055a", "url": "https://github.com/apache/beam/commit/efda4310ad3006b37997de227de3d2aaa0ea055a", "message": "[BEAM-4379] change info", "committedDate": "2020-08-26T18:16:59Z", "type": "commit"}, {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd", "url": "https://github.com/apache/beam/commit/1149881eee8d9508f423a8a45306fd826ca8bbbd", "message": "[BEAM-4379] change import", "committedDate": "2020-08-28T18:45:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxNzk2NA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479517964", "bodyText": "Please add javadoc above to explain what withSplit() will do.", "author": "boyuanzz", "createdAt": "2020-08-28T20:23:12Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -133,15 +161,18 @@\n    * pattern).\n    */\n   public static Read read(Schema schema) {\n-    return new AutoValue_ParquetIO_Read.Builder().setSchema(schema).build();\n+    return new AutoValue_ParquetIO_Read.Builder().setSchema(schema).setSplittable(false).build();", "originalCommit": "1149881eee8d9508f423a8a45306fd826ca8bbbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY0ODM0OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r480648349", "bodyText": "It seems like this comment is not addressed yet. Can you add related explanation(what's this option for and what will happens if the option is set to true) and code snippet(how to enable this option) into L89-L155? You can also link your design doc there if that helps end users to use this attribute.", "author": "boyuanzz", "createdAt": "2020-09-01T02:50:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxNzk2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxODM3Nw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479518377", "bodyText": "Do we really need this log?", "author": "boyuanzz", "createdAt": "2020-08-28T20:24:18Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(", "originalCommit": "1149881eee8d9508f423a8a45306fd826ca8bbbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyMzc2NA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479523764", "bodyText": "Same above. I'm thinking about whether we really these INFO log since it looks too much if per element per offset.", "author": "boyuanzz", "createdAt": "2020-08-28T20:38:00Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.info(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());", "originalCommit": "1149881eee8d9508f423a8a45306fd826ca8bbbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyNDY2Mw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479524663", "bodyText": "Same above.", "author": "boyuanzz", "createdAt": "2020-08-28T20:40:32Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.info(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));", "originalCommit": "1149881eee8d9508f423a8a45306fd826ca8bbbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1MTU2NQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479551565", "bodyText": "Changed all the log to debug", "author": "danielxjd", "createdAt": "2020-08-28T21:57:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyNDY2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyNTE5NA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479525194", "bodyText": "Any reason to use SPLIT_LIMIT / 1000?", "author": "boyuanzz", "createdAt": "2020-08-28T20:41:57Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.info(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 1000)) {", "originalCommit": "1149881eee8d9508f423a8a45306fd826ca8bbbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1MTYyMw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479551623", "bodyText": "Sorry, was for the testing purpose", "author": "danielxjd", "createdAt": "2020-08-28T21:57:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyNTE5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMDcxNg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479530716", "bodyText": "Can we have a simple AutoValue class for countAndSize?", "author": "boyuanzz", "createdAt": "2020-08-28T20:56:24Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.info(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 1000)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        List<Double> recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.get(1)),\n+            Math.round(recordCountAndSize.get(0)));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).get(1);\n+      }\n+\n+      public List<Double> getRecordCountAndSize(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        double size = 0;\n+        double recordCount = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          BlockMetaData block = reader.getRowGroups().get((int) i);\n+          recordCount += block.getRowCount();\n+          size += block.getTotalByteSize();\n+        }\n+        List<Double> countAndSize = new ArrayList<>();", "originalCommit": "1149881eee8d9508f423a8a45306fd826ca8bbbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1MTk3MQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479551971", "bodyText": "It's a bit hard to do so since we need to access the footer of the file.", "author": "danielxjd", "createdAt": "2020-08-28T21:59:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMDcxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1NTc4MA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479555780", "bodyText": "Can you explain more about the difficulty on this? My point is we can create a simple object to represent the countAndSize, instead of a List.", "author": "boyuanzz", "createdAt": "2020-08-28T22:12:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMDcxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1NzYwMQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479557601", "bodyText": "Ok, I think I misunderstood it.", "author": "danielxjd", "createdAt": "2020-08-28T22:19:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMDcxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMTU1NQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479531555", "bodyText": "To compare 2 double, you can use assertEquals(double expected, double actual, double epsilon)", "author": "boyuanzz", "createdAt": "2020-08-28T20:58:41Z", "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -69,6 +76,44 @@\n         \"Faraday\", \"Newton\", \"Bohr\", \"Galilei\", \"Maxwell\"\n       };\n \n+  @Test\n+  public void testBlockTracker() throws Exception {\n+    OffsetRange range = new OffsetRange(0, 1);\n+    ParquetIO.ReadFiles.BlockTracker tracker = new ParquetIO.ReadFiles.BlockTracker(range, 7, 3);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkRemaining() - 6) < 0.01);", "originalCommit": "1149881eee8d9508f423a8a45306fd826ca8bbbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1MjIzMQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479552231", "bodyText": "Noted!", "author": "danielxjd", "createdAt": "2020-08-28T22:00:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMTU1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMjEwOQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479532109", "bodyText": "assertEquals?", "author": "boyuanzz", "createdAt": "2020-08-28T21:00:04Z", "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -69,6 +76,44 @@\n         \"Faraday\", \"Newton\", \"Bohr\", \"Galilei\", \"Maxwell\"\n       };\n \n+  @Test\n+  public void testBlockTracker() throws Exception {\n+    OffsetRange range = new OffsetRange(0, 1);\n+    ParquetIO.ReadFiles.BlockTracker tracker = new ParquetIO.ReadFiles.BlockTracker(range, 7, 3);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkRemaining() - 6) < 0.01);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkCompleted()) < 0.01);\n+    tracker.tryClaim((long) 0);\n+    tracker.makeProgress();\n+    assertTrue(Math.abs(tracker.getProgress().getWorkRemaining() - 4) < 0.01);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkCompleted() - 2) < 0.01);\n+    assertThrows(RuntimeException.class, () -> tracker.tryClaim((long) 0));\n+    tracker.makeProgress();\n+    tracker.makeProgress();\n+    assertTrue(Math.abs(tracker.getProgress().getWorkRemaining() - 0) < 0.01);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkCompleted() - 6) < 0.01);\n+    assertThrows(\"Making progress out of range\", IOException.class, () -> tracker.makeProgress());\n+  }\n+\n+  @Test\n+  public void testSplitBlockWithLimit() {\n+    ParquetIO.ReadFiles.SplitReadFn testFn = new ParquetIO.ReadFiles.SplitReadFn(null);\n+    ArrayList<BlockMetaData> blockList = new ArrayList<BlockMetaData>();\n+    ArrayList<OffsetRange> rangeList;\n+    BlockMetaData testBlock = mock(BlockMetaData.class);\n+    when(testBlock.getTotalByteSize()).thenReturn((long) 60);\n+    rangeList = testFn.splitBlockWithLimit(0, blockList.size(), blockList, 200);\n+    assertTrue(rangeList.isEmpty());\n+    for (int i = 0; i < 6; i++) {\n+      blockList.add(testBlock);\n+    }\n+    rangeList = testFn.splitBlockWithLimit(1, blockList.size(), blockList, 200);\n+    assertTrue(rangeList.get(0).getFrom() == (long) 1);", "originalCommit": "1149881eee8d9508f423a8a45306fd826ca8bbbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1MjMxOA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479552318", "bodyText": "Okay!", "author": "danielxjd", "createdAt": "2020-08-28T22:00:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMjEwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMjY0OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479532649", "bodyText": "you can use 0L to represent that 0 is a long.", "author": "boyuanzz", "createdAt": "2020-08-28T21:01:29Z", "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -69,6 +76,44 @@\n         \"Faraday\", \"Newton\", \"Bohr\", \"Galilei\", \"Maxwell\"\n       };\n \n+  @Test\n+  public void testBlockTracker() throws Exception {\n+    OffsetRange range = new OffsetRange(0, 1);\n+    ParquetIO.ReadFiles.BlockTracker tracker = new ParquetIO.ReadFiles.BlockTracker(range, 7, 3);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkRemaining() - 6) < 0.01);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkCompleted()) < 0.01);\n+    tracker.tryClaim((long) 0);", "originalCommit": "1149881eee8d9508f423a8a45306fd826ca8bbbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1MjI3NA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479552274", "bodyText": "Noted!", "author": "danielxjd", "createdAt": "2020-08-28T22:00:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMjY0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0MTU0Mg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479541542", "bodyText": "It seems like in the DoFn body, you will call tracker.makeProgress() for each record. It doesn't work under fnapi context since we only expose limit APIs: \n  \n    \n      beam/sdks/java/fn-execution/src/main/java/org/apache/beam/sdk/fn/splittabledofn/RestrictionTrackers.java\n    \n    \n        Lines 91 to 104\n      in\n      7fb07ff\n    \n    \n    \n    \n\n        \n          \n           private static class RestrictionTrackerObserverWithProgress<RestrictionT, PositionT> \n        \n\n        \n          \n               extends RestrictionTrackerObserver<RestrictionT, PositionT> implements HasProgress { \n        \n\n        \n          \n            \n        \n\n        \n          \n             protected RestrictionTrackerObserverWithProgress( \n        \n\n        \n          \n                 RestrictionTracker<RestrictionT, PositionT> delegate, \n        \n\n        \n          \n                 ClaimObserver<PositionT> claimObserver) { \n        \n\n        \n          \n               super(delegate, claimObserver); \n        \n\n        \n          \n             } \n        \n\n        \n          \n            \n        \n\n        \n          \n             @Override \n        \n\n        \n          \n             public synchronized Progress getProgress() { \n        \n\n        \n          \n               return ((HasProgress) delegate).getProgress(); \n        \n\n        \n          \n             } \n        \n\n        \n          \n           }", "author": "boyuanzz", "createdAt": "2020-08-28T21:25:50Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.info(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 1000)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        List<Double> recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.get(1)),\n+            Math.round(recordCountAndSize.get(0)));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).get(1);\n+      }\n+\n+      public List<Double> getRecordCountAndSize(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        double size = 0;\n+        double recordCount = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          BlockMetaData block = reader.getRowGroups().get((int) i);\n+          recordCount += block.getRowCount();\n+          size += block.getTotalByteSize();\n+        }\n+        List<Double> countAndSize = new ArrayList<>();\n+        countAndSize.add(recordCount);\n+        countAndSize.add(size);\n+        return countAndSize;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long totalByteSize, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = totalByteSize / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;\n+          this.progress = 0;\n+        }\n+      }\n+\n+      public void makeProgress() throws Exception {\n+        progress += approximateRecordSize;", "originalCommit": "1149881eee8d9508f423a8a45306fd826ca8bbbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1MTk5Nw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479551997", "bodyText": "Do you mean that we do not pass in the restriction tracker created by the user to the DoFn but instead pass in a delegate ? If so, I think this is a significant limitation since that will not allow users to adequately customize the way RestrictionTracker is used.", "author": "chamikaramj", "createdAt": "2020-08-28T21:59:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0MTU0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1Mzk5MA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479553990", "bodyText": "Yes, we wrap the user-defined tracker with certain locking guarantee.\ncc @lukecwik for exposing APIs discussion.\nThe purpose of wrapper is to reduce user effort on complex things, like locking. And in most case. we don't really expect the user to alter the tracker by calling non-exposing APIs.", "author": "boyuanzz", "createdAt": "2020-08-28T22:06:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0MTU0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1NDE4Mg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479554182", "bodyText": "For any block based source (Avro, Parquet, etc), I think it's important to report an approximate progress within blocks instead of just reporting the progress of blocks. We do this for the current sources: https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/BlockBasedSource.java#L241\ncc: @lukecwik @robertwb", "author": "chamikaramj", "createdAt": "2020-08-28T22:06:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0MTU0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2ODIwMQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479568201", "bodyText": "@chamikaramj Customizing the restriction tracker allows the user to mutate it's state in many ways and leads to lots of concurrency issues and also correctness guarantees.", "author": "lukecwik", "createdAt": "2020-08-28T23:03:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0MTU0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3MTc0NA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479571744", "bodyText": "I agree that current implementation might be safer but seems like it significantly limits the usability of RestrictionTrackers as well. I think source authors are advanced users. So it might be OK to stay in the side of usability in some cases over safety. But this is debatable.\nAnother option might be to add a method to the RestrictionTracker to mark progress without claiming. We have such a method in the old RangeTracker interface: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/iobase.py#L393", "author": "chamikaramj", "createdAt": "2020-08-28T23:20:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0MTU0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NDU0Ng==", "url": "https://github.com/apache/beam/pull/12223#discussion_r479544546", "bodyText": "Also if makeProgress() is never called because of the guard you have, the progress will always be (0, totalWork), which is not correct to some extent.", "author": "boyuanzz", "createdAt": "2020-08-28T21:34:49Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.info(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 1000)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        List<Double> recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.get(1)),\n+            Math.round(recordCountAndSize.get(0)));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).get(1);\n+      }\n+\n+      public List<Double> getRecordCountAndSize(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        double size = 0;\n+        double recordCount = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          BlockMetaData block = reader.getRowGroups().get((int) i);\n+          recordCount += block.getRowCount();\n+          size += block.getTotalByteSize();\n+        }\n+        List<Double> countAndSize = new ArrayList<>();\n+        countAndSize.add(recordCount);\n+        countAndSize.add(size);\n+        return countAndSize;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long totalByteSize, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = totalByteSize / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;\n+          this.progress = 0;\n+        }\n+      }\n+\n+      public void makeProgress() throws Exception {\n+        progress += approximateRecordSize;\n+        if (progress > totalWork) {\n+          throw new IOException(\"Making progress out of range\");\n+        }\n+      }\n+\n+      @Override\n+      public Progress getProgress() {", "originalCommit": "1149881eee8d9508f423a8a45306fd826ca8bbbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b", "url": "https://github.com/apache/beam/commit/8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b", "message": "[BEAM-4379] Adding TODO", "committedDate": "2020-08-31T21:28:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY0MTAwMA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r480641000", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  // TODO:[BEAM-10842] A more precise progress update\n          \n          \n            \n                  // TODO(BEAM-10842): Refine the BlockTracker to provide better progress.", "author": "boyuanzz", "createdAt": "2020-09-01T02:44:45Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +279,259 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.debug(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.debug(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        CountAndSize recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.getSize()),\n+            Math.round(recordCountAndSize.getCount()));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).getSize();\n+      }\n+\n+      public CountAndSize getRecordCountAndSize(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        double size = 0;\n+        double recordCount = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          BlockMetaData block = reader.getRowGroups().get((int) i);\n+          recordCount += block.getRowCount();\n+          size += block.getTotalByteSize();\n+        }\n+        CountAndSize countAndSize = CountAndSize.create(recordCount, size);\n+        return countAndSize;\n+      }\n+\n+      @AutoValue\n+      abstract static class CountAndSize {\n+        static CountAndSize create(double count, double size) {\n+          return new AutoValue_ParquetIO_ReadFiles_SplitReadFn_CountAndSize(count, size);\n+        }\n+\n+        abstract double getCount();\n+\n+        abstract double getSize();\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long totalByteSize, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = totalByteSize / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;\n+          this.progress = 0;\n+        }\n+      }\n+\n+      public void makeProgress() throws Exception {\n+        progress += approximateRecordSize;\n+        if (progress > totalWork) {\n+          throw new IOException(\"Making progress out of range\");\n+        }\n+      }\n+\n+      @Override\n+      // TODO:[BEAM-10842] A more precise progress update", "originalCommit": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1NDEzMg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r480654132", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    while ((tracker).tryClaim(currentBlock)) {\n          \n          \n            \n                    while (tracker.tryClaim(currentBlock)) {", "author": "boyuanzz", "createdAt": "2020-09-01T02:54:24Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +279,259 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {", "originalCommit": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2MzM1Nw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r480663357", "bodyText": "IIUC, it seems like there is no need to track currentRow, we can do something like\nrecord = recordReader.read();\nwhile(record != null) {\n  // do something\n  record = recordReader.read();\n}\nIf that's true, we can have a follow-up PR to clean up.", "author": "boyuanzz", "createdAt": "2020-09-01T03:01:17Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +279,259 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.debug(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {", "originalCommit": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMzNDAzNQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r481334035", "bodyText": "I have tried that once and it might have some issue when reading out of the range, the original ParquetReader also tracked the row number when reading.", "author": "danielxjd", "createdAt": "2020-09-01T18:05:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2MzM1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTY0OTE5OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r481649199", "bodyText": "I see. Thanks for the explanation!", "author": "boyuanzz", "createdAt": "2020-09-02T04:13:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2MzM1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2NzE5NQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r480667195", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n          \n          \n            \n                      FileIO.ReadableFile file,  OffsetRange restriction)\n          \n      \n    \n    \n  \n\nThe annotations like @Element and @Restriction are only for common APIs of a DoFn.", "author": "boyuanzz", "createdAt": "2020-09-01T03:04:04Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +279,259 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.debug(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.debug(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        CountAndSize recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.getSize()),\n+            Math.round(recordCountAndSize.getCount()));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).getSize();\n+      }\n+\n+      public CountAndSize getRecordCountAndSize(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)", "originalCommit": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2Nzk3OA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r480667978", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  public CountAndSize getRecordCountAndSize(\n          \n          \n            \n                 private CountAndSize getRecordCountAndSize(\n          \n      \n    \n    \n  \n\ngetRecordCountAndSize is only for this DoFn, right?", "author": "boyuanzz", "createdAt": "2020-09-01T03:04:40Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +279,259 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.debug(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.debug(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        CountAndSize recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.getSize()),\n+            Math.round(recordCountAndSize.getCount()));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).getSize();\n+      }\n+\n+      public CountAndSize getRecordCountAndSize(", "originalCommit": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDc2Mzg0NA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r480763844", "bodyText": "Yes, private should be used here.", "author": "danielxjd", "createdAt": "2020-09-01T04:15:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2Nzk3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3MzQ2Mg==", "url": "https://github.com/apache/beam/pull/12223#discussion_r480673462", "bodyText": "Any reason to use double for count and size? If that's not preferable, we can have a follow-up PR to clean up.", "author": "boyuanzz", "createdAt": "2020-09-01T03:08:37Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +279,259 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.debug(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.debug(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        CountAndSize recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.getSize()),\n+            Math.round(recordCountAndSize.getCount()));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).getSize();\n+      }\n+\n+      public CountAndSize getRecordCountAndSize(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        double size = 0;\n+        double recordCount = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          BlockMetaData block = reader.getRowGroups().get((int) i);\n+          recordCount += block.getRowCount();\n+          size += block.getTotalByteSize();\n+        }\n+        CountAndSize countAndSize = CountAndSize.create(recordCount, size);\n+        return countAndSize;\n+      }\n+\n+      @AutoValue\n+      abstract static class CountAndSize {", "originalCommit": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTMzMzI1Mw==", "url": "https://github.com/apache/beam/pull/12223#discussion_r481333253", "bodyText": "Because the return type for getSize is double.", "author": "danielxjd", "createdAt": "2020-09-01T18:04:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3MzQ2Mg=="}], "type": "inlineReview"}, {"oid": "7a0b0c5d8c34b297ceda64f29bc7e60ca8c50152", "url": "https://github.com/apache/beam/commit/7a0b0c5d8c34b297ceda64f29bc7e60ca8c50152", "message": "Update sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\nCo-authored-by: Boyuan Zhang <36090911+boyuanzz@users.noreply.github.com>", "committedDate": "2020-09-01T04:12:41Z", "type": "commit"}, {"oid": "bc831a7095f4711f7ffbdcfc9915f4f687f60039", "url": "https://github.com/apache/beam/commit/bc831a7095f4711f7ffbdcfc9915f4f687f60039", "message": "Update sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\nCo-authored-by: Boyuan Zhang <36090911+boyuanzz@users.noreply.github.com>", "committedDate": "2020-09-01T04:13:05Z", "type": "commit"}, {"oid": "aea4439254fd3e86e45aad6647375cd33d526fca", "url": "https://github.com/apache/beam/commit/aea4439254fd3e86e45aad6647375cd33d526fca", "message": "Update sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\nCo-authored-by: Boyuan Zhang <36090911+boyuanzz@users.noreply.github.com>", "committedDate": "2020-09-01T04:14:28Z", "type": "commit"}, {"oid": "11cc8819a9b21e0cfc25a5f207e50dc0b2e9b746", "url": "https://github.com/apache/beam/commit/11cc8819a9b21e0cfc25a5f207e50dc0b2e9b746", "message": "[BEAM-4379] adding javadoc", "committedDate": "2020-09-01T18:10:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1OTA1NA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r481459054", "bodyText": "looks like these two lines could be also replaced with getParquetFileReader().", "author": "ihji", "createdAt": "2020-09-01T22:07:24Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +290,258 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =", "originalCommit": "11cc8819a9b21e0cfc25a5f207e50dc0b2e9b746", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ2NTc0NA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r481465744", "bodyText": "The debug was from the Hadoop Parquet reader also. So in Hadoop the message is categorized in debug. So maybe they will allow the corrupted record to be skipped.", "author": "danielxjd", "createdAt": "2020-09-01T22:24:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1OTA1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ2NjIwOA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r481466208", "bodyText": "The options need to be used later so we might as well leave these two lines.", "author": "danielxjd", "createdAt": "2020-09-01T22:25:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1OTA1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ2NzQ4MA==", "url": "https://github.com/apache/beam/pull/12223#discussion_r481467480", "bodyText": "The options need to be used later so we might as well leave these two lines.\n\nMake sense.", "author": "ihji", "createdAt": "2020-09-01T22:29:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1OTA1NA=="}], "type": "inlineReview"}, {"oid": "a05bb16fe43f752588e2459bef947e0db89c77d0", "url": "https://github.com/apache/beam/commit/a05bb16fe43f752588e2459bef947e0db89c77d0", "message": "[BEAM-4379] Add Debug Info", "committedDate": "2020-09-02T21:08:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjUxODQ0OQ==", "url": "https://github.com/apache/beam/pull/12223#discussion_r482518449", "bodyText": "We could remove rangeEnd here (and change rangeEnd at L478 to end.", "author": "ihji", "createdAt": "2020-09-02T22:02:41Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +290,273 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while (tracker.tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.debug(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\n+                    \"skipping a corrupt record at {} in block {} in file {}\",\n+                    currentRow,\n+                    currentBlock,\n+                    file.toString());\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\n+                    \"filtered record reader reached end of block in block {} in file {}\",\n+                    currentBlock,\n+                    file.toString());\n+                break;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\n+                    \"skipping record at {} in block {} in file {}\",\n+                    currentRow,\n+                    currentBlock,\n+                    file.toString());\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.debug(\n+              \"Finish processing {} rows from block {} in file {}\",\n+              currentRow,\n+              currentBlock - 1,\n+              file.toString());\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;", "originalCommit": "a05bb16fe43f752588e2459bef947e0db89c77d0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "974ef8e8b817e0b3b03d75ca7504725d044174f3", "url": "https://github.com/apache/beam/commit/974ef8e8b817e0b3b03d75ca7504725d044174f3", "message": "[BEAM-4379] change debug to warn", "committedDate": "2020-09-02T22:14:38Z", "type": "commit"}, {"oid": "abbfd626424f6d2e3168d67ad289f1ec0ec1323d", "url": "https://github.com/apache/beam/commit/abbfd626424f6d2e3168d67ad289f1ec0ec1323d", "message": "[BEAM-4379] remove rangeend", "committedDate": "2020-09-02T22:24:59Z", "type": "commit"}]}