{"pr_number": 13112, "pr_title": "[BEAM-11065] Apache Beam pipeline example to ingest from Apache Kafka to Google Pub/Sub ", "pr_createdAt": "2020-10-14T15:25:05Z", "pr_url": "https://github.com/apache/beam/pull/13112", "timeline": [{"oid": "45ef0091df248595f6da944c399857ed71c27a56", "url": "https://github.com/apache/beam/commit/45ef0091df248595f6da944c399857ed71c27a56", "message": "add initial template and dependencies", "committedDate": "2020-09-25T12:52:49Z", "type": "commit"}, {"oid": "321c7158f61765c8d515f20e607dbb7ef07473a4", "url": "https://github.com/apache/beam/commit/321c7158f61765c8d515f20e607dbb7ef07473a4", "message": "Added flex template creation with metadata support and instructions", "committedDate": "2020-10-02T14:21:36Z", "type": "commit"}, {"oid": "e88fd31914a79223164181f151ea7ec14965ec8f", "url": "https://github.com/apache/beam/commit/e88fd31914a79223164181f151ea7ec14965ec8f", "message": "added new gradle modules for templates", "committedDate": "2020-10-02T14:40:42Z", "type": "commit"}, {"oid": "ed9be584996f3d210e55c72d7c2b7ea737c01a2e", "url": "https://github.com/apache/beam/commit/ed9be584996f3d210e55c72d7c2b7ea737c01a2e", "message": "moved metadata to template location, reverted examples build.gradle", "committedDate": "2020-10-02T14:40:42Z", "type": "commit"}, {"oid": "e2a8317b55d3969ead0b58fd8d11f68bec1db40e", "url": "https://github.com/apache/beam/commit/e2a8317b55d3969ead0b58fd8d11f68bec1db40e", "message": "Moved KafkaToPubsub to template, implemented options in separate package", "committedDate": "2020-10-02T14:40:42Z", "type": "commit"}, {"oid": "4ab8e58a8b96ac3d5ce4ee8189b2268a103829ce", "url": "https://github.com/apache/beam/commit/4ab8e58a8b96ac3d5ce4ee8189b2268a103829ce", "message": "Added package-info.java to new packages", "committedDate": "2020-10-02T14:40:42Z", "type": "commit"}, {"oid": "aeb64114430f20a22abd671a698400d1b012f6e9", "url": "https://github.com/apache/beam/commit/aeb64114430f20a22abd671a698400d1b012f6e9", "message": "Reverted build.gradle to master branch state", "committedDate": "2020-10-02T15:02:53Z", "type": "commit"}, {"oid": "61bf9921454afa9923c438d68437eeb1af6949e9", "url": "https://github.com/apache/beam/commit/61bf9921454afa9923c438d68437eeb1af6949e9", "message": "fixed JAVADOC and metadata", "committedDate": "2020-10-02T15:11:46Z", "type": "commit"}, {"oid": "015e6e6913656d3f95a423bdee49d3731dc40826", "url": "https://github.com/apache/beam/commit/015e6e6913656d3f95a423bdee49d3731dc40826", "message": "Added the Read Me section with a step-by-step guide", "committedDate": "2020-10-05T13:55:53Z", "type": "commit"}, {"oid": "129f4666adf36c453ff56f69b60ae8f13456b38e", "url": "https://github.com/apache/beam/commit/129f4666adf36c453ff56f69b60ae8f13456b38e", "message": "Update README.md", "committedDate": "2020-10-05T16:52:02Z", "type": "commit"}, {"oid": "6db17adc37596b202332e38ddc2f8b61df482335", "url": "https://github.com/apache/beam/commit/6db17adc37596b202332e38ddc2f8b61df482335", "message": "Readme fixes regarding comments", "committedDate": "2020-10-07T08:32:33Z", "type": "commit"}, {"oid": "01e4987c5957272409dc4f4f86f359b3aac77aa5", "url": "https://github.com/apache/beam/commit/01e4987c5957272409dc4f4f86f359b3aac77aa5", "message": "Update README.md", "committedDate": "2020-10-07T13:56:24Z", "type": "commit"}, {"oid": "b6decbd1d617770f5e74e048306d6eb94c01ebd5", "url": "https://github.com/apache/beam/commit/b6decbd1d617770f5e74e048306d6eb94c01ebd5", "message": "Update README.md", "committedDate": "2020-10-07T14:13:02Z", "type": "commit"}, {"oid": "8e41d136f1cc94eb2442608fd7117b8d7972fd2c", "url": "https://github.com/apache/beam/commit/8e41d136f1cc94eb2442608fd7117b8d7972fd2c", "message": "Fixed typos in README.md", "committedDate": "2020-10-07T14:24:01Z", "type": "commit"}, {"oid": "369a2058ac462da9bb2658bc756a85b66b9d1f28", "url": "https://github.com/apache/beam/commit/369a2058ac462da9bb2658bc756a85b66b9d1f28", "message": "refactored README.md added case to run template locally", "committedDate": "2020-10-07T15:32:31Z", "type": "commit"}, {"oid": "01db016e4bafb1aa1e525b02e44e2923d82488f6", "url": "https://github.com/apache/beam/commit/01db016e4bafb1aa1e525b02e44e2923d82488f6", "message": "Update README.md", "committedDate": "2020-10-07T15:56:03Z", "type": "commit"}, {"oid": "5821bdff66137df7a9978419d384e3b18079b6a6", "url": "https://github.com/apache/beam/commit/5821bdff66137df7a9978419d384e3b18079b6a6", "message": "fix build script for dataflow in README.md", "committedDate": "2020-10-08T12:46:52Z", "type": "commit"}, {"oid": "98de91f880cbf01585a24de12c6c459af1ee7380", "url": "https://github.com/apache/beam/commit/98de91f880cbf01585a24de12c6c459af1ee7380", "message": "Added unit test and fixed metadata file", "committedDate": "2020-10-09T08:19:40Z", "type": "commit"}, {"oid": "fc1ed9adb283ad69441c74cb1858dcc7e81a5668", "url": "https://github.com/apache/beam/commit/fc1ed9adb283ad69441c74cb1858dcc7e81a5668", "message": "Added Licenses and style fixes", "committedDate": "2020-10-09T09:21:25Z", "type": "commit"}, {"oid": "6e159c7551d135223ef2653992936e5515b88d66", "url": "https://github.com/apache/beam/commit/6e159c7551d135223ef2653992936e5515b88d66", "message": "Added support for retrieving Kafka credentials from HashiCorp Vault secret storage with url and token", "committedDate": "2020-10-12T15:29:01Z", "type": "commit"}, {"oid": "523f796b5bd42eca177c3e4a170cb6c5b26bf2e2", "url": "https://github.com/apache/beam/commit/523f796b5bd42eca177c3e4a170cb6c5b26bf2e2", "message": "Updated README.md and metadata with parameters for Vault access; refactored Kafka configuration", "committedDate": "2020-10-13T14:04:52Z", "type": "commit"}, {"oid": "e78c02cf851ee4fa634eee4b838b6752e6645d20", "url": "https://github.com/apache/beam/commit/e78c02cf851ee4fa634eee4b838b6752e6645d20", "message": "Style fix", "committedDate": "2020-10-13T14:09:26Z", "type": "commit"}, {"oid": "1055ec2c6e12f11fae67c6885721ba3f5e86d58f", "url": "https://github.com/apache/beam/commit/1055ec2c6e12f11fae67c6885721ba3f5e86d58f", "message": "Added description for Vault parameters in metadata", "committedDate": "2020-10-14T14:54:35Z", "type": "commit"}, {"oid": "77defc11c7707acd417b45a0779be634dcf8953f", "url": "https://github.com/apache/beam/commit/77defc11c7707acd417b45a0779be634dcf8953f", "message": "FIX trailing whitespaces in README.md", "committedDate": "2020-10-15T14:49:40Z", "type": "commit"}, {"oid": "d6ab0f6b13652260cb50b159575c253a7036b97f", "url": "https://github.com/apache/beam/commit/d6ab0f6b13652260cb50b159575c253a7036b97f", "message": "FIX. Blank line contains whitespace README.md", "committedDate": "2020-10-15T14:54:03Z", "type": "commit"}, {"oid": "8881ff34a381cb3985aa0e2621c7a54fda8396e8", "url": "https://github.com/apache/beam/commit/8881ff34a381cb3985aa0e2621c7a54fda8396e8", "message": "Update README.md", "committedDate": "2020-10-15T17:43:09Z", "type": "commit"}, {"oid": "a821560f90e6a02928f30d132251dec9a22baa21", "url": "https://github.com/apache/beam/commit/a821560f90e6a02928f30d132251dec9a22baa21", "message": "Refactored to examples folder", "committedDate": "2020-10-16T12:16:21Z", "type": "commit"}, {"oid": "c22f110872aa19ba60e8b593556acabcd08381eb", "url": "https://github.com/apache/beam/commit/c22f110872aa19ba60e8b593556acabcd08381eb", "message": "Added conversion from JSON into PubsubMessage and extracted all transformations from the pipeline class into the separate class", "committedDate": "2020-10-16T15:58:14Z", "type": "commit"}, {"oid": "b56ec7bd557ad965b7ce8c0fdcaa5331a8858245", "url": "https://github.com/apache/beam/commit/b56ec7bd557ad965b7ce8c0fdcaa5331a8858245", "message": "Whitespacelint fix", "committedDate": "2020-10-16T16:37:37Z", "type": "commit"}, {"oid": "71308e573cd3f9b94913534020ddd76d67eba748", "url": "https://github.com/apache/beam/commit/71308e573cd3f9b94913534020ddd76d67eba748", "message": "Updated README.md and output formats", "committedDate": "2020-10-21T15:09:13Z", "type": "commit"}, {"oid": "3c218dfc039276c684fe2218bf3a326b76bcd791", "url": "https://github.com/apache/beam/commit/3c218dfc039276c684fe2218bf3a326b76bcd791", "message": "Update README.md", "committedDate": "2020-10-22T07:11:43Z", "type": "commit"}, {"oid": "8bdfc3413ae39c7f3451a675b15ae41a56d4689a", "url": "https://github.com/apache/beam/commit/8bdfc3413ae39c7f3451a675b15ae41a56d4689a", "message": "Update README.md", "committedDate": "2020-10-22T14:49:44Z", "type": "commit"}, {"oid": "72c38e08a85445f627696c14e775010fe70d2d71", "url": "https://github.com/apache/beam/commit/72c38e08a85445f627696c14e775010fe70d2d71", "message": "Merge pull request #2 from akvelon/Readme\n\nUpdate README.md", "committedDate": "2020-10-22T14:50:18Z", "type": "commit"}, {"oid": "1d94fcdd79cd485b1d8193c443f72dcd88da6df6", "url": "https://github.com/apache/beam/commit/1d94fcdd79cd485b1d8193c443f72dcd88da6df6", "message": "Added support for SSL and removed outputFormat option", "committedDate": "2020-10-22T16:13:13Z", "type": "commit"}, {"oid": "a194d54ffc9a54d7a76607203bbb1a3a5dd963f4", "url": "https://github.com/apache/beam/commit/a194d54ffc9a54d7a76607203bbb1a3a5dd963f4", "message": "Added avro usage example", "committedDate": "2020-10-22T20:31:05Z", "type": "commit"}, {"oid": "12c553fbb925bcc3c6240bfc89feb6f0fb9ae03a", "url": "https://github.com/apache/beam/commit/12c553fbb925bcc3c6240bfc89feb6f0fb9ae03a", "message": "Merge branch 'KafkaToPubsubTemplate' of github.com:akvelon/beam into KafkaToPubsubTemplate\n\n\u0001 Conflicts:\n\u0001\texamples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/KafkaToPubsub.java\n\u0001\texamples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/transforms/FormatTransform.java", "committedDate": "2020-10-22T20:41:11Z", "type": "commit"}, {"oid": "f754aa81f0a307ab78c73266d576256da0ac8ecc", "url": "https://github.com/apache/beam/commit/f754aa81f0a307ab78c73266d576256da0ac8ecc", "message": "Added ssl to AVRO reader", "committedDate": "2020-10-22T20:45:56Z", "type": "commit"}, {"oid": "695467aea548fa0a4c57cc0cac8db757d886a680", "url": "https://github.com/apache/beam/commit/695467aea548fa0a4c57cc0cac8db757d886a680", "message": "FIX whitespaces.", "committedDate": "2020-10-22T20:51:57Z", "type": "commit"}, {"oid": "0212b90f22159e083f67fe595284fa4b76ce43f1", "url": "https://github.com/apache/beam/commit/0212b90f22159e083f67fe595284fa4b76ce43f1", "message": "added readme/docs regarding of Avro", "committedDate": "2020-10-23T14:52:51Z", "type": "commit"}, {"oid": "0d0c8249e1143b5198509539b3cb313cea103c7a", "url": "https://github.com/apache/beam/commit/0d0c8249e1143b5198509539b3cb313cea103c7a", "message": "README.md and javadoc fixes", "committedDate": "2020-10-26T09:56:25Z", "type": "commit"}, {"oid": "a4fea29c76c33321271f21b699e379e3b66a2edc", "url": "https://github.com/apache/beam/commit/a4fea29c76c33321271f21b699e379e3b66a2edc", "message": "Added Vault's response JSON schema description", "committedDate": "2020-10-26T14:52:07Z", "type": "commit"}, {"oid": "17957b8a9f5468883de469dd9999fbb051301beb", "url": "https://github.com/apache/beam/commit/17957b8a9f5468883de469dd9999fbb051301beb", "message": "Style fix", "committedDate": "2020-10-28T13:10:59Z", "type": "commit"}, {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245", "url": "https://github.com/apache/beam/commit/a22c6966012dfbfed51aa0e23c8e8ca2e74b2245", "message": "Merge branch 'master' into KafkaToPubsubTemplate", "committedDate": "2020-10-28T15:20:38Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkyODU2MQ==", "url": "https://github.com/apache/beam/pull/13112#discussion_r523928561", "bodyText": "Can we write a positive test as well ?", "author": "manavgarg", "createdAt": "2020-11-16T06:55:16Z", "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Test of KafkaToPubsub. */\n+@RunWith(JUnit4.class)\n+public class KafkaToPubsubTest {\n+\n+  @Rule public final transient TestPipeline pipeline = TestPipeline.create();\n+\n+  @Rule public ExpectedException thrown = ExpectedException.none();\n+\n+  @Test\n+  public void testKafkaReadingFailsWrongBootstrapServer() {", "originalCommit": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODcxNzI3NQ==", "url": "https://github.com/apache/beam/pull/13112#discussion_r528717275", "bodyText": "Yes, we've already implemented 5 tests with positive and negative cases.", "author": "KhaninArtur", "createdAt": "2020-11-23T13:52:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkyODU2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkyODA5OA==", "url": "https://github.com/apache/beam/pull/13112#discussion_r528928098", "bodyText": "Thanks.", "author": "manavgarg", "createdAt": "2020-11-23T18:58:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkyODU2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkzMjg1Nw==", "url": "https://github.com/apache/beam/pull/13112#discussion_r523932857", "bodyText": "Remove default value.", "author": "manavgarg", "createdAt": "2020-11-16T07:08:28Z", "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/options/KafkaToPubsubOptions.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates.options;\n+\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.Validation;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+\n+public interface KafkaToPubsubOptions extends PipelineOptions {\n+  @Description(\"Kafka Bootstrap Servers\")\n+  @Validation.Required\n+  String getBootstrapServers();\n+\n+  void setBootstrapServers(String value);\n+\n+  @Description(\"Kafka topics to read the input from\")\n+  @Validation.Required\n+  String getInputTopics();\n+\n+  void setInputTopics(String value);\n+\n+  @Description(\n+      \"The Cloud Pub/Sub topic to publish to. \"\n+          + \"The name should be in the format of \"\n+          + \"projects/<project-id>/topics/<topic-name>.\")\n+  @Validation.Required\n+  String getOutputTopic();\n+\n+  void setOutputTopic(String outputTopic);\n+\n+  @Description(\"\")\n+  @Validation.Required\n+  @Default.Enum(\"PUBSUB\")", "originalCommit": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMzYwOQ==", "url": "https://github.com/apache/beam/pull/13112#discussion_r525623609", "bodyText": "This looks like a dupe of the README, could the javadoc just refer to that instead?", "author": "TheNeuralBit", "createdAt": "2020-11-18T00:53:04Z", "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.gson.JsonObject;\n+import com.google.gson.JsonParser;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.templates.avro.TaxiRide;\n+import org.apache.beam.templates.options.KafkaToPubsubOptions;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.client.HttpClient;\n+import org.apache.http.client.methods.HttpGet;\n+import org.apache.http.impl.client.HttpClientBuilder;\n+import org.apache.http.util.EntityUtils;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.common.security.scram.ScramMechanism;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+ *       --image-gcr-path \"${TARGET_GCR_IMAGE}\" \\\n+ *       --sdk-language \"JAVA\" \\\n+ *       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+ *       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+ *       --jar \"build/libs/beam-templates-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n+ *       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+ *\n+ * # Execute template:\n+ *    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+ *    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+ *    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+ *\n+ *    time curl -X POST -H \"Content-Type: application/json\" \\\n+ *            -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+ *            -d '\n+ *             {\n+ *                 \"launch_parameter\": {\n+ *                     \"jobName\": \"'$JOB_NAME'\",\n+ *                     \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+ *                     \"parameters\": {\n+ *                         \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+ *                         \"inputTopics\": \"topic1, topic2\",\n+ *                         \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+ *                         \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+ *                         \"vaultToken\": \"your-token\"\n+ *                     }\n+ *                 }\n+ *             }\n+ *            '\n+ *            \"${TEMPLATES_LAUNCH_API}\"\n+ * </pre>\n+ *\n+ * <p><b>Example Avro usage</b>\n+ *\n+ * <pre>\n+ * This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.\n+ *\n+ * To use this example in the specific case, follow the few steps:\n+ * <ul>\n+ * <li> Create your own class to describe AVRO schema. As an example use {@link TaxiRide}. Just define necessary fields.\n+ * <li> Create your own Avro Deserializer class. As an example use {@link org.apache.beam.templates.avro.TaxiRidesKafkaAvroDeserializer}. Just rename it, and put your own Schema class as the necessary types.\n+ * <li> Modify the {@link FormatTransform}. Put your Schema class and Deserializer to the related parameter.\n+ * <li> Modify write step in the {@link KafkaToPubsub} by put your Schema class to \"writeAvrosToPubSub\" step.\n+ * </ul>", "originalCommit": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODcyMjc3Ng==", "url": "https://github.com/apache/beam/pull/13112#discussion_r528722776", "bodyText": "I think README and javadoc have a bit different use-cases. When we work in IDE or text editors javadoc is a good place to look at class or module documentation, but when we work with GitHub, it provides us awesome approaches to render MD files and we can just open the template folder and look at the rendered readme.", "author": "ilya-kozyrev", "createdAt": "2020-11-23T14:01:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMzYwOQ=="}], "type": "inlineReview"}, {"oid": "b4e7081334f1f21e7e28e522c884099ecc34605e", "url": "https://github.com/apache/beam/commit/b4e7081334f1f21e7e28e522c884099ecc34605e", "message": "Refactoring.", "committedDate": "2020-11-19T16:10:01Z", "type": "commit"}, {"oid": "aa857d7b314ade00e5e387cbf7cc8dca7b8ef649", "url": "https://github.com/apache/beam/commit/aa857d7b314ade00e5e387cbf7cc8dca7b8ef649", "message": "Fixed ssl parameters", "committedDate": "2020-11-19T16:53:58Z", "type": "commit"}, {"oid": "25efe95508dedcacd9374fffd67e9f0d40e7b51e", "url": "https://github.com/apache/beam/commit/25efe95508dedcacd9374fffd67e9f0d40e7b51e", "message": "Fixed style", "committedDate": "2020-11-19T16:57:19Z", "type": "commit"}, {"oid": "7fd7ea62ef7cc7fb788685d596a9866a0eda1da2", "url": "https://github.com/apache/beam/commit/7fd7ea62ef7cc7fb788685d596a9866a0eda1da2", "message": "Merge branch 'master' into KafkaToPubsubTemplate", "committedDate": "2020-11-20T15:07:22Z", "type": "commit"}, {"oid": "f300de51ca3b07d575002b3e9a14970911aa5ce0", "url": "https://github.com/apache/beam/commit/f300de51ca3b07d575002b3e9a14970911aa5ce0", "message": "optimize build.gradle", "committedDate": "2020-11-20T15:12:39Z", "type": "commit"}, {"oid": "c0e6ad0d5943795839635b977cf1a875680ef2d9", "url": "https://github.com/apache/beam/commit/c0e6ad0d5943795839635b977cf1a875680ef2d9", "message": "Resolve conversations", "committedDate": "2020-11-20T15:31:43Z", "type": "commit"}, {"oid": "998f4c0f5d1631e157cb61fca02908e7bdad2a33", "url": "https://github.com/apache/beam/commit/998f4c0f5d1631e157cb61fca02908e7bdad2a33", "message": "Updated regarding comments and added unit tests", "committedDate": "2020-11-23T10:07:24Z", "type": "commit"}, {"oid": "6a6aa46a96838ad3a989fee61adba8902f3023d3", "url": "https://github.com/apache/beam/commit/6a6aa46a96838ad3a989fee61adba8902f3023d3", "message": "README.md update", "committedDate": "2020-11-23T11:53:41Z", "type": "commit"}, {"oid": "0858e4717809d44ddb5ca4fa1924fa55b9f85ce2", "url": "https://github.com/apache/beam/commit/0858e4717809d44ddb5ca4fa1924fa55b9f85ce2", "message": "made Avro class more abstract", "committedDate": "2020-11-23T13:37:37Z", "type": "commit"}, {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c", "url": "https://github.com/apache/beam/commit/8a3d85d19e45613b28ad5bc0042921a0f961598c", "message": "fix style", "committedDate": "2020-11-23T13:54:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMzAyMw==", "url": "https://github.com/apache/beam/pull/13112#discussion_r528933023", "bodyText": "Maybe I am missing something here but these tests only validate the config.\nIs it possible to have a more e2e test where we create & execute a pipeline like what we had before ?", "author": "manavgarg", "createdAt": "2020-11-23T19:07:06Z", "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.templates.KafkaPubsubConstants.PASSWORD;\n+import static org.apache.beam.templates.KafkaPubsubConstants.USERNAME;\n+import static org.apache.beam.templates.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.templates.kafka.consumer.Utils;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.security.scram.ScramMechanism;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Test of KafkaToPubsub. */\n+@RunWith(JUnit4.class)\n+public class KafkaToPubsubTest {", "originalCommit": "8a3d85d19e45613b28ad5bc0042921a0f961598c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMyOTE5Nw==", "url": "https://github.com/apache/beam/pull/13112#discussion_r533329197", "bodyText": "We've made an investigation towards e2e tests and for such tests we need Kafka and Pub/Sub being set up in some environment or in a GCP account with billing. That is why seems like there is no way to do it properly.", "author": "KhaninArtur", "createdAt": "2020-12-01T11:13:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMzAyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU1OTM3NQ==", "url": "https://github.com/apache/beam/pull/13112#discussion_r533559375", "bodyText": "You might take a look at how our tests for PubsubIO and KafkaIO work as they have to contend with this same problem.\nFor KafkaIO, we use testcontainers to stand up a fake Kafka service to test against.\nFor E2E tests of PubsubIO and other GCP-specific features we have the apache-beam-testing project. We could probably give you access to this. Alternatively, there is now a PubSub emulator that you could use to stand up a fake PubSub service, just like the Kafka testcontainer. I don't think there are any examples using it in Beam yet though.", "author": "TheNeuralBit", "createdAt": "2020-12-01T16:42:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMzAyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwMjY5MA==", "url": "https://github.com/apache/beam/pull/13112#discussion_r537802690", "bodyText": "Thank you for bringing up the requirement of the e2e tests, we understand the solution needs it.\nOur focus is to get this example available to the community and be able to collect any feedback from customers using this example in their scenarios and the approach to the examples. We propose to keep this PR scope, create a JIRA ticket (for Kafka and Pub/Sub) to extend this example with e2e tests, and set customer expectations on e2e testing in the readme for this PR.", "author": "ilya-kozyrev", "createdAt": "2020-12-07T20:17:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMzAyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzkzMDEzMQ==", "url": "https://github.com/apache/beam/pull/13112#discussion_r537930131", "bodyText": "I'm a +1 for @ilya-kozyrev's proposal.\nIf the solution passes all tests, I'm supportive of merging the solution so we entice users to test it according to their use cases. The consideration here is to complete the two steps he suggests: adding the note on e2e tests needed and creating Jira tickets to create these.", "author": "griscz", "createdAt": "2020-12-08T00:07:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMzAyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODcyNzcyNw==", "url": "https://github.com/apache/beam/pull/13112#discussion_r538727727", "bodyText": "Sounds good. Please add the note and the link the JIRA tickets in that case.", "author": "manavgarg", "createdAt": "2020-12-08T19:00:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMzAyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDAzOTA4NQ==", "url": "https://github.com/apache/beam/pull/13112#discussion_r530039085", "bodyText": "Since your implementation for getGcsFileAsLocal relies on FileSystems.matchSingleFileSpec I think it will actually work for the else path here as well (It could also pull a truststore from AWS s3 if you include the dependency).\nIt's worth noting that the local file option will fail at execution time for a distributed runner, we may want to catch that and raise a more helpful error - e.g. suggest that they stage the file on cloud storage", "author": "TheNeuralBit", "createdAt": "2020-11-25T00:54:26Z", "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/kafka/consumer/SslConsumerFactoryFn.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates.kafka.consumer;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** Class to create Kafka Consumer with configured SSL. */\n+public class SslConsumerFactoryFn\n+    implements SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> {\n+  private final Map<String, String> sslConfig;\n+  private static final String TRUSTSTORE_LOCAL_PATH = \"/tmp/kafka.truststore.jks\";\n+  private static final String KEYSTORE_LOCAL_PATH = \"/tmp/kafka.keystore.jks\";\n+\n+  /* Logger for class.*/\n+  private static final Logger LOG = LoggerFactory.getLogger(SslConsumerFactoryFn.class);\n+\n+  public SslConsumerFactoryFn(Map<String, String> sslConfig) {\n+    this.sslConfig = sslConfig;\n+  }\n+\n+  @SuppressWarnings(\"nullness\")\n+  @Override\n+  public Consumer<byte[], byte[]> apply(Map<String, Object> config) {\n+    try {\n+      String truststoreLocation = sslConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG);\n+      if (truststoreLocation.startsWith(\"gs://\")) {\n+        getGcsFileAsLocal(truststoreLocation, TRUSTSTORE_LOCAL_PATH);\n+      } else {\n+        checkFileExists(truststoreLocation);", "originalCommit": "8a3d85d19e45613b28ad5bc0042921a0f961598c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzUxNzEwNg==", "url": "https://github.com/apache/beam/pull/13112#discussion_r533517106", "bodyText": "Thank you for the great idea to use different filesystems. We will add it to future plans.\nI added a new message to mention what we cant use local paths when we using distribute runners.", "author": "ilya-kozyrev", "createdAt": "2020-12-01T15:47:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDAzOTA4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA0MDAzNw==", "url": "https://github.com/apache/beam/pull/13112#discussion_r530040037", "bodyText": "This will never be triggered its the same condition as the if", "author": "TheNeuralBit", "createdAt": "2020-11-25T00:57:28Z", "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.templates.kafka.consumer.Utils.configureKafka;\n+import static org.apache.beam.templates.kafka.consumer.Utils.configureSsl;\n+import static org.apache.beam.templates.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+import static org.apache.beam.templates.kafka.consumer.Utils.isSslSpecified;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.templates.avro.AvroDataClass;\n+import org.apache.beam.templates.avro.AvroDataClassKafkaAvroDeserializer;\n+import org.apache.beam.templates.options.KafkaToPubsubOptions;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+ *       --image-gcr-path \"${TARGET_GCR_IMAGE}\" \\\n+ *       --sdk-language \"JAVA\" \\\n+ *       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+ *       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+ *       --jar \"build/libs/beam-templates-kafka-to-pubsub-<version>-all.jar\" \\\n+ *       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+ *\n+ * # Execute template:\n+ *    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+ *    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+ *    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+ *\n+ *    time curl -X POST -H \"Content-Type: application/json\" \\\n+ *            -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+ *            -d '\n+ *             {\n+ *                 \"launch_parameter\": {\n+ *                     \"jobName\": \"'$JOB_NAME'\",\n+ *                     \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+ *                     \"parameters\": {\n+ *                         \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+ *                         \"inputTopics\": \"topic1, topic2\",\n+ *                         \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+ *                         \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+ *                         \"vaultToken\": \"your-token\"\n+ *                     }\n+ *                 }\n+ *             }\n+ *            '\n+ *            \"${TEMPLATES_LAUNCH_API}\"\n+ * </pre>\n+ *\n+ * <p><b>Example Avro usage</b>\n+ *\n+ * <pre>\n+ * This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.\n+ *\n+ * To use this example in the specific case, follow the few steps:\n+ * <ul>\n+ * <li> Create your own class to describe AVRO schema. As an example use {@link AvroDataClass}. Just define necessary fields.\n+ * <li> Create your own Avro Deserializer class. As an example use {@link AvroDataClassKafkaAvroDeserializer}. Just rename it, and put your own Schema class as the necessary types.\n+ * <li> Modify the {@link FormatTransform}. Put your Schema class and Deserializer to the related parameter.\n+ * <li> Modify write step in the {@link KafkaToPubsub} by put your Schema class to \"writeAvrosToPubSub\" step.\n+ * </ul>\n+ * </pre>\n+ */\n+public class KafkaToPubsub {\n+\n+  /* Logger for class.*/\n+  private static final Logger LOG = LoggerFactory.getLogger(KafkaToPubsub.class);\n+\n+  /**\n+   * Main entry point for pipeline execution.\n+   *\n+   * @param args Command line arguments to the pipeline.\n+   */\n+  public static void main(String[] args) {\n+    KafkaToPubsubOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(KafkaToPubsubOptions.class);\n+\n+    run(options);\n+  }\n+\n+  /**\n+   * Runs a pipeline which reads message from Kafka and writes it to GCS.\n+   *\n+   * @param options arguments to the pipeline\n+   */\n+  public static PipelineResult run(KafkaToPubsubOptions options) {\n+    // Configure Kafka consumer properties\n+    Map<String, Object> kafkaConfig = new HashMap<>();\n+    if (options.getSecretStoreUrl() != null && options.getVaultToken() != null) {\n+      Map<String, Map<String, String>> credentials =\n+          getKafkaCredentialsFromVault(options.getSecretStoreUrl(), options.getVaultToken());\n+      kafkaConfig = configureKafka(credentials.get(KafkaPubsubConstants.KAFKA_CREDENTIALS));\n+    } else {\n+      LOG.warn(\n+          \"No information to retrieve Kafka credentials was provided. \"\n+              + \"Trying to initiate an unauthorized connection.\");\n+    }\n+\n+    Map<String, String> sslConfig = new HashMap<>();\n+    if (isSslSpecified(options)) {\n+      sslConfig.putAll(configureSsl(options));\n+    } else {\n+      LOG.info(\n+          \"No information to retrieve SSL certificate was provided. \"\n+              + \"Trying to initiate a plain text connection.\");\n+    }\n+\n+    List<String> topicsList = new ArrayList<>(Arrays.asList(options.getInputTopics().split(\",\")));\n+\n+    checkArgument(\n+        topicsList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"inputTopics cannot be an empty string.\");\n+\n+    List<String> bootstrapServersList =\n+        new ArrayList<>(Arrays.asList(options.getBootstrapServers().split(\",\")));\n+\n+    checkArgument(\n+        bootstrapServersList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"bootstrapServers cannot be an empty string.\");\n+\n+    // Create the pipeline\n+    Pipeline pipeline = Pipeline.create(options);\n+    LOG.info(\n+        \"Starting Kafka-To-PubSub pipeline with parameters bootstrap servers:\"\n+            + options.getBootstrapServers()\n+            + \" input topics: \"\n+            + options.getInputTopics()\n+            + \" output pubsub topic: \"\n+            + options.getOutputTopic());\n+\n+    /*\n+     * Steps:\n+     *  1) Read messages in from Kafka\n+     *  2) Extract values only\n+     *  3) Write successful records to PubSub\n+     */\n+\n+    if (options.getOutputFormat() == FormatTransform.FORMAT.AVRO) {\n+      pipeline\n+          .apply(\n+              \"readAvrosFromKafka\",\n+              FormatTransform.readAvrosFromKafka(\n+                  options.getBootstrapServers(), topicsList, kafkaConfig, sslConfig))\n+          .apply(\"createValues\", Values.create())\n+          .apply(\"writeAvrosToPubSub\", PubsubIO.writeAvros(AvroDataClass.class));\n+\n+    } else if (options.getOutputFormat() == FormatTransform.FORMAT.AVRO) {", "originalCommit": "8a3d85d19e45613b28ad5bc0042921a0f961598c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTI1MzQ0Mw==", "url": "https://github.com/apache/beam/pull/13112#discussion_r531253443", "bodyText": "fixed", "author": "ilya-kozyrev", "createdAt": "2020-11-26T23:34:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA0MDAzNw=="}], "type": "inlineReview"}, {"oid": "4172c96f2b7fe9119a5e05e38732c89fe5e868ff", "url": "https://github.com/apache/beam/commit/4172c96f2b7fe9119a5e05e38732c89fe5e868ff", "message": "fixed review conversation items", "committedDate": "2020-11-26T23:49:02Z", "type": "commit"}, {"oid": "3c63298179b3f7a485885e2de59994f6cf05897d", "url": "https://github.com/apache/beam/commit/3c63298179b3f7a485885e2de59994f6cf05897d", "message": "fix getting ssl credentials from Vault", "committedDate": "2020-11-30T14:02:38Z", "type": "commit"}, {"oid": "477df3e234f351e43f91105eef79ecf68eebbf56", "url": "https://github.com/apache/beam/commit/477df3e234f351e43f91105eef79ecf68eebbf56", "message": "FIX add empty && null map validation to sslConfig", "committedDate": "2020-11-30T14:10:19Z", "type": "commit"}, {"oid": "83ff7b78aa84a9f7b79cb47ff896b10a41016d37", "url": "https://github.com/apache/beam/commit/83ff7b78aa84a9f7b79cb47ff896b10a41016d37", "message": "FIX. remove vault ssl certs parameters", "committedDate": "2020-11-30T15:06:02Z", "type": "commit"}, {"oid": "b1ba8e123a2000ff14a6dd7b4118cd296654df3b", "url": "https://github.com/apache/beam/commit/b1ba8e123a2000ff14a6dd7b4118cd296654df3b", "message": "metadata fix", "committedDate": "2020-11-30T15:27:18Z", "type": "commit"}, {"oid": "08bc3fefca612d43d1abab890c7d96fc8aa8bf15", "url": "https://github.com/apache/beam/commit/08bc3fefca612d43d1abab890c7d96fc8aa8bf15", "message": "Local paths fix for SSL from GCS", "committedDate": "2020-11-30T15:38:37Z", "type": "commit"}, {"oid": "9fb43b63430253f4a2b3cd57de7b9886ab4b3f67", "url": "https://github.com/apache/beam/commit/9fb43b63430253f4a2b3cd57de7b9886ab4b3f67", "message": "add new log message to avoid wrong local files usage", "committedDate": "2020-12-01T15:43:49Z", "type": "commit"}, {"oid": "e99a7d31210f202f80fab645ce4f787c849160f9", "url": "https://github.com/apache/beam/commit/e99a7d31210f202f80fab645ce4f787c849160f9", "message": "fix style", "committedDate": "2020-12-01T15:48:59Z", "type": "commit"}, {"oid": "2ae7d024a5576823415a5448a29bff62c9b19893", "url": "https://github.com/apache/beam/commit/2ae7d024a5576823415a5448a29bff62c9b19893", "message": "Moved kafka-to-pubsub to examples/ directory and updated README.md (#6)", "committedDate": "2020-12-04T16:16:12Z", "type": "commit"}, {"oid": "3e558a39aeed7671d4d9ae1f630d22f41e354044", "url": "https://github.com/apache/beam/commit/3e558a39aeed7671d4d9ae1f630d22f41e354044", "message": "Stylefix", "committedDate": "2020-12-04T18:07:01Z", "type": "commit"}, {"oid": "51ddeadef3daf09f1d2af569d5d747c185a5f529", "url": "https://github.com/apache/beam/commit/51ddeadef3daf09f1d2af569d5d747c185a5f529", "message": "Removed unused file", "committedDate": "2020-12-07T11:46:30Z", "type": "commit"}, {"oid": "e8ca92d68646cec459d3ecd94b0efe7551699268", "url": "https://github.com/apache/beam/commit/e8ca92d68646cec459d3ecd94b0efe7551699268", "message": "add tbd section for e-2-e tests", "committedDate": "2020-12-07T20:12:26Z", "type": "commit"}, {"oid": "880f7e6d00ad6e582a0edce14c4bdaff4173313e", "url": "https://github.com/apache/beam/commit/880f7e6d00ad6e582a0edce14c4bdaff4173313e", "message": "fix styles", "committedDate": "2020-12-07T20:13:07Z", "type": "commit"}, {"oid": "0eba7ba5c7661c1e8a60178103f698064686eda4", "url": "https://github.com/apache/beam/commit/0eba7ba5c7661c1e8a60178103f698064686eda4", "message": "specifying kafka-clients version", "committedDate": "2020-12-07T22:48:02Z", "type": "commit"}, {"oid": "5919f9b15047c6308bac5d8b7134895023cc0a75", "url": "https://github.com/apache/beam/commit/5919f9b15047c6308bac5d8b7134895023cc0a75", "message": "fix readme", "committedDate": "2020-12-07T23:33:32Z", "type": "commit"}, {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f", "url": "https://github.com/apache/beam/commit/4372d6ff56bdef40e106f358adb1881cd3032e6f", "message": "template -> exmples", "committedDate": "2020-12-07T23:39:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY2NDc4OA==", "url": "https://github.com/apache/beam/pull/13112#discussion_r538664788", "bodyText": "Please remove the Dataflow template specific parts from this javadoc", "author": "TheNeuralBit", "createdAt": "2020-12-08T17:56:35Z", "path": "examples/kafka-to-pubsub/src/main/java/org/apache/beam/examples/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.examples;\n+\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureKafka;\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureSsl;\n+import static org.apache.beam.examples.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+import static org.apache.beam.examples.kafka.consumer.Utils.isSslSpecified;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.examples.avro.AvroDataClass;\n+import org.apache.beam.examples.avro.AvroDataClassKafkaAvroDeserializer;\n+import org.apache.beam.examples.options.KafkaToPubsubOptions;\n+import org.apache.beam.examples.transforms.FormatTransform;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\", "originalCommit": "4372d6ff56bdef40e106f358adb1881cd3032e6f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ3OTkwNA==", "url": "https://github.com/apache/beam/pull/13112#discussion_r539479904", "bodyText": "Removed", "author": "KhaninArtur", "createdAt": "2020-12-09T16:58:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY2NDc4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY2NzI2NQ==", "url": "https://github.com/apache/beam/pull/13112#discussion_r538667265", "bodyText": "Is it worth having this PUBSUB path? The README and javadoc only discuss the AVRO path. I think we should just have that one and remove the enum", "author": "TheNeuralBit", "createdAt": "2020-12-08T17:58:50Z", "path": "examples/kafka-to-pubsub/src/main/java/org/apache/beam/examples/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.examples;\n+\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureKafka;\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureSsl;\n+import static org.apache.beam.examples.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+import static org.apache.beam.examples.kafka.consumer.Utils.isSslSpecified;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.examples.avro.AvroDataClass;\n+import org.apache.beam.examples.avro.AvroDataClassKafkaAvroDeserializer;\n+import org.apache.beam.examples.options.KafkaToPubsubOptions;\n+import org.apache.beam.examples.transforms.FormatTransform;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+ *       --image-gcr-path \"${TARGET_GCR_IMAGE}\" \\\n+ *       --sdk-language \"JAVA\" \\\n+ *       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+ *       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+ *       --jar \"build/libs/beam-templates-kafka-to-pubsub-<version>-all.jar\" \\\n+ *       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+ *\n+ * # Execute template:\n+ *    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+ *    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+ *    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+ *\n+ *    time curl -X POST -H \"Content-Type: application/json\" \\\n+ *            -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+ *            -d '\n+ *             {\n+ *                 \"launch_parameter\": {\n+ *                     \"jobName\": \"'$JOB_NAME'\",\n+ *                     \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+ *                     \"parameters\": {\n+ *                         \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+ *                         \"inputTopics\": \"topic1, topic2\",\n+ *                         \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+ *                         \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+ *                         \"vaultToken\": \"your-token\"\n+ *                     }\n+ *                 }\n+ *             }\n+ *            '\n+ *            \"${TEMPLATES_LAUNCH_API}\"\n+ * </pre>\n+ *\n+ * <p><b>Example Avro usage</b>\n+ *\n+ * <pre>\n+ * This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.\n+ *\n+ * To use this example in the specific case, follow the few steps:\n+ * <ul>\n+ * <li> Create your own class to describe AVRO schema. As an example use {@link AvroDataClass}. Just define necessary fields.\n+ * <li> Create your own Avro Deserializer class. As an example use {@link AvroDataClassKafkaAvroDeserializer}. Just rename it, and put your own Schema class as the necessary types.\n+ * <li> Modify the {@link FormatTransform}. Put your Schema class and Deserializer to the related parameter.\n+ * <li> Modify write step in the {@link KafkaToPubsub} by put your Schema class to \"writeAvrosToPubSub\" step.\n+ * </ul>\n+ * </pre>\n+ */\n+public class KafkaToPubsub {\n+\n+  /* Logger for class.*/\n+  private static final Logger LOG = LoggerFactory.getLogger(KafkaToPubsub.class);\n+\n+  /**\n+   * Main entry point for pipeline execution.\n+   *\n+   * @param args Command line arguments to the pipeline.\n+   */\n+  public static void main(String[] args) {\n+    KafkaToPubsubOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(KafkaToPubsubOptions.class);\n+\n+    run(options);\n+  }\n+\n+  /**\n+   * Runs a pipeline which reads message from Kafka and writes it to GCS.\n+   *\n+   * @param options arguments to the pipeline\n+   */\n+  public static PipelineResult run(KafkaToPubsubOptions options) {\n+    // Configure Kafka consumer properties\n+    Map<String, Object> kafkaConfig = new HashMap<>();\n+    Map<String, String> sslConfig = new HashMap<>();\n+    if (options.getSecretStoreUrl() != null && options.getVaultToken() != null) {\n+      Map<String, Map<String, String>> credentials =\n+          getKafkaCredentialsFromVault(options.getSecretStoreUrl(), options.getVaultToken());\n+      kafkaConfig = configureKafka(credentials.get(KafkaPubsubConstants.KAFKA_CREDENTIALS));\n+    } else {\n+      LOG.warn(\n+          \"No information to retrieve Kafka credentials was provided. \"\n+              + \"Trying to initiate an unauthorized connection.\");\n+    }\n+\n+    if (isSslSpecified(options)) {\n+      sslConfig.putAll(configureSsl(options));\n+    } else {\n+      LOG.info(\n+          \"No information to retrieve SSL certificate was provided by parameters.\"\n+              + \"Trying to initiate a plain text connection.\");\n+    }\n+\n+    List<String> topicsList = new ArrayList<>(Arrays.asList(options.getInputTopics().split(\",\")));\n+\n+    checkArgument(\n+        topicsList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"inputTopics cannot be an empty string.\");\n+\n+    List<String> bootstrapServersList =\n+        new ArrayList<>(Arrays.asList(options.getBootstrapServers().split(\",\")));\n+\n+    checkArgument(\n+        bootstrapServersList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"bootstrapServers cannot be an empty string.\");\n+\n+    // Create the pipeline\n+    Pipeline pipeline = Pipeline.create(options);\n+    LOG.info(\n+        \"Starting Kafka-To-PubSub pipeline with parameters bootstrap servers:\"\n+            + options.getBootstrapServers()\n+            + \" input topics: \"\n+            + options.getInputTopics()\n+            + \" output pubsub topic: \"\n+            + options.getOutputTopic());\n+\n+    /*\n+     * Steps:\n+     *  1) Read messages in from Kafka\n+     *  2) Extract values only\n+     *  3) Write successful records to PubSub\n+     */\n+\n+    if (options.getOutputFormat() == FormatTransform.FORMAT.AVRO) {\n+      pipeline\n+          .apply(\n+              \"readAvrosFromKafka\",\n+              FormatTransform.readAvrosFromKafka(\n+                  options.getBootstrapServers(), topicsList, kafkaConfig, sslConfig))\n+          .apply(\"createValues\", Values.create())\n+          .apply(\"writeAvrosToPubSub\", PubsubIO.writeAvros(AvroDataClass.class));\n+\n+    } else {", "originalCommit": "4372d6ff56bdef40e106f358adb1881cd3032e6f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ4Njc1OQ==", "url": "https://github.com/apache/beam/pull/13112#discussion_r539486759", "bodyText": "I see why it may seem invaluable for the example, thank you for noticing this!\nI suppose it is worth having the PUBSUB path because this example works out-of-the-box with it. For the AVRO path, the user has to add some code to make it work and also to understand what and how should be changed - the PUBSUB path doesn't require it.\nI also updated the README file to highlight the value of it.", "author": "KhaninArtur", "createdAt": "2020-12-09T17:07:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY2NzI2NQ=="}], "type": "inlineReview"}, {"oid": "dd85c93e8d405bb3a936ae5a14eca14825f89eaf", "url": "https://github.com/apache/beam/commit/dd85c93e8d405bb3a936ae5a14eca14825f89eaf", "message": "Update examples/kafka-to-pubsub/README.md\n\nCo-authored-by: Brian Hulette <hulettbh@gmail.com>", "committedDate": "2020-12-08T19:46:41Z", "type": "commit"}, {"oid": "89adf0bdc98aef01092a62494f1bce2d96e7fc01", "url": "https://github.com/apache/beam/commit/89adf0bdc98aef01092a62494f1bce2d96e7fc01", "message": "Merge branch 'master' into KafkaToPubsubTemplate", "committedDate": "2020-12-09T08:45:54Z", "type": "commit"}, {"oid": "51182eb24d4d7f2c7f424259f5287aed26dd67af", "url": "https://github.com/apache/beam/commit/51182eb24d4d7f2c7f424259f5287aed26dd67af", "message": "Fixed outdated import", "committedDate": "2020-12-09T08:58:27Z", "type": "commit"}, {"oid": "a5406a558b095ea2eded4536d1bcb9aed1c8e449", "url": "https://github.com/apache/beam/commit/a5406a558b095ea2eded4536d1bcb9aed1c8e449", "message": "Moved template to examples/complete", "committedDate": "2020-12-09T14:56:51Z", "type": "commit"}, {"oid": "e7fc00e6df390e84d55cbc4e8e03c98b66f56625", "url": "https://github.com/apache/beam/commit/e7fc00e6df390e84d55cbc4e8e03c98b66f56625", "message": "Updated paths in readme file", "committedDate": "2020-12-09T15:10:54Z", "type": "commit"}, {"oid": "ac76e738bd9d466669721c8511f8d8bf999521fe", "url": "https://github.com/apache/beam/commit/ac76e738bd9d466669721c8511f8d8bf999521fe", "message": "Updated README.md and javadoc regarding comments", "committedDate": "2020-12-09T16:54:10Z", "type": "commit"}, {"oid": "b0c44289db70c7a4dc2b6f0f6275c62bc2214c86", "url": "https://github.com/apache/beam/commit/b0c44289db70c7a4dc2b6f0f6275c62bc2214c86", "message": "README.md stylefix", "committedDate": "2020-12-09T17:16:07Z", "type": "commit"}, {"oid": "3931fba06323d0e238355e8c5538c61e6a26310a", "url": "https://github.com/apache/beam/commit/3931fba06323d0e238355e8c5538c61e6a26310a", "message": "Added link to KafkaToPubsub example into complete/README.md", "committedDate": "2020-12-09T17:33:04Z", "type": "commit"}, {"oid": "21d8b36652b9be13ba17be1c2ae8ef6e3c351472", "url": "https://github.com/apache/beam/commit/21d8b36652b9be13ba17be1c2ae8ef6e3c351472", "message": "Stylefix", "committedDate": "2020-12-09T17:47:58Z", "type": "commit"}]}