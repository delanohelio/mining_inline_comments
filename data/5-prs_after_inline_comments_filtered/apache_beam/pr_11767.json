{"pr_number": 11767, "pr_title": "[BEAM-11648] BigQuery Storage API sink", "pr_createdAt": "2020-05-20T22:14:26Z", "pr_url": "https://github.com/apache/beam/pull/11767", "timeline": [{"oid": "cbf53136b71a9ec0fb9caff4a933f94751a70b13", "url": "https://github.com/apache/beam/commit/cbf53136b71a9ec0fb9caff4a933f94751a70b13", "message": "add retries and logging", "committedDate": "2020-06-02T17:47:29Z", "type": "forcePushed"}, {"oid": "f2cf3a0ab5e1a0f99f8bed91f2b87405ec1017db", "url": "https://github.com/apache/beam/commit/f2cf3a0ab5e1a0f99f8bed91f2b87405ec1017db", "message": "resolve with new API version", "committedDate": "2021-01-10T21:13:53Z", "type": "forcePushed"}, {"oid": "6de33f48a2d8056eabbcf7c1300cc7e9680320ef", "url": "https://github.com/apache/beam/commit/6de33f48a2d8056eabbcf7c1300cc7e9680320ef", "message": "Vortex sink", "committedDate": "2021-01-12T21:42:27Z", "type": "forcePushed"}, {"oid": "c7b10cad67f2513c207b8c584492c1d36e870583", "url": "https://github.com/apache/beam/commit/c7b10cad67f2513c207b8c584492c1d36e870583", "message": "Vortex sink", "committedDate": "2021-01-25T05:11:30Z", "type": "forcePushed"}, {"oid": "b4b60625a4a5c593ce0cf77457867efe61137e70", "url": "https://github.com/apache/beam/commit/b4b60625a4a5c593ce0cf77457867efe61137e70", "message": "foo", "committedDate": "2021-02-22T21:38:56Z", "type": "forcePushed"}, {"oid": "61fc80b59a6872ee6b8e5686ed4e1bdca670042f", "url": "https://github.com/apache/beam/commit/61fc80b59a6872ee6b8e5686ed4e1bdca670042f", "message": "spotless", "committedDate": "2021-02-26T22:30:23Z", "type": "forcePushed"}, {"oid": "3d924f7946cba8bee050a871ac023b4a1afb1132", "url": "https://github.com/apache/beam/commit/3d924f7946cba8bee050a871ac023b4a1afb1132", "message": "Vortex sink", "committedDate": "2021-03-05T06:37:14Z", "type": "forcePushed"}, {"oid": "4656af590a4c1d91f9c8abc3934f632e0f345d3b", "url": "https://github.com/apache/beam/commit/4656af590a4c1d91f9c8abc3934f632e0f345d3b", "message": "Vortex sink", "committedDate": "2021-03-05T06:45:16Z", "type": "forcePushed"}, {"oid": "19153f3094cd7a7cbe584e54922a2bf20e01bd0b", "url": "https://github.com/apache/beam/commit/19153f3094cd7a7cbe584e54922a2bf20e01bd0b", "message": "Spotless fixes", "committedDate": "2021-03-05T16:19:57Z", "type": "forcePushed"}, {"oid": "b8eed6448f6ca36f0d810ac25f7023452f55c445", "url": "https://github.com/apache/beam/commit/b8eed6448f6ca36f0d810ac25f7023452f55c445", "message": "spotless", "committedDate": "2021-03-05T18:01:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODk5ODY2Mg==", "url": "https://github.com/apache/beam/pull/11767#discussion_r588998662", "bodyText": "To see if the operation really succeeded, commit_time in the response cannot be empty. If it appears to be a success response and commit_time is empty, commit actually failed.\nIt will return detailed stream level error info here:\nhttps://source.corp.google.com/piper///depot/google3/google/cloud/bigquery/storage/v1beta2/storage.proto;l=751?q=BatchCommitWriteStreams", "author": "yirutang", "createdAt": "2021-03-07T09:11:04Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiFinalizeWritesDoFn.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.gcp.bigquery;\n+\n+import com.google.cloud.bigquery.storage.v1beta2.BatchCommitWriteStreamsResponse;\n+import com.google.cloud.bigquery.storage.v1beta2.FinalizeWriteStreamResponse;\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.io.gcp.bigquery.BigQueryServices.DatasetService;\n+import org.apache.beam.sdk.io.gcp.bigquery.RetryManager.Operation.Context;\n+import org.apache.beam.sdk.io.gcp.bigquery.RetryManager.RetryType;\n+import org.apache.beam.sdk.metrics.Counter;\n+import org.apache.beam.sdk.metrics.Metrics;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Lists;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Maps;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** This DoFn finalizes and commits Storage API streams. */\n+class StorageApiFinalizeWritesDoFn extends DoFn<KV<String, String>, Void> {\n+  private static final Logger LOG = LoggerFactory.getLogger(StorageApiFinalizeWritesDoFn.class);\n+\n+  private final Counter finalizeOperationsSent =\n+      Metrics.counter(StorageApiFinalizeWritesDoFn.class, \"finalizeOperationsSent\");\n+  private final Counter finalizeOperationsSucceeded =\n+      Metrics.counter(StorageApiFinalizeWritesDoFn.class, \"finalizeOperationsSucceeded\");\n+  private final Counter finalizeOperationsFailed =\n+      Metrics.counter(StorageApiFinalizeWritesDoFn.class, \"finalizeOperationsFailed\");\n+  private final Counter batchCommitOperationsSent =\n+      Metrics.counter(StorageApiFinalizeWritesDoFn.class, \"batchCommitOperationsSent\");\n+  private final Counter batchCommitOperationsSucceeded =\n+      Metrics.counter(StorageApiFinalizeWritesDoFn.class, \"batchCommitOperationsSucceeded\");\n+  private final Counter batchCommitOperationsFailed =\n+      Metrics.counter(StorageApiFinalizeWritesDoFn.class, \"batchCommitOperationsFailed\");\n+\n+  private Map<String, Collection<String>> commitStreams;\n+  private final BigQueryServices bqServices;\n+  @Nullable private DatasetService datasetService;\n+\n+  public StorageApiFinalizeWritesDoFn(BigQueryServices bqServices) {\n+    this.bqServices = bqServices;\n+    this.commitStreams = Maps.newHashMap();\n+    this.datasetService = null;\n+  }\n+\n+  private DatasetService getDatasetService(PipelineOptions pipelineOptions) throws IOException {\n+    if (datasetService == null) {\n+      datasetService = bqServices.getDatasetService(pipelineOptions.as(BigQueryOptions.class));\n+    }\n+    return datasetService;\n+  }\n+\n+  @StartBundle\n+  public void startBundle() throws IOException {\n+    commitStreams = Maps.newHashMap();\n+  }\n+\n+  @ProcessElement\n+  @SuppressWarnings({\"nullness\"})\n+  public void process(PipelineOptions pipelineOptions, @Element KV<String, String> element)\n+      throws Exception {\n+    String tableId = element.getKey();\n+    String streamId = element.getValue();\n+    DatasetService datasetService = getDatasetService(pipelineOptions);\n+\n+    RetryManager<FinalizeWriteStreamResponse, Context<FinalizeWriteStreamResponse>> retryManager =\n+        new RetryManager<>(Duration.standardSeconds(1), Duration.standardMinutes(1), 3);\n+    retryManager.addOperation(\n+        c -> {\n+          finalizeOperationsSent.inc();\n+          return datasetService.finalizeWriteStream(streamId);\n+        },\n+        contexts -> {\n+          LOG.error(\n+              \"Finalize of stream \"\n+                  + streamId\n+                  + \" failed with \"\n+                  + Iterables.getFirst(contexts, null).getError());\n+          finalizeOperationsFailed.inc();\n+          return RetryType.RETRY_ALL_OPERATIONS;\n+        },\n+        c -> {\n+          LOG.info(\"Finalize of stream \" + streamId + \" finished with \" + c.getResult());\n+          finalizeOperationsSucceeded.inc();\n+          commitStreams.computeIfAbsent(tableId, d -> Lists.newArrayList()).add(streamId);\n+        },\n+        new Context<>());\n+    retryManager.run(true);\n+  }\n+\n+  @FinishBundle\n+  @SuppressWarnings({\"nullness\"})\n+  public void finishBundle(PipelineOptions pipelineOptions) throws Exception {\n+    DatasetService datasetService = getDatasetService(pipelineOptions);\n+    for (Map.Entry<String, Collection<String>> entry : commitStreams.entrySet()) {\n+      final String tableId = entry.getKey();\n+      final Collection<String> streamNames = entry.getValue();\n+      RetryManager<BatchCommitWriteStreamsResponse, Context<BatchCommitWriteStreamsResponse>>\n+          retryManager =\n+              new RetryManager<>(Duration.standardSeconds(1), Duration.standardMinutes(1), 3);\n+      retryManager.addOperation(\n+          c -> {\n+            batchCommitOperationsSent.inc();", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTc0NjI1MQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r589746251", "bodyText": "Is there a reason for this? Sounds like an error-prone API, as most users will assume that a success response means success (usual ROT for RPCS - success means success. failure means it may have failed or may have succeeded).", "author": "reuvenlax", "createdAt": "2021-03-08T21:03:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODk5ODY2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgyNDkwNA==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593824904", "bodyText": "@yirutang Looking at the code, it seems a bit tricky. The current code structure handles failure/retry at the RPC layer. Unfortunately it appears the commit is not idempotent - retrying a commit will return STREAM_ALREADY_FINALIZED. This means that if one stream fails and we retry, we'll get stuck in an infinite retry loop. (we can't ignore STREAM_ALREADY_FINALIZED, since there may be other failed responses that need to be retried).\nI think the simplest solution for now would be for me to stop batching commits and issue a separate commit per stream.", "author": "reuvenlax", "createdAt": "2021-03-14T01:28:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODk5ODY2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDg3MjExMQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r594872111", "bodyText": "Ok, retrying just the correct streams ended up not being difficult. PTAL", "author": "reuvenlax", "createdAt": "2021-03-16T05:30:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODk5ODY2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTAwMDQ2NA==", "url": "https://github.com/apache/beam/pull/11767#discussion_r589000464", "bodyText": "High level, do you want to offer user an option of not to use deduplication? If user don't need deduplication, they could use default stream and subject to less quota and higher performance on writes. It is in COMMITTED mode only and more like using the old insertall without insert_id.", "author": "yirutang", "createdAt": "2021-03-07T09:25:32Z", "path": "examples/java/src/test/java/org/apache/beam/examples/cookbook/BigQueryStorageAPIStreamingIT.java", "diffHunk": "@@ -0,0 +1,133 @@\n+/*", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTc1MzcxNw==", "url": "https://github.com/apache/beam/pull/11767#discussion_r589753717", "bodyText": "We could add this pretty easily, but I'd prefer to wait until there's a request. The rate of duplicates is likely to be higher than expected (especially with a runner like Dataflow that dynamically rebalances and retries largish chunks of work) so the savings would have to be significant to justify it IMO.", "author": "reuvenlax", "createdAt": "2021-03-08T21:16:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTAwMDQ2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MjkyMTQxMQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r592921411", "bodyText": "Should this be experimental initially ?", "author": "chamikaramj", "createdAt": "2021-03-12T05:37:11Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.java", "diffHunk": "@@ -1724,7 +1725,8 @@ static String getExtractDestinationUri(String extractDestinationDir) {\n        * href=\"https://cloud.google.com/bigquery/streaming-data-into-bigquery\">Streaming Data into\n        * BigQuery</a>.\n        */\n-      STREAMING_INSERTS\n+      STREAMING_INSERTS,\n+      STORAGE_API_WRITES", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDg3MjE1Mg==", "url": "https://github.com/apache/beam/pull/11767#discussion_r594872152", "bodyText": "Experimental is generally used for Beam-level items, so it seems a bit strange here. I'll add a comment instead (BTW, the experimental nature is why I didn't add example usage to the Javadoc).", "author": "reuvenlax", "createdAt": "2021-03-16T05:30:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MjkyMTQxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc3NjUzOQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593776539", "bodyText": "There are some commented out lines here and below.", "author": "chamikaramj", "createdAt": "2021-03-13T17:15:48Z", "path": "examples/java/src/test/java/org/apache/beam/examples/cookbook/BigQueryStorageAPIStreamingIT.java", "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.examples.cookbook;\n+\n+import com.google.auto.value.AutoValue;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.concurrent.ThreadLocalRandom;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;\n+import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write;\n+import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.WriteDisposition;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.schemas.AutoValueSchema;\n+import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.TestPipelineOptions;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.joda.time.Duration;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+@RunWith(JUnit4.class)\n+public class BigQueryStorageAPIStreamingIT {\n+  @DefaultSchema(AutoValueSchema.class)\n+  @AutoValue\n+  public abstract static class Value {\n+    public abstract long getNumber();\n+\n+    @Nullable\n+    public abstract ByteBuffer getPayload();\n+  }\n+\n+  public interface Options extends TestPipelineOptions {\n+    @Description(\"The number of records per second to generate.\")\n+    @Default.Integer(10000)\n+    Integer getRecordsPerSecond();\n+\n+    void setRecordsPerSecond(Integer recordsPerSecond);\n+\n+    @Description(\"The size of the records to write in bytes.\")\n+    @Default.Integer(1024)\n+    Integer getPayloadSizeBytes();\n+\n+    void setPayloadSizeBytes(Integer payloadSizeBytes);\n+\n+    @Description(\"Parallelism used for Storage API writes.\")\n+    @Default.Integer(5)\n+    Integer getNumShards();\n+\n+    void setNumShards(Integer numShards);\n+\n+    @Description(\"Frequency to trigger appends. Each shard triggers independently.\")\n+    @Default.Integer(5)\n+    Integer getTriggerFrequencySec();\n+\n+    void setTriggerFrequencySec(Integer triggerFrequencySec);\n+\n+    @Description(\"The table to write to.\")\n+    String getTargetTable();\n+\n+    void setTargetTable(String table);\n+  }\n+\n+  @BeforeClass\n+  public static void setUp() {\n+    PipelineOptionsFactory.register(Options.class);\n+  }\n+\n+  @Test\n+  public void testStorageAPIStreaming() throws Exception {\n+    Options options = TestPipeline.testingPipelineOptions().as(Options.class);\n+    Pipeline p = Pipeline.create(options);\n+    final int payloadSizeBytes = options.getPayloadSizeBytes();\n+\n+    // Generate input.\n+    PCollection<Value> values =\n+        //   p.apply(GenerateSequence.from(1).to(1000000L))", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDg3MjE4NQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r594872185", "bodyText": "Fixed", "author": "reuvenlax", "createdAt": "2021-03-16T05:30:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc3NjUzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc3NjkyMQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593776921", "bodyText": "I assume zero just means default ? (probably good to clarify with a comment).", "author": "chamikaramj", "createdAt": "2021-03-13T17:18:44Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.java", "diffHunk": "@@ -1664,6 +1664,7 @@ static String getExtractDestinationUri(String extractDestinationDir) {\n         .setWriteDisposition(Write.WriteDisposition.WRITE_EMPTY)\n         .setSchemaUpdateOptions(Collections.emptySet())\n         .setNumFileShards(0)\n+        .setNumStorageApiStreams(0)", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDg3MjE3NQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r594872175", "bodyText": "It's mostly because AutoValue will complain if it's not set (e.g. if not using the storage api)", "author": "reuvenlax", "createdAt": "2021-03-16T05:30:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc3NjkyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc3ODU1Ng==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593778556", "bodyText": "Replace \"StorageApi\" with \"WriteApi\" for clarity here and elsewhere ? I'm not sure what the correct terminology is (Storage API or Write API) but we use term \"Storage API\" for the Read API [1] elsewhere.\n[1] https://cloud.google.com/bigquery/docs/reference/storage", "author": "chamikaramj", "createdAt": "2021-03-13T17:34:52Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.java", "diffHunk": "@@ -1771,6 +1773,8 @@ static String getExtractDestinationUri(String extractDestinationDir) {\n \n     abstract int getNumFileShards();\n \n+    abstract int getNumStorageApiStreams();", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDg3MjIwNQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r594872205", "bodyText": "The full name is the Storage Write API. Updated the public API to use that name", "author": "reuvenlax", "createdAt": "2021-03-16T05:30:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc3ODU1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc3OTIyMQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593779221", "bodyText": "Ditto regarding checking naming.", "author": "chamikaramj", "createdAt": "2021-03-13T17:40:17Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryOptions.java", "diffHunk": "@@ -78,4 +78,19 @@\n   Integer getBqStreamingApiLoggingFrequencySec();\n \n   void setBqStreamingApiLoggingFrequencySec(Integer value);\n+\n+  @Description(\"If set, then BigQueryIO.Write will default to using the Storage API.\")\n+  @Default.Boolean(false)\n+  Boolean getUseStorageApiWrites();", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDg3MjIxNQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r594872215", "bodyText": "done", "author": "reuvenlax", "createdAt": "2021-03-16T05:30:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc3OTIyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc4MDE5Ng==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593780196", "bodyText": "Probably cleaner to move these methods to a new interface ?", "author": "chamikaramj", "createdAt": "2021-03-13T17:49:33Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryServices.java", "diffHunk": "@@ -164,6 +172,47 @@ void createDataset(\n     /** Patch BigQuery {@link Table} description. */\n     Table patchTableDescription(TableReference tableReference, @Nullable String tableDescription)\n         throws IOException, InterruptedException;\n+\n+    /** Create a Write Stream for use with the the Storage Write API. */\n+    WriteStream createWriteStream(String tableUrn, WriteStream.Type type)\n+        throws IOException, InterruptedException;\n+\n+    /**\n+     * Create an append client for a given Storage API write stream. The stream must be created\n+     * first.\n+     */\n+    StreamAppendClient getStreamAppendClient(String streamName) throws Exception;\n+\n+    /** Flush a given stream up to the given offset. The stream must have type BUFFERED. */\n+    ApiFuture<FlushRowsResponse> flush(String streamName, long flushOffset)\n+        throws IOException, InterruptedException;\n+\n+    /**", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc5MTExMA==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593791110", "bodyText": "Are these methods thread safe ? Probably we should clarify here.", "author": "chamikaramj", "createdAt": "2021-03-13T19:30:34Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryServices.java", "diffHunk": "@@ -164,6 +172,47 @@ void createDataset(\n     /** Patch BigQuery {@link Table} description. */\n     Table patchTableDescription(TableReference tableReference, @Nullable String tableDescription)\n         throws IOException, InterruptedException;\n+\n+    /** Create a Write Stream for use with the the Storage Write API. */\n+    WriteStream createWriteStream(String tableUrn, WriteStream.Type type)\n+        throws IOException, InterruptedException;\n+\n+    /**\n+     * Create an append client for a given Storage API write stream. The stream must be created\n+     * first.\n+     */\n+    StreamAppendClient getStreamAppendClient(String streamName) throws Exception;\n+\n+    /** Flush a given stream up to the given offset. The stream must have type BUFFERED. */\n+    ApiFuture<FlushRowsResponse> flush(String streamName, long flushOffset)\n+        throws IOException, InterruptedException;\n+\n+    /**\n+     * Finalize a write stream. After finalization, no more records can be appended to the stream.\n+     */\n+    ApiFuture<FinalizeWriteStreamResponse> finalizeWriteStream(String streamName);\n+\n+    /** Commit write streams of type PENDING. The streams must be finalized before committing. */\n+    ApiFuture<BatchCommitWriteStreamsResponse> commitWriteStreams(\n+        String tableUrn, Iterable<String> writeStreamNames);\n+  }\n+\n+  /** An interface for appending records to a Storage API write stream. */\n+  interface StreamAppendClient extends AutoCloseable {", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc5MTk0Mw==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593791943", "bodyText": "Does offset==0 mean the current offset or do we have to explicitly set it to 0 ?", "author": "chamikaramj", "createdAt": "2021-03-13T19:38:53Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryServicesImpl.java", "diffHunk": "@@ -1055,6 +1084,104 @@ public Table patchTableDescription(\n           createDefaultBackoff(),\n           ALWAYS_RETRY);\n     }\n+\n+    @Override\n+    public WriteStream createWriteStream(String tableUrn, WriteStream.Type type)\n+        throws IOException {\n+      return newWriteClient.createWriteStream(\n+          CreateWriteStreamRequest.newBuilder()\n+              .setParent(tableUrn)\n+              .setWriteStream(WriteStream.newBuilder().setType(type).build())\n+              .build());\n+    }\n+\n+    @Override\n+    public StreamAppendClient getStreamAppendClient(String streamName) throws Exception {\n+      StreamWriterV2 streamWriter = StreamWriterV2.newBuilder(streamName).build();\n+      return new StreamAppendClient() {\n+        private int pins = 0;\n+        private boolean closed = false;\n+\n+        @Override\n+        public void close() throws Exception {\n+          boolean closeWriter;\n+          synchronized (this) {\n+            Preconditions.checkState(!closed);\n+            closed = true;\n+            closeWriter = (pins == 0);\n+          }\n+          if (closeWriter) {\n+            streamWriter.close();\n+          }\n+        }\n+\n+        @Override\n+        public void pin() {\n+          synchronized (this) {\n+            Preconditions.checkState(!closed);\n+            ++pins;\n+          }\n+        }\n+\n+        @Override\n+        public void unpin() throws Exception {\n+          boolean closeWriter;\n+          synchronized (this) {\n+            Preconditions.checkState(pins > 0);\n+            --pins;\n+            closeWriter = (pins == 0) && closed;\n+          }\n+          if (closeWriter) {\n+            streamWriter.close();\n+          }\n+        }\n+\n+        @Override\n+        public ApiFuture<AppendRowsResponse> appendRows(\n+            long offset, ProtoRows rows, Descriptor descriptor) throws Exception {\n+          final AppendRowsRequest.ProtoData data =\n+              AppendRowsRequest.ProtoData.newBuilder()\n+                  .setWriterSchema(\n+                      ProtoSchema.newBuilder().setProtoDescriptor(descriptor.toProto()).build())\n+                  .setRows(rows)\n+                  .build();\n+          AppendRowsRequest.Builder appendRequestBuilder =\n+              AppendRowsRequest.newBuilder().setProtoRows(data).setWriteStream(streamName);\n+          if (offset >= 0) {\n+            appendRequestBuilder = appendRequestBuilder.setOffset(Int64Value.of(offset));\n+          }\n+          return streamWriter.append(appendRequestBuilder.build());", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc5MjE2NQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593792165", "bodyText": "Could you clarify what this means ? I'm bit confused since flushing a stream is usually not conditional on an offset.", "author": "chamikaramj", "createdAt": "2021-03-13T19:40:51Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryServices.java", "diffHunk": "@@ -164,6 +172,47 @@ void createDataset(\n     /** Patch BigQuery {@link Table} description. */\n     Table patchTableDescription(TableReference tableReference, @Nullable String tableDescription)\n         throws IOException, InterruptedException;\n+\n+    /** Create a Write Stream for use with the the Storage Write API. */\n+    WriteStream createWriteStream(String tableUrn, WriteStream.Type type)\n+        throws IOException, InterruptedException;\n+\n+    /**\n+     * Create an append client for a given Storage API write stream. The stream must be created\n+     * first.\n+     */\n+    StreamAppendClient getStreamAppendClient(String streamName) throws Exception;\n+\n+    /** Flush a given stream up to the given offset. The stream must have type BUFFERED. */", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NjUwOTM0MA==", "url": "https://github.com/apache/beam/pull/11767#discussion_r596509340", "bodyText": "Ping regarding previous comments for this class (feel free to just resolve if you think the suggestion is not needed).", "author": "chamikaramj", "createdAt": "2021-03-18T02:31:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc5MjE2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5Mzc5MjI4OA==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593792288", "bodyText": "Do we want to support more customization here (in the future) ?", "author": "chamikaramj", "createdAt": "2021-03-13T19:42:09Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/RetryManager.java", "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.gcp.bigquery;\n+\n+import com.google.api.core.ApiFuture;\n+import com.google.api.core.ApiFutureCallback;\n+import com.google.api.core.ApiFutures;\n+import java.util.Queue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.io.gcp.bigquery.RetryManager.Operation.Context;\n+import org.apache.beam.sdk.util.BackOff;\n+import org.apache.beam.sdk.util.BackOffUtils;\n+import org.apache.beam.sdk.util.FluentBackoff;\n+import org.apache.beam.sdk.util.Sleeper;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Queues;\n+import org.joda.time.Duration;\n+\n+/**\n+ * Retry manager used by Storage API operations. This class manages a sequence of operations (e.g.\n+ * sequential appends to a stream) and retries of those operations.\n+ */\n+class RetryManager<ResultT, ContextT extends Context<ResultT>> {\n+  private Queue<Operation<ResultT, ContextT>> operations;\n+  private final BackOff backoff;\n+  private final ExecutorService executor;\n+\n+  enum RetryType {\n+    DONT_RETRY,", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgyOTI2OQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593829269", "bodyText": "Probably cleaner to explicitly check for \"RetryType.DONT_RETRY\" here and fail for else clause.", "author": "chamikaramj", "createdAt": "2021-03-14T02:19:40Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/RetryManager.java", "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.gcp.bigquery;\n+\n+import com.google.api.core.ApiFuture;\n+import com.google.api.core.ApiFutureCallback;\n+import com.google.api.core.ApiFutures;\n+import java.util.Queue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.io.gcp.bigquery.RetryManager.Operation.Context;\n+import org.apache.beam.sdk.util.BackOff;\n+import org.apache.beam.sdk.util.BackOffUtils;\n+import org.apache.beam.sdk.util.FluentBackoff;\n+import org.apache.beam.sdk.util.Sleeper;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Queues;\n+import org.joda.time.Duration;\n+\n+/**\n+ * Retry manager used by Storage API operations. This class manages a sequence of operations (e.g.\n+ * sequential appends to a stream) and retries of those operations.\n+ */\n+class RetryManager<ResultT, ContextT extends Context<ResultT>> {\n+  private Queue<Operation<ResultT, ContextT>> operations;\n+  private final BackOff backoff;\n+  private final ExecutorService executor;\n+\n+  enum RetryType {\n+    DONT_RETRY,\n+    RETRY_ALL_OPERATIONS\n+  };\n+\n+  RetryManager(Duration initialBackoff, Duration maxBackoff, int maxRetries) {\n+    this.operations = Queues.newArrayDeque();\n+    backoff =\n+        FluentBackoff.DEFAULT\n+            .withInitialBackoff(initialBackoff)\n+            .withMaxBackoff(maxBackoff)\n+            .withMaxRetries(maxRetries)\n+            .backoff();\n+    this.executor = Executors.newCachedThreadPool();\n+  }\n+\n+  static class Operation<ResultT, ContextT extends Context<ResultT>> {\n+    static class Context<ResultT> {\n+      private @Nullable Throwable error = null;\n+      private @Nullable ResultT result = null;\n+\n+      public void setError(@Nullable Throwable error) {\n+        this.error = error;\n+      }\n+\n+      public @Nullable Throwable getError() {\n+        return error;\n+      }\n+\n+      public void setResult(@Nullable ResultT result) {\n+        this.result = result;\n+      }\n+\n+      public @Nullable ResultT getResult() {\n+        return result;\n+      }\n+    }\n+\n+    private final Function<ContextT, ApiFuture<ResultT>> runOperation;\n+    private final Function<Iterable<ContextT>, RetryType> onError;\n+    private final Consumer<ContextT> onSuccess;\n+    @Nullable private ApiFuture<ResultT> future = null;\n+    @Nullable private Callback<ResultT> callback = null;\n+    @Nullable ContextT context = null;\n+\n+    public Operation(\n+        Function<ContextT, ApiFuture<ResultT>> runOperation,\n+        Function<Iterable<ContextT>, RetryType> onError,\n+        Consumer<ContextT> onSuccess,\n+        ContextT context) {\n+      this.runOperation = runOperation;\n+      this.onError = onError;\n+      this.onSuccess = onSuccess;\n+      this.context = context;\n+    }\n+\n+    @SuppressWarnings({\"nullness\"})\n+    void run(Executor executor) {\n+      this.future = runOperation.apply(context);\n+      this.callback = new Callback<>();\n+      ApiFutures.addCallback(future, callback, executor);\n+    }\n+\n+    @SuppressWarnings({\"nullness\"})\n+    boolean await() throws Exception {\n+      callback.await();\n+      return callback.getFailed();\n+    }\n+  }\n+\n+  private static class Callback<ResultT> implements ApiFutureCallback<ResultT> {\n+    private final CountDownLatch waiter;\n+    @Nullable private Throwable failure = null;\n+    boolean failed = false;\n+\n+    Callback() {\n+      this.waiter = new CountDownLatch(1);\n+    }\n+\n+    void await() throws InterruptedException {\n+      waiter.await();\n+    }\n+\n+    boolean await(long timeoutSec) throws InterruptedException {\n+      return waiter.await(timeoutSec, TimeUnit.SECONDS);\n+    }\n+\n+    @Override\n+    public void onFailure(Throwable t) {\n+      synchronized (this) {\n+        failure = t;\n+        failed = true;\n+      }\n+      waiter.countDown();\n+    }\n+\n+    @Override\n+    public void onSuccess(ResultT result) {\n+      synchronized (this) {\n+        failure = null;\n+      }\n+      waiter.countDown();\n+    }\n+\n+    @Nullable\n+    Throwable getFailure() {\n+      synchronized (this) {\n+        return failure;\n+      }\n+    }\n+\n+    boolean getFailed() {\n+      synchronized (this) {\n+        return failed;\n+      }\n+    }\n+  }\n+\n+  void addOperation(\n+      Function<ContextT, ApiFuture<ResultT>> runOperation,\n+      Function<Iterable<ContextT>, RetryType> onError,\n+      Consumer<ContextT> onSuccess,\n+      ContextT context)\n+      throws Exception {\n+    addOperation(new Operation<>(runOperation, onError, onSuccess, context));\n+  }\n+\n+  void addAndRunOperation(\n+      Function<ContextT, ApiFuture<ResultT>> runOperation,\n+      Function<Iterable<ContextT>, RetryType> onError,\n+      Consumer<ContextT> onSuccess,\n+      ContextT context)\n+      throws Exception {\n+    addAndRunOperation(new Operation<>(runOperation, onError, onSuccess, context));\n+  }\n+\n+  void addOperation(Operation<ResultT, ContextT> operation) {\n+    operations.add(operation);\n+  }\n+\n+  void addAndRunOperation(Operation<ResultT, ContextT> operation) {\n+    operation.run(executor);\n+    operations.add(operation);\n+  }\n+\n+  void run(boolean await) throws Exception {\n+    for (Operation<ResultT, ContextT> operation : operations) {\n+      operation.run(executor);\n+    }\n+    if (await) {\n+      await();\n+    }\n+  }\n+\n+  @SuppressWarnings({\"nullness\"})\n+  void await() throws Exception {\n+    while (!this.operations.isEmpty()) {\n+      Operation<ResultT, ContextT> operation = this.operations.element();\n+      boolean failed = operation.await();\n+      if (failed) {\n+        Throwable failure = operation.callback.getFailure();\n+        operation.context.setError(failure);\n+        RetryType retryType =\n+            operation.onError.apply(\n+                operations.stream().map(o -> o.context).collect(Collectors.toList()));\n+        if (retryType != RetryType.DONT_RETRY) {\n+          Preconditions.checkState(RetryType.RETRY_ALL_OPERATIONS == retryType);\n+          if (!BackOffUtils.next(Sleeper.DEFAULT, backoff)) {\n+            throw new RuntimeException(failure);\n+          }\n+          for (Operation<ResultT, ?> awaitOperation : operations) {\n+            awaitOperation.await();\n+          }\n+          // Run all the operations again.\n+          run(false);\n+        } else {", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDg3MjI1OA==", "url": "https://github.com/apache/beam/pull/11767#discussion_r594872258", "bodyText": "done", "author": "reuvenlax", "createdAt": "2021-03-16T05:30:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgyOTI2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgzMjk5MA==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593832990", "bodyText": "This sounds strange. Does \"Code.INVALID_ARGUMENT\" always mean already finalized ? I wonder if we'll end up swallowing some valid errors here.", "author": "chamikaramj", "createdAt": "2021-03-14T03:03:54Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiFlushAndFinalizeDoFn.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.gcp.bigquery;\n+\n+import com.google.api.gax.rpc.ApiException;\n+import com.google.api.gax.rpc.StatusCode.Code;\n+import com.google.cloud.bigquery.storage.v1beta2.FinalizeWriteStreamResponse;\n+import com.google.cloud.bigquery.storage.v1beta2.FlushRowsResponse;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.time.Instant;\n+import java.util.Objects;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.io.gcp.bigquery.BigQueryServices.DatasetService;\n+import org.apache.beam.sdk.io.gcp.bigquery.RetryManager.Operation.Context;\n+import org.apache.beam.sdk.io.gcp.bigquery.RetryManager.RetryType;\n+import org.apache.beam.sdk.io.gcp.bigquery.StorageApiFlushAndFinalizeDoFn.Operation;\n+import org.apache.beam.sdk.metrics.Counter;\n+import org.apache.beam.sdk.metrics.Distribution;\n+import org.apache.beam.sdk.metrics.Metrics;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.schemas.JavaFieldSchema;\n+import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n+import org.apache.beam.sdk.schemas.annotations.SchemaCreate;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** This DoFn flushes and optionally (if requested) finalizes Storage API streams. */\n+public class StorageApiFlushAndFinalizeDoFn extends DoFn<KV<String, Operation>, Void> {\n+  private static final Logger LOG = LoggerFactory.getLogger(StorageApiFlushAndFinalizeDoFn.class);\n+\n+  private final BigQueryServices bqServices;\n+  @Nullable private DatasetService datasetService = null;\n+  private final Counter flushOperationsSent =\n+      Metrics.counter(StorageApiFlushAndFinalizeDoFn.class, \"flushOperationsSent\");\n+  private final Counter flushOperationsSucceeded =\n+      Metrics.counter(StorageApiFlushAndFinalizeDoFn.class, \"flushOperationsSucceeded\");\n+  private final Counter flushOperationsFailed =\n+      Metrics.counter(StorageApiFlushAndFinalizeDoFn.class, \"flushOperationsFailed\");\n+  private final Counter flushOperationsAlreadyExists =\n+      Metrics.counter(StorageApiFlushAndFinalizeDoFn.class, \"flushOperationsAlreadyExists\");\n+  private final Counter flushOperationsInvalidArgument =\n+      Metrics.counter(StorageApiFlushAndFinalizeDoFn.class, \"flushOperationsInvalidArgument\");\n+  private final Distribution flushLatencyDistribution =\n+      Metrics.distribution(StorageApiFlushAndFinalizeDoFn.class, \"flushOperationLatencyMs\");\n+  private final Counter finalizeOperationsSent =\n+      Metrics.counter(StorageApiFlushAndFinalizeDoFn.class, \"finalizeOperationsSent\");\n+  private final Counter finalizeOperationsSucceeded =\n+      Metrics.counter(StorageApiFlushAndFinalizeDoFn.class, \"finalizeOperationsSucceeded\");\n+  private final Counter finalizeOperationsFailed =\n+      Metrics.counter(StorageApiFlushAndFinalizeDoFn.class, \"finalizeOperationsFailed\");\n+\n+  @DefaultSchema(JavaFieldSchema.class)\n+  static class Operation implements Comparable<Operation>, Serializable {\n+    final long flushOffset;\n+    final boolean finalizeStream;\n+\n+    @SchemaCreate\n+    public Operation(long flushOffset, boolean finalizeStream) {\n+      this.flushOffset = flushOffset;\n+      this.finalizeStream = finalizeStream;\n+    }\n+\n+    @Override\n+    public boolean equals(@Nullable Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+      Operation operation = (Operation) o;\n+      return flushOffset == operation.flushOffset && finalizeStream == operation.finalizeStream;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hash(flushOffset, finalizeStream);\n+    }\n+\n+    @Override\n+    public int compareTo(Operation other) {\n+      int compValue = Long.compare(this.flushOffset, other.flushOffset);\n+      if (compValue == 0) {\n+        compValue = Boolean.compare(this.finalizeStream, other.finalizeStream);\n+      }\n+      return compValue;\n+    }\n+  }\n+\n+  public StorageApiFlushAndFinalizeDoFn(BigQueryServices bqServices) {\n+    this.bqServices = bqServices;\n+  }\n+\n+  private DatasetService getDatasetService(PipelineOptions pipelineOptions) throws IOException {\n+    if (datasetService == null) {\n+      datasetService = bqServices.getDatasetService(pipelineOptions.as(BigQueryOptions.class));\n+    }\n+    return datasetService;\n+  }\n+\n+  @SuppressWarnings({\"nullness\"})\n+  @ProcessElement\n+  public void process(PipelineOptions pipelineOptions, @Element KV<String, Operation> element)\n+      throws Exception {\n+    final String streamId = element.getKey();\n+    final Operation operation = element.getValue();\n+    final DatasetService datasetService = getDatasetService(pipelineOptions);\n+    // Flush the stream. If the flush offset < 0, that means we only need to finalize.\n+    long offset = operation.flushOffset;\n+    if (offset >= 0) {\n+      Instant now = Instant.now();\n+      RetryManager<FlushRowsResponse, Context<FlushRowsResponse>> retryManager =\n+          new RetryManager<>(Duration.standardSeconds(1), Duration.standardMinutes(1), 3);\n+      retryManager.addOperation(\n+          // runOperation\n+          c -> {\n+            try {\n+              flushOperationsSent.inc();\n+              return datasetService.flush(streamId, offset);\n+            } catch (Exception e) {\n+              throw new RuntimeException(e);\n+            }\n+          },\n+          // onError\n+          contexts -> {\n+            Throwable error = Iterables.getFirst(contexts, null).getError();\n+            LOG.warn(\n+                \"Flush of stream \" + streamId + \" to offset \" + offset + \" failed with \" + error);\n+            flushOperationsFailed.inc();\n+            if (error instanceof ApiException) {\n+              Code statusCode = ((ApiException) error).getStatusCode().getCode();\n+              if (statusCode.equals(Code.ALREADY_EXISTS)) {\n+                flushOperationsAlreadyExists.inc();\n+                // Implies that we have already flushed up to this point, so don't retry.\n+                return RetryType.DONT_RETRY;\n+              }\n+              if (statusCode.equals(Code.INVALID_ARGUMENT)) {", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDg3MjI2Nw==", "url": "https://github.com/apache/beam/pull/11767#discussion_r594872267", "bodyText": "Agreed, I would like a better error code as per the TODO. However right now I don't see a better way of doing this.", "author": "reuvenlax", "createdAt": "2021-03-16T05:31:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzgzMjk5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkyNDk1NA==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593924954", "bodyText": "Why do we need triggered writes for Storage API ? Is batching at the write DoFn inadequate for some reason.", "author": "chamikaramj", "createdAt": "2021-03-14T16:20:16Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/StorageApiLoads.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.gcp.bigquery;\n+\n+import com.google.api.services.bigquery.model.TableRow;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.ThreadLocalRandom;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.windowing.AfterFirst;\n+import org.apache.beam.sdk.transforms.windowing.AfterPane;\n+import org.apache.beam.sdk.transforms.windowing.AfterProcessingTime;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindows;\n+import org.apache.beam.sdk.transforms.windowing.Repeatedly;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.util.ShardedKey;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TupleTag;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** This {@link PTransform} manages loads into BigQuery using the Storage API. */\n+public class StorageApiLoads<DestinationT, ElementT>\n+    extends PTransform<PCollection<KV<DestinationT, ElementT>>, WriteResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(StorageApiLoads.class);\n+  static final int FILE_TRIGGERING_RECORD_COUNT = 100;\n+\n+  private final Coder<DestinationT> destinationCoder;\n+  private final Coder<ElementT> elementCoder;\n+  private final StorageApiDynamicDestinations<ElementT, DestinationT> dynamicDestinations;\n+  private final CreateDisposition createDisposition;\n+  private final String kmsKey;\n+  private final Duration triggeringFrequency;\n+  private final BigQueryServices bqServices;\n+  private final int numShards;\n+\n+  public StorageApiLoads(\n+      Coder<DestinationT> destinationCoder,\n+      Coder<ElementT> elementCoder,\n+      StorageApiDynamicDestinations<ElementT, DestinationT> dynamicDestinations,\n+      CreateDisposition createDisposition,\n+      String kmsKey,\n+      Duration triggeringFrequency,\n+      BigQueryServices bqServices,\n+      int numShards) {\n+    this.destinationCoder = destinationCoder;\n+    this.elementCoder = elementCoder;\n+    this.dynamicDestinations = dynamicDestinations;\n+    this.createDisposition = createDisposition;\n+    this.kmsKey = kmsKey;\n+    this.triggeringFrequency = triggeringFrequency;\n+    this.bqServices = bqServices;\n+    this.numShards = numShards;\n+  }\n+\n+  @Override\n+  public WriteResult expand(PCollection<KV<DestinationT, ElementT>> input) {\n+    return triggeringFrequency != null ? expandTriggered(input) : expandUntriggered(input);\n+  }\n+\n+  public WriteResult expandTriggered(PCollection<KV<DestinationT, ElementT>> input) {", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDg3MjI4MQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r594872281", "bodyText": "I'm not sure what batching at the write DoFn you're referring to. Eventually I would like to support auto sharding using GroupIntoBatches, however this is not yet ready for use by this sink", "author": "reuvenlax", "createdAt": "2021-03-16T05:31:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkyNDk1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NjQ5NjM0MA==", "url": "https://github.com/apache/beam/pull/11767#discussion_r596496340", "bodyText": "So, I'm not sure why we need a triggering frequency option for write API. Seems like it's similar to streaming inserts where we can continuously write records (or batches of records) unlike FILE_LOAD path where we have to trigger regular load jobs. I might not be understanding something related to the way Write API works.", "author": "chamikaramj", "createdAt": "2021-03-18T01:48:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkyNDk1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkyNzY4MA==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593927680", "bodyText": "Having both \"getTableSpec\" and \"getTableUrn\" can be confusing. At least we should clearly document the difference.", "author": "chamikaramj", "createdAt": "2021-03-14T16:39:51Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/TableDestination.java", "diffHunk": "@@ -123,6 +123,13 @@ public String getTableSpec() {\n     return tableSpec;\n   }\n \n+  public String getTableUrn() {", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDg3MjI5Nw==", "url": "https://github.com/apache/beam/pull/11767#discussion_r594872297", "bodyText": "done", "author": "reuvenlax", "createdAt": "2021-03-16T05:31:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkyNzY4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkzMDU2Ng==", "url": "https://github.com/apache/beam/pull/11767#discussion_r593930566", "bodyText": "I didn't look into tests in detail yet but given the extremely large change and number of potential users, we should try to get close to 100% code coverage through unit testing and 100% test parity with existing streaming inserts tests (both unit and integration).", "author": "chamikaramj", "createdAt": "2021-03-14T17:02:09Z", "path": "sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOWriteTest.java", "diffHunk": "@@ -135,16 +135,36 @@\n import org.junit.rules.TestRule;\n import org.junit.runner.Description;\n import org.junit.runner.RunWith;\n-import org.junit.runners.JUnit4;\n+import org.junit.runners.Parameterized;", "originalCommit": "deda8d18e01adeda856bb1368844f143c6e99f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDg3MjMwNg==", "url": "https://github.com/apache/beam/pull/11767#discussion_r594872306", "bodyText": "AFIACT all tests are run against the new path, except for the ones that don't make sense (e.g. the ones that test specific functionality that doesn't exist in the storage api path)", "author": "reuvenlax", "createdAt": "2021-03-16T05:31:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MzkzMDU2Ng=="}], "type": "inlineReview"}, {"oid": "874941d4ead00f571e60ac1ea42d61e418f5a983", "url": "https://github.com/apache/beam/commit/874941d4ead00f571e60ac1ea42d61e418f5a983", "message": "Vortex sink", "committedDate": "2021-03-17T03:30:18Z", "type": "commit"}, {"oid": "42ead01e2d21025d67d6ee60191343343382c993", "url": "https://github.com/apache/beam/commit/42ead01e2d21025d67d6ee60191343343382c993", "message": "spotless", "committedDate": "2021-03-17T06:03:17Z", "type": "commit"}, {"oid": "42ead01e2d21025d67d6ee60191343343382c993", "url": "https://github.com/apache/beam/commit/42ead01e2d21025d67d6ee60191343343382c993", "message": "spotless", "committedDate": "2021-03-17T06:03:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NjUwODQ4Mg==", "url": "https://github.com/apache/beam/pull/11767#discussion_r596508482", "bodyText": "Should we add such \"useXYZ\" options for other read/write modes as well ? (to make the API consistent).", "author": "chamikaramj", "createdAt": "2021-03-18T02:28:52Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryOptions.java", "diffHunk": "@@ -78,4 +78,23 @@\n   Integer getBqStreamingApiLoggingFrequencySec();\n \n   void setBqStreamingApiLoggingFrequencySec(Integer value);\n+\n+  @Description(\"If set, then BigQueryIO.Write will default to using the Storage Write API.\")\n+  @Default.Boolean(false)\n+  Boolean getUseStorageWriteApi();", "originalCommit": "42ead01e2d21025d67d6ee60191343343382c993", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI3ODYwMQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r597278601", "bodyText": "I'd rather not in this PR, though we can look at it in a followup", "author": "reuvenlax", "createdAt": "2021-03-18T22:15:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NjUwODQ4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NjUyMTYwOQ==", "url": "https://github.com/apache/beam/pull/11767#discussion_r596521609", "bodyText": "Add unit tests for new RetryManager ?", "author": "chamikaramj", "createdAt": "2021-03-18T03:10:06Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/RetryManager.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file", "originalCommit": "42ead01e2d21025d67d6ee60191343343382c993", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzI3ODMzMw==", "url": "https://github.com/apache/beam/pull/11767#discussion_r597278333", "bodyText": "done", "author": "reuvenlax", "createdAt": "2021-03-18T22:14:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NjUyMTYwOQ=="}], "type": "inlineReview"}, {"oid": "896b2847eef25f91bc1ad8ada9673fe1ffd9b6d8", "url": "https://github.com/apache/beam/commit/896b2847eef25f91bc1ad8ada9673fe1ffd9b6d8", "message": "address comments", "committedDate": "2021-03-18T22:14:27Z", "type": "commit"}, {"oid": "6fc9e40311e555ba85f386199b7353fa20cedd31", "url": "https://github.com/apache/beam/commit/6fc9e40311e555ba85f386199b7353fa20cedd31", "message": "tests", "committedDate": "2021-03-19T22:25:10Z", "type": "commit"}, {"oid": "ee385410e238a656ca002dde3de87c933e7b0d48", "url": "https://github.com/apache/beam/commit/ee385410e238a656ca002dde3de87c933e7b0d48", "message": "update integration test", "committedDate": "2021-03-19T22:32:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMTgwNTI3Ng==", "url": "https://github.com/apache/beam/pull/11767#discussion_r601805276", "bodyText": "Drive-by comment: Would this be misleading? Since auto-sharding is not yet integrated.", "author": "nehsyc", "createdAt": "2021-03-25T20:08:06Z", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.java", "diffHunk": "@@ -2285,6 +2292,19 @@ static String getExtractDestinationUri(String extractDestinationDir) {\n       return toBuilder().setNumFileShards(numFileShards).build();\n     }\n \n+    /**\n+     * Control how many parallel streams are used when using Storage API writes. Applicable only\n+     * when also setting {@link #withTriggeringFrequency}. To let runner determine the sharding at\n+     * runtime, set {@link #withAutoSharding()} instead.", "originalCommit": "ee385410e238a656ca002dde3de87c933e7b0d48", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}