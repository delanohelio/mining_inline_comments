{"pr_number": 12436, "pr_title": "[BEAM-9891] TPC-DS module initialization, tables and queries stored", "pr_createdAt": "2020-07-31T18:15:40Z", "pr_url": "https://github.com/apache/beam/pull/12436", "timeline": [{"oid": "450dbd262989eed1f569cd9ed0ac87bfcf790b3d", "url": "https://github.com/apache/beam/commit/450dbd262989eed1f569cd9ed0ac87bfcf790b3d", "message": "Added initial tests for TPC-DS on both direct runner and dataflow runner", "committedDate": "2020-07-19T07:09:19Z", "type": "commit"}, {"oid": "f36bbc5b6adde7e48b32898490ea6c9acff64aa4", "url": "https://github.com/apache/beam/commit/f36bbc5b6adde7e48b32898490ea6c9acff64aa4", "message": "[BEAM-9891] TPC-DS module init, table schemas and queries stored", "committedDate": "2020-07-31T06:47:46Z", "type": "commit"}, {"oid": "3d845264db21c422fff56dd4974aca2503952fbf", "url": "https://github.com/apache/beam/commit/3d845264db21c422fff56dd4974aca2503952fbf", "message": "Merge branch 'master' into tpcds-dev", "committedDate": "2020-07-31T18:25:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc2OTA4Nw==", "url": "https://github.com/apache/beam/pull/12436#discussion_r463769087", "bodyText": "let's avoid using the wildcard imports, you might run ./gradlew -p sdks/java/testing/tpcds/ check to have a checkstyle check : )", "author": "vectorijk", "createdAt": "2020-07-31T18:35:59Z", "path": "sdks/java/testing/tpcds/src/main/java/org/apache/beam/sdk/tpcds/BeamTpcds.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.tpcds;\n+\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.text.TextTableProvider;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+import org.apache.beam.sdk.extensions.sql.meta.store.InMemoryMetaStore;\n+import org.apache.beam.sdk.extensions.sql.impl.BeamSqlEnv;\n+import org.apache.beam.sdk.extensions.sql.impl.rel.BeamSqlRelUtils;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import java.util.List;\n+import java.util.concurrent.*;", "originalCommit": "3d845264db21c422fff56dd4974aca2503952fbf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc2OTQzMQ==", "url": "https://github.com/apache/beam/pull/12436#discussion_r463769431", "bodyText": "ditto", "author": "vectorijk", "createdAt": "2020-07-31T18:36:45Z", "path": "sdks/java/testing/tpcds/src/main/java/org/apache/beam/sdk/tpcds/QueryReader.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.tpcds;\n+\n+import java.io.*;", "originalCommit": "3d845264db21c422fff56dd4974aca2503952fbf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ebcaa29220f80d3a5c309b14ea90c80bb1705ddd", "url": "https://github.com/apache/beam/commit/ebcaa29220f80d3a5c309b14ea90c80bb1705ddd", "message": "Removed wildcard imports", "committedDate": "2020-07-31T19:19:11Z", "type": "commit"}, {"oid": "3ba3acdcef9ad5b6898b18fd4ca8d9cba7a18231", "url": "https://github.com/apache/beam/commit/3ba3acdcef9ad5b6898b18fd4ca8d9cba7a18231", "message": "Merge branch 'master' into tpcds-dev", "committedDate": "2020-07-31T19:25:24Z", "type": "commit"}, {"oid": "8b395ab593a821f2af1777e96c899be83211afcc", "url": "https://github.com/apache/beam/commit/8b395ab593a821f2af1777e96c899be83211afcc", "message": "Fixed settings.gradle", "committedDate": "2020-07-31T19:36:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDExMjQ1NQ==", "url": "https://github.com/apache/beam/pull/12436#discussion_r464112455", "bodyText": "seems to me that this is not the place where you should handle \"\" to null conversion.", "author": "amaliujia", "createdAt": "2020-08-02T19:08:28Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/schema/BeamTableUtils.java", "diffHunk": "@@ -149,13 +149,35 @@ public static Object autoCastField(Schema.Field field, Object rawObj) {\n         case INT16:\n           return Short.valueOf(raw);\n         case INT32:\n+          if (raw.equals(\"\")) {", "originalCommit": "8b395ab593a821f2af1777e96c899be83211afcc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDEyNzI2MA==", "url": "https://github.com/apache/beam/pull/12436#discussion_r464127260", "bodyText": "Removed modifications on case INT32, FLOAT and DECIMAL.", "author": "Imfuyuwei", "createdAt": "2020-08-02T21:47:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDExMjQ1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDExMjQ5OA==", "url": "https://github.com/apache/beam/pull/12436#discussion_r464112498", "bodyText": "remove useless comment.", "author": "amaliujia", "createdAt": "2020-08-02T19:08:54Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/schema/BeamTableUtils.java", "diffHunk": "@@ -149,13 +149,35 @@ public static Object autoCastField(Schema.Field field, Object rawObj) {\n         case INT16:\n           return Short.valueOf(raw);\n         case INT32:\n+          if (raw.equals(\"\")) {\n+            return null;\n+          }\n           return Integer.valueOf(raw);\n         case INT64:\n+          if (raw.equals(\"\")) {\n+            return null;\n+          }\n           return Long.valueOf(raw);\n         case FLOAT:\n+          if (raw.equals(\"\")) {\n+            return null;\n+          }\n           return Float.valueOf(raw);\n         case DOUBLE:\n+          if (raw.equals(\"\")) {\n+            return null;\n+          }\n           return Double.valueOf(raw);\n+          //          BigDecimal bdvalue = new BigDecimal(raw);", "originalCommit": "8b395ab593a821f2af1777e96c899be83211afcc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDExMjcxMw==", "url": "https://github.com/apache/beam/pull/12436#discussion_r464112713", "bodyText": "Can you remove such gs://beamsql_tpcds_1/tpcds_results/ to some static pulic final String?", "author": "amaliujia", "createdAt": "2020-08-02T19:10:17Z", "path": "sdks/java/testing/tpcds/src/main/java/org/apache/beam/sdk/tpcds/BeamTpcds.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.tpcds;\n+\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.text.TextTableProvider;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+import org.apache.beam.sdk.extensions.sql.meta.store.InMemoryMetaStore;\n+import org.apache.beam.sdk.extensions.sql.impl.BeamSqlEnv;\n+import org.apache.beam.sdk.extensions.sql.impl.rel.BeamSqlRelUtils;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import java.util.List;\n+import java.util.concurrent.CompletionService;\n+import java.util.concurrent.ExecutorCompletionService;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+\n+/**\n+ * To execute this main() method, run the following example command from the command line.\n+ *\n+ * ./gradlew :sdks:java:testing:tpcds:run -Ptpcds.args=\"--dataSize=1G \\\n+ *         --queries=3,26,55 \\\n+ *         --tpcParallel=2 \\\n+ *         --project=apache-beam-testing \\\n+ *         --stagingLocation=gs://beamsql_tpcds_1/staging \\\n+ *         --tempLocation=gs://beamsql_tpcds_2/temp \\\n+ *         --runner=DataflowRunner \\\n+ *         --region=us-west1 \\\n+ *         --maxNumWorkers=10\"\n+ */\n+public class BeamTpcds {\n+    public static void main(String[] args) throws Exception {\n+        InMemoryMetaStore inMemoryMetaStore = new InMemoryMetaStore();\n+        inMemoryMetaStore.registerProvider(new TextTableProvider());\n+\n+        TpcdsOptions tpcdsOptions = PipelineOptionsFactory.fromArgs(args).withValidation().as(TpcdsOptions.class);\n+\n+        String dataSize = TpcdsParametersReader.getAndCheckDataSize(tpcdsOptions);\n+        String[] queryNameArr = TpcdsParametersReader.getAndCheckQueryNameArray(tpcdsOptions);\n+        int nThreads = TpcdsParametersReader.getAndCheckTpcParallel(tpcdsOptions);\n+\n+        // Using ExecutorService and CompletionService to fulfill multi-threading functionality\n+        ExecutorService executor = Executors.newFixedThreadPool(nThreads);\n+        CompletionService<PipelineResult> completion = new ExecutorCompletionService<>(executor);\n+\n+        // After getting necessary parameters from tpcdsOptions, cast tpcdsOptions as a DataflowPipelineOptions object to read and set required parameters for pipeline execution.\n+        DataflowPipelineOptions dataflowPipelineOptions = tpcdsOptions.as(DataflowPipelineOptions.class);\n+\n+        BeamSqlEnv env =\n+                BeamSqlEnv\n+                        .builder(inMemoryMetaStore)\n+                        .setPipelineOptions(dataflowPipelineOptions)\n+                        .build();\n+\n+        // Register all tables, set their schemas, and set the locations where their corresponding data are stored.\n+        List<String> tableNames = TableSchemaJSONLoader.getAllTableNames();\n+        for (String tableName : tableNames) {\n+            String createStatement = \"CREATE EXTERNAL TABLE \" + tableName + \" (%s) TYPE text LOCATION '%s' TBLPROPERTIES '{\\\"format\\\":\\\"csv\\\", \\\"csvformat\\\": \\\"InformixUnload\\\"}'\";\n+            String tableSchema = TableSchemaJSONLoader.parseTableSchema(tableName);\n+            String dataLocation = \"gs://beamsql_tpcds_1/data/\" + dataSize +\"/\" + tableName + \".dat\";\n+            env.executeDdl(String.format(createStatement, tableSchema, dataLocation));\n+        }\n+\n+        // Make an array of pipelines, each pipeline is responsible for running a corresponding query.\n+        Pipeline[] pipelines = new Pipeline[queryNameArr.length];\n+\n+        // Execute all queries, transform the each result into a PCollection<String>, write them into the txt file and store in a GCP directory.\n+        for (int i = 0; i < queryNameArr.length; i++) {\n+            // For each query, get a copy of pipelineOptions from command line arguments, set a unique job name using the time stamp so that multiple different pipelines can run together.\n+            TpcdsOptions tpcdsOptionsCopy = PipelineOptionsFactory.fromArgs(args).withValidation().as(TpcdsOptions.class);\n+            DataflowPipelineOptions dataflowPipelineOptionsCopy = tpcdsOptionsCopy.as(DataflowPipelineOptions.class);\n+            dataflowPipelineOptionsCopy.setJobName(queryNameArr[i] + \"result\" + System.currentTimeMillis());\n+\n+            pipelines[i] = Pipeline.create(dataflowPipelineOptionsCopy);\n+            String queryString = QueryReader.readQuery(queryNameArr[i]);\n+\n+            // Query execution\n+            PCollection<Row> rows = BeamSqlRelUtils.toPCollection(pipelines[i], env.parseQuery(queryString));\n+\n+            // Transform the result from PCollection<Row> into PCollection<String>, and write it to the location where results are stored.\n+            PCollection<String> rowStrings = rows.apply(MapElements\n+                    .into(TypeDescriptors.strings())\n+                    .via((Row row) -> row.toString()));\n+            rowStrings.apply(TextIO.write().to(\"gs://beamsql_tpcds_1/tpcds_results/\" + dataSize + \"/\" + pipelines[i].getOptions().getJobName()).withSuffix(\".txt\").withNumShards(1));", "originalCommit": "8b395ab593a821f2af1777e96c899be83211afcc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b57b27dcfca1a73e8658020f07e5b49c6c4645f0", "url": "https://github.com/apache/beam/commit/b57b27dcfca1a73e8658020f07e5b49c6c4645f0", "message": "Removed unnecessary modifications", "committedDate": "2020-08-02T21:45:24Z", "type": "commit"}, {"oid": "14f3be088949d013ea1ca373087b815154a61f03", "url": "https://github.com/apache/beam/commit/14f3be088949d013ea1ca373087b815154a61f03", "message": "Added license headers to all query files", "committedDate": "2020-08-03T04:53:09Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NzA3MA==", "url": "https://github.com/apache/beam/pull/12436#discussion_r464567070", "bodyText": "add a newline", "author": "amaliujia", "createdAt": "2020-08-03T17:45:03Z", "path": "sdks/java/testing/tpcds/src/main/java/org/apache/beam/sdk/tpcds/TableSchemaJSONLoader.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.tpcds;\n+\n+import org.apache.beam.repackaged.core.org.apache.commons.compress.utils.FileNameUtils;\n+import org.json.simple.JSONArray;\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.JSONParser;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.ArrayList;\n+\n+\n+/**\n+ * TableSchemaJSONLoader can get all table's names from resource/schemas directory and parse a table's schema into a string.\n+ */\n+public class TableSchemaJSONLoader {\n+    /**\n+     * Read a table schema json file from resource/schemas directory, parse the file into a string which can be utilized by BeamSqlEnv.executeDdl method.\n+     * @param tableName The name of the json file to be read (fo example: item, store_sales).\n+     * @return A string that matches the format in BeamSqlEnv.executeDdl method, such as \"d_date_sk bigint, d_date_id varchar\"\n+     * @throws Exception\n+     */\n+    public static String parseTableSchema(String tableName) throws Exception {\n+        String tableFilePath = Objects.requireNonNull(TableSchemaJSONLoader.class.getClassLoader().getResource(\"schemas/\" + tableName +\".json\")).getPath();\n+\n+        JSONObject jsonObject = (JSONObject) new JSONParser().parse(new FileReader(new File(tableFilePath)));\n+        JSONArray jsonArray = (JSONArray) jsonObject.get(\"schema\");\n+\n+        // Iterate each element in jsonArray to construct the schema string\n+        StringBuilder schemaStringBuilder = new StringBuilder();\n+\n+        Iterator jsonArrIterator = jsonArray.iterator();\n+        Iterator<Map.Entry> recordIterator;\n+        while (jsonArrIterator.hasNext()) {\n+            recordIterator = ((Map) jsonArrIterator.next()).entrySet().iterator();\n+            while (recordIterator.hasNext()) {\n+                Map.Entry pair = recordIterator.next();\n+\n+                if (pair.getKey().equals(\"type\")) {\n+                    // If the key of the pair is \"type\", make some modification before appending it to the schemaStringBuilder, then append a comma.\n+                    String typeName = (String) pair.getValue();\n+                    if (typeName.toLowerCase().equals(\"identifier\") || typeName.toLowerCase().equals(\"integer\")) {\n+                        // Use long type to represent int, prevent overflow\n+                        schemaStringBuilder.append(\"bigint\");\n+                    } else if (typeName.contains(\"decimal\")) {\n+                        // Currently Beam SQL doesn't handle \"decimal\" type properly, use \"double\" to replace it for now.\n+                        schemaStringBuilder.append(\"double\");\n+                    } else {\n+                        // Currently Beam SQL doesn't handle \"date\" type properly, use \"varchar\" replace it for now.\n+                        schemaStringBuilder.append(\"varchar\");\n+                    }\n+                    schemaStringBuilder.append(',');\n+                } else {\n+                    // If the key of the pair is \"name\", directly append it to the StringBuilder, then append a space.\n+                    schemaStringBuilder.append((pair.getValue()));\n+                    schemaStringBuilder.append(' ');\n+                }\n+            }\n+        }\n+\n+        // Delete the last ',' in schema string\n+        if (schemaStringBuilder.length() > 0) {\n+            schemaStringBuilder.deleteCharAt(schemaStringBuilder.length() - 1);\n+        }\n+\n+        String schemaString = schemaStringBuilder.toString();\n+\n+        return schemaString;\n+    }\n+\n+    /**\n+     * Get all tables' names. Tables are stored in resource/schemas directory in the form of json files, such as \"item.json\", \"store_sales.json\", they'll be converted to \"item\", \"store_sales\".\n+     * @return The list of names of all tables.\n+     */\n+    public static List<String> getAllTableNames() {\n+        String tableDirPath = Objects.requireNonNull(TableSchemaJSONLoader.class.getClassLoader().getResource(\"schemas\")).getPath();\n+        File tableDir = new File(tableDirPath);\n+        File[] tableDirListing = tableDir.listFiles();\n+\n+        List<String> tableNames = new ArrayList<>();\n+\n+        if (tableDirListing != null) {\n+            for (File file : tableDirListing) {\n+                // Remove the .json extension in file name\n+                tableNames.add(FileNameUtils.getBaseName((file.getName())));\n+            }\n+        }\n+\n+        return tableNames;\n+    }\n+}", "originalCommit": "14f3be088949d013ea1ca373087b815154a61f03", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6972fa486ff6ef9db957dbe9140912064a33c0d2", "url": "https://github.com/apache/beam/commit/6972fa486ff6ef9db957dbe9140912064a33c0d2", "message": "Adjusted settings.gradle", "committedDate": "2020-08-03T18:38:05Z", "type": "commit"}, {"oid": "98230634616d5b6e6ebb14aa503a6ce2acc71046", "url": "https://github.com/apache/beam/commit/98230634616d5b6e6ebb14aa503a6ce2acc71046", "message": "Added a newline in TableSchemaJSONLoader.java", "committedDate": "2020-08-03T18:41:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU3MDc1NA==", "url": "https://github.com/apache/beam/pull/12436#discussion_r477570754", "bodyText": "Just curious about this. Is this because autoCastField is how CSV is converted to INT64? I happen to be looking at other things and found this.", "author": "kennknowles", "createdAt": "2020-08-26T20:30:32Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/schema/BeamTableUtils.java", "diffHunk": "@@ -151,10 +151,16 @@ public static Object autoCastField(Schema.Field field, Object rawObj) {\n         case INT32:\n           return Integer.valueOf(raw);\n         case INT64:\n+          if (raw.equals(\"\")) {\n+            return null;", "originalCommit": "98230634616d5b6e6ebb14aa503a6ce2acc71046", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU4MDYyMQ==", "url": "https://github.com/apache/beam/pull/12436#discussion_r477580621", "bodyText": "Yes, data in format of CSV files are read by this autoCastField method and converted to various types.\nWhen input data field is empty, it needs to be treated as NULL manually, otherwise it will cause error.", "author": "Imfuyuwei", "createdAt": "2020-08-26T20:49:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzU3MDc1NA=="}], "type": "inlineReview"}]}