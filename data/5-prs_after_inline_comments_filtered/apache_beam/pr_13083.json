{"pr_number": 13083, "pr_title": "[BEAM-7034] Add example snippet to read fromQuery using BQ Storage API.", "pr_createdAt": "2020-10-13T10:46:35Z", "pr_url": "https://github.com/apache/beam/pull/13083", "timeline": [{"oid": "01b75962042e98ae09d6745136e44ea2a2393a3f", "url": "https://github.com/apache/beam/commit/01b75962042e98ae09d6745136e44ea2a2393a3f", "message": "Add example snippets to read fromQuery using BQ Storage API.", "committedDate": "2020-10-13T10:43:53Z", "type": "commit"}, {"oid": "393bbdeef9148266082b0211f87869656213a942", "url": "https://github.com/apache/beam/commit/393bbdeef9148266082b0211f87869656213a942", "message": "Make the query example consistent with the previous one for the table.", "committedDate": "2020-10-13T12:15:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjUxNjEwOA==", "url": "https://github.com/apache/beam/pull/13083#discussion_r522516108", "bodyText": "I would avoid using readTableRows in an example snippet, both for the storage API and also for the existing export-based model -- this involves a needless conversion from Avro to JSON, where customers should instead be able to consume the Avro GenericRecords directly.", "author": "kmjung", "createdAt": "2020-11-12T23:56:34Z", "path": "examples/java/src/main/java/org/apache/beam/examples/snippets/transforms/io/gcp/bigquery/BigQueryReadFromQueryWithBigQueryStorageAPI.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.examples.snippets.transforms.io.gcp.bigquery;\n+\n+// [START bigquery_read_from_query_with_bigquery_storage_api]\n+\n+import java.util.Arrays;\n+import org.apache.beam.examples.snippets.transforms.io.gcp.bigquery.BigQueryMyData.MyData;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;\n+import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.TypedRead.Method;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+\n+class BigQueryReadFromQueryWithBigQueryStorageAPI {\n+  public static PCollection<MyData> readFromQueryWithBigQueryStorageAPI(\n+          String project, String dataset, String table, String query, Pipeline pipeline) {\n+\n+    // String project = \"my-project-id\";\n+    // String dataset = \"my_bigquery_dataset_id\";\n+    // String table = \"my_bigquery_table_id\";\n+\n+    // Pipeline pipeline = Pipeline.create();\n+\n+    /*\n+    String query = String.format(\"SELECT\\n\" +\n+        \"  string_field,\\n\" +\n+        \"  int64_field,\\n\" +\n+        \"  float64_field,\\n\" +\n+        \"  numeric_field,\\n\" +\n+        \"  bool_field,\\n\" +\n+        \"  bytes_field,\\n\" +\n+        \"  date_field,\\n\" +\n+        \"  datetime_field,\\n\" +\n+        \"  time_field,\\n\" +\n+        \"  timestamp_field,\\n\" +\n+        \"  geography_field,\\n\" +\n+        \"  array_field,\\n\" +\n+        \"  struct_field\\n\" +\n+        \"FROM\\n\" +\n+        \"  `%s:%s.%s`\", project, dataset, table)\n+    */\n+\n+    PCollection<MyData> rows =\n+        pipeline\n+            .apply(\n+                \"Read from BigQuery table\",\n+                BigQueryIO.readTableRows()", "originalCommit": "393bbdeef9148266082b0211f87869656213a942", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzM4OTc3Mw==", "url": "https://github.com/apache/beam/pull/13083#discussion_r527389773", "bodyText": "Okay, agree. What would be prefered way to continue with this then?\n\nFinish this PR with using TableRows to have all 3 read examples using the same undesired readTableRows() call\nrefactor this example only to use read<T>(SerializableFunction<SchemaAndRecord, T> f) as a part of this PR\nrefactor all 3 examples using the preferred read<T>(SerializableFunction<SchemaAndRecord, T> f)?\n\nReading from a table\nReading with a query string\nUsing the BigQuery Storage API", "author": "fpopic", "createdAt": "2020-11-20T04:27:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjUxNjEwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzg0MjI2Ng==", "url": "https://github.com/apache/beam/pull/13083#discussion_r527842266", "bodyText": "If you have the cycles, let's do (3). Otherwise, you can go ahead with (1) and I will take care of updating them when you're done.", "author": "kmjung", "createdAt": "2020-11-20T17:20:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjUxNjEwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk5NDA1NA==", "url": "https://github.com/apache/beam/pull/13083#discussion_r527994054", "bodyText": "Then let's merge this, and next week I can refactor all 3 examples.", "author": "fpopic", "createdAt": "2020-11-20T22:01:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjUxNjEwOA=="}], "type": "inlineReview"}, {"oid": "3c15bf3bb609ca51a9c7860f156a2c885969eb67", "url": "https://github.com/apache/beam/commit/3c15bf3bb609ca51a9c7860f156a2c885969eb67", "message": "Run spotelessApply.", "committedDate": "2021-01-27T20:14:14Z", "type": "commit"}]}