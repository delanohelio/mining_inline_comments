{"pr_number": 12924, "pr_title": "[BEAM-10124] Add ContextualTextIO ", "pr_createdAt": "2020-09-23T23:52:59Z", "pr_url": "https://github.com/apache/beam/pull/12924", "timeline": [{"oid": "be0bd98e0c45f8b52f502d0f8204fdcbd79fb7c4", "url": "https://github.com/apache/beam/commit/be0bd98e0c45f8b52f502d0f8204fdcbd79fb7c4", "message": "[BEAM-10124] Add ContextualTextIO as a copy of TextIO", "committedDate": "2020-09-24T01:07:39Z", "type": "commit"}, {"oid": "07b4d173a24d683f82bafa259024ef7677193d87", "url": "https://github.com/apache/beam/commit/07b4d173a24d683f82bafa259024ef7677193d87", "message": "[BEAM-10124] Change access modifiers.\n* Change access modifiers of the following function for visibility outside the package\n* modify access modifier for getEmptyMatchTreatment() and getWatchInterval() from FileIO\n* modify access modifier for getSingleFileMetadata() from FileBasedSource", "committedDate": "2020-09-24T01:07:39Z", "type": "commit"}, {"oid": "f61aad061c4fa0ce6c3d546e73b34801ce4f75d9", "url": "https://github.com/apache/beam/commit/f61aad061c4fa0ce6c3d546e73b34801ce4f75d9", "message": "[BEAM-10124] Add ContextualTextIO Implementation\n* Modify and add additional tests for ContextualTextIO\n* Add implementation for ContextualTextIO", "committedDate": "2020-09-24T01:07:39Z", "type": "commit"}, {"oid": "90617ce3fe22a218a633c4203bc6f0a9586d1bcd", "url": "https://github.com/apache/beam/commit/90617ce3fe22a218a633c4203bc6f0a9586d1bcd", "message": "[BEAM-10124] Refactor Code and Add option for recordOffset\n* Refactored requested changes\n* Add recordOffset feild that gives the offset of a record in the file", "committedDate": "2020-09-24T01:07:39Z", "type": "commit"}, {"oid": "85d576e9907a263ca0f063b9fc93de7db918db19", "url": "https://github.com/apache/beam/commit/85d576e9907a263ca0f063b9fc93de7db918db19", "message": "[BEAM-10124] Refactor requested changes", "committedDate": "2020-09-24T01:07:39Z", "type": "commit"}, {"oid": "e60c4fa986a87e88f98d46500bb7ae097ca2059c", "url": "https://github.com/apache/beam/commit/e60c4fa986a87e88f98d46500bb7ae097ca2059c", "message": "[BEAM-10124] Refactor requested changes", "committedDate": "2020-09-24T01:07:39Z", "type": "commit"}, {"oid": "4bbd2ca912d1e3f691e3df42c72b1831c8dc3d9f", "url": "https://github.com/apache/beam/commit/4bbd2ca912d1e3f691e3df42c72b1831c8dc3d9f", "message": "[BEAM-10124] Change from RecordWithMetadata To Row", "committedDate": "2020-09-24T01:07:39Z", "type": "commit"}, {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "url": "https://github.com/apache/beam/commit/0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "message": "Fixup\nCo-authored-by: Lukasz Cwik <lcwik@google.com>", "committedDate": "2020-09-24T01:48:49Z", "type": "commit"}, {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "url": "https://github.com/apache/beam/commit/0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "message": "Fixup\nCo-authored-by: Lukasz Cwik <lcwik@google.com>", "committedDate": "2020-09-24T01:48:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAxNzIyNQ==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495017225", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n          \n          \n            \n             * <p>Prefer {@link TextIO} when not reading files with multi-line records or additional record metadata is not", "author": "lukecwik", "createdAt": "2020-09-25T14:13:51Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAxNzYzMQ==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495017631", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n          \n          \n            \n             * <p>{@link #read} returns a {@link PCollection} of {@link Row}s with schema {@link", "author": "lukecwik", "createdAt": "2020-09-25T14:14:26Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAxODQwMQ==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495018401", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n          \n          \n            \n             * ContextualTextIO.Read#withDelimiter})\n          \n          \n            \n             * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter via {@link\n          \n          \n            \n             * ContextualTextIO.Read#withDelimiter}).", "author": "lukecwik", "createdAt": "2020-09-25T14:15:32Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAxOTU2NQ==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495019565", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *      .withHasMultilineCSVRecords(true));\n          \n          \n            \n             *     .withHasMultilineCSVRecords(true));", "author": "lukecwik", "createdAt": "2020-09-25T14:17:17Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyMDUwMw==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495020503", "bodyText": "Why would someone choose this over example 5?\nThey seem to both be watching for new files.", "author": "lukecwik", "createdAt": "2020-09-25T14:18:43Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyMTU3Ng==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495021576", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n          \n          \n            \n             * <p>NOTE: When using the {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)}", "author": "lukecwik", "createdAt": "2020-09-25T14:20:22Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyMjQ5Ng==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495022496", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * read from different offsets. For a large file this can result in lower performance.\n          \n          \n            \n             * read from different offsets. This limits parallelism to the number of files being read and will likely lower performance.", "author": "lukecwik", "createdAt": "2020-09-25T14:21:41Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyNDQwOA==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495024408", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n          \n          \n            \n                 * <p>This can be a local path (if running locally), or a remote path such as a Google Cloud Storage filename or", "author": "lukecwik", "createdAt": "2020-09-25T14:24:35Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyNTEwOA==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495025108", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n          \n          \n            \n                public Read withMultilineCSVRecords(Boolean withMultilineCSVRecords) {", "author": "lukecwik", "createdAt": "2020-09-25T14:25:34Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyNzk0Ng==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495027946", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *      .setWithoutRecordNumMetadata(true));\n          \n          \n            \n             *     .setWithoutRecordNumMetadata());", "author": "lukecwik", "createdAt": "2020-09-25T14:29:43Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyODg2NA==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495028864", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n          \n          \n            \n             * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n          \n          \n            \n             * introduces a shuffle step, which increases the resources used by the pipeline. <b>By default\n          \n          \n            \n             * withoutRecordNumMetadata is set to false requiring an additional grouping operation.</b>", "author": "lukecwik", "createdAt": "2020-09-25T14:31:04Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAzOTE1Mw==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495039153", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      input.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));\n          \n          \n            \n                      input.getPipeline().apply(\"CreateSingletonPColl\", Create.of(Arrays.asList(1)));", "author": "lukecwik", "createdAt": "2020-09-25T14:46:56Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n+      return toBuilder().setHasMultilineCSVRecords(hasMultilineCSVRecords).build();\n+    }\n+\n+    /**\n+     * Reads from input sources using the specified compression type.\n+     *\n+     * <p>If no compression type is specified, the default is {@link Compression#AUTO}.\n+     */\n+    public Read withCompression(Compression compression) {\n+      return toBuilder().setCompression(compression).build();\n+    }\n+\n+    /**\n+     * Hints that the filepattern specified in {@link #from(String)} matches a very large number of\n+     * files.\n+     *\n+     * <p>This hint may cause a runner to execute the transform differently, in a way that improves\n+     * performance for this case, but it may worsen performance if the filepattern matches only a\n+     * small number of files (e.g., in a runner that supports dynamic work rebalancing, it will\n+     * happen less efficiently within individual files).\n+     */\n+    public Read withHintMatchesManyFiles() {\n+      return toBuilder().setHintMatchesManyFiles(true).build();\n+    }\n+\n+    /**\n+     * Allows the user to opt out of getting recordNums associated with each record.\n+     *\n+     * <p>When set to true, it will introduce a shuffle step to assemble the recordNums for each\n+     * record, which will increase the resources used by the pipeline.\n+     *\n+     * <p>Use this when metadata like fileNames are required and their position/order can be\n+     * ignored.\n+     */\n+    public Read withoutRecordNumMetadata() {\n+      return toBuilder().setWithoutRecordNumMetadata(true).build();\n+    }\n+\n+    /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */\n+    public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {\n+      return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));\n+    }\n+\n+    /** Set the custom delimiter to be used in place of the default ones ('\\r', '\\n' or '\\r\\n'). */\n+    public Read withDelimiter(byte[] delimiter) {\n+      checkArgument(delimiter != null, \"delimiter can not be null\");\n+      checkArgument(!isSelfOverlapping(delimiter), \"delimiter must not self-overlap\");\n+      return toBuilder().setDelimiter(delimiter).build();\n+    }\n+\n+    static boolean isSelfOverlapping(byte[] s) {\n+      // s self-overlaps if v exists such as s = vu = wv with u and w non empty\n+      for (int i = 1; i < s.length - 1; ++i) {\n+        if (ByteBuffer.wrap(s, 0, i).equals(ByteBuffer.wrap(s, s.length - i, i))) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PBegin input) {\n+      checkNotNull(\n+          getFilepattern(), \"need to set the filepattern of a ContextualTextIO.Read transform\");\n+      PCollection<Row> records = null;\n+      if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {\n+        records = input.apply(\"Read\", org.apache.beam.sdk.io.Read.from(getSource()));\n+      } else {\n+        // All other cases go through FileIO + ReadFiles\n+        records =\n+            input\n+                .apply(\n+                    \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+                .apply(\"Match All\", FileIO.matchAll().withConfiguration(getMatchConfiguration()))\n+                .apply(\n+                    \"Read Matches\",\n+                    FileIO.readMatches()\n+                        .withCompression(getCompression())\n+                        .withDirectoryTreatment(DirectoryTreatment.PROHIBIT))\n+                .apply(\"Via ReadFiles\", readFiles().withDelimiter(getDelimiter()));\n+      }\n+\n+      // Check if the user decided to opt out of recordNums associated with records\n+      if (getWithoutRecordNumMetadata()) {\n+        return records;\n+      }\n+\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =\n+          records\n+              .apply(\"AddFileNameAndRange\", ParDo.of(new AddFileNameAndRange()))\n+              .setCoder(\n+                  KvCoder.of(\n+                      KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()),\n+                      RowCoder.of(RecordWithMetadata.getSchema())));\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> rangeSizes =\n+          recordsGroupedByFileAndRange\n+              .apply(\"CountRecordsForEachFileRange\", Count.perKey())\n+              .apply(\"SizesAsView\", View.asMap());\n+\n+      // Get Pipeline to create a dummy PCollection with one element to help compute the lines\n+      // before each Range\n+      PCollection<Integer> singletonPcoll =\n+          input.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAzOTkwOQ==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495039909", "bodyText": "nit: ComputeRecordsBeforeEachRange -> ComputeNumRecordsBeforeEachRange", "author": "lukecwik", "createdAt": "2020-09-25T14:48:07Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n+      return toBuilder().setHasMultilineCSVRecords(hasMultilineCSVRecords).build();\n+    }\n+\n+    /**\n+     * Reads from input sources using the specified compression type.\n+     *\n+     * <p>If no compression type is specified, the default is {@link Compression#AUTO}.\n+     */\n+    public Read withCompression(Compression compression) {\n+      return toBuilder().setCompression(compression).build();\n+    }\n+\n+    /**\n+     * Hints that the filepattern specified in {@link #from(String)} matches a very large number of\n+     * files.\n+     *\n+     * <p>This hint may cause a runner to execute the transform differently, in a way that improves\n+     * performance for this case, but it may worsen performance if the filepattern matches only a\n+     * small number of files (e.g., in a runner that supports dynamic work rebalancing, it will\n+     * happen less efficiently within individual files).\n+     */\n+    public Read withHintMatchesManyFiles() {\n+      return toBuilder().setHintMatchesManyFiles(true).build();\n+    }\n+\n+    /**\n+     * Allows the user to opt out of getting recordNums associated with each record.\n+     *\n+     * <p>When set to true, it will introduce a shuffle step to assemble the recordNums for each\n+     * record, which will increase the resources used by the pipeline.\n+     *\n+     * <p>Use this when metadata like fileNames are required and their position/order can be\n+     * ignored.\n+     */\n+    public Read withoutRecordNumMetadata() {\n+      return toBuilder().setWithoutRecordNumMetadata(true).build();\n+    }\n+\n+    /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */\n+    public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {\n+      return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));\n+    }\n+\n+    /** Set the custom delimiter to be used in place of the default ones ('\\r', '\\n' or '\\r\\n'). */\n+    public Read withDelimiter(byte[] delimiter) {\n+      checkArgument(delimiter != null, \"delimiter can not be null\");\n+      checkArgument(!isSelfOverlapping(delimiter), \"delimiter must not self-overlap\");\n+      return toBuilder().setDelimiter(delimiter).build();\n+    }\n+\n+    static boolean isSelfOverlapping(byte[] s) {\n+      // s self-overlaps if v exists such as s = vu = wv with u and w non empty\n+      for (int i = 1; i < s.length - 1; ++i) {\n+        if (ByteBuffer.wrap(s, 0, i).equals(ByteBuffer.wrap(s, s.length - i, i))) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PBegin input) {\n+      checkNotNull(\n+          getFilepattern(), \"need to set the filepattern of a ContextualTextIO.Read transform\");\n+      PCollection<Row> records = null;\n+      if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {\n+        records = input.apply(\"Read\", org.apache.beam.sdk.io.Read.from(getSource()));\n+      } else {\n+        // All other cases go through FileIO + ReadFiles\n+        records =\n+            input\n+                .apply(\n+                    \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+                .apply(\"Match All\", FileIO.matchAll().withConfiguration(getMatchConfiguration()))\n+                .apply(\n+                    \"Read Matches\",\n+                    FileIO.readMatches()\n+                        .withCompression(getCompression())\n+                        .withDirectoryTreatment(DirectoryTreatment.PROHIBIT))\n+                .apply(\"Via ReadFiles\", readFiles().withDelimiter(getDelimiter()));\n+      }\n+\n+      // Check if the user decided to opt out of recordNums associated with records\n+      if (getWithoutRecordNumMetadata()) {\n+        return records;\n+      }\n+\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =\n+          records\n+              .apply(\"AddFileNameAndRange\", ParDo.of(new AddFileNameAndRange()))\n+              .setCoder(\n+                  KvCoder.of(\n+                      KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()),\n+                      RowCoder.of(RecordWithMetadata.getSchema())));\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> rangeSizes =\n+          recordsGroupedByFileAndRange\n+              .apply(\"CountRecordsForEachFileRange\", Count.perKey())\n+              .apply(\"SizesAsView\", View.asMap());\n+\n+      // Get Pipeline to create a dummy PCollection with one element to help compute the lines\n+      // before each Range\n+      PCollection<Integer> singletonPcoll =\n+          input.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));\n+\n+      /*\n+       * For each (File, Offset) pair, calculate the number of lines occurring before the Range for each file\n+       *\n+       * After computing the number of lines before each range, we can find the line number in original file as numLiesBeforeOffset + lineNumInCurrentOffset\n+       */\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> numRecordsBeforeEachRange =\n+          singletonPcoll\n+              .apply(\n+                  \"ComputeRecordsBeforeRange\",\n+                  ParDo.of(new ComputeRecordsBeforeEachRange(rangeSizes))\n+                      .withSideInputs(rangeSizes))\n+              .apply(\"NumRecordsBeforeEachRangeAsView\", View.asMap());\n+\n+      return recordsGroupedByFileAndRange\n+          .apply(\n+              \"AssignLineNums\",\n+              ParDo.of(new AssignRecordNums(numRecordsBeforeEachRange))\n+                  .withSideInputs(numRecordsBeforeEachRange))\n+          .setRowSchema(RecordWithMetadata.getSchema());\n+    }\n+\n+    @VisibleForTesting\n+    static class AddFileNameAndRange extends DoFn<Row, KV<KV<String, Long>, Row>> {\n+      @ProcessElement\n+      public void processElement(\n+          @Element Row record, OutputReceiver<KV<KV<String, Long>, Row>> out) {\n+\n+        out.output(\n+            KV.of(\n+                KV.of(\n+                    record\n+                        .getLogicalTypeValue(RecordWithMetadata.RESOURCE_ID, ResourceId.class)\n+                        .toString(),\n+                    record.getInt64(RecordWithMetadata.RANGE_OFFSET)),\n+                record));\n+      }\n+    }\n+\n+    /**\n+     * Helper class for computing number of record in the File preceding the beginning of the Range\n+     * in this file.\n+     */\n+    @VisibleForTesting\n+    static class ComputeRecordsBeforeEachRange extends DoFn<Integer, KV<KV<String, Long>, Long>> {", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY0ODAxNg==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495648016", "bodyText": "Done", "author": "rezarokni", "createdAt": "2020-09-28T01:29:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAzOTkwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA0MDkxNw==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495040917", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    for (Map.Entry<KV<String, Long>, Long> entry : rangeSizesMap.entrySet()) {\n          \n          \n            \n                      sorted.put(entry.getKey(), entry.getValue());\n          \n          \n            \n                    }\n          \n          \n            \n                    sorted.putAll(rangeSizesMap);", "author": "lukecwik", "createdAt": "2020-09-25T14:49:43Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n+      return toBuilder().setHasMultilineCSVRecords(hasMultilineCSVRecords).build();\n+    }\n+\n+    /**\n+     * Reads from input sources using the specified compression type.\n+     *\n+     * <p>If no compression type is specified, the default is {@link Compression#AUTO}.\n+     */\n+    public Read withCompression(Compression compression) {\n+      return toBuilder().setCompression(compression).build();\n+    }\n+\n+    /**\n+     * Hints that the filepattern specified in {@link #from(String)} matches a very large number of\n+     * files.\n+     *\n+     * <p>This hint may cause a runner to execute the transform differently, in a way that improves\n+     * performance for this case, but it may worsen performance if the filepattern matches only a\n+     * small number of files (e.g., in a runner that supports dynamic work rebalancing, it will\n+     * happen less efficiently within individual files).\n+     */\n+    public Read withHintMatchesManyFiles() {\n+      return toBuilder().setHintMatchesManyFiles(true).build();\n+    }\n+\n+    /**\n+     * Allows the user to opt out of getting recordNums associated with each record.\n+     *\n+     * <p>When set to true, it will introduce a shuffle step to assemble the recordNums for each\n+     * record, which will increase the resources used by the pipeline.\n+     *\n+     * <p>Use this when metadata like fileNames are required and their position/order can be\n+     * ignored.\n+     */\n+    public Read withoutRecordNumMetadata() {\n+      return toBuilder().setWithoutRecordNumMetadata(true).build();\n+    }\n+\n+    /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */\n+    public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {\n+      return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));\n+    }\n+\n+    /** Set the custom delimiter to be used in place of the default ones ('\\r', '\\n' or '\\r\\n'). */\n+    public Read withDelimiter(byte[] delimiter) {\n+      checkArgument(delimiter != null, \"delimiter can not be null\");\n+      checkArgument(!isSelfOverlapping(delimiter), \"delimiter must not self-overlap\");\n+      return toBuilder().setDelimiter(delimiter).build();\n+    }\n+\n+    static boolean isSelfOverlapping(byte[] s) {\n+      // s self-overlaps if v exists such as s = vu = wv with u and w non empty\n+      for (int i = 1; i < s.length - 1; ++i) {\n+        if (ByteBuffer.wrap(s, 0, i).equals(ByteBuffer.wrap(s, s.length - i, i))) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PBegin input) {\n+      checkNotNull(\n+          getFilepattern(), \"need to set the filepattern of a ContextualTextIO.Read transform\");\n+      PCollection<Row> records = null;\n+      if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {\n+        records = input.apply(\"Read\", org.apache.beam.sdk.io.Read.from(getSource()));\n+      } else {\n+        // All other cases go through FileIO + ReadFiles\n+        records =\n+            input\n+                .apply(\n+                    \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+                .apply(\"Match All\", FileIO.matchAll().withConfiguration(getMatchConfiguration()))\n+                .apply(\n+                    \"Read Matches\",\n+                    FileIO.readMatches()\n+                        .withCompression(getCompression())\n+                        .withDirectoryTreatment(DirectoryTreatment.PROHIBIT))\n+                .apply(\"Via ReadFiles\", readFiles().withDelimiter(getDelimiter()));\n+      }\n+\n+      // Check if the user decided to opt out of recordNums associated with records\n+      if (getWithoutRecordNumMetadata()) {\n+        return records;\n+      }\n+\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =\n+          records\n+              .apply(\"AddFileNameAndRange\", ParDo.of(new AddFileNameAndRange()))\n+              .setCoder(\n+                  KvCoder.of(\n+                      KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()),\n+                      RowCoder.of(RecordWithMetadata.getSchema())));\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> rangeSizes =\n+          recordsGroupedByFileAndRange\n+              .apply(\"CountRecordsForEachFileRange\", Count.perKey())\n+              .apply(\"SizesAsView\", View.asMap());\n+\n+      // Get Pipeline to create a dummy PCollection with one element to help compute the lines\n+      // before each Range\n+      PCollection<Integer> singletonPcoll =\n+          input.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));\n+\n+      /*\n+       * For each (File, Offset) pair, calculate the number of lines occurring before the Range for each file\n+       *\n+       * After computing the number of lines before each range, we can find the line number in original file as numLiesBeforeOffset + lineNumInCurrentOffset\n+       */\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> numRecordsBeforeEachRange =\n+          singletonPcoll\n+              .apply(\n+                  \"ComputeRecordsBeforeRange\",\n+                  ParDo.of(new ComputeRecordsBeforeEachRange(rangeSizes))\n+                      .withSideInputs(rangeSizes))\n+              .apply(\"NumRecordsBeforeEachRangeAsView\", View.asMap());\n+\n+      return recordsGroupedByFileAndRange\n+          .apply(\n+              \"AssignLineNums\",\n+              ParDo.of(new AssignRecordNums(numRecordsBeforeEachRange))\n+                  .withSideInputs(numRecordsBeforeEachRange))\n+          .setRowSchema(RecordWithMetadata.getSchema());\n+    }\n+\n+    @VisibleForTesting\n+    static class AddFileNameAndRange extends DoFn<Row, KV<KV<String, Long>, Row>> {\n+      @ProcessElement\n+      public void processElement(\n+          @Element Row record, OutputReceiver<KV<KV<String, Long>, Row>> out) {\n+\n+        out.output(\n+            KV.of(\n+                KV.of(\n+                    record\n+                        .getLogicalTypeValue(RecordWithMetadata.RESOURCE_ID, ResourceId.class)\n+                        .toString(),\n+                    record.getInt64(RecordWithMetadata.RANGE_OFFSET)),\n+                record));\n+      }\n+    }\n+\n+    /**\n+     * Helper class for computing number of record in the File preceding the beginning of the Range\n+     * in this file.\n+     */\n+    @VisibleForTesting\n+    static class ComputeRecordsBeforeEachRange extends DoFn<Integer, KV<KV<String, Long>, Long>> {\n+      private final PCollectionView<Map<KV<String, Long>, Long>> rangeSizes;\n+\n+      public ComputeRecordsBeforeEachRange(\n+          PCollectionView<Map<KV<String, Long>, Long>> rangeSizes) {\n+        this.rangeSizes = rangeSizes;\n+      }\n+\n+      // Add custom comparator as KV<K, V> is not comparable by default\n+      private static class FileRangeComparator<K extends Comparable<K>, V extends Comparable<V>>\n+          implements Comparator<KV<K, V>>, Serializable {\n+        @Override\n+        public int compare(KV<K, V> a, KV<K, V> b) {\n+          if (a.getKey().compareTo(b.getKey()) == 0) {\n+            return a.getValue().compareTo(b.getValue());\n+          }\n+          return a.getKey().compareTo(b.getKey());\n+        }\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext p) {\n+        // Get the Map Containing the size from side-input\n+        Map<KV<String, Long>, Long> rangeSizesMap = p.sideInput(rangeSizes);\n+\n+        // The FileRange Pair must be sorted\n+        SortedMap<KV<String, Long>, Long> sorted = new TreeMap<>(new FileRangeComparator<>());\n+\n+        // Initialize sorted map with values\n+        for (Map.Entry<KV<String, Long>, Long> entry : rangeSizesMap.entrySet()) {\n+          sorted.put(entry.getKey(), entry.getValue());\n+        }", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA0MzIwMg==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495043202", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                              \"ComputeRecordsBeforeRange\",\n          \n          \n            \n                              \"ComputeNumRecordsBeforeRange\",", "author": "lukecwik", "createdAt": "2020-09-25T14:53:12Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n+      return toBuilder().setHasMultilineCSVRecords(hasMultilineCSVRecords).build();\n+    }\n+\n+    /**\n+     * Reads from input sources using the specified compression type.\n+     *\n+     * <p>If no compression type is specified, the default is {@link Compression#AUTO}.\n+     */\n+    public Read withCompression(Compression compression) {\n+      return toBuilder().setCompression(compression).build();\n+    }\n+\n+    /**\n+     * Hints that the filepattern specified in {@link #from(String)} matches a very large number of\n+     * files.\n+     *\n+     * <p>This hint may cause a runner to execute the transform differently, in a way that improves\n+     * performance for this case, but it may worsen performance if the filepattern matches only a\n+     * small number of files (e.g., in a runner that supports dynamic work rebalancing, it will\n+     * happen less efficiently within individual files).\n+     */\n+    public Read withHintMatchesManyFiles() {\n+      return toBuilder().setHintMatchesManyFiles(true).build();\n+    }\n+\n+    /**\n+     * Allows the user to opt out of getting recordNums associated with each record.\n+     *\n+     * <p>When set to true, it will introduce a shuffle step to assemble the recordNums for each\n+     * record, which will increase the resources used by the pipeline.\n+     *\n+     * <p>Use this when metadata like fileNames are required and their position/order can be\n+     * ignored.\n+     */\n+    public Read withoutRecordNumMetadata() {\n+      return toBuilder().setWithoutRecordNumMetadata(true).build();\n+    }\n+\n+    /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */\n+    public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {\n+      return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));\n+    }\n+\n+    /** Set the custom delimiter to be used in place of the default ones ('\\r', '\\n' or '\\r\\n'). */\n+    public Read withDelimiter(byte[] delimiter) {\n+      checkArgument(delimiter != null, \"delimiter can not be null\");\n+      checkArgument(!isSelfOverlapping(delimiter), \"delimiter must not self-overlap\");\n+      return toBuilder().setDelimiter(delimiter).build();\n+    }\n+\n+    static boolean isSelfOverlapping(byte[] s) {\n+      // s self-overlaps if v exists such as s = vu = wv with u and w non empty\n+      for (int i = 1; i < s.length - 1; ++i) {\n+        if (ByteBuffer.wrap(s, 0, i).equals(ByteBuffer.wrap(s, s.length - i, i))) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PBegin input) {\n+      checkNotNull(\n+          getFilepattern(), \"need to set the filepattern of a ContextualTextIO.Read transform\");\n+      PCollection<Row> records = null;\n+      if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {\n+        records = input.apply(\"Read\", org.apache.beam.sdk.io.Read.from(getSource()));\n+      } else {\n+        // All other cases go through FileIO + ReadFiles\n+        records =\n+            input\n+                .apply(\n+                    \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+                .apply(\"Match All\", FileIO.matchAll().withConfiguration(getMatchConfiguration()))\n+                .apply(\n+                    \"Read Matches\",\n+                    FileIO.readMatches()\n+                        .withCompression(getCompression())\n+                        .withDirectoryTreatment(DirectoryTreatment.PROHIBIT))\n+                .apply(\"Via ReadFiles\", readFiles().withDelimiter(getDelimiter()));\n+      }\n+\n+      // Check if the user decided to opt out of recordNums associated with records\n+      if (getWithoutRecordNumMetadata()) {\n+        return records;\n+      }\n+\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =\n+          records\n+              .apply(\"AddFileNameAndRange\", ParDo.of(new AddFileNameAndRange()))\n+              .setCoder(\n+                  KvCoder.of(\n+                      KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()),\n+                      RowCoder.of(RecordWithMetadata.getSchema())));\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> rangeSizes =\n+          recordsGroupedByFileAndRange\n+              .apply(\"CountRecordsForEachFileRange\", Count.perKey())\n+              .apply(\"SizesAsView\", View.asMap());\n+\n+      // Get Pipeline to create a dummy PCollection with one element to help compute the lines\n+      // before each Range\n+      PCollection<Integer> singletonPcoll =\n+          input.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));\n+\n+      /*\n+       * For each (File, Offset) pair, calculate the number of lines occurring before the Range for each file\n+       *\n+       * After computing the number of lines before each range, we can find the line number in original file as numLiesBeforeOffset + lineNumInCurrentOffset\n+       */\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> numRecordsBeforeEachRange =\n+          singletonPcoll\n+              .apply(\n+                  \"ComputeRecordsBeforeRange\",", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA1MDA1OQ==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495050059", "bodyText": "We should be using a multimap PCollectionView here of filename -> KV<offset, numRecordsInOffset>\nThis will alleviate concerns about how much of the map we need to load into memory at any given time since we will do a sort per key (filename).\nIf you have 10s of thousands of files which possibly have 1000s of offset ranges the other format will take up too much memory.", "author": "lukecwik", "createdAt": "2020-09-25T15:03:53Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n+      return toBuilder().setHasMultilineCSVRecords(hasMultilineCSVRecords).build();\n+    }\n+\n+    /**\n+     * Reads from input sources using the specified compression type.\n+     *\n+     * <p>If no compression type is specified, the default is {@link Compression#AUTO}.\n+     */\n+    public Read withCompression(Compression compression) {\n+      return toBuilder().setCompression(compression).build();\n+    }\n+\n+    /**\n+     * Hints that the filepattern specified in {@link #from(String)} matches a very large number of\n+     * files.\n+     *\n+     * <p>This hint may cause a runner to execute the transform differently, in a way that improves\n+     * performance for this case, but it may worsen performance if the filepattern matches only a\n+     * small number of files (e.g., in a runner that supports dynamic work rebalancing, it will\n+     * happen less efficiently within individual files).\n+     */\n+    public Read withHintMatchesManyFiles() {\n+      return toBuilder().setHintMatchesManyFiles(true).build();\n+    }\n+\n+    /**\n+     * Allows the user to opt out of getting recordNums associated with each record.\n+     *\n+     * <p>When set to true, it will introduce a shuffle step to assemble the recordNums for each\n+     * record, which will increase the resources used by the pipeline.\n+     *\n+     * <p>Use this when metadata like fileNames are required and their position/order can be\n+     * ignored.\n+     */\n+    public Read withoutRecordNumMetadata() {\n+      return toBuilder().setWithoutRecordNumMetadata(true).build();\n+    }\n+\n+    /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */\n+    public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {\n+      return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));\n+    }\n+\n+    /** Set the custom delimiter to be used in place of the default ones ('\\r', '\\n' or '\\r\\n'). */\n+    public Read withDelimiter(byte[] delimiter) {\n+      checkArgument(delimiter != null, \"delimiter can not be null\");\n+      checkArgument(!isSelfOverlapping(delimiter), \"delimiter must not self-overlap\");\n+      return toBuilder().setDelimiter(delimiter).build();\n+    }\n+\n+    static boolean isSelfOverlapping(byte[] s) {\n+      // s self-overlaps if v exists such as s = vu = wv with u and w non empty\n+      for (int i = 1; i < s.length - 1; ++i) {\n+        if (ByteBuffer.wrap(s, 0, i).equals(ByteBuffer.wrap(s, s.length - i, i))) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PBegin input) {\n+      checkNotNull(\n+          getFilepattern(), \"need to set the filepattern of a ContextualTextIO.Read transform\");\n+      PCollection<Row> records = null;\n+      if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {\n+        records = input.apply(\"Read\", org.apache.beam.sdk.io.Read.from(getSource()));\n+      } else {\n+        // All other cases go through FileIO + ReadFiles\n+        records =\n+            input\n+                .apply(\n+                    \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+                .apply(\"Match All\", FileIO.matchAll().withConfiguration(getMatchConfiguration()))\n+                .apply(\n+                    \"Read Matches\",\n+                    FileIO.readMatches()\n+                        .withCompression(getCompression())\n+                        .withDirectoryTreatment(DirectoryTreatment.PROHIBIT))\n+                .apply(\"Via ReadFiles\", readFiles().withDelimiter(getDelimiter()));\n+      }\n+\n+      // Check if the user decided to opt out of recordNums associated with records\n+      if (getWithoutRecordNumMetadata()) {\n+        return records;\n+      }\n+\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =\n+          records\n+              .apply(\"AddFileNameAndRange\", ParDo.of(new AddFileNameAndRange()))\n+              .setCoder(\n+                  KvCoder.of(\n+                      KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()),\n+                      RowCoder.of(RecordWithMetadata.getSchema())));\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> rangeSizes =\n+          recordsGroupedByFileAndRange\n+              .apply(\"CountRecordsForEachFileRange\", Count.perKey())\n+              .apply(\"SizesAsView\", View.asMap());\n+\n+      // Get Pipeline to create a dummy PCollection with one element to help compute the lines\n+      // before each Range\n+      PCollection<Integer> singletonPcoll =\n+          input.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));\n+\n+      /*\n+       * For each (File, Offset) pair, calculate the number of lines occurring before the Range for each file\n+       *\n+       * After computing the number of lines before each range, we can find the line number in original file as numLiesBeforeOffset + lineNumInCurrentOffset\n+       */\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> numRecordsBeforeEachRange =", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTUwOTY2OA==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495509668", "bodyText": "Multimap does not yet have support in all runners does it?", "author": "rezarokni", "createdAt": "2020-09-27T00:26:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA1MDA1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQyMzQxNA==", "url": "https://github.com/apache/beam/pull/12924#discussion_r498423414", "bodyText": "It has support from all runners that support side inputs.\nThis isn't like map state which is only supported by some runners.", "author": "lukecwik", "createdAt": "2020-10-01T18:00:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA1MDA1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA1Mjg5OA==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495052898", "bodyText": "The algorithm used with these side inputs is only safe if we are using a trigger that fires once (like the default trigger). We need to ensure that the windowing strategy is compatible with our algorithm and throw an error otherwise.", "author": "lukecwik", "createdAt": "2020-09-25T15:08:34Z", "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n+      return toBuilder().setHasMultilineCSVRecords(hasMultilineCSVRecords).build();\n+    }\n+\n+    /**\n+     * Reads from input sources using the specified compression type.\n+     *\n+     * <p>If no compression type is specified, the default is {@link Compression#AUTO}.\n+     */\n+    public Read withCompression(Compression compression) {\n+      return toBuilder().setCompression(compression).build();\n+    }\n+\n+    /**\n+     * Hints that the filepattern specified in {@link #from(String)} matches a very large number of\n+     * files.\n+     *\n+     * <p>This hint may cause a runner to execute the transform differently, in a way that improves\n+     * performance for this case, but it may worsen performance if the filepattern matches only a\n+     * small number of files (e.g., in a runner that supports dynamic work rebalancing, it will\n+     * happen less efficiently within individual files).\n+     */\n+    public Read withHintMatchesManyFiles() {\n+      return toBuilder().setHintMatchesManyFiles(true).build();\n+    }\n+\n+    /**\n+     * Allows the user to opt out of getting recordNums associated with each record.\n+     *\n+     * <p>When set to true, it will introduce a shuffle step to assemble the recordNums for each\n+     * record, which will increase the resources used by the pipeline.\n+     *\n+     * <p>Use this when metadata like fileNames are required and their position/order can be\n+     * ignored.\n+     */\n+    public Read withoutRecordNumMetadata() {\n+      return toBuilder().setWithoutRecordNumMetadata(true).build();\n+    }\n+\n+    /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */\n+    public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {\n+      return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));\n+    }\n+\n+    /** Set the custom delimiter to be used in place of the default ones ('\\r', '\\n' or '\\r\\n'). */\n+    public Read withDelimiter(byte[] delimiter) {\n+      checkArgument(delimiter != null, \"delimiter can not be null\");\n+      checkArgument(!isSelfOverlapping(delimiter), \"delimiter must not self-overlap\");\n+      return toBuilder().setDelimiter(delimiter).build();\n+    }\n+\n+    static boolean isSelfOverlapping(byte[] s) {\n+      // s self-overlaps if v exists such as s = vu = wv with u and w non empty\n+      for (int i = 1; i < s.length - 1; ++i) {\n+        if (ByteBuffer.wrap(s, 0, i).equals(ByteBuffer.wrap(s, s.length - i, i))) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PBegin input) {\n+      checkNotNull(\n+          getFilepattern(), \"need to set the filepattern of a ContextualTextIO.Read transform\");\n+      PCollection<Row> records = null;\n+      if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {\n+        records = input.apply(\"Read\", org.apache.beam.sdk.io.Read.from(getSource()));\n+      } else {\n+        // All other cases go through FileIO + ReadFiles\n+        records =\n+            input\n+                .apply(\n+                    \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+                .apply(\"Match All\", FileIO.matchAll().withConfiguration(getMatchConfiguration()))\n+                .apply(\n+                    \"Read Matches\",\n+                    FileIO.readMatches()\n+                        .withCompression(getCompression())\n+                        .withDirectoryTreatment(DirectoryTreatment.PROHIBIT))\n+                .apply(\"Via ReadFiles\", readFiles().withDelimiter(getDelimiter()));\n+      }\n+\n+      // Check if the user decided to opt out of recordNums associated with records\n+      if (getWithoutRecordNumMetadata()) {\n+        return records;\n+      }\n+\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =", "originalCommit": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY1NDM5OQ==", "url": "https://github.com/apache/beam/pull/12924#discussion_r495654399", "bodyText": "Working on this, the other change I am making is to set setWithRecordNumMetadata option to false by default.", "author": "rezarokni", "createdAt": "2020-09-28T02:00:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA1Mjg5OA=="}], "type": "inlineReview"}, {"oid": "1aaa1e9ac9991d16eb68342d90c93b05391af34c", "url": "https://github.com/apache/beam/commit/1aaa1e9ac9991d16eb68342d90c93b05391af34c", "message": "Update sdks/java/io/contextual-text-io/build.gradle\n\nCo-authored-by: Lukasz Cwik <lcwik@google.com>", "committedDate": "2020-09-27T00:24:28Z", "type": "commit"}, {"oid": "8404af268fd31adbdf75849d611e38734c27f10f", "url": "https://github.com/apache/beam/commit/8404af268fd31adbdf75849d611e38734c27f10f", "message": "Update sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java\n\nCo-authored-by: Lukasz Cwik <lcwik@google.com>", "committedDate": "2020-09-28T04:35:53Z", "type": "commit"}, {"oid": "46fc4729775d1c5fe6232010a78151f2afc4e6f3", "url": "https://github.com/apache/beam/commit/46fc4729775d1c5fe6232010a78151f2afc4e6f3", "message": "[BEAM-10124] Change default for RecordNum to be false.\nAdd Check for trigger\nChange module location", "committedDate": "2020-09-28T04:40:07Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQyNDYwOQ==", "url": "https://github.com/apache/beam/pull/12924#discussion_r498424609", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * record positions currently introduces a shuffle step, which increases the resources used by the\n          \n          \n            \n             * record positions currently introduces a grouping step, which increases the resources used by the", "author": "lukecwik", "createdAt": "2020-10-01T18:02:33Z", "path": "sdks/java/io/contextualtextio/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -167,26 +177,26 @@\n  *      .apply(ContextualTextIO.readFiles());\n  * }</pre>\n  *\n- * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n- * Objects would still contain recordNums, but these recordNums would correspond to their positions\n- * in their respective offsets rather than their positions within the entire file).\n+ * <p>Example 6: reading with recordNum metadata. (the Objects still contain recordNums, but these\n+ * recordNums would correspond to their positions in their respective offsets rather than their\n+ * positions within the entire file).\n  *\n  * <pre>{@code\n  * Pipeline p = ...;\n  *\n  * PCollection<Row> records = p.apply(ContextualTextIO.read()\n  *     .from(\"/local/path/to/files/*.csv\")\n- *      .setWithoutRecordNumMetadata(true));\n+ *      .setWithRecordNumMetadata(true));\n  * }</pre>\n  *\n  * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n  * option, a single reader will be used to process the file, rather than multiple readers which can\n  * read from different offsets. For a large file this can result in lower performance.\n  *\n- * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n- * example, when when only filename metadata is required. Computing record positions currently\n- * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n- * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ * <p>NOTE: Use {@link Read#withRecordNumMetadata()} when recordNum metadata is required. Computing\n+ * record positions currently introduces a shuffle step, which increases the resources used by the", "originalCommit": "46fc4729775d1c5fe6232010a78151f2afc4e6f3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQyNTU2OQ==", "url": "https://github.com/apache/beam/pull/12924#discussion_r498425569", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                          \"getWithRecordNumMetadata only support the default trigger not. %s\", currentTrigger));\n          \n          \n            \n                          \"getWithRecordNumMetadata(true) only supports the default trigger not: %s\", currentTrigger));", "author": "lukecwik", "createdAt": "2020-10-01T18:04:32Z", "path": "sdks/java/io/contextualtextio/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -637,4 +619,68 @@ private CreateTextSourceFn(byte[] delimiter, boolean hasMultilineCSVRecords) {\n \n   /** Disable construction of utility class. */\n   private ContextualTextIO() {}\n+\n+  private static class ProcessRecordNumbers extends PTransform<PCollection<Row>, PCollection<Row>> {\n+\n+    @Override\n+    public PCollection<Row> expand(PCollection<Row> records) {\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      // This algorithm only works with triggers that fire once, for now only default trigger is\n+      // supported.\n+      Trigger currentTrigger = records.getWindowingStrategy().getTrigger();\n+\n+      Set<Trigger> allowedTriggers =\n+          ImmutableSet.of(\n+              Repeatedly.forever(AfterWatermark.pastEndOfWindow()), DefaultTrigger.of());\n+\n+      Preconditions.checkArgument(\n+          allowedTriggers.contains(currentTrigger),\n+          String.format(\n+              \"getWithRecordNumMetadata only support the default trigger not. %s\", currentTrigger));", "originalCommit": "46fc4729775d1c5fe6232010a78151f2afc4e6f3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQyNjE1MA==", "url": "https://github.com/apache/beam/pull/12924#discussion_r498426150", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                   * After computing the number of lines before each range, we can find the line number in original file as numLiesBeforeOffset + lineNumInCurrentOffset\n          \n          \n            \n                   * After computing the number of lines before each range, we can find the line number in original file as numLinesBeforeOffset + lineNumInCurrentOffset", "author": "lukecwik", "createdAt": "2020-10-01T18:05:43Z", "path": "sdks/java/io/contextualtextio/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -637,4 +619,68 @@ private CreateTextSourceFn(byte[] delimiter, boolean hasMultilineCSVRecords) {\n \n   /** Disable construction of utility class. */\n   private ContextualTextIO() {}\n+\n+  private static class ProcessRecordNumbers extends PTransform<PCollection<Row>, PCollection<Row>> {\n+\n+    @Override\n+    public PCollection<Row> expand(PCollection<Row> records) {\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      // This algorithm only works with triggers that fire once, for now only default trigger is\n+      // supported.\n+      Trigger currentTrigger = records.getWindowingStrategy().getTrigger();\n+\n+      Set<Trigger> allowedTriggers =\n+          ImmutableSet.of(\n+              Repeatedly.forever(AfterWatermark.pastEndOfWindow()), DefaultTrigger.of());\n+\n+      Preconditions.checkArgument(\n+          allowedTriggers.contains(currentTrigger),\n+          String.format(\n+              \"getWithRecordNumMetadata only support the default trigger not. %s\", currentTrigger));\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =\n+          records\n+              .apply(\"AddFileNameAndRange\", ParDo.of(new AddFileNameAndRange()))\n+              .setCoder(\n+                  KvCoder.of(\n+                      KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()),\n+                      RowCoder.of(RecordWithMetadata.getSchema())));\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> rangeSizes =\n+          recordsGroupedByFileAndRange\n+              .apply(\"CountRecordsForEachFileRange\", Count.perKey())\n+              .apply(\"SizesAsView\", View.asMap());\n+\n+      // Get Pipeline to create a dummy PCollection with one element to help compute the lines\n+      // before each Range\n+      PCollection<Integer> singletonPcoll =\n+          records.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));\n+\n+      /*\n+       * For each (File, Offset) pair, calculate the number of lines occurring before the Range for each file\n+       *\n+       * After computing the number of lines before each range, we can find the line number in original file as numLiesBeforeOffset + lineNumInCurrentOffset", "originalCommit": "46fc4729775d1c5fe6232010a78151f2afc4e6f3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "35d8567fd60058365122d0c4da925de765293134", "url": "https://github.com/apache/beam/commit/35d8567fd60058365122d0c4da925de765293134", "message": "[BEAM-10124] Change default for RecordNum to be false.\nAdd Check for trigger\nChange module location\nChange to MultiMapView", "committedDate": "2020-10-05T04:24:34Z", "type": "commit"}, {"oid": "35d8567fd60058365122d0c4da925de765293134", "url": "https://github.com/apache/beam/commit/35d8567fd60058365122d0c4da925de765293134", "message": "[BEAM-10124] Change default for RecordNum to be false.\nAdd Check for trigger\nChange module location\nChange to MultiMapView", "committedDate": "2020-10-05T04:24:34Z", "type": "forcePushed"}]}