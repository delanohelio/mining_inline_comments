{"pr_number": 13572, "pr_title": "[BEAM-11482] Thrift support for KafkaTableProvider", "pr_createdAt": "2020-12-17T14:40:34Z", "pr_url": "https://github.com/apache/beam/pull/13572", "timeline": [{"oid": "a42657c783e459f4c79fcac24c51c5dc86dca9f9", "url": "https://github.com/apache/beam/commit/a42657c783e459f4c79fcac24c51c5dc86dca9f9", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-17T14:46:19Z", "type": "forcePushed"}, {"oid": "73397c9b0f0e9766b6e6c3b191aa59c918cbd412", "url": "https://github.com/apache/beam/commit/73397c9b0f0e9766b6e6c3b191aa59c918cbd412", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-17T14:54:08Z", "type": "forcePushed"}, {"oid": "945fe14c57b3117579513fcc0cf71e2cc94a07df", "url": "https://github.com/apache/beam/commit/945fe14c57b3117579513fcc0cf71e2cc94a07df", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-18T13:29:17Z", "type": "forcePushed"}, {"oid": "5eae86159d692c2db98fbfcd1f62939b2a53e702", "url": "https://github.com/apache/beam/commit/5eae86159d692c2db98fbfcd1f62939b2a53e702", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-18T16:13:35Z", "type": "forcePushed"}, {"oid": "a203a0969bcf450bf92a43d1ad7e16467f1131cd", "url": "https://github.com/apache/beam/commit/a203a0969bcf450bf92a43d1ad7e16467f1131cd", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-18T16:32:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0MzExNw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546243117", "bodyText": "I may be wrong but I think that@NonNull annotations are quite redundant as it's the default for the checker and only @Nullable params are worth annotating. The same below.", "author": "piotr-szuberski", "createdAt": "2020-12-19T14:16:22Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaThriftTable.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.beam.sdk.io.thrift.ThriftCoder;\n+import org.apache.beam.sdk.io.thrift.ThriftSchema;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaProvider;\n+import org.apache.beam.sdk.schemas.transforms.Convert;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+import org.apache.thrift.TBase;\n+import org.apache.thrift.TFieldIdEnum;\n+import org.apache.thrift.protocol.TProtocolFactory;\n+import org.checkerframework.checker.nullness.qual.NonNull;\n+\n+public class BeamKafkaThriftTable<FieldT extends TFieldIdEnum, T extends TBase<T, FieldT>>\n+    extends BeamKafkaTable {\n+  private final ThriftCoder<T> thriftCoder;\n+  private final TypeDescriptor<T> typeDescriptor;\n+\n+  public BeamKafkaThriftTable(\n+      @NonNull Schema requiredSchema,", "originalCommit": "a203a0969bcf450bf92a43d1ad7e16467f1131cd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0MzM0OQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546243349", "bodyText": "Can't it be written without suppressing the nullness? Sometimes checker is awkward and requires some workarounds but I think it should be possible.", "author": "piotr-szuberski", "createdAt": "2020-12-19T14:18:33Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaThriftTable.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.beam.sdk.io.thrift.ThriftCoder;\n+import org.apache.beam.sdk.io.thrift.ThriftSchema;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaProvider;\n+import org.apache.beam.sdk.schemas.transforms.Convert;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+import org.apache.thrift.TBase;\n+import org.apache.thrift.TFieldIdEnum;\n+import org.apache.thrift.protocol.TProtocolFactory;\n+import org.checkerframework.checker.nullness.qual.NonNull;\n+\n+public class BeamKafkaThriftTable<FieldT extends TFieldIdEnum, T extends TBase<T, FieldT>>\n+    extends BeamKafkaTable {\n+  private final ThriftCoder<T> thriftCoder;\n+  private final TypeDescriptor<T> typeDescriptor;\n+\n+  public BeamKafkaThriftTable(\n+      @NonNull Schema requiredSchema,\n+      @NonNull String bootstrapServers,\n+      @NonNull List<String> topics,\n+      @NonNull Class<T> thriftClass,\n+      @NonNull TProtocolFactory protocolFactory) {\n+    super(thriftSchema(thriftClass, requiredSchema), bootstrapServers, topics);\n+    typeDescriptor = TypeDescriptor.of(thriftClass);\n+    thriftCoder = ThriftCoder.of(thriftClass, protocolFactory);\n+  }\n+\n+  private static Schema thriftSchema(\n+      @NonNull Class<?> thriftClass, @NonNull Schema requiredSchema) {\n+    @SuppressWarnings(\"nullness\")", "originalCommit": "a203a0969bcf450bf92a43d1ad7e16467f1131cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI5Mjg0MQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546292841", "bodyText": "I'll try that.", "author": "ccciudatu", "createdAt": "2020-12-19T22:47:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0MzM0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0MzQ1OA==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546243458", "bodyText": "As the rest of @NonNull when a param is not @Nullable we assume it can't be null", "author": "piotr-szuberski", "createdAt": "2020-12-19T14:19:52Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaThriftTable.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.beam.sdk.io.thrift.ThriftCoder;\n+import org.apache.beam.sdk.io.thrift.ThriftSchema;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaProvider;\n+import org.apache.beam.sdk.schemas.transforms.Convert;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+import org.apache.thrift.TBase;\n+import org.apache.thrift.TFieldIdEnum;\n+import org.apache.thrift.protocol.TProtocolFactory;\n+import org.checkerframework.checker.nullness.qual.NonNull;\n+\n+public class BeamKafkaThriftTable<FieldT extends TFieldIdEnum, T extends TBase<T, FieldT>>\n+    extends BeamKafkaTable {\n+  private final ThriftCoder<T> thriftCoder;\n+  private final TypeDescriptor<T> typeDescriptor;\n+\n+  public BeamKafkaThriftTable(\n+      @NonNull Schema requiredSchema,\n+      @NonNull String bootstrapServers,\n+      @NonNull List<String> topics,\n+      @NonNull Class<T> thriftClass,\n+      @NonNull TProtocolFactory protocolFactory) {\n+    super(thriftSchema(thriftClass, requiredSchema), bootstrapServers, topics);\n+    typeDescriptor = TypeDescriptor.of(thriftClass);\n+    thriftCoder = ThriftCoder.of(thriftClass, protocolFactory);\n+  }\n+\n+  private static Schema thriftSchema(\n+      @NonNull Class<?> thriftClass, @NonNull Schema requiredSchema) {\n+    @SuppressWarnings(\"nullness\")\n+    final @NonNull Schema thriftSchema =", "originalCommit": "a203a0969bcf450bf92a43d1ad7e16467f1131cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjM3NTE1OQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546375159", "bodyText": "SchemaProvider.schemaFor() returns a @Nullable schema, so I was trying to force a \"cast\" to @NonNull here.\nLooks like the only elegant way around this is to use Preconditions.checkArgumentNotNull() (the beam preconditions, as the guava ones or the standard Objects.requireNonNull() won't work).", "author": "ccciudatu", "createdAt": "2020-12-20T13:09:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0MzQ1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0Mzc1OQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546243759", "bodyText": "I'd try to get rid of this nullness suppression. Usually if (var != null) {} else { throw NullPointerException(\"var was null\") is unfortunatelly needed to achieve that (that's my experience, maybe there is some cleaner way that I don't know about)", "author": "piotr-szuberski", "createdAt": "2020-12-19T14:23:11Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaThriftTable.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.beam.sdk.io.thrift.ThriftCoder;\n+import org.apache.beam.sdk.io.thrift.ThriftSchema;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaProvider;\n+import org.apache.beam.sdk.schemas.transforms.Convert;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+import org.apache.thrift.TBase;\n+import org.apache.thrift.TFieldIdEnum;\n+import org.apache.thrift.protocol.TProtocolFactory;\n+import org.checkerframework.checker.nullness.qual.NonNull;\n+\n+public class BeamKafkaThriftTable<FieldT extends TFieldIdEnum, T extends TBase<T, FieldT>>\n+    extends BeamKafkaTable {\n+  private final ThriftCoder<T> thriftCoder;\n+  private final TypeDescriptor<T> typeDescriptor;\n+\n+  public BeamKafkaThriftTable(\n+      @NonNull Schema requiredSchema,\n+      @NonNull String bootstrapServers,\n+      @NonNull List<String> topics,\n+      @NonNull Class<T> thriftClass,\n+      @NonNull TProtocolFactory protocolFactory) {\n+    super(thriftSchema(thriftClass, requiredSchema), bootstrapServers, topics);\n+    typeDescriptor = TypeDescriptor.of(thriftClass);\n+    thriftCoder = ThriftCoder.of(thriftClass, protocolFactory);\n+  }\n+\n+  private static Schema thriftSchema(\n+      @NonNull Class<?> thriftClass, @NonNull Schema requiredSchema) {\n+    @SuppressWarnings(\"nullness\")\n+    final @NonNull Schema thriftSchema =\n+        ThriftSchema.provider().schemaFor(TypeDescriptor.of(thriftClass));\n+    if (!requiredSchema.equivalent(thriftSchema)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Given message schema: '%s'%n\"\n+                  + \"does not match schema inferred from thrift class.%n\"\n+                  + \"Thrift class: '%s'%n\"\n+                  + \"Inferred schema: '%s'\",\n+              requiredSchema, thriftClass.getName(), thriftSchema));\n+    }\n+    return thriftSchema;\n+  }\n+\n+  @Override\n+  protected PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>> getPTransformForInput() {\n+    final @NonNull SchemaProvider schemaProvider = ThriftSchema.provider();\n+    return new PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>>() {\n+      @Override\n+      @SuppressWarnings(\"nullness\")", "originalCommit": "a203a0969bcf450bf92a43d1ad7e16467f1131cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI5MjkyMQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546292921", "bodyText": "I'll give it a try.", "author": "ccciudatu", "createdAt": "2020-12-19T22:48:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0Mzc1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0Mzg3NQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546243875", "bodyText": "Wouldn't this::decode be sufficient?", "author": "piotr-szuberski", "createdAt": "2020-12-19T14:24:26Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaThriftTable.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.beam.sdk.io.thrift.ThriftCoder;\n+import org.apache.beam.sdk.io.thrift.ThriftSchema;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaProvider;\n+import org.apache.beam.sdk.schemas.transforms.Convert;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+import org.apache.thrift.TBase;\n+import org.apache.thrift.TFieldIdEnum;\n+import org.apache.thrift.protocol.TProtocolFactory;\n+import org.checkerframework.checker.nullness.qual.NonNull;\n+\n+public class BeamKafkaThriftTable<FieldT extends TFieldIdEnum, T extends TBase<T, FieldT>>\n+    extends BeamKafkaTable {\n+  private final ThriftCoder<T> thriftCoder;\n+  private final TypeDescriptor<T> typeDescriptor;\n+\n+  public BeamKafkaThriftTable(\n+      @NonNull Schema requiredSchema,\n+      @NonNull String bootstrapServers,\n+      @NonNull List<String> topics,\n+      @NonNull Class<T> thriftClass,\n+      @NonNull TProtocolFactory protocolFactory) {\n+    super(thriftSchema(thriftClass, requiredSchema), bootstrapServers, topics);\n+    typeDescriptor = TypeDescriptor.of(thriftClass);\n+    thriftCoder = ThriftCoder.of(thriftClass, protocolFactory);\n+  }\n+\n+  private static Schema thriftSchema(\n+      @NonNull Class<?> thriftClass, @NonNull Schema requiredSchema) {\n+    @SuppressWarnings(\"nullness\")\n+    final @NonNull Schema thriftSchema =\n+        ThriftSchema.provider().schemaFor(TypeDescriptor.of(thriftClass));\n+    if (!requiredSchema.equivalent(thriftSchema)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Given message schema: '%s'%n\"\n+                  + \"does not match schema inferred from thrift class.%n\"\n+                  + \"Thrift class: '%s'%n\"\n+                  + \"Inferred schema: '%s'\",\n+              requiredSchema, thriftClass.getName(), thriftSchema));\n+    }\n+    return thriftSchema;\n+  }\n+\n+  @Override\n+  protected PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>> getPTransformForInput() {\n+    final @NonNull SchemaProvider schemaProvider = ThriftSchema.provider();\n+    return new PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>>() {\n+      @Override\n+      @SuppressWarnings(\"nullness\")\n+      public PCollection<Row> expand(PCollection<KV<byte[], byte[]>> input) {\n+        return input\n+            .apply(Values.create())\n+            .apply(MapElements.into(typeDescriptor).via(BeamKafkaThriftTable.this::decode))", "originalCommit": "a203a0969bcf450bf92a43d1ad7e16467f1131cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0NzY2NA==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546247664", "bodyText": "Just one more comment.\nIt would be good to have some SimpleFunction or DoFn declared somewhere in Thrift package like it is done for protobuf so we don't have to repeat methods like encode and decode together with ThriftCoder in every table provider that supports thrift.\nWe could then just import a function returning SimpleFunction/DoFnand use it like MapElements.via(thriftBytesToBeamRowFn(thriftClass, protocolFactory)) or ParDo.of(thriftBytesToBeamRowFn)\nProtobuf example: \n  \n    \n      beam/sdks/java/extensions/protobuf/src/main/java/org/apache/beam/sdk/extensions/protobuf/ProtoMessageSchema.java\n    \n    \n         Line 137\n      in\n      1466db9\n    \n    \n    \n    \n\n        \n          \n           private static class ProtoBytesToRowFn<T extends Message> extends SimpleFunction<byte[], Row> {", "author": "piotr-szuberski", "createdAt": "2020-12-19T15:02:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0Mzg3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0Mzk5Mg==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546243992", "bodyText": "Could you add some message to this exception?", "author": "piotr-szuberski", "createdAt": "2020-12-19T14:25:33Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaThriftTable.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.beam.sdk.io.thrift.ThriftCoder;\n+import org.apache.beam.sdk.io.thrift.ThriftSchema;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaProvider;\n+import org.apache.beam.sdk.schemas.transforms.Convert;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+import org.apache.thrift.TBase;\n+import org.apache.thrift.TFieldIdEnum;\n+import org.apache.thrift.protocol.TProtocolFactory;\n+import org.checkerframework.checker.nullness.qual.NonNull;\n+\n+public class BeamKafkaThriftTable<FieldT extends TFieldIdEnum, T extends TBase<T, FieldT>>\n+    extends BeamKafkaTable {\n+  private final ThriftCoder<T> thriftCoder;\n+  private final TypeDescriptor<T> typeDescriptor;\n+\n+  public BeamKafkaThriftTable(\n+      @NonNull Schema requiredSchema,\n+      @NonNull String bootstrapServers,\n+      @NonNull List<String> topics,\n+      @NonNull Class<T> thriftClass,\n+      @NonNull TProtocolFactory protocolFactory) {\n+    super(thriftSchema(thriftClass, requiredSchema), bootstrapServers, topics);\n+    typeDescriptor = TypeDescriptor.of(thriftClass);\n+    thriftCoder = ThriftCoder.of(thriftClass, protocolFactory);\n+  }\n+\n+  private static Schema thriftSchema(\n+      @NonNull Class<?> thriftClass, @NonNull Schema requiredSchema) {\n+    @SuppressWarnings(\"nullness\")\n+    final @NonNull Schema thriftSchema =\n+        ThriftSchema.provider().schemaFor(TypeDescriptor.of(thriftClass));\n+    if (!requiredSchema.equivalent(thriftSchema)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Given message schema: '%s'%n\"\n+                  + \"does not match schema inferred from thrift class.%n\"\n+                  + \"Thrift class: '%s'%n\"\n+                  + \"Inferred schema: '%s'\",\n+              requiredSchema, thriftClass.getName(), thriftSchema));\n+    }\n+    return thriftSchema;\n+  }\n+\n+  @Override\n+  protected PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>> getPTransformForInput() {\n+    final @NonNull SchemaProvider schemaProvider = ThriftSchema.provider();\n+    return new PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>>() {\n+      @Override\n+      @SuppressWarnings(\"nullness\")\n+      public PCollection<Row> expand(PCollection<KV<byte[], byte[]>> input) {\n+        return input\n+            .apply(Values.create())\n+            .apply(MapElements.into(typeDescriptor).via(BeamKafkaThriftTable.this::decode))\n+            .setSchema(\n+                schema,\n+                typeDescriptor,\n+                schemaProvider.toRowFunction(typeDescriptor),\n+                schemaProvider.fromRowFunction(typeDescriptor))\n+            .apply(Convert.toRows());\n+      }\n+    };\n+  }\n+\n+  private T decode(byte[] bytes) {\n+    try {\n+      return thriftCoder.decode(new ByteArrayInputStream(bytes));\n+    } catch (IOException e) {\n+      throw new IllegalStateException(e);", "originalCommit": "a203a0969bcf450bf92a43d1ad7e16467f1131cd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0NDI4OA==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546244288", "bodyText": "Please add some message to the exception", "author": "piotr-szuberski", "createdAt": "2020-12-19T14:28:07Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaThriftTable.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.beam.sdk.io.thrift.ThriftCoder;\n+import org.apache.beam.sdk.io.thrift.ThriftSchema;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaProvider;\n+import org.apache.beam.sdk.schemas.transforms.Convert;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+import org.apache.thrift.TBase;\n+import org.apache.thrift.TFieldIdEnum;\n+import org.apache.thrift.protocol.TProtocolFactory;\n+import org.checkerframework.checker.nullness.qual.NonNull;\n+\n+public class BeamKafkaThriftTable<FieldT extends TFieldIdEnum, T extends TBase<T, FieldT>>\n+    extends BeamKafkaTable {\n+  private final ThriftCoder<T> thriftCoder;\n+  private final TypeDescriptor<T> typeDescriptor;\n+\n+  public BeamKafkaThriftTable(\n+      @NonNull Schema requiredSchema,\n+      @NonNull String bootstrapServers,\n+      @NonNull List<String> topics,\n+      @NonNull Class<T> thriftClass,\n+      @NonNull TProtocolFactory protocolFactory) {\n+    super(thriftSchema(thriftClass, requiredSchema), bootstrapServers, topics);\n+    typeDescriptor = TypeDescriptor.of(thriftClass);\n+    thriftCoder = ThriftCoder.of(thriftClass, protocolFactory);\n+  }\n+\n+  private static Schema thriftSchema(\n+      @NonNull Class<?> thriftClass, @NonNull Schema requiredSchema) {\n+    @SuppressWarnings(\"nullness\")\n+    final @NonNull Schema thriftSchema =\n+        ThriftSchema.provider().schemaFor(TypeDescriptor.of(thriftClass));\n+    if (!requiredSchema.equivalent(thriftSchema)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Given message schema: '%s'%n\"\n+                  + \"does not match schema inferred from thrift class.%n\"\n+                  + \"Thrift class: '%s'%n\"\n+                  + \"Inferred schema: '%s'\",\n+              requiredSchema, thriftClass.getName(), thriftSchema));\n+    }\n+    return thriftSchema;\n+  }\n+\n+  @Override\n+  protected PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>> getPTransformForInput() {\n+    final @NonNull SchemaProvider schemaProvider = ThriftSchema.provider();\n+    return new PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>>() {\n+      @Override\n+      @SuppressWarnings(\"nullness\")\n+      public PCollection<Row> expand(PCollection<KV<byte[], byte[]>> input) {\n+        return input\n+            .apply(Values.create())\n+            .apply(MapElements.into(typeDescriptor).via(BeamKafkaThriftTable.this::decode))\n+            .setSchema(\n+                schema,\n+                typeDescriptor,\n+                schemaProvider.toRowFunction(typeDescriptor),\n+                schemaProvider.fromRowFunction(typeDescriptor))\n+            .apply(Convert.toRows());\n+      }\n+    };\n+  }\n+\n+  private T decode(byte[] bytes) {\n+    try {\n+      return thriftCoder.decode(new ByteArrayInputStream(bytes));\n+    } catch (IOException e) {\n+      throw new IllegalStateException(e);\n+    }\n+  }\n+\n+  @Override\n+  protected PTransform<PCollection<Row>, PCollection<KV<byte[], byte[]>>> getPTransformForOutput() {\n+    final byte[] emptyKey = {};\n+    final TypeDescriptor<byte[]> binTypeDescriptor = TypeDescriptor.of(byte[].class);\n+    return new PTransform<PCollection<Row>, PCollection<KV<byte[], byte[]>>>() {\n+      @Override\n+      public PCollection<KV<byte[], byte[]>> expand(PCollection<Row> input) {\n+        return input\n+            .apply(Convert.fromRows(typeDescriptor))\n+            .apply(\n+                MapElements.into(TypeDescriptors.kvs(binTypeDescriptor, binTypeDescriptor))\n+                    .via(thrift -> KV.of(emptyKey, encode(thrift))));\n+      }\n+    };\n+  }\n+\n+  private byte[] encode(T thrift) {\n+    final ByteArrayOutputStream out = new ByteArrayOutputStream();\n+    try {\n+      thriftCoder.encode(thrift, out);\n+      return out.toByteArray();\n+    } catch (IOException e) {\n+      throw new IllegalStateException(e);", "originalCommit": "a203a0969bcf450bf92a43d1ad7e16467f1131cd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0NDY5Mw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546244693", "bodyText": "I'd delegate this part of code to a function. It'll get a bit messy after we add another case to the switch statement.", "author": "piotr-szuberski", "createdAt": "2020-12-19T14:33:06Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProvider.java", "diffHunk": "@@ -85,6 +89,33 @@ public BeamSqlTable buildBeamSqlTable(Table table) {\n         } catch (ClassNotFoundException e) {\n           throw new IllegalArgumentException(\"Incorrect proto class provided: \" + protoClassName);\n         }\n+      case THRIFT:\n+        final String thriftClassName = properties.getString(\"thriftClass\");", "originalCommit": "a203a0969bcf450bf92a43d1ad7e16467f1131cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI5MjE4Nw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546292187", "bodyText": "Everything else was inlined here, so I thought I'd keep it \"consistent\".\nBut you're right, thrift also requires a protocol spec, which makes it even more involved than the proto case, so it probably deserves its own method.", "author": "ccciudatu", "createdAt": "2020-12-19T22:40:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0NDY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjM3MTg1Mw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546371853", "bodyText": "I agree that at least PROTO case should be delegated to a method as well", "author": "piotr-szuberski", "createdAt": "2020-12-20T12:40:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0NDY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjM3MzIzMQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546373231", "bodyText": "I'll extract that one too", "author": "ccciudatu", "createdAt": "2020-12-20T12:53:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0NDY5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0NDgxMw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546244813", "bodyText": "Please add a message here as well.", "author": "piotr-szuberski", "createdAt": "2020-12-19T14:34:02Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProvider.java", "diffHunk": "@@ -85,6 +89,33 @@ public BeamSqlTable buildBeamSqlTable(Table table) {\n         } catch (ClassNotFoundException e) {\n           throw new IllegalArgumentException(\"Incorrect proto class provided: \" + protoClassName);\n         }\n+      case THRIFT:\n+        final String thriftClassName = properties.getString(\"thriftClass\");\n+        final String thriftProtocolFactoryClassName =\n+            properties.getString(\"thriftProtocolFactoryClass\");\n+        try {\n+          final Class<TBase> thriftClass = (Class<TBase>) Class.forName(thriftClassName);\n+          final TProtocolFactory thriftProtocolFactory;\n+          try {\n+            final Class<TProtocolFactory> thriftProtocolFactoryClass =\n+                (Class<TProtocolFactory>) Class.forName(thriftProtocolFactoryClassName);\n+            thriftProtocolFactory =\n+                thriftProtocolFactoryClass.getDeclaredConstructor().newInstance();\n+          } catch (ClassNotFoundException e) {\n+            throw new IllegalArgumentException(\n+                \"Incorrect thrift protocol factory class provided: \"\n+                    + thriftProtocolFactoryClassName);\n+          } catch (InstantiationException\n+              | IllegalAccessException\n+              | InvocationTargetException\n+              | NoSuchMethodException e) {\n+            throw new IllegalStateException(e);", "originalCommit": "a203a0969bcf450bf92a43d1ad7e16467f1131cd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0NTEwNQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546245105", "bodyText": "if {} else {} or  return fieldDescriptor.requirementType == REQUIRED ? ... : ... would be neater as there is only one case.", "author": "piotr-szuberski", "createdAt": "2020-12-19T14:36:34Z", "path": "sdks/java/io/thrift/src/main/java/org/apache/beam/sdk/io/thrift/ThriftSchema.java", "diffHunk": "@@ -183,7 +184,12 @@ private Schema schemaFor(Class<?> targetClass) {\n   private Schema.Field beamField(FieldMetaData fieldDescriptor) {\n     try {\n       final FieldType type = beamType(fieldDescriptor.valueMetaData);\n-      return Schema.Field.nullable(fieldDescriptor.fieldName, type);\n+      switch (fieldDescriptor.requirementType) {", "originalCommit": "a203a0969bcf450bf92a43d1ad7e16467f1131cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI5MTM5OA==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546291398", "bodyText": "Agreed.\nMy intention here was to leave room for special handling of the DEFAULT \"field requiredness\", as that's somewhere in between, i.e. defined as \"opt-in, req-out\", so it may not always fall into the \"nullable\" default case, the same as OPTIONAL.\nWould it be better if I listed all the options (with some comments, perhaps) and just fall-through to the default (nullable) case?\nI can also switch (pun intended) to an if/else or ternary statement, if you think we don't have to make it obvious that each thrift requirement type can be handled separately.", "author": "ccciudatu", "createdAt": "2020-12-19T22:32:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0NTEwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjM3MjA1MQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546372051", "bodyText": "Well. it's purely cosmetic, I agree that switch is more readable and makes it clear that there are another options. I'd leave it as it is. Listing all the remaining options is definitely redundant.", "author": "piotr-szuberski", "createdAt": "2020-12-20T12:42:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI0NTEwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI4MjQzMA==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546282430", "bodyText": "The last comment from me. Could you also add a test that will run on SQL precommit as well? You could use the same thrift file as in the IT test.\nExample here:\n\n  \n    \n      beam/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaTableAvroTest.java\n    \n    \n         Line 31\n      in\n      1466db9\n    \n    \n    \n    \n\n        \n          \n           public class BeamKafkaTableAvroTest extends BeamKafkaTableTest { \n        \n    \n  \n\n\nAnd a unit test testBuildBeamSqlThriftTable for thrift table creation, like here:\n\n  \n    \n      beam/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderTest.java\n    \n    \n         Line 66\n      in\n      1466db9\n    \n    \n    \n    \n\n        \n          \n           public void testBuildBeamSqlProtoTable() {", "author": "piotr-szuberski", "createdAt": "2020-12-19T21:01:14Z", "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderThriftIT.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*", "originalCommit": "a203a0969bcf450bf92a43d1ad7e16467f1131cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI5MTA5OA==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546291098", "bodyText": "Sure, I'll add those.", "author": "ccciudatu", "createdAt": "2020-12-19T22:28:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI4MjQzMA=="}], "type": "inlineReview"}, {"oid": "bd1aded20c05012197eacff2faac33c8f902533e", "url": "https://github.com/apache/beam/commit/bd1aded20c05012197eacff2faac33c8f902533e", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T12:55:17Z", "type": "forcePushed"}, {"oid": "c0680a4e3da57e8c239009d8bed73de51e663846", "url": "https://github.com/apache/beam/commit/c0680a4e3da57e8c239009d8bed73de51e663846", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T13:09:57Z", "type": "forcePushed"}, {"oid": "52487eb98bfe672b4cd81b0892686587f5ed3016", "url": "https://github.com/apache/beam/commit/52487eb98bfe672b4cd81b0892686587f5ed3016", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T13:17:34Z", "type": "forcePushed"}, {"oid": "98f695906f4dae8ebc676e030694ae24ac0e70bc", "url": "https://github.com/apache/beam/commit/98f695906f4dae8ebc676e030694ae24ac0e70bc", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T13:20:18Z", "type": "forcePushed"}, {"oid": "d9ecd618409f7305ffad602d711553aa04c440f2", "url": "https://github.com/apache/beam/commit/d9ecd618409f7305ffad602d711553aa04c440f2", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T13:22:48Z", "type": "forcePushed"}, {"oid": "3f24ab1333689af592520138c3a239948d0d9c7d", "url": "https://github.com/apache/beam/commit/3f24ab1333689af592520138c3a239948d0d9c7d", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T13:33:09Z", "type": "forcePushed"}, {"oid": "5ab68f72686311f08290e086477593510ededccd", "url": "https://github.com/apache/beam/commit/5ab68f72686311f08290e086477593510ededccd", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T14:16:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjM4MjM0Nw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546382347", "bodyText": "Good catch! Thanks!", "author": "piotr-szuberski", "createdAt": "2020-12-20T14:11:26Z", "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderTest.java", "diffHunk": "@@ -64,15 +66,34 @@ public void testBuildBeamSqlAvroTable() {\n \n   @Test\n   public void testBuildBeamSqlProtoTable() {\n-    Table table = mockTable(\"hello\", \"proto\", KafkaMessages.SimpleMessage.class.getName());\n+    Table table =\n+        mockTable(\"hello\", \"proto\", KafkaMessages.SimpleMessage.class.getName(), null, null);\n     BeamSqlTable sqlTable = provider.buildBeamSqlTable(table);\n \n     assertNotNull(sqlTable);\n     assertTrue(sqlTable instanceof BeamKafkaProtoTable);\n \n-    BeamKafkaProtoTable csvTable = (BeamKafkaProtoTable) sqlTable;\n-    assertEquals(\"localhost:9092\", csvTable.getBootstrapServers());\n-    assertEquals(ImmutableList.of(\"topic1\", \"topic2\"), csvTable.getTopics());\n+    BeamKafkaProtoTable protoTable = (BeamKafkaProtoTable) sqlTable;", "originalCommit": "3f24ab1333689af592520138c3a239948d0d9c7d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjM4NTcxNQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546385715", "bodyText": "What I meant was to create some SerializableFunction or SimpleFuntion outside of this package (e.g. in  io/thrift) that would do the encoding/decoding. Right now we'll have to implement encode/decode it in every table provider that supports thrift format. You wouldn't need InputTransformer and OutputTransformer then.\nWhat I have in mind it's the same as is done here:\n\n  \n    \n      beam/sdks/java/extensions/protobuf/src/main/java/org/apache/beam/sdk/extensions/protobuf/ProtoMessageSchema.java\n    \n    \n         Line 132\n      in\n      1466db9\n    \n    \n    \n    \n\n        \n          \n           public static <T> SimpleFunction<byte[], Row> getProtoBytesToRowFn(Class<T> clazz) { \n        \n    \n  \n\n\nAnd then here:\n\n  \n    \n      beam/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaProtoTable.java\n    \n    \n         Line 83\n      in\n      1466db9\n    \n    \n    \n    \n\n        \n          \n           .apply(\"Map bytes to rows\", MapElements.via(toRowFn)) \n        \n    \n  \n\n\nonly for Thrift encoding/decoding", "author": "piotr-szuberski", "createdAt": "2020-12-20T14:39:20Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaThriftTable.java", "diffHunk": "@@ -74,55 +92,81 @@ private static Schema thriftSchema(\n \n   @Override\n   protected PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>> getPTransformForInput() {\n-    final @NonNull SchemaProvider schemaProvider = ThriftSchema.provider();\n-    return new PTransform<PCollection<KV<byte[], byte[]>>, PCollection<Row>>() {\n-      @Override\n-      @SuppressWarnings(\"nullness\")\n-      public PCollection<Row> expand(PCollection<KV<byte[], byte[]>> input) {\n-        return input\n-            .apply(Values.create())\n-            .apply(MapElements.into(typeDescriptor).via(BeamKafkaThriftTable.this::decode))\n-            .setSchema(\n-                schema,\n-                typeDescriptor,\n-                schemaProvider.toRowFunction(typeDescriptor),\n-                schemaProvider.fromRowFunction(typeDescriptor))\n-            .apply(Convert.toRows());\n-      }\n-    };\n+    return new InputTransformer(typeDescriptor, coder, schema);\n   }\n \n-  private T decode(byte[] bytes) {\n-    try {\n-      return thriftCoder.decode(new ByteArrayInputStream(bytes));\n-    } catch (IOException e) {\n-      throw new IllegalStateException(e);\n+  private static class InputTransformer<T extends TBase<?, ?>>", "originalCommit": "5ab68f72686311f08290e086477593510ededccd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjM4NzMyOQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546387329", "bodyText": "Got it, makes sense. I tried to solve it locally for Kafka, as I think making this truly reusable/composable requires a larger refactoring that should follow up. But those functions do seem to rather belong in the ThriftSchema anyway, so I'll fix this.", "author": "ccciudatu", "createdAt": "2020-12-20T14:53:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjM4NTcxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQwMzc5Nw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546403797", "bodyText": "I had a closer look and it seems like the proto bytesToRow and rowToBytes code has no real reason to be proto specific, as it can work with any SchemaProvider and Coder pair. I'll try to extract that into org.apache.beam.sdk.util and reuse it from both proto and thrift (and perhaps others, if I find more like this). This way I'll also add support for shuffled schemas in the thrift implementation for free.", "author": "ccciudatu", "createdAt": "2020-12-20T17:10:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjM4NTcxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQxMzMwMg==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546413302", "bodyText": "Just pushed another fixup for this. It's pretty big and I'll try to draw your attention to the potentially dangerous/incompatible changes.", "author": "ccciudatu", "createdAt": "2020-12-20T18:39:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjM4NTcxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQxNTE5MQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546415191", "bodyText": "I can revert the change breaking change in proto (i.e. Class<?> -> Class<? extends Message>) and use some ugly casts internally, if this is a concern.", "author": "ccciudatu", "createdAt": "2020-12-20T18:57:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjM4NTcxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQyOTAwNw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546429007", "bodyText": "Nevermind, I just noticed the comments which read:\n// Other modules are not allowed to use non-vendored Message class\n\nSo I'll revert that change.", "author": "ccciudatu", "createdAt": "2020-12-20T19:59:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjM4NTcxNQ=="}], "type": "inlineReview"}, {"oid": "14890bcfee5bb05ce9c6414f7bd9f7a783545945", "url": "https://github.com/apache/beam/commit/14890bcfee5bb05ce9c6414f7bd9f7a783545945", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T14:38:43Z", "type": "forcePushed"}, {"oid": "fc719a495343f192fec877bdae8cc0e9498dba10", "url": "https://github.com/apache/beam/commit/fc719a495343f192fec877bdae8cc0e9498dba10", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T18:41:21Z", "type": "forcePushed"}, {"oid": "488370798f06270d5eb431f2dd48746bded6a9c9", "url": "https://github.com/apache/beam/commit/488370798f06270d5eb431f2dd48746bded6a9c9", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T18:44:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQxMzk2NQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546413965", "bodyText": "This change breaks any clients that used to pass a mere Class<?> here!\nIs this acceptable?", "author": "ccciudatu", "createdAt": "2020-12-20T18:46:17Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaProtoTable.java", "diffHunk": "@@ -32,10 +34,10 @@\n   \"nullness\" // TODO(https://issues.apache.org/jira/browse/BEAM-10402)\n })\n public class BeamKafkaProtoTable extends BeamKafkaTable {\n-  private final Class<?> protoClass;\n+  private final Class<? extends Message> protoClass;\n \n   public BeamKafkaProtoTable(\n-      Schema messageSchema, String bootstrapServers, List<String> topics, Class<?> protoClass) {\n+      Schema messageSchema, String bootstrapServers, List<String> topics, Class<? extends Message> protoClass) {", "originalCommit": "488370798f06270d5eb431f2dd48746bded6a9c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQzMzA2OQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546433069", "bodyText": "reverted", "author": "ccciudatu", "createdAt": "2020-12-20T20:36:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQxMzk2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQxNDE0Mg==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546414142", "bodyText": "Is this acceptable?", "author": "ccciudatu", "createdAt": "2020-12-20T18:47:51Z", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/BeamKafkaProtoTable.java", "diffHunk": "@@ -95,9 +97,9 @@ private static Schema inferAndVerifySchema(Class<?> protoClass, Schema messageSc\n   /** A PTransform to convert {@link Row} to {@code KV<byte[], byte[]>}. */\n   private static class ProtoRecorderEncoder\n       extends PTransform<PCollection<Row>, PCollection<KV<byte[], byte[]>>> {\n-    private final Class<?> clazz;\n+    private final Class<? extends Message> clazz;\n \n-    public ProtoRecorderEncoder(Class<?> clazz) {\n+    public ProtoRecorderEncoder(Class<? extends Message> clazz) {", "originalCommit": "488370798f06270d5eb431f2dd48746bded6a9c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQzMzA0MQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546433041", "bodyText": "reverted", "author": "ccciudatu", "createdAt": "2020-12-20T20:36:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQxNDE0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQxNDQzMA==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546414430", "bodyText": "This again breaks compatibility with Class<?>.", "author": "ccciudatu", "createdAt": "2020-12-20T18:50:37Z", "path": "sdks/java/extensions/protobuf/src/main/java/org/apache/beam/sdk/extensions/protobuf/ProtoMessageSchema.java", "diffHunk": "@@ -129,68 +129,29 @@ public SchemaUserTypeCreator schemaTypeCreator(Class<?> targetClass, Schema sche\n     \"rawtypes\", // TODO(https://issues.apache.org/jira/browse/BEAM-10556)\n     \"unchecked\"\n   })\n-  public static <T> SimpleFunction<byte[], Row> getProtoBytesToRowFn(Class<T> clazz) {\n+  public static <T extends Message> SimpleFunction<byte[], Row> getProtoBytesToRowFn(", "originalCommit": "488370798f06270d5eb431f2dd48746bded6a9c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQzMzAxMA==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546433010", "bodyText": "reverted", "author": "ccciudatu", "createdAt": "2020-12-20T20:36:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQxNDQzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQxNDQ2Ng==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546414466", "bodyText": "Class<?> won't work anymore", "author": "ccciudatu", "createdAt": "2020-12-20T18:51:05Z", "path": "sdks/java/extensions/protobuf/src/main/java/org/apache/beam/sdk/extensions/protobuf/ProtoMessageSchema.java", "diffHunk": "@@ -129,68 +129,29 @@ public SchemaUserTypeCreator schemaTypeCreator(Class<?> targetClass, Schema sche\n     \"rawtypes\", // TODO(https://issues.apache.org/jira/browse/BEAM-10556)\n     \"unchecked\"\n   })\n-  public static <T> SimpleFunction<byte[], Row> getProtoBytesToRowFn(Class<T> clazz) {\n+  public static <T extends Message> SimpleFunction<byte[], Row> getProtoBytesToRowFn(\n+      Class<T> clazz) {\n     checkForMessageType(clazz);\n-    return new ProtoBytesToRowFn(clazz);\n-  }\n-\n-  private static class ProtoBytesToRowFn<T extends Message> extends SimpleFunction<byte[], Row> {\n-    private final ProtoCoder<T> protoCoder;\n-    private final SerializableFunction<T, Row> toRowFunction;\n-\n-    public ProtoBytesToRowFn(Class<T> clazz) {\n-      this.protoCoder = ProtoCoder.of(clazz);\n-      this.toRowFunction = new ProtoMessageSchema().toRowFunction(TypeDescriptor.of(clazz));\n-    }\n-\n-    @Override\n-    public Row apply(byte[] bytes) {\n-      try {\n-        T message = protoCoder.getParser().parseFrom(bytes);\n-        return toRowFunction.apply(message);\n-      } catch (IOException e) {\n-        throw new IllegalArgumentException(\"Could not decode row from proto payload.\", e);\n-      }\n-    }\n+    final ProtoCoder<T> protoCoder = ProtoCoder.of(clazz);\n+    final SerializableFunction<T, Row> toRowFunction =\n+        new ProtoMessageSchema().toRowFunction(TypeDescriptor.of(clazz));\n+    return new SqlRows.BytesToRowFn<>(protoCoder.getParser()::parseFrom, toRowFunction);\n   }\n \n   // Other modules are not allowed to use non-vendored Message class\n   @SuppressWarnings({\n     \"rawtypes\", // TODO(https://issues.apache.org/jira/browse/BEAM-10556)\n     \"unchecked\"\n   })\n-  public static <T> SimpleFunction<Row, byte[]> getRowToProtoBytesFn(Class<T> clazz) {\n+  public static <T extends Message> SimpleFunction<Row, byte[]> getRowToProtoBytesFn(", "originalCommit": "488370798f06270d5eb431f2dd48746bded6a9c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQzMjk4Ng==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546432986", "bodyText": "reverted", "author": "ccciudatu", "createdAt": "2020-12-20T20:36:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQxNDQ2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQxNDY2Mg==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546414662", "bodyText": "@chrlarsen Just trying to make sure I haven't missed anything: was there a reason why we're currently writing to a temporary in-memory stream and then copy that to the actual stream? i.e. do you see any potential pitfalls with the optimization that I just made?", "author": "ccciudatu", "createdAt": "2020-12-20T18:53:17Z", "path": "sdks/java/io/thrift/src/main/java/org/apache/beam/sdk/io/thrift/ThriftCoder.java", "diffHunk": "@@ -72,15 +71,13 @@ protected ThriftCoder(Class<T> type, TProtocolFactory protocolFactory) {\n    */\n   @Override\n   public void encode(T value, OutputStream outStream) throws CoderException, IOException {\n-    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n-    TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(baos));\n+    TProtocol protocol = protocolFactory.getProtocol(new TIOStreamTransport(outStream));", "originalCommit": "488370798f06270d5eb431f2dd48746bded6a9c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM3NDI5MQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r547374291", "bodyText": "LGTM. I think this was an artifact from earlier development.", "author": "chrlarsen", "createdAt": "2020-12-22T16:27:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQxNDY2Mg=="}], "type": "inlineReview"}, {"oid": "dd269c1c24263e1887a3048cdf55153a773b76b5", "url": "https://github.com/apache/beam/commit/dd269c1c24263e1887a3048cdf55153a773b76b5", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T18:55:21Z", "type": "forcePushed"}, {"oid": "c5b2d3a04c7010389ce196e359da85625bc32407", "url": "https://github.com/apache/beam/commit/c5b2d3a04c7010389ce196e359da85625bc32407", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T19:31:21Z", "type": "forcePushed"}, {"oid": "b1dbfbfc5860840b2047388eb04f4be83bee350b", "url": "https://github.com/apache/beam/commit/b1dbfbfc5860840b2047388eb04f4be83bee350b", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T19:46:08Z", "type": "forcePushed"}, {"oid": "1ed4f44c800de49eeaeaff00152bbb10f8cc6cd3", "url": "https://github.com/apache/beam/commit/1ed4f44c800de49eeaeaff00152bbb10f8cc6cd3", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T19:49:25Z", "type": "forcePushed"}, {"oid": "2bc7260f0f10a32628982a60b9a4292517aa45a3", "url": "https://github.com/apache/beam/commit/2bc7260f0f10a32628982a60b9a4292517aa45a3", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T20:07:15Z", "type": "forcePushed"}, {"oid": "77adcca90c03cfb684adaf52ca723de2ec4fcd1d", "url": "https://github.com/apache/beam/commit/77adcca90c03cfb684adaf52ca723de2ec4fcd1d", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T20:10:08Z", "type": "forcePushed"}, {"oid": "79c718f3901529d65883a94f7a4fc4c5effd306d", "url": "https://github.com/apache/beam/commit/79c718f3901529d65883a94f7a4fc4c5effd306d", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T20:30:24Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQzMzY4MQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546433681", "bodyText": "this can't be a method reference (protoCoder.getParser()::parseFrom) because the protobuf Parser is not Serializable.", "author": "ccciudatu", "createdAt": "2020-12-20T20:42:51Z", "path": "sdks/java/extensions/protobuf/src/main/java/org/apache/beam/sdk/extensions/protobuf/ProtoMessageSchema.java", "diffHunk": "@@ -130,28 +129,12 @@ public SchemaUserTypeCreator schemaTypeCreator(Class<?> targetClass, Schema sche\n     \"unchecked\"\n   })\n   public static <T> SimpleFunction<byte[], Row> getProtoBytesToRowFn(Class<T> clazz) {\n-    checkForMessageType(clazz);\n-    return new ProtoBytesToRowFn(clazz);\n-  }\n-\n-  private static class ProtoBytesToRowFn<T extends Message> extends SimpleFunction<byte[], Row> {\n-    private final ProtoCoder<T> protoCoder;\n-    private final SerializableFunction<T, Row> toRowFunction;\n-\n-    public ProtoBytesToRowFn(Class<T> clazz) {\n-      this.protoCoder = ProtoCoder.of(clazz);\n-      this.toRowFunction = new ProtoMessageSchema().toRowFunction(TypeDescriptor.of(clazz));\n-    }\n-\n-    @Override\n-    public Row apply(byte[] bytes) {\n-      try {\n-        T message = protoCoder.getParser().parseFrom(bytes);\n-        return toRowFunction.apply(message);\n-      } catch (IOException e) {\n-        throw new IllegalArgumentException(\"Could not decode row from proto payload.\", e);\n-      }\n-    }\n+    Class<Message> protoClass = ensureMessageType(clazz);\n+    ProtoCoder<Message> protoCoder = ProtoCoder.of(protoClass);\n+    return new SqlRows.BytesToRowFn<>(\n+        new ProtoMessageSchema(),\n+        TypeDescriptor.of(protoClass),\n+        bytes -> protoCoder.getParser().parseFrom(bytes));", "originalCommit": "79c718f3901529d65883a94f7a4fc4c5effd306d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9557ed6a29868c0a61e9181edfe35cc20e471d78", "url": "https://github.com/apache/beam/commit/9557ed6a29868c0a61e9181edfe35cc20e471d78", "message": "fixup! [BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-20T21:30:38Z", "type": "forcePushed"}, {"oid": "fe2089100461236459f7449e6345e0faed9681d3", "url": "https://github.com/apache/beam/commit/fe2089100461236459f7449e6345e0faed9681d3", "message": "support for field reordering in thrift schema", "committedDate": "2020-12-21T00:33:52Z", "type": "forcePushed"}, {"oid": "b34a224497b3dfb4e6927db1b978184e6b26f243", "url": "https://github.com/apache/beam/commit/b34a224497b3dfb4e6927db1b978184e6b26f243", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-21T00:48:23Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjQ2NDgyOA==", "url": "https://github.com/apache/beam/pull/13572#discussion_r546464828", "bodyText": "This wasn't possible before because of the \"nullness\" checks, but now it seems to work so I cleaned it up.", "author": "ccciudatu", "createdAt": "2020-12-21T01:09:07Z", "path": "sdks/java/io/thrift/src/main/java/org/apache/beam/sdk/io/thrift/ThriftSchema.java", "diffHunk": "@@ -243,37 +253,65 @@ private FieldValueTypeInformation fieldValueTypeInfo(Class<?> type, String field\n   @Override\n   public @NonNull SchemaUserTypeCreator schemaTypeCreator(\n       @NonNull Class<?> targetClass, @NonNull Schema schema) {\n-    return params -> restoreThriftObject(targetClass, params);\n+    return thriftMapper(targetClass, schema);\n+  }\n+\n+  private <FieldT extends Enum<FieldT> & TFieldIdEnum, T extends TBase<T, FieldT>>\n+      SchemaUserTypeCreator thriftMapper(Class<?> targetClass, Schema schema) {\n+    final Map<FieldT, FieldMetaData> fieldDescriptors = schemaFieldDescriptors(targetClass, schema);\n+    return params -> restoreThriftObject(targetClass, fieldDescriptors, params);\n   }\n \n+  @SuppressWarnings(\"nullness\")\n   private <FieldT extends Enum<FieldT> & TFieldIdEnum, T extends TBase<T, FieldT>>\n-      T restoreThriftObject(Class<?> targetClass, Object[] params) {\n+      Map<FieldT, FieldMetaData> schemaFieldDescriptors(Class<?> targetClass, Schema schema) {\n+    final Map<FieldT, FieldMetaData> fieldDescriptors = thriftFieldDescriptors(targetClass);\n+    final Map<String, FieldT> fields =\n+        fieldDescriptors.keySet().stream()\n+            .collect(Collectors.toMap(FieldT::getFieldName, Function.identity()));\n+\n+    return schema.getFields().stream()\n+        .map(Schema.Field::getName)\n+        .map(fields::get)\n+        .collect(\n+            Collectors.toMap(\n+                Function.identity(),\n+                fieldDescriptors::get,\n+                ThriftSchema::throwingCombiner,\n+                LinkedHashMap::new));\n+  }\n+\n+  private <FieldT extends Enum<FieldT> & TFieldIdEnum, T extends TBase<T, FieldT>>\n+      T restoreThriftObject(\n+          Class<?> targetClass, Map<FieldT, FieldMetaData> fields, Object[] params) {\n+    if (params.length != fields.size()) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"The parameter list: %s does not match the expected fields: %s\",\n+              Arrays.toString(params), fields.keySet()));\n+    }\n     try {\n       @SuppressWarnings(\"unchecked\")\n       final T thrift = (T) targetClass.getDeclaredConstructor().newInstance();\n-      final Map<FieldT, FieldMetaData> fieldMap = thriftFieldDescriptors(targetClass);\n-      // the underlying Map is an EnumMap, so it's safe to rely on the order of its keys\n-      fieldMap.forEach(\n-          (field, descriptor) ->\n-              setThriftField(thrift, field, descriptor, params[field.ordinal()]));\n+      final Iterator<Entry<FieldT, FieldMetaData>> iter = fields.entrySet().iterator();\n+      Stream.of(params).forEach(param -> setThriftField(thrift, iter.next(), param));\n       return thrift;\n     } catch (Exception e) {\n       throw new IllegalStateException(e);\n     }\n   }\n \n   private <FieldT extends TFieldIdEnum, T extends TBase<T, FieldT>> void setThriftField(\n-      T thrift, FieldT field, FieldMetaData descriptor, Object value) {\n+      T thrift, Entry<FieldT, FieldMetaData> fieldDescriptor, Object value) {\n+    final FieldT field = fieldDescriptor.getKey();\n+    final FieldMetaData descriptor = fieldDescriptor.getValue();\n     if (value != null) {\n       final Object actualValue;\n       switch (descriptor.valueMetaData.type) {\n         case TType.SET:\n-          final Set<Object> set = new HashSet<>();\n-          final Iterable<@NonNull ?> iterable = (Iterable<@NonNull ?>) value;\n-          for (@NonNull Object elem : iterable) {\n-            set.add(elem);\n-          }\n-          actualValue = set;\n+          actualValue =\n+              StreamSupport.stream(((Iterable<?>) value).spliterator(), false)\n+                  .collect(Collectors.toSet());", "originalCommit": "b34a224497b3dfb4e6927db1b978184e6b26f243", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3NTA2Nw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r547575067", "bodyText": "Shoot sorry I just merged a refactor from @piotr-szuberski that conflicts with this: #13542\nCan you update this PR to use the parameterized test approach for KafkaTableProividerIT:\n\n  \n    \n      beam/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderIT.java\n    \n    \n        Lines 105 to 113\n      in\n      6171a36\n    \n    \n    \n    \n\n        \n          \n           @Parameters \n        \n\n        \n          \n           public static Collection<Object[]> data() { \n        \n\n        \n          \n             return Arrays.asList( \n        \n\n        \n          \n                 new Object[][] { \n        \n\n        \n          \n                   {new KafkaJsonObjectProvider(), \"json_topic\"}, \n        \n\n        \n          \n                   {new KafkaAvroObjectProvider(), \"avro_topic\"}, \n        \n\n        \n          \n                   {new KafkaProtoObjectProvider(), \"proto_topic\"}, \n        \n\n        \n          \n                   {new KafkaCsvObjectProvider(), \"csv_topic\"} \n        \n\n        \n          \n                 });", "author": "TheNeuralBit", "createdAt": "2020-12-23T00:42:56Z", "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/meta/provider/kafka/KafkaTableProviderThriftIT.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider.kafka;\n+\n+import org.apache.beam.sdk.extensions.sql.meta.provider.kafka.thrift.ItThriftMessage;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.thrift.TException;\n+import org.apache.thrift.TSerializer;\n+import org.apache.thrift.protocol.TBinaryProtocol;\n+import org.apache.thrift.protocol.TProtocolFactory;\n+\n+public class KafkaTableProviderThriftIT extends KafkaTableProviderIT {", "originalCommit": "b34a224497b3dfb4e6927db1b978184e6b26f243", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzkwMTgyOA==", "url": "https://github.com/apache/beam/pull/13572#discussion_r547901828", "bodyText": "done", "author": "ccciudatu", "createdAt": "2020-12-23T11:01:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3NTA2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3OTI5OQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r547579299", "bodyText": "This is getting close to the abstraction that I was discussing in your other PR. A couple suggestions for this class:\n\nLet's make it @Internal, we don't want Beam users to use this and expect backwards compatibility. It's only public so other Beam modules can use it.\nI think it belongs in the schemas package\nDrop SQL from the name. Right now this is only used in SQL, but we will want to use this infra outside of SQL at some point (which is why I don't think it belongs in the sql package). Maybe something like RowMessages?", "author": "TheNeuralBit", "createdAt": "2020-12-23T00:59:26Z", "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/util/SqlRows.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.util;\n+\n+import static org.apache.beam.sdk.util.Preconditions.checkArgumentNotNull;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaProvider;\n+import org.apache.beam.sdk.transforms.ProcessFunction;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.SimpleFunction;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+\n+public final class SqlRows {", "originalCommit": "b34a224497b3dfb4e6927db1b978184e6b26f243", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzkwMTkxNw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r547901917", "bodyText": "done", "author": "ccciudatu", "createdAt": "2020-12-23T11:01:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3OTI5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk4OTkzNg==", "url": "https://github.com/apache/beam/pull/13572#discussion_r547989936", "bodyText": "It's just a tiny step towards whatever abstractions we want to end up with, as it doesn't even cover Avro yet.\nI'm still trying to wrap my head around how would a proper data format abstraction look like in Beam.\nI mean, instead of FileIO, AvroIO, KafkaIO, ThriftIO plus Kafka tables for each data format, what if we took the format out of the IO as an entirely different concern and provide the ability to mix those freely. If IOs are just [File, Kafka, PubSub, Socket, whatever], and data formats are [Avro, Protobuf, CSV, Thrift, whatever], we can (in theory) allow any combination like PubSub+Thrift or File+Proto with no extra cost (except for incompatible candidates, like Kafka+parquet, perhaps).\nDo you think this is worth considering (for 3.0, of course)? I am aware that such an abstraction is really difficult to get right (if at all feasible), but I still think it may be worth trying, as we should at least end up reducing the number of data_source+data_format combinations that are now modelled as new datasource types.", "author": "ccciudatu", "createdAt": "2020-12-23T14:39:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3OTI5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA5NTI1NA==", "url": "https://github.com/apache/beam/pull/13572#discussion_r548095254", "bodyText": "Yes this sounds ideal. As far as 3.0 - I think it would make sense to start working on it in 2.x versions as an @Experimental API. We shouldn't remove the existing IOs prior to a major version change, but it would be fine to start work on an alternative. This would be similar to (and related to) the SchemaIOProvider concept we had a Google intern work on last summer:\n\nhttps://s.apache.org/schemaio-development-guide\nhttp://s.apache.org/beam-schema-io\n\nWould you have any interest in working on an effort like this?\nCC: @robinyqiu a design like this would be relevant for your work on IOs", "author": "TheNeuralBit", "createdAt": "2020-12-23T17:59:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3OTI5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODEyNDg0Nw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r548124847", "bodyText": "I'd be more than happy to get involved. Most likely, I'll need to join your slack channel, and for that I need an apache.org email, apparently. I'll see how I can sort that out...", "author": "ccciudatu", "createdAt": "2020-12-23T18:36:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3OTI5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODEyNzg3MQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r548127871", "bodyText": "It looks like you can get an invite at https://s.apache.org/slack-invite (more info here: https://infra.apache.org/slack.html)", "author": "TheNeuralBit", "createdAt": "2020-12-23T18:40:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3OTI5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODE0MDMxNw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r548140317", "bodyText": "Sounds easy. Not sure who's supposed to invite me, but I'll ask around. If you can help with that, I'd be grateful.", "author": "ccciudatu", "createdAt": "2020-12-23T18:56:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3OTI5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODE1Mjg1NQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r548152855", "bodyText": "Sure I can invite you as a guest. I think https://s.apache.org/slack-invite is supposed to be self-service but the page indicates it breaks it often. Can you send me a message at bhulette@apache.org so I know what email to use?", "author": "TheNeuralBit", "createdAt": "2020-12-23T19:11:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3OTI5OQ=="}], "type": "inlineReview"}, {"oid": "f7251165c1b42d80f44eaffd5e0fa0a8206437c0", "url": "https://github.com/apache/beam/commit/f7251165c1b42d80f44eaffd5e0fa0a8206437c0", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-23T11:00:41Z", "type": "forcePushed"}, {"oid": "ab96c02254cd2b7d17063b4605edaf744d8b15e8", "url": "https://github.com/apache/beam/commit/ab96c02254cd2b7d17063b4605edaf744d8b15e8", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-23T11:49:36Z", "type": "forcePushed"}, {"oid": "98ed67395e8c562425e7809ee4517d4e56796967", "url": "https://github.com/apache/beam/commit/98ed67395e8c562425e7809ee4517d4e56796967", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-23T12:07:55Z", "type": "forcePushed"}, {"oid": "3e452e02b5f20793244ffe44751b068b0fdbb7cc", "url": "https://github.com/apache/beam/commit/3e452e02b5f20793244ffe44751b068b0fdbb7cc", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-23T12:54:19Z", "type": "forcePushed"}, {"oid": "a73e3c2e15a5e253c1f3d9713bc9eb6fba312339", "url": "https://github.com/apache/beam/commit/a73e3c2e15a5e253c1f3d9713bc9eb6fba312339", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-23T18:23:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQzMDI5Mg==", "url": "https://github.com/apache/beam/pull/13572#discussion_r548430292", "bodyText": "This is still the case with thrift metadata, but it's no longer a \"strong assumption\" for this class, as now the field order is infered from the schema, allowing for field reordering.", "author": "ccciudatu", "createdAt": "2020-12-24T07:33:57Z", "path": "sdks/java/io/thrift/src/main/java/org/apache/beam/sdk/io/thrift/ThriftSchema.java", "diffHunk": "@@ -90,17 +95,17 @@\n  *       parameter exists.\n  *   <li>All non-union types have a corresponding java field with the same name for every field in\n  *       the original thrift source file.\n- *   <li>The underlying {@link FieldMetaData#getStructMetaDataMap(Class) metadata maps} are {@link\n- *       java.util.EnumMap enum maps}, so the natural order of the field keys is preserved.", "originalCommit": "a73e3c2e15a5e253c1f3d9713bc9eb6fba312339", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQzMDc3MQ==", "url": "https://github.com/apache/beam/pull/13572#discussion_r548430771", "bodyText": "I changed the set-ALL-fields policy to set non-null only, as I think this is safer (no NPE if primitive thrift fields are null in the beam row), easier to reason about and more natural for thrift clients (who are used to check if fields are set before using them).", "author": "ccciudatu", "createdAt": "2020-12-24T07:35:48Z", "path": "sdks/java/io/thrift/src/main/java/org/apache/beam/sdk/io/thrift/ThriftSchema.java", "diffHunk": "@@ -90,17 +95,17 @@\n  *       parameter exists.\n  *   <li>All non-union types have a corresponding java field with the same name for every field in\n  *       the original thrift source file.\n- *   <li>The underlying {@link FieldMetaData#getStructMetaDataMap(Class) metadata maps} are {@link\n- *       java.util.EnumMap enum maps}, so the natural order of the field keys is preserved.\n  * </ul>\n  *\n  * <p>Thrift typedefs for container types (and possibly others) do not preserve the full type\n  * information. For this reason, this class allows for {@link #custom() manual registration} of such\n  * \"lossy\" typedefs with their corresponding beam types.\n  *\n- * <p>Note: upon restoring the same thrift object from a Beam {@link\n- * org.apache.beam.sdk.values.Row}, the {@link TBase#isSet(TFieldIdEnum) isSet flag} will be {@code\n- * true} for all fields, except for non-primitive types with no default values.\n+ * <p>Note: Thrift encoding and decoding are not fully symmetrical, i.e. the {@link\n+ * TBase#isSet(TFieldIdEnum) isSet} flag may not be preserved upon converting a thrift object to a\n+ * beam row and back. On encoding, we extract all thrift values, no matter if the fields are set or\n+ * not. On decoding, we set all non-{@code null} beam row values to the corresponding thrift fields,\n+ * leaving the rest unset.", "originalCommit": "a73e3c2e15a5e253c1f3d9713bc9eb6fba312339", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQzMjU3Nw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r548432577", "bodyText": "This seemed like a good idea at the time, as it prevented null values in the beam row from ending up as fields with default values in thrift (in case defaults were provided in the thrift descriptor), so it seemed closer to a \"symmetrical\" transformation.\nHowever, leaving the fields unset when they're null in the source row is both safer and more natural.", "author": "ccciudatu", "createdAt": "2020-12-24T07:42:31Z", "path": "sdks/java/io/thrift/src/main/java/org/apache/beam/sdk/io/thrift/ThriftSchema.java", "diffHunk": "@@ -286,8 +324,6 @@ T restoreThriftObject(Class<?> targetClass, Object[] params) {\n           actualValue = value;\n       }\n       thrift.setFieldValue(field, actualValue);\n-    } else if (!TUnion.class.isInstance(thrift)) {\n-      thrift.setFieldValue(field, value); // nullness checks don't allow setting null here", "originalCommit": "a73e3c2e15a5e253c1f3d9713bc9eb6fba312339", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQzMzM5Ng==", "url": "https://github.com/apache/beam/pull/13572#discussion_r548433396", "bodyText": "This allows using the order of fields in the incoming schema, instead of the original order in the thrift descriptor, so this will still work if fields are reordered by clients (the shuffled schema table test validates this).", "author": "ccciudatu", "createdAt": "2020-12-24T07:45:31Z", "path": "sdks/java/io/thrift/src/main/java/org/apache/beam/sdk/io/thrift/ThriftSchema.java", "diffHunk": "@@ -243,37 +253,65 @@ private FieldValueTypeInformation fieldValueTypeInfo(Class<?> type, String field\n   @Override\n   public @NonNull SchemaUserTypeCreator schemaTypeCreator(\n       @NonNull Class<?> targetClass, @NonNull Schema schema) {\n-    return params -> restoreThriftObject(targetClass, params);\n+    return thriftMapper(targetClass, schema);\n+  }\n+\n+  private <FieldT extends Enum<FieldT> & TFieldIdEnum, T extends TBase<T, FieldT>>\n+      SchemaUserTypeCreator thriftMapper(Class<?> targetClass, Schema schema) {\n+    final Map<FieldT, FieldMetaData> fieldDescriptors = schemaFieldDescriptors(targetClass, schema);\n+    return params -> restoreThriftObject(targetClass, fieldDescriptors, params);\n   }\n \n+  @SuppressWarnings(\"nullness\")\n   private <FieldT extends Enum<FieldT> & TFieldIdEnum, T extends TBase<T, FieldT>>\n-      T restoreThriftObject(Class<?> targetClass, Object[] params) {\n+      Map<FieldT, FieldMetaData> schemaFieldDescriptors(Class<?> targetClass, Schema schema) {", "originalCommit": "a73e3c2e15a5e253c1f3d9713bc9eb6fba312339", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQzNDI5Mw==", "url": "https://github.com/apache/beam/pull/13572#discussion_r548434293", "bodyText": "Fields used to always be nullable for thrift schemas and the table integration test requires non-nullable fields. So I just added what was actually missing, i.e. marking required as non-nullable in beam. default is still nullable, the same as optional, for safety reasons -- as it's defined as \"opt-in, req-out\".", "author": "ccciudatu", "createdAt": "2020-12-24T07:48:29Z", "path": "sdks/java/io/thrift/src/main/java/org/apache/beam/sdk/io/thrift/ThriftSchema.java", "diffHunk": "@@ -183,7 +188,12 @@ private Schema schemaFor(Class<?> targetClass) {\n   private Schema.Field beamField(FieldMetaData fieldDescriptor) {\n     try {\n       final FieldType type = beamType(fieldDescriptor.valueMetaData);\n-      return Schema.Field.nullable(fieldDescriptor.fieldName, type);\n+      switch (fieldDescriptor.requirementType) {\n+        case TFieldRequirementType.REQUIRED:\n+          return Schema.Field.of(fieldDescriptor.fieldName, type);\n+        default:\n+          return Schema.Field.nullable(fieldDescriptor.fieldName, type);\n+      }", "originalCommit": "a73e3c2e15a5e253c1f3d9713bc9eb6fba312339", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQzODM4Mg==", "url": "https://github.com/apache/beam/pull/13572#discussion_r548438382", "bodyText": "this method only exists for \"pinning\" the generics between the two invocations", "author": "ccciudatu", "createdAt": "2020-12-24T08:03:18Z", "path": "sdks/java/io/thrift/src/main/java/org/apache/beam/sdk/io/thrift/ThriftSchema.java", "diffHunk": "@@ -243,37 +253,65 @@ private FieldValueTypeInformation fieldValueTypeInfo(Class<?> type, String field\n   @Override\n   public @NonNull SchemaUserTypeCreator schemaTypeCreator(\n       @NonNull Class<?> targetClass, @NonNull Schema schema) {\n-    return params -> restoreThriftObject(targetClass, params);\n+    return thriftMapper(targetClass, schema);\n+  }\n+\n+  private <FieldT extends Enum<FieldT> & TFieldIdEnum, T extends TBase<T, FieldT>>\n+      SchemaUserTypeCreator thriftMapper(Class<?> targetClass, Schema schema) {", "originalCommit": "a73e3c2e15a5e253c1f3d9713bc9eb6fba312339", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "339ad46ceeafc3bf6b24c1bdae99bac99c9c7e93", "url": "https://github.com/apache/beam/commit/339ad46ceeafc3bf6b24c1bdae99bac99c9c7e93", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-25T11:19:21Z", "type": "forcePushed"}, {"oid": "549ba04c4b1d208ac19e45bb3e0bde511e490c94", "url": "https://github.com/apache/beam/commit/549ba04c4b1d208ac19e45bb3e0bde511e490c94", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-25T14:44:53Z", "type": "forcePushed"}, {"oid": "962da298b8f82b526728c62f662dc6d32ae13173", "url": "https://github.com/apache/beam/commit/962da298b8f82b526728c62f662dc6d32ae13173", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-26T11:29:10Z", "type": "forcePushed"}, {"oid": "1ef342d1dffd208e46e5a14e8af0c1ca8d36be83", "url": "https://github.com/apache/beam/commit/1ef342d1dffd208e46e5a14e8af0c1ca8d36be83", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-26T11:57:40Z", "type": "forcePushed"}, {"oid": "705174512fb7aff23d42ea3f7fa016553022b278", "url": "https://github.com/apache/beam/commit/705174512fb7aff23d42ea3f7fa016553022b278", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-26T13:11:21Z", "type": "forcePushed"}, {"oid": "be47279ec27e44378ae4109f74ba86a6c3474c72", "url": "https://github.com/apache/beam/commit/be47279ec27e44378ae4109f74ba86a6c3474c72", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-26T13:18:35Z", "type": "commit"}, {"oid": "be47279ec27e44378ae4109f74ba86a6c3474c72", "url": "https://github.com/apache/beam/commit/be47279ec27e44378ae4109f74ba86a6c3474c72", "message": "[BEAM-11482] Thrift support for KafkaTableProvider", "committedDate": "2020-12-26T13:18:35Z", "type": "forcePushed"}]}