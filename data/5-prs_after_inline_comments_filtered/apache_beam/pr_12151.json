{"pr_number": 12151, "pr_title": "[BEAM-9896] Add streaming for SnowflakeIO.Write to Java SDK", "pr_createdAt": "2020-07-01T15:31:38Z", "pr_url": "https://github.com/apache/beam/pull/12151", "timeline": [{"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "url": "https://github.com/apache/beam/commit/8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "message": "[BEAM-9896] Spotless Apply", "committedDate": "2020-07-06T14:38:02Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4NjQ4Mg==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451286482", "bodyText": "Please add detailed Javadoc for these functions, as they will be the point of entry for users - they also will help me understand what's a snow pipe : )", "author": "pabloem", "createdAt": "2020-07-08T05:10:57Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -644,6 +747,26 @@ public void populateDisplayData(DisplayData.Builder builder) {\n       return toBuilder().setUserDataMapper(userDataMapper).build();\n     }\n \n+    public Write<T> withFlushTimeLimit(Duration triggeringFrequency) {\n+      return toBuilder().setFlushTimeLimit(triggeringFrequency).build();\n+    }\n+\n+    public Write<T> withSnowPipe(String snowPipe) {\n+      return toBuilder().setSnowPipe(ValueProvider.StaticValueProvider.of(snowPipe)).build();\n+    }\n+\n+    public Write<T> withSnowPipe(ValueProvider<String> snowPipe) {\n+      return toBuilder().setSnowPipe(snowPipe).build();\n+    }\n+\n+    public Write<T> withShardsNumber(Integer shardsNumber) {\n+      return toBuilder().setShardsNumber(shardsNumber).build();\n+    }\n+\n+    public Write<T> withFlushRowLimit(Integer rowsCount) {\n+      return toBuilder().setFlushRowLimit(rowsCount).build();\n+    }", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NjAyNw==", "url": "https://github.com/apache/beam/pull/12151#discussion_r455086027", "bodyText": "Sure, thanks for noticing. I hope it will help :)", "author": "kkucharc", "createdAt": "2020-07-15T14:16:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4NjQ4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4NzExMw==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451287113", "bodyText": "Please add javadoc", "author": "pabloem", "createdAt": "2020-07-08T05:13:13Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -654,28 +777,40 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     }\n \n     /**\n-     * A snowflake service which is supposed to be used. Note: Currently we have {@link\n-     * SnowflakeServiceImpl} with corresponding {@link FakeSnowflakeServiceImpl} used for testing.\n+     * A snowflake service {@link SnowflakeService} implementation which is supposed to be used.\n      *\n      * @param snowflakeService - an instance of {@link SnowflakeService}.\n      */\n     public Write<T> withSnowflakeService(SnowflakeService snowflakeService) {\n       return toBuilder().setSnowflakeService(snowflakeService).build();\n     }\n \n+    public Write<T> withQuotationMark(String quotationMark) {\n+      return toBuilder().setQuotationMark(quotationMark).build();\n+    }\n+\n+    public Write<T> withDebugMode(StreamingLogLevel debugLevel) {\n+      return toBuilder().setDebugMode(debugLevel).build();\n+    }", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NjAwNA==", "url": "https://github.com/apache/beam/pull/12151#discussion_r455086004", "bodyText": "Sure, thanks for noticing", "author": "kkucharc", "createdAt": "2020-07-15T14:16:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4NzExMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4NzIwOQ==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451287209", "bodyText": "Javadoc please", "author": "pabloem", "createdAt": "2020-07-08T05:13:36Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -352,15 +385,16 @@\n       return toBuilder().setCoder(coder).build();\n     }\n \n+    public Read<T> withQuotationMark(String quotationMark) {", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NTk2OQ==", "url": "https://github.com/apache/beam/pull/12151#discussion_r455085969", "bodyText": "Sure, I will add to all missing methods.", "author": "kkucharc", "createdAt": "2020-07-15T14:16:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4NzIwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4Nzk2NA==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451287964", "bodyText": "Please improve the error message specifying that withSnowPipe is required for streaming / unbounded PCollections", "author": "pabloem", "createdAt": "2020-07-08T05:15:57Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -684,14 +819,61 @@ private void checkArguments() {\n           (getDataSourceProviderFn() != null),\n           \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n \n-      checkArgument(getTable() != null, \"withTable() is required\");\n+      if (input.isBounded() == PCollection.IsBounded.UNBOUNDED) {\n+        checkArgument(getSnowPipe() != null, \"withSnowPipe() is required\");", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NTkzMg==", "url": "https://github.com/apache/beam/pull/12151#discussion_r455085932", "bodyText": "Sure, I will change it.", "author": "kkucharc", "createdAt": "2020-07-15T14:16:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4Nzk2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4ODAyOQ==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451288029", "bodyText": "Please make the error message more informative", "author": "pabloem", "createdAt": "2020-07-08T05:16:14Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -684,14 +819,61 @@ private void checkArguments() {\n           (getDataSourceProviderFn() != null),\n           \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n \n-      checkArgument(getTable() != null, \"withTable() is required\");\n+      if (input.isBounded() == PCollection.IsBounded.UNBOUNDED) {\n+        checkArgument(getSnowPipe() != null, \"withSnowPipe() is required\");\n+      } else {\n+        checkArgument(getTable() != null, \"to() is required\");", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NTkwNA==", "url": "https://github.com/apache/beam/pull/12151#discussion_r455085904", "bodyText": "Sure, I will change it.", "author": "kkucharc", "createdAt": "2020-07-15T14:16:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4ODAyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NDQ2NA==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451294464", "bodyText": "A PCollection after a combine / GBK / etc - keeps the trigger configuration from upstream. Did you find that you needed to restate the trigger like this?", "author": "pabloem", "createdAt": "2020-07-08T05:38:41Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -684,14 +819,61 @@ private void checkArguments() {\n           (getDataSourceProviderFn() != null),\n           \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n \n-      checkArgument(getTable() != null, \"withTable() is required\");\n+      if (input.isBounded() == PCollection.IsBounded.UNBOUNDED) {\n+        checkArgument(getSnowPipe() != null, \"withSnowPipe() is required\");\n+      } else {\n+        checkArgument(getTable() != null, \"to() is required\");\n+      }\n     }\n \n-    private PCollection<String> write(PCollection<T> input, String stagingBucketDir) {\n+    private PCollection<T> writeStream(PCollection<T> input, String stagingBucketDir) {\n       SnowflakeService snowflakeService =\n-          getSnowflakeService() != null ? getSnowflakeService() : new SnowflakeServiceImpl();\n+          getSnowflakeService() != null\n+              ? getSnowflakeService()\n+              : new SnowflakeStreamingServiceImpl();\n+\n+      /* Ensure that files will be created after specific record count or duration specified */\n+      PCollection<T> inputInGlobalWindow =\n+          input.apply(\n+              \"rewindowIntoGlobal\",\n+              Window.<T>into(new GlobalWindows())\n+                  .triggering(\n+                      Repeatedly.forever(\n+                          AfterFirst.of(\n+                              AfterProcessingTime.pastFirstElementInPane()\n+                                  .plusDelayOf(getFlushTimeLimit()),\n+                              AfterPane.elementCountAtLeast(getFlushRowLimit()))))\n+                  .discardingFiredPanes());\n+\n+      int shards = (getShardsNumber() > 0) ? getShardsNumber() : DEFAULT_STREAMING_SHARDS_NUMBER;\n+      PCollection files = writeFiles(inputInGlobalWindow, stagingBucketDir, shards);\n+\n+      /* Ensuring that files will be ingested after flush time */\n+      files =\n+          (PCollection)\n+              files.apply(\n+                  \"applyUserTrigger\",\n+                  Window.<T>into(new GlobalWindows())\n+                      .triggering(\n+                          Repeatedly.forever(\n+                              AfterProcessingTime.pastFirstElementInPane()\n+                                  .plusDelayOf(getFlushTimeLimit())))\n+                      .discardingFiredPanes());", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NTg3NQ==", "url": "https://github.com/apache/beam/pull/12151#discussion_r455085875", "bodyText": "I am not sure if I know what should be correct approach and what are the consequences of current one. Can you elaborate on that? @pabloem", "author": "kkucharc", "createdAt": "2020-07-15T14:16:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NDQ2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzIxOTcyMw==", "url": "https://github.com/apache/beam/pull/12151#discussion_r463219723", "bodyText": "I think we can leave this as it is. I see that the trigger is not the exact same.", "author": "pabloem", "createdAt": "2020-07-30T19:24:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NDQ2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NDgwMg==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451294802", "bodyText": "We are concatenating this in memory. Is this intended? I guess you would end up with a number of files equal to the number shards, which shouldn't be too large?", "author": "pabloem", "createdAt": "2020-07-08T05:39:52Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -684,14 +819,61 @@ private void checkArguments() {\n           (getDataSourceProviderFn() != null),\n           \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n \n-      checkArgument(getTable() != null, \"withTable() is required\");\n+      if (input.isBounded() == PCollection.IsBounded.UNBOUNDED) {\n+        checkArgument(getSnowPipe() != null, \"withSnowPipe() is required\");\n+      } else {\n+        checkArgument(getTable() != null, \"to() is required\");\n+      }\n     }\n \n-    private PCollection<String> write(PCollection<T> input, String stagingBucketDir) {\n+    private PCollection<T> writeStream(PCollection<T> input, String stagingBucketDir) {\n       SnowflakeService snowflakeService =\n-          getSnowflakeService() != null ? getSnowflakeService() : new SnowflakeServiceImpl();\n+          getSnowflakeService() != null\n+              ? getSnowflakeService()\n+              : new SnowflakeStreamingServiceImpl();\n+\n+      /* Ensure that files will be created after specific record count or duration specified */\n+      PCollection<T> inputInGlobalWindow =\n+          input.apply(\n+              \"rewindowIntoGlobal\",\n+              Window.<T>into(new GlobalWindows())\n+                  .triggering(\n+                      Repeatedly.forever(\n+                          AfterFirst.of(\n+                              AfterProcessingTime.pastFirstElementInPane()\n+                                  .plusDelayOf(getFlushTimeLimit()),\n+                              AfterPane.elementCountAtLeast(getFlushRowLimit()))))\n+                  .discardingFiredPanes());\n+\n+      int shards = (getShardsNumber() > 0) ? getShardsNumber() : DEFAULT_STREAMING_SHARDS_NUMBER;\n+      PCollection files = writeFiles(inputInGlobalWindow, stagingBucketDir, shards);\n+\n+      /* Ensuring that files will be ingested after flush time */\n+      files =\n+          (PCollection)\n+              files.apply(\n+                  \"applyUserTrigger\",\n+                  Window.<T>into(new GlobalWindows())\n+                      .triggering(\n+                          Repeatedly.forever(\n+                              AfterProcessingTime.pastFirstElementInPane()\n+                                  .plusDelayOf(getFlushTimeLimit())))\n+                      .discardingFiredPanes());\n+      files =\n+          (PCollection)\n+              files.apply(\n+                  \"Create list of logs to copy\",\n+                  Combine.globally(new Concatenate()).withoutDefaults());", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQ2MTM4NQ==", "url": "https://github.com/apache/beam/pull/12151#discussion_r454461385", "bodyText": "I haven't met any limitations yet but I agree - logically too large list of files might be a problem. But I am not sure how differently solve this. The only thing that comes to my mind if Reify but I am not sure if it's correct path. WDYT @pabloem ?", "author": "kkucharc", "createdAt": "2020-07-14T15:53:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NDgwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDUyNDM2NQ==", "url": "https://github.com/apache/beam/pull/12151#discussion_r454524365", "bodyText": "You can just KeyBy(same key) + GroupByKey, and read one by one - but it may be fine to leave as is if we don't expect > 100 files or so", "author": "pabloem", "createdAt": "2020-07-14T17:31:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NDgwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NjE0MQ==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451296141", "bodyText": "TODO(pabloem) Review this section", "author": "pabloem", "createdAt": "2020-07-08T05:44:19Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -828,31 +1037,152 @@ private String quoteField(String field, String quotation) {\n         String stagingBucketDir,\n         String storageIntegrationName,\n         WriteDisposition writeDisposition,\n-        SnowflakeService snowflakeService) {\n+        SnowflakeService snowflakeService,\n+        String quotationMark) {\n       this.dataSourceProviderFn = dataSourceProviderFn;\n-      this.table = table;\n       this.query = query;\n+      this.table = table;\n       this.stagingBucketDir = stagingBucketDir;\n       this.storageIntegrationName = storageIntegrationName;\n       this.writeDisposition = writeDisposition;\n       this.snowflakeService = snowflakeService;\n+      this.quotationMark = quotationMark;\n+\n+      DataSourceProviderFromDataSourceConfiguration dataSourceProviderFromDataSourceConfiguration =\n+          (DataSourceProviderFromDataSourceConfiguration) this.dataSourceProviderFn;\n+      DataSourceConfiguration config = dataSourceProviderFromDataSourceConfiguration.getConfig();\n+\n+      this.database = config.getDatabase();\n+      this.schema = config.getSchema();\n     }\n \n     @ProcessElement\n     public void processElement(ProcessContext context) throws Exception {\n-      SnowflakeServiceConfig config =\n-          new SnowflakeServiceConfig(\n+      SnowflakeBatchServiceConfig config =\n+          new SnowflakeBatchServiceConfig(\n               dataSourceProviderFn,\n               (List<String>) context.element(),\n+              database,\n+              schema,\n               table,\n               query,\n               writeDisposition,\n               storageIntegrationName,\n-              stagingBucketDir);\n+              stagingBucketDir,\n+              quotationMark);\n       snowflakeService.write(config);\n     }\n   }\n \n+  /** Custom DoFn that streams data to Snowflake table. */\n+  private static class StreamToTableFn<ParameterT, OutputT> extends DoFn<ParameterT, OutputT> {\n+    private final SerializableFunction<Void, DataSource> dataSourceProviderFn;\n+    private final String stagingBucketDir;\n+    private final ValueProvider<String> snowPipe;\n+    private final StreamingLogLevel debugMode;\n+    private final SnowflakeService snowflakeService;\n+    private transient SimpleIngestManager ingestManager;\n+\n+    private transient DataSource dataSource;\n+    ArrayList<String> trackedFilesNames;\n+\n+    StreamToTableFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn,\n+        ValueProvider<String> snowPipe,\n+        String stagingBucketDir,\n+        StreamingLogLevel debugMode,\n+        SnowflakeService snowflakeService) {\n+      this.dataSourceProviderFn = dataSourceProviderFn;\n+      this.stagingBucketDir = stagingBucketDir;\n+      this.snowPipe = snowPipe;\n+      this.debugMode = debugMode;\n+      this.snowflakeService = snowflakeService;\n+      trackedFilesNames = new ArrayList<>();\n+    }\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      dataSource = dataSourceProviderFn.apply(null);\n+\n+      DataSourceProviderFromDataSourceConfiguration dataSourceProviderFromDataSourceConfiguration =\n+          (DataSourceProviderFromDataSourceConfiguration) this.dataSourceProviderFn;\n+      DataSourceConfiguration config = dataSourceProviderFromDataSourceConfiguration.getConfig();\n+\n+      checkArgument(config.getPrivateKey() != null, \"KeyPair is required for authentication\");\n+\n+      String hostName = config.getServerName();\n+      List<String> path = Splitter.on('.').splitToList(hostName);\n+      String account = path.get(0);\n+      String username = config.getUsername();\n+      PrivateKey privateKey = config.getPrivateKey();\n+      String schema = config.getSchema();\n+      String database = config.getDatabase();\n+      String snowPipeName = String.format(\"%s.%s.%s\", database, schema, snowPipe.get());\n+\n+      this.ingestManager =\n+          new SimpleIngestManager(\n+              account, username, snowPipeName, privateKey, \"https\", hostName, 443);\n+    }\n+\n+    @ProcessElement\n+    public void processElement(ProcessContext context) throws Exception {\n+      List<String> filesList = (List<String>) context.element();\n+\n+      if (debugMode != null) {\n+        trackedFilesNames.addAll(filesList);\n+      }\n+      SnowflakeStreamingServiceConfig config =\n+          new SnowflakeStreamingServiceConfig(filesList, this.stagingBucketDir, this.ingestManager);\n+      snowflakeService.write(config);\n+    }\n+\n+    @FinishBundle\n+    public void finishBundle() throws Exception {\n+      if (debugMode != null) {", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NjY2MA==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451296660", "bodyText": "Add Javadoc to this class? (including whether it's considered part of the public API)", "author": "pabloem", "createdAt": "2020-07-08T05:45:51Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/services/SnowflakeStreamingServiceConfig.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.services;\n+\n+import java.util.List;\n+import net.snowflake.ingest.SimpleIngestManager;\n+\n+public class SnowflakeStreamingServiceConfig extends ServiceConfig {", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NjEwNg==", "url": "https://github.com/apache/beam/pull/12151#discussion_r455086106", "bodyText": "Sure, I will add it", "author": "kkucharc", "createdAt": "2020-07-15T14:16:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NjY2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NzgwNQ==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451297805", "bodyText": "Maybe these fields should be private and final since we have getters?", "author": "pabloem", "createdAt": "2020-07-08T05:49:18Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/services/SnowflakeStreamingServiceConfig.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.services;\n+\n+import java.util.List;\n+import net.snowflake.ingest.SimpleIngestManager;\n+\n+public class SnowflakeStreamingServiceConfig extends ServiceConfig {\n+  SimpleIngestManager ingestManager;\n+  List<String> filesList;\n+  String stagingBucketDir;", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NzU0OA==", "url": "https://github.com/apache/beam/pull/12151#discussion_r455087548", "bodyText": "You are right, thanks for noticing this.", "author": "kkucharc", "createdAt": "2020-07-15T14:18:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NzgwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5ODIwMg==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451298202", "bodyText": "I am a little confused about these lines (53-62). What does this do?", "author": "pabloem", "createdAt": "2020-07-08T05:50:26Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/services/SnowflakeStreamingServiceImpl.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.services;\n+\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.TreeSet;\n+import java.util.stream.Collectors;\n+import net.snowflake.ingest.SimpleIngestManager;\n+import net.snowflake.ingest.connection.IngestResponseException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** Implemenation of {@link SnowflakeService} used in production. */\n+public class SnowflakeStreamingServiceImpl\n+    implements SnowflakeService<SnowflakeStreamingServiceConfig> {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeStreamingServiceImpl.class);\n+  private transient SimpleIngestManager ingestManager;\n+\n+  @Override\n+  public void write(SnowflakeStreamingServiceConfig config) throws Exception {\n+    ingest(config);\n+  }\n+\n+  @Override\n+  public String read(SnowflakeStreamingServiceConfig config) throws Exception {\n+    throw new UnsupportedOperationException(\"Not supported by SnowflakeIO.\");\n+  }\n+\n+  public void ingest(SnowflakeStreamingServiceConfig config)\n+      throws IngestResponseException, IOException, URISyntaxException {\n+    List<String> filesList = config.filesList;\n+    String stagingBucketDir = config.stagingBucketDir;\n+    ingestManager = config.ingestManager;\n+\n+    List<String> newList =\n+        filesList.stream()\n+            .map(e -> e.replaceAll(String.valueOf(stagingBucketDir), \"\"))\n+            .map(e -> e.replaceAll(\"'\", \"\"))\n+            .collect(Collectors.toList());\n+\n+    Set<String> files = new TreeSet<>();\n+    for (String file : newList) {\n+      files.add(file);\n+    }", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzczNDIxOQ==", "url": "https://github.com/apache/beam/pull/12151#discussion_r453734219", "bodyText": "Maybe it should be somehow moved to another method and named properly.\nThe main idea is that the output PCollection from TextIO.write has the method which lets extract names of saved files: .getPerDestinationOutputFilenames(). Unfortunately Snowflake's ingestFiles accepts only Set of files' names to be loaded, but without full path and quotation mark which the .getPerDestinationOutputFilenames() returns.\nThat's why we need remove from filelist name of the bucket directory where they lie and also '.\nI just saw that I can make it shorter:\n Set<String> files =\n        filesList.stream()\n            .map(e -> e.replaceAll(String.valueOf(stagingBucketDir), \"\"))\n            .map(e -> e.replaceAll(\"'\", \"\"))\n            .collect(Collectors.toSet());\n\n\ud83d\ude0a", "author": "kkucharc", "createdAt": "2020-07-13T15:28:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5ODIwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg0NTE0Ng==", "url": "https://github.com/apache/beam/pull/12151#discussion_r453845146", "bodyText": "cool: )", "author": "pabloem", "createdAt": "2020-07-13T18:25:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5ODIwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTMwMTg5Mw==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451301893", "bodyText": "TODO(pabloem) Review these tests", "author": "pabloem", "createdAt": "2020-07-08T06:01:44Z", "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/unit/write/StreamingWriteTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.test.unit.write;\n+\n+import static org.hamcrest.CoreMatchers.equalTo;\n+\n+import java.io.IOException;\n+import java.nio.file.DirectoryStream;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.LongStream;\n+import net.snowflake.client.jdbc.SnowflakeSQLException;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeIO;\n+import org.apache.beam.sdk.io.snowflake.SnowflakePipelineOptions;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory;\n+import org.apache.beam.sdk.io.snowflake.services.SnowflakeService;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeBasicDataSource;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeDatabase;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeStreamingServiceImpl;\n+import org.apache.beam.sdk.io.snowflake.test.TestUtils;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.TestStream;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.hamcrest.MatcherAssert;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@RunWith(JUnit4.class)\n+public class StreamingWriteTest {\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingWriteTest.class);\n+  private static final String FAKE_TABLE = \"TEST_TABLE\";\n+  private static final String STAGING_BUCKET_NAME = \"BUCKET/\";\n+  private static final String STORAGE_INTEGRATION_NAME = \"STORAGE_INTEGRATION\";\n+  private static final String SNOW_PIPE = \"Snowpipe\";\n+  private static final Instant START_TIME = new Instant(0);\n+\n+  @Rule public final transient TestPipeline pipeline = TestPipeline.create();\n+\n+  @Rule public ExpectedException exceptionRule = ExpectedException.none();\n+  private static SnowflakeIO.DataSourceConfiguration dataSourceConfiguration;\n+  private static SnowflakeService snowflakeService;\n+  private static SnowflakePipelineOptions options;\n+  private static List<Long> testData;\n+\n+  private static final List<String> SENTENCES =", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTMwMjQ2MA==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451302460", "bodyText": "Can you add javadoc for this class? It'll help understand its role, and it'll help users figure out if they're meant to use this class or not.", "author": "pabloem", "createdAt": "2020-07-08T06:03:34Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/services/SnowflakeBatchServiceConfig.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.services;\n+\n+import java.util.List;\n+import javax.sql.DataSource;\n+import org.apache.beam.sdk.io.snowflake.enums.WriteDisposition;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+\n+public class SnowflakeBatchServiceConfig extends ServiceConfig {", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTMwMzk3MA==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451303970", "bodyText": "(I recommend you add javadoc to any new public class. A couple sentences is fine for internal classes)", "author": "pabloem", "createdAt": "2020-07-08T06:07:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTMwMjQ2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NjQxNg==", "url": "https://github.com/apache/beam/pull/12151#discussion_r455086416", "bodyText": "Thanks for noticing, I will scan all classes and add where it's missing", "author": "kkucharc", "createdAt": "2020-07-15T14:16:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTMwMjQ2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTMwMzM2MQ==", "url": "https://github.com/apache/beam/pull/12151#discussion_r451303361", "bodyText": "I know this was discussed earlier, but I am somewhat concerned about using CSV as the method to write. Is this not risky? Have you verified that it works well for all types (Numeric, Date/Time, Decimal, Strings with newlines, etc.)?\nI recommend you add an integration test that tests all - or most - data types once we have the Snowflake instance to test.", "author": "pabloem", "createdAt": "2020-07-08T06:06:05Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -719,7 +906,9 @@ private void checkArguments() {\n                           return getUserDataMapper().mapRow(element);\n                         }\n                       }))\n-              .apply(\"Map Objects array to CSV lines\", ParDo.of(new MapObjectsArrayToCsvFn()))\n+              .apply(\n+                  \"Map Objects array to CSV lines\",\n+                  ParDo.of(new MapObjectsArrayToCsvFn(getQuotationMark())))", "originalCommit": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzcyMTEwOA==", "url": "https://github.com/apache/beam/pull/12151#discussion_r453721108", "bodyText": "Yes, I agree that CSV can be a little bit problematic here. On the other hand apparently it is the most efficient way of loading data to Snowflake.\nWe have some tests that were checking different ways of data quotation and problematic characters. But I agree it should be included in integration tests as well. Would it be ok with you if I create Jira ticket for extending integration tests?", "author": "kkucharc", "createdAt": "2020-07-13T15:10:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTMwMzM2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzIxOTkzMg==", "url": "https://github.com/apache/beam/pull/12151#discussion_r463219932", "bodyText": "that makes sense to me. Thanks Kasia!", "author": "pabloem", "createdAt": "2020-07-30T19:24:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTMwMzM2MQ=="}], "type": "inlineReview"}, {"oid": "d1d838f737137ff310b117f0519ff83e6455a523", "url": "https://github.com/apache/beam/commit/d1d838f737137ff310b117f0519ff83e6455a523", "message": "[BEAM-9896] Added Snowflake streaming write with debug mode and unit tests.", "committedDate": "2020-07-15T15:55:33Z", "type": "commit"}, {"oid": "076be87a131b0a619449e9d6daf54fe400ea6db2", "url": "https://github.com/apache/beam/commit/076be87a131b0a619449e9d6daf54fe400ea6db2", "message": "[BEAM-9896] Removed default /data directory in Snowflake write and added parametrized quotation mark.", "committedDate": "2020-07-15T15:55:33Z", "type": "commit"}, {"oid": "5b466ad69248e53976ae57cd94a73f320f3c700c", "url": "https://github.com/apache/beam/commit/5b466ad69248e53976ae57cd94a73f320f3c700c", "message": "[BEAM-9896] Changed default name.", "committedDate": "2020-07-15T15:55:33Z", "type": "commit"}, {"oid": "dc7db2882c7b1a6cde15915231956586e9269e44", "url": "https://github.com/apache/beam/commit/dc7db2882c7b1a6cde15915231956586e9269e44", "message": "[BEAM-9896] Added enum for streaming log level.", "committedDate": "2020-07-15T15:55:33Z", "type": "commit"}, {"oid": "c5e872d63a82f9e1fb171b22c9ccee4f557ff672", "url": "https://github.com/apache/beam/commit/c5e872d63a82f9e1fb171b22c9ccee4f557ff672", "message": "[BEAM-9896] Spotless Apply", "committedDate": "2020-07-15T15:55:33Z", "type": "commit"}, {"oid": "dd38f056d66744e5b6c6d04ede385840c0e55582", "url": "https://github.com/apache/beam/commit/dd38f056d66744e5b6c6d04ede385840c0e55582", "message": "[BEAM-9896] Updated javadocs, error messages and parsing filenames in streaming", "committedDate": "2020-07-15T16:09:57Z", "type": "commit"}, {"oid": "1f8230c476344939fdf0be3a4bd747471735ee1a", "url": "https://github.com/apache/beam/commit/1f8230c476344939fdf0be3a4bd747471735ee1a", "message": "[BEAM-9896] Updated javadocs, error messages and parsing filenames in streaming", "committedDate": "2020-07-15T15:18:18Z", "type": "forcePushed"}, {"oid": "daa675e56a556a20703afd102cdce684385d3e41", "url": "https://github.com/apache/beam/commit/daa675e56a556a20703afd102cdce684385d3e41", "message": "[BEAM-9896] Updated javadocs, error messages and parsing filenames in streaming", "committedDate": "2020-07-15T15:55:33Z", "type": "forcePushed"}, {"oid": "dd38f056d66744e5b6c6d04ede385840c0e55582", "url": "https://github.com/apache/beam/commit/dd38f056d66744e5b6c6d04ede385840c0e55582", "message": "[BEAM-9896] Updated javadocs, error messages and parsing filenames in streaming", "committedDate": "2020-07-15T16:09:57Z", "type": "forcePushed"}, {"oid": "24fc6e3ba74dd5ba3edef29df11631426b4f24dc", "url": "https://github.com/apache/beam/commit/24fc6e3ba74dd5ba3edef29df11631426b4f24dc", "message": "Merge remote-tracking branch 'origin/master' into snowflake-streaming-write", "committedDate": "2020-07-27T09:19:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzI3MzM2Mw==", "url": "https://github.com/apache/beam/pull/12151#discussion_r463273363", "bodyText": "should these attributes also be final?", "author": "pabloem", "createdAt": "2020-07-30T21:10:53Z", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/services/SnowflakeStreamingServiceConfig.java", "diffHunk": "@@ -20,26 +20,51 @@\n import java.util.List;\n import net.snowflake.ingest.SimpleIngestManager;\n \n+/** Class for preparing configuration for streaming write. */\n public class SnowflakeStreamingServiceConfig extends ServiceConfig {\n-  SimpleIngestManager ingestManager;\n-  List<String> filesList;\n-  String stagingBucketDir;\n+  private SimpleIngestManager ingestManager;\n+  private List<String> filesList;\n+  private String stagingBucketDir;", "originalCommit": "dd38f056d66744e5b6c6d04ede385840c0e55582", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDYyODA5MQ==", "url": "https://github.com/apache/beam/pull/12151#discussion_r464628091", "bodyText": "You are right thanks :)", "author": "kkucharc", "createdAt": "2020-08-03T19:52:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzI3MzM2Mw=="}], "type": "inlineReview"}, {"oid": "d92ddb97147e32b0864881165c042ff033f25e5e", "url": "https://github.com/apache/beam/commit/d92ddb97147e32b0864881165c042ff033f25e5e", "message": "[BEAM-9896] Added final keyword to parameters in Snowflake Batch and Streaming configs", "committedDate": "2020-08-05T11:26:57Z", "type": "forcePushed"}, {"oid": "bc980d7cfbb8421d729888b78f956ce1305a32d8", "url": "https://github.com/apache/beam/commit/bc980d7cfbb8421d729888b78f956ce1305a32d8", "message": "[BEAM-9896] Updated CHANGES.md", "committedDate": "2020-08-05T12:53:32Z", "type": "commit"}, {"oid": "eadd31bba3994f4d488dec5f36a13700cd03dfcf", "url": "https://github.com/apache/beam/commit/eadd31bba3994f4d488dec5f36a13700cd03dfcf", "message": "[BEAM-9896] Added final keyword to parameters in Snowflake Batch and Streaming configs", "committedDate": "2020-08-05T12:53:56Z", "type": "commit"}, {"oid": "eadd31bba3994f4d488dec5f36a13700cd03dfcf", "url": "https://github.com/apache/beam/commit/eadd31bba3994f4d488dec5f36a13700cd03dfcf", "message": "[BEAM-9896] Added final keyword to parameters in Snowflake Batch and Streaming configs", "committedDate": "2020-08-05T12:53:56Z", "type": "forcePushed"}]}