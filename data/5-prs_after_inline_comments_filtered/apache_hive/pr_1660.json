{"pr_number": 1660, "pr_title": "HIVE-23410: ACID: Improve the delete and update operations to avoid t\u2026", "pr_createdAt": "2020-11-10T20:21:23Z", "pr_url": "https://github.com/apache/hive/pull/1660", "timeline": [{"oid": "1c72ad893856579c6f63d64bb41e895cdce471eb", "url": "https://github.com/apache/hive/commit/1c72ad893856579c6f63d64bb41e895cdce471eb", "message": "HIVE-23410: ACID: Improve the delete and update operations to avoid the move step", "committedDate": "2020-11-10T19:47:53Z", "type": "commit"}, {"oid": "f0037a9c4df24076bb1ce2d2e90085aad1d782be", "url": "https://github.com/apache/hive/commit/f0037a9c4df24076bb1ce2d2e90085aad1d782be", "message": "Fixed the MoveTask assignment for merge statements with multiple statements with the same ACID operation", "committedDate": "2020-11-13T14:57:34Z", "type": "commit"}, {"oid": "7c7d7d570931cc1cc81953aee50156886e058f3d", "url": "https://github.com/apache/hive/commit/7c7d7d570931cc1cc81953aee50156886e058f3d", "message": "Fix the MoveTask assignment if the query contains multiple statements with the same ACID operation", "committedDate": "2020-11-23T18:29:27Z", "type": "commit"}, {"oid": "7f32b88649c6a4b2e2a3659ccc053fa616f6ba02", "url": "https://github.com/apache/hive/commit/7f32b88649c6a4b2e2a3659ccc053fa616f6ba02", "message": "Fix the file listings in MoveTasks for insert overwrite for mm tables", "committedDate": "2020-11-24T19:17:04Z", "type": "commit"}, {"oid": "1f6edc428c7f47bc4c22ffe47d2dae3b82719d3c", "url": "https://github.com/apache/hive/commit/1f6edc428c7f47bc4c22ffe47d2dae3b82719d3c", "message": "Fix the file listing for union all queries", "committedDate": "2020-11-30T18:36:45Z", "type": "commit"}, {"oid": "0720b3809a9776b3983b68a3c993fe4790a83ea4", "url": "https://github.com/apache/hive/commit/0720b3809a9776b3983b68a3c993fe4790a83ea4", "message": "Merge remote-tracking branch 'apache/master' into HIVE-23410-1110", "committedDate": "2020-12-01T11:59:19Z", "type": "commit"}, {"oid": "98a29c50395d50a7fcaa670c7c9ed2a69077b75f", "url": "https://github.com/apache/hive/commit/98a29c50395d50a7fcaa670c7c9ed2a69077b75f", "message": "Fixing the acidSink sorting and some test outputs", "committedDate": "2020-12-02T17:46:54Z", "type": "commit"}, {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca", "url": "https://github.com/apache/hive/commit/6f9cfcd37341fe990b4411d56ad3a4d050ec2bca", "message": "Fix the acidSink sorting and add ordering to the q test", "committedDate": "2020-12-03T13:02:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzQ2NDc1Mw==", "url": "https://github.com/apache/hive/pull/1660#discussion_r537464753", "bodyText": "Is this change expected? What are these records?", "author": "pvargacl", "createdAt": "2020-12-07T12:22:35Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "diffHunk": "@@ -1747,15 +1747,15 @@ public void testMultiInsertOnDynamicallyPartitionedMmTable() throws Exception {\n     final String completedTxnComponentsContents =\n         TxnDbUtil.queryToString(conf, \"select * from \\\"COMPLETED_TXN_COMPONENTS\\\"\");\n     Assert.assertEquals(completedTxnComponentsContents,\n-        2, TxnDbUtil.countQueryAgent(conf, \"select count(*) from \\\"COMPLETED_TXN_COMPONENTS\\\"\"));\n+        4, TxnDbUtil.countQueryAgent(conf, \"select count(*) from \\\"COMPLETED_TXN_COMPONENTS\\\"\"));", "originalCommit": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ0NTE5Ng==", "url": "https://github.com/apache/hive/pull/1660#discussion_r538445196", "bodyText": "Those records are duplicates. It is a \"side-effect\" of fixing the FileSinkOperator-MoveTask assignment.\nFor ACID tables for an insert like in the test, 4 records were created even before the direct insert got introduced. Because then the FSO-MoveTask assignment was based on the staging directories. And for insert like this there were 2 FSOs and 2 MoveTasks. Each MoveTasks called the metastore method which creates an entry in the TXN_COMPONENTS table for each partition. So there were 4 records at the end of the insert. But for MM tables (and later for direct insert) there is no staging directory and all MoveTasks and all FSOs will contain the table directory. So for every FSO it will find the same MoveTask (which is the first in the list) and only this one will be executed. This is not correct, but didn't cause any issue, so it was undetected until the direct delete and update came in. To make them work properly, had to fix the FSO-MoveTask assignment, but then for MM tables and with direct insert it will have duplicate records just like for ACID tables without direct insert. The Java doc of the TxnHandler.addDynamicPartitions method says that duplicates won't cause any trouble, but if you know issues with that, please share it with me.", "author": "kuczoram", "createdAt": "2020-12-08T14:39:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzQ2NDc1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU2MDYzMw==", "url": "https://github.com/apache/hive/pull/1660#discussion_r537560633", "bodyText": "Could we start it from 0?", "author": "pvary", "createdAt": "2020-12-07T14:43:37Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/Context.java", "diffHunk": "@@ -105,6 +105,7 @@\n \n   private Configuration conf;\n   protected int pathid = 10000;\n+  private int moveTaskId = 100;", "originalCommit": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ1MjQ1NQ==", "url": "https://github.com/apache/hive/pull/1660#discussion_r538452455", "bodyText": "Sure! Fixed it.", "author": "kuczoram", "createdAt": "2020-12-08T14:46:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU2MDYzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU2NjQxMg==", "url": "https://github.com/apache/hive/pull/1660#discussion_r537566412", "bodyText": "Maybe javadoc instead of a comment?", "author": "pvary", "createdAt": "2020-12-07T14:50:51Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java", "diffHunk": "@@ -189,6 +191,49 @@ public WriteEntity getAcidAnalyzeTable() {\n     return acidSinks;\n   }\n \n+  public Integer getStatementIdForAcidWriteType(long writeId, String moveTaskId, AcidUtils.Operation acidOperation, Path path) {\n+    FileSinkDesc result = null;\n+    for (FileSinkDesc acidSink : acidSinks) {\n+      if (acidOperation.equals(acidSink.getAcidOperation()) && path.equals(acidSink.getDestPath())\n+          && acidSink.getTableWriteId() == writeId\n+          && (moveTaskId == null || acidSink.getMoveTaskId() == null || moveTaskId.equals(acidSink.getMoveTaskId()))) {\n+        // There is a problem with the union all optimisation. In this case, there will be multiple FileSinkOperators", "originalCommit": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ1NTQ2NQ==", "url": "https://github.com/apache/hive/pull/1660#discussion_r538455465", "bodyText": "Yeah, it would be better. Added Java doc.", "author": "kuczoram", "createdAt": "2020-12-08T14:49:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU2NjQxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU2NzEzMQ==", "url": "https://github.com/apache/hive/pull/1660#discussion_r537567131", "bodyText": "Javadoc", "author": "pvary", "createdAt": "2020-12-07T14:51:42Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java", "diffHunk": "@@ -189,6 +191,49 @@ public WriteEntity getAcidAnalyzeTable() {\n     return acidSinks;\n   }\n \n+  public Integer getStatementIdForAcidWriteType(long writeId, String moveTaskId, AcidUtils.Operation acidOperation, Path path) {\n+    FileSinkDesc result = null;\n+    for (FileSinkDesc acidSink : acidSinks) {\n+      if (acidOperation.equals(acidSink.getAcidOperation()) && path.equals(acidSink.getDestPath())\n+          && acidSink.getTableWriteId() == writeId\n+          && (moveTaskId == null || acidSink.getMoveTaskId() == null || moveTaskId.equals(acidSink.getMoveTaskId()))) {\n+        // There is a problem with the union all optimisation. In this case, there will be multiple FileSinkOperators\n+        // with the same operation, writeId and moveTaskId. But one of these FSOs doesn't write data and its statementId\n+        // is not valid, so if this FSO is selected and its statementId is returned, the file listing will find nothing.\n+        // So check the acidSinks and if two of them have the same writeId, path and moveTaskId, then return -1 as statementId.\n+        // Like this, the file listing will find all partitions and files correctly.\n+        if (result != null) {\n+          return -1;\n+        }\n+        result = acidSink;\n+      }\n+    }\n+    if (result != null) {\n+      return result.getStatementId();\n+    } else {\n+      return -1;\n+    }\n+  }\n+\n+  public Set<String> getDynamicPartitionSpecs(long writeId, String moveTaskId, AcidUtils.Operation acidOperation, Path path) {", "originalCommit": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ3NDEwMQ==", "url": "https://github.com/apache/hive/pull/1660#discussion_r538474101", "bodyText": "Added it.", "author": "kuczoram", "createdAt": "2020-12-08T15:06:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU2NzEzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU3MjM0Mw==", "url": "https://github.com/apache/hive/pull/1660#discussion_r537572343", "bodyText": "Could we do something like a switch?", "author": "pvary", "createdAt": "2020-12-07T14:57:50Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "diffHunk": "@@ -232,9 +236,25 @@ public void closeWriters(boolean abort) throws HiveException {\n       for (int i = 0; i < updaters.length; i++) {\n         if (updaters[i] != null) {\n           SerDeStats stats = updaters[i].getStats();\n-          // Ignore 0 row files except in case of insert overwrite\n-          if (isDirectInsert && (stats.getRowCount() > 0 || isInsertOverwrite)) {\n-            outPathsCommitted[i] = updaters[i].getUpdatedFilePath();\n+          // Ignore 0 row files except in case of insert overwrite or delete or update\n+          if (isDirectInsert\n+              && (stats.getRowCount() > 0 || isInsertOverwrite || AcidUtils.Operation.DELETE.equals(acidOperation)\n+                  || AcidUtils.Operation.UPDATE.equals(acidOperation))) {\n+            // In case of delete operation, the deleteFilePath has to be used, not the updatedFilePath\n+            // In case of update operation, we need both paths. The updateFilePath will be added\n+            // to the outPathsCommitted array and the deleteFilePath will be collected in a separate list.\n+            OrcRecordUpdater recordUpdater = (OrcRecordUpdater) updaters[i];\n+            outPathsCommitted[i] = recordUpdater.getUpdatedFilePath();", "originalCommit": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODU1NjU0NA==", "url": "https://github.com/apache/hive/pull/1660#discussion_r538556544", "bodyText": "Sure! Fixed", "author": "kuczoram", "createdAt": "2020-12-08T16:18:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU3MjM0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU3NjA2Ng==", "url": "https://github.com/apache/hive/pull/1660#discussion_r537576066", "bodyText": "Think through 1 more time. I can accept that this is the best solution, but this is UGLY \ud83d\ude04", "author": "pvary", "createdAt": "2020-12-07T15:02:39Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "diffHunk": "@@ -251,7 +271,7 @@ public void closeWriters(boolean abort) throws HiveException {\n       }\n     }\n \n-    private void commit(FileSystem fs, List<Path> commitPaths) throws HiveException {\n+    private void commit(FileSystem fs, List<Path> commitPaths, List<Path> deleteDeltas) throws HiveException {", "originalCommit": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5NjQzMg==", "url": "https://github.com/apache/hive/pull/1660#discussion_r538496432", "bodyText": "I know, but I don't really know a better solution, only if we change the internal structures in FileSinkOperator. Like using Lists instead of arrays. But this could have unexpected side effects. I am open to try it but I would do it under a separate Jira. I create one about investigating this refactoring.\nhttps://issues.apache.org/jira/browse/HIVE-24505", "author": "kuczoram", "createdAt": "2020-12-08T15:26:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU3NjA2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU4ODEyMw==", "url": "https://github.com/apache/hive/pull/1660#discussion_r537588123", "bodyText": "Javadoc please", "author": "pvary", "createdAt": "2020-12-07T15:17:52Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -563,6 +564,21 @@ else if (filename.startsWith(BUCKET_PREFIX)) {\n     return result;\n   }\n \n+  public static Map<String, Integer> getDeltaToAttemptIdMap(", "originalCommit": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUyNjg3Nw==", "url": "https://github.com/apache/hive/pull/1660#discussion_r538526877", "bodyText": "Done", "author": "kuczoram", "createdAt": "2020-12-08T15:53:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU4ODEyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU5MzUwNw==", "url": "https://github.com/apache/hive/pull/1660#discussion_r537593507", "bodyText": "Too long line", "author": "pvary", "createdAt": "2020-12-07T15:24:45Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java", "diffHunk": "@@ -2694,7 +2699,7 @@ private void constructOneLBLocationMap(FileStatus fSta,\n    */\n   private Set<Path> getValidPartitionsInPath(\n       int numDP, int numLB, Path loadPath, Long writeId, int stmtId,\n-      boolean isMmTable, boolean isInsertOverwrite, boolean isDirectInsert) throws HiveException {\n+      boolean isMmTable, boolean isInsertOverwrite, boolean isDirectInsert, AcidUtils.Operation operation, Set<String> dynamiPartitionSpecs) throws HiveException {", "originalCommit": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUwMjkzMQ==", "url": "https://github.com/apache/hive/pull/1660#discussion_r538502931", "bodyText": "Fixed", "author": "kuczoram", "createdAt": "2020-12-08T15:32:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU5MzUwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU5NzA3Nw==", "url": "https://github.com/apache/hive/pull/1660#discussion_r537597077", "bodyText": "Double check if this is needed", "author": "pvary", "createdAt": "2020-12-07T15:29:09Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "diffHunk": "@@ -1895,7 +1901,20 @@ public static boolean isSkewedStoredAsDirs(FileSinkDesc fsInputDesc) {\n       }\n \n       if ((srcDir != null) && srcDir.equals(fsopFinalDir)) {\n-        return mvTsk;\n+        if (isDirectInsert || isMmFsop) {\n+          if (moveTaskId != null && fsoMoveTaskId != null && moveTaskId.equals(fsoMoveTaskId)) {\n+            // If the ACID direct insert is on, the MoveTasks cannot be identified by the srcDir as\n+            // in this case the srcDir is always the root directory of the table.\n+            // We need to consider the ACID write type to identify the MoveTasks.\n+            return mvTsk;\n+          }\n+          if ((moveTaskId == null || fsoMoveTaskId == null) && moveTaskWriteType != null", "originalCommit": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUyMDc2OA==", "url": "https://github.com/apache/hive/pull/1660#discussion_r538520768", "bodyText": "There was a test which was failing if this was not there, but since then I think I fixed the moveTaskId generation, so cannot be null. It think this is not needed. I will remove it and let's see what the tests say.", "author": "kuczoram", "createdAt": "2020-12-08T15:47:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU5NzA3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYwMTYxNw==", "url": "https://github.com/apache/hive/pull/1660#discussion_r537601617", "bodyText": "Remove", "author": "pvary", "createdAt": "2020-12-07T15:34:56Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java", "diffHunk": "@@ -52,6 +52,8 @@\n   @Mock\n   private DataInput mockDataInput;\n \n+  // IRJUNK IDE TESZTET!!!", "originalCommit": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ3NTQ3OA==", "url": "https://github.com/apache/hive/pull/1660#discussion_r538475478", "bodyText": "Removed.", "author": "kuczoram", "createdAt": "2020-12-08T15:07:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYwMTYxNw=="}], "type": "inlineReview"}, {"oid": "3bb8e5b0a6c83cf47a7e75daaf6a5edaa9f7c290", "url": "https://github.com/apache/hive/commit/3bb8e5b0a6c83cf47a7e75daaf6a5edaa9f7c290", "message": "Address review findings", "committedDate": "2020-12-08T19:17:44Z", "type": "commit"}]}