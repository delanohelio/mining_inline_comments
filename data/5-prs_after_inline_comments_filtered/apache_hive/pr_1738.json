{"pr_number": 1738, "pr_title": "HIVE-24481: Skipped compaction can cause data corruption with streaming", "pr_createdAt": "2020-12-03T17:13:00Z", "pr_url": "https://github.com/apache/hive/pull/1738", "timeline": [{"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34", "url": "https://github.com/apache/hive/commit/84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34", "message": "HIVE-24481: Skipped compaction can cause data corruption with streaming", "committedDate": "2020-12-03T17:11:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTkzOTMyOQ==", "url": "https://github.com/apache/hive/pull/1738#discussion_r535939329", "bodyText": "In a follow up Jira it might be worth change this whole AcidUtils approach and start to put everything from the beginning in a DirectoryImpl, so the argument count could be decreased to a sane amount.", "author": "pvargacl", "createdAt": "2020-12-04T08:59:39Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -1369,14 +1383,14 @@ private static Directory getAcidState(FileSystem fileSystem, Path candidateDirec\n     if (childrenWithId != null) {\n       for (HdfsFileStatusWithId child : childrenWithId) {\n         getChildState(child, writeIdList, working, originalDirectories, original, obsolete,\n-            bestBase, ignoreEmptyFiles, abortedDirectories, abortedWriteIds, fs, validTxnList);\n+            bestBase, ignoreEmptyFiles, abortedDirectories, abortedWriteIds, uncompactedAborts, fs, validTxnList);", "originalCommit": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk2NTM2OA==", "url": "https://github.com/apache/hive/pull/1738#discussion_r535965368", "bodyText": "It's starting to affect readability, maybe refactor in the following patches.", "author": "deniskuzZ", "createdAt": "2020-12-04T09:40:20Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -566,20 +567,20 @@ else if (filename.startsWith(BUCKET_PREFIX)) {\n   public static final class DirectoryImpl implements Directory {\n     private final List<Path> abortedDirectories;\n     private final Set<Long> abortedWriteIds;\n+    private final boolean uncompactedAborts;\n     private final boolean isBaseInRawFormat;\n     private final List<HdfsFileStatusWithId> original;\n     private final List<Path> obsolete;\n     private final List<ParsedDelta> deltas;\n     private final Path base;\n     private List<HdfsFileStatusWithId> baseFiles;\n \n-    public DirectoryImpl(List<Path> abortedDirectories, Set<Long> abortedWriteIds,\n-        boolean isBaseInRawFormat, List<HdfsFileStatusWithId> original,\n-        List<Path> obsolete, List<ParsedDelta> deltas, Path base) {\n-      this.abortedDirectories = abortedDirectories == null ?\n-          Collections.emptyList() : abortedDirectories;\n-      this.abortedWriteIds = abortedWriteIds == null ?\n-        Collections.emptySet() : abortedWriteIds;\n+    public DirectoryImpl(List<Path> abortedDirectories, Set<Long> abortedWriteIds, boolean uncompactedAborts,", "originalCommit": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzMyOTE0Mg==", "url": "https://github.com/apache/hive/pull/1738#discussion_r537329142", "bodyText": "Yes, I plan to do that. I think I will make DirectoryImpl mutable, and pass it down the getAcidState calls, to gather all the info in it along the way.", "author": "pvargacl", "createdAt": "2020-12-07T08:51:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk2NTM2OA=="}], "type": "inlineReview"}, {"oid": "6a4cb91c2bd1eaaba88cafbff3446904ea69a5f2", "url": "https://github.com/apache/hive/commit/6a4cb91c2bd1eaaba88cafbff3446904ea69a5f2", "message": "Merge remote-tracking branch 'origin/master' into HIVE-24481-skipped-compaction", "committedDate": "2020-12-04T10:24:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDE3NDQ2Mw==", "url": "https://github.com/apache/hive/pull/1738#discussion_r540174463", "bodyText": "expectation msg copy-paste - \"there should be single record for the 2nd aborted txn\"", "author": "deniskuzZ", "createdAt": "2020-12-10T13:37:59Z", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -1177,6 +1177,104 @@ private HiveStreamingConnection prepareTableTwoPartitionsAndConnection(String db\n         .connect();\n   }\n \n+  /**\n+   * There is a special case handled in Compaction Worker that will skip compaction\n+   * if there is only one valid delta. But this compaction will be still cleaned up, if there are aborted directories.\n+   * @see Worker.isEnoughToCompact\n+   * However if no compaction was done, deltas containing mixed aborted / committed writes from streaming can not be cleaned\n+   * and the metadata belonging to those aborted transactions can not be removed.\n+   * @throws Exception ex\n+   */\n+  @Test\n+  public void testSkippedCompactionCleanerKeepsAborted() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    String agentInfo = \"UT_\" + Thread.currentThread().getName();\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+\n+    executeStatementOnDriver(\"drop table if exists \" + tblName, driver);\n+    executeStatementOnDriver(\"CREATE TABLE \" + tblName + \"(b STRING) \" +\n+        \" PARTITIONED BY (a INT) STORED AS ORC  TBLPROPERTIES ('transactional'='true')\", driver);\n+    executeStatementOnDriver(\"alter table \" + tblName + \" add partition(a=1)\", driver);\n+\n+    StrictDelimitedInputWriter writer = StrictDelimitedInputWriter.newBuilder()\n+        .withFieldDelimiter(',')\n+        .build();\n+\n+    // Create initial aborted txn\n+    HiveStreamingConnection connection = HiveStreamingConnection.newBuilder()\n+        .withDatabase(dbName)\n+        .withTable(tblName)\n+        .withStaticPartitionValues(Collections.singletonList(\"1\"))\n+        .withAgentInfo(agentInfo)\n+        .withHiveConf(conf)\n+        .withRecordWriter(writer)\n+        .withStreamingOptimizations(true)\n+        .withTransactionBatchSize(1)\n+        .connect();\n+\n+    connection.beginTransaction();\n+    connection.write(\"3,1\".getBytes());\n+    connection.write(\"4,1\".getBytes());\n+    connection.abortTransaction();\n+\n+    connection.close();\n+\n+    // Create a sequence of commit, abort, commit to the same delta folder\n+    connection = HiveStreamingConnection.newBuilder()\n+        .withDatabase(dbName)\n+        .withTable(tblName)\n+        .withStaticPartitionValues(Collections.singletonList(\"1\"))\n+        .withAgentInfo(agentInfo)\n+        .withHiveConf(conf)\n+        .withRecordWriter(writer)\n+        .withStreamingOptimizations(true)\n+        .withTransactionBatchSize(3)\n+        .connect();\n+\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,1\".getBytes());\n+    connection.commitTransaction();\n+\n+    connection.beginTransaction();\n+    connection.write(\"3,1\".getBytes());\n+    connection.write(\"4,1\".getBytes());\n+    connection.abortTransaction();\n+\n+    connection.beginTransaction();\n+    connection.write(\"5,1\".getBytes());\n+    connection.write(\"6,1\".getBytes());\n+    connection.commitTransaction();\n+\n+    connection.close();\n+\n+    // Check that aborted are not read back\n+    driver.run(\"select * from cws\");\n+    List res = new ArrayList();\n+    driver.getFetchTask().fetch(res);\n+    Assert.assertEquals(4, res.size());\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(\"There should be 2 record for two aborted transaction\", 2, count);\n+\n+    // Start a compaction, that will be skipped, because only one valid delta is there\n+    driver.run(\"alter table cws partition(a='1') compact 'minor'\");\n+    runWorker(conf);\n+    // Cleaner should not delete info about aborted txn 2\n+    runCleaner(conf);\n+    txnHandler.cleanEmptyAbortedAndCommittedTxns();\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(\"There should be 1 record for two aborted transaction\", 1, count);", "originalCommit": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDE5MDcxNQ==", "url": "https://github.com/apache/hive/pull/1738#discussion_r540190715", "bodyText": "fix the javadoc", "author": "deniskuzZ", "createdAt": "2020-12-10T14:00:09Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -923,6 +929,13 @@ public String toString() {\n      * @return the list of aborted writeIds\n      */\n     Set<Long> getAbortedWriteIds();\n+\n+    /**\n+     * Get the list of writeIds that belong to aborted transactions, but can not be cleaned,", "originalCommit": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "03fa4e9799f9995e7a16422e65b52f147ad3e7a1", "url": "https://github.com/apache/hive/commit/03fa4e9799f9995e7a16422e65b52f147ad3e7a1", "message": "Fix review comments", "committedDate": "2020-12-10T14:29:54Z", "type": "commit"}]}