{"pr_number": 1325, "pr_title": "HIVE-21196 HIVE-23940 Multi-column semijoin reducers and TPC-H datasets ", "pr_createdAt": "2020-07-27T17:35:21Z", "pr_url": "https://github.com/apache/hive/pull/1325", "timeline": [{"oid": "7ec73bb62d7fab120793e7a223c827db943bebd6", "url": "https://github.com/apache/hive/commit/7ec73bb62d7fab120793e7a223c827db943bebd6", "message": "HIVE-21196: Update TestTezPerfConstraintsCliDriver query plans\n\nImportant changes only on query24, query50, query93 plans.\nSingle columns semi join reducers are merged to composite ones as per optimization.", "committedDate": "2020-07-28T16:13:03Z", "type": "forcePushed"}, {"oid": "555ac23c2bc2b13e2b45529e78003d32a5b3ff58", "url": "https://github.com/apache/hive/commit/555ac23c2bc2b13e2b45529e78003d32a5b3ff58", "message": "HIVE-21196: Update TestTezPerfConstraintsCliDriver query plans\n\nImportant changes only on query24, query50, query93 plans.\nSingle columns semi join reducers are merged to composite ones as per optimization.", "committedDate": "2020-07-29T13:32:23Z", "type": "forcePushed"}, {"oid": "792f7f2faece0017c524270c3dd0df674057d4c8", "url": "https://github.com/apache/hive/commit/792f7f2faece0017c524270c3dd0df674057d4c8", "message": "HIVE-21196: Update TestTezPerfConstraintsCliDriver query plans\n\nImportant changes only on query24, query50, query93 plans.\nSingle columns semi join reducers are merged to composite ones as per optimization.", "committedDate": "2020-07-30T12:55:14Z", "type": "forcePushed"}, {"oid": "25383ae9e530378c31bbfbf6b4124bb8e4cfd0bd", "url": "https://github.com/apache/hive/commit/25383ae9e530378c31bbfbf6b4124bb8e4cfd0bd", "message": "HIVE-23940: Add TPCH tables (scale factor 0.001) as qt datasets\n\n1. Add new tables in a separate Hive database namely tpch_0_001 to distinguish\nfrom existing lineitem, part tables in default database.\n2. Compress existing part-tiny and lineitem data to gain a bit of space.", "committedDate": "2020-08-03T13:05:14Z", "type": "commit"}, {"oid": "c47de66808b13d1c2d02bcb2b623f4b6b144146a", "url": "https://github.com/apache/hive/commit/c47de66808b13d1c2d02bcb2b623f4b6b144146a", "message": "HIVE-21196: Add transformation merging multiple single-col SJ reducers to one multi-col SJ\n\n1. Put the merge transformation logic in self-contained SemiJoinReductionMerge class\n2. Extend SJ related code (outside of SemiJoinReductionMerge) to account for multiple columns\n3. Add zero as lower bound for in bloomfilter reduction factor estimation\n4. Execute semi-join reducers merge transformation just before stats annotation\n5. Remove duplicate case branch in SemanticAnalyzer#groupByDescModeToUDAFMode\n6. Add utility function for finding an ancestor operator provided a path\n7. Add configuration property for multi-column semi-join reduction", "committedDate": "2020-08-03T13:40:51Z", "type": "commit"}, {"oid": "a4c6b5b889ae1477a8fe016c3e21d4f0ac11d0d3", "url": "https://github.com/apache/hive/commit/a4c6b5b889ae1477a8fe016c3e21d4f0ac11d0d3", "message": "HIVE-21196: Add tests for multicol semijoin reduction", "committedDate": "2020-08-03T13:40:51Z", "type": "commit"}, {"oid": "089ffbe841c80eccce9827185991d0ec860c5e9e", "url": "https://github.com/apache/hive/commit/089ffbe841c80eccce9827185991d0ec860c5e9e", "message": "HIVE-21196: Update plans in Perf cli drivers\n\nMulti-col semijoin reduction optimization correctly kicks in in queries:\nquery24, query50, query93\n\nThe remaining changes are not significant and consist only in different\nopperator ids.", "committedDate": "2020-08-03T13:40:51Z", "type": "commit"}, {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d", "url": "https://github.com/apache/hive/commit/edffed725aa5cb794578688d353cd015bc54cb6d", "message": "HIVE-21196: Update plans in MiniLlap drivers", "committedDate": "2020-08-03T13:40:51Z", "type": "commit"}, {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d", "url": "https://github.com/apache/hive/commit/edffed725aa5cb794578688d353cd015bc54cb6d", "message": "HIVE-21196: Update plans in MiniLlap drivers", "committedDate": "2020-08-03T13:40:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2MzMxOQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464763319", "bodyText": "Neat! Interesting method... Reminds me of our good old times with XPath \ud83d\ude04", "author": "jcamachor", "createdAt": "2020-08-04T02:33:38Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java", "diffHunk": "@@ -53,6 +53,34 @@\n \n   private static final Logger LOG = LoggerFactory.getLogger(OperatorUtils.class);\n \n+  /**\n+   * Return the ancestor of the specified operator at the provided path or null if the path is invalid.\n+   *\n+   * The method is equivalent to following code:\n+   * <pre>{@code\n+   *     op.getParentOperators().get(path[0])", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NDA1OQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464764059", "bodyText": "nit. We use { } even for single line statements.\nPlease, check below in other code changes in this PR too since I have seen the same.", "author": "jcamachor", "createdAt": "2020-08-04T02:36:31Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java", "diffHunk": "@@ -53,6 +53,34 @@\n \n   private static final Logger LOG = LoggerFactory.getLogger(OperatorUtils.class);\n \n+  /**\n+   * Return the ancestor of the specified operator at the provided path or null if the path is invalid.\n+   *\n+   * The method is equivalent to following code:\n+   * <pre>{@code\n+   *     op.getParentOperators().get(path[0])\n+   *     .getParentOperators().get(path[1])\n+   *     ...\n+   *     .getParentOperators().get(path[n])\n+   * }</pre>\n+   * with additional checks about the validity of the provided path and the type of the ancestor.\n+   *\n+   * @param op the operator for which we\n+   * @param clazz the class of the ancestor operator\n+   * @param path the path leading to the desired ancestor\n+   * @param <T> the type of the ancestor\n+   * @return the ancestor of the specified operator at the provided path or null if the path is invalid.\n+   */\n+  public static <T> T ancestor(Operator<?> op, Class<T> clazz, int... path) {\n+    Operator<?> target = op;\n+    for (int i = 0; i < path.length; i++) {\n+      if (target.getParentOperators() == null || path[i] > target.getParentOperators().size())", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0Nzc4Mw==", "url": "https://github.com/apache/hive/pull/1325#discussion_r466347783", "bodyText": "Done. I also configured IntelliJ to force their usage in single line statements so hopefully they should never appear.", "author": "zabetak", "createdAt": "2020-08-06T11:31:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NDA1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NDI5OQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464764299", "bodyText": "Please add comment with general explanation of purpose to the class.\nIt would also be useful to add comments to the methods below.", "author": "jcamachor", "createdAt": "2020-08-04T02:37:31Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0NzgzOQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r466347839", "bodyText": "Done", "author": "zabetak", "createdAt": "2020-08-06T11:31:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NDI5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NDg1OQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464764859", "bodyText": "Wondering whether we should make these checks Preconditions rather than assert.", "author": "jcamachor", "createdAt": "2020-08-04T02:39:53Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE5MjUyOQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r465192529", "bodyText": "I was having the same internal debate with my self and in the end I concluded to the following. If we arrive here then something went terribly wrong so it's better to fail miserably rather than throwing an exception that someone might catch higher up and move forward.\nAssertions have also disadvantages so let me know what you prefer I am fine with any.", "author": "zabetak", "createdAt": "2020-08-04T16:53:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NDg1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE5NzEwMg==", "url": "https://github.com/apache/hive/pull/1325#discussion_r465197102", "bodyText": "The problem with the assert is that it may be disabled in production. That's why I would rather add something like Preconditions.checkNotNull or Preconditions.checkState", "author": "jcamachor", "createdAt": "2020-08-04T17:00:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NDg1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2NTQ1Mw==", "url": "https://github.com/apache/hive/pull/1325#discussion_r466365453", "bodyText": "Done. Didn't add checkNotNull since NPE will be thrown anyways and it is rather informative as well.", "author": "zabetak", "createdAt": "2020-08-06T12:08:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NDg1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NzUwOA==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464767508", "bodyText": "Maybe using a LinkedHashMap for rsToSemiJoinBranchInfo from the onset would eliminate the need for sorting and above (just a thought).", "author": "jcamachor", "createdAt": "2020-08-04T02:49:55Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2NTU4Mw==", "url": "https://github.com/apache/hive/pull/1325#discussion_r466365583", "bodyText": "Good idea, I will try that out.", "author": "zabetak", "createdAt": "2020-08-06T12:08:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NzUwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxODI3MA==", "url": "https://github.com/apache/hive/pull/1325#discussion_r467318270", "bodyText": "Done in f387d78", "author": "zabetak", "createdAt": "2020-08-07T22:51:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NzUwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NzY4MA==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464767680", "bodyText": "typo? sjBrances -> sjBranches", "author": "jcamachor", "createdAt": "2020-08-04T02:50:37Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2NTYzNg==", "url": "https://github.com/apache/hive/pull/1325#discussion_r466365636", "bodyText": "Fixed", "author": "zabetak", "createdAt": "2020-08-06T12:08:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NzY4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3ODE2NA==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464778164", "bodyText": "Can these SEL operators have multiple columns? I thought they would project a single column for the SJ?", "author": "jcamachor", "createdAt": "2020-08-04T03:32:12Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?\n+      // parseContext.getColExprToGBMap().put(key, gb);\n+    }\n+    return parseContext;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc createSemiJoinPredicate(List<ReduceSinkOperator> sjBranches,\n+      RuntimeValuesInfo sjValueInfo, ParseContext context) {\n+    // Performance note: To speed-up evaluation 'BETWEEN' predicates should come before the 'IN_BLOOM_FILTER'\n+    Deque<String> dynamicIds = new ArrayDeque<>(sjValueInfo.getDynamicValueIDs());\n+    List<ExprNodeDesc> sjPredicates = new ArrayList<>();\n+    List<ExprNodeDesc> hashArgs = new ArrayList<>();\n+    for (ReduceSinkOperator rs : sjBranches) {\n+      RuntimeValuesInfo info = context.getRsToRuntimeValuesInfoMap().get(rs);\n+      assert info.getTargetColumns().size() == 1;\n+      final ExprNodeDesc targetColumn = info.getTargetColumns().get(0);\n+      TypeInfo typeInfo = targetColumn.getTypeInfo();\n+      DynamicValue minDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+      DynamicValue maxDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+\n+      List<ExprNodeDesc> betweenArgs = Arrays.asList(\n+          // Use false to not invert between result\n+          new ExprNodeConstantDesc(Boolean.FALSE),\n+          targetColumn,\n+          new ExprNodeDynamicValueDesc(minDynamic),\n+          new ExprNodeDynamicValueDesc(maxDynamic));\n+      ExprNodeDesc betweenExp =\n+          new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFBetween(), \"between\", betweenArgs);\n+      sjPredicates.add(betweenExp);\n+      hashArgs.add(targetColumn);\n+    }\n+\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", hashArgs);\n+\n+    assert dynamicIds.size() == 1 : \"There should be one column left untreated the one with the bloom filter\";\n+    DynamicValue bloomDynamic = new DynamicValue(dynamicIds.poll(), TypeInfoFactory.binaryTypeInfo);\n+    sjPredicates.add(\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFInBloomFilter(), \"in_bloom_filter\",\n+            Arrays.asList(hashExp, new ExprNodeDynamicValueDesc(bloomDynamic))));\n+    return and(sjPredicates);\n+  }\n+\n+  private static RuntimeValuesInfo createRuntimeValuesInfo(ReduceSinkOperator rs, List<ReduceSinkOperator> sjBranches,\n+      ParseContext parseContext) {\n+    List<ExprNodeDesc> valueCols = rs.getConf().getValueCols();\n+    RuntimeValuesInfo info = new RuntimeValuesInfo();\n+    TableDesc rsFinalTableDesc =\n+        PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, \"_col\"));\n+    List<String> dynamicValueIDs = new ArrayList<>();\n+    for (ExprNodeDesc rsCol : valueCols) {\n+      dynamicValueIDs.add(rs.toString() + rsCol.getExprString());\n+    }\n+\n+    info.setTableDesc(rsFinalTableDesc);\n+    info.setDynamicValueIDs(dynamicValueIDs);\n+    info.setColExprs(valueCols);\n+    List<ExprNodeDesc> targetTableExpressions = new ArrayList<>();\n+    for (ReduceSinkOperator sjBranch : sjBranches) {\n+      RuntimeValuesInfo sjInfo = parseContext.getRsToRuntimeValuesInfoMap().get(sjBranch);\n+      assert sjInfo.getTargetColumns().size() == 1;\n+      targetTableExpressions.add(sjInfo.getTargetColumns().get(0));\n+    }\n+    info.setTargetColumns(targetTableExpressions);\n+    return info;\n+  }\n+\n+  private static SelectOperator mergeSelectOps(Operator<?> parent, List<SelectOperator> selectOperators) {\n+    List<String> colNames = new ArrayList<>();\n+    List<ExprNodeDesc> colDescs = new ArrayList<>();\n+    List<ColumnInfo> columnInfos = new ArrayList<>();\n+    Map<String, ExprNodeDesc> selectColumnExprMap = new HashMap<>();\n+    for (SelectOperator sel : selectOperators) {\n+      for (ExprNodeDesc col : sel.getConf().getColList()) {", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2NjE5Nw==", "url": "https://github.com/apache/hive/pull/1325#discussion_r466366197", "bodyText": "Nope, at this point we should have only single column selections. I added some doc and precondition checks.", "author": "zabetak", "createdAt": "2020-08-06T12:09:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3ODE2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3OTcxNQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464779715", "bodyText": "Add comment mentioning you need these for min/max", "author": "jcamachor", "createdAt": "2020-08-04T03:38:24Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?\n+      // parseContext.getColExprToGBMap().put(key, gb);\n+    }\n+    return parseContext;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc createSemiJoinPredicate(List<ReduceSinkOperator> sjBranches,\n+      RuntimeValuesInfo sjValueInfo, ParseContext context) {\n+    // Performance note: To speed-up evaluation 'BETWEEN' predicates should come before the 'IN_BLOOM_FILTER'\n+    Deque<String> dynamicIds = new ArrayDeque<>(sjValueInfo.getDynamicValueIDs());\n+    List<ExprNodeDesc> sjPredicates = new ArrayList<>();\n+    List<ExprNodeDesc> hashArgs = new ArrayList<>();\n+    for (ReduceSinkOperator rs : sjBranches) {\n+      RuntimeValuesInfo info = context.getRsToRuntimeValuesInfoMap().get(rs);\n+      assert info.getTargetColumns().size() == 1;\n+      final ExprNodeDesc targetColumn = info.getTargetColumns().get(0);\n+      TypeInfo typeInfo = targetColumn.getTypeInfo();\n+      DynamicValue minDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+      DynamicValue maxDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+\n+      List<ExprNodeDesc> betweenArgs = Arrays.asList(\n+          // Use false to not invert between result\n+          new ExprNodeConstantDesc(Boolean.FALSE),\n+          targetColumn,\n+          new ExprNodeDynamicValueDesc(minDynamic),\n+          new ExprNodeDynamicValueDesc(maxDynamic));\n+      ExprNodeDesc betweenExp =\n+          new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFBetween(), \"between\", betweenArgs);\n+      sjPredicates.add(betweenExp);\n+      hashArgs.add(targetColumn);\n+    }\n+\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", hashArgs);\n+\n+    assert dynamicIds.size() == 1 : \"There should be one column left untreated the one with the bloom filter\";\n+    DynamicValue bloomDynamic = new DynamicValue(dynamicIds.poll(), TypeInfoFactory.binaryTypeInfo);\n+    sjPredicates.add(\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFInBloomFilter(), \"in_bloom_filter\",\n+            Arrays.asList(hashExp, new ExprNodeDynamicValueDesc(bloomDynamic))));\n+    return and(sjPredicates);\n+  }\n+\n+  private static RuntimeValuesInfo createRuntimeValuesInfo(ReduceSinkOperator rs, List<ReduceSinkOperator> sjBranches,\n+      ParseContext parseContext) {\n+    List<ExprNodeDesc> valueCols = rs.getConf().getValueCols();\n+    RuntimeValuesInfo info = new RuntimeValuesInfo();\n+    TableDesc rsFinalTableDesc =\n+        PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, \"_col\"));\n+    List<String> dynamicValueIDs = new ArrayList<>();\n+    for (ExprNodeDesc rsCol : valueCols) {\n+      dynamicValueIDs.add(rs.toString() + rsCol.getExprString());\n+    }\n+\n+    info.setTableDesc(rsFinalTableDesc);\n+    info.setDynamicValueIDs(dynamicValueIDs);\n+    info.setColExprs(valueCols);\n+    List<ExprNodeDesc> targetTableExpressions = new ArrayList<>();\n+    for (ReduceSinkOperator sjBranch : sjBranches) {\n+      RuntimeValuesInfo sjInfo = parseContext.getRsToRuntimeValuesInfoMap().get(sjBranch);\n+      assert sjInfo.getTargetColumns().size() == 1;\n+      targetTableExpressions.add(sjInfo.getTargetColumns().get(0));\n+    }\n+    info.setTargetColumns(targetTableExpressions);\n+    return info;\n+  }\n+\n+  private static SelectOperator mergeSelectOps(Operator<?> parent, List<SelectOperator> selectOperators) {\n+    List<String> colNames = new ArrayList<>();\n+    List<ExprNodeDesc> colDescs = new ArrayList<>();\n+    List<ColumnInfo> columnInfos = new ArrayList<>();\n+    Map<String, ExprNodeDesc> selectColumnExprMap = new HashMap<>();\n+    for (SelectOperator sel : selectOperators) {\n+      for (ExprNodeDesc col : sel.getConf().getColList()) {\n+        String colName = HiveConf.getColumnInternalName(colDescs.size());\n+        colNames.add(colName);\n+        columnInfos.add(new ColumnInfo(colName, col.getTypeInfo(), \"\", false));\n+        colDescs.add(col);\n+        selectColumnExprMap.put(colName, col);\n+      }\n+    }\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", colDescs);\n+    String hashName = HiveConf.getColumnInternalName(colDescs.size() + 1);\n+    colNames.add(hashName);\n+    columnInfos.add(new ColumnInfo(hashName, hashExp.getTypeInfo(), \"\", false));\n+\n+    List<ExprNodeDesc> selDescs = new ArrayList<>(colDescs);", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2NjI3Ng==", "url": "https://github.com/apache/hive/pull/1325#discussion_r466366276", "bodyText": "Done", "author": "zabetak", "createdAt": "2020-08-06T12:10:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3OTcxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MDM5NA==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464780394", "bodyText": "It seems we are somehow duplicating part of the logic to create SJs below. We can create a follow-up JIRA to try to consolidate this logic with the creation path for single column SJs.", "author": "jcamachor", "createdAt": "2020-08-04T03:41:14Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQxMDY2OQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r466410669", "bodyText": "Created https://issues.apache.org/jira/browse/CALCITE-4163 for this purpose.", "author": "zabetak", "createdAt": "2020-08-06T13:26:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MDM5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQxNDM5NQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r466414395", "bodyText": "And here is the JIRA opened to the correct project https://issues.apache.org/jira/browse/HIVE-23999 :)", "author": "zabetak", "createdAt": "2020-08-06T13:32:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MDM5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MDc4Mg==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464780782", "bodyText": "It would be interesting to flatten here in case any of the args is an AND.", "author": "jcamachor", "createdAt": "2020-08-04T03:42:50Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?\n+      // parseContext.getColExprToGBMap().put(key, gb);\n+    }\n+    return parseContext;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc createSemiJoinPredicate(List<ReduceSinkOperator> sjBranches,\n+      RuntimeValuesInfo sjValueInfo, ParseContext context) {\n+    // Performance note: To speed-up evaluation 'BETWEEN' predicates should come before the 'IN_BLOOM_FILTER'\n+    Deque<String> dynamicIds = new ArrayDeque<>(sjValueInfo.getDynamicValueIDs());\n+    List<ExprNodeDesc> sjPredicates = new ArrayList<>();\n+    List<ExprNodeDesc> hashArgs = new ArrayList<>();\n+    for (ReduceSinkOperator rs : sjBranches) {\n+      RuntimeValuesInfo info = context.getRsToRuntimeValuesInfoMap().get(rs);\n+      assert info.getTargetColumns().size() == 1;\n+      final ExprNodeDesc targetColumn = info.getTargetColumns().get(0);\n+      TypeInfo typeInfo = targetColumn.getTypeInfo();\n+      DynamicValue minDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+      DynamicValue maxDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+\n+      List<ExprNodeDesc> betweenArgs = Arrays.asList(\n+          // Use false to not invert between result\n+          new ExprNodeConstantDesc(Boolean.FALSE),\n+          targetColumn,\n+          new ExprNodeDynamicValueDesc(minDynamic),\n+          new ExprNodeDynamicValueDesc(maxDynamic));\n+      ExprNodeDesc betweenExp =\n+          new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFBetween(), \"between\", betweenArgs);\n+      sjPredicates.add(betweenExp);\n+      hashArgs.add(targetColumn);\n+    }\n+\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", hashArgs);\n+\n+    assert dynamicIds.size() == 1 : \"There should be one column left untreated the one with the bloom filter\";\n+    DynamicValue bloomDynamic = new DynamicValue(dynamicIds.poll(), TypeInfoFactory.binaryTypeInfo);\n+    sjPredicates.add(\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFInBloomFilter(), \"in_bloom_filter\",\n+            Arrays.asList(hashExp, new ExprNodeDynamicValueDesc(bloomDynamic))));\n+    return and(sjPredicates);\n+  }\n+\n+  private static RuntimeValuesInfo createRuntimeValuesInfo(ReduceSinkOperator rs, List<ReduceSinkOperator> sjBranches,\n+      ParseContext parseContext) {\n+    List<ExprNodeDesc> valueCols = rs.getConf().getValueCols();\n+    RuntimeValuesInfo info = new RuntimeValuesInfo();\n+    TableDesc rsFinalTableDesc =\n+        PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, \"_col\"));\n+    List<String> dynamicValueIDs = new ArrayList<>();\n+    for (ExprNodeDesc rsCol : valueCols) {\n+      dynamicValueIDs.add(rs.toString() + rsCol.getExprString());\n+    }\n+\n+    info.setTableDesc(rsFinalTableDesc);\n+    info.setDynamicValueIDs(dynamicValueIDs);\n+    info.setColExprs(valueCols);\n+    List<ExprNodeDesc> targetTableExpressions = new ArrayList<>();\n+    for (ReduceSinkOperator sjBranch : sjBranches) {\n+      RuntimeValuesInfo sjInfo = parseContext.getRsToRuntimeValuesInfoMap().get(sjBranch);\n+      assert sjInfo.getTargetColumns().size() == 1;\n+      targetTableExpressions.add(sjInfo.getTargetColumns().get(0));\n+    }\n+    info.setTargetColumns(targetTableExpressions);\n+    return info;\n+  }\n+\n+  private static SelectOperator mergeSelectOps(Operator<?> parent, List<SelectOperator> selectOperators) {\n+    List<String> colNames = new ArrayList<>();\n+    List<ExprNodeDesc> colDescs = new ArrayList<>();\n+    List<ColumnInfo> columnInfos = new ArrayList<>();\n+    Map<String, ExprNodeDesc> selectColumnExprMap = new HashMap<>();\n+    for (SelectOperator sel : selectOperators) {\n+      for (ExprNodeDesc col : sel.getConf().getColList()) {\n+        String colName = HiveConf.getColumnInternalName(colDescs.size());\n+        colNames.add(colName);\n+        columnInfos.add(new ColumnInfo(colName, col.getTypeInfo(), \"\", false));\n+        colDescs.add(col);\n+        selectColumnExprMap.put(colName, col);\n+      }\n+    }\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", colDescs);\n+    String hashName = HiveConf.getColumnInternalName(colDescs.size() + 1);\n+    colNames.add(hashName);\n+    columnInfos.add(new ColumnInfo(hashName, hashExp.getTypeInfo(), \"\", false));\n+\n+    List<ExprNodeDesc> selDescs = new ArrayList<>(colDescs);\n+    selDescs.add(hashExp);\n+\n+    SelectDesc select = new SelectDesc(selDescs, colNames);\n+    SelectOperator selectOp =\n+        (SelectOperator) OperatorFactory.getAndMakeChild(select, new RowSchema(columnInfos), parent);\n+    selectOp.setColumnExprMap(selectColumnExprMap);\n+    return selectOp;\n+  }\n+\n+  private static ReduceSinkOperator createReduceSink(Operator<?> parentOp, NullOrdering nullOrder)\n+      throws SemanticException {\n+    List<ExprNodeDesc> valueCols = new ArrayList<>();\n+    RowSchema parentSchema = parentOp.getSchema();\n+    List<String> outColNames = new ArrayList<>();\n+    for (int i = 0; i < parentSchema.getSignature().size(); i++) {\n+      ColumnInfo colInfo = parentSchema.getSignature().get(i);\n+      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName(), \"\", false);\n+      valueCols.add(colExpr);\n+      outColNames.add(SemanticAnalyzer.getColumnInternalName(i));\n+    }\n+\n+    ReduceSinkDesc rsDesc = PlanUtils\n+        .getReduceSinkDesc(Collections.emptyList(), valueCols, outColNames, false, -1, 0, 1,\n+            AcidUtils.Operation.NOT_ACID, nullOrder);\n+    rsDesc.setColumnExprMap(Collections.emptyMap());\n+    return (ReduceSinkOperator) OperatorFactory.getAndMakeChild(rsDesc, new RowSchema(parentSchema), parentOp);\n+  }\n+\n+  private static GroupByOperator createGroupBy(SelectOperator selectOp, Operator<?> parentOp, GroupByDesc.Mode gbMode,\n+      HiveConf hiveConf) {\n+\n+    final List<ExprNodeDesc> params;\n+    final GenericUDAFEvaluator.Mode udafMode = SemanticAnalyzer.groupByDescModeToUDAFMode(gbMode, false);\n+    switch (gbMode) {\n+    case FINAL:\n+      params = createGroupByAggregationParameters((ReduceSinkOperator) parentOp);\n+      break;\n+    case HASH:\n+      params = createGroupByAggregationParameters(selectOp);\n+      break;\n+    default:\n+      throw new AssertionError(gbMode.toString() + \" is not supported\");\n+    }\n+\n+    List<AggregationDesc> gbAggs = new ArrayList<>();\n+    Deque<ExprNodeDesc> paramsCopy = new ArrayDeque<>(params);\n+    while (paramsCopy.size() > 1) {\n+      gbAggs.add(minAggregation(udafMode, paramsCopy.poll()));\n+      gbAggs.add(maxAggregation(udafMode, paramsCopy.poll()));\n+    }\n+    gbAggs.add(bloomFilterAggregation(udafMode, paramsCopy.poll(), selectOp, hiveConf));\n+    assert paramsCopy.size() == 0;\n+\n+    List<String> gbOutputNames = new ArrayList<>(gbAggs.size());\n+    List<ColumnInfo> gbColInfos = new ArrayList<>(gbAggs.size());\n+    for (int i = 0; i < params.size(); i++) {\n+      String colName = HiveConf.getColumnInternalName(i);\n+      gbOutputNames.add(colName);\n+      final TypeInfo colType;\n+      if (i == params.size() - 1) {\n+        colType = TypeInfoFactory.binaryTypeInfo; // Bloom type\n+      } else {\n+        colType = params.get(i).getTypeInfo(); // Min/Max type\n+      }\n+      gbColInfos.add(new ColumnInfo(colName, colType, \"\", false));\n+    }\n+\n+    float groupByMemoryUsage = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n+    float memoryThreshold = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n+    float minReductionHashAggr = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMINREDUCTION);\n+    GroupByDesc groupBy =\n+        new GroupByDesc(gbMode, gbOutputNames, Collections.emptyList(), gbAggs, false, groupByMemoryUsage,\n+            memoryThreshold, minReductionHashAggr, null, false, -1, false);\n+    groupBy.setColumnExprMap(Collections.emptyMap());\n+    return (GroupByOperator) OperatorFactory.getAndMakeChild(groupBy, new RowSchema(gbColInfos), parentOp);\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(SelectOperator selectOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // The first n-1 cols are used as parameters for min & max so we need two expressions\n+    for (ColumnInfo c : selectOp.getSchema().getSignature()) {\n+      String name = c.getInternalName();\n+      ExprNodeColumnDesc p = new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false));\n+      params.add(p);\n+      params.add(p);\n+    }\n+    // The last col is used as parameter for bloom so we need only one expression\n+    params.remove(params.size() - 1);\n+    return params;\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(ReduceSinkOperator reduceOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // There is a 1-1 mapping between columns and parameters for the aggregation functions min, max, bloom\n+    for (ColumnInfo c : reduceOp.getSchema().getSignature()) {\n+      String name = Utilities.ReduceField.VALUE + \".\" + c.getInternalName();\n+      params.add(new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false)));\n+    }\n+    return params;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc and(List<ExprNodeDesc> args) {\n+    return new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFOPAnd(), \"and\", args);", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2NzQ4MQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r466367481", "bodyText": "Indeed, I added the following method for this purpose: ExprNodeDescUtils#and(List<ExprNodeDesc>)", "author": "zabetak", "createdAt": "2020-08-06T12:12:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MDc4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MTE2Mw==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464781163", "bodyText": "Objects.hash(source, target);", "author": "jcamachor", "createdAt": "2020-08-04T03:44:23Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?\n+      // parseContext.getColExprToGBMap().put(key, gb);\n+    }\n+    return parseContext;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc createSemiJoinPredicate(List<ReduceSinkOperator> sjBranches,\n+      RuntimeValuesInfo sjValueInfo, ParseContext context) {\n+    // Performance note: To speed-up evaluation 'BETWEEN' predicates should come before the 'IN_BLOOM_FILTER'\n+    Deque<String> dynamicIds = new ArrayDeque<>(sjValueInfo.getDynamicValueIDs());\n+    List<ExprNodeDesc> sjPredicates = new ArrayList<>();\n+    List<ExprNodeDesc> hashArgs = new ArrayList<>();\n+    for (ReduceSinkOperator rs : sjBranches) {\n+      RuntimeValuesInfo info = context.getRsToRuntimeValuesInfoMap().get(rs);\n+      assert info.getTargetColumns().size() == 1;\n+      final ExprNodeDesc targetColumn = info.getTargetColumns().get(0);\n+      TypeInfo typeInfo = targetColumn.getTypeInfo();\n+      DynamicValue minDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+      DynamicValue maxDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+\n+      List<ExprNodeDesc> betweenArgs = Arrays.asList(\n+          // Use false to not invert between result\n+          new ExprNodeConstantDesc(Boolean.FALSE),\n+          targetColumn,\n+          new ExprNodeDynamicValueDesc(minDynamic),\n+          new ExprNodeDynamicValueDesc(maxDynamic));\n+      ExprNodeDesc betweenExp =\n+          new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFBetween(), \"between\", betweenArgs);\n+      sjPredicates.add(betweenExp);\n+      hashArgs.add(targetColumn);\n+    }\n+\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", hashArgs);\n+\n+    assert dynamicIds.size() == 1 : \"There should be one column left untreated the one with the bloom filter\";\n+    DynamicValue bloomDynamic = new DynamicValue(dynamicIds.poll(), TypeInfoFactory.binaryTypeInfo);\n+    sjPredicates.add(\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFInBloomFilter(), \"in_bloom_filter\",\n+            Arrays.asList(hashExp, new ExprNodeDynamicValueDesc(bloomDynamic))));\n+    return and(sjPredicates);\n+  }\n+\n+  private static RuntimeValuesInfo createRuntimeValuesInfo(ReduceSinkOperator rs, List<ReduceSinkOperator> sjBranches,\n+      ParseContext parseContext) {\n+    List<ExprNodeDesc> valueCols = rs.getConf().getValueCols();\n+    RuntimeValuesInfo info = new RuntimeValuesInfo();\n+    TableDesc rsFinalTableDesc =\n+        PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, \"_col\"));\n+    List<String> dynamicValueIDs = new ArrayList<>();\n+    for (ExprNodeDesc rsCol : valueCols) {\n+      dynamicValueIDs.add(rs.toString() + rsCol.getExprString());\n+    }\n+\n+    info.setTableDesc(rsFinalTableDesc);\n+    info.setDynamicValueIDs(dynamicValueIDs);\n+    info.setColExprs(valueCols);\n+    List<ExprNodeDesc> targetTableExpressions = new ArrayList<>();\n+    for (ReduceSinkOperator sjBranch : sjBranches) {\n+      RuntimeValuesInfo sjInfo = parseContext.getRsToRuntimeValuesInfoMap().get(sjBranch);\n+      assert sjInfo.getTargetColumns().size() == 1;\n+      targetTableExpressions.add(sjInfo.getTargetColumns().get(0));\n+    }\n+    info.setTargetColumns(targetTableExpressions);\n+    return info;\n+  }\n+\n+  private static SelectOperator mergeSelectOps(Operator<?> parent, List<SelectOperator> selectOperators) {\n+    List<String> colNames = new ArrayList<>();\n+    List<ExprNodeDesc> colDescs = new ArrayList<>();\n+    List<ColumnInfo> columnInfos = new ArrayList<>();\n+    Map<String, ExprNodeDesc> selectColumnExprMap = new HashMap<>();\n+    for (SelectOperator sel : selectOperators) {\n+      for (ExprNodeDesc col : sel.getConf().getColList()) {\n+        String colName = HiveConf.getColumnInternalName(colDescs.size());\n+        colNames.add(colName);\n+        columnInfos.add(new ColumnInfo(colName, col.getTypeInfo(), \"\", false));\n+        colDescs.add(col);\n+        selectColumnExprMap.put(colName, col);\n+      }\n+    }\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", colDescs);\n+    String hashName = HiveConf.getColumnInternalName(colDescs.size() + 1);\n+    colNames.add(hashName);\n+    columnInfos.add(new ColumnInfo(hashName, hashExp.getTypeInfo(), \"\", false));\n+\n+    List<ExprNodeDesc> selDescs = new ArrayList<>(colDescs);\n+    selDescs.add(hashExp);\n+\n+    SelectDesc select = new SelectDesc(selDescs, colNames);\n+    SelectOperator selectOp =\n+        (SelectOperator) OperatorFactory.getAndMakeChild(select, new RowSchema(columnInfos), parent);\n+    selectOp.setColumnExprMap(selectColumnExprMap);\n+    return selectOp;\n+  }\n+\n+  private static ReduceSinkOperator createReduceSink(Operator<?> parentOp, NullOrdering nullOrder)\n+      throws SemanticException {\n+    List<ExprNodeDesc> valueCols = new ArrayList<>();\n+    RowSchema parentSchema = parentOp.getSchema();\n+    List<String> outColNames = new ArrayList<>();\n+    for (int i = 0; i < parentSchema.getSignature().size(); i++) {\n+      ColumnInfo colInfo = parentSchema.getSignature().get(i);\n+      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName(), \"\", false);\n+      valueCols.add(colExpr);\n+      outColNames.add(SemanticAnalyzer.getColumnInternalName(i));\n+    }\n+\n+    ReduceSinkDesc rsDesc = PlanUtils\n+        .getReduceSinkDesc(Collections.emptyList(), valueCols, outColNames, false, -1, 0, 1,\n+            AcidUtils.Operation.NOT_ACID, nullOrder);\n+    rsDesc.setColumnExprMap(Collections.emptyMap());\n+    return (ReduceSinkOperator) OperatorFactory.getAndMakeChild(rsDesc, new RowSchema(parentSchema), parentOp);\n+  }\n+\n+  private static GroupByOperator createGroupBy(SelectOperator selectOp, Operator<?> parentOp, GroupByDesc.Mode gbMode,\n+      HiveConf hiveConf) {\n+\n+    final List<ExprNodeDesc> params;\n+    final GenericUDAFEvaluator.Mode udafMode = SemanticAnalyzer.groupByDescModeToUDAFMode(gbMode, false);\n+    switch (gbMode) {\n+    case FINAL:\n+      params = createGroupByAggregationParameters((ReduceSinkOperator) parentOp);\n+      break;\n+    case HASH:\n+      params = createGroupByAggregationParameters(selectOp);\n+      break;\n+    default:\n+      throw new AssertionError(gbMode.toString() + \" is not supported\");\n+    }\n+\n+    List<AggregationDesc> gbAggs = new ArrayList<>();\n+    Deque<ExprNodeDesc> paramsCopy = new ArrayDeque<>(params);\n+    while (paramsCopy.size() > 1) {\n+      gbAggs.add(minAggregation(udafMode, paramsCopy.poll()));\n+      gbAggs.add(maxAggregation(udafMode, paramsCopy.poll()));\n+    }\n+    gbAggs.add(bloomFilterAggregation(udafMode, paramsCopy.poll(), selectOp, hiveConf));\n+    assert paramsCopy.size() == 0;\n+\n+    List<String> gbOutputNames = new ArrayList<>(gbAggs.size());\n+    List<ColumnInfo> gbColInfos = new ArrayList<>(gbAggs.size());\n+    for (int i = 0; i < params.size(); i++) {\n+      String colName = HiveConf.getColumnInternalName(i);\n+      gbOutputNames.add(colName);\n+      final TypeInfo colType;\n+      if (i == params.size() - 1) {\n+        colType = TypeInfoFactory.binaryTypeInfo; // Bloom type\n+      } else {\n+        colType = params.get(i).getTypeInfo(); // Min/Max type\n+      }\n+      gbColInfos.add(new ColumnInfo(colName, colType, \"\", false));\n+    }\n+\n+    float groupByMemoryUsage = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n+    float memoryThreshold = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n+    float minReductionHashAggr = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMINREDUCTION);\n+    GroupByDesc groupBy =\n+        new GroupByDesc(gbMode, gbOutputNames, Collections.emptyList(), gbAggs, false, groupByMemoryUsage,\n+            memoryThreshold, minReductionHashAggr, null, false, -1, false);\n+    groupBy.setColumnExprMap(Collections.emptyMap());\n+    return (GroupByOperator) OperatorFactory.getAndMakeChild(groupBy, new RowSchema(gbColInfos), parentOp);\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(SelectOperator selectOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // The first n-1 cols are used as parameters for min & max so we need two expressions\n+    for (ColumnInfo c : selectOp.getSchema().getSignature()) {\n+      String name = c.getInternalName();\n+      ExprNodeColumnDesc p = new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false));\n+      params.add(p);\n+      params.add(p);\n+    }\n+    // The last col is used as parameter for bloom so we need only one expression\n+    params.remove(params.size() - 1);\n+    return params;\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(ReduceSinkOperator reduceOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // There is a 1-1 mapping between columns and parameters for the aggregation functions min, max, bloom\n+    for (ColumnInfo c : reduceOp.getSchema().getSignature()) {\n+      String name = Utilities.ReduceField.VALUE + \".\" + c.getInternalName();\n+      params.add(new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false)));\n+    }\n+    return params;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc and(List<ExprNodeDesc> args) {\n+    return new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFOPAnd(), \"and\", args);\n+  }\n+\n+  private static AggregationDesc minAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col) {\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    return new AggregationDesc(\"min\", new GenericUDAFMin.GenericUDAFMinEvaluator(), p, false, mode);\n+  }\n+\n+  private static AggregationDesc maxAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col) {\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    return new AggregationDesc(\"max\", new GenericUDAFMax.GenericUDAFMaxEvaluator(), p, false, mode);\n+  }\n+\n+  private static AggregationDesc bloomFilterAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col,\n+      SelectOperator source, HiveConf conf) {\n+    GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator bloomFilterEval =\n+        new GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator();\n+    bloomFilterEval.setSourceOperator(source);\n+    bloomFilterEval.setMaxEntries(conf.getLongVar(HiveConf.ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n+    bloomFilterEval.setMinEntries(conf.getLongVar(HiveConf.ConfVars.TEZ_MIN_BLOOM_FILTER_ENTRIES));\n+    bloomFilterEval.setFactor(conf.getFloatVar(HiveConf.ConfVars.TEZ_BLOOM_FILTER_FACTOR));\n+    // TODO Setup hints\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    AggregationDesc bloom = new AggregationDesc(\"bloom_filter\", bloomFilterEval, p, false, mode);\n+    // TODO Why do we need to set it explicitly?\n+    bloom.setGenericUDAFWritableEvaluator(bloomFilterEval);\n+    return bloom;\n+  }\n+\n+  private static final class SJSourceTarget {\n+    private final Operator<?> source;\n+    private final TableScanOperator target;\n+\n+    public SJSourceTarget(Operator<?> source, TableScanOperator target) {\n+      this.source = source;\n+      this.target = target;\n+    }\n+\n+    @Override public boolean equals(Object o) {\n+      if (this == o)\n+        return true;\n+      if (o == null || getClass() != o.getClass())\n+        return false;\n+\n+      SJSourceTarget that = (SJSourceTarget) o;\n+\n+      if (!source.equals(that.source))\n+        return false;\n+      return target.equals(that.target);\n+    }\n+\n+    @Override public int hashCode() {\n+      int result = source.hashCode();", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIwNjE3OQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r465206179", "bodyText": "I agree that the proposed change makes the code more readable, but it is also slightly less efficient since we are instantiating an Object[] array only to throw it away later on.", "author": "zabetak", "createdAt": "2020-08-04T17:17:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MTE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIxMzU0MQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r465213541", "bodyText": "Fair enough... Though I would be surprised if such overhead would be noticeable in our query compilation latency :)", "author": "jcamachor", "createdAt": "2020-08-04T17:29:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MTE2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MTcwMw==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464781703", "bodyText": "TODO can be removed?", "author": "jcamachor", "createdAt": "2020-08-04T03:46:42Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxODYyNQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r467318625", "bodyText": "Resolved in 6b4253d and created  https://issues.apache.org/jira/browse/HIVE-24016 as a follow up.", "author": "zabetak", "createdAt": "2020-08-07T22:53:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MTcwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4Mjk3Nw==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464782977", "bodyText": "To the question, I am not sure... I would assume it would be initialized correctly.", "author": "jcamachor", "createdAt": "2020-08-04T03:52:01Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?\n+      // parseContext.getColExprToGBMap().put(key, gb);\n+    }\n+    return parseContext;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc createSemiJoinPredicate(List<ReduceSinkOperator> sjBranches,\n+      RuntimeValuesInfo sjValueInfo, ParseContext context) {\n+    // Performance note: To speed-up evaluation 'BETWEEN' predicates should come before the 'IN_BLOOM_FILTER'\n+    Deque<String> dynamicIds = new ArrayDeque<>(sjValueInfo.getDynamicValueIDs());\n+    List<ExprNodeDesc> sjPredicates = new ArrayList<>();\n+    List<ExprNodeDesc> hashArgs = new ArrayList<>();\n+    for (ReduceSinkOperator rs : sjBranches) {\n+      RuntimeValuesInfo info = context.getRsToRuntimeValuesInfoMap().get(rs);\n+      assert info.getTargetColumns().size() == 1;\n+      final ExprNodeDesc targetColumn = info.getTargetColumns().get(0);\n+      TypeInfo typeInfo = targetColumn.getTypeInfo();\n+      DynamicValue minDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+      DynamicValue maxDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+\n+      List<ExprNodeDesc> betweenArgs = Arrays.asList(\n+          // Use false to not invert between result\n+          new ExprNodeConstantDesc(Boolean.FALSE),\n+          targetColumn,\n+          new ExprNodeDynamicValueDesc(minDynamic),\n+          new ExprNodeDynamicValueDesc(maxDynamic));\n+      ExprNodeDesc betweenExp =\n+          new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFBetween(), \"between\", betweenArgs);\n+      sjPredicates.add(betweenExp);\n+      hashArgs.add(targetColumn);\n+    }\n+\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", hashArgs);\n+\n+    assert dynamicIds.size() == 1 : \"There should be one column left untreated the one with the bloom filter\";\n+    DynamicValue bloomDynamic = new DynamicValue(dynamicIds.poll(), TypeInfoFactory.binaryTypeInfo);\n+    sjPredicates.add(\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFInBloomFilter(), \"in_bloom_filter\",\n+            Arrays.asList(hashExp, new ExprNodeDynamicValueDesc(bloomDynamic))));\n+    return and(sjPredicates);\n+  }\n+\n+  private static RuntimeValuesInfo createRuntimeValuesInfo(ReduceSinkOperator rs, List<ReduceSinkOperator> sjBranches,\n+      ParseContext parseContext) {\n+    List<ExprNodeDesc> valueCols = rs.getConf().getValueCols();\n+    RuntimeValuesInfo info = new RuntimeValuesInfo();\n+    TableDesc rsFinalTableDesc =\n+        PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, \"_col\"));\n+    List<String> dynamicValueIDs = new ArrayList<>();\n+    for (ExprNodeDesc rsCol : valueCols) {\n+      dynamicValueIDs.add(rs.toString() + rsCol.getExprString());\n+    }\n+\n+    info.setTableDesc(rsFinalTableDesc);\n+    info.setDynamicValueIDs(dynamicValueIDs);\n+    info.setColExprs(valueCols);\n+    List<ExprNodeDesc> targetTableExpressions = new ArrayList<>();\n+    for (ReduceSinkOperator sjBranch : sjBranches) {\n+      RuntimeValuesInfo sjInfo = parseContext.getRsToRuntimeValuesInfoMap().get(sjBranch);\n+      assert sjInfo.getTargetColumns().size() == 1;\n+      targetTableExpressions.add(sjInfo.getTargetColumns().get(0));\n+    }\n+    info.setTargetColumns(targetTableExpressions);\n+    return info;\n+  }\n+\n+  private static SelectOperator mergeSelectOps(Operator<?> parent, List<SelectOperator> selectOperators) {\n+    List<String> colNames = new ArrayList<>();\n+    List<ExprNodeDesc> colDescs = new ArrayList<>();\n+    List<ColumnInfo> columnInfos = new ArrayList<>();\n+    Map<String, ExprNodeDesc> selectColumnExprMap = new HashMap<>();\n+    for (SelectOperator sel : selectOperators) {\n+      for (ExprNodeDesc col : sel.getConf().getColList()) {\n+        String colName = HiveConf.getColumnInternalName(colDescs.size());\n+        colNames.add(colName);\n+        columnInfos.add(new ColumnInfo(colName, col.getTypeInfo(), \"\", false));\n+        colDescs.add(col);\n+        selectColumnExprMap.put(colName, col);\n+      }\n+    }\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", colDescs);\n+    String hashName = HiveConf.getColumnInternalName(colDescs.size() + 1);\n+    colNames.add(hashName);\n+    columnInfos.add(new ColumnInfo(hashName, hashExp.getTypeInfo(), \"\", false));\n+\n+    List<ExprNodeDesc> selDescs = new ArrayList<>(colDescs);\n+    selDescs.add(hashExp);\n+\n+    SelectDesc select = new SelectDesc(selDescs, colNames);\n+    SelectOperator selectOp =\n+        (SelectOperator) OperatorFactory.getAndMakeChild(select, new RowSchema(columnInfos), parent);\n+    selectOp.setColumnExprMap(selectColumnExprMap);\n+    return selectOp;\n+  }\n+\n+  private static ReduceSinkOperator createReduceSink(Operator<?> parentOp, NullOrdering nullOrder)\n+      throws SemanticException {\n+    List<ExprNodeDesc> valueCols = new ArrayList<>();\n+    RowSchema parentSchema = parentOp.getSchema();\n+    List<String> outColNames = new ArrayList<>();\n+    for (int i = 0; i < parentSchema.getSignature().size(); i++) {\n+      ColumnInfo colInfo = parentSchema.getSignature().get(i);\n+      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName(), \"\", false);\n+      valueCols.add(colExpr);\n+      outColNames.add(SemanticAnalyzer.getColumnInternalName(i));\n+    }\n+\n+    ReduceSinkDesc rsDesc = PlanUtils\n+        .getReduceSinkDesc(Collections.emptyList(), valueCols, outColNames, false, -1, 0, 1,\n+            AcidUtils.Operation.NOT_ACID, nullOrder);\n+    rsDesc.setColumnExprMap(Collections.emptyMap());\n+    return (ReduceSinkOperator) OperatorFactory.getAndMakeChild(rsDesc, new RowSchema(parentSchema), parentOp);\n+  }\n+\n+  private static GroupByOperator createGroupBy(SelectOperator selectOp, Operator<?> parentOp, GroupByDesc.Mode gbMode,\n+      HiveConf hiveConf) {\n+\n+    final List<ExprNodeDesc> params;\n+    final GenericUDAFEvaluator.Mode udafMode = SemanticAnalyzer.groupByDescModeToUDAFMode(gbMode, false);\n+    switch (gbMode) {\n+    case FINAL:\n+      params = createGroupByAggregationParameters((ReduceSinkOperator) parentOp);\n+      break;\n+    case HASH:\n+      params = createGroupByAggregationParameters(selectOp);\n+      break;\n+    default:\n+      throw new AssertionError(gbMode.toString() + \" is not supported\");\n+    }\n+\n+    List<AggregationDesc> gbAggs = new ArrayList<>();\n+    Deque<ExprNodeDesc> paramsCopy = new ArrayDeque<>(params);\n+    while (paramsCopy.size() > 1) {\n+      gbAggs.add(minAggregation(udafMode, paramsCopy.poll()));\n+      gbAggs.add(maxAggregation(udafMode, paramsCopy.poll()));\n+    }\n+    gbAggs.add(bloomFilterAggregation(udafMode, paramsCopy.poll(), selectOp, hiveConf));\n+    assert paramsCopy.size() == 0;\n+\n+    List<String> gbOutputNames = new ArrayList<>(gbAggs.size());\n+    List<ColumnInfo> gbColInfos = new ArrayList<>(gbAggs.size());\n+    for (int i = 0; i < params.size(); i++) {\n+      String colName = HiveConf.getColumnInternalName(i);\n+      gbOutputNames.add(colName);\n+      final TypeInfo colType;\n+      if (i == params.size() - 1) {\n+        colType = TypeInfoFactory.binaryTypeInfo; // Bloom type\n+      } else {\n+        colType = params.get(i).getTypeInfo(); // Min/Max type\n+      }\n+      gbColInfos.add(new ColumnInfo(colName, colType, \"\", false));\n+    }\n+\n+    float groupByMemoryUsage = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n+    float memoryThreshold = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n+    float minReductionHashAggr = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMINREDUCTION);\n+    GroupByDesc groupBy =\n+        new GroupByDesc(gbMode, gbOutputNames, Collections.emptyList(), gbAggs, false, groupByMemoryUsage,\n+            memoryThreshold, minReductionHashAggr, null, false, -1, false);\n+    groupBy.setColumnExprMap(Collections.emptyMap());\n+    return (GroupByOperator) OperatorFactory.getAndMakeChild(groupBy, new RowSchema(gbColInfos), parentOp);\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(SelectOperator selectOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // The first n-1 cols are used as parameters for min & max so we need two expressions\n+    for (ColumnInfo c : selectOp.getSchema().getSignature()) {\n+      String name = c.getInternalName();\n+      ExprNodeColumnDesc p = new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false));\n+      params.add(p);\n+      params.add(p);\n+    }\n+    // The last col is used as parameter for bloom so we need only one expression\n+    params.remove(params.size() - 1);\n+    return params;\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(ReduceSinkOperator reduceOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // There is a 1-1 mapping between columns and parameters for the aggregation functions min, max, bloom\n+    for (ColumnInfo c : reduceOp.getSchema().getSignature()) {\n+      String name = Utilities.ReduceField.VALUE + \".\" + c.getInternalName();\n+      params.add(new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false)));\n+    }\n+    return params;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc and(List<ExprNodeDesc> args) {\n+    return new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFOPAnd(), \"and\", args);\n+  }\n+\n+  private static AggregationDesc minAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col) {\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    return new AggregationDesc(\"min\", new GenericUDAFMin.GenericUDAFMinEvaluator(), p, false, mode);\n+  }\n+\n+  private static AggregationDesc maxAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col) {\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    return new AggregationDesc(\"max\", new GenericUDAFMax.GenericUDAFMaxEvaluator(), p, false, mode);\n+  }\n+\n+  private static AggregationDesc bloomFilterAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col,\n+      SelectOperator source, HiveConf conf) {\n+    GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator bloomFilterEval =\n+        new GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator();\n+    bloomFilterEval.setSourceOperator(source);\n+    bloomFilterEval.setMaxEntries(conf.getLongVar(HiveConf.ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n+    bloomFilterEval.setMinEntries(conf.getLongVar(HiveConf.ConfVars.TEZ_MIN_BLOOM_FILTER_ENTRIES));\n+    bloomFilterEval.setFactor(conf.getFloatVar(HiveConf.ConfVars.TEZ_BLOOM_FILTER_FACTOR));\n+    // TODO Setup hints\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    AggregationDesc bloom = new AggregationDesc(\"bloom_filter\", bloomFilterEval, p, false, mode);\n+    // TODO Why do we need to set it explicitly?", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIwNzY1OQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r465207659", "bodyText": "This was mostly a reminder for me to log a JIRA. There is a reason to do so but we shouldn't be forced to pass it both at the constructor and also via the setter.", "author": "zabetak", "createdAt": "2020-08-04T17:19:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4Mjk3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxODk3Ng==", "url": "https://github.com/apache/hive/pull/1325#discussion_r467318976", "bodyText": "Removed in bf5cad3 and created https://issues.apache.org/jira/browse/HIVE-24018 if I ever feel like investigating further.", "author": "zabetak", "createdAt": "2020-08-07T22:54:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4Mjk3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MzE3OQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464783179", "bodyText": "What's the current behavior in the presence of hints? It seems we do not propagate them... Should we create a follow-up JIRA to keep track?", "author": "jcamachor", "createdAt": "2020-08-04T03:53:04Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?\n+      // parseContext.getColExprToGBMap().put(key, gb);\n+    }\n+    return parseContext;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc createSemiJoinPredicate(List<ReduceSinkOperator> sjBranches,\n+      RuntimeValuesInfo sjValueInfo, ParseContext context) {\n+    // Performance note: To speed-up evaluation 'BETWEEN' predicates should come before the 'IN_BLOOM_FILTER'\n+    Deque<String> dynamicIds = new ArrayDeque<>(sjValueInfo.getDynamicValueIDs());\n+    List<ExprNodeDesc> sjPredicates = new ArrayList<>();\n+    List<ExprNodeDesc> hashArgs = new ArrayList<>();\n+    for (ReduceSinkOperator rs : sjBranches) {\n+      RuntimeValuesInfo info = context.getRsToRuntimeValuesInfoMap().get(rs);\n+      assert info.getTargetColumns().size() == 1;\n+      final ExprNodeDesc targetColumn = info.getTargetColumns().get(0);\n+      TypeInfo typeInfo = targetColumn.getTypeInfo();\n+      DynamicValue minDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+      DynamicValue maxDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+\n+      List<ExprNodeDesc> betweenArgs = Arrays.asList(\n+          // Use false to not invert between result\n+          new ExprNodeConstantDesc(Boolean.FALSE),\n+          targetColumn,\n+          new ExprNodeDynamicValueDesc(minDynamic),\n+          new ExprNodeDynamicValueDesc(maxDynamic));\n+      ExprNodeDesc betweenExp =\n+          new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFBetween(), \"between\", betweenArgs);\n+      sjPredicates.add(betweenExp);\n+      hashArgs.add(targetColumn);\n+    }\n+\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", hashArgs);\n+\n+    assert dynamicIds.size() == 1 : \"There should be one column left untreated the one with the bloom filter\";\n+    DynamicValue bloomDynamic = new DynamicValue(dynamicIds.poll(), TypeInfoFactory.binaryTypeInfo);\n+    sjPredicates.add(\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFInBloomFilter(), \"in_bloom_filter\",\n+            Arrays.asList(hashExp, new ExprNodeDynamicValueDesc(bloomDynamic))));\n+    return and(sjPredicates);\n+  }\n+\n+  private static RuntimeValuesInfo createRuntimeValuesInfo(ReduceSinkOperator rs, List<ReduceSinkOperator> sjBranches,\n+      ParseContext parseContext) {\n+    List<ExprNodeDesc> valueCols = rs.getConf().getValueCols();\n+    RuntimeValuesInfo info = new RuntimeValuesInfo();\n+    TableDesc rsFinalTableDesc =\n+        PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, \"_col\"));\n+    List<String> dynamicValueIDs = new ArrayList<>();\n+    for (ExprNodeDesc rsCol : valueCols) {\n+      dynamicValueIDs.add(rs.toString() + rsCol.getExprString());\n+    }\n+\n+    info.setTableDesc(rsFinalTableDesc);\n+    info.setDynamicValueIDs(dynamicValueIDs);\n+    info.setColExprs(valueCols);\n+    List<ExprNodeDesc> targetTableExpressions = new ArrayList<>();\n+    for (ReduceSinkOperator sjBranch : sjBranches) {\n+      RuntimeValuesInfo sjInfo = parseContext.getRsToRuntimeValuesInfoMap().get(sjBranch);\n+      assert sjInfo.getTargetColumns().size() == 1;\n+      targetTableExpressions.add(sjInfo.getTargetColumns().get(0));\n+    }\n+    info.setTargetColumns(targetTableExpressions);\n+    return info;\n+  }\n+\n+  private static SelectOperator mergeSelectOps(Operator<?> parent, List<SelectOperator> selectOperators) {\n+    List<String> colNames = new ArrayList<>();\n+    List<ExprNodeDesc> colDescs = new ArrayList<>();\n+    List<ColumnInfo> columnInfos = new ArrayList<>();\n+    Map<String, ExprNodeDesc> selectColumnExprMap = new HashMap<>();\n+    for (SelectOperator sel : selectOperators) {\n+      for (ExprNodeDesc col : sel.getConf().getColList()) {\n+        String colName = HiveConf.getColumnInternalName(colDescs.size());\n+        colNames.add(colName);\n+        columnInfos.add(new ColumnInfo(colName, col.getTypeInfo(), \"\", false));\n+        colDescs.add(col);\n+        selectColumnExprMap.put(colName, col);\n+      }\n+    }\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", colDescs);\n+    String hashName = HiveConf.getColumnInternalName(colDescs.size() + 1);\n+    colNames.add(hashName);\n+    columnInfos.add(new ColumnInfo(hashName, hashExp.getTypeInfo(), \"\", false));\n+\n+    List<ExprNodeDesc> selDescs = new ArrayList<>(colDescs);\n+    selDescs.add(hashExp);\n+\n+    SelectDesc select = new SelectDesc(selDescs, colNames);\n+    SelectOperator selectOp =\n+        (SelectOperator) OperatorFactory.getAndMakeChild(select, new RowSchema(columnInfos), parent);\n+    selectOp.setColumnExprMap(selectColumnExprMap);\n+    return selectOp;\n+  }\n+\n+  private static ReduceSinkOperator createReduceSink(Operator<?> parentOp, NullOrdering nullOrder)\n+      throws SemanticException {\n+    List<ExprNodeDesc> valueCols = new ArrayList<>();\n+    RowSchema parentSchema = parentOp.getSchema();\n+    List<String> outColNames = new ArrayList<>();\n+    for (int i = 0; i < parentSchema.getSignature().size(); i++) {\n+      ColumnInfo colInfo = parentSchema.getSignature().get(i);\n+      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName(), \"\", false);\n+      valueCols.add(colExpr);\n+      outColNames.add(SemanticAnalyzer.getColumnInternalName(i));\n+    }\n+\n+    ReduceSinkDesc rsDesc = PlanUtils\n+        .getReduceSinkDesc(Collections.emptyList(), valueCols, outColNames, false, -1, 0, 1,\n+            AcidUtils.Operation.NOT_ACID, nullOrder);\n+    rsDesc.setColumnExprMap(Collections.emptyMap());\n+    return (ReduceSinkOperator) OperatorFactory.getAndMakeChild(rsDesc, new RowSchema(parentSchema), parentOp);\n+  }\n+\n+  private static GroupByOperator createGroupBy(SelectOperator selectOp, Operator<?> parentOp, GroupByDesc.Mode gbMode,\n+      HiveConf hiveConf) {\n+\n+    final List<ExprNodeDesc> params;\n+    final GenericUDAFEvaluator.Mode udafMode = SemanticAnalyzer.groupByDescModeToUDAFMode(gbMode, false);\n+    switch (gbMode) {\n+    case FINAL:\n+      params = createGroupByAggregationParameters((ReduceSinkOperator) parentOp);\n+      break;\n+    case HASH:\n+      params = createGroupByAggregationParameters(selectOp);\n+      break;\n+    default:\n+      throw new AssertionError(gbMode.toString() + \" is not supported\");\n+    }\n+\n+    List<AggregationDesc> gbAggs = new ArrayList<>();\n+    Deque<ExprNodeDesc> paramsCopy = new ArrayDeque<>(params);\n+    while (paramsCopy.size() > 1) {\n+      gbAggs.add(minAggregation(udafMode, paramsCopy.poll()));\n+      gbAggs.add(maxAggregation(udafMode, paramsCopy.poll()));\n+    }\n+    gbAggs.add(bloomFilterAggregation(udafMode, paramsCopy.poll(), selectOp, hiveConf));\n+    assert paramsCopy.size() == 0;\n+\n+    List<String> gbOutputNames = new ArrayList<>(gbAggs.size());\n+    List<ColumnInfo> gbColInfos = new ArrayList<>(gbAggs.size());\n+    for (int i = 0; i < params.size(); i++) {\n+      String colName = HiveConf.getColumnInternalName(i);\n+      gbOutputNames.add(colName);\n+      final TypeInfo colType;\n+      if (i == params.size() - 1) {\n+        colType = TypeInfoFactory.binaryTypeInfo; // Bloom type\n+      } else {\n+        colType = params.get(i).getTypeInfo(); // Min/Max type\n+      }\n+      gbColInfos.add(new ColumnInfo(colName, colType, \"\", false));\n+    }\n+\n+    float groupByMemoryUsage = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n+    float memoryThreshold = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n+    float minReductionHashAggr = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMINREDUCTION);\n+    GroupByDesc groupBy =\n+        new GroupByDesc(gbMode, gbOutputNames, Collections.emptyList(), gbAggs, false, groupByMemoryUsage,\n+            memoryThreshold, minReductionHashAggr, null, false, -1, false);\n+    groupBy.setColumnExprMap(Collections.emptyMap());\n+    return (GroupByOperator) OperatorFactory.getAndMakeChild(groupBy, new RowSchema(gbColInfos), parentOp);\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(SelectOperator selectOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // The first n-1 cols are used as parameters for min & max so we need two expressions\n+    for (ColumnInfo c : selectOp.getSchema().getSignature()) {\n+      String name = c.getInternalName();\n+      ExprNodeColumnDesc p = new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false));\n+      params.add(p);\n+      params.add(p);\n+    }\n+    // The last col is used as parameter for bloom so we need only one expression\n+    params.remove(params.size() - 1);\n+    return params;\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(ReduceSinkOperator reduceOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // There is a 1-1 mapping between columns and parameters for the aggregation functions min, max, bloom\n+    for (ColumnInfo c : reduceOp.getSchema().getSignature()) {\n+      String name = Utilities.ReduceField.VALUE + \".\" + c.getInternalName();\n+      params.add(new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false)));\n+    }\n+    return params;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc and(List<ExprNodeDesc> args) {\n+    return new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFOPAnd(), \"and\", args);\n+  }\n+\n+  private static AggregationDesc minAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col) {\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    return new AggregationDesc(\"min\", new GenericUDAFMin.GenericUDAFMinEvaluator(), p, false, mode);\n+  }\n+\n+  private static AggregationDesc maxAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col) {\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    return new AggregationDesc(\"max\", new GenericUDAFMax.GenericUDAFMaxEvaluator(), p, false, mode);\n+  }\n+\n+  private static AggregationDesc bloomFilterAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col,\n+      SelectOperator source, HiveConf conf) {\n+    GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator bloomFilterEval =\n+        new GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator();\n+    bloomFilterEval.setSourceOperator(source);\n+    bloomFilterEval.setMaxEntries(conf.getLongVar(HiveConf.ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n+    bloomFilterEval.setMinEntries(conf.getLongVar(HiveConf.ConfVars.TEZ_MIN_BLOOM_FILTER_ENTRIES));\n+    bloomFilterEval.setFactor(conf.getFloatVar(HiveConf.ConfVars.TEZ_BLOOM_FILTER_FACTOR));\n+    // TODO Setup hints", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxOTIxMg==", "url": "https://github.com/apache/hive/pull/1325#discussion_r467319212", "bodyText": "Fixed in de66827.", "author": "zabetak", "createdAt": "2020-08-07T22:55:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MzE3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4NzA4OQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464787089", "bodyText": "Shouldn't this be the min (selectivity initialized to 1)? The existing code was overly complex with the dichotomy between selectivity and benefit. However, I think we are trying to get the lowest selectivity so we can obtain the highest benefit? Or am I getting confused?", "author": "jcamachor", "createdAt": "2020-08-04T04:09:00Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java", "diffHunk": "@@ -1737,35 +1737,46 @@ private static double getBloomFilterBenefit(\n       }\n     }\n \n-    // Selectivity: key cardinality of semijoin / domain cardinality\n-    // Benefit (rows filtered from ts): (1 - selectivity) * # ts rows\n-    double selectivity = selKeyCardinality / (double) keyDomainCardinality;\n-    selectivity = Math.min(selectivity, 1);\n-    benefit = tsRows * (1 - selectivity);\n-\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"BloomFilter benefit for \" + selCol + \" to \" + tsCol\n-          + \", selKeyCardinality=\" + selKeyCardinality\n-          + \", tsKeyCardinality=\" + tsKeyCardinality\n-          + \", tsRows=\" + tsRows\n-          + \", keyDomainCardinality=\" + keyDomainCardinality);\n-      LOG.debug(\"SemiJoin key selectivity=\" + selectivity\n-          + \", benefit=\" + benefit);\n+      LOG.debug(\"BloomFilter selectivity for \" + selCol + \" to \" + tsCol + \", selKeyCardinality=\" + selKeyCardinality\n+          + \", tsKeyCardinality=\" + tsKeyCardinality + \", keyDomainCardinality=\" + keyDomainCardinality);\n     }\n+    // Selectivity: key cardinality of semijoin / domain cardinality\n+    return selKeyCardinality / (double) keyDomainCardinality;\n+  }\n \n-    return benefit;\n+  private static double getBloomFilterBenefit(\n+      SelectOperator sel, List<ExprNodeDesc> selExpr,\n+      Statistics filStats, List<ExprNodeDesc> tsExpr) {\n+    if (sel.getStatistics() == null || filStats == null) {\n+      LOG.debug(\"No stats available to compute BloomFilter benefit\");\n+      return -1;\n+    }\n+    double selectivity = 0.0;\n+    for (int i = 0; i < tsExpr.size(); i++) {\n+      selectivity = Math.max(selectivity, getBloomFilterSelectivity(sel, selExpr.get(i), filStats, tsExpr.get(i)));", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2ODA2Nw==", "url": "https://github.com/apache/hive/pull/1325#discussion_r466368067", "bodyText": "You are right, I was the one confused. I applied the change along with some doc.", "author": "zabetak", "createdAt": "2020-08-06T12:13:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4NzA4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4OTA3MQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464789071", "bodyText": "TODO?", "author": "jcamachor", "createdAt": "2020-08-04T04:16:35Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java", "diffHunk": "@@ -2054,7 +2067,8 @@ private void markSemiJoinForDPP(OptimizeTezProcContext procCtx)\n               // Lookup nDVs on TS side.\n               RuntimeValuesInfo rti = procCtx.parseContext\n                       .getRsToRuntimeValuesInfoMap().get(rs);\n-              ExprNodeDesc tsExpr = rti.getTsColExpr();\n+              // TODO Adapt for multi column semi-joins.", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQwNDc3NQ==", "url": "https://github.com/apache/hive/pull/1325#discussion_r466404775", "bodyText": "I meant to handle this as part of HIVE-23934. I added the reference to the comment.", "author": "zabetak", "createdAt": "2020-08-06T13:18:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4OTA3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4OTI1Mg==", "url": "https://github.com/apache/hive/pull/1325#discussion_r464789252", "bodyText": "This does not seem to distinguish between single column and multi column. Could we clarify in the comment?", "author": "jcamachor", "createdAt": "2020-08-04T04:17:18Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java", "diffHunk": "@@ -1887,13 +1898,14 @@ private void removeSemijoinOptimizationByBenefit(OptimizeTezProcContext procCtx)\n         // Check the ndv/rows from the SEL vs the destination tablescan the semijoin opt is going to.\n         TableScanOperator ts = sjInfo.getTsOp();\n         RuntimeValuesInfo rti = procCtx.parseContext.getRsToRuntimeValuesInfoMap().get(rs);\n-        ExprNodeDesc tsExpr = rti.getTsColExpr();\n-        // In the SEL operator of the semijoin branch, there should be only one column in the operator\n-        ExprNodeDesc selExpr = sel.getConf().getColList().get(0);\n+        List<ExprNodeDesc> targetColumns = rti.getTargetColumns();\n+        // In multi column semijoin branches the last column of the SEL operator is hash(c1, c2, ..., cn)\n+        // so we shouldn't consider it.\n+        List<ExprNodeDesc> sourceColumns = sel.getConf().getColList().subList(0, targetColumns.size());", "originalCommit": "edffed725aa5cb794578688d353cd015bc54cb6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQwNDI1Ng==", "url": "https://github.com/apache/hive/pull/1325#discussion_r466404256", "bodyText": "Done", "author": "zabetak", "createdAt": "2020-08-06T13:17:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4OTI1Mg=="}], "type": "inlineReview"}, {"oid": "0c36444b9c97507342376c9d34a864fdbf27975b", "url": "https://github.com/apache/hive/commit/0c36444b9c97507342376c9d34a864fdbf27975b", "message": "HIVE-21196: Add missing braces in single line if statements", "committedDate": "2020-08-04T12:57:55Z", "type": "commit"}, {"oid": "31e70a1c0e152d08267ae462b6134a583c40d817", "url": "https://github.com/apache/hive/commit/31e70a1c0e152d08267ae462b6134a583c40d817", "message": "HIVE-21196: Add documentation in SemiJoinReductionMerge transformation", "committedDate": "2020-08-04T16:42:43Z", "type": "commit"}, {"oid": "5421ee2b79e52946b1dd5d708f476ff23d85237d", "url": "https://github.com/apache/hive/commit/5421ee2b79e52946b1dd5d708f476ff23d85237d", "message": "HIVE-21196: Fix typo (sjBrances -> sjBranches)", "committedDate": "2020-08-04T16:55:56Z", "type": "commit"}, {"oid": "522c1d53e2630b51f922776ebc02cbe2778adf25", "url": "https://github.com/apache/hive/commit/522c1d53e2630b51f922776ebc02cbe2778adf25", "message": "HIVE-21196: Ensure select operators in mergeSelectOps have only one column", "committedDate": "2020-08-04T17:04:12Z", "type": "commit"}, {"oid": "921e4e0c8e1f29e254877f3621a1d2b3f161b810", "url": "https://github.com/apache/hive/commit/921e4e0c8e1f29e254877f3621a1d2b3f161b810", "message": "HIVE-21196: Extra doc about columns in the merged select operator", "committedDate": "2020-08-04T17:09:30Z", "type": "commit"}, {"oid": "216d858e871cde9dbf0da4b96c13f3efc78ecfb5", "url": "https://github.com/apache/hive/commit/216d858e871cde9dbf0da4b96c13f3efc78ecfb5", "message": "HIVE-21196: Remove unused import", "committedDate": "2020-08-05T08:14:46Z", "type": "commit"}, {"oid": "b1043e5f5add779269b637bb79e0b865852b1375", "url": "https://github.com/apache/hive/commit/b1043e5f5add779269b637bb79e0b865852b1375", "message": "HIVE-21196: Flatten conjunctions (AND) if possible", "committedDate": "2020-08-05T11:09:59Z", "type": "commit"}, {"oid": "c037d81bf2cd5d90f7f9bc91bd21c2d814b9adbe", "url": "https://github.com/apache/hive/commit/c037d81bf2cd5d90f7f9bc91bd21c2d814b9adbe", "message": "HIVE-21196: Compute bloom benefit using min selectivity of columns", "committedDate": "2020-08-06T09:34:08Z", "type": "commit"}, {"oid": "c9f9112d0802906dce7442f3d4c01535a584af11", "url": "https://github.com/apache/hive/commit/c9f9112d0802906dce7442f3d4c01535a584af11", "message": "HIVE-21196: Update plans in Perf cli drivers\n\nWith the new estimation in the benefit of the bloom the multi-col\nsemijoin reduction kicks in also in query5.\n\nInterestingly there are also some important changes in query24. Due to\nthe new estimation a multi-col semijoin reduction on the same columns is\nused in two scan operators (on the same table). This also triggers the\nshared work optimizer that is able to merge the semijoin branches and\nthe respective scan operators.", "committedDate": "2020-08-06T09:40:13Z", "type": "commit"}, {"oid": "7b3798af1141ebcbae8502f0410269b640787533", "url": "https://github.com/apache/hive/commit/7b3798af1141ebcbae8502f0410269b640787533", "message": "HIVE-21196: Update few plans in MiniLLap drivers due to AND flattening", "committedDate": "2020-08-06T09:54:01Z", "type": "commit"}, {"oid": "e23569c8d35573bd784d3e0cb14fdbcf3c341b40", "url": "https://github.com/apache/hive/commit/e23569c8d35573bd784d3e0cb14fdbcf3c341b40", "message": "Merge branch 'master' into HIVE-21196", "committedDate": "2020-08-06T10:04:17Z", "type": "commit"}, {"oid": "99f046db8ba95afac3677fa5c1abf7aa375ad973", "url": "https://github.com/apache/hive/commit/99f046db8ba95afac3677fa5c1abf7aa375ad973", "message": "HIVE-21196: Update dynamic_semijoin_reduction_2.q in MiniLlap driver\n\nChanges only due to semijoin reduction merge transformation merging\ncombining three sj reducers to one.", "committedDate": "2020-08-06T10:35:29Z", "type": "commit"}, {"oid": "f14c4b6197c8decb801c070bf9e395965662d586", "url": "https://github.com/apache/hive/commit/f14c4b6197c8decb801c070bf9e395965662d586", "message": "HIVE-21196: Update plans in Perf cli drivers", "committedDate": "2020-08-06T10:55:50Z", "type": "commit"}, {"oid": "403160445b331505ad7e81354026afaabb8ca202", "url": "https://github.com/apache/hive/commit/403160445b331505ad7e81354026afaabb8ca202", "message": "HIVE-21196: Replace certain assertions in SemiJoinReductionMerge with exceptions\n\nAvoid the case where assertions are disabled in production", "committedDate": "2020-08-06T12:06:42Z", "type": "commit"}, {"oid": "409f49ac29f23bf7bd75cb44332be843868d1d66", "url": "https://github.com/apache/hive/commit/409f49ac29f23bf7bd75cb44332be843868d1d66", "message": "HIVE-21196: Update comment in removeSemijoinOptimizationByBenefit", "committedDate": "2020-08-06T13:16:17Z", "type": "commit"}, {"oid": "47039f54ab4c772ba0419cd20293321720a36e41", "url": "https://github.com/apache/hive/commit/47039f54ab4c772ba0419cd20293321720a36e41", "message": "HIVE-21196: Use OperatorUtils#ancestor to obtain SEL operator in semijoins", "committedDate": "2020-08-06T13:16:17Z", "type": "commit"}, {"oid": "a794cf9248338c1d6b65a7828238f8d64f5dab2a", "url": "https://github.com/apache/hive/commit/a794cf9248338c1d6b65a7828238f8d64f5dab2a", "message": "HIVE-21196: Add HIVE-23934 reference in TODO inside markSemiJoinForDPP", "committedDate": "2020-08-06T13:16:17Z", "type": "commit"}, {"oid": "de66827208461ca01228d4bf97d5c5a217f4a636", "url": "https://github.com/apache/hive/commit/de66827208461ca01228d4bf97d5c5a217f4a636", "message": "HIVE-21196: Exploit bloom entries hint from single-col semijoin reducers\n\n1. Add a FunctionUtils#extractEvaluators utility function to\nconveniently obtain a class of UDAFE evaluators from the provided\naggregations.\n2. Add a test in dynamic_semijoin_reduction_multicol.q to verify that\nbloom filter's entries hint is propagated correctly when multi-column\nsemijoin reducers are in use.", "committedDate": "2020-08-07T09:58:29Z", "type": "commit"}, {"oid": "6b4253d8c888179b4408156553dc606679e1f605", "url": "https://github.com/apache/hive/commit/6b4253d8c888179b4408156553dc606679e1f605", "message": "HIVE-21196: Remove TODO for getColExprToGBMap() in SemiJoinReductionMerge\n\nThe map is used for finding sharing opportunities in the plan\nconstructing single column bloom filters. For multi column bloom filters\nit cannot be applied directly. We will explore sharing opportunities for\nmulti column bloom filter in HIVE-24016.", "committedDate": "2020-08-07T12:55:05Z", "type": "commit"}, {"oid": "f387d78447d9d68e2df7b20fdaab7d0c1bac905e", "url": "https://github.com/apache/hive/commit/f387d78447d9d68e2df7b20fdaab7d0c1bac905e", "message": "HIVE-21196: Use LinkedHashMap for keeping semijoin information in ParseContext", "committedDate": "2020-08-07T15:48:17Z", "type": "commit"}, {"oid": "827be9ea067806e9c703f6c1c756ecd89465e104", "url": "https://github.com/apache/hive/commit/827be9ea067806e9c703f6c1c756ecd89465e104", "message": "HIVE-21196: Extract creation of semijoin merge candidates to method\n\nOnly refactoring can be dropped if necessary.", "committedDate": "2020-08-07T18:16:35Z", "type": "commit"}, {"oid": "bf5cad3aa0861eef811565214231e5dec87cd3e6", "url": "https://github.com/apache/hive/commit/bf5cad3aa0861eef811565214231e5dec87cd3e6", "message": "HIVE-21196: Replace TODO about bloom filter evaluator with comment", "committedDate": "2020-08-07T22:28:29Z", "type": "commit"}, {"oid": "2394c4bab74e2364827ce47fe001f52067df7881", "url": "https://github.com/apache/hive/commit/2394c4bab74e2364827ce47fe001f52067df7881", "message": "HIVE-21196: Update query plans in Perf and MiniLlap drivers", "committedDate": "2020-08-07T22:49:56Z", "type": "commit"}, {"oid": "cec37ba17a259aae6a6f525d5b790d52a0561a1f", "url": "https://github.com/apache/hive/commit/cec37ba17a259aae6a6f525d5b790d52a0561a1f", "message": "Merge branch 'master' into HIVE-21196", "committedDate": "2020-08-08T06:47:00Z", "type": "commit"}, {"oid": "7464cb3d3d2dc19a498cbef41bde09c84ba7cb65", "url": "https://github.com/apache/hive/commit/7464cb3d3d2dc19a498cbef41bde09c84ba7cb65", "message": "Update query plans in MiniLlap and Perf drivers", "committedDate": "2020-08-09T22:03:18Z", "type": "commit"}]}