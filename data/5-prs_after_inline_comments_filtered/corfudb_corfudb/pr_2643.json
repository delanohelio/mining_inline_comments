{"pr_number": 2643, "pr_title": "Config the message size for both snapshot full sync and delta sync.", "pr_createdAt": "2020-07-20T20:59:24Z", "pr_url": "https://github.com/CorfuDB/CorfuDB/pull/2643", "timeline": [{"oid": "ac05eedb97815a166243f74c532ac24acfc5e11a", "url": "https://github.com/CorfuDB/CorfuDB/commit/ac05eedb97815a166243f74c532ac24acfc5e11a", "message": "Make snapshot message size configurable.\n   * Make the message size configurable.\n   * While reading a SMR entry, setup its size.\n   * While processing an opaque entry, get its size.\n   * Send snapshot message with the maxMessageSize.", "committedDate": "2020-07-20T20:24:28Z", "type": "commit"}, {"oid": "18ed759aaf2de8917a1942c49171ca75628cf5e3", "url": "https://github.com/CorfuDB/CorfuDB/commit/18ed759aaf2de8917a1942c49171ca75628cf5e3", "message": "Some cleanup:\n   * The function getSerializedSize works only for an opaque smr entry.\n   * While passing buffer, truncate it to the proper size.", "committedDate": "2020-07-20T20:26:23Z", "type": "commit"}, {"oid": "cad850979100fa3d5b2b949aeebf820e32e3aac7", "url": "https://github.com/CorfuDB/CorfuDB/commit/cad850979100fa3d5b2b949aeebf820e32e3aac7", "message": "Support the configuration for LogEntryDataMessage size.\n  * Add options for input of data message size.\n  * Add unit tests to verify the data message size is correctly set.", "committedDate": "2020-07-20T20:26:50Z", "type": "commit"}, {"oid": "fdc9adda2a903080450bd42f219075d9f92e158b", "url": "https://github.com/CorfuDB/CorfuDB/commit/fdc9adda2a903080450bd42f219075d9f92e158b", "message": "Rebase and merge the change.", "committedDate": "2020-07-20T20:57:25Z", "type": "commit"}, {"oid": "6a2d16f5a512623999593234d0b9d1ddf9d8a482", "url": "https://github.com/CorfuDB/CorfuDB/commit/6a2d16f5a512623999593234d0b9d1ddf9d8a482", "message": "Some cleanup.", "committedDate": "2020-07-20T22:00:57Z", "type": "commit"}, {"oid": "9d1117d719dd92f90b0c17948a051bf1521d89f6", "url": "https://github.com/CorfuDB/CorfuDB/commit/9d1117d719dd92f90b0c17948a051bf1521d89f6", "message": "Merge branch 'log-replication-master' into xq/0720_msg_size_01", "committedDate": "2020-07-20T22:43:56Z", "type": "commit"}, {"oid": "99e78c613f007d81a6b4d3d64b516da21e504e13", "url": "https://github.com/CorfuDB/CorfuDB/commit/99e78c613f007d81a6b4d3d64b516da21e504e13", "message": "Fix issues with SMREntry size.", "committedDate": "2020-07-20T23:40:13Z", "type": "commit"}, {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "url": "https://github.com/CorfuDB/CorfuDB/commit/15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "message": "Change some logging level from info to debug.", "committedDate": "2020-07-21T00:52:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMxODQ2MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458318461", "bodyText": "if this is only related to Log Replication, can we rename it to getLogReplication....()?", "author": "pankti-m", "createdAt": "2020-07-21T18:54:51Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java", "diffHunk": "@@ -216,16 +219,30 @@ public String getPluginConfigFilePath() {\n         return pluginConfigFilePath == null ? PLUGIN_CONFIG_FILE_PATH : pluginConfigFilePath;\n     }\n \n-    public int getSnapshotSyncBatchSize() {\n-        Integer snapshotSyncBatchSize = getServerConfig(Integer.class, \"--snapshot-batch\");\n-        return snapshotSyncBatchSize == null ? SnapshotSender.DEFAULT_SNAPSHOT_BATCH_SIZE : snapshotSyncBatchSize;\n+    /**\n+     * Get the max number of messages can be sent over per batch.\n+     * @return\n+     */\n+    public int getMaxNumMsgPerBatch() {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMDAyOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458320029", "bodyText": "can we rename to DEFAULT_TIMEOUT_MS?", "author": "pankti-m", "createdAt": "2020-07-21T18:57:35Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java", "diffHunk": "@@ -13,15 +13,40 @@\n @Data\n public class LogReplicationConfig {\n \n+    // Log Replication message timeout time in milliseconds.\n+    public static final int DEFAULT_TIMEOUT = 5000;", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMTg4OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458321888", "bodyText": "can we rename this to max_message_per_snapshot or snapshot_batch_num_messages?", "author": "pankti-m", "createdAt": "2020-07-21T19:00:55Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java", "diffHunk": "@@ -44,7 +44,8 @@\n                     + \"\\n\"\n                     + \"Usage:\\n\"\n                     + \"\\tlog_replication_server (-l <path>|-m) [-nsN] [-a <address>|-q <interface-name>] \"\n-                    + \"[--snapshot-batch=<batch-size>]\"\n+                    + \"[--snapshot-batch=<batch-size>] \"", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMjI3OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458322279", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T19:01:42Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java", "diffHunk": "@@ -171,14 +172,20 @@\n                     + \" --metrics-port=<metrics_port>                                            \"\n                     + \"              Metrics provider server port [default: 9999].\\n             \"\n                     + \" --snapshot-batch=<batch-size>                                            \"\n-                    + \"              Snapshot (Full) Sync batch size (number of entries)\\n       \"\n+                    + \"              Snapshot (Full) Sync batch size.\\n                          \"\n+                    + \"              The max number of messages per batch)\\n                      \"\n+                    + \"                                                                          \"\n+                    + \" --max-data-message-size=<msg-size>                                       \"\n+                    + \"              The max size of replication data message in bytes.\\n   \"\n+                    + \"                                                                          \"\n                     + \" --lock-lease=<lease-duration>                                            \"\n                     + \"              Lock lease duration in seconds\\n                            \"\n                     + \" -h, --help                                                               \"\n                     + \"              Show this screen\\n\"\n                     + \" --version                                                                \"\n                     + \"              Show version\\n\";\n \n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMjkyNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458322927", "bodyText": "why did we reduce it?", "author": "pankti-m", "createdAt": "2020-07-21T19:02:53Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/plugins/DefaultClusterConfig.java", "diffHunk": "@@ -45,7 +45,7 @@ private DefaultClusterConfig() {\n     private static String standbyLogReplicationPort = \"9020\";\n \n     @Getter\n-    private static int logSenderBufferSize = 20;\n+    private static int logSenderBufferSize = 1;", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2NTIxMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458465211", "bodyText": "As we increase the size of the message to 64MB, there is no point to have a big buffer.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:31:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMjkyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMzkzMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458323932", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T19:04:38Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,25 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyODMzNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458328335", "bodyText": "the comment says timestamp but we are checking on version.  Is it intended?", "author": "pankti-m", "createdAt": "2020-07-21T19:12:46Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntryWriter.java", "diffHunk": "@@ -98,21 +93,37 @@ void processMsg(LogReplicationEntry txMessage) {\n \n         lastMsgTs = Math.max(persistLogTS, lastMsgTs);\n \n+\n+        // If this entry's max timestamp is not bigger than the persistLogTs, skip the whole message.\n         if (topologyConfigId != persistSiteConfigID || ts != persistSnapStart || ts != persistSnapDone ||\n-                txMessage.getMetadata().getPreviousTimestamp() != persistLogTS) {\n+                entryTS <= persistLogTS) {\n             log.warn(\"Skip write this msg {} as its timestamp is later than the persisted one \" +\n                     txMessage.getMetadata() +  \" persisteSiteConfig \" + persistSiteConfigID + \" persistSnapStart \" + persistSnapStart +\n                     \" persistSnapDone \" + persistSnapDone + \" persistLogTs \" + persistLogTS);\n             return;\n         }\n \n+        // Skip Opaque entries with timestamp that are not larger than persistedTs\n+        OpaqueEntry[] newOpaqueEntryList = opaqueEntryList.stream().filter(x->x.getVersion() > persistLogTS).toArray(OpaqueEntry[]::new);", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMjM5OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459202399", "bodyText": "For opaque entry, the version is the timestamp.", "author": "xiaoqin2012", "createdAt": "2020-07-23T03:58:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyODMzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyOTUwOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458329508", "bodyText": "will opaque entry list contain all streams to be replicated, even if they do not have any data?", "author": "pankti-m", "createdAt": "2020-07-21T19:15:02Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntryWriter.java", "diffHunk": "@@ -98,21 +93,37 @@ void processMsg(LogReplicationEntry txMessage) {\n \n         lastMsgTs = Math.max(persistLogTS, lastMsgTs);\n \n+\n+        // If this entry's max timestamp is not bigger than the persistLogTs, skip the whole message.\n         if (topologyConfigId != persistSiteConfigID || ts != persistSnapStart || ts != persistSnapDone ||\n-                txMessage.getMetadata().getPreviousTimestamp() != persistLogTS) {\n+                entryTS <= persistLogTS) {\n             log.warn(\"Skip write this msg {} as its timestamp is later than the persisted one \" +\n                     txMessage.getMetadata() +  \" persisteSiteConfig \" + persistSiteConfigID + \" persistSnapStart \" + persistSnapStart +\n                     \" persistSnapDone \" + persistSnapDone + \" persistLogTs \" + persistLogTS);\n             return;\n         }\n \n+        // Skip Opaque entries with timestamp that are not larger than persistedTs\n+        OpaqueEntry[] newOpaqueEntryList = opaqueEntryList.stream().filter(x->x.getVersion() > persistLogTS).toArray(OpaqueEntry[]::new);\n+\n+        // Check that all opaque entries contain the correct streams\n+        for (OpaqueEntry opaqueEntry : newOpaqueEntryList) {\n+            if (!streamMap.keySet().containsAll(opaqueEntry.getEntries().keySet())) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2NjIzMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458466232", "bodyText": "For tx opqaque entry is a SMRentry that contains all data changed in one transaction.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:35:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyOTUwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzMDkxNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458330916", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T19:17:36Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java", "diffHunk": "@@ -131,24 +133,25 @@ public LogReplicationSinkManager(String localCorfuEndpoint, LogReplicationConfig\n          */\n         this.rxState = RxState.LOG_ENTRY_SYNC;\n         this.config = config;\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzMTExMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458331111", "bodyText": "nit - extra newline.  Also, since this is measuring time, can we rename to something like default_ack_delay_ms or something else?", "author": "pankti-m", "createdAt": "2020-07-21T19:18:00Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java", "diffHunk": "@@ -36,10 +36,12 @@\n      */\n     private static final String config_file = \"/config/corfu/corfu_replication_config.properties\";\n \n+    private final int DEFAULT_ACK_CNT = 1;\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2NjY3Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458466677", "bodyText": "This is the count of messages. As the message becomes bigger now, so we send an ACK for each message.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:37:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzMTExMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzNTYxMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458335610", "bodyText": "sequencer -> sequence number", "author": "pankti-m", "createdAt": "2020-07-21T19:26:33Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java", "diffHunk": "@@ -178,23 +178,28 @@ public void apply(LogReplicationEntry message) {\n \n         if (message.getMetadata().getSnapshotSyncSeqNum() != recvSeq ||\n                 message.getMetadata().getMessageMetadataType() != MessageType.SNAPSHOT_MESSAGE) {\n-            log.error(\"Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",\n-                    message.getMetadata().getSnapshotSyncSeqNum(), recvSeq,\n+            log.error(\"Received {} Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzNzA5OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458337099", "bodyText": "we expect the opaqueEntry size == 1.  Why is the iteration needed?", "author": "pankti-m", "createdAt": "2020-07-21T19:29:19Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java", "diffHunk": "@@ -178,23 +178,28 @@ public void apply(LogReplicationEntry message) {\n \n         if (message.getMetadata().getSnapshotSyncSeqNum() != recvSeq ||\n                 message.getMetadata().getMessageMetadataType() != MessageType.SNAPSHOT_MESSAGE) {\n-            log.error(\"Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",\n-                    message.getMetadata().getSnapshotSyncSeqNum(), recvSeq,\n+            log.error(\"Received {} Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",\n+                    message.getMetadata(), message.getMetadata().getSnapshotSyncSeqNum(), recvSeq,\n                     message.getMetadata().getMessageMetadataType(), MessageType.SNAPSHOT_MESSAGE);\n             throw new ReplicationWriterException(\"Message is out of order or wrong type\");\n         }\n \n-        byte[] payload = message.getPayload();\n-        OpaqueEntry opaqueEntry = OpaqueEntry.deserialize(Unpooled.wrappedBuffer(payload));\n-\n-        if (opaqueEntry.getEntries().keySet().size() != 1) {\n-            log.error(\"The opaqueEntry has more than one entry {}\", opaqueEntry);\n+        // For snapshot message, it has only one opaque entry.\n+        if (message.getOpaqueEntryList().size() > 1) {\n+            log.error(\" Get {} instead of one opaque entry in Snapshot Message\", message.getOpaqueEntryList().size());\n             return;\n         }\n \n-        UUID uuid = opaqueEntry.getEntries().keySet().stream().findFirst().get();\n-        processOpaqueEntry(opaqueEntry.getEntries().get(uuid), message.getMetadata().getSnapshotSyncSeqNum(), uuidMap.get(uuid));\n-        recvSeq++;\n+        for (OpaqueEntry opaqueEntry : message.getOpaqueEntryList()) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NDQ4NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458344484", "bodyText": "static import of a method is in general not a recommended practice..", "author": "pankti-m", "createdAt": "2020-07-21T19:43:55Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -14,11 +13,16 @@\n import org.corfudb.runtime.view.stream.OpaqueStream;\n \n import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.ArrayList;\n import java.util.HashSet;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Set;\n import java.util.UUID;\n \n+import static org.corfudb.infrastructure.logreplication.LogReplicationConfig.DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE;\n+import static org.corfudb.infrastructure.logreplication.replication.send.logreader.StreamsSnapshotReader.calculateSize;", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NDgxOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458344818", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T19:44:27Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +36,59 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NTQzNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458345434", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T19:45:33Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +36,59 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NTczNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458345734", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T19:46:11Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +36,59 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n+\n         Set<String> streams = config.getStreamsToReplicate();\n+\n         streamUUIDs = new HashSet<>();\n         for (String s : streams) {\n             streamUUIDs.add(CorfuRuntime.getStreamID(s));\n         }\n \n         //create an opaque stream for transaction stream\n-        txStream = new TxOpaqueStream(rt);\n+        txOpaqueStream = new TxOpaqueStream(rt);\n     }\n \n-    LogReplicationEntry generateMessage(OpaqueEntry entry, UUID logEntryRequestId) {\n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, entry);\n-        currentMsgTs = entry.getVersion();\n+    LogReplicationEntry generateMessageWithOpaqueEntryList(List<OpaqueEntry> opaqueEntryList, UUID logEntryRequestId) {\n+        // Set the last timestamp as the max timestamp\n+        currentMsgTs = opaqueEntryList.get(opaqueEntryList.size() - 1).getVersion();\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NjM2Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458346366", "bodyText": "nit - can we remove the extra newlines?", "author": "pankti-m", "createdAt": "2020-07-21T19:47:23Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -89,34 +107,88 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n                     entry.getEntries().keySet(), streamUUIDs);\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        hasNoiseData = true;\n+        return false;\n     }\n \n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n         log.info(\"snapshot {} ackTimestamp {} preMsgTs {} seek {}\", snapshot, ackTimestamp, preMsgTs, preMsgTs + 1);\n-        txStream.seek(preMsgTs + 1);\n+        txOpaqueStream.seek(preMsgTs + 1);\n         sequence = 0;\n     }\n \n+    private int calculateOpaqueEntrySize(OpaqueEntry opaqueEntry) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MDE1NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458360154", "bodyText": "since this is used by both LogEntryReader and SnapshotReader, can we move it to a common place?", "author": "pankti-m", "createdAt": "2020-07-21T20:13:43Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +62,116 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Given a list of SMREntries, calculate the total sizeInBytes.\n+     * @param smrEntries\n+     * @return\n+     */\n+    public static int calculateSize(List<SMREntry> smrEntries) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0ODU2OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459648568", "bodyText": "Move to an utility class.", "author": "xiaoqin2012", "createdAt": "2020-07-23T18:32:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MDE1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MTA4MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458361081", "bodyText": "why do we check for both default size and user-configured size?  It should be either one, right?", "author": "pankti-m", "createdAt": "2020-07-21T20:15:22Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +62,116 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Given a list of SMREntries, calculate the total sizeInBytes.\n+     * @param smrEntries\n+     * @return\n+     */\n+    public static int calculateSize(List<SMREntry> smrEntries) {\n+        int size = 0;\n+        for (SMREntry entry : smrEntries) {\n+            size += entry.getSerializedSize();\n+        }\n+\n+        log.trace(\"current entry sizeInBytes {}\", size);\n+        return size;\n+    }\n+\n+    /**\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2OTY0NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458469644", "bodyText": "If an entry's size is bigger than the user's setup but is smaller than the default that we can handle, we want that the log replication still make progress, but log a warning. But if the entry's size is bigger than we can handle, we will fail.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:47:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MTA4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MjE4NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458362185", "bodyText": "this is not an error.  the entry will get sent in the next batch.  Can we change to debug or trace?", "author": "pankti-m", "createdAt": "2020-07-21T20:17:28Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +62,116 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Given a list of SMREntries, calculate the total sizeInBytes.\n+     * @param smrEntries\n+     * @return\n+     */\n+    public static int calculateSize(List<SMREntry> smrEntries) {\n+        int size = 0;\n+        for (SMREntry entry : smrEntries) {\n+            size += entry.getSerializedSize();\n+        }\n+\n+        log.trace(\"current entry sizeInBytes {}\", size);\n+        return size;\n+    }\n+\n+    /**\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE);", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2OTgxMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458469812", "bodyText": "This is an error and will cause the log replication transport layer failure.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:48:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MjE4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MjM3MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458362371", "bodyText": "debug or trace?", "author": "pankti-m", "createdAt": "2020-07-21T20:17:50Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +62,116 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Given a list of SMREntries, calculate the total sizeInBytes.\n+     * @param smrEntries\n+     * @return\n+     */\n+    public static int calculateSize(List<SMREntry> smrEntries) {\n+        int size = 0;\n+        for (SMREntry entry : smrEntries) {\n+            size += entry.getSerializedSize();\n+        }\n+\n+        log.trace(\"current entry sizeInBytes {}\", size);\n+        return size;\n+    }\n+\n+    /**\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE);\n+                        } else if (currentEntrySize > maxDataSizePerMsg) {\n+                            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2OTkxNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458469915", "bodyText": "This is a warning.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:48:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MjM3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3MDA5Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458470093", "bodyText": "As I rebased, this is what Anny and I discussed from the previous PR.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:49:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MjM3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MzEyNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458363125", "bodyText": "can it be private?", "author": "pankti-m", "createdAt": "2020-07-21T20:19:16Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -223,4 +281,21 @@ public void setTopologyConfigId(long topologyConfigId) {\n         this.topologyConfigId = topologyConfigId;\n     }\n \n+    /**\n+     * Record a list of SMR entries\n+     */\n+    static class SMREntryList {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0ODk1Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459648953", "bodyText": "made the change.", "author": "xiaoqin2012", "createdAt": "2020-07-23T18:33:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MzEyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMjEwNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458422105", "bodyText": "can we change the method name to something like calculateOpaqueSMRSerializedSize or something of that kind?", "author": "pankti-m", "createdAt": "2020-07-21T22:20:08Z", "path": "runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java", "diffHunk": "@@ -135,6 +146,42 @@ void deserializeBuffer(ByteBuf b, CorfuRuntime rt) {\n             b.skipBytes(len);\n         }\n         SMRArguments = arguments;\n+        serializedSize = b.readerIndex() - readIndex + 1;\n+    }\n+\n+\n+    /**\n+     * Calculate an Opaque SMR entry's serialized size.\n+     * @throws IllegalAccessException\n+     */\n+    private int calculateSerializedSize() {\n+        if (!opaque) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMjg5Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458422896", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T22:22:03Z", "path": "runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java", "diffHunk": "@@ -135,6 +146,42 @@ void deserializeBuffer(ByteBuf b, CorfuRuntime rt) {\n             b.skipBytes(len);\n         }\n         SMRArguments = arguments;\n+        serializedSize = b.readerIndex() - readIndex + 1;\n+    }\n+\n+\n+    /**\n+     * Calculate an Opaque SMR entry's serialized size.\n+     * @throws IllegalAccessException\n+     */\n+    private int calculateSerializedSize() {\n+        if (!opaque) {\n+            log.error(\"This operation only supported for an opaque SMR entry\");\n+            return 0;\n+        }\n+\n+        int size = 0;\n+\n+        for (Object smrArg : SMRArguments) {\n+            size += ((byte[])smrArg).length;\n+        }\n+\n+        size += (SMRMethod.length() * Character.BYTES);\n+        size += Integer.BYTES;\n+\n+        return size;\n+    }\n+\n+    /**\n+     * The serialized size of an opaque SMR entry.\n+     * @return\n+     */\n+    public synchronized int getSerializedSize() {\n+        if (serializedSize == null) {\n+            serializedSize = calculateSerializedSize();\n+        }\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyNDEzMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458424131", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T22:25:04Z", "path": "runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java", "diffHunk": "@@ -190,6 +238,8 @@ public void serialize(ByteBuf b) {\n                     b.writeInt(length);\n                     b.writerIndex(lengthIndex + length + 4);\n                 });\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjI4OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458432289", "bodyText": "can we import logReplicationConfig instead?", "author": "pankti-m", "createdAt": "2020-07-21T22:45:53Z", "path": "test/src/test/java/org/corfudb/infrastructure/logreplication/ReplicationReaderWriterTest.java", "diffHunk": "@@ -247,4 +257,39 @@ public void testUFOWithLogUpdate() throws NoSuchMethodException, IllegalAccessEx\n         assertThat(bSet.containsAll(aSet)).isTrue();\n         assertThat(aSet.containsAll(bSet)).isTrue();\n     }\n+\n+    public static void readLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) throws\n+            TrimmedException {\n+        LogReplicationConfig config = new LogReplicationConfig(streams, BATCH_SIZE, MAX_MSG_SIZE);\n+        StreamsLogEntryReader reader = new StreamsLogEntryReader(rt, config);\n+        reader.setGlobalBaseSnapshot(Address.NON_ADDRESS, Address.NON_ADDRESS);\n+\n+        LogReplicationEntry entry = null;\n+\n+        do {\n+            entry = reader.read(UUID.randomUUID());\n+\n+            if (entry != null) {\n+                msgQ.add(entry);\n+            }\n+\n+            System.out.println(\" msgQ size \" + msgQ.size());\n+\n+        } while (entry != null);\n+    }\n+\n+    public static void writeLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) {\n+        org.corfudb.infrastructure.logreplication.LogReplicationConfig config = new LogReplicationConfig(streams);", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjY4OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458432688", "bodyText": "is this a test?  what does it test?", "author": "pankti-m", "createdAt": "2020-07-21T22:47:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjI4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3MTI2NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458471265", "bodyText": "Same as the above.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:53:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjI4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjU2NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458432564", "bodyText": "is this a test?  what does it test?", "author": "pankti-m", "createdAt": "2020-07-21T22:46:44Z", "path": "test/src/test/java/org/corfudb/infrastructure/logreplication/ReplicationReaderWriterTest.java", "diffHunk": "@@ -247,4 +257,39 @@ public void testUFOWithLogUpdate() throws NoSuchMethodException, IllegalAccessEx\n         assertThat(bSet.containsAll(aSet)).isTrue();\n         assertThat(aSet.containsAll(bSet)).isTrue();\n     }\n+\n+    public static void readLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) throws", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3MTE2OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458471169", "bodyText": "This is a function used by this test. This test the reader/writer directly not through the transport layer.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:53:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjU2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMzA1Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458433056", "bodyText": "how was this number derived?", "author": "pankti-m", "createdAt": "2020-07-21T22:48:04Z", "path": "test/src/test/java/org/corfudb/integration/AbstractIT.java", "diffHunk": "@@ -60,6 +60,8 @@\n     private static final int SHUTDOWN_RETRIES = 10;\n     private static final long SHUTDOWN_RETRY_WAIT = 500;\n \n+    private static final int MSG_SIZE = 131072;", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3MTQ1OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458471458", "bodyText": "This is set for the test case. Not too big as then all entries will be sent over in one message.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:54:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMzA1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMzIyOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459223229", "bodyText": "Can we add this comment in the code so it is easy to follow?", "author": "annym", "createdAt": "2020-07-23T05:35:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMzA1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMzUxMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458433510", "bodyText": "remove", "author": "pankti-m", "createdAt": "2020-07-21T22:49:24Z", "path": "test/src/test/java/org/corfudb/integration/AbstractIT.java", "diffHunk": "@@ -556,7 +564,7 @@ public String getOptionsString() {\n          * @throws IOException\n          */\n         public Process runServer() throws IOException {\n-            final String serverConsoleLogPath = CORFU_LOG_PATH + File.separator + host + \"_\" + port + \"_consolelog\";\n+            final String serverConsoleLogPath = \"/Users/maxi/Projects/tmp/test.result\"; //CORFU_LOG_PATH + File.separator + host + \"_\" + port + \"_consolelog\";", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MTEzNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459651137", "bodyText": "Done.", "author": "xiaoqin2012", "createdAt": "2020-07-23T18:37:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMzUxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzNzg0Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458437843", "bodyText": "what is the purpose of this method?", "author": "pankti-m", "createdAt": "2020-07-21T23:02:03Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -311,6 +324,16 @@ void verifyTables(HashMap<String, CorfuTable<Long, Long>> tables0, HashMap<Strin\n             }\n     }\n \n+    void waitData(HashMap<String, CorfuTable<Long, Long>> tables, HashMap<String, HashMap<Long, Long>> hashMap) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYzNjQwMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459636400", "bodyText": "Wait replication data reach at the standby cluster.", "author": "xiaoqin2012", "createdAt": "2020-07-23T18:11:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzNzg0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0MTkyOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458441928", "bodyText": "numKeys -> NUM_KEYS", "author": "pankti-m", "createdAt": "2020-07-21T23:14:42Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -1042,9 +1094,12 @@ public void testLogEntrySyncLargeTables() throws Exception {\n \n     /* ********************** AUXILIARY METHODS ********************** */\n \n+    private void generateTxCrossTables(Set<String> crossTableTransactions, boolean startCrossTx, int numKeys) throws Exception {\n+        generateTxCrossTables(crossTableTransactions, startCrossTx, numKeys, 0);", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0MjAyNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458442025", "bodyText": "numKeys -> NUM_KEYS", "author": "pankti-m", "createdAt": "2020-07-21T23:14:57Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -1042,9 +1094,12 @@ public void testLogEntrySyncLargeTables() throws Exception {\n \n     /* ********************** AUXILIARY METHODS ********************** */\n \n+    private void generateTxCrossTables(Set<String> crossTableTransactions, boolean startCrossTx, int numKeys) throws Exception {\n+        generateTxCrossTables(crossTableTransactions, startCrossTx, numKeys, 0);\n+    }\n \n-    // startCrossTx indicates if we start with a transaction across Tables\n-    private void testSnapshotSyncCrossTables(Set<String> crossTableTransactions, boolean startCrossTx) throws Exception {\n+        // startCrossTx indicates if we start with a transaction across Tables\n+    private void generateTxCrossTables(Set<String> crossTableTransactions, boolean startCrossTx, int numKeys, int startValue) throws Exception {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0Mjg4OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458442888", "bodyText": "numKeys -> NUM_KEYS", "author": "pankti-m", "createdAt": "2020-07-21T23:17:40Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -1054,20 +1109,20 @@ private void testSnapshotSyncCrossTables(Set<String> crossTableTransactions, boo\n \n         // Write data across to tables specified in crossTableTransactions in transaction\n         if (startCrossTx) {\n-            generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, NUM_KEYS, srcDataRuntime, 0);\n+            generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, numKeys, srcDataRuntime, startValue);\n         }\n \n         // Write data to t0\n-        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t0), srcDataForVerification, NUM_KEYS, srcDataRuntime, NUM_KEYS);\n+        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t0), srcDataForVerification, numKeys, srcDataRuntime, numKeys);\n \n         // Write data to t1\n-        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t1), srcDataForVerification, NUM_KEYS, srcDataRuntime, NUM_KEYS);\n+        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t1), srcDataForVerification, numKeys, srcDataRuntime, numKeys);\n \n         // Write data to t2\n-        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t2), srcDataForVerification, NUM_KEYS, srcDataRuntime, 0);\n+        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t2), srcDataForVerification, numKeys, srcDataRuntime, 0);\n \n         // Write data across to tables specified in crossTableTransactions in transaction\n-        generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, NUM_KEYS, srcDataRuntime, NUM_KEYS*2);\n+        generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, numKeys, srcDataRuntime, numKeys*2);", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "url": "https://github.com/CorfuDB/CorfuDB/commit/49dc824b6fca4e7bc9c5639308f6db6440016cc1", "message": "Address comments and fix tests.", "committedDate": "2020-07-22T23:39:13Z", "type": "commit"}, {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "url": "https://github.com/CorfuDB/CorfuDB/commit/ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "message": "Merge branch 'log-replication-master' into xq/0720_msg_size_01", "committedDate": "2020-07-23T04:26:00Z", "type": "commit"}, {"oid": "558d9bc783fa25db0791b58efbb4e33da951a052", "url": "https://github.com/CorfuDB/CorfuDB/commit/558d9bc783fa25db0791b58efbb4e33da951a052", "message": "Fix compiling issues.", "committedDate": "2020-07-23T05:06:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2Mzg3OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459163878", "bodyText": "Thinking on the readers. I'm not sure the right words, because even I was confused the first time I reviewed cause I understood that if it is set to 5 we send 5 messages embedded in a single message (a batch of 5). But, we use it is to send 5 messages and yield the thread, but 5 messages and not 1 fat one. Maybe it's more accurate to say: \"Get the max number of messages sent per Log Replication Runtime in between FSM worker thread yield\" (IDK just giving ideas, something in that line)", "author": "annym", "createdAt": "2020-07-23T00:58:29Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java", "diffHunk": "@@ -216,16 +219,30 @@ public String getPluginConfigFilePath() {\n         return pluginConfigFilePath == null ? PLUGIN_CONFIG_FILE_PATH : pluginConfigFilePath;\n     }\n \n-    public int getSnapshotSyncBatchSize() {\n-        Integer snapshotSyncBatchSize = getServerConfig(Integer.class, \"--snapshot-batch\");\n-        return snapshotSyncBatchSize == null ? SnapshotSender.DEFAULT_SNAPSHOT_BATCH_SIZE : snapshotSyncBatchSize;\n+    /**", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYzNzkxMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459637912", "bodyText": "You gave the word earlier. If you want to change, you can change it in your PR.", "author": "xiaoqin2012", "createdAt": "2020-07-23T18:13:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2Mzg3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NDA0Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459164047", "bodyText": "Can we shorten it? Since it already has \"LogReplication\" from LogReplicationConfig. --> maybe MAX_DATA_MSG_SIZE", "author": "annym", "createdAt": "2020-07-23T00:59:11Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java", "diffHunk": "@@ -44,6 +43,8 @@\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n \n+import static org.corfudb.infrastructure.logreplication.LogReplicationConfig.MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED;", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NDc2NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459164765", "bodyText": "message -> messages", "author": "annym", "createdAt": "2020-07-23T01:02:08Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java", "diffHunk": "@@ -13,15 +13,40 @@\n @Data\n public class LogReplicationConfig {\n \n+    // Log Replication message timeout time in milliseconds.\n+    public static final int DEFAULT_TIMEOUT_MS = 5000;\n+\n+    // Log Replication default max number of message generated at the active cluster for each batch.", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MDg4Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459170886", "bodyText": "We can simplify with:\nthis(streamsToReplicate, DEFAULT_MAX_NUM_MSG_PER_BATCH, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);", "author": "annym", "createdAt": "2020-07-23T01:27:11Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java", "diffHunk": "@@ -30,16 +55,21 @@\n      */\n     public LogReplicationConfig(Set<String> streamsToReplicate) {\n         this.streamsToReplicate = streamsToReplicate;\n+        this.maxNumMsgPerBatch = DEFAULT_MAX_NUM_MSG_PER_BATCH;", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MzgxMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459653812", "bodyText": "It is harder for later code review. It is easier logic to init all things together in one place instead of jumping code around for simple logic.", "author": "xiaoqin2012", "createdAt": "2020-07-23T18:42:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MDg4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY4NTkxNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459685915", "bodyText": "it is actually a good practice, to avoid code duplication, and having all inits in one single place or when a change comes, you'll have to change in several places (prone to errors):\nhttps://stackoverflow.com/questions/7577627/purpose-of-constructor-chaining", "author": "annym", "createdAt": "2020-07-23T19:42:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MDg4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MTE1Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459171156", "bodyText": "What is this buffer?", "author": "annym", "createdAt": "2020-07-23T01:28:35Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/plugins/DefaultClusterConfig.java", "diffHunk": "@@ -45,7 +45,7 @@ private DefaultClusterConfig() {\n     private static String standbyLogReplicationPort = \"9020\";\n \n     @Getter\n-    private static int logSenderBufferSize = 20;\n+    private static int logSenderBufferSize = 2;", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY3MTkzMw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459671933", "bodyText": "At both the sender and receiver we have buffers for messages. As the message is big, the standard double buffering may be enough.", "author": "xiaoqin2012", "createdAt": "2020-07-23T19:15:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MTE1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MTMxOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459171318", "bodyText": "Can we add a comment. Not sure what this means.", "author": "annym", "createdAt": "2020-07-23T01:29:19Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/fsm/TestLogEntryReader.java", "diffHunk": "@@ -26,4 +26,9 @@ public void reset(long lastSentBaseSnapshotTimestamp, long lastAckedTimestamp) {\n     public void setTopologyConfigId(long siteConfigID) {\n \n     }\n+\n+    @Override\n+    public boolean hasNoiseData() {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MTkyOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459171929", "bodyText": "Shouldn't we remove metadata.getTimestamp?", "author": "annym", "createdAt": "2020-07-23T01:32:06Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+        /**\n+         *  For each message in the  buffer, if its timestamp is smaller than last processed log entry's timestamp,\n+         *  skip processing and remove it from buffer.\n+         *  If its preTs and currentTs is overlapping with the last processed log entry's timestamp, process it.\n+         */\n+        for (LogReplicationEntry entry : buffer.values()) {\n+            LogReplicationEntryMetadata metadata = entry.getMetadata();\n+            if (metadata.getTimestamp() < lastProcessedSeq) {\n+                //remove it\n+                buffer.remove(metadata.getPreviousTimestamp());", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MTU2OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459181569", "bodyText": "remove lastProcessedSeq or metadata.getTimestamp? as lastProcessedSeq is the one that has already been processed and we just applied entry.", "author": "annym", "createdAt": "2020-07-23T02:16:46Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+        /**\n+         *  For each message in the  buffer, if its timestamp is smaller than last processed log entry's timestamp,\n+         *  skip processing and remove it from buffer.\n+         *  If its preTs and currentTs is overlapping with the last processed log entry's timestamp, process it.\n+         */\n+        for (LogReplicationEntry entry : buffer.values()) {\n+            LogReplicationEntryMetadata metadata = entry.getMetadata();\n+            if (metadata.getTimestamp() < lastProcessedSeq) {\n+                //remove it\n+                buffer.remove(metadata.getPreviousTimestamp());\n+            } else if (metadata.getPreviousTimestamp() <= lastProcessedSeq && metadata.getTimestamp() > lastProcessedSeq) {\n+                sinkManager.processMessage(entry);\n+                ackCnt++;\n+                buffer.remove(lastProcessedSeq);", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxNDc0OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459714748", "bodyText": "For each message, it has the previous time stamp and the currentTimestamp( the max ts of all transactions in this message). So if all transactions in one message have been processed, we will remove it. If it has transactions that haven't been processed, we will still buffer it and process it later.\nThis happens because the possible leadership change and the two leaders at the active cluster may packaging the messages differently as the polling transaction log speed  are different at the different nodes.", "author": "xiaoqin2012", "createdAt": "2020-07-23T20:39:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MTU2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MjA4Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459182086", "bodyText": "This comment might be redundant.", "author": "annym", "createdAt": "2020-07-23T02:19:01Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+        /**\n+         *  For each message in the  buffer, if its timestamp is smaller than last processed log entry's timestamp,\n+         *  skip processing and remove it from buffer.\n+         *  If its preTs and currentTs is overlapping with the last processed log entry's timestamp, process it.\n+         */\n+        for (LogReplicationEntry entry : buffer.values()) {\n+            LogReplicationEntryMetadata metadata = entry.getMetadata();\n+            if (metadata.getTimestamp() < lastProcessedSeq) {\n+                //remove it", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4Mjk1MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459182951", "bodyText": "<= , right? cause if the same last processed entry was resent we can remove or we'll have a leak.", "author": "annym", "createdAt": "2020-07-23T02:22:28Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+        /**\n+         *  For each message in the  buffer, if its timestamp is smaller than last processed log entry's timestamp,\n+         *  skip processing and remove it from buffer.\n+         *  If its preTs and currentTs is overlapping with the last processed log entry's timestamp, process it.\n+         */\n+        for (LogReplicationEntry entry : buffer.values()) {\n+            LogReplicationEntryMetadata metadata = entry.getMetadata();\n+            if (metadata.getTimestamp() < lastProcessedSeq) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MzUzOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459183539", "bodyText": "What happens if there are remaining messages in the queue which are skipped because they were not the subsequent sequence numbers and no further messages are received? would we ever apply them? Let me illustrate it. For instance:\nUpdates to Stream A: 0, 1, 2, 5, 7, 8\nlastProcessedSeqNum = 2\nLet's say the buffer looks like this (as some messages were resent and they are out of order):\n0, 2, 1, 0, 7, 8, 5\nWe ignore 0, 2, 1, 0... We leave 7 in the buffer but do not process, same with 8... Then we process 5. But now 7 and 8 are left in the queue. And if no more data is received for an hour, 7 and 8 won't be applied, as this is kicked off as part of receiving data.", "author": "annym", "createdAt": "2020-07-23T02:25:25Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NTk5Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459185992", "bodyText": "mmm I kept looking and I see we have a processBuffer in SinkBufferManager, which would take care of out of order messages, but one question, in that method we get the lastProcessedSeqNum, isn't that the one that we already applied, and that should not be available anymore in the buffer? Thus, we are not really processing the unordered until a new message is received?", "author": "annym", "createdAt": "2020-07-23T02:37:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MzUzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxNjk5Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459716993", "bodyText": "In your case, when it receives 5, it will process 5 and right after process it:\n\nit will update the lastSeqNum\nit will look at the buffer to see if there are any messages are in order to be processed.", "author": "xiaoqin2012", "createdAt": "2020-07-23T20:43:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MzUzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyMjY0Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459722642", "bodyText": "the map is indexed according to the preTs/preSeq number.\nFor your case the map:\nafter process message 2, the lastTs = 2\nfor message 7, the preTs is 5\nfor message 8, the preTs is 7\nfor message 5, the preTs is 2: we will process it and update the lastTs as 5, use 5 as the index to lookup and find message 7.", "author": "xiaoqin2012", "createdAt": "2020-07-23T20:55:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MzUzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4ODgzNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459188837", "bodyText": "haven't -> haven't been applied", "author": "annym", "createdAt": "2020-07-23T02:51:15Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java", "diffHunk": "@@ -141,7 +141,8 @@ public LogReplicationEntry processMsgAndBuffer(LogReplicationEntry dataMessage)\n         long preTs = getPreSeq(dataMessage);\n         long currentTs = getCurrentSeq(dataMessage);\n \n-        if (preTs == lastProcessedSeq) {\n+        // This message contains entries that haven't applied yet", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5Mjk5Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459192997", "bodyText": "Can this happen because a LogEntryMessage might have numerous delta's inside? and it picks the min of all as the previous and the max of all as the timestamp?", "author": "annym", "createdAt": "2020-07-23T03:11:31Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java", "diffHunk": "@@ -141,7 +141,8 @@ public LogReplicationEntry processMsgAndBuffer(LogReplicationEntry dataMessage)\n         long preTs = getPreSeq(dataMessage);\n         long currentTs = getCurrentSeq(dataMessage);\n \n-        if (preTs == lastProcessedSeq) {\n+        // This message contains entries that haven't applied yet\n+        if (preTs <= lastProcessedSeq && currentTs > lastProcessedSeq) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NjE0Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459196143", "bodyText": "Can you please add a description to this.", "author": "annym", "createdAt": "2020-07-23T03:26:45Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/LogEntryReader.java", "diffHunk": "@@ -23,4 +23,6 @@\n     void reset(long lastSentBaseSnapshotTimestamp, long lastAckedTimestamp);\n \n     void setTopologyConfigId(long topologyConfigId);\n+", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NzEwMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459197102", "bodyText": "This class can be removed right?", "author": "annym", "createdAt": "2020-07-23T03:31:16Z", "path": "test/src/test/java/org/corfudb/integration/ReplicationReaderWriterWithUFOIT.java", "diffHunk": "@@ -0,0 +1,4 @@\n+package org.corfudb.integration;", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5OTAwNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459199005", "bodyText": "curious why did we remove this assertion? it should still hold valid.", "author": "annym", "createdAt": "2020-07-23T03:40:48Z", "path": "test/src/test/java/org/corfudb/infrastructure/logreplication/LogReplicationFSMTest.java", "diffHunk": "@@ -349,8 +349,6 @@ public void testSnapshotSyncStreamImplementation() throws Exception {\n \n         Queue<LogReplicationEntry> listenerQueue = ((TestDataSender) dataSender).getEntryQueue();\n \n-        assertThat(LARGE_NUM_ENTRIES/ StreamsSnapshotReader.MAX_NUM_SMR_ENTRY).isLessThanOrEqualTo(listenerQueue.size());", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNjE3Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459726177", "bodyText": "Now we are using the size not the number of entries.", "author": "xiaoqin2012", "createdAt": "2020-07-23T21:02:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5OTAwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5OTA2OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459199069", "bodyText": "we can remove this commented code.", "author": "annym", "createdAt": "2020-07-23T03:41:15Z", "path": "runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java", "diffHunk": "@@ -135,6 +145,41 @@ void deserializeBuffer(ByteBuf b, CorfuRuntime rt) {\n             b.skipBytes(len);\n         }\n         SMRArguments = arguments;\n+        serializedSize = b.readerIndex() - readIndex + 1;\n+    }\n+\n+\n+    /**\n+     * Calculate an Opaque SMR entry's serialized size.\n+     * @throws IllegalAccessException\n+     */\n+    private int calculateOpaqueSMREntrySerializedSize() {\n+        if (!opaque) {\n+            log.error(\"This operation only supported for an opaque SMR entry\");\n+            return 0;\n+        }\n+\n+        int size = 0;\n+\n+        for (Object smrArg : SMRArguments) {\n+            size += ((byte[])smrArg).length;\n+        }\n+\n+        size += (SMRMethod.length() * Character.BYTES);\n+        size += Integer.BYTES;\n+\n+        return size;\n+    }\n+\n+    /**\n+     * The serialized size of an opaque SMR entry.\n+     * @return\n+     */\n+    public synchronized Integer getSerializedSize() {\n+        //if (serializedSize == null) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMDQ0Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459200446", "bodyText": "Shouldn't this be LogEntryReader as it could have any implementation? like the test one we use in some test.", "author": "annym", "createdAt": "2020-07-23T03:48:31Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/LogEntrySender.java", "diffHunk": "@@ -32,7 +33,7 @@\n     /*\n      * Implementation of Log Entry Reader. Default implementation reads at the stream layer.\n      */\n-    private LogEntryReader logEntryReader;\n+    private StreamsLogEntryReader logEntryReader;", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMTk2Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459201963", "bodyText": "A bit confusing, maybe -> Max number of messages sent in burst during a snapshot cycle. (or something in that line)", "author": "annym", "createdAt": "2020-07-23T03:56:04Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/SnapshotSender.java", "diffHunk": "@@ -39,15 +42,14 @@\n @Slf4j\n public class SnapshotSender {\n \n-    public static int DEFAULT_SNAPSHOT_BATCH_SIZE = 100;\n-    public static final int DEFAULT_TIMEOUT = 5000;\n-\n     private CorfuRuntime runtime;\n     private SnapshotReader snapshotReader;\n     private SenderBufferManager dataSenderBufferManager;\n     private LogReplicationFSM fsm;\n     private long baseSnapshotTimestamp;\n-    private final int snapshotSyncBatchSize;\n+\n+    // The max number of message can be sent over per cycle run during snapshot full sync state.", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMzcwMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459203702", "bodyText": "entry is never used.", "author": "annym", "createdAt": "2020-07-23T04:04:48Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMzgzNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459203835", "bodyText": "please add access modifier, private. As this is always a static analysis error.", "author": "annym", "createdAt": "2020-07-23T04:05:15Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMzg5Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459203896", "bodyText": "private", "author": "annym", "createdAt": "2020-07-23T04:05:38Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +34,58 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n+\n         Set<String> streams = config.getStreamsToReplicate();\n+\n         streamUUIDs = new HashSet<>();\n         for (String s : streams) {\n             streamUUIDs.add(CorfuRuntime.getStreamID(s));\n         }\n \n         //create an opaque stream for transaction stream\n-        txStream = new TxOpaqueStream(rt);\n+        txOpaqueStream = new TxOpaqueStream(rt);\n     }\n \n-    LogReplicationEntry generateMessage(OpaqueEntry entry, UUID logEntryRequestId) {\n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, entry);\n-        currentMsgTs = entry.getVersion();\n+    LogReplicationEntry generateMessageWithOpaqueEntryList(List<OpaqueEntry> opaqueEntryList, UUID logEntryRequestId) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNTQyMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459205422", "bodyText": "private", "author": "annym", "createdAt": "2020-07-23T04:13:43Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +34,58 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n+\n         Set<String> streams = config.getStreamsToReplicate();\n+\n         streamUUIDs = new HashSet<>();\n         for (String s : streams) {\n             streamUUIDs.add(CorfuRuntime.getStreamID(s));\n         }\n \n         //create an opaque stream for transaction stream\n-        txStream = new TxOpaqueStream(rt);\n+        txOpaqueStream = new TxOpaqueStream(rt);\n     }\n \n-    LogReplicationEntry generateMessage(OpaqueEntry entry, UUID logEntryRequestId) {\n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, entry);\n-        currentMsgTs = entry.getVersion();\n+    LogReplicationEntry generateMessageWithOpaqueEntryList(List<OpaqueEntry> opaqueEntryList, UUID logEntryRequestId) {\n+        // Set the last timestamp as the max timestamp\n+        currentMsgTs = opaqueEntryList.get(opaqueEntryList.size() - 1).getVersion();\n         LogReplicationEntry txMessage = new LogReplicationEntry(MSG_TYPE, topologyConfigId, logEntryRequestId,\n-                currentMsgTs, preMsgTs, globalBaseSnapshot, sequence, buf.array());\n+                currentMsgTs, preMsgTs, globalBaseSnapshot, sequence, opaqueEntryList);\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        return  txMessage;\n+        log.trace(\"Generate a log entry message {} with {} transactions \", txMessage.getMetadata(), opaqueEntryList.size());\n+        return txMessage;\n     }\n \n-    boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n+\n+    // Check if it has the correct streams.\n+    boolean shouldProcess(OpaqueEntry entry) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNTcyOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459205728", "bodyText": "maybe rename to checkValidSize?", "author": "annym", "createdAt": "2020-07-23T04:15:13Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNjc2Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459206762", "bodyText": "No need to break as it is already in the while condition, or if we want to break before the while loop remove from the loop.", "author": "annym", "createdAt": "2020-07-23T04:19:56Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {\n+            return false;\n+        }\n+\n+        return true;\n     }\n \n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n         log.info(\"snapshot {} ackTimestamp {} preMsgTs {} seek {}\", snapshot, ackTimestamp, preMsgTs, preMsgTs + 1);\n-        txStream.seek(preMsgTs + 1);\n+        txOpaqueStream.seek(preMsgTs + 1);\n         sequence = 0;\n     }\n \n+\n     @Override\n     public LogReplicationEntry read(UUID logEntryRequestId) throws TrimmedException, IllegalTransactionStreamsException {\n+        List<OpaqueEntry> opaqueEntryList = new ArrayList<>();\n+        int currentEntrySize = 0;\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (txStream.hasNext()) {\n-                OpaqueEntry opaqueEntry = txStream.next();\n-                if (!shouldProcess(opaqueEntry)) {\n-                    continue;\n+            while (currentMsgSize < maxDataSizePerMsg && !hasNoiseData) {\n+\n+                if (lastOpaqueEntry != null && shouldProcess(lastOpaqueEntry)) {\n+\n+                    // If the currentEntry is too big to append the current message, will skip it and\n+                    // append it to the next message as the first entry.\n+                    currentEntrySize = ReaderUtility.calculateOpaqueEntrySize(lastOpaqueEntry);\n+\n+                    if (!checkSizeOK(lastOpaqueEntry, currentMsgSize, currentEntrySize)) {\n+                        break;\n+                    }\n+\n+                    // Add the lastOpaqueEntry to the current message.\n+                    opaqueEntryList.add(lastOpaqueEntry);\n+                    currentMsgSize += currentEntrySize;\n+                    lastOpaqueEntry = null;\n+                }\n+\n+                if (hasNoiseData) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNjkwOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459206909", "bodyText": "I think it's better in the while loop.\nwhile(currentMsgSize < maxDataSizePerMsg && !hasNoiseData && txOpaqueStream.hasNext()) ... it's ok if we read a next (when there is no, as that method returns null if !hasNext)\nBut if you want to take it out of the while and leave it here, we might simplify for readiness:\nif (hasNoiseData || !txOpaqueStream.hasNext()) {\nbreak;\n}", "author": "annym", "createdAt": "2020-07-23T04:20:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNjc2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4MjIxNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460182217", "bodyText": "Consider that the max_msg_size is 100 and there are only two entrys. entry0 with size  90, entry1 with size 20\nafter processing the first entry, the while is correct, then we set  lastEntry = entry2.\nbut entry2 could not go with the first msg.\nwhile constructing the second message, if there are no new entrys, the hasNext gives false, but we need to add entry2 first. So the logic is check the lastEntry first. then calling the next.", "author": "xiaoqin2012", "createdAt": "2020-07-24T17:11:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNjc2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNDM5OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459214399", "bodyText": "why do we set hasNoiseData to true, the fact that the size of the entry is greater than the boundary does not have implications on this, right?", "author": "annym", "createdAt": "2020-07-23T04:56:54Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5MjYxNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460192614", "bodyText": "Changed it.", "author": "xiaoqin2012", "createdAt": "2020-07-24T17:32:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNDM5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNDcxNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459214715", "bodyText": "this is repeated. Same statement as before.", "author": "annym", "createdAt": "2020-07-23T04:58:39Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNTY5OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459825698", "bodyText": "I think this was accidentally copied. Same as in L.113.", "author": "annym", "createdAt": "2020-07-24T02:33:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNDcxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNDk1Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459214953", "bodyText": "simplify:\nreturn !(currentEntrySize + CurrentMsgSize > maxDataSizePerMsg);", "author": "annym", "createdAt": "2020-07-23T04:59:34Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNTQyNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459215426", "bodyText": "what happens if there is noisy data but the list is not empty? we should still abort, right?", "author": "annym", "createdAt": "2020-07-23T05:01:58Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {\n+            return false;\n+        }\n+\n+        return true;\n     }\n \n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n         log.info(\"snapshot {} ackTimestamp {} preMsgTs {} seek {}\", snapshot, ackTimestamp, preMsgTs, preMsgTs + 1);\n-        txStream.seek(preMsgTs + 1);\n+        txOpaqueStream.seek(preMsgTs + 1);\n         sequence = 0;\n     }\n \n+\n     @Override\n     public LogReplicationEntry read(UUID logEntryRequestId) throws TrimmedException, IllegalTransactionStreamsException {\n+        List<OpaqueEntry> opaqueEntryList = new ArrayList<>();\n+        int currentEntrySize = 0;\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (txStream.hasNext()) {\n-                OpaqueEntry opaqueEntry = txStream.next();\n-                if (!shouldProcess(opaqueEntry)) {\n-                    continue;\n+            while (currentMsgSize < maxDataSizePerMsg && !hasNoiseData) {\n+\n+                if (lastOpaqueEntry != null && shouldProcess(lastOpaqueEntry)) {\n+\n+                    // If the currentEntry is too big to append the current message, will skip it and\n+                    // append it to the next message as the first entry.\n+                    currentEntrySize = ReaderUtility.calculateOpaqueEntrySize(lastOpaqueEntry);\n+\n+                    if (!checkSizeOK(lastOpaqueEntry, currentMsgSize, currentEntrySize)) {\n+                        break;\n+                    }\n+\n+                    // Add the lastOpaqueEntry to the current message.\n+                    opaqueEntryList.add(lastOpaqueEntry);\n+                    currentMsgSize += currentEntrySize;\n+                    lastOpaqueEntry = null;\n+                }\n+\n+                if (hasNoiseData) {\n+                    break;\n                 }\n-                LogReplicationEntry txMessage = generateMessage(opaqueEntry, logEntryRequestId);\n-                return txMessage;\n+\n+                if (!txOpaqueStream.hasNext()) {\n+                    break;\n+                }\n+\n+                lastOpaqueEntry = txOpaqueStream.next();\n             }\n+\n+            log.trace(\"Generate LogEntryDataMessage size {} with {} entries for maxDataSizePerMsg {}. lastEnry size {}\",\n+                    currentMsgSize, opaqueEntryList.size(), maxDataSizePerMsg, lastOpaqueEntry == null? 0 : currentEntrySize);\n+\n+            if (opaqueEntryList.size() == 0 && hasNoiseData) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNTk1MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459825951", "bodyText": "??", "author": "annym", "createdAt": "2020-07-24T02:35:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNTQyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3NzY4OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460177688", "bodyText": "For example, we have log0, log1, log2, log3. All logs from 0 to 2 are good, but log3 is bad. Should we transfer log0, log1, log2, first, then abort when process log3?", "author": "xiaoqin2012", "createdAt": "2020-07-24T17:02:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNTQyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDIwNTM0MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460205340", "bodyText": "Well from early discussions with Medhavi my understanding is that it's just an invalid state, so we can directly abort without processing anything.", "author": "annym", "createdAt": "2020-07-24T17:57:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNTQyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNTc0MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459215741", "bodyText": "It's cleaner if generateMessageWithOpaqueEntryList accepts the empty list and returns a null. Less if conditions.", "author": "annym", "createdAt": "2020-07-23T05:03:06Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {\n+            return false;\n+        }\n+\n+        return true;\n     }\n \n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n         log.info(\"snapshot {} ackTimestamp {} preMsgTs {} seek {}\", snapshot, ackTimestamp, preMsgTs, preMsgTs + 1);\n-        txStream.seek(preMsgTs + 1);\n+        txOpaqueStream.seek(preMsgTs + 1);\n         sequence = 0;\n     }\n \n+\n     @Override\n     public LogReplicationEntry read(UUID logEntryRequestId) throws TrimmedException, IllegalTransactionStreamsException {\n+        List<OpaqueEntry> opaqueEntryList = new ArrayList<>();\n+        int currentEntrySize = 0;\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (txStream.hasNext()) {\n-                OpaqueEntry opaqueEntry = txStream.next();\n-                if (!shouldProcess(opaqueEntry)) {\n-                    continue;\n+            while (currentMsgSize < maxDataSizePerMsg && !hasNoiseData) {\n+\n+                if (lastOpaqueEntry != null && shouldProcess(lastOpaqueEntry)) {\n+\n+                    // If the currentEntry is too big to append the current message, will skip it and\n+                    // append it to the next message as the first entry.\n+                    currentEntrySize = ReaderUtility.calculateOpaqueEntrySize(lastOpaqueEntry);\n+\n+                    if (!checkSizeOK(lastOpaqueEntry, currentMsgSize, currentEntrySize)) {\n+                        break;\n+                    }\n+\n+                    // Add the lastOpaqueEntry to the current message.\n+                    opaqueEntryList.add(lastOpaqueEntry);\n+                    currentMsgSize += currentEntrySize;\n+                    lastOpaqueEntry = null;\n+                }\n+\n+                if (hasNoiseData) {\n+                    break;\n                 }\n-                LogReplicationEntry txMessage = generateMessage(opaqueEntry, logEntryRequestId);\n-                return txMessage;\n+\n+                if (!txOpaqueStream.hasNext()) {\n+                    break;\n+                }\n+\n+                lastOpaqueEntry = txOpaqueStream.next();\n             }\n+\n+            log.trace(\"Generate LogEntryDataMessage size {} with {} entries for maxDataSizePerMsg {}. lastEnry size {}\",\n+                    currentMsgSize, opaqueEntryList.size(), maxDataSizePerMsg, lastOpaqueEntry == null? 0 : currentEntrySize);\n+\n+            if (opaqueEntryList.size() == 0 && hasNoiseData) {\n+                throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+            }\n+\n+            if (opaqueEntryList.size() == 0) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNjAxNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459216014", "bodyText": "can this logger be removed?", "author": "annym", "createdAt": "2020-07-23T05:04:24Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +67,103 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNzEyOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459217128", "bodyText": "L 151 -157 can be simplified:\nif (!stream.iterator.hasNext()) {\nbreak;\n}\nlastEntry = (OpaqueEntry) stream.iterator.next();", "author": "annym", "createdAt": "2020-07-23T05:09:31Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +67,103 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = ReaderUtility.calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                            throw new IllegalSnapshotEntrySizeException(\" The snapshot entry is bigger than the system supported\");\n+                        } else if (currentEntrySize > maxDataSizePerMsg) {\n+                            observeBiggerMsg.setValue(observeBiggerMsg.getValue()+1);\n+                            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                                    currentEntrySize, maxDataSizePerMsg);\n+                        }\n+\n+                        // Skip append this entry in this message. Will process it first at the next round.\n+                        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg && currentMsgSize != 0) {\n+                            break;\n+                        }\n+\n+                        smrList.addAll(smrEntries);\n+                        currentMsgSize += currentEntrySize;\n+                        stream.maxVersion = Math.max(stream.maxVersion, lastEntry.getVersion());\n+                    }\n+                    lastEntry = null;\n+                }\n+\n+                if (stream.iterator.hasNext()) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxODUyOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459218529", "bodyText": "I'm confused, why do we have this same comparison of a single currentEntrySize twice? shouldn't it only compare with maxDataSizePerMsg, as this is the user configured (which could default to MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED)", "author": "annym", "createdAt": "2020-07-23T05:15:40Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +67,103 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = ReaderUtility.calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                            throw new IllegalSnapshotEntrySizeException(\" The snapshot entry is bigger than the system supported\");\n+                        } else if (currentEntrySize > maxDataSizePerMsg) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5NzA3Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460197077", "bodyText": "User can setup a different one. Supported is the one we can support. The user setup one is the one the user thinks it gives the best performance. Like 64MB is the max we can support. But in reality maybe 2M gives the best performance. But it depends the real hardware config that the users are aware of or have done experiments with.", "author": "xiaoqin2012", "createdAt": "2020-07-24T17:41:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxODUyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxODk2OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459218969", "bodyText": "why do we need currentMsgSize != 0? If we already checked that currentEntrySize is bigger previously and we throw an exception.", "author": "annym", "createdAt": "2020-07-23T05:17:45Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +67,103 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = ReaderUtility.calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                            throw new IllegalSnapshotEntrySizeException(\" The snapshot entry is bigger than the system supported\");\n+                        } else if (currentEntrySize > maxDataSizePerMsg) {\n+                            observeBiggerMsg.setValue(observeBiggerMsg.getValue()+1);\n+                            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                                    currentEntrySize, maxDataSizePerMsg);\n+                        }\n+\n+                        // Skip append this entry in this message. Will process it first at the next round.\n+                        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg && currentMsgSize != 0) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5Nzg0Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460197846", "bodyText": "If it is the only entry and its size is bigger than the configured but smaller than the max_supported, we still continue the replication , right? I thought we have discussed this before.", "author": "xiaoqin2012", "createdAt": "2020-07-24T17:42:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxODk2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMjQ4NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459222484", "bodyText": "private", "author": "annym", "createdAt": "2020-07-23T05:32:52Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/logreplication/LogReplicationEntry.java", "diffHunk": "@@ -13,41 +21,102 @@\n  *\n  * @author annym\n  */\n+@Slf4j\n @Data\n public class LogReplicationEntry implements ICorfuPayload<LogReplicationEntry> {\n \n     private LogReplicationEntryMetadata metadata;\n \n-    private byte[] payload;\n+    private List<OpaqueEntry> opaqueEntryList = new ArrayList<>();\n \n-    public LogReplicationEntry(LogReplicationEntryMetadata metadata, byte[] payload) {\n-        this.payload = payload;\n+\n+    // Only used by test cases\n+    @VisibleForTesting\n+    byte[] payload;", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMzMxMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459223311", "bodyText": "Camel case please.", "author": "annym", "createdAt": "2020-07-23T05:36:15Z", "path": "test/src/test/java/org/corfudb/integration/AbstractIT.java", "diffHunk": "@@ -313,6 +315,7 @@ public static Process runReplicationServer(int port, String pluginConfigFilePath\n                 .setHost(DEFAULT_HOST)\n                 .setPort(port)\n                 .setPluginConfigFilePath(pluginConfigFilePath)\n+                .setMsg_size(MSG_SIZE)", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMzY4NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459223684", "bodyText": "Java uses CamelCase.", "author": "annym", "createdAt": "2020-07-23T05:37:36Z", "path": "test/src/test/java/org/corfudb/integration/AbstractIT.java", "diffHunk": "@@ -507,6 +510,7 @@ public Process runServer() throws IOException {\n         private String compressionCodec = null;\n         private String pluginConfigFilePath = null;\n         private String logPath = null;\n+        private int msg_size = 0;", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNDM0OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459224348", "bodyText": "private", "author": "annym", "createdAt": "2020-07-23T05:40:18Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -311,6 +324,16 @@ void verifyTables(HashMap<String, CorfuTable<Long, Long>> tables0, HashMap<Strin\n             }\n     }\n \n+    void waitData(HashMap<String, CorfuTable<Long, Long>> tables, HashMap<String, HashMap<Long, Long>> hashMap) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNDYwNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459224605", "bodyText": "Why did this change?", "author": "annym", "createdAt": "2020-07-23T05:41:17Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -695,7 +742,7 @@ public void testLogEntrySyncValidCrossTablesWithTriggerTimeout() throws Exceptio\n         expectedAckMessages =  NUM_KEYS*WRITE_CYCLES;\n \n         testConfig.clear().setDropMessageLevel(2);\n-        startLogEntrySync(crossTables, WAIT.ON_ERROR);\n+        startLogEntrySync(crossTables, WAIT.ON_TIMEOUT_ERROR);", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5OTU4OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460199589", "bodyText": "It is much clear about what each test is wait on.", "author": "xiaoqin2012", "createdAt": "2020-07-24T17:46:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNDYwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNTU1Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459225556", "bodyText": "why did this change? there is a method that takes only one condition.", "author": "annym", "createdAt": "2020-07-23T05:45:18Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -811,16 +832,22 @@ public void testLogEntrySyncValidCrossTablesWithWritingAtSrc() throws Exception\n         testConfig.clear();\n         testConfig.setWritingSrc(true);\n         testConfig.setDeleteOP(true);\n-\n         testConfig.setWaitOn(WAIT.ON_ACK);\n-        startLogEntrySync(crossTables, WAIT.ON_ACK, false);\n+\n+        HashSet<WAIT> waitHashSet = new HashSet<>();\n+        waitHashSet.add(WAIT.ON_ACK);\n+        startLogEntrySync(crossTables, waitHashSet, true);", "originalCommit": "558d9bc783fa25db0791b58efbb4e33da951a052", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTczMDI4Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459730287", "bodyText": "I want to put all things clear for each test case instead of using the wrapper, otherwise, while debugging, I need to jump around to see what are the real arguments passed.", "author": "xiaoqin2012", "createdAt": "2020-07-23T21:10:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNTU1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjE4MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459226180", "bodyText": "The intention of the observables and the WAIT.ON_ACK is to avoid while loops that are just stuck there until a condition is met. Can't we set the expected number of ACKS by knowing the size of what we wrote and how many messages will be sent, hence how many calks we will receive back?", "author": "annym", "createdAt": "2020-07-23T05:48:01Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -811,16 +832,22 @@ public void testLogEntrySyncValidCrossTablesWithWritingAtSrc() throws Exception\n         testConfig.clear();\n         testConfig.setWritingSrc(true);\n         testConfig.setDeleteOP(true);\n-\n         testConfig.setWaitOn(WAIT.ON_ACK);\n-        startLogEntrySync(crossTables, WAIT.ON_ACK, false);\n+\n+        HashSet<WAIT> waitHashSet = new HashSet<>();\n+        waitHashSet.add(WAIT.ON_ACK);\n+        startLogEntrySync(crossTables, waitHashSet, true);\n \n         expectedAckTimestamp = Long.MAX_VALUE;\n-        // Verify Data on Destination site\n-        System.out.println(\"****** Verify Data on Destination\");\n+\n         // Because t2 is not specified as a replicated table, we should not see it on the destination\n         srcDataForVerification.get(t2).clear();\n \n+        // Verify Data on Destination site\n+        System.out.println(\"****** Wait Data on Destination\");\n+        waitData(dstCorfuTables, srcDataForVerification);", "originalCommit": "558d9bc783fa25db0791b58efbb4e33da951a052", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjI4Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459226282", "bodyText": "All you need is to set expectedAckMessages to the correct value.", "author": "annym", "createdAt": "2020-07-23T05:48:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjE4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTczMTI1Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459731256", "bodyText": "It is not accurate right. Even we know the value size, but not the MSR entry size in advance.", "author": "xiaoqin2012", "createdAt": "2020-07-23T21:13:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjE4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjMyMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459226322", "bodyText": "required?", "author": "annym", "createdAt": "2020-07-23T05:48:44Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -971,6 +998,8 @@ public void testLogEntrySyncWithTrim() throws Exception {\n         // Setup Environment: two corfu servers (source & destination)\n         setupEnv();\n \n+        log.info(\"Have setutEnv Done\");", "originalCommit": "558d9bc783fa25db0791b58efbb4e33da951a052", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTczMTcwMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459731702", "bodyText": "When the test goes wrong, we know the process and where it stuck.", "author": "xiaoqin2012", "createdAt": "2020-07-23T21:14:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjMyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjg4OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459226888", "bodyText": "droppingNum (not starting with caps)", "author": "annym", "createdAt": "2020-07-23T05:51:10Z", "path": "test/src/test/java/org/corfudb/integration/SourceForwardingDataSender.java", "diffHunk": "@@ -50,7 +50,9 @@\n \n     final static int DROP_INCREMENT = 4;\n \n-    private int firstDrop = DROP_INCREMENT;\n+    private int DroppingNum = 2;", "originalCommit": "558d9bc783fa25db0791b58efbb4e33da951a052", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ccaa7a42499718ded47259cfc73b1c68572706a7", "url": "https://github.com/CorfuDB/CorfuDB/commit/ccaa7a42499718ded47259cfc73b1c68572706a7", "message": "Address all the comments.", "committedDate": "2020-07-23T23:06:10Z", "type": "commit"}, {"oid": "83232ab1f7264f15b0deb2923ae3956b71348901", "url": "https://github.com/CorfuDB/CorfuDB/commit/83232ab1f7264f15b0deb2923ae3956b71348901", "message": "Fix compiling erros.", "committedDate": "2020-07-23T23:19:19Z", "type": "commit"}, {"oid": "11d8e5b78240c2f2468d656cf48ffc8c680aba86", "url": "https://github.com/CorfuDB/CorfuDB/commit/11d8e5b78240c2f2468d656cf48ffc8c680aba86", "message": "Merge branch 'log-replication-master' into xq/0720_msg_size_01", "committedDate": "2020-07-24T16:56:51Z", "type": "commit"}]}