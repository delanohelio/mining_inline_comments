{"pr_number": 1393, "pr_title": "Cloud compaction retries and throttling", "pr_createdAt": "2020-02-20T04:25:26Z", "pr_url": "https://github.com/linkedin/ambry/pull/1393", "timeline": [{"oid": "f98e670ea11a26bc009273bbcd4db0fce619c3b0", "url": "https://github.com/linkedin/ambry/commit/f98e670ea11a26bc009273bbcd4db0fce619c3b0", "message": "Add retries and throttling for cloud blob compaction", "committedDate": "2020-02-20T04:12:04Z", "type": "commit"}, {"oid": "3a8603f7c5aa982d22ff549170d173acaa43827c", "url": "https://github.com/linkedin/ambry/commit/3a8603f7c5aa982d22ff549170d173acaa43827c", "message": "Move doWithRetries to CloudRequestAgent class", "committedDate": "2020-02-20T04:22:25Z", "type": "commit"}, {"oid": "5281695d8019200c18a4a20c4bf0e69bc2e647a3", "url": "https://github.com/linkedin/ambry/commit/5281695d8019200c18a4a20c4bf0e69bc2e647a3", "message": "Fixed AzureIntegrationTest", "committedDate": "2020-02-21T19:02:26Z", "type": "commit"}, {"oid": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017", "url": "https://github.com/linkedin/ambry/commit/9c181d83c21c8a6be0872dfd74d0d87ad7bd0017", "message": "Make one mock lenient to fix test exceptions", "committedDate": "2020-02-21T20:53:45Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzMjczOA==", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382832738", "bodyText": "what is the format of results here? Instead of findFirst could you get the relevant entry by looking up its key?", "author": "cgtz", "createdAt": "2020-02-21T22:17:30Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -155,6 +157,28 @@ void testConnectivity() {\n     }\n   }\n \n+  /**\n+   * Get the number of blobs in the specified partition matching the specified DocumentDB query.\n+   * @param partitionPath the partition to query.\n+   * @param querySpec the DocumentDB query to execute.\n+   * @param timer the {@link Timer} to use to record query time (excluding waiting).\n+   * @return the number of matching blobs.\n+   */\n+  int countMetadata(String partitionPath, SqlQuerySpec querySpec, Timer timer)\n+      throws DocumentClientException {\n+    FeedOptions feedOptions = new FeedOptions();\n+    feedOptions.setPartitionKey(new PartitionKey(partitionPath));\n+    try {\n+      FeedResponse<Document> response = executeCosmosQuery(querySpec, feedOptions, timer).single();\n+      return ((Number) response.getResults().get(0).getHashMap().values().stream().findFirst().get()).intValue();", "originalCommit": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg1NDE1Mw==", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382854153", "bodyText": "The returned document is: {\"_aggregate\":20}\nI can change it to lookup by the key, it just involves ugly hardcoding and I could break if they decide to change the key name.", "author": "lightningrob", "createdAt": "2020-02-21T23:34:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzMjczOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg1NTQ1OQ==", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382855459", "bodyText": "Never mind, Cosmos does have a constant for it: Constants.Properties.AGGREGATE, so I'll make the change.", "author": "lightningrob", "createdAt": "2020-02-21T23:40:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzMjczOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzMzY0NQ==", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382833645", "bodyText": "let's make a github issue to track that this test needs a fix", "author": "cgtz", "createdAt": "2020-02-21T22:20:10Z", "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/AzureIntegrationTest.java", "diffHunk": "@@ -298,23 +269,58 @@ public void testPurgeDeadBlobs() throws Exception {\n           azureDest.uploadBlob(blobId, blobSize, cloudBlobMetadata, inputStream));\n     }\n \n-    // run getDeadBlobs query, should return 20\n+    // run getDeadBlobs query, should return 2 * bucketCount\n     String partitionPath = String.valueOf(testPartition);\n-    List<CloudBlobMetadata> deadBlobs = azureDest.getDeadBlobs(partitionPath);\n-    assertEquals(\"Unexpected number of dead blobs\", expectedDeadBlobs, deadBlobs.size());\n-\n-    logger.info(\"Running purge\");\n-    int numPurged = azureDest.purgeBlobs(deadBlobs);\n-    assertEquals(\"Not all blobs were purged\", expectedDeadBlobs, numPurged);\n+    assertEquals(\"Unexpected number of dead blobs\", expectedDeadBlobs, azureDest.getDeadBlobCount(partitionPath, now));\n+    logger.info(\"First call to getDeadBlobs\");\n+    List<CloudBlobMetadata> deadBlobs = azureDest.getDeadBlobs(partitionPath, now, bucketCount);\n+    assertEquals(\"Unexpected number returned\", bucketCount, deadBlobs.size());\n+    logger.info(\"First call to purge\");\n+    assertEquals(\"Not all blobs were purged\", bucketCount, azureDest.purgeBlobs(deadBlobs));\n+    logger.info(\"Second call to getDeadBlobs\");\n+    deadBlobs = azureDest.getDeadBlobs(partitionPath, now, bucketCount);\n+    assertEquals(\"Unexpected number returned\", bucketCount, deadBlobs.size());\n+    logger.info(\"Second call to purge\");\n+    assertEquals(\"Not all blobs were purged\", bucketCount, azureDest.purgeBlobs(deadBlobs));\n+    logger.info(\"Final call to getDeadBlobs\");\n+    deadBlobs = azureDest.getDeadBlobs(partitionPath, now, bucketCount);\n+    assertEquals(\"Expected zero\", 0, deadBlobs.size());\n     cleanup();\n   }\n \n   /**\n-   * Test findEntriesSince.\n-   * @throws Exception on error\n+   * Test findEntriesSince with CosmosUpdateTimeFindTokenFactory.\n+   */\n+  @Test\n+  public void testFindEntriesSinceByUpdateTime() throws Exception {\n+    testFindEntriesSince(\"com.github.ambry.cloud.azure.CosmosUpdateTimeFindTokenFactory\");\n+  }\n+\n+  /**\n+   * Test findEntriesSince with CosmosChangeFeedFindTokenFactory.\n    */\n+  @Ignore // Fails with wrong number of queries.", "originalCommit": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg1ODAyMg==", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382858022", "bodyText": "Good point.  Filed #1394 to track.", "author": "lightningrob", "createdAt": "2020-02-21T23:52:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzMzY0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzNTI1MQ==", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382835251", "bodyText": "do we need to sleep in between successive calls to getDeadBlobs, or do you feel that the cosmo's retry-with-backoff will be good enough for rate limiting this query?", "author": "cgtz", "createdAt": "2020-02-21T22:24:59Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudStorageCompactor.java", "diffHunk": "@@ -91,14 +99,25 @@ public int compactPartitions() {\n   /**\n    * Purge the inactive blobs in the specified partition.\n    * @param partitionPath the partition to compact.\n-   * @return the number of blobs purged.\n+   * @param cutoffTime the time at which a blob's active status should be evaluated.\n+   * @return the number of blobs purged or found.\n    */\n-  public int compactPartition(String partitionPath) throws CloudStorageException {\n-    List<CloudBlobMetadata> deadBlobs = cloudDestination.getDeadBlobs(partitionPath);\n-    if (!testMode) {\n-      return cloudDestination.purgeBlobs(deadBlobs);\n-    } else {\n-      return deadBlobs.size();\n+  public int compactPartition(String partitionPath, long cutoffTime) throws CloudStorageException {\n+    if (testMode) {\n+      return cloudDestination.getDeadBlobCount(partitionPath, cutoffTime);\n     }\n+\n+    int numDeadBlobs = 0;\n+    // Iterate until returned list size < limit or time runs out\n+    while (System.currentTimeMillis() < cutoffTime + compactionTimeLimitMs) {\n+      List<CloudBlobMetadata> deadBlobs =", "originalCommit": "9c181d83c21c8a6be0872dfd74d0d87ad7bd0017", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg1NTkzNw==", "url": "https://github.com/linkedin/ambry/pull/1393#discussion_r382855937", "bodyText": "There is an expensive call to cloudDestination.purgeBlobs() between calls to getDeadBlobs, and that should provide more than enough throttling between queries.", "author": "lightningrob", "createdAt": "2020-02-21T23:42:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgzNTI1MQ=="}], "type": "inlineReview"}, {"oid": "97a0c512583aee3bd07720ae8f1562dcc43417d8", "url": "https://github.com/linkedin/ambry/commit/97a0c512583aee3bd07720ae8f1562dcc43417d8", "message": "Address Casey's comments, use Cosmos property constant.", "committedDate": "2020-02-21T23:46:10Z", "type": "commit"}, {"oid": "f16cf0b61e26b3169c4f61173d32ad8ad297ca66", "url": "https://github.com/linkedin/ambry/commit/f16cf0b61e26b3169c4f61173d32ad8ad297ca66", "message": "Merge branch 'master' of github.com:linkedin/ambry into cloud-compaction-throttle", "committedDate": "2020-02-21T23:47:03Z", "type": "commit"}]}