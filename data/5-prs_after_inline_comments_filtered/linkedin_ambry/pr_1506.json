{"pr_number": 1506, "pr_title": "Break compaction query into smaller time buckets", "pr_createdAt": "2020-05-06T06:53:11Z", "pr_url": "https://github.com/linkedin/ambry/pull/1506", "timeline": [{"oid": "1114f1ce514441d954a4758fd56e8516101af009", "url": "https://github.com/linkedin/ambry/commit/1114f1ce514441d954a4758fd56e8516101af009", "message": "Break cloud compaction into smaller query buckets to avoid throttling", "committedDate": "2020-05-06T06:46:53Z", "type": "commit"}, {"oid": "f225dd78a34348f73ddea2b75125d5389522e841", "url": "https://github.com/linkedin/ambry/commit/f225dd78a34348f73ddea2b75125d5389522e841", "message": "Merge branch 'master' of github.com:linkedin/ambry into compactor-query-buckets", "committedDate": "2020-05-06T06:48:10Z", "type": "commit"}, {"oid": "63b5edcb847442a763f6a4f859ca58e2dba0cee6", "url": "https://github.com/linkedin/ambry/commit/63b5edcb847442a763f6a4f859ca58e2dba0cee6", "message": "Test fixes", "committedDate": "2020-05-06T07:23:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQwMzIwMg==", "url": "https://github.com/linkedin/ambry/pull/1506#discussion_r423403202", "bodyText": "Is this statement a TODO? We should mark it as such in that case.", "author": "ankagrawal", "createdAt": "2020-05-12T00:59:27Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -124,30 +128,30 @@ public int compactPartitions() {\n     long compactionStartTime = now;\n     long timeToQuit = now + compactionTimeLimitMs;\n     long queryEndTime = now - retentionPeriodMs;\n-    // TODO: we can cache the latest timestamp that we know we have cleared and use that on subsequent calls\n+    // TODO: checkpoint the latest timestamp and use that on subsequent compactions\n     // Starting from beginning of time is too expensive\n     // Order partitions by earliest time at which dead blob exists", "originalCommit": "63b5edcb847442a763f6a4f859ca58e2dba0cee6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3NTQ0Mg==", "url": "https://github.com/linkedin/ambry/pull/1506#discussion_r424075442", "bodyText": "I'm removing these comments.  I will file a ticket for the checkpointing enhancement", "author": "lightningrob", "createdAt": "2020-05-12T22:43:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQwMzIwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQwNDUwOA==", "url": "https://github.com/linkedin/ambry/pull/1506#discussion_r423404508", "bodyText": "We should move the logic that actually does the compaction (which is basically everything outside the if statement in line 218), in a separate method of its own, unless we have reason not to do so. Makes code easy to read, keeps methods small and separates out the control part of compaction logic from the actual compaction.", "author": "ankagrawal", "createdAt": "2020-05-12T01:04:27Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -212,6 +214,28 @@ public int compactPartition(String partitionPath, String fieldName, long querySt\n \n     // Iterate until returned list size < limit, time runs out or we get shut down\n     int totalPurged = 0;\n+    long chunkTimeRange = TimeUnit.DAYS.toMillis(queryBucketDays);\n+    if (queryEndTime - queryStartTime > chunkTimeRange) {\n+      logger.debug(\"Dividing compaction query for {} into buckets of {} days\", partitionPath, queryBucketDays);\n+      long chunkedStartTime = queryStartTime;\n+      while (chunkedStartTime < queryEndTime) {\n+        long chunkedEndTime = Math.min(chunkedStartTime + chunkTimeRange, queryEndTime);\n+        int numPurged = compactPartition(partitionPath, fieldName, chunkedStartTime, chunkedEndTime, timeToQuit);", "originalCommit": "63b5edcb847442a763f6a4f859ca58e2dba0cee6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3NzUzMA==", "url": "https://github.com/linkedin/ambry/pull/1506#discussion_r424077530", "bodyText": "I had it that way initially but then consolidated it into single method.  I can change it back if it helps readability.", "author": "lightningrob", "createdAt": "2020-05-12T22:49:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQwNDUwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQxMjAyOA==", "url": "https://github.com/linkedin/ambry/pull/1506#discussion_r423412028", "bodyText": "Can there be a case where we cannot complete compaction of all partitions in 24 hours?  In that case, it's possible that same that partitions that are at the end of the list of partitions, miss the compaction every time. There are atleast 2 ways we can approach this:\n\nIf instead of draining out each partition in a loop, we do n batches of compactPartittion calls for each partition, before looping through the list of partitions again, it will keep every partition compacted uniformly.\nAnother way we can solve this could be by setting cloudConfig.cloudBlobCompactionIntervalHours large enough so that this scenario can never happen. For this, it might be good to have metrics that can tell us how many times we were able to successfully loop through all partitions, and how many times we couldn't loop through all partitions.", "author": "ankagrawal", "createdAt": "2020-05-12T01:32:12Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -124,30 +128,30 @@ public int compactPartitions() {\n     long compactionStartTime = now;\n     long timeToQuit = now + compactionTimeLimitMs;\n     long queryEndTime = now - retentionPeriodMs;\n-    // TODO: we can cache the latest timestamp that we know we have cleared and use that on subsequent calls\n+    // TODO: checkpoint the latest timestamp and use that on subsequent compactions\n     // Starting from beginning of time is too expensive\n     // Order partitions by earliest time at which dead blob exists\n-    // Can start with retention period and go back additional retention periods until no more found\n-    long queryStartTime = 1;\n+    long queryStartTime = now - TimeUnit.DAYS.toMillis(lookbackDays);\n+    Date queryStartDate = new Date(queryStartTime);\n+    Date queryEndDate = new Date(queryEndTime);\n     int totalBlobsPurged = 0;\n     for (PartitionId partitionId : partitionsSnapshot) {\n       String partitionPath = partitionId.toPathString();\n       if (!partitions.contains(partitionId)) {\n         // Looks like partition was reassigned since the loop started, so skip it\n         continue;\n       }\n-      logger.info(\"Running compaction on partition {}\", partitionPath);\n+      logger.info(\"Compacting partition {} over time range {} - {}\", partitionPath, queryStartDate, queryEndDate);\n       try {\n-        // TODO: before compacting, call getOldestBlob to get queryStartTime\n         int numPurged =\n             compactPartition(partitionPath, CloudBlobMetadata.FIELD_DELETION_TIME, queryStartTime, queryEndTime,", "originalCommit": "63b5edcb847442a763f6a4f859ca58e2dba0cee6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3NjEwMw==", "url": "https://github.com/linkedin/ambry/pull/1506#discussion_r424076103", "bodyText": "I think the first suggestion is a good idea.  I will file a separate ticket to do that in a follow up PR.  We can limit each partition by either number of blobs cleared or number of days scanned.", "author": "lightningrob", "createdAt": "2020-05-12T22:45:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQxMjAyOA=="}], "type": "inlineReview"}, {"oid": "c31a42cba7b63485607d66a53e2f726f5b114d40", "url": "https://github.com/linkedin/ambry/commit/c31a42cba7b63485607d66a53e2f726f5b114d40", "message": "Address Ankur review comments", "committedDate": "2020-05-12T23:12:53Z", "type": "commit"}]}