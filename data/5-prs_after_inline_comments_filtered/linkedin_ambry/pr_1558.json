{"pr_number": 1558, "pr_title": "FindKey method takes FileSpan as the entire search space.", "pr_createdAt": "2020-06-10T02:49:19Z", "pr_url": "https://github.com/linkedin/ambry/pull/1558", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzMDQ1NA==", "url": "https://github.com/linkedin/ambry/pull/1558#discussion_r437830454", "bodyText": "now that findKey treats filespan as the search space, we don't need to compare it again here.", "author": "justinlin-linkedin", "createdAt": "2020-06-10T02:50:08Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -577,8 +569,7 @@ public void delete(List<MessageInfo> infosToDelete) throws StoreException {\n             IndexValue value = index.findKey(info.getStoreKey(), fileSpan,\n                 EnumSet.of(PersistentIndex.IndexEntryType.PUT, PersistentIndex.IndexEntryType.DELETE,\n                     PersistentIndex.IndexEntryType.UNDELETE));\n-            if (value != null && value.getOffset().compareTo(indexEndOffsetBeforeCheck) >= 0) {\n-              // Make sure the value is actually after the indexEndOffsetBeforeCheck\n+            if (value != null) {", "originalCommit": "3384a9c2b50fc49a67df33856c7be12d4517b1f2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzMjgwNw==", "url": "https://github.com/linkedin/ambry/pull/1558#discussion_r437832807", "bodyText": "Use end offset instead of start offset. Since we now changed how findKey treats filespan, the start offset will not make findKey to \"include the whole index segment in the search\" anymore.\nThis offset is persisted in the disk while compaction cycle is not finished, which means we have to deal with the old offset if we deploy the new version of ambry-server. Luckily for us, even if we treat the old start offset as the new end offset, it will only ignore some PUT records in the current compaction. When the next compaction kicks off, it will compact those PUT records eventually.\nFor example, assume we have two three index segment [0, 1000), [1000, 2000), [2000, 3000) and the cutoff offset on disk is 1000. Before this PR, 1000 means when findKey searches keys, it will search in the first and second index segment. After this PR, findKey will only search within the first segment and ignore the second segment. If there are delete records in the second segment, it will not be searched. So some PUT records will not see delete records while compacting. But this is fine, since when the next compaction starts, it will now use the end offset of the second, and all the delete records in the second index segment will be seen by the compaction.", "author": "justinlin-linkedin", "createdAt": "2020-06-10T02:59:11Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java", "diffHunk": "@@ -1178,9 +1178,7 @@ private Offset getStartOffsetOfLastIndexSegmentForDeleteCheck() {\n         long referenceTimeMs = compactionLog.getCompactionDetails().getReferenceTimeMs();\n         for (IndexSegment indexSegment : srcIndex.getIndexSegments().descendingMap().values()) {\n           if (indexSegment.getLastModifiedTimeMs() < referenceTimeMs) {\n-            // NOTE: using start offset here because of the way FileSpan is treated in PersistentIndex.findKey().\n-            // using this as the end offset for delete includes the whole index segment in the search.\n-            cutoffOffset = indexSegment.getStartOffset();\n+            cutoffOffset = indexSegment.getEndOffset();", "originalCommit": "5bceabec3856344188548ef7d0cb2fd7f9c02232", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzMjkwMQ==", "url": "https://github.com/linkedin/ambry/pull/1558#discussion_r437832901", "bodyText": "FileSpan is the search space now, if the space is empty, just return null.", "author": "justinlin-linkedin", "createdAt": "2020-06-10T02:59:35Z", "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "diffHunk": "@@ -575,6 +588,9 @@ IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> type\n    */\n   private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,\n       ConcurrentSkipListMap<Offset, IndexSegment> indexSegments) throws StoreException {\n+    if (fileSpan != null && fileSpan.isEmpty()) {", "originalCommit": "5bceabec3856344188548ef7d0cb2fd7f9c02232", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgzMzg4NQ==", "url": "https://github.com/linkedin/ambry/pull/1558#discussion_r437833885", "bodyText": "End offset in FileSpan is exclusive so we don't have to include the end offset. Assuming we have several index segments and the map of indexSegments look like this (it's offset to index segment).\n{\n0       :index segment1\n1000 :index segment2\n2000 :index segment3\n3000 :index segment4\n}\nWhen the filespan is [500, 2000), then we should only search on index segment1 and index segment2. index segment 3's start offset is 2000, but 2000 in filespan is exclusive, we don't need it.", "author": "justinlin-linkedin", "createdAt": "2020-06-10T03:03:28Z", "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "diffHunk": "@@ -587,7 +603,7 @@ private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryTy\n         logger.trace(\"Searching for {} in index with filespan ranging from {} to {}\", key, fileSpan.getStartOffset(),\n             fileSpan.getEndOffset());\n         segmentsMapToSearch = indexSegments.subMap(indexSegments.floorKey(fileSpan.getStartOffset()), true,\n-            indexSegments.floorKey(fileSpan.getEndOffset()), true).descendingMap();\n+            indexSegments.lowerKey(fileSpan.getEndOffset()), true).descendingMap();", "originalCommit": "5bceabec3856344188548ef7d0cb2fd7f9c02232", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4MjU5MQ==", "url": "https://github.com/linkedin/ambry/pull/1558#discussion_r439582591", "bodyText": "I looked at the callers of the this method, looks like they are still using [start, end] assumption. Any reason not change caller to [start, end+1)?", "author": "zzmao", "createdAt": "2020-06-12T18:35:18Z", "path": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java", "diffHunk": "@@ -575,6 +588,9 @@ IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> type\n    */\n   private IndexValue findKey(StoreKey key, FileSpan fileSpan, EnumSet<IndexEntryType> types,", "originalCommit": "1af2b3f55e144408a6454bb2c59f59ef40481b62", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4NzQyMA==", "url": "https://github.com/linkedin/ambry/pull/1558#discussion_r439587420", "bodyText": "I don't think the callers ever care about the inclusiveness of the end offset. For example, in PersistentIndex.getDeletedBlobReadOptions method, we call findKey with a FileSpan, whose start offset is the beginning of the index, and end offset if the offset of the DELETE record. In this case, it obviously doesn't want to include the end offset, the DELETE record.", "author": "justinlin-linkedin", "createdAt": "2020-06-12T18:46:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4MjU5MQ=="}], "type": "inlineReview"}, {"oid": "cc0b66b243d66677c07054a9bd5ce8bca4e23136", "url": "https://github.com/linkedin/ambry/commit/cc0b66b243d66677c07054a9bd5ce8bca4e23136", "message": "More tests", "committedDate": "2020-06-14T04:41:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQzOTg2OA==", "url": "https://github.com/linkedin/ambry/pull/1558#discussion_r442439868", "bodyText": "I feel like there shouldn't be issues here, but could you double check the queries that PersistentIndex.findEntriesSince() uses to make sure that the exclusive end offset is okay? I think those methods use getCurrentEndOffset which uses the end offset of the last index segment. Is IndexSegment.getEndOffset() inclusive or exclusive?", "author": "cgtz", "createdAt": "2020-06-18T19:02:10Z", "path": "ambry-store/src/main/java/com/github/ambry/store/FileSpan.java", "diffHunk": "@@ -53,7 +54,14 @@ Offset getEndOffset() {\n    * @return {@code true} if {@code offset} is in this {@link FileSpan} (start and end offsets are considered inclusive)\n    */\n   boolean inSpan(Offset offset) {\n-    return offset.compareTo(startOffset) >= 0 && offset.compareTo(endOffset) <= 0;\n+    return offset.compareTo(startOffset) >= 0 && offset.compareTo(endOffset) < 0;", "originalCommit": "cc0b66b243d66677c07054a9bd5ce8bca4e23136", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyNzI5MA==", "url": "https://github.com/linkedin/ambry/pull/1558#discussion_r442527290", "bodyText": "I have to double check with the code. But I want to point out something here, that end offset of index segment or log segment are kinda different than the end offset in the filespan.\nSay if we have a 10 puts in a log segment and each put is 1000 bytes, then the start offset of this log segment is 18 (with header), and the end offset is 1000 * 10 + 18 = 10018, which is expected.  Of course, this log segment would have index segment(s). Let's say, now we want to search within [18, 10018], both inclusive, we can have a filespan with start offset = 18 and end offset = 10018. Wait, the end offset in filespan is not inclusive, would this be a problem? Actually no. The 10 Puts, their offsets are 18, 1018, 2018, 3018... 9018 respectively. Even if we don't include 10018, we effectively covers all 10 puts here.\nAs you can see, the end offset of a log segment is greater than the offset of LAST RECORD in the segment, so an exclusive end offset in filespan can still cover all the records. (No records would be a zero-byte record)\nThis is also quite convenient for boundaries. There are two examples I want to share here.\n\nSay we have a DELETE index value, and we have to find the corresponding PUT for this key. Then we can call index.findKey(key, new FileSpan(index.getStartOffset(), deleteValue.getOffset(), EnumSet.of(PersistentIndex.IndexEntryType.PUT)). The filespan's start offset is the start offset of the index, and the end offset is the DELETE's offset. Since the end offset is not included, so the DELETE record will be excluded from the search.\nSay in blobstore, we have indexEndOffsetBeforeCheck and we want to do a findKey up to that offset. If we take a log segment above as an example, the indexEndOffsetBeforeCheck would be 10018 now. If before we call findKey, there is another put being inserted, then this last put's offset would be 10018, and in findKey, it will be excluded.", "author": "justinlin-linkedin", "createdAt": "2020-06-18T22:04:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQzOTg2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM1OTM1Mw==", "url": "https://github.com/linkedin/ambry/pull/1558#discussion_r449359353", "bodyText": "minor: has to follow ...", "author": "jsjtzyy", "createdAt": "2020-07-03T03:34:51Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -574,16 +566,24 @@ public void delete(List<MessageInfo> infosToDelete) throws StoreException {\n           FileSpan fileSpan = new FileSpan(indexEndOffsetBeforeCheck, currentIndexEndOffset);\n           int i = 0;\n           for (MessageInfo info : infosToDelete) {\n-            IndexValue value = index.findKey(info.getStoreKey(), fileSpan,\n-                EnumSet.of(PersistentIndex.IndexEntryType.PUT, PersistentIndex.IndexEntryType.DELETE,\n-                    PersistentIndex.IndexEntryType.UNDELETE));\n-            if (value != null && value.getOffset().compareTo(indexEndOffsetBeforeCheck) >= 0) {\n-              // Make sure the value is actually after the indexEndOffsetBeforeCheck\n-              if (value.isDelete() && value.getLifeVersion() == lifeVersions.get(i)) {\n-                throw new StoreException(\n-                    \"Cannot delete id \" + info.getStoreKey() + \" since it is already deleted in the index.\",\n-                    StoreErrorCodes.ID_Deleted);\n+            IndexValue value =\n+                index.findKey(info.getStoreKey(), fileSpan, EnumSet.allOf(PersistentIndex.IndexEntryType.class));\n+            if (value != null) {\n+              // There are several possible cases that can exist here. Delete has be follow either PUT, TTL_UPDATE or UNDELETE.", "originalCommit": "cc0b66b243d66677c07054a9bd5ce8bca4e23136", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM2NTMxNA==", "url": "https://github.com/linkedin/ambry/pull/1558#discussion_r449365314", "bodyText": "I need a little clarification of this if-else block. The if branch makes sense to me but for the else branch, what if the value.isDelete() == true,  it seems to throw exception directly.  Actually this might be a valid case.\n(Correct me if I am wrong and let me know anything I missed here)", "author": "jsjtzyy", "createdAt": "2020-07-03T04:06:34Z", "path": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java", "diffHunk": "@@ -801,8 +803,7 @@ public short undelete(MessageInfo info) throws StoreException {\n           FileSpan fileSpan = new FileSpan(indexEndOffsetBeforeCheck, currentIndexEndOffset);\n           IndexValue value = index.findKey(info.getStoreKey(), fileSpan,\n               EnumSet.of(PersistentIndex.IndexEntryType.DELETE, PersistentIndex.IndexEntryType.UNDELETE));\n-          if (value != null && value.getOffset().compareTo(indexEndOffsetBeforeCheck) >= 0) {\n-            // Make sure the value is actually after the indexEndOffsetBeforeCheck\n+          if (value != null) {\n             if (value.isUndelete() && value.getLifeVersion() == revisedLifeVersion) {", "originalCommit": "cc0b66b243d66677c07054a9bd5ce8bca4e23136", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc3MzkxNQ==", "url": "https://github.com/linkedin/ambry/pull/1558#discussion_r459773915", "bodyText": "It's not a valid delete. If we are here, then the previous record should be DELETE. For example, if the index values for this blob is PUT, DELETEv0. Then this UNDELETE's lifeVersion should be 1. so in if statement, we see a UNDELETEv1 out of the previous scope, this UNDELETEv1 might come from replication, this is valid. But if it's not UNDELETEv1, then all the alternatives should be invalid. For instance, UNDELETEv2 is invalid since when the undelete method is invoked, the DELETE is at version 0. DELETEv2 is also invalid since we don't really know the order of DELETEv2 and UNDELETEv1 anymore. Imagine that this host some how freezes for a long time, and within this period, client send a DELETE, UNDELETE and DELETE. So in other host, the final state is DELETE, if we append UNDELETE after this DELETE, then we change the final state. This is not right.\nBasically what I am doing here is to make sure that only same UNDELETE from replication can be valid.", "author": "justinlin-linkedin", "createdAt": "2020-07-23T23:00:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM2NTMxNA=="}], "type": "inlineReview"}, {"oid": "4e310aab44b9cfbb202a334b9e1fb695e06d1558", "url": "https://github.com/linkedin/ambry/commit/4e310aab44b9cfbb202a334b9e1fb695e06d1558", "message": "Comments", "committedDate": "2020-07-23T23:00:44Z", "type": "forcePushed"}, {"oid": "50917d097902859c0399d244ed31bdb62e21e7c3", "url": "https://github.com/linkedin/ambry/commit/50917d097902859c0399d244ed31bdb62e21e7c3", "message": "FindKey method takes FileSpan as the entire search space.", "committedDate": "2020-08-06T19:53:43Z", "type": "commit"}, {"oid": "374e7ac0d4a1a4540fda8ef9bcde7dd312c89555", "url": "https://github.com/linkedin/ambry/commit/374e7ac0d4a1a4540fda8ef9bcde7dd312c89555", "message": "typo", "committedDate": "2020-08-06T19:53:43Z", "type": "commit"}, {"oid": "b484d7aa752bdd0d055e4fed61e4d84f43f0d558", "url": "https://github.com/linkedin/ambry/commit/b484d7aa752bdd0d055e4fed61e4d84f43f0d558", "message": "Typo", "committedDate": "2020-08-06T19:53:43Z", "type": "commit"}, {"oid": "ccc75ca507783b2bba0b1eb879d5be00ab903cd7", "url": "https://github.com/linkedin/ambry/commit/ccc75ca507783b2bba0b1eb879d5be00ab903cd7", "message": "More tests", "committedDate": "2020-08-06T19:53:43Z", "type": "commit"}, {"oid": "9c9c68b964bdecbb810cd8cdf4e439c345b36991", "url": "https://github.com/linkedin/ambry/commit/9c9c68b964bdecbb810cd8cdf4e439c345b36991", "message": "Comments", "committedDate": "2020-08-06T19:53:43Z", "type": "commit"}, {"oid": "9c9c68b964bdecbb810cd8cdf4e439c345b36991", "url": "https://github.com/linkedin/ambry/commit/9c9c68b964bdecbb810cd8cdf4e439c345b36991", "message": "Comments", "committedDate": "2020-08-06T19:53:43Z", "type": "forcePushed"}]}