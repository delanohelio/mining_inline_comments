{"pr_number": 1418, "pr_title": "Split compaction query into deletion/expiration", "pr_createdAt": "2020-03-06T05:24:10Z", "pr_url": "https://github.com/linkedin/ambry/pull/1418", "timeline": [{"oid": "c77f920c8b5da78550d535c34aa5ed7b4dcd88b4", "url": "https://github.com/linkedin/ambry/commit/c77f920c8b5da78550d535c34aa5ed7b4dcd88b4", "message": "Split compaction query into deletion/expiration\nAdd config properties for query batch size etc", "committedDate": "2020-03-06T05:20:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ2MTE4Mg==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r389461182", "bodyText": "Minor: To be consistent with java doc of doWithRetries method, I think last parameter should be cloudBlobMetadata.getPartitionId().toPathString().", "author": "jsjtzyy", "createdAt": "2020-03-09T04:44:13Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudBlobStore.java", "diffHunk": "@@ -184,14 +185,14 @@ void downloadBlob(CloudBlobMetadata cloudBlobMetadata, BlobId blobId, OutputStre\n         requestAgent.doWithRetries(() -> {\n           cloudDestination.downloadBlob(blobId, new ByteBufferOutputStream(encryptedBlob));\n           return null;\n-        }, \"Download\");\n+        }, \"Download\", cloudBlobMetadata.getPartitionId());", "originalCommit": "c77f920c8b5da78550d535c34aa5ed7b4dcd88b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTkzNjEwMg==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r389936102", "bodyText": "This is CloudBlobMetadata not BlobId, and  getPartitionId() already returns the partition id string.", "author": "lightningrob", "createdAt": "2020-03-09T20:15:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ2MTE4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ2MTIxMg==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r389461212", "bodyText": "same here", "author": "jsjtzyy", "createdAt": "2020-03-09T04:44:24Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudBlobStore.java", "diffHunk": "@@ -184,14 +185,14 @@ void downloadBlob(CloudBlobMetadata cloudBlobMetadata, BlobId blobId, OutputStre\n         requestAgent.doWithRetries(() -> {\n           cloudDestination.downloadBlob(blobId, new ByteBufferOutputStream(encryptedBlob));\n           return null;\n-        }, \"Download\");\n+        }, \"Download\", cloudBlobMetadata.getPartitionId());\n         ByteBuffer decryptedBlob = cryptoAgent.decrypt(encryptedBlob);\n         outputStream.write(decryptedBlob.array());\n       } else {\n         requestAgent.doWithRetries(() -> {\n           cloudDestination.downloadBlob(blobId, outputStream);\n           return null;\n-        }, \"Download\");\n+        }, \"Download\", cloudBlobMetadata.getPartitionId());", "originalCommit": "c77f920c8b5da78550d535c34aa5ed7b4dcd88b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ2MjU0Ng==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r389462546", "bodyText": "typo: occurred", "author": "jsjtzyy", "createdAt": "2020-03-09T04:51:58Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudBlobStore.java", "diffHunk": "@@ -184,14 +185,14 @@ void downloadBlob(CloudBlobMetadata cloudBlobMetadata, BlobId blobId, OutputStre\n         requestAgent.doWithRetries(() -> {\n           cloudDestination.downloadBlob(blobId, new ByteBufferOutputStream(encryptedBlob));\n           return null;\n-        }, \"Download\");\n+        }, \"Download\", cloudBlobMetadata.getPartitionId());\n         ByteBuffer decryptedBlob = cryptoAgent.decrypt(encryptedBlob);\n         outputStream.write(decryptedBlob.array());\n       } else {\n         requestAgent.doWithRetries(() -> {\n           cloudDestination.downloadBlob(blobId, outputStream);\n           return null;\n-        }, \"Download\");\n+        }, \"Download\", cloudBlobMetadata.getPartitionId());\n       }\n     } catch (CloudStorageException | GeneralSecurityException | IOException e) {\n       throw new StoreException(\"Error occured in downloading blob for blobid :\" + blobId, StoreErrorCodes.IOError);", "originalCommit": "c77f920c8b5da78550d535c34aa5ed7b4dcd88b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ2Mjc3Mw==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r389462773", "bodyText": "add java doc for this method please (I am trying to understand what does Internal mean in the name of this method)", "author": "jsjtzyy", "createdAt": "2020-03-09T04:53:23Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudBlobStore.java", "diffHunk": "@@ -316,7 +317,7 @@ private void putBlob(MessageInfo messageInfo, ByteBuffer messageBuf, long size)\n   private void uploadBlobInternal(BlobId blobId, long bufferlen, CloudBlobMetadata blobMetadata,\n       InputStream inputStream) throws CloudStorageException {\n     requestAgent.doWithRetries(() -> cloudDestination.uploadBlob(blobId, bufferlen, blobMetadata, inputStream),\n-        \"Upload\");\n+        \"Upload\", partitionId.toPathString());", "originalCommit": "c77f920c8b5da78550d535c34aa5ed7b4dcd88b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTk1MTQyOQ==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r389951429", "bodyText": "That method didn't really add anything, so I removed it.", "author": "lightningrob", "createdAt": "2020-03-09T20:46:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ2Mjc3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ2MjkwMQ==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r389462901", "bodyText": "nit: bufferlen -> bufferLen", "author": "jsjtzyy", "createdAt": "2020-03-09T04:54:00Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudBlobStore.java", "diffHunk": "@@ -316,7 +317,7 @@ private void putBlob(MessageInfo messageInfo, ByteBuffer messageBuf, long size)\n   private void uploadBlobInternal(BlobId blobId, long bufferlen, CloudBlobMetadata blobMetadata,", "originalCommit": "c77f920c8b5da78550d535c34aa5ed7b4dcd88b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ2Nzk0OQ==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r389467949", "bodyText": "nit: referencing the deleted blobs", "author": "jsjtzyy", "createdAt": "2020-03-09T05:23:15Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudDestination.java", "diffHunk": "@@ -74,26 +74,28 @@ boolean uploadBlob(BlobId blobId, long inputLength, CloudBlobMetadata cloudBlobM\n   Map<String, CloudBlobMetadata> getBlobMetadata(List<BlobId> blobIds) throws CloudStorageException;\n \n   /**\n-   * Get the list of blobs in the specified partition that have been deleted or expired for at least the\n+   * Get the list of blobs in the specified partition that have been deleted for at least the\n    * configured retention period.\n    * @param partitionPath the partition to query.\n    * @param cutoffTime the cutoff time for the query time range.\n    * @param maxEntries the max number of metadata records to return.\n    * @return a List of {@link CloudBlobMetadata} referencing the dead blobs found.", "originalCommit": "c77f920c8b5da78550d535c34aa5ed7b4dcd88b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ2ODAwOQ==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r389468009", "bodyText": "nit: referencing the expired blobs.", "author": "jsjtzyy", "createdAt": "2020-03-09T05:23:35Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudDestination.java", "diffHunk": "@@ -74,26 +74,28 @@ boolean uploadBlob(BlobId blobId, long inputLength, CloudBlobMetadata cloudBlobM\n   Map<String, CloudBlobMetadata> getBlobMetadata(List<BlobId> blobIds) throws CloudStorageException;\n \n   /**\n-   * Get the list of blobs in the specified partition that have been deleted or expired for at least the\n+   * Get the list of blobs in the specified partition that have been deleted for at least the\n    * configured retention period.\n    * @param partitionPath the partition to query.\n    * @param cutoffTime the cutoff time for the query time range.\n    * @param maxEntries the max number of metadata records to return.\n    * @return a List of {@link CloudBlobMetadata} referencing the dead blobs found.\n    * @throws CloudStorageException\n    */\n-  List<CloudBlobMetadata> getDeadBlobs(String partitionPath, long cutoffTime, int maxEntries)\n+  List<CloudBlobMetadata> getDeletedBlobs(String partitionPath, long cutoffTime, int maxEntries)\n       throws CloudStorageException;\n \n   /**\n-   * Get the number of blobs in the specified partition that have been deleted or expired for at least the\n+   * Get the list of blobs in the specified partition that have been expired for at least the\n    * configured retention period.\n    * @param partitionPath the partition to query.\n    * @param cutoffTime the cutoff time for the query time range.\n-   * @return the number of dead blobs found.\n+   * @param maxEntries the max number of metadata records to return.\n+   * @return a List of {@link CloudBlobMetadata} referencing the dead blobs found.", "originalCommit": "c77f920c8b5da78550d535c34aa5ed7b4dcd88b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg2OTg0NQ==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r389869845", "bodyText": "please add java docs to these methods", "author": "jsjtzyy", "createdAt": "2020-03-09T18:08:53Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudStorageCompactor.java", "diffHunk": "@@ -96,28 +99,53 @@ public int compactPartitions() {\n     return totalBlobsPurged;\n   }\n \n+  public CloudBlobMetadata getOldestExpiredBlob(String partitionPath) throws CloudStorageException {\n+    List<CloudBlobMetadata> deadBlobs = requestAgent.doWithRetries(\n+        () -> cloudDestination.getExpiredBlobs(partitionPath, System.currentTimeMillis(), queryLimit), \"GetDeadBlobs\",\n+        partitionPath);\n+    return deadBlobs.isEmpty() ? null : deadBlobs.get(0);\n+  }\n+\n+  public CloudBlobMetadata getOldestDeletedBlob(String partitionPath) throws CloudStorageException {", "originalCommit": "c77f920c8b5da78550d535c34aa5ed7b4dcd88b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg5NDgyMQ==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r389894821", "bodyText": "Looks like DEFAULT_COSMOS_MAX_RETRIES is never used for now. Will it be adopted in future PR?", "author": "jsjtzyy", "createdAt": "2020-03-09T18:54:44Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureCloudConfig.java", "diffHunk": "@@ -29,12 +28,20 @@\n   public static final String COSMOS_COLLECTION_LINK = \"cosmos.collection.link\";\n   public static final String COSMOS_KEY = \"cosmos.key\";\n   public static final String COSMOS_DIRECT_HTTPS = \"cosmos.direct.https\";\n+  public static final String COSMOS_QUERY_BATCH_SIZE = \"cosmos.query.batch.size\";\n+  public static final String COSMOS_REQUEST_CHARGE_THRESHOLD = \"cosmos.request.charge.threshold\";\n+  public static final String COSMOS_CONTINUATION_TOKEN_LIMIT = \"cosmos.continuation.token.limit\";\n   public static final String AZURE_PURGE_BATCH_SIZE = \"azure.purge.batch.size\";\n   public static final String AZURE_NAME_SCHEME_VERSION = \"azure.name.scheme.version\";\n   public static final String AZURE_BLOB_CONTAINER_STRATEGY = \"azure.blob.container.strategy\";\n   // Per docs.microsoft.com/en-us/rest/api/storageservices/blob-batch\n   public static final int MAX_PURGE_BATCH_SIZE = 256;\n   public static final int DEFAULT_PURGE_BATCH_SIZE = 100;\n+  public static final int DEFAULT_QUERY_BATCH_SIZE = 100;\n+  public static final int DEFAULT_COSMOS_MAX_RETRIES = 5;", "originalCommit": "c77f920c8b5da78550d535c34aa5ed7b4dcd88b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTk1MTA1NQ==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r389951055", "bodyText": "Removed", "author": "lightningrob", "createdAt": "2020-03-09T20:45:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg5NDgyMQ=="}], "type": "inlineReview"}, {"oid": "31756543eab6548113ef62088e0f57ea1c00c955", "url": "https://github.com/linkedin/ambry/commit/31756543eab6548113ef62088e0f57ea1c00c955", "message": "Address Yingyi's review comments.", "committedDate": "2020-03-09T20:43:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTk1MjI3Mw==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r389952273", "bodyText": "should the query limit be 1 here and in the method below since you are only interested in a single blob?", "author": "cgtz", "createdAt": "2020-03-09T20:47:32Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/CloudStorageCompactor.java", "diffHunk": "@@ -96,28 +99,65 @@ public int compactPartitions() {\n     return totalBlobsPurged;\n   }\n \n+  /**\n+   * Returns the expired blob in the specified partition with the earliest expiration time.\n+   * @param partitionPath the partition to check.\n+   * @return the {@link CloudBlobMetadata} for the expired blob, or NULL if none was found.\n+   * @throws CloudStorageException\n+   */\n+  public CloudBlobMetadata getOldestExpiredBlob(String partitionPath) throws CloudStorageException {\n+    List<CloudBlobMetadata> deadBlobs = requestAgent.doWithRetries(\n+        () -> cloudDestination.getExpiredBlobs(partitionPath, System.currentTimeMillis(), queryLimit), \"GetDeadBlobs\",", "originalCommit": "31756543eab6548113ef62088e0f57ea1c00c955", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMzOTEyMA==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r391339120", "bodyText": "Yes, changed.", "author": "lightningrob", "createdAt": "2020-03-12T00:12:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTk1MjI3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA4Mjg0Mw==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r391082843", "bodyText": "Could you put the unit in the config name: cosmosContinuationTokenLimitKB", "author": "cgtz", "createdAt": "2020-03-11T16:03:19Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/AzureCloudConfig.java", "diffHunk": "@@ -74,6 +80,24 @@\n   @Default(\"Partition\")\n   public final String azureBlobContainerStrategy;\n \n+  /**\n+   * Max number of metadata records to fetch in a single Cosmos query.\n+   */\n+  @Config(COSMOS_QUERY_BATCH_SIZE)\n+  public final int cosmosQueryBatchSize;\n+\n+  /**\n+   * The size limit in KB on Cosmos continuation token.\n+   */\n+  @Config(COSMOS_CONTINUATION_TOKEN_LIMIT)\n+  public final int cosmosContinuationTokenLimit;", "originalCommit": "31756543eab6548113ef62088e0f57ea1c00c955", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA4MzU5Nw==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r391083597", "bodyText": "What happens when the continuation token gets bigger than this limit? Does the query iterator stop returning new values until another query is made?", "author": "cgtz", "createdAt": "2020-03-11T16:04:26Z", "path": "ambry-cloud/src/main/java/com.github.ambry.cloud/azure/CosmosDataAccessor.java", "diffHunk": "@@ -169,54 +208,118 @@ void testConnectivity() {\n   }\n \n   /**\n-   * Get the list of blobs in the specified partition matching the specified DocumentDB query.\n+   * Get the list of blobs in the specified partition that have been deleted or expired for at least the\n+   * configured retention period.\n    * @param partitionPath the partition to query.\n-   * @param querySpec the DocumentDB query to execute.\n-   * @param timer the {@link Timer} to use to record query time (excluding waiting).\n-   * @return a List of {@link CloudBlobMetadata} referencing the matching blobs.\n+   * @param fieldName the field name to query on. Allowed values are {@link CloudBlobMetadata#FIELD_DELETION_TIME} and\n+   *                  {@link CloudBlobMetadata#FIELD_EXPIRATION_TIME}.\n+   * @param retentionThreshold the latest time where blobs are considered dead if they were expired\n+   *                           or deleted before that point.\n+   * @param maxEntries the max number of metadata records to return.\n+   * @return a List of {@link CloudBlobMetadata} referencing the dead blobs found.\n+   * @throws CloudStorageException\n    */\n-  List<CloudBlobMetadata> queryMetadata(String partitionPath, SqlQuerySpec querySpec, Timer timer)\n+  List<CloudBlobMetadata> getDeadBlobs(String partitionPath, String fieldName, long retentionThreshold, int maxEntries)\n       throws DocumentClientException {\n-    azureMetrics.documentQueryCount.inc();\n+\n+    String deadBlobsQuery;\n+    if (fieldName.equals(CloudBlobMetadata.FIELD_DELETION_TIME)) {\n+      deadBlobsQuery = DELETED_BLOBS_QUERY;\n+    } else if (fieldName.equals(CloudBlobMetadata.FIELD_EXPIRATION_TIME)) {\n+      deadBlobsQuery = EXPIRED_BLOBS_QUERY;\n+    } else {\n+      throw new IllegalArgumentException(\"Invalid field: \" + fieldName);\n+    }\n+    SqlQuerySpec querySpec = new SqlQuerySpec(deadBlobsQuery,\n+        new SqlParameterCollection(new SqlParameter(LIMIT_PARAM, maxEntries),\n+            new SqlParameter(THRESHOLD_PARAM, retentionThreshold)));\n+\n     FeedOptions feedOptions = new FeedOptions();\n+    feedOptions.setMaxItemCount(maxEntries);\n+    feedOptions.setResponseContinuationTokenLimitInKb(continuationTokenLimit);", "originalCommit": "31756543eab6548113ef62088e0f57ea1c00c955", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMzOTY2Nw==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r391339667", "bodyText": "No, acc to MS it just makes subsequent queries (during paging) less efficient if it can't stuff as much state in the token.  Doubt it will affect us much.", "author": "lightningrob", "createdAt": "2020-03-12T00:14:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA4MzU5Nw=="}], "type": "inlineReview"}, {"oid": "48c581e60c92ecba3e9fffc66920be9bfe6cf13e", "url": "https://github.com/linkedin/ambry/commit/48c581e60c92ecba3e9fffc66920be9bfe6cf13e", "message": "Changed dead blobs queries to use adjustable start time (reduce request charge).\nAddress Casey's review comments.", "committedDate": "2020-03-12T00:09:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM3NzQwMA==", "url": "https://github.com/linkedin/ambry/pull/1418#discussion_r391377400", "bodyText": "Can we remove this?", "author": "jsjtzyy", "createdAt": "2020-03-12T03:00:04Z", "path": "ambry-cloud/src/test/java/com.github.ambry.cloud/azure/AzureIntegrationTest.java", "diffHunk": "@@ -308,7 +312,7 @@ public void testFindEntriesSinceByUpdateTime() throws Exception {\n   /**\n    * Test findEntriesSince with CosmosChangeFeedFindTokenFactory.\n    */\n-  @Ignore // Fails with wrong number of queries.\n+  //@Ignore // Fails with wrong number of queries.", "originalCommit": "48c581e60c92ecba3e9fffc66920be9bfe6cf13e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "fabef6d6ebc973ff1852050353a54c111f1c885f", "url": "https://github.com/linkedin/ambry/commit/fabef6d6ebc973ff1852050353a54c111f1c885f", "message": "Minor cleanup", "committedDate": "2020-03-12T06:27:25Z", "type": "commit"}]}