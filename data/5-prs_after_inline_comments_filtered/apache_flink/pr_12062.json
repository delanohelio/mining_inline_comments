{"pr_number": 12062, "pr_title": "[FLINK-17587][filesystem] Filesystem streaming sink support commit success file", "pr_createdAt": "2020-05-11T02:10:15Z", "pr_url": "https://github.com/apache/flink/pull/12062", "timeline": [{"oid": "8ea1504837c815c55f8a41a7fa887285333c62a6", "url": "https://github.com/apache/flink/commit/8ea1504837c815c55f8a41a7fa887285333c62a6", "message": "Fix", "committedDate": "2020-05-11T06:11:36Z", "type": "forcePushed"}, {"oid": "e5127c6a73d24f0c9c03deb0a7518fb18832b744", "url": "https://github.com/apache/flink/commit/e5127c6a73d24f0c9c03deb0a7518fb18832b744", "message": "Rebase", "committedDate": "2020-05-11T08:54:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjgzNjAyOA==", "url": "https://github.com/apache/flink/pull/12062#discussion_r422836028", "bodyText": "Why do we need this change?", "author": "lirui-apache", "createdAt": "2020-05-11T07:26:43Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java", "diffHunk": "@@ -467,7 +483,11 @@ public void onProcessingTime(long timestamp) throws Exception {\n \n \t@Override\n \tpublic void invoke(IN value, SinkFunction.Context context) throws Exception {\n-\t\tbuckets.onElement(value, context);\n+\t\tbuckets.onElement(", "originalCommit": "8ea1504837c815c55f8a41a7fa887285333c62a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ0ODk0Nw==", "url": "https://github.com/apache/flink/pull/12062#discussion_r423448947", "bodyText": "Will remove.", "author": "JingsongLi", "createdAt": "2020-05-12T04:04:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjgzNjAyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjkxMjEyNQ==", "url": "https://github.com/apache/flink/pull/12062#discussion_r422912125", "bodyText": "add descriptions", "author": "lirui-apache", "createdAt": "2020-05-11T09:35:58Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+\n+import java.time.Duration;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+\n+/**\n+ * This class holds configuration constants used by filesystem(Including hive) connector.\n+ */\n+public class FileSystemOptions {\n+\n+\tpublic static final ConfigOption<String> PARTITION_TIME_EXTRACTOR_TYPE =\n+\t\t\tkey(\"partition.time-extractor.type\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"default\")\n+\t\t\t\t\t.withDescription(\"Time extractor to extract time from partition values. Only be\" +\n+\t\t\t\t\t\t\t\" used if order is set to partition-time. Support default and custom.\" +\n+\t\t\t\t\t\t\t\" For default, can configure timestamp pattern.\" +\n+\t\t\t\t\t\t\t\" For custom, should configure extractor class.\");\n+\n+\tpublic static final ConfigOption<String> PARTITION_TIME_EXTRACTOR_CLASS =\n+\t\t\tkey(\"partition.time-extractor.class\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"The extractor class for implement PartitionTimeExtractor interface.\");\n+\n+\tpublic static final ConfigOption<String> PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN =\n+\t\t\tkey(\"partition.time-extractor.timestamp-pattern\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"The 'default' construction way allows users to use partition\" +\n+\t\t\t\t\t\t\t\" fields to get a legal timestamp pattern.\" +\n+\t\t\t\t\t\t\t\" Default support 'yyyy-mm-dd hh:mm:ss' from first field.\" +\n+\t\t\t\t\t\t\t\" If timestamp in partition is single field 'dt', can configure: '$dt'.\" +\n+\t\t\t\t\t\t\t\" If timestamp in partition is year, month, day, hour,\" +\n+\t\t\t\t\t\t\t\" can configure: '$year-$month-$day $hour:00:00'.\" +\n+\t\t\t\t\t\t\t\" If timestamp in partition is dt and hour, can configure: '$dt $hour:00:00'.\");\n+\n+\tpublic static final ConfigOption<Duration> PARTITION_TIME_INTERVAL =\n+\t\t\tkey(\"partition.time-interval\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"Interval time of partition,\" +\n+\t\t\t\t\t\t\t\" if it is a day partition, should be '1 d',\" +\n+\t\t\t\t\t\t\t\" if it is a hour partition, should be '1 h'\");\n+\n+\tpublic static final ConfigOption<String> PARTITION_COMMIT_POLICY_TYPE =\n+\t\t\tkey(\"partition.commit-policy.type\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"\");", "originalCommit": "e5127c6a73d24f0c9c03deb0a7518fb18832b744", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk4OTMwNA==", "url": "https://github.com/apache/flink/pull/12062#discussion_r422989304", "bodyText": "Can we have some comments about which partitions should be in this list? My understanding is it should include partitions for which some files should be committed, right?", "author": "lirui-apache", "createdAt": "2020-05-11T12:01:15Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.filesystem.TableMetaStoreFactory;\n+\n+import java.io.Serializable;\n+import java.util.HashSet;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_COMMIT_POLICY_SUCCESS_FILE_NAME;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_COMMIT_POLICY_TYPE;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+import static org.apache.flink.table.utils.PartitionPathUtils.generatePartitionPath;\n+\n+/**\n+ * Committer for {@link StreamingFileWriter}. This is the single (non-parallel) task.\n+ */\n+public class StreamingFileCommitter extends AbstractStreamOperator<Void>\n+\t\timplements OneInputStreamOperator<StreamingFileCommitter.CommitMessage, Void> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final Configuration conf;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final TableMetaStoreFactory metaStoreFactory;\n+\n+\tprivate transient PartitionCommitManager commitManager;\n+\n+\tprivate transient TaskTracker taskTracker;\n+\n+\tprivate transient long currentWatermark = Long.MIN_VALUE;\n+\n+\tprivate transient List<PartitionCommitPolicy> policies;\n+\n+\tpublic StreamingFileCommitter(\n+\t\t\tList<String> partitionKeys, TableMetaStoreFactory metaStoreFactory, Configuration conf) {\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.metaStoreFactory = metaStoreFactory;\n+\t\tthis.conf = conf;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(StateInitializationContext context) throws Exception {\n+\t\tsuper.initializeState(context);\n+\t\tthis.commitManager = new PartitionCommitManager(\n+\t\t\t\tcontext.isRestored(),\n+\t\t\t\tcontext.getOperatorStateStore(),\n+\t\t\t\tgetUserCodeClassloader(),\n+\t\t\t\tpartitionKeys,\n+\t\t\t\tconf);\n+\t\tthis.policies = PartitionCommitPolicy.createCommitChain(\n+\t\t\t\tconf.get(PARTITION_COMMIT_POLICY_TYPE),\n+\t\t\t\tconf.get(PARTITION_COMMIT_POLICY_SUCCESS_FILE_NAME));\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<CommitMessage> element) throws Exception {\n+\t\tCommitMessage message = element.getValue();\n+\t\tfor (String partition : message.partitions) {\n+\t\t\tcommitManager.addPartition(partition);\n+\t\t}\n+\n+\t\tif (taskTracker == null) {\n+\t\t\ttaskTracker = new TaskTracker(message.numberOfTasks);\n+\t\t}\n+\t\tboolean needCommit = taskTracker.add(message.checkpointId, message.taskId);\n+\t\tif (needCommit) {\n+\t\t\tcommitPartitions(commitManager.triggerCommit(message.checkpointId));\n+\t\t}\n+\t}\n+\n+\tprivate void commitPartitions(List<String> partitions) throws Exception {\n+\t\ttry (TableMetaStoreFactory.TableMetaStore metaStore = metaStoreFactory.createTableMetaStore()) {\n+\t\t\tFileSystem fs = metaStore.getLocationPath().getFileSystem();\n+\t\t\tfor (String partition : partitions) {\n+\t\t\t\tLinkedHashMap<String, String> partSpec = extractPartitionSpecFromPath(new Path(partition));\n+\t\t\t\tPath path = new Path(metaStore.getLocationPath(), generatePartitionPath(partSpec));\n+\t\t\t\tfor (PartitionCommitPolicy policy : policies) {\n+\t\t\t\t\tpolicy.commit(partSpec, path, fs, metaStore);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void processWatermark(Watermark mark) throws Exception {\n+\t\tsuper.processWatermark(mark);\n+\t\tthis.currentWatermark = mark.getTimestamp();\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(StateSnapshotContext context) throws Exception {\n+\t\tsuper.snapshotState(context);\n+\t\tcommitManager.snapshotState(context.getCheckpointId(), currentWatermark);\n+\t}\n+\n+\t/**\n+\t * The message sent upstream.\n+\t */\n+\tpublic static class CommitMessage implements Serializable {\n+\n+\t\tpublic long checkpointId;\n+\t\tpublic int taskId;\n+\t\tpublic int numberOfTasks;\n+\t\tpublic List<String> partitions;", "originalCommit": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ1MDczOA==", "url": "https://github.com/apache/flink/pull/12062#discussion_r423450738", "bodyText": "Yes.", "author": "JingsongLi", "createdAt": "2020-05-12T04:11:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk4OTMwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk4OTk4Mw==", "url": "https://github.com/apache/flink/pull/12062#discussion_r422989983", "bodyText": "I don't think this method really \"triggers\" the commit. Seems it just decides which partitions should be committed. So maybe rename to getPartitionsToCommit?", "author": "lirui-apache", "createdAt": "2020-05-11T12:02:38Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionCommitManager.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.state.OperatorStateStore;\n+import org.apache.flink.api.common.typeutils.base.ListSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.MapSerializer;\n+import org.apache.flink.api.common.typeutils.base.StringSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.util.StringUtils;\n+\n+import java.time.Duration;\n+import java.time.LocalDateTime;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_INTERVAL;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Manage partition and watermark information.\n+ */\n+public class PartitionCommitManager {\n+\n+\tprivate static final ListStateDescriptor<Map<Long, Long>> WATERMARKS_STATE_DESC =\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\t\"checkpoint-id-to-watermark\",\n+\t\t\t\t\tnew MapSerializer<>(LongSerializer.INSTANCE, LongSerializer.INSTANCE));\n+\tprivate static final ListStateDescriptor<List<String>> PENDING_PARTITIONS_STATE_DESC =\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\t\"pending-partitions\",\n+\t\t\t\t\tnew ListSerializer<>(StringSerializer.INSTANCE));\n+\n+\tprivate final ListState<Map<Long, Long>> watermarksState;\n+\tprivate final ListState<List<String>> pendingPartitionsState;\n+\tprivate final TreeMap<Long, Long> watermarks;\n+\tprivate final Set<String> pendingPartitions;\n+\tprivate final PartitionTimeExtractor extractor;\n+\tprivate final long timeIntervalMills;\n+\tprivate final List<String> partitionKeys;\n+\n+\tpublic PartitionCommitManager(\n+\t\t\tboolean isRestored,\n+\t\t\tOperatorStateStore operatorStateStore,\n+\t\t\tClassLoader userCodeClassLoader,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tConfiguration conf) throws Exception {\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tString extractorKind = conf.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\tString extractorClass = conf.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\tString extractorPattern = conf.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\t\tthis.timeIntervalMills = conf.getOptional(PARTITION_TIME_INTERVAL)\n+\t\t\t\t.map(Duration::toMillis)\n+\t\t\t\t.orElse(Long.MAX_VALUE);\n+\t\tthis.extractor = PartitionTimeExtractor.create(\n+\t\t\t\tuserCodeClassLoader,\n+\t\t\t\textractorKind,\n+\t\t\t\textractorClass,\n+\t\t\t\textractorPattern);\n+\n+\t\tthis.watermarksState = operatorStateStore.getListState(WATERMARKS_STATE_DESC);\n+\t\tthis.pendingPartitionsState = operatorStateStore.getListState(PENDING_PARTITIONS_STATE_DESC);\n+\n+\t\tthis.watermarks = new TreeMap<>();\n+\t\tthis.pendingPartitions = new HashSet<>();\n+\t\tif (isRestored) {\n+\t\t\twatermarks.putAll(watermarksState.get().iterator().next());\n+\t\t\tpendingPartitions.addAll(pendingPartitionsState.get().iterator().next());\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Add a pending partition.\n+\t */\n+\tpublic void addPartition(String partition) {\n+\t\tif (!StringUtils.isNullOrWhitespaceOnly(partition)) {\n+\t\t\tthis.pendingPartitions.add(partition);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Trigger commit of pending partitions, and cleanup useless watermarks and partitions.\n+\t */\n+\tpublic List<String> triggerCommit(long checkpointId) {", "originalCommit": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ1MjgyNA==", "url": "https://github.com/apache/flink/pull/12062#discussion_r423452824", "bodyText": "committablePartitions", "author": "JingsongLi", "createdAt": "2020-05-12T04:20:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk4OTk4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk5MTk5OA==", "url": "https://github.com/apache/flink/pull/12062#discussion_r422991998", "bodyText": "add constants for them", "author": "lirui-apache", "createdAt": "2020-05-11T12:06:40Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/PartitionCommitPolicy.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.filesystem.TableMetaStoreFactory;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Policy for partition commit.\n+ */\n+public interface PartitionCommitPolicy extends Serializable {\n+\n+\t/**\n+\t * Commit partition by partitionSpec and path.\n+\t */\n+\tvoid commit(\n+\t\t\tLinkedHashMap<String, String> partitionSpec,\n+\t\t\tPath partitionPath,\n+\t\t\tFileSystem fileSystem,\n+\t\t\tTableMetaStoreFactory.TableMetaStore metaStore) throws Exception;\n+\n+\tstatic List<PartitionCommitPolicy> createCommitChain(String policy, String successFileName) {\n+\t\tif (policy == null) {\n+\t\t\treturn Collections.emptyList();\n+\t\t}\n+\t\tString[] policyStrings = policy.split(\",\");\n+\t\treturn Arrays.stream(policyStrings).map(name -> {\n+\t\t\tswitch (name) {\n+\t\t\t\tcase \"metastore\":", "originalCommit": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk5NTIwMQ==", "url": "https://github.com/apache/flink/pull/12062#discussion_r422995201", "bodyText": "Does this invoke traffic to HMS? If so maybe we should only do it when partitions is not empty.", "author": "lirui-apache", "createdAt": "2020-05-11T12:13:14Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileCommitter.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.filesystem.TableMetaStoreFactory;\n+\n+import java.io.Serializable;\n+import java.util.HashSet;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.TreeMap;\n+\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_COMMIT_POLICY_SUCCESS_FILE_NAME;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_COMMIT_POLICY_TYPE;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+import static org.apache.flink.table.utils.PartitionPathUtils.generatePartitionPath;\n+\n+/**\n+ * Committer for {@link StreamingFileWriter}. This is the single (non-parallel) task.\n+ */\n+public class StreamingFileCommitter extends AbstractStreamOperator<Void>\n+\t\timplements OneInputStreamOperator<StreamingFileCommitter.CommitMessage, Void> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final Configuration conf;\n+\n+\tprivate final List<String> partitionKeys;\n+\n+\tprivate final TableMetaStoreFactory metaStoreFactory;\n+\n+\tprivate transient PartitionCommitManager commitManager;\n+\n+\tprivate transient TaskTracker taskTracker;\n+\n+\tprivate transient long currentWatermark = Long.MIN_VALUE;\n+\n+\tprivate transient List<PartitionCommitPolicy> policies;\n+\n+\tpublic StreamingFileCommitter(\n+\t\t\tList<String> partitionKeys, TableMetaStoreFactory metaStoreFactory, Configuration conf) {\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.metaStoreFactory = metaStoreFactory;\n+\t\tthis.conf = conf;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(StateInitializationContext context) throws Exception {\n+\t\tsuper.initializeState(context);\n+\t\tthis.commitManager = new PartitionCommitManager(\n+\t\t\t\tcontext.isRestored(),\n+\t\t\t\tcontext.getOperatorStateStore(),\n+\t\t\t\tgetUserCodeClassloader(),\n+\t\t\t\tpartitionKeys,\n+\t\t\t\tconf);\n+\t\tthis.policies = PartitionCommitPolicy.createCommitChain(\n+\t\t\t\tconf.get(PARTITION_COMMIT_POLICY_TYPE),\n+\t\t\t\tconf.get(PARTITION_COMMIT_POLICY_SUCCESS_FILE_NAME));\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<CommitMessage> element) throws Exception {\n+\t\tCommitMessage message = element.getValue();\n+\t\tfor (String partition : message.partitions) {\n+\t\t\tcommitManager.addPartition(partition);\n+\t\t}\n+\n+\t\tif (taskTracker == null) {\n+\t\t\ttaskTracker = new TaskTracker(message.numberOfTasks);\n+\t\t}\n+\t\tboolean needCommit = taskTracker.add(message.checkpointId, message.taskId);\n+\t\tif (needCommit) {\n+\t\t\tcommitPartitions(commitManager.triggerCommit(message.checkpointId));\n+\t\t}\n+\t}\n+\n+\tprivate void commitPartitions(List<String> partitions) throws Exception {\n+\t\ttry (TableMetaStoreFactory.TableMetaStore metaStore = metaStoreFactory.createTableMetaStore()) {", "originalCommit": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk5OTAxMg==", "url": "https://github.com/apache/flink/pull/12062#discussion_r422999012", "bodyText": "Is it possible that there's some pending data between close and the last notifyCheckpointComplete?", "author": "lirui-apache", "createdAt": "2020-05-11T12:20:25Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileWriter.java", "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.state.OperatorStateStore;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.Buckets;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeCallback;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.StreamingFileCommitter.CommitMessage;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Operator for file system sink.\n+ */\n+public class StreamingFileWriter extends AbstractStreamOperator<CommitMessage>\n+\t\timplements OneInputStreamOperator<RowData, CommitMessage>, ProcessingTimeCallback {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t// -------------------------- state descriptors ---------------------------\n+\n+\tprivate static final ListStateDescriptor<byte[]> BUCKET_STATE_DESC =\n+\t\t\tnew ListStateDescriptor<>(\"bucket-states\", BytePrimitiveArraySerializer.INSTANCE);\n+\n+\tprivate static final ListStateDescriptor<Long> MAX_PART_COUNTER_STATE_DESC =\n+\t\t\tnew ListStateDescriptor<>(\"max-part-counter\", LongSerializer.INSTANCE);\n+\n+\t// ------------------------ configuration fields --------------------------\n+\n+\tprivate final long bucketCheckInterval;\n+\n+\tprivate final StreamingFileSink.BucketsBuilder<RowData, ?, ? extends\n+\t\t\tStreamingFileSink.BucketsBuilder<RowData, ?, ?>> bucketsBuilder;\n+\n+\tprivate final FileSystemBucketListener listener;\n+\n+\t// --------------------------- runtime fields -----------------------------\n+\n+\tprivate transient Buckets<RowData, ?> buckets;\n+\n+\tprivate transient ProcessingTimeService processingTimeService;\n+\n+\tprivate transient long currentWatermark = Long.MIN_VALUE;\n+\n+\tprivate transient Set<String> inactivePartitions;\n+\n+\t// --------------------------- State Related Fields -----------------------------\n+\n+\tprivate transient ListState<byte[]> bucketStates;\n+\n+\tprivate transient ListState<Long> maxPartCountersState;\n+\n+\tpublic StreamingFileWriter(\n+\t\t\tlong bucketCheckInterval,\n+\t\t\tStreamingFileSink.BucketsBuilder<RowData, ?, ? extends\n+\t\t\t\t\tStreamingFileSink.BucketsBuilder<RowData, ?, ?>> bucketsBuilder,\n+\t\t\tFileSystemBucketListener listener) {\n+\t\tthis.bucketCheckInterval = bucketCheckInterval;\n+\t\tthis.bucketsBuilder = bucketsBuilder;\n+\t\tthis.listener = listener;\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(StateInitializationContext context) throws Exception {\n+\t\tsuper.initializeState(context);\n+\t\tfinal int subtaskIndex = getRuntimeContext().getIndexOfThisSubtask();\n+\t\tthis.buckets = bucketsBuilder.createBuckets(subtaskIndex);\n+\n+\t\tfinal OperatorStateStore stateStore = context.getOperatorStateStore();\n+\t\tbucketStates = stateStore.getListState(BUCKET_STATE_DESC);\n+\t\tmaxPartCountersState = stateStore.getUnionListState(MAX_PART_COUNTER_STATE_DESC);\n+\n+\t\tif (context.isRestored()) {\n+\t\t\tbuckets.initializeState(bucketStates, maxPartCountersState);\n+\t\t}\n+\t\tinactivePartitions = new HashSet<>();\n+\t\tlistener.setInactiveConsumer(b -> inactivePartitions.add(b));\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(StateSnapshotContext context) throws Exception {\n+\t\tsuper.snapshotState(context);\n+\t\tPreconditions.checkState(bucketStates != null && maxPartCountersState != null, \"sink has not been initialized\");\n+\t\tbuckets.snapshotState(\n+\t\t\t\tcontext.getCheckpointId(),\n+\t\t\t\tbucketStates,\n+\t\t\t\tmaxPartCountersState);\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tsuper.open();\n+\t\tthis.processingTimeService = getRuntimeContext().getProcessingTimeService();\n+\t\tlong currentProcessingTime = processingTimeService.getCurrentProcessingTime();\n+\t\tprocessingTimeService.registerTimer(currentProcessingTime + bucketCheckInterval, this);\n+\t}\n+\n+\t@Override\n+\tpublic void onProcessingTime(long timestamp) throws Exception {\n+\t\tfinal long currentTime = processingTimeService.getCurrentProcessingTime();\n+\t\tbuckets.onProcessingTime(currentTime);\n+\t\tprocessingTimeService.registerTimer(currentTime + bucketCheckInterval, this);\n+\t}\n+\n+\t@Override\n+\tpublic void processWatermark(Watermark mark) throws Exception {\n+\t\tsuper.processWatermark(mark);\n+\t\tthis.currentWatermark = mark.getTimestamp();\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<RowData> element) throws Exception {\n+\t\tbuckets.onElement(\n+\t\t\t\telement.getValue(),\n+\t\t\t\tgetProcessingTimeService().getCurrentProcessingTime(),\n+\t\t\t\telement.getTimestamp(),\n+\t\t\t\tcurrentWatermark);\n+\t}\n+\n+\t@Override\n+\tpublic void notifyCheckpointComplete(long checkpointId) throws Exception {\n+\t\tsuper.notifyCheckpointComplete(checkpointId);\n+\t\tbuckets.commitUpToCheckpoint(checkpointId);\n+\t\tCommitMessage message = new CommitMessage(\n+\t\t\t\tcheckpointId,\n+\t\t\t\tgetRuntimeContext().getIndexOfThisSubtask(),\n+\t\t\t\tgetRuntimeContext().getNumberOfParallelSubtasks(),\n+\t\t\t\tnew ArrayList<>(inactivePartitions));\n+\t\toutput.collect(new StreamRecord<>(message));\n+\t\tinactivePartitions.clear();\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {", "originalCommit": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ2ODI5MA==", "url": "https://github.com/apache/flink/pull/12062#discussion_r423468290", "bodyText": "I'll modify to endInput and support commit pending data.", "author": "JingsongLi", "createdAt": "2020-05-12T05:21:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk5OTAxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk5OTY4Mw==", "url": "https://github.com/apache/flink/pull/12062#discussion_r422999683", "bodyText": "incorrect java doc", "author": "lirui-apache", "createdAt": "2020-05-11T12:21:39Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/SuccessFileCommitPolicy.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.filesystem.TableMetaStoreFactory;\n+\n+import java.util.LinkedHashMap;\n+\n+/**\n+ * Partition commit policy to update metastore.", "originalCommit": "f8bdc31fb33986d81fe96a21019c1beaf0487cfe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0317794acef460e3a2930a2ce2ee88184d3133d2", "url": "https://github.com/apache/flink/commit/0317794acef460e3a2930a2ce2ee88184d3133d2", "message": "[FLINK-17587][filesystem] Filesystem streaming sink support commit success file", "committedDate": "2020-05-12T03:07:25Z", "type": "forcePushed"}, {"oid": "e7040a4ef029a828ce0ad2584ecadac9cb55522b", "url": "https://github.com/apache/flink/commit/e7040a4ef029a828ce0ad2584ecadac9cb55522b", "message": "[FLINK-17587][filesystem] Filesystem streaming sink support commit success file", "committedDate": "2020-05-12T10:10:05Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzY4NTQ3NA==", "url": "https://github.com/apache/flink/pull/12062#discussion_r423685474", "bodyText": "This class is copy pasting quite a bit of code/fields from StreamingFileSink. Can not we extract a common abstraction or re-use one in the another?", "author": "pnowojski", "createdAt": "2020-05-12T12:16:06Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/stream/StreamingFileWriter.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem.stream;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.state.OperatorStateStore;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.Buckets;\n+import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeCallback;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.StreamingFileCommitter.CommitMessage;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Operator for file system sink. It is a operator version of {@link StreamingFileSink}.\n+ * It sends partition commit message to downstream for committing.\n+ *\n+ * <p>See {@link StreamingFileCommitter}.\n+ */\n+public class StreamingFileWriter extends AbstractStreamOperator<CommitMessage> implements", "originalCommit": "e7040a4ef029a828ce0ad2584ecadac9cb55522b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDMzOTAxOQ==", "url": "https://github.com/apache/flink/pull/12062#discussion_r424339019", "bodyText": "I've discussed with @gaoyunhaii  offline, mainly because feel that there is too little code, so don't reuse it.\nBut +1 for abstraction. I've extracted a helper from StreamingFileSink. See\n36fa3d2", "author": "JingsongLi", "createdAt": "2020-05-13T10:35:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzY4NTQ3NA=="}], "type": "inlineReview"}, {"oid": "2503b8d118226338e569fbe430287eb4f47816a4", "url": "https://github.com/apache/flink/commit/2503b8d118226338e569fbe430287eb4f47816a4", "message": "[FLINK-17587][filesystem] Filesystem streaming sink support commit success file", "committedDate": "2020-05-13T10:32:33Z", "type": "forcePushed"}, {"oid": "05ef446da66d95a0c61f7f663ea1a94addec75fb", "url": "https://github.com/apache/flink/commit/05ef446da66d95a0c61f7f663ea1a94addec75fb", "message": "[FLINK-17587][filesystem] Filesystem streaming sink support commit success file", "committedDate": "2020-05-13T10:36:34Z", "type": "forcePushed"}, {"oid": "6c24f2b260515cef76e314b70b0e376fa1b9d8a5", "url": "https://github.com/apache/flink/commit/6c24f2b260515cef76e314b70b0e376fa1b9d8a5", "message": "[FLINK-17587][filesystem] Filesystem streaming sink support commit success file", "committedDate": "2020-05-14T06:52:59Z", "type": "forcePushed"}, {"oid": "e7ab892003f4268a1a4073e1a4b29f3c677154e4", "url": "https://github.com/apache/flink/commit/e7ab892003f4268a1a4073e1a4b29f3c677154e4", "message": "[FLINK-17587][filesystem] Filesystem streaming sink support commit success file", "committedDate": "2020-05-14T06:57:59Z", "type": "forcePushed"}, {"oid": "e2b2ede56019d7b04ca9a2623eb552916d598b35", "url": "https://github.com/apache/flink/commit/e2b2ede56019d7b04ca9a2623eb552916d598b35", "message": "[FLINK-17587][filesystem] Filesystem streaming sink support commit success file", "committedDate": "2020-05-14T12:18:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc4MzAwNw==", "url": "https://github.com/apache/flink/pull/12062#discussion_r425783007", "bodyText": "I think we should explain how \"partition creation time\" is determined.", "author": "lirui-apache", "createdAt": "2020-05-15T12:57:40Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -88,4 +88,48 @@\n \t\t\t\t\t\t\t\" If timestamp in partition is year, month, day, hour,\" +\n \t\t\t\t\t\t\t\" can configure: '$year-$month-$day $hour:00:00'.\" +\n \t\t\t\t\t\t\t\" If timestamp in partition is dt and hour, can configure: '$dt $hour:00:00'.\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_TRIGGER =\n+\t\t\tkey(\"sink.partition-commit.trigger\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"partition-time\")\n+\t\t\t\t\t.withDescription(\"Trigger type for partition commit:\" +\n+\t\t\t\t\t\t\t\" 'partition-time': extract time from partition,\" +\n+\t\t\t\t\t\t\t\" if 'watermark' > 'partition-time' + 'delay', will commit the partition.\" +\n+\t\t\t\t\t\t\t\" 'process-time': use processing time, if 'current processing time' > \" +\n+\t\t\t\t\t\t\t\"'partition creation time' + 'delay', will commit the partition.\");", "originalCommit": "e2b2ede56019d7b04ca9a2623eb552916d598b35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgzNDkwNg==", "url": "https://github.com/apache/flink/pull/12062#discussion_r425834906", "bodyText": "\"partition directory creation time\"", "author": "JingsongLi", "createdAt": "2020-05-15T14:20:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc4MzAwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc5MTU1NA==", "url": "https://github.com/apache/flink/pull/12062#discussion_r425791554", "bodyText": "Does this only work custom commit policy? If so it should be mentioned in the description and reflected in the config name.", "author": "lirui-apache", "createdAt": "2020-05-15T13:12:32Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -88,4 +88,48 @@\n \t\t\t\t\t\t\t\" If timestamp in partition is year, month, day, hour,\" +\n \t\t\t\t\t\t\t\" can configure: '$year-$month-$day $hour:00:00'.\" +\n \t\t\t\t\t\t\t\" If timestamp in partition is dt and hour, can configure: '$dt $hour:00:00'.\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_TRIGGER =\n+\t\t\tkey(\"sink.partition-commit.trigger\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"partition-time\")\n+\t\t\t\t\t.withDescription(\"Trigger type for partition commit:\" +\n+\t\t\t\t\t\t\t\" 'partition-time': extract time from partition,\" +\n+\t\t\t\t\t\t\t\" if 'watermark' > 'partition-time' + 'delay', will commit the partition.\" +\n+\t\t\t\t\t\t\t\" 'process-time': use processing time, if 'current processing time' > \" +\n+\t\t\t\t\t\t\t\"'partition creation time' + 'delay', will commit the partition.\");\n+\n+\tpublic static final ConfigOption<Duration> SINK_PARTITION_COMMIT_DELAY =\n+\t\t\tkey(\"sink.partition-commit.delay\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(0))\n+\t\t\t\t\t.withDescription(\"The partition will not commit until the delay time.\" +\n+\t\t\t\t\t\t\t\" if it is a day partition, should be '1 d',\" +\n+\t\t\t\t\t\t\t\" if it is a hour partition, should be '1 h'\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_POLICY_KIND =\n+\t\t\tkey(\"sink.partition-commit.policy.kind\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"Policy to commit a partition is to notify the downstream\" +\n+\t\t\t\t\t\t\t\" application that the partition has finished writing, the partition\" +\n+\t\t\t\t\t\t\t\" is ready to be read.\" +\n+\t\t\t\t\t\t\t\" metastore: add partition to metastore.\" +\n+\t\t\t\t\t\t\t\" success-file: add '_success' file to directory.\" +\n+\t\t\t\t\t\t\t\" Both can be configured at the same time: 'metastore,success-file'.\" +\n+\t\t\t\t\t\t\t\" custom: use policy class to create a commit policy.\" +\n+\t\t\t\t\t\t\t\" Support to configure multiple policies: 'metastore,success-file'.\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_POLICY_CLASS =\n+\t\t\tkey(\"sink.partition-commit.policy.class\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"The partition commit policy class for implement PartitionCommitPolicy interface.\");", "originalCommit": "e2b2ede56019d7b04ca9a2623eb552916d598b35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgzNTg1Ng==", "url": "https://github.com/apache/flink/pull/12062#discussion_r425835856", "bodyText": "I will add comment, it is OK, has been mentioned in policy kind.", "author": "JingsongLi", "createdAt": "2020-05-15T14:21:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc5MTU1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTc5NzA4Nw==", "url": "https://github.com/apache/flink/pull/12062#discussion_r425797087", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <p>The implemented commit method needs to be reentrant because the same partition may be\n          \n          \n            \n             * <p>The implemented commit method needs to be idempotent because the same partition may be", "author": "lirui-apache", "createdAt": "2020-05-15T13:21:19Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionCommitPolicy.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Policy for commit a partition.\n+ *\n+ * <p>The implemented commit method needs to be reentrant because the same partition may be", "originalCommit": "e2b2ede56019d7b04ca9a2623eb552916d598b35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgwMTQxOA==", "url": "https://github.com/apache/flink/pull/12062#discussion_r425801418", "bodyText": "If users choose metastore policy, don't they need to specify/provide a TableMetaStoreFactory implementation?", "author": "lirui-apache", "createdAt": "2020-05-15T13:27:55Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -88,4 +88,48 @@\n \t\t\t\t\t\t\t\" If timestamp in partition is year, month, day, hour,\" +\n \t\t\t\t\t\t\t\" can configure: '$year-$month-$day $hour:00:00'.\" +\n \t\t\t\t\t\t\t\" If timestamp in partition is dt and hour, can configure: '$dt $hour:00:00'.\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_TRIGGER =\n+\t\t\tkey(\"sink.partition-commit.trigger\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"partition-time\")\n+\t\t\t\t\t.withDescription(\"Trigger type for partition commit:\" +\n+\t\t\t\t\t\t\t\" 'partition-time': extract time from partition,\" +\n+\t\t\t\t\t\t\t\" if 'watermark' > 'partition-time' + 'delay', will commit the partition.\" +\n+\t\t\t\t\t\t\t\" 'process-time': use processing time, if 'current processing time' > \" +\n+\t\t\t\t\t\t\t\"'partition creation time' + 'delay', will commit the partition.\");\n+\n+\tpublic static final ConfigOption<Duration> SINK_PARTITION_COMMIT_DELAY =\n+\t\t\tkey(\"sink.partition-commit.delay\")\n+\t\t\t\t\t.durationType()\n+\t\t\t\t\t.defaultValue(Duration.ofMillis(0))\n+\t\t\t\t\t.withDescription(\"The partition will not commit until the delay time.\" +\n+\t\t\t\t\t\t\t\" if it is a day partition, should be '1 d',\" +\n+\t\t\t\t\t\t\t\" if it is a hour partition, should be '1 h'\");\n+\n+\tpublic static final ConfigOption<String> SINK_PARTITION_COMMIT_POLICY_KIND =\n+\t\t\tkey(\"sink.partition-commit.policy.kind\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.noDefaultValue()\n+\t\t\t\t\t.withDescription(\"Policy to commit a partition is to notify the downstream\" +\n+\t\t\t\t\t\t\t\" application that the partition has finished writing, the partition\" +\n+\t\t\t\t\t\t\t\" is ready to be read.\" +\n+\t\t\t\t\t\t\t\" metastore: add partition to metastore.\" +", "originalCommit": "e2b2ede56019d7b04ca9a2623eb552916d598b35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgzNzUxOA==", "url": "https://github.com/apache/flink/pull/12062#discussion_r425837518", "bodyText": "I will add comment: Only work with hive table, it is empty implementation for file system table.", "author": "JingsongLi", "createdAt": "2020-05-15T14:24:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgwMTQxOA=="}], "type": "inlineReview"}, {"oid": "b5c04196e6d3c0f0e88c688216913001930d3a20", "url": "https://github.com/apache/flink/commit/b5c04196e6d3c0f0e88c688216913001930d3a20", "message": "Fix comments", "committedDate": "2020-05-15T14:24:25Z", "type": "forcePushed"}, {"oid": "196579752867c527af380839b1f9c3f559ae4bf0", "url": "https://github.com/apache/flink/commit/196579752867c527af380839b1f9c3f559ae4bf0", "message": "[FLINK-17587][filesystem] Filesystem streaming sink support commit success file", "committedDate": "2020-05-16T03:04:13Z", "type": "forcePushed"}, {"oid": "27349ac964ef16dceae54bcae4cfb09ab17c14ea", "url": "https://github.com/apache/flink/commit/27349ac964ef16dceae54bcae4cfb09ab17c14ea", "message": "[FLINK-17587][runtime] Extract StreamingFileSinkHelper from StreamingFileSink", "committedDate": "2020-05-16T05:33:08Z", "type": "commit"}, {"oid": "e0a38b8cba26ad3059e7535e277c2f50d22c5e54", "url": "https://github.com/apache/flink/commit/e0a38b8cba26ad3059e7535e277c2f50d22c5e54", "message": "[FLINK-17587][filesystem] Filesystem streaming sink support commit success file", "committedDate": "2020-05-16T05:33:08Z", "type": "commit"}, {"oid": "e0a38b8cba26ad3059e7535e277c2f50d22c5e54", "url": "https://github.com/apache/flink/commit/e0a38b8cba26ad3059e7535e277c2f50d22c5e54", "message": "[FLINK-17587][filesystem] Filesystem streaming sink support commit success file", "committedDate": "2020-05-16T05:33:08Z", "type": "forcePushed"}]}