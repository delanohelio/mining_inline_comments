{"pr_number": 12206, "pr_title": "[FLINK-14255][hive] Integrate hive to streaming file sink with parquet and orc", "pr_createdAt": "2020-05-18T02:15:51Z", "pr_url": "https://github.com/apache/flink/pull/12206", "timeline": [{"oid": "521334c523a5004883f94e1fb996cdd18081500c", "url": "https://github.com/apache/flink/commit/521334c523a5004883f94e1fb996cdd18081500c", "message": "[FLINK-14255][hive] Integrate hive to streaming file sink with parquet and orc", "committedDate": "2020-05-18T02:13:50Z", "type": "commit"}, {"oid": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca", "url": "https://github.com/apache/flink/commit/2ea5f52f890c9cb5728fd7772f59bea4b38f07ca", "message": "Add hive 1.x support", "committedDate": "2020-05-18T02:39:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0MjQ5NQ==", "url": "https://github.com/apache/flink/pull/12206#discussion_r426342495", "bodyText": "Perhaps we should rename the old HivePartitionComputer to HiveRowPartitionComputer?", "author": "lirui-apache", "createdAt": "2020-05-18T02:34:16Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveRowDataPartitionComputer.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.filesystem.RowDataPartitionComputer;\n+import org.apache.flink.table.functions.hive.conversion.HiveInspectors;\n+import org.apache.flink.table.functions.hive.conversion.HiveObjectConversion;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+\n+/**\n+ * A {@link RowDataPartitionComputer} that converts Flink objects to Hive objects before computing the partition value strings.\n+ */\n+public class HiveRowDataPartitionComputer extends RowDataPartitionComputer {", "originalCommit": "521334c523a5004883f94e1fb996cdd18081500c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NTI4Mg==", "url": "https://github.com/apache/flink/pull/12206#discussion_r426345282", "bodyText": "We have already get the output format class. I think it's more reliable to check the class to decide whether the table is stored as orc or parquet.", "author": "lirui-apache", "createdAt": "2020-05-18T02:49:18Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java", "diffHunk": "@@ -143,21 +206,51 @@ public HiveTableSink(JobConf jobConf, ObjectPath tablePath, CatalogTable table)\n \t\t}\n \t}\n \n-\t@Override\n-\tpublic TableSink<Row> configure(String[] fieldNames, TypeInformation<?>[] fieldTypes) {\n-\t\treturn new HiveTableSink(jobConf, tablePath, catalogTable);\n+\tprivate BulkWriter.Factory<RowData> createBulkWriterFactory(String[] partitionColumns,\n+\t\t\tStorageDescriptor sd) {\n+\t\tString serLib = sd.getSerdeInfo().getSerializationLib().toLowerCase();\n+\t\tint formatFieldCount = tableSchema.getFieldCount() - partitionColumns.length;\n+\t\tString[] formatNames = new String[formatFieldCount];\n+\t\tLogicalType[] formatTypes = new LogicalType[formatFieldCount];\n+\t\tfor (int i = 0; i < formatFieldCount; i++) {\n+\t\t\tformatNames[i] = tableSchema.getFieldName(i).get();\n+\t\t\tformatTypes[i] = tableSchema.getFieldDataType(i).get().getLogicalType();\n+\t\t}\n+\t\tRowType formatType = RowType.of(formatTypes, formatNames);\n+\t\tConfiguration formatConf = new Configuration(jobConf);\n+\t\tsd.getSerdeInfo().getParameters().forEach(formatConf::set);\n+\t\tBulkWriter.Factory<RowData> bulkFactory;\n+\t\tif (serLib.contains(\"parquet\")) {", "originalCommit": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM1MDI5OA==", "url": "https://github.com/apache/flink/pull/12206#discussion_r426350298", "bodyText": "I created a JIRA for this refactor: https://issues.apache.org/jira/browse/FLINK-17784", "author": "JingsongLi", "createdAt": "2020-05-18T03:16:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NTI4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NTg3Ng==", "url": "https://github.com/apache/flink/pull/12206#discussion_r426345876", "bodyText": "I think we already have a HiveOutputFormatFactory, what's the difference between them?", "author": "lirui-apache", "createdAt": "2020-05-18T02:52:08Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/write/HiveOutputFormatFactory.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.write;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.fs.hdfs.HadoopFileSystem;\n+import org.apache.flink.table.filesystem.OutputFormatFactory;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;\n+import org.apache.hadoop.io.Writable;\n+\n+import java.io.IOException;\n+import java.util.function.Function;\n+\n+/**\n+ * Hive {@link OutputFormatFactory}, use {@link RecordWriter} to write record.\n+ */\n+public class HiveOutputFormatFactory implements OutputFormatFactory<Row> {", "originalCommit": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0ODYyMw==", "url": "https://github.com/apache/flink/pull/12206#discussion_r426348623", "bodyText": "I will delete old one to extract RecordWriterFactory.", "author": "JingsongLi", "createdAt": "2020-05-18T03:07:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NTg3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NzQ4OQ==", "url": "https://github.com/apache/flink/pull/12206#discussion_r426347489", "bodyText": "Shouldn't this method be placed in OrcShim? It's strange that HiveShim needs to understand how flink-orc works.", "author": "lirui-apache", "createdAt": "2020-05-18T03:00:34Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/client/HiveShim.java", "diffHunk": "@@ -205,4 +208,10 @@ SimpleGenericUDAFParameterInfo createUDAFParameterInfo(ObjectInspector[] params,\n \t */\n \tvoid createTableWithConstraints(IMetaStoreClient client, Table table, Configuration conf,\n \t\t\tUniqueConstraint pk, List<Byte> pkTraits, List<String> notNullCols, List<Byte> nnTraits);\n+\n+\t/**\n+\t * Create orc {@link BulkWriter.Factory} for different hive versions.\n+\t */\n+\tBulkWriter.Factory<RowData> createOrcBulkWriterFactory(", "originalCommit": "2ea5f52f890c9cb5728fd7772f59bea4b38f07ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0ODc4NQ==", "url": "https://github.com/apache/flink/pull/12206#discussion_r426348785", "bodyText": "I think this is hive shim, because orc factory is related to hive version.\nIf not this, will be a ugly if else.", "author": "JingsongLi", "createdAt": "2020-05-18T03:08:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NzQ4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM1MDQ5Mg==", "url": "https://github.com/apache/flink/pull/12206#discussion_r426350492", "bodyText": "I created a orc shim improvement: https://issues.apache.org/jira/browse/FLINK-17785", "author": "JingsongLi", "createdAt": "2020-05-18T03:17:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjM0NzQ4OQ=="}], "type": "inlineReview"}, {"oid": "299012acbbf33c20a524bfa366bb0a1d07551ec1", "url": "https://github.com/apache/flink/commit/299012acbbf33c20a524bfa366bb0a1d07551ec1", "message": "Fix comments", "committedDate": "2020-05-18T03:09:38Z", "type": "commit"}]}