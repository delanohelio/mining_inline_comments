{"pr_number": 12867, "pr_title": "[FLINK-18558][streaming] Introduce collect iterator with at least once semantics and exactly once semantics without fault tolerance", "pr_createdAt": "2020-07-10T09:18:15Z", "pr_url": "https://github.com/apache/flink/pull/12867", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwOTA2MA==", "url": "https://github.com/apache/flink/pull/12867#discussion_r454909060", "bodyText": "typo: Funtion", "author": "godfreyhe", "createdAt": "2020-07-15T09:13:36Z", "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/collect/utils/CollectSinkFunctionTestWrapper.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect.utils;\n+\n+import org.apache.flink.api.common.accumulators.Accumulator;\n+import org.apache.flink.api.common.accumulators.SerializedListAccumulator;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManager;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync;\n+import org.apache.flink.runtime.memory.MemoryManager;\n+import org.apache.flink.runtime.operators.testutils.MockEnvironment;\n+import org.apache.flink.runtime.operators.testutils.MockEnvironmentBuilder;\n+import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;\n+import org.apache.flink.streaming.api.operators.collect.CollectCoordinationRequest;\n+import org.apache.flink.streaming.api.operators.collect.CollectCoordinationResponse;\n+import org.apache.flink.streaming.api.operators.collect.CollectSinkFunction;\n+import org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator;\n+import org.apache.flink.streaming.util.MockStreamingRuntimeContext;\n+\n+import org.junit.Assert;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * A wrapper class for creating, checkpointing and closing\n+ * {@link org.apache.flink.streaming.api.operators.collect.CollectSinkFunction} for tests.\n+ */\n+public class CollectSinkFunctionTestWrapper<IN> {\n+\n+\tpublic static final String ACCUMULATOR_NAME = \"tableCollectAccumulator\";\n+\n+\tprivate static final int SOCKET_TIMEOUT_MILLIS = 1000;\n+\tprivate static final int FUTURE_TIMEOUT_MILLIS = 10000;\n+\tprivate static final int MAX_RETIRES = 100;\n+\n+\tprivate final TypeSerializer<IN> serializer;\n+\tprivate final int maxBytesPerBatch;\n+\n+\tprivate final IOManager ioManager;\n+\tprivate final StreamingRuntimeContext runtimeContext;\n+\tprivate final MockOperatorEventGateway gateway;\n+\tprivate final CollectSinkOperatorCoordinator coordinator;\n+\tprivate final MockFunctionInitializationContext functionInitializationContext;\n+\n+\tprivate CollectSinkFunction<IN> function;\n+\n+\tpublic CollectSinkFunctionTestWrapper(TypeSerializer<IN> serializer, int maxBytesPerBatch) throws Exception {\n+\t\tthis.serializer = serializer;\n+\t\tthis.maxBytesPerBatch = maxBytesPerBatch;\n+\n+\t\tthis.ioManager = new IOManagerAsync();\n+\t\tMockEnvironment environment = new MockEnvironmentBuilder()\n+\t\t\t.setTaskName(\"mockTask\")\n+\t\t\t.setManagedMemorySize(4 * MemoryManager.DEFAULT_PAGE_SIZE)\n+\t\t\t.setIOManager(ioManager)\n+\t\t\t.build();\n+\t\tthis.runtimeContext = new MockStreamingRuntimeContext(false, 1, 0, environment);\n+\t\tthis.gateway = new MockOperatorEventGateway();\n+\n+\t\tthis.coordinator = new CollectSinkOperatorCoordinator(SOCKET_TIMEOUT_MILLIS);\n+\t\tthis.coordinator.start();\n+\n+\t\tthis.functionInitializationContext = new MockFunctionInitializationContext();\n+\t}\n+\n+\tpublic void closeWrapper() throws Exception {\n+\t\tcoordinator.close();\n+\t\tioManager.close();\n+\t}\n+\n+\tpublic CollectSinkOperatorCoordinator getCoordinator() {\n+\t\treturn coordinator;\n+\t}\n+\n+\tpublic void openFunction() throws Exception {\n+\t\tfunction = new CollectSinkFunction<>(serializer, maxBytesPerBatch, ACCUMULATOR_NAME);\n+\t\tfunction.setRuntimeContext(runtimeContext);\n+\t\tfunction.setOperatorEventGateway(gateway);\n+\t\tfunction.open(new Configuration());\n+\t\tcoordinator.handleEventFromOperator(0, gateway.getNextEvent());\n+\t}\n+\n+\tpublic void openFunctionWithState() throws Exception {\n+\t\tfunctionInitializationContext.getOperatorStateStore().revertToLastSuccessCheckpoint();\n+\t\tfunction = new CollectSinkFunction<>(serializer, maxBytesPerBatch, ACCUMULATOR_NAME);\n+\t\tfunction.setRuntimeContext(runtimeContext);\n+\t\tfunction.setOperatorEventGateway(gateway);\n+\t\tfunction.initializeState(functionInitializationContext);\n+\t\tfunction.open(new Configuration());\n+\t\tcoordinator.handleEventFromOperator(0, gateway.getNextEvent());\n+\t}\n+\n+\tpublic void invoke(IN record) throws Exception {\n+\t\tfunction.invoke(record, null);\n+\t}\n+\n+\tpublic void checkpointFunction(long checkpointId) throws Exception {\n+\t\tfunction.snapshotState(new MockFunctionSnapshotContext(checkpointId));\n+\t\tfunctionInitializationContext.getOperatorStateStore().checkpointBegin(checkpointId);\n+\t}\n+\n+\tpublic void checkpointComplete(long checkpointId) {\n+\t\tfunction.notifyCheckpointComplete(checkpointId);\n+\t\tfunctionInitializationContext.getOperatorStateStore().checkpointSuccess(checkpointId);\n+\t}\n+\n+\tpublic void closeFunctionNormally() throws Exception {\n+\t\t// this is a normal shutdown\n+\t\tfunction.accumulateFinalResults();\n+\t\tfunction.close();\n+\t}\n+\n+\tpublic void closeFuntionAbnormally() throws Exception {", "originalCommit": "c3ac42923fc06b5d0d86c01c814918916ab86790", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkxMTk2NA==", "url": "https://github.com/apache/flink/pull/12867#discussion_r454911964", "bodyText": "typo: Accumualtor", "author": "godfreyhe", "createdAt": "2020-07-15T09:18:31Z", "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/collect/utils/CollectSinkFunctionTestWrapper.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect.utils;\n+\n+import org.apache.flink.api.common.accumulators.Accumulator;\n+import org.apache.flink.api.common.accumulators.SerializedListAccumulator;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManager;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync;\n+import org.apache.flink.runtime.memory.MemoryManager;\n+import org.apache.flink.runtime.operators.testutils.MockEnvironment;\n+import org.apache.flink.runtime.operators.testutils.MockEnvironmentBuilder;\n+import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;\n+import org.apache.flink.streaming.api.operators.collect.CollectCoordinationRequest;\n+import org.apache.flink.streaming.api.operators.collect.CollectCoordinationResponse;\n+import org.apache.flink.streaming.api.operators.collect.CollectSinkFunction;\n+import org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorCoordinator;\n+import org.apache.flink.streaming.util.MockStreamingRuntimeContext;\n+\n+import org.junit.Assert;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * A wrapper class for creating, checkpointing and closing\n+ * {@link org.apache.flink.streaming.api.operators.collect.CollectSinkFunction} for tests.\n+ */\n+public class CollectSinkFunctionTestWrapper<IN> {\n+\n+\tpublic static final String ACCUMULATOR_NAME = \"tableCollectAccumulator\";\n+\n+\tprivate static final int SOCKET_TIMEOUT_MILLIS = 1000;\n+\tprivate static final int FUTURE_TIMEOUT_MILLIS = 10000;\n+\tprivate static final int MAX_RETIRES = 100;\n+\n+\tprivate final TypeSerializer<IN> serializer;\n+\tprivate final int maxBytesPerBatch;\n+\n+\tprivate final IOManager ioManager;\n+\tprivate final StreamingRuntimeContext runtimeContext;\n+\tprivate final MockOperatorEventGateway gateway;\n+\tprivate final CollectSinkOperatorCoordinator coordinator;\n+\tprivate final MockFunctionInitializationContext functionInitializationContext;\n+\n+\tprivate CollectSinkFunction<IN> function;\n+\n+\tpublic CollectSinkFunctionTestWrapper(TypeSerializer<IN> serializer, int maxBytesPerBatch) throws Exception {\n+\t\tthis.serializer = serializer;\n+\t\tthis.maxBytesPerBatch = maxBytesPerBatch;\n+\n+\t\tthis.ioManager = new IOManagerAsync();\n+\t\tMockEnvironment environment = new MockEnvironmentBuilder()\n+\t\t\t.setTaskName(\"mockTask\")\n+\t\t\t.setManagedMemorySize(4 * MemoryManager.DEFAULT_PAGE_SIZE)\n+\t\t\t.setIOManager(ioManager)\n+\t\t\t.build();\n+\t\tthis.runtimeContext = new MockStreamingRuntimeContext(false, 1, 0, environment);\n+\t\tthis.gateway = new MockOperatorEventGateway();\n+\n+\t\tthis.coordinator = new CollectSinkOperatorCoordinator(SOCKET_TIMEOUT_MILLIS);\n+\t\tthis.coordinator.start();\n+\n+\t\tthis.functionInitializationContext = new MockFunctionInitializationContext();\n+\t}\n+\n+\tpublic void closeWrapper() throws Exception {\n+\t\tcoordinator.close();\n+\t\tioManager.close();\n+\t}\n+\n+\tpublic CollectSinkOperatorCoordinator getCoordinator() {\n+\t\treturn coordinator;\n+\t}\n+\n+\tpublic void openFunction() throws Exception {\n+\t\tfunction = new CollectSinkFunction<>(serializer, maxBytesPerBatch, ACCUMULATOR_NAME);\n+\t\tfunction.setRuntimeContext(runtimeContext);\n+\t\tfunction.setOperatorEventGateway(gateway);\n+\t\tfunction.open(new Configuration());\n+\t\tcoordinator.handleEventFromOperator(0, gateway.getNextEvent());\n+\t}\n+\n+\tpublic void openFunctionWithState() throws Exception {\n+\t\tfunctionInitializationContext.getOperatorStateStore().revertToLastSuccessCheckpoint();\n+\t\tfunction = new CollectSinkFunction<>(serializer, maxBytesPerBatch, ACCUMULATOR_NAME);\n+\t\tfunction.setRuntimeContext(runtimeContext);\n+\t\tfunction.setOperatorEventGateway(gateway);\n+\t\tfunction.initializeState(functionInitializationContext);\n+\t\tfunction.open(new Configuration());\n+\t\tcoordinator.handleEventFromOperator(0, gateway.getNextEvent());\n+\t}\n+\n+\tpublic void invoke(IN record) throws Exception {\n+\t\tfunction.invoke(record, null);\n+\t}\n+\n+\tpublic void checkpointFunction(long checkpointId) throws Exception {\n+\t\tfunction.snapshotState(new MockFunctionSnapshotContext(checkpointId));\n+\t\tfunctionInitializationContext.getOperatorStateStore().checkpointBegin(checkpointId);\n+\t}\n+\n+\tpublic void checkpointComplete(long checkpointId) {\n+\t\tfunction.notifyCheckpointComplete(checkpointId);\n+\t\tfunctionInitializationContext.getOperatorStateStore().checkpointSuccess(checkpointId);\n+\t}\n+\n+\tpublic void closeFunctionNormally() throws Exception {\n+\t\t// this is a normal shutdown\n+\t\tfunction.accumulateFinalResults();\n+\t\tfunction.close();\n+\t}\n+\n+\tpublic void closeFuntionAbnormally() throws Exception {\n+\t\t// this is an exceptional shutdown\n+\t\tfunction.close();\n+\t\tcoordinator.subtaskFailed(0, null);\n+\t}\n+\n+\tpublic CollectCoordinationResponse sendRequestAndGetResponse(String version, long offset) throws Exception {\n+\t\tCollectCoordinationResponse response;\n+\t\tfor (int i = 0; i < MAX_RETIRES; i++) {\n+\t\t\tresponse = sendRequest(version, offset);\n+\t\t\tif (response.getLastCheckpointedOffset() >= 0) {\n+\t\t\t\treturn response;\n+\t\t\t}\n+\t\t}\n+\t\tthrow new RuntimeException(\"Too many retries in sendRequestAndGetValidResponse\");\n+\t}\n+\n+\tprivate CollectCoordinationResponse sendRequest(String version, long offset) throws Exception {\n+\t\tCollectCoordinationRequest request = new CollectCoordinationRequest(version, offset);\n+\t\t// we add a timeout to not block the tests if it fails\n+\t\treturn ((CollectCoordinationResponse) coordinator\n+\t\t\t.handleCoordinationRequest(request).get(FUTURE_TIMEOUT_MILLIS, TimeUnit.MILLISECONDS));\n+\t}\n+\n+\tpublic Tuple2<Long, CollectCoordinationResponse> getAccumualtorResults() throws Exception {", "originalCommit": "c3ac42923fc06b5d0d86c01c814918916ab86790", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkzMDcwMQ==", "url": "https://github.com/apache/flink/pull/12867#discussion_r454930701", "bodyText": "I think this branch is unnecessary, because it's illegal that checkpoint interval is less than MINIMAL_CHECKPOINT_TIME, many places have such validation, e.g. CheckpointConfig.setCheckpointInterval", "author": "godfreyhe", "createdAt": "2020-07-15T09:50:27Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectResultIterator.java", "diffHunk": "@@ -40,18 +43,37 @@\n \tpublic CollectResultIterator(\n \t\t\tCompletableFuture<OperatorID> operatorIdFuture,\n \t\t\tTypeSerializer<T> serializer,\n-\t\t\tString accumulatorName) {\n-\t\tthis.fetcher = new CollectResultFetcher<>(operatorIdFuture, serializer, accumulatorName);\n+\t\t\tString accumulatorName,\n+\t\t\tCheckpointConfig checkpointConfig) {\n+\t\tif (checkpointConfig.getCheckpointingMode() == CheckpointingMode.EXACTLY_ONCE) {\n+\t\t\tif (checkpointConfig.getCheckpointInterval() >= CheckpointCoordinatorConfiguration.MINIMAL_CHECKPOINT_TIME) {\n+\t\t\t\tthis.fetcher = new CollectResultFetcher<>(\n+\t\t\t\t\tnew CheckpointedCollectResultBuffer<>(serializer),\n+\t\t\t\t\toperatorIdFuture,\n+\t\t\t\t\taccumulatorName);\n+\t\t\t} else {\n+\t\t\t\tthis.fetcher = new CollectResultFetcher<>(\n+\t\t\t\t\tnew UncheckpointedCollectResultBuffer<>(serializer, false),\n+\t\t\t\t\toperatorIdFuture,\n+\t\t\t\t\taccumulatorName);\n+\t\t\t}", "originalCommit": "c3ac42923fc06b5d0d86c01c814918916ab86790", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYzMzMyNA==", "url": "https://github.com/apache/flink/pull/12867#discussion_r455633324", "bodyText": "If checkpointConfig.getCheckpointInterval() >= CheckpointCoordinatorConfiguration.MINIMAL_CHECKPOINT_TIME we are sure that the user explicitly enables a checkpoint. Otherwise we have to sync with the default value of checkpoint interval in CheckpointCoordinatorConfiguration.", "author": "tsreaper", "createdAt": "2020-07-16T08:58:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkzMDcwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTY0MTI2NQ==", "url": "https://github.com/apache/flink/pull/12867#discussion_r455641265", "bodyText": "OK... It seems that checkpointConfig.isCheckpointingEnabled() is a better solution.", "author": "tsreaper", "createdAt": "2020-07-16T09:11:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkzMDcwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk4NzkyNw==", "url": "https://github.com/apache/flink/pull/12867#discussion_r454987927", "bodyText": "change addResults  as a utility method ? so that we can handle the variables (e.g. offset) in one method, and make this method more readable.", "author": "godfreyhe", "createdAt": "2020-07-15T11:41:11Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/UncheckpointedCollectResultBuffer.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+\n+import java.io.IOException;\n+\n+/**\n+ * A buffer which encapsulates the logic of dealing with the response from the {@link CollectSinkFunction}.\n+ * It ignores the checkpoint related fields in the response.\n+ * See Java doc of {@link CollectSinkFunction} for explanation of this communication protocol.\n+ */\n+public class UncheckpointedCollectResultBuffer<T> extends AbstractCollectResultBuffer<T> {\n+\n+\tprivate final boolean failureTolerance;\n+\n+\tpublic UncheckpointedCollectResultBuffer(TypeSerializer<T> serializer, boolean failureTolerance) {\n+\t\tsuper(serializer);\n+\t\tthis.failureTolerance = failureTolerance;\n+\t}\n+\n+\t@Override\n+\tpublic void dealWithResponse(CollectCoordinationResponse response, long responseOffset) throws IOException {\n+\t\tString responseVersion = response.getVersion();\n+\n+\t\tif (!version.equals(responseVersion)) {\n+\t\t\tif (!INIT_VERSION.equals(version) && !failureTolerance) {\n+\t\t\t\t// sink restarted but we do not tolerate failure\n+\t\t\t\tthrow new RuntimeException(\"Job restarted\");\n+\t\t\t}\n+\n+\t\t\treset();\n+\t\t\tversion = responseVersion;\n+\t\t}\n+\n+\t\taddResults(response, responseOffset);\n+\t\t// the results are instantly visible by users\n+\t\tuserVisibleTail = offset;", "originalCommit": "c3ac42923fc06b5d0d86c01c814918916ab86790", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5OTg3NQ==", "url": "https://github.com/apache/flink/pull/12867#discussion_r454999875", "bodyText": "checkpointedData for  UncheckpointedDataFeeder ?", "author": "godfreyhe", "createdAt": "2020-07-15T12:04:36Z", "path": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/collect/CollectSinkFunctionRandomITCase.java", "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect;\n+\n+import org.apache.flink.api.common.JobID;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.runtime.jobgraph.OperatorID;\n+import org.apache.flink.streaming.api.operators.collect.utils.CollectSinkFunctionTestWrapper;\n+import org.apache.flink.streaming.api.operators.collect.utils.TestJobClient;\n+import org.apache.flink.util.OptionalFailure;\n+import org.apache.flink.util.TestLogger;\n+import org.apache.flink.util.function.RunnableWithException;\n+\n+import org.hamcrest.CoreMatchers;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.ListIterator;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+\n+import static org.apache.flink.streaming.api.operators.collect.utils.CollectSinkFunctionTestWrapper.ACCUMULATOR_NAME;\n+\n+/**\n+ * Random IT cases for {@link CollectSinkFunction}.\n+ * It will perform random insert, random checkpoint and random restart.\n+ */\n+public class CollectSinkFunctionRandomITCase extends TestLogger {\n+\n+\tprivate static final int MAX_RESULTS_PER_BATCH = 3;\n+\tprivate static final JobID TEST_JOB_ID = new JobID();\n+\tprivate static final OperatorID TEST_OPERATOR_ID = new OperatorID();\n+\n+\tprivate static final TypeSerializer<Integer> serializer = IntSerializer.INSTANCE;\n+\n+\tprivate CollectSinkFunctionTestWrapper<Integer> functionWrapper;\n+\tprivate boolean jobFinished;\n+\n+\t@Test\n+\tpublic void testUncheckpointedFunction() throws Exception {\n+\t\t// run multiple times for this random test\n+\t\tfor (int testCount = 30; testCount > 0; testCount--) {\n+\t\t\tfunctionWrapper = new CollectSinkFunctionTestWrapper<>(serializer, MAX_RESULTS_PER_BATCH * 4);\n+\t\t\tjobFinished = false;\n+\n+\t\t\tList<Integer> expected = new ArrayList<>();\n+\t\t\tfor (int i = 0; i < 50; i++) {\n+\t\t\t\texpected.add(i);\n+\t\t\t}\n+\t\t\tThread feeder = new ThreadWithException(new UncheckpointedDataFeeder(expected));\n+\n+\t\t\tList<Integer> actual = runFunctionRandomTest(feeder);\n+\t\t\tassertResultsEqualAfterSort(expected, actual);\n+\n+\t\t\tfunctionWrapper.closeWrapper();\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testCheckpointedFunction() throws Exception {\n+\t\t// run multiple times for this random test\n+\t\tfor (int testCount = 30; testCount > 0; testCount--) {\n+\t\t\tfunctionWrapper = new CollectSinkFunctionTestWrapper<>(serializer, MAX_RESULTS_PER_BATCH * 4);\n+\t\t\tjobFinished = false;\n+\n+\t\t\tList<Integer> expected = new ArrayList<>();\n+\t\t\tfor (int i = 0; i < 50; i++) {\n+\t\t\t\texpected.add(i);\n+\t\t\t}\n+\t\t\tThread feeder = new ThreadWithException(new CheckpointedDataFeeder(expected));\n+\n+\t\t\tList<Integer> actual = runFunctionRandomTest(feeder);\n+\t\t\tassertResultsEqualAfterSort(expected, actual);\n+\n+\t\t\tfunctionWrapper.closeWrapper();\n+\t\t}\n+\t}\n+\n+\tprivate List<Integer> runFunctionRandomTest(Thread feeder) throws Exception {\n+\t\tCollectClient collectClient = new CollectClient();\n+\t\tThread client = new ThreadWithException(collectClient);\n+\n+\t\tThread.UncaughtExceptionHandler exceptionHandler = (t, e) -> {\n+\t\t\tfeeder.interrupt();\n+\t\t\tclient.interrupt();\n+\t\t\te.printStackTrace();\n+\t\t};\n+\t\tfeeder.setUncaughtExceptionHandler(exceptionHandler);\n+\t\tclient.setUncaughtExceptionHandler(exceptionHandler);\n+\n+\t\tfeeder.start();\n+\t\tclient.start();\n+\t\tfeeder.join();\n+\t\tclient.join();\n+\n+\t\treturn collectClient.results;\n+\t}\n+\n+\tprivate void assertResultsEqualAfterSort(List<Integer> expected, List<Integer> actual) {\n+\t\tCollections.sort(expected);\n+\t\tCollections.sort(actual);\n+\t\tAssert.assertThat(actual, CoreMatchers.is(expected));\n+\t}\n+\n+\t/**\n+\t * A {@link RunnableWithException} feeding data to the function. It will fail when half of the data is fed.\n+\t */\n+\tprivate class UncheckpointedDataFeeder implements RunnableWithException {\n+\n+\t\tprivate LinkedList<Integer> data;\n+\t\tprivate final List<Integer> checkpointedData;", "originalCommit": "c3ac42923fc06b5d0d86c01c814918916ab86790", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTYzMzcxNQ==", "url": "https://github.com/apache/flink/pull/12867#discussion_r455633715", "bodyText": "originalData might be the proper name.", "author": "tsreaper", "createdAt": "2020-07-16T08:59:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk5OTg3NQ=="}], "type": "inlineReview"}, {"oid": "5f4341766d42b1bb60db4b94e028ede58bf5a661", "url": "https://github.com/apache/flink/commit/5f4341766d42b1bb60db4b94e028ede58bf5a661", "message": "[FLINK-18559][tests] Refactor tests for datastream / table collect", "committedDate": "2020-07-17T02:32:23Z", "type": "commit"}, {"oid": "8d114eb7c57a869bd094a0fd8164d671f1475dac", "url": "https://github.com/apache/flink/commit/8d114eb7c57a869bd094a0fd8164d671f1475dac", "message": "[FLINK-18560][streaming] Introduce collect iterator with at least once semantics and exactly once semantics without fault tolerance", "committedDate": "2020-07-17T02:32:23Z", "type": "commit"}, {"oid": "965cb49e682f803553e028a13595921cb95f8684", "url": "https://github.com/apache/flink/commit/965cb49e682f803553e028a13595921cb95f8684", "message": "[FLINK-18560][fix] Add comments for newly added class and change iterator selection a little", "committedDate": "2020-07-17T02:32:23Z", "type": "commit"}, {"oid": "454479efb6de5855111bfa45ff10cf55148f4117", "url": "https://github.com/apache/flink/commit/454479efb6de5855111bfa45ff10cf55148f4117", "message": "[FLINK-18560][docs] Update documentation for Table API and SQL queries", "committedDate": "2020-07-17T02:32:23Z", "type": "commit"}, {"oid": "45bcb707df83715483595caf6b3a0c03964abc3d", "url": "https://github.com/apache/flink/commit/45bcb707df83715483595caf6b3a0c03964abc3d", "message": "[FLINK-18560][fix] Fix collect result buffer logic which is different with the logic before refactor", "committedDate": "2020-07-17T03:02:24Z", "type": "commit"}, {"oid": "a64ca67c2e50380c4b6aca1748fb71a3c620377a", "url": "https://github.com/apache/flink/commit/a64ca67c2e50380c4b6aca1748fb71a3c620377a", "message": "[FLINK-18560][docs] Update java docs for TableResult#collect and #print", "committedDate": "2020-07-17T03:11:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIyNTQ4Mw==", "url": "https://github.com/apache/flink/pull/12867#discussion_r456225483", "bodyText": "ditto", "author": "godfreyhe", "createdAt": "2020-07-17T05:26:01Z", "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "diffHunk": "@@ -132,10 +132,22 @@\n \t *  }\n \t * }</pre>\n \t *\n-\t * <p>For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-\t * which requires the checkpointing mechanism to be enabled.\n-\t * By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-\t * (see ExecutionCheckpointingOptions) through {@link TableConfig#getConfiguration()}.\n+\t * <p>This method has slightly different behaviors under different checkpointing settings\n+\t * (to enable checkpointing for a streaming job,\n+\t * set checkpointing properties through {@link TableConfig#getConfiguration()}).\n+\t * <ul>\n+\t *     <li>If the user is running a batch job, or does not enable checkpointing for a streaming job,\n+\t *     this method has neither exactly-once nor at-least-once guarantee.\n+\t *     Query results are immediately accessible by the clients once they're produced,\n+\t *     but the function calls will throw an exception when the job fails and restarts.", "originalCommit": "a64ca67c2e50380c4b6aca1748fb71a3c620377a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIyNTc3Ng==", "url": "https://github.com/apache/flink/pull/12867#discussion_r456225776", "bodyText": "ditto", "author": "godfreyhe", "createdAt": "2020-07-17T05:27:17Z", "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "diffHunk": "@@ -146,10 +158,22 @@\n \t/**\n \t * Print the result contents as tableau form to client console.\n \t *\n-\t * <p>For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-\t * which requires the checkpointing mechanism to be enabled.\n-\t * By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-\t * (see ExecutionCheckpointingOptions) through {@link TableConfig#getConfiguration()}.\n+\t * <p>This method has slightly different behaviors under different checkpointing settings\n+\t * (to enable checkpointing for a streaming job,\n+\t * set checkpointing properties through {@link TableConfig#getConfiguration()}).\n+\t * <ul>\n+\t *     <li>If the user is running a batch job, or does not enable checkpointing for a streaming job,\n+\t *     this method has neither exactly-once nor at-least-once guarantee.\n+\t *     Query results are immediately accessible by the clients once they're produced,\n+\t *     but the function calls will throw an exception when the job fails and restarts.", "originalCommit": "a64ca67c2e50380c4b6aca1748fb71a3c620377a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIyNTg5Mg==", "url": "https://github.com/apache/flink/pull/12867#discussion_r456225892", "bodyText": "If exactly-once checkpointing is enabled for a streaming job, ?", "author": "godfreyhe", "createdAt": "2020-07-17T05:27:45Z", "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "diffHunk": "@@ -132,10 +132,22 @@\n \t *  }\n \t * }</pre>\n \t *\n-\t * <p>For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-\t * which requires the checkpointing mechanism to be enabled.\n-\t * By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-\t * (see ExecutionCheckpointingOptions) through {@link TableConfig#getConfiguration()}.\n+\t * <p>This method has slightly different behaviors under different checkpointing settings\n+\t * (to enable checkpointing for a streaming job,\n+\t * set checkpointing properties through {@link TableConfig#getConfiguration()}).\n+\t * <ul>\n+\t *     <li>If the user is running a batch job, or does not enable checkpointing for a streaming job,\n+\t *     this method has neither exactly-once nor at-least-once guarantee.\n+\t *     Query results are immediately accessible by the clients once they're produced,\n+\t *     but the function calls will throw an exception when the job fails and restarts.\n+\t *     <li>If the user enables exactly-once checkpointing for a streaming job,", "originalCommit": "a64ca67c2e50380c4b6aca1748fb71a3c620377a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIyNjAxNQ==", "url": "https://github.com/apache/flink/pull/12867#discussion_r456226015", "bodyText": "please update the doc of TableResult in flink-python", "author": "godfreyhe", "createdAt": "2020-07-17T05:28:18Z", "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableResult.java", "diffHunk": "@@ -146,10 +158,22 @@\n \t/**\n \t * Print the result contents as tableau form to client console.\n \t *\n-\t * <p>For streaming mode, this method guarantees end-to-end exactly-once record delivery\n-\t * which requires the checkpointing mechanism to be enabled.\n-\t * By default, checkpointing is disabled. To enable checkpointing, set checkpointing properties\n-\t * (see ExecutionCheckpointingOptions) through {@link TableConfig#getConfiguration()}.\n+\t * <p>This method has slightly different behaviors under different checkpointing settings", "originalCommit": "a64ca67c2e50380c4b6aca1748fb71a3c620377a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "bf49c2297508e5dde1a9e0d8d4dfbafefc5f46f2", "url": "https://github.com/apache/flink/commit/bf49c2297508e5dde1a9e0d8d4dfbafefc5f46f2", "message": "[FLINK-18560][docs] Update docs according to godfrey's comments", "committedDate": "2020-07-20T02:57:49Z", "type": "commit"}, {"oid": "fb16ba0168cae5742f27c87acae3c477dd42150b", "url": "https://github.com/apache/flink/commit/fb16ba0168cae5742f27c87acae3c477dd42150b", "message": "[FLINK-18560][fix] Fix indent in python docstring", "committedDate": "2020-07-22T08:19:25Z", "type": "commit"}, {"oid": "a1ea81a9dd67cab4c8b483dfe3eed51d7ab4a9e2", "url": "https://github.com/apache/flink/commit/a1ea81a9dd67cab4c8b483dfe3eed51d7ab4a9e2", "message": "[FLINK-18560][fix] Fix line too long", "committedDate": "2020-07-23T02:09:43Z", "type": "commit"}]}