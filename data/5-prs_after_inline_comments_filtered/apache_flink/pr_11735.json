{"pr_number": 11735, "pr_title": "[FLINK-16802][hive] Set schema info in JobConf for Hive readers", "pr_createdAt": "2020-04-14T11:35:29Z", "pr_url": "https://github.com/apache/flink/pull/11735", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcyOTQyNw==", "url": "https://github.com/apache/flink/pull/11735#discussion_r412729427", "bodyText": "Need this branch?\nWe can just:\njobConf.set(SCHEMA_EVOLUTION_COLUMNS, String.join(\",\", Arrays.copyOfRange(fieldNames, 0, firstPartColIndex)));\njobConf.set(SCHEMA_EVOLUTION_COLUMNS_TYPES, String.join(\",\", typeStrs.subList(0, firstPartColIndex)));", "author": "JingsongLi", "createdAt": "2020-04-22T07:13:57Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -122,12 +131,40 @@ public void open(HiveTableInputSplit split) throws IOException {\n \t\t\tthis.reader = new HiveVectorizedParquetSplitReader(\n \t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n \t\t} else {\n-\t\t\tthis.reader = new HiveMapredSplitReader(jobConf, partitionKeys, fieldTypes, selectedFields, split,\n+\t\t\tJobConf clonedConf = new JobConf(jobConf);\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tthis.reader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, split,\n \t\t\t\t\tHiveShimLoader.loadHiveShim(hiveVersion));\n \t\t}\n \t\tcurrentReadCount = 0L;\n \t}\n \n+\t// Hive readers may rely on the schema info in configuration\n+\tprivate void addSchemaToConf(JobConf jobConf) {\n+\t\t// set columns/types -- including partition cols\n+\t\tList<String> typeStrs = Arrays.stream(fieldTypes)\n+\t\t\t\t.map(t -> HiveTypeUtil.toHiveTypeInfo(t, true).toString())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tjobConf.set(IOConstants.COLUMNS, String.join(\",\", fieldNames));\n+\t\tjobConf.set(IOConstants.COLUMNS_TYPES, String.join(\",\", typeStrs));\n+\t\t// set schema evolution -- excluding partition cols\n+\t\tint numPartCol = partitionKeys != null ? partitionKeys.size() : 0;\n+\t\tint firstPartColIndex = fieldNames.length - numPartCol;\n+\t\tif (numPartCol == 0) {", "originalCommit": "60d86e4a86f0afefe1bfbf398b1d28f139ca97f2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc0OTUzOA==", "url": "https://github.com/apache/flink/pull/11735#discussion_r412749538", "bodyText": "Yeah I added the branch to avoid array copies if the table is not partitioned. But perhaps terseness is more desirable here.", "author": "lirui-apache", "createdAt": "2020-04-22T07:45:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcyOTQyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjczMzYwMQ==", "url": "https://github.com/apache/flink/pull/11735#discussion_r412733601", "bodyText": "partitionKeys never null", "author": "JingsongLi", "createdAt": "2020-04-22T07:20:35Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -122,12 +131,40 @@ public void open(HiveTableInputSplit split) throws IOException {\n \t\t\tthis.reader = new HiveVectorizedParquetSplitReader(\n \t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n \t\t} else {\n-\t\t\tthis.reader = new HiveMapredSplitReader(jobConf, partitionKeys, fieldTypes, selectedFields, split,\n+\t\t\tJobConf clonedConf = new JobConf(jobConf);\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tthis.reader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, split,\n \t\t\t\t\tHiveShimLoader.loadHiveShim(hiveVersion));\n \t\t}\n \t\tcurrentReadCount = 0L;\n \t}\n \n+\t// Hive readers may rely on the schema info in configuration\n+\tprivate void addSchemaToConf(JobConf jobConf) {\n+\t\t// set columns/types -- including partition cols\n+\t\tList<String> typeStrs = Arrays.stream(fieldTypes)\n+\t\t\t\t.map(t -> HiveTypeUtil.toHiveTypeInfo(t, true).toString())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tjobConf.set(IOConstants.COLUMNS, String.join(\",\", fieldNames));\n+\t\tjobConf.set(IOConstants.COLUMNS_TYPES, String.join(\",\", typeStrs));\n+\t\t// set schema evolution -- excluding partition cols\n+\t\tint numPartCol = partitionKeys != null ? partitionKeys.size() : 0;", "originalCommit": "60d86e4a86f0afefe1bfbf398b1d28f139ca97f2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjczNDMwMw==", "url": "https://github.com/apache/flink/pull/11735#discussion_r412734303", "bodyText": "numNonPartCol?", "author": "JingsongLi", "createdAt": "2020-04-22T07:21:38Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -122,12 +131,40 @@ public void open(HiveTableInputSplit split) throws IOException {\n \t\t\tthis.reader = new HiveVectorizedParquetSplitReader(\n \t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n \t\t} else {\n-\t\t\tthis.reader = new HiveMapredSplitReader(jobConf, partitionKeys, fieldTypes, selectedFields, split,\n+\t\t\tJobConf clonedConf = new JobConf(jobConf);\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tthis.reader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, split,\n \t\t\t\t\tHiveShimLoader.loadHiveShim(hiveVersion));\n \t\t}\n \t\tcurrentReadCount = 0L;\n \t}\n \n+\t// Hive readers may rely on the schema info in configuration\n+\tprivate void addSchemaToConf(JobConf jobConf) {\n+\t\t// set columns/types -- including partition cols\n+\t\tList<String> typeStrs = Arrays.stream(fieldTypes)\n+\t\t\t\t.map(t -> HiveTypeUtil.toHiveTypeInfo(t, true).toString())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tjobConf.set(IOConstants.COLUMNS, String.join(\",\", fieldNames));\n+\t\tjobConf.set(IOConstants.COLUMNS_TYPES, String.join(\",\", typeStrs));\n+\t\t// set schema evolution -- excluding partition cols\n+\t\tint numPartCol = partitionKeys != null ? partitionKeys.size() : 0;\n+\t\tint firstPartColIndex = fieldNames.length - numPartCol;", "originalCommit": "60d86e4a86f0afefe1bfbf398b1d28f139ca97f2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b77e52194cbab40028c15a6ab4a747f5e900e37c", "url": "https://github.com/apache/flink/commit/b77e52194cbab40028c15a6ab4a747f5e900e37c", "message": "address comments", "committedDate": "2020-04-22T07:49:16Z", "type": "forcePushed"}, {"oid": "f9fa39640ee8e4082f0e3d7a7963080d56689fa9", "url": "https://github.com/apache/flink/commit/f9fa39640ee8e4082f0e3d7a7963080d56689fa9", "message": "[FLINK-16802][hive] Set schema info in JobConf for Hive readers", "committedDate": "2020-04-23T08:55:15Z", "type": "commit"}, {"oid": "67355f513149f11b717932a98f406b8be92bfbe2", "url": "https://github.com/apache/flink/commit/67355f513149f11b717932a98f406b8be92bfbe2", "message": "address comments", "committedDate": "2020-04-23T08:55:15Z", "type": "commit"}, {"oid": "67355f513149f11b717932a98f406b8be92bfbe2", "url": "https://github.com/apache/flink/commit/67355f513149f11b717932a98f406b8be92bfbe2", "message": "address comments", "committedDate": "2020-04-23T08:55:15Z", "type": "forcePushed"}]}