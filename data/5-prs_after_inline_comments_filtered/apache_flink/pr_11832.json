{"pr_number": 11832, "pr_title": "[FLINK-17148][python] Support converting pandas DataFrame to Flink Table", "pr_createdAt": "2020-04-20T14:59:38Z", "pr_url": "https://github.com/apache/flink/pull/11832", "timeline": [{"oid": "f427a2215c9bb6ecb5458e8693887cafffbef491", "url": "https://github.com/apache/flink/commit/f427a2215c9bb6ecb5458e8693887cafffbef491", "message": "[FLINK-17148][python] Support converting pandas dataframe to flink table", "committedDate": "2020-04-22T05:56:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTUzODM4NQ==", "url": "https://github.com/apache/flink/pull/11832#discussion_r415538385", "bodyText": "protected", "author": "hequn8128", "createdAt": "2020-04-27T06:15:58Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/arrow/sources/AbstractArrowSourceFunction.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.arrow.sources;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.api.java.typeutils.runtime.TupleSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.table.runtime.arrow.ArrowReader;\n+import org.apache.flink.table.runtime.arrow.ArrowUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.VectorLoader;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ReadChannel;\n+import org.apache.arrow.vector.ipc.message.ArrowRecordBatch;\n+import org.apache.arrow.vector.ipc.message.MessageSerializer;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.nio.channels.Channels;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+\n+/**\n+ * An Arrow {@link SourceFunction} which takes the serialized arrow record batch data as input.\n+ *\n+ * @param <OUT> The type of the records produced by this source.\n+ */\n+@Internal\n+public abstract class AbstractArrowSourceFunction<OUT>\n+\t\textends RichParallelSourceFunction<OUT>\n+\t\timplements ResultTypeQueryable<OUT>, CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tstatic {\n+\t\tArrowUtils.checkArrowUsable();\n+\t}\n+\n+\t/**\n+\t * The type of the records produced by this source.\n+\t */\n+\tfinal DataType dataType;\n+\n+\t/**\n+\t * The array of byte array of the source data. Each element is an array\n+\t * representing an arrow batch.\n+\t */\n+\tprivate final byte[][] arrowData;\n+\n+\t/**\n+\t * Allocator which is used for byte buffer allocation.\n+\t */\n+\tprivate transient BufferAllocator allocator;\n+\n+\t/**\n+\t * Container that holds a set of vectors for the source data to emit.\n+\t */\n+\tprivate transient VectorSchemaRoot root;\n+\n+\tprivate transient volatile boolean running;\n+\n+\t/**\n+\t * The indexes of the collection of source data to emit. Each element is a tuple of\n+\t * the index of the arrow batch and the staring index inside the arrow batch.\n+\t */\n+\tprivate transient Deque<Tuple2<Integer, Integer>> indexesToEmit;\n+\n+\t/**\n+\t * The indexes of the source data which have not been emitted.\n+\t */\n+\tprivate transient ListState<Tuple2<Integer, Integer>> checkpointedState;\n+\n+\tAbstractArrowSourceFunction(DataType dataType, byte[][] arrowData) {\n+\t\tthis.dataType = Preconditions.checkNotNull(dataType);\n+\t\tthis.arrowData = Preconditions.checkNotNull(arrowData);\n+\t}\n+\n+\t@Override\n+\tpublic void open(Configuration parameters) throws Exception {\n+\t\tallocator = ArrowUtils.getRootAllocator().newChildAllocator(\"ArrowSourceFunction\", 0, Long.MAX_VALUE);\n+\t\troot = VectorSchemaRoot.create(ArrowUtils.toArrowSchema((RowType) dataType.getLogicalType()), allocator);\n+\t\trunning = true;\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\ttry {\n+\t\t\tsuper.close();\n+\t\t} finally {\n+\t\t\tif (root != null) {\n+\t\t\t\troot.close();\n+\t\t\t\troot = null;\n+\t\t\t}\n+\t\t\tif (allocator != null) {\n+\t\t\t\tallocator.close();\n+\t\t\t\tallocator = null;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n+\t\tPreconditions.checkState(this.checkpointedState == null,\n+\t\t\t\"The \" + getClass().getSimpleName() + \" has already been initialized.\");\n+\n+\t\tthis.checkpointedState = context.getOperatorStateStore().getListState(\n+\t\t\tnew ListStateDescriptor<>(\n+\t\t\t\t\"arrow-source-state\",\n+\t\t\t\tnew TupleSerializer<>(\n+\t\t\t\t\t(Class<Tuple2<Integer, Integer>>) (Class<?>) Tuple2.class,\n+\t\t\t\t\tnew TypeSerializer[]{IntSerializer.INSTANCE, IntSerializer.INSTANCE})\n+\t\t\t)\n+\t\t);\n+\n+\t\tthis.indexesToEmit = new ArrayDeque<>();\n+\t\tif (context.isRestored()) {\n+\t\t\t// upon restoring\n+\t\t\tfor (Tuple2<Integer, Integer> v : this.checkpointedState.get()) {\n+\t\t\t\tthis.indexesToEmit.add(v);\n+\t\t\t}\n+\t\t} else {\n+\t\t\t// the first time the job is executed\n+\t\t\tfinal int stepSize = getRuntimeContext().getNumberOfParallelSubtasks();\n+\t\t\tfinal int taskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+\n+\t\t\tfor (int i = taskIdx; i < arrowData.length; i += stepSize) {\n+\t\t\t\tthis.indexesToEmit.add(Tuple2.of(i, 0));\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void snapshotState(FunctionSnapshotContext context) throws Exception {\n+\t\tPreconditions.checkState(this.checkpointedState != null,\n+\t\t\t\"The \" + getClass().getSimpleName() + \" state has not been properly initialized.\");\n+\n+\t\tthis.checkpointedState.clear();\n+\t\tfor (Tuple2<Integer, Integer> v : indexesToEmit) {\n+\t\t\tthis.checkpointedState.add(v);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void run(SourceContext<OUT> ctx) throws Exception {\n+\t\tVectorLoader vectorLoader = new VectorLoader(root);\n+\t\twhile (running && !indexesToEmit.isEmpty()) {\n+\t\t\tTuple2<Integer, Integer> indexToEmit = indexesToEmit.peek();\n+\t\t\tArrowRecordBatch arrowRecordBatch = loadBatch(indexToEmit.f0);\n+\t\t\tvectorLoader.load(arrowRecordBatch);\n+\t\t\tarrowRecordBatch.close();\n+\n+\t\t\tArrowReader<OUT> arrowReader = createArrowReader(root);\n+\t\t\tint rowCount = root.getRowCount();\n+\t\t\tint nextRowId = indexToEmit.f1;\n+\t\t\twhile (nextRowId < rowCount) {\n+\t\t\t\tOUT element = arrowReader.read(nextRowId);\n+\t\t\t\tsynchronized (ctx.getCheckpointLock()) {\n+\t\t\t\t\tctx.collect(element);\n+\t\t\t\t\tindexToEmit.setField(++nextRowId, 1);\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tsynchronized (ctx.getCheckpointLock()) {\n+\t\t\t\tindexesToEmit.pop();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void cancel() {\n+\t\trunning = false;\n+\t}\n+\n+\tpublic abstract ArrowReader<OUT> createArrowReader(VectorSchemaRoot root);", "originalCommit": "f427a2215c9bb6ecb5458e8693887cafffbef491", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTU0NDc2NQ==", "url": "https://github.com/apache/flink/pull/11832#discussion_r415544765", "bodyText": "Maybe add some log in this method? For example, LOG.info the restored information.", "author": "hequn8128", "createdAt": "2020-04-27T06:30:07Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/arrow/sources/AbstractArrowSourceFunction.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.arrow.sources;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.api.java.typeutils.runtime.TupleSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.table.runtime.arrow.ArrowReader;\n+import org.apache.flink.table.runtime.arrow.ArrowUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.VectorLoader;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ReadChannel;\n+import org.apache.arrow.vector.ipc.message.ArrowRecordBatch;\n+import org.apache.arrow.vector.ipc.message.MessageSerializer;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.nio.channels.Channels;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+\n+/**\n+ * An Arrow {@link SourceFunction} which takes the serialized arrow record batch data as input.\n+ *\n+ * @param <OUT> The type of the records produced by this source.\n+ */\n+@Internal\n+public abstract class AbstractArrowSourceFunction<OUT>\n+\t\textends RichParallelSourceFunction<OUT>\n+\t\timplements ResultTypeQueryable<OUT>, CheckpointedFunction {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tstatic {\n+\t\tArrowUtils.checkArrowUsable();\n+\t}\n+\n+\t/**\n+\t * The type of the records produced by this source.\n+\t */\n+\tfinal DataType dataType;\n+\n+\t/**\n+\t * The array of byte array of the source data. Each element is an array\n+\t * representing an arrow batch.\n+\t */\n+\tprivate final byte[][] arrowData;\n+\n+\t/**\n+\t * Allocator which is used for byte buffer allocation.\n+\t */\n+\tprivate transient BufferAllocator allocator;\n+\n+\t/**\n+\t * Container that holds a set of vectors for the source data to emit.\n+\t */\n+\tprivate transient VectorSchemaRoot root;\n+\n+\tprivate transient volatile boolean running;\n+\n+\t/**\n+\t * The indexes of the collection of source data to emit. Each element is a tuple of\n+\t * the index of the arrow batch and the staring index inside the arrow batch.\n+\t */\n+\tprivate transient Deque<Tuple2<Integer, Integer>> indexesToEmit;\n+\n+\t/**\n+\t * The indexes of the source data which have not been emitted.\n+\t */\n+\tprivate transient ListState<Tuple2<Integer, Integer>> checkpointedState;\n+\n+\tAbstractArrowSourceFunction(DataType dataType, byte[][] arrowData) {\n+\t\tthis.dataType = Preconditions.checkNotNull(dataType);\n+\t\tthis.arrowData = Preconditions.checkNotNull(arrowData);\n+\t}\n+\n+\t@Override\n+\tpublic void open(Configuration parameters) throws Exception {\n+\t\tallocator = ArrowUtils.getRootAllocator().newChildAllocator(\"ArrowSourceFunction\", 0, Long.MAX_VALUE);\n+\t\troot = VectorSchemaRoot.create(ArrowUtils.toArrowSchema((RowType) dataType.getLogicalType()), allocator);\n+\t\trunning = true;\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\ttry {\n+\t\t\tsuper.close();\n+\t\t} finally {\n+\t\t\tif (root != null) {\n+\t\t\t\troot.close();\n+\t\t\t\troot = null;\n+\t\t\t}\n+\t\t\tif (allocator != null) {\n+\t\t\t\tallocator.close();\n+\t\t\t\tallocator = null;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void initializeState(FunctionInitializationContext context) throws Exception {", "originalCommit": "f427a2215c9bb6ecb5458e8693887cafffbef491", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTU2NDkyMQ==", "url": "https://github.com/apache/flink/pull/11832#discussion_r415564921", "bodyText": "Also verify the content of the data?", "author": "hequn8128", "createdAt": "2020-04-27T07:10:49Z", "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTestBase.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.arrow.sources;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.testutils.MultiShotLatch;\n+import org.apache.flink.core.testutils.OneShotLatch;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.StreamSource;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.runtime.arrow.ArrowUtils;\n+import org.apache.flink.table.runtime.arrow.ArrowWriter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;\n+\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.channels.Channels;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Abstract test base for the Arrow source function processing.\n+ */\n+public abstract class ArrowSourceFunctionTestBase<T> {\n+\n+\tstatic DataType dataType;\n+\tprivate static BufferAllocator allocator;\n+\n+\t@BeforeClass\n+\tpublic static void init() {\n+\t\tdataType = DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.STRING()));\n+\t\tallocator = ArrowUtils.getRootAllocator().newChildAllocator(\"stdout\", 0, Long.MAX_VALUE);\n+\t}\n+\n+\t@Test\n+\tpublic void testRestore() throws Exception {\n+\t\tTuple2<List<T>, Integer> testData = getTestData();\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\n+\t\tfinal AbstractStreamOperatorTestHarness<T> testHarness =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction), 1, 1, 0);\n+\t\ttestHarness.open();\n+\n+\t\tfinal Throwable[] error = new Throwable[1];\n+\t\tfinal MultiShotLatch latch = new MultiShotLatch();\n+\t\tfinal AtomicInteger numOfEmittedElements = new AtomicInteger(0);\n+\n+\t\tfinal DummySourceContext<T> sourceContext = new DummySourceContext<T>() {\n+\t\t\t@Override\n+\t\t\tpublic void collect(T element) {\n+\t\t\t\tif (numOfEmittedElements.get() == 2) {\n+\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t// fail the source function at the the second element\n+\t\t\t\t\tthrow new RuntimeException(\"Fail the arrow source\");\n+\t\t\t\t}\n+\t\t\t\tnumOfEmittedElements.incrementAndGet();\n+\t\t\t}\n+\t\t};\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction.run(sourceContext);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!t.getMessage().equals(\"Fail the arrow source\")) {\n+\t\t\t\t\terror[0] = t;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t});\n+\t\trunner.start();\n+\n+\t\tif (!latch.isTriggered()) {\n+\t\t\tlatch.await();\n+\t\t}\n+\n+\t\tOperatorSubtaskState snapshot;\n+\t\tsynchronized (sourceContext.getCheckpointLock()) {\n+\t\t\tsnapshot = testHarness.snapshot(0, 0);\n+\t\t}\n+\n+\t\trunner.join();\n+\t\ttestHarness.close();\n+\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction2 =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\t\tAbstractStreamOperatorTestHarness<T> testHarnessCopy =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction2), 1, 1, 0);\n+\t\ttestHarnessCopy.initializeState(snapshot);\n+\t\ttestHarnessCopy.open();\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner2 = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction2.run(new DummySourceContext<T>() {\n+\t\t\t\t\t@Override\n+\t\t\t\t\tpublic void collect(T element) {\n+\t\t\t\t\t\tif (numOfEmittedElements.incrementAndGet() == testData.f0.size()) {\n+\t\t\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\terror[0] = t;\n+\t\t\t}\n+\t\t});\n+\t\trunner2.start();\n+\n+\t\tif (!latch.isTriggered()) {\n+\t\t\tlatch.await();\n+\t\t}\n+\t\trunner2.join();\n+\n+\t\tAssert.assertNull(error[0]);\n+\t\tAssert.assertEquals(testData.f0.size(), numOfEmittedElements.get());", "originalCommit": "f427a2215c9bb6ecb5458e8693887cafffbef491", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTU2NDk2NA==", "url": "https://github.com/apache/flink/pull/11832#discussion_r415564964", "bodyText": "Also verify the content of the data?", "author": "hequn8128", "createdAt": "2020-04-27T07:10:55Z", "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTestBase.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.arrow.sources;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.testutils.MultiShotLatch;\n+import org.apache.flink.core.testutils.OneShotLatch;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.StreamSource;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.runtime.arrow.ArrowUtils;\n+import org.apache.flink.table.runtime.arrow.ArrowWriter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;\n+\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.channels.Channels;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Abstract test base for the Arrow source function processing.\n+ */\n+public abstract class ArrowSourceFunctionTestBase<T> {\n+\n+\tstatic DataType dataType;\n+\tprivate static BufferAllocator allocator;\n+\n+\t@BeforeClass\n+\tpublic static void init() {\n+\t\tdataType = DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.STRING()));\n+\t\tallocator = ArrowUtils.getRootAllocator().newChildAllocator(\"stdout\", 0, Long.MAX_VALUE);\n+\t}\n+\n+\t@Test\n+\tpublic void testRestore() throws Exception {\n+\t\tTuple2<List<T>, Integer> testData = getTestData();\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\n+\t\tfinal AbstractStreamOperatorTestHarness<T> testHarness =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction), 1, 1, 0);\n+\t\ttestHarness.open();\n+\n+\t\tfinal Throwable[] error = new Throwable[1];\n+\t\tfinal MultiShotLatch latch = new MultiShotLatch();\n+\t\tfinal AtomicInteger numOfEmittedElements = new AtomicInteger(0);\n+\n+\t\tfinal DummySourceContext<T> sourceContext = new DummySourceContext<T>() {\n+\t\t\t@Override\n+\t\t\tpublic void collect(T element) {\n+\t\t\t\tif (numOfEmittedElements.get() == 2) {\n+\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t// fail the source function at the the second element\n+\t\t\t\t\tthrow new RuntimeException(\"Fail the arrow source\");\n+\t\t\t\t}\n+\t\t\t\tnumOfEmittedElements.incrementAndGet();\n+\t\t\t}\n+\t\t};\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction.run(sourceContext);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!t.getMessage().equals(\"Fail the arrow source\")) {\n+\t\t\t\t\terror[0] = t;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t});\n+\t\trunner.start();\n+\n+\t\tif (!latch.isTriggered()) {\n+\t\t\tlatch.await();\n+\t\t}\n+\n+\t\tOperatorSubtaskState snapshot;\n+\t\tsynchronized (sourceContext.getCheckpointLock()) {\n+\t\t\tsnapshot = testHarness.snapshot(0, 0);\n+\t\t}\n+\n+\t\trunner.join();\n+\t\ttestHarness.close();\n+\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction2 =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\t\tAbstractStreamOperatorTestHarness<T> testHarnessCopy =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction2), 1, 1, 0);\n+\t\ttestHarnessCopy.initializeState(snapshot);\n+\t\ttestHarnessCopy.open();\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner2 = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction2.run(new DummySourceContext<T>() {\n+\t\t\t\t\t@Override\n+\t\t\t\t\tpublic void collect(T element) {\n+\t\t\t\t\t\tif (numOfEmittedElements.incrementAndGet() == testData.f0.size()) {\n+\t\t\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\terror[0] = t;\n+\t\t\t}\n+\t\t});\n+\t\trunner2.start();\n+\n+\t\tif (!latch.isTriggered()) {\n+\t\t\tlatch.await();\n+\t\t}\n+\t\trunner2.join();\n+\n+\t\tAssert.assertNull(error[0]);\n+\t\tAssert.assertEquals(testData.f0.size(), numOfEmittedElements.get());\n+\t}\n+\n+\t@Test\n+\tpublic void testParallelProcessing() throws Exception {\n+\t\tTuple2<List<T>, Integer> testData = getTestData();\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\n+\t\tfinal AbstractStreamOperatorTestHarness<T> testHarness =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction), 2, 2, 0);\n+\t\ttestHarness.open();\n+\n+\t\tfinal Throwable[] error = new Throwable[2];\n+\t\tfinal OneShotLatch latch = new OneShotLatch();\n+\t\tfinal AtomicInteger numOfEmittedElements = new AtomicInteger(0);\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction.run(new DummySourceContext<T>() {\n+\t\t\t\t\t@Override\n+\t\t\t\t\tpublic void collect(T element) {\n+\t\t\t\t\t\tif (numOfEmittedElements.incrementAndGet() == testData.f0.size()) {\n+\t\t\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\terror[0] = t;\n+\t\t\t}\n+\t\t});\n+\t\trunner.start();\n+\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction2 =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\t\tfinal AbstractStreamOperatorTestHarness<T> testHarness2 =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction2), 2, 2, 1);\n+\t\ttestHarness2.open();\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner2 = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction2.run(new DummySourceContext<T>() {\n+\t\t\t\t\t@Override\n+\t\t\t\t\tpublic void collect(T element) {\n+\t\t\t\t\t\tif (numOfEmittedElements.incrementAndGet() == testData.f0.size()) {\n+\t\t\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\terror[1] = t;\n+\t\t\t}\n+\t\t});\n+\t\trunner2.start();\n+\n+\t\tif (!latch.isTriggered()) {\n+\t\t\tlatch.await();\n+\t\t}\n+\n+\t\trunner.join();\n+\t\trunner2.join();\n+\t\ttestHarness.close();\n+\t\ttestHarness2.close();\n+\n+\t\tAssert.assertNull(error[0]);\n+\t\tAssert.assertNull(error[1]);\n+\t\tAssert.assertEquals(testData.f0.size(), numOfEmittedElements.get());", "originalCommit": "f427a2215c9bb6ecb5458e8693887cafffbef491", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTU3MDIzNw==", "url": "https://github.com/apache/flink/pull/11832#discussion_r415570237", "bodyText": "Add the corresponding assert to verify that error[0] is not null?", "author": "hequn8128", "createdAt": "2020-04-27T07:20:03Z", "path": "flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTestBase.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.arrow.sources;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.testutils.MultiShotLatch;\n+import org.apache.flink.core.testutils.OneShotLatch;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.StreamSource;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.runtime.arrow.ArrowUtils;\n+import org.apache.flink.table.runtime.arrow.ArrowWriter;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;\n+\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowStreamWriter;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.channels.Channels;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Abstract test base for the Arrow source function processing.\n+ */\n+public abstract class ArrowSourceFunctionTestBase<T> {\n+\n+\tstatic DataType dataType;\n+\tprivate static BufferAllocator allocator;\n+\n+\t@BeforeClass\n+\tpublic static void init() {\n+\t\tdataType = DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.STRING()));\n+\t\tallocator = ArrowUtils.getRootAllocator().newChildAllocator(\"stdout\", 0, Long.MAX_VALUE);\n+\t}\n+\n+\t@Test\n+\tpublic void testRestore() throws Exception {\n+\t\tTuple2<List<T>, Integer> testData = getTestData();\n+\t\tfinal AbstractArrowSourceFunction<T> arrowSourceFunction =\n+\t\t\tcreateTestArrowSourceFunction(testData.f0, testData.f1);\n+\n+\t\tfinal AbstractStreamOperatorTestHarness<T> testHarness =\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamSource<>(arrowSourceFunction), 1, 1, 0);\n+\t\ttestHarness.open();\n+\n+\t\tfinal Throwable[] error = new Throwable[1];\n+\t\tfinal MultiShotLatch latch = new MultiShotLatch();\n+\t\tfinal AtomicInteger numOfEmittedElements = new AtomicInteger(0);\n+\n+\t\tfinal DummySourceContext<T> sourceContext = new DummySourceContext<T>() {\n+\t\t\t@Override\n+\t\t\tpublic void collect(T element) {\n+\t\t\t\tif (numOfEmittedElements.get() == 2) {\n+\t\t\t\t\tlatch.trigger();\n+\t\t\t\t\t// fail the source function at the the second element\n+\t\t\t\t\tthrow new RuntimeException(\"Fail the arrow source\");\n+\t\t\t\t}\n+\t\t\t\tnumOfEmittedElements.incrementAndGet();\n+\t\t\t}\n+\t\t};\n+\n+\t\t// run the source asynchronously\n+\t\tThread runner = new Thread(() -> {\n+\t\t\ttry {\n+\t\t\t\tarrowSourceFunction.run(sourceContext);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!t.getMessage().equals(\"Fail the arrow source\")) {\n+\t\t\t\t\terror[0] = t;", "originalCommit": "f427a2215c9bb6ecb5458e8693887cafffbef491", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA3NzUwMw==", "url": "https://github.com/apache/flink/pull/11832#discussion_r417077503", "bodyText": "error[0] should always be null and it has been asserted at the end of the test. I'm not sure what do you mean about verify that error[0] is not null?", "author": "dianfu", "createdAt": "2020-04-29T05:39:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTU3MDIzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzExMDI2Mg==", "url": "https://github.com/apache/flink/pull/11832#discussion_r417110262", "bodyText": "Ignore my comment here. You are right.", "author": "hequn8128", "createdAt": "2020-04-29T07:13:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTU3MDIzNw=="}], "type": "inlineReview"}, {"oid": "a30c4d84d218a3ab91a232c648f0a586df75eaa9", "url": "https://github.com/apache/flink/commit/a30c4d84d218a3ab91a232c648f0a586df75eaa9", "message": "[FLINK-17148][python] Support converting pandas dataframe to flink table", "committedDate": "2020-04-29T07:25:38Z", "type": "commit"}, {"oid": "a30c4d84d218a3ab91a232c648f0a586df75eaa9", "url": "https://github.com/apache/flink/commit/a30c4d84d218a3ab91a232c648f0a586df75eaa9", "message": "[FLINK-17148][python] Support converting pandas dataframe to flink table", "committedDate": "2020-04-29T07:25:38Z", "type": "forcePushed"}, {"oid": "ada363133be3dccbc3ad8da5af44d3fcd08a0269", "url": "https://github.com/apache/flink/commit/ada363133be3dccbc3ad8da5af44d3fcd08a0269", "message": "address review", "committedDate": "2020-04-29T13:05:46Z", "type": "forcePushed"}, {"oid": "0a37369514bce10618debc2947cf852e6d188b2f", "url": "https://github.com/apache/flink/commit/0a37369514bce10618debc2947cf852e6d188b2f", "message": "address review", "committedDate": "2020-04-29T13:18:57Z", "type": "commit"}, {"oid": "0a37369514bce10618debc2947cf852e6d188b2f", "url": "https://github.com/apache/flink/commit/0a37369514bce10618debc2947cf852e6d188b2f", "message": "address review", "committedDate": "2020-04-29T13:18:57Z", "type": "forcePushed"}]}