{"pr_number": 11935, "pr_title": "[FLINK-17431][sql-parser-hive][hive] Implement table DDLs for Hive di\u2026", "pr_createdAt": "2020-04-28T13:16:44Z", "pr_url": "https://github.com/apache/flink/pull/11935", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyNTYwMw==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417025603", "bodyText": "We use \"-\" instead of \"_\" in property key.", "author": "JingsongLi", "createdAt": "2020-04-29T01:54:31Z", "path": "flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/ddl/SqlCreateHiveTable.java", "diffHunk": "@@ -0,0 +1,467 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive.ddl;\n+\n+import org.apache.flink.sql.parser.ddl.SqlCreateTable;\n+import org.apache.flink.sql.parser.ddl.SqlTableColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.sql.parser.hive.impl.ParseException;\n+import org.apache.flink.table.catalog.config.CatalogConfig;\n+\n+import org.apache.calcite.sql.SqlCharStringLiteral;\n+import org.apache.calcite.sql.SqlIdentifier;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.SqlWriter;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * CREATE Table DDL for Hive dialect.\n+ */\n+public class SqlCreateHiveTable extends SqlCreateTable {\n+\n+\tpublic static final String TABLE_LOCATION_URI = \"table.location_uri\";", "originalCommit": "7e431d6474f2e1ae03355a68b481d686b162c509", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA0MTk2OA==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417041968", "bodyText": "This is following the example of database.location_uri which has been used for quite a while. I'd like to clarify these properties are all internal to hive connector and not supposed to be used by users. Does the name matter for such properties?", "author": "lirui-apache", "createdAt": "2020-04-29T03:04:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyNTYwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA0MzQ1NQ==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417043455", "bodyText": "Even just for internal now, I don't think we can random pick one.\nThere is a way to create hive table from Flink DDL, I don't think we can say there is no one to use it.", "author": "JingsongLi", "createdAt": "2020-04-29T03:10:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyNTYwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA1MDM2OQ==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417050369", "bodyText": "I'm fine with changing the names to comply with Flink convention. But I really don't like the idea that users can decipher these properties and use them to create Hive tables. It's hacky and error-prone. We should mention that in our doc or even consider banning it in code.", "author": "lirui-apache", "createdAt": "2020-04-29T03:42:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyNTYwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA1MjIzNg==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417052236", "bodyText": "OK:\n\nCreate hive table from Flink DDL is not good, we should avoid this.\nBut, we need to comply with Flink convention, it is property of Flink table.", "author": "JingsongLi", "createdAt": "2020-04-29T03:51:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyNTYwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyNTY0Nw==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417025647", "bodyText": "Do we need add \"table.\"?", "author": "JingsongLi", "createdAt": "2020-04-29T01:54:42Z", "path": "flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/ddl/SqlCreateHiveTable.java", "diffHunk": "@@ -0,0 +1,467 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive.ddl;\n+\n+import org.apache.flink.sql.parser.ddl.SqlCreateTable;\n+import org.apache.flink.sql.parser.ddl.SqlTableColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.sql.parser.hive.impl.ParseException;\n+import org.apache.flink.table.catalog.config.CatalogConfig;\n+\n+import org.apache.calcite.sql.SqlCharStringLiteral;\n+import org.apache.calcite.sql.SqlIdentifier;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.SqlWriter;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * CREATE Table DDL for Hive dialect.\n+ */\n+public class SqlCreateHiveTable extends SqlCreateTable {\n+\n+\tpublic static final String TABLE_LOCATION_URI = \"table.location_uri\";\n+\tpublic static final String TABLE_IS_TEMPORARY = \"table.is.temporary\";", "originalCommit": "7e431d6474f2e1ae03355a68b481d686b162c509", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA4MDgzOQ==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417080839", "bodyText": "I think it's better to have the explicit prefix to indicate it's a table property. Besides, I'd like to make these properties somehow \"verbose\", because they're reserved and verbosity may reduce the chance we collide with user defined properties.", "author": "lirui-apache", "createdAt": "2020-04-29T05:51:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyNTY0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA4Mzk1Mw==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417083953", "bodyText": "If you want \"reserved and verbosity\".\nWhy not unify them to \"hive.\" prefix for all(create table create database....)?", "author": "JingsongLi", "createdAt": "2020-04-29T06:03:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyNTY0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA5Mjg3OA==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417092878", "bodyText": "Yes I think that's viable. Will update to add the prefix.", "author": "lirui-apache", "createdAt": "2020-04-29T06:31:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyNTY0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyNTY3Mg==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417025672", "bodyText": "\"is-external\"?", "author": "JingsongLi", "createdAt": "2020-04-29T01:54:51Z", "path": "flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/ddl/SqlCreateHiveTable.java", "diffHunk": "@@ -0,0 +1,467 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive.ddl;\n+\n+import org.apache.flink.sql.parser.ddl.SqlCreateTable;\n+import org.apache.flink.sql.parser.ddl.SqlTableColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.sql.parser.hive.impl.ParseException;\n+import org.apache.flink.table.catalog.config.CatalogConfig;\n+\n+import org.apache.calcite.sql.SqlCharStringLiteral;\n+import org.apache.calcite.sql.SqlIdentifier;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.SqlWriter;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * CREATE Table DDL for Hive dialect.\n+ */\n+public class SqlCreateHiveTable extends SqlCreateTable {\n+\n+\tpublic static final String TABLE_LOCATION_URI = \"table.location_uri\";\n+\tpublic static final String TABLE_IS_TEMPORARY = \"table.is.temporary\";\n+\tpublic static final String TABLE_IS_EXTERNAL = \"table.is.external\";", "originalCommit": "7e431d6474f2e1ae03355a68b481d686b162c509", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAyNTg1Nw==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417025857", "bodyText": "Can you take a look to presto? We can refer to its properties.", "author": "JingsongLi", "createdAt": "2020-04-29T01:55:48Z", "path": "flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/ddl/SqlCreateHiveTable.java", "diffHunk": "@@ -0,0 +1,467 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive.ddl;\n+\n+import org.apache.flink.sql.parser.ddl.SqlCreateTable;\n+import org.apache.flink.sql.parser.ddl.SqlTableColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.sql.parser.hive.impl.ParseException;\n+import org.apache.flink.table.catalog.config.CatalogConfig;\n+\n+import org.apache.calcite.sql.SqlCharStringLiteral;\n+import org.apache.calcite.sql.SqlIdentifier;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.SqlWriter;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * CREATE Table DDL for Hive dialect.\n+ */\n+public class SqlCreateHiveTable extends SqlCreateTable {\n+\n+\tpublic static final String TABLE_LOCATION_URI = \"table.location_uri\";\n+\tpublic static final String TABLE_IS_TEMPORARY = \"table.is.temporary\";\n+\tpublic static final String TABLE_IS_EXTERNAL = \"table.is.external\";\n+\tpublic static final String PK_CONSTRAINT_NAME = \"pk.constraint.name\";", "originalCommit": "7e431d6474f2e1ae03355a68b481d686b162c509", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA5NTQyNA==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417095424", "bodyText": "A better name? initiateTableFromProperties too, I don't know you remove key from properties here...", "author": "JingsongLi", "createdAt": "2020-04-29T06:38:00Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java", "diffHunk": "@@ -175,6 +193,77 @@ public static boolean requireRelyConstraint(byte trait) {\n \t\treturn Optional.of(String.join(\" and \", filters));\n \t}\n \n+\t/**\n+\t * Extract DDL semantics from properties and use it to initiate the table.\n+\t */\n+\tpublic static void initiateTableFromProperties(Table hiveTable, Map<String, String> properties) {\n+\t\tsetExternal(hiveTable, properties);\n+\t\tsetRowFormat(hiveTable.getSd(), properties);\n+\t\tsetStoredAs(hiveTable.getSd(), properties);\n+\t\tsetLocation(hiveTable.getSd(), properties);\n+\t}\n+\n+\tprivate static void setExternal(Table hiveTable, Map<String, String> properties) {\n+\t\tboolean external = Boolean.parseBoolean(properties.remove(TABLE_IS_EXTERNAL));\n+\t\tif (external) {\n+\t\t\thiveTable.setTableType(TableType.EXTERNAL_TABLE.toString());\n+\t\t\t// follow Hive to set this property\n+\t\t\thiveTable.getParameters().put(\"EXTERNAL\", \"TRUE\");\n+\t\t}\n+\t}\n+\n+\tprivate static void setLocation(StorageDescriptor sd, Map<String, String> properties) {\n+\t\tString location = properties.remove(TABLE_LOCATION_URI);\n+\t\tif (location != null) {\n+\t\t\tsd.setLocation(location);\n+\t\t}\n+\t}\n+\n+\tprivate static void setRowFormat(StorageDescriptor sd, Map<String, String> properties) {", "originalCommit": "7e431d6474f2e1ae03355a68b481d686b162c509", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzExOTg4NQ==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417119885", "bodyText": "I will mention in the java doc that the keys will be removed. Not sure about the name though. Does extractRowFormat sound better? Or any suggested names?", "author": "lirui-apache", "createdAt": "2020-04-29T07:33:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA5NTQyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzE3OTEwOQ==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417179109", "bodyText": "extract looks better", "author": "JingsongLi", "createdAt": "2020-04-29T09:21:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA5NTQyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzE4MDk3NA==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417180974", "bodyText": "can we remove \"table\" for these three keys?", "author": "JingsongLi", "createdAt": "2020-04-29T09:25:00Z", "path": "flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/ddl/SqlCreateHiveTable.java", "diffHunk": "@@ -0,0 +1,467 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive.ddl;\n+\n+import org.apache.flink.sql.parser.ddl.SqlCreateTable;\n+import org.apache.flink.sql.parser.ddl.SqlTableColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.sql.parser.hive.impl.ParseException;\n+import org.apache.flink.table.catalog.config.CatalogConfig;\n+\n+import org.apache.calcite.sql.SqlCharStringLiteral;\n+import org.apache.calcite.sql.SqlIdentifier;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.SqlWriter;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * CREATE Table DDL for Hive dialect.\n+ */\n+public class SqlCreateHiveTable extends SqlCreateTable {\n+\n+\tpublic static final String TABLE_LOCATION_URI = \"hive.table.location-uri\";", "originalCommit": "062114a456dffdef8b5226105037c398c012a56d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzE4MTQ0MQ==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417181441", "bodyText": "Consider kafka, filesystem and etc... there is no \"table\" prefix, it is reduntant.", "author": "JingsongLi", "createdAt": "2020-04-29T09:25:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzE4MDk3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzI5OTYwOA==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417299608", "bodyText": "OK.", "author": "lirui-apache", "createdAt": "2020-04-29T13:07:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzE4MDk3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzE4MzQ3Mw==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417183473", "bodyText": "storage.file-format?", "author": "JingsongLi", "createdAt": "2020-04-29T09:29:28Z", "path": "flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/ddl/SqlCreateHiveTable.java", "diffHunk": "@@ -0,0 +1,467 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive.ddl;\n+\n+import org.apache.flink.sql.parser.ddl.SqlCreateTable;\n+import org.apache.flink.sql.parser.ddl.SqlTableColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.sql.parser.hive.impl.ParseException;\n+import org.apache.flink.table.catalog.config.CatalogConfig;\n+\n+import org.apache.calcite.sql.SqlCharStringLiteral;\n+import org.apache.calcite.sql.SqlIdentifier;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.SqlWriter;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * CREATE Table DDL for Hive dialect.\n+ */\n+public class SqlCreateHiveTable extends SqlCreateTable {\n+\n+\tpublic static final String TABLE_LOCATION_URI = \"hive.table.location-uri\";\n+\tpublic static final String TABLE_IS_TEMPORARY = \"hive.table.is-temporary\";\n+\tpublic static final String TABLE_IS_EXTERNAL = \"hive.table.is-external\";\n+\tpublic static final String PK_CONSTRAINT_NAME = \"hive.pk.constraint.name\";\n+\tpublic static final String PK_CONSTRAINT_TRAIT = \"hive.pk.constraint.trait\";\n+\tpublic static final String NOT_NULL_CONSTRAINT_TRAITS = \"hive.not.null.constraint.traits\";\n+\n+\tprivate final HiveTableCreationContext creationContext;\n+\tprivate final SqlNodeList originPropList;\n+\tprivate final boolean isTemporary;\n+\tprivate final boolean isExternal;\n+\tprivate final HiveTableRowFormat rowFormat;\n+\tprivate final HiveTableStoredAs storedAs;\n+\tprivate final SqlCharStringLiteral location;\n+\n+\tpublic SqlCreateHiveTable(SqlParserPos pos, SqlIdentifier tableName, SqlNodeList columnList,\n+\t\t\tHiveTableCreationContext creationContext, SqlNodeList propertyList,\n+\t\t\tSqlNodeList partColList, @Nullable SqlCharStringLiteral comment, boolean isTemporary, boolean isExternal,\n+\t\t\tHiveTableRowFormat rowFormat, HiveTableStoredAs storedAs, SqlCharStringLiteral location) throws ParseException {\n+\n+\t\tsuper(pos, tableName, columnList, creationContext.primaryKeyList, creationContext.uniqueKeysList,\n+\t\t\t\tHiveDDLUtils.checkReservedTableProperties(propertyList), extractPartColIdentifiers(partColList), null,\n+\t\t\t\tcomment, null);\n+\n+\t\tHiveDDLUtils.convertDataTypes(columnList);\n+\t\tHiveDDLUtils.convertDataTypes(partColList);\n+\t\toriginPropList = new SqlNodeList(propertyList.getList(), propertyList.getParserPosition());\n+\t\t// mark it as a hive table\n+\t\tHiveDDLUtils.ensureNonGeneric(propertyList);\n+\t\tpropertyList.add(HiveDDLUtils.toTableOption(CatalogConfig.IS_GENERIC, \"false\", pos));\n+\t\t// set temporary\n+\t\tthis.isTemporary = isTemporary;\n+\t\tif (isTemporary) {\n+\t\t\tpropertyList.add(HiveDDLUtils.toTableOption(TABLE_IS_TEMPORARY, \"true\", pos));\n+\t\t}\n+\t\t// set external\n+\t\tthis.isExternal = isExternal;\n+\t\tif (isExternal) {\n+\t\t\tpropertyList.add(HiveDDLUtils.toTableOption(TABLE_IS_EXTERNAL, \"true\", pos));\n+\t\t}\n+\t\t// add partition cols to col list\n+\t\tif (partColList != null) {\n+\t\t\tfor (SqlNode partCol : partColList) {\n+\t\t\t\tcolumnList.add(partCol);\n+\t\t\t}\n+\t\t}\n+\t\t// set PRIMARY KEY\n+\t\tthis.creationContext = creationContext;\n+\t\tif (creationContext.primaryKeyList.size() > 0) {\n+\t\t\t// PK list is taken care of by super class, we need to set constraint name and trait here\n+\t\t\tSqlIdentifier pkName = creationContext.pkName;\n+\t\t\tif (pkName != null) {\n+\t\t\t\tpropertyList.add(HiveDDLUtils.toTableOption(\n+\t\t\t\t\t\tPK_CONSTRAINT_NAME, pkName.getSimple(), pkName.getParserPosition()));\n+\t\t\t}\n+\t\t\tpropertyList.add(HiveDDLUtils.toTableOption(\n+\t\t\t\t\tPK_CONSTRAINT_TRAIT, creationContext.pkTrait.toString(), propertyList.getParserPosition()));\n+\t\t}\n+\t\t// set NOT NULL\n+\t\tif (creationContext.notNullTraits != null) {\n+\t\t\t// NOT NULL cols are taken care of by super class, we need to set constraint traits here\n+\t\t\tString notNullTraits = creationContext.notNullTraits.stream()\n+\t\t\t\t\t.map(Object::toString).collect(Collectors.joining(HiveDDLUtils.COL_DELIMITER));\n+\t\t\tpropertyList.add(HiveDDLUtils.toTableOption(\n+\t\t\t\t\tNOT_NULL_CONSTRAINT_TRAITS, notNullTraits, propertyList.getParserPosition()));\n+\t\t}\n+\t\t// set row format\n+\t\tthis.rowFormat = rowFormat;\n+\t\tif (rowFormat != null) {\n+\t\t\tfor (SqlNode node : rowFormat.toPropList()) {\n+\t\t\t\tpropertyList.add(node);\n+\t\t\t}\n+\t\t}\n+\t\t// set stored as\n+\t\tthis.storedAs = storedAs;\n+\t\tif (storedAs != null) {\n+\t\t\tfor (SqlNode node : storedAs.toPropList()) {\n+\t\t\t\tpropertyList.add(node);\n+\t\t\t}\n+\t\t}\n+\t\t// set location\n+\t\tthis.location = location;\n+\t\tif (location != null) {\n+\t\t\tpropertyList.add(HiveDDLUtils.toTableOption(TABLE_LOCATION_URI, location, location.getParserPosition()));\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void unparse(SqlWriter writer, int leftPrec, int rightPrec) {\n+\t\twriter.keyword(\"CREATE\");\n+\t\tif (isTemporary) {\n+\t\t\twriter.keyword(\"TEMPORARY\");\n+\t\t}\n+\t\tif (isExternal) {\n+\t\t\twriter.keyword(\"EXTERNAL\");\n+\t\t}\n+\t\twriter.keyword(\"TABLE\");\n+\t\tif (ifNotExists) {\n+\t\t\twriter.keyword(\"IF NOT EXISTS\");\n+\t\t}\n+\t\tgetTableName().unparse(writer, leftPrec, rightPrec);\n+\t\t// columns\n+\t\tint numPartCol = getPartitionKeyList() == null ? 0 : getPartitionKeyList().size();\n+\t\tSqlWriter.Frame frame = writer.startList(SqlWriter.FrameTypeEnum.create(\"sds\"), \"(\", \")\");\n+\t\tSqlNodeList allCols = getColumnList();\n+\t\tunparseColumns(creationContext,\n+\t\t\t\tnew SqlNodeList(allCols.getList().subList(0, allCols.size() - numPartCol), allCols.getParserPosition()),\n+\t\t\t\twriter, leftPrec, rightPrec);\n+\t\tif (creationContext.primaryKeyList.size() > 0) {\n+\t\t\tprintIndent(writer);\n+\t\t\tif (creationContext.pkName != null) {\n+\t\t\t\twriter.keyword(\"CONSTRAINT\");\n+\t\t\t\tcreationContext.pkName.unparse(writer, leftPrec, rightPrec);\n+\t\t\t}\n+\t\t\twriter.keyword(\"PRIMARY KEY\");\n+\t\t\tSqlWriter.Frame pkFrame = writer.startList(\"(\", \")\");\n+\t\t\tcreationContext.primaryKeyList.unparse(writer, leftPrec, rightPrec);\n+\t\t\twriter.endList(pkFrame);\n+\t\t\tHiveDDLUtils.unparseConstraintTrait(creationContext.pkTrait, writer);\n+\t\t}\n+\t\twriter.newlineAndIndent();\n+\t\twriter.endList(frame);\n+\t\t// table comment\n+\t\tgetComment().ifPresent(c -> {\n+\t\t\twriter.keyword(\"COMMENT\");\n+\t\t\tc.unparse(writer, leftPrec, rightPrec);\n+\t\t});\n+\t\t// partitions\n+\t\tif (numPartCol > 0) {\n+\t\t\twriter.newlineAndIndent();\n+\t\t\twriter.keyword(\"PARTITIONED BY\");\n+\t\t\tSqlWriter.Frame partitionedByFrame = writer.startList(\"(\", \")\");\n+\t\t\tunparseColumns(creationContext,\n+\t\t\t\t\tnew SqlNodeList(allCols.getList().subList(allCols.size() - numPartCol, allCols.size()), allCols.getParserPosition()),\n+\t\t\t\t\twriter, leftPrec, rightPrec);\n+\t\t\twriter.newlineAndIndent();\n+\t\t\twriter.endList(partitionedByFrame);\n+\t\t}\n+\t\t// row format\n+\t\tunparseRowFormat(writer, leftPrec, rightPrec);\n+\t\t// stored as\n+\t\tunparseStoredAs(writer, leftPrec, rightPrec);\n+\t\t// location\n+\t\tif (location != null) {\n+\t\t\twriter.newlineAndIndent();\n+\t\t\twriter.keyword(\"LOCATION\");\n+\t\t\tlocation.unparse(writer, leftPrec, rightPrec);\n+\t\t}\n+\t\t// properties\n+\t\tif (originPropList.size() > 0) {\n+\t\t\twriter.newlineAndIndent();\n+\t\t\twriter.keyword(\"TBLPROPERTIES\");\n+\t\t\tunparsePropList(originPropList, writer, leftPrec, rightPrec);\n+\t\t}\n+\t}\n+\n+\tprivate void unparseStoredAs(SqlWriter writer, int leftPrec, int rightPrec) {\n+\t\tif (storedAs == null) {\n+\t\t\treturn;\n+\t\t}\n+\t\twriter.newlineAndIndent();\n+\t\twriter.keyword(\"STORED AS\");\n+\t\tif (storedAs.fileFormat != null) {\n+\t\t\tstoredAs.fileFormat.unparse(writer, leftPrec, rightPrec);\n+\t\t} else {\n+\t\t\twriter.keyword(\"INPUTFORMAT\");\n+\t\t\tstoredAs.intputFormat.unparse(writer, leftPrec, rightPrec);\n+\t\t\twriter.keyword(\"OUTPUTFORMAT\");\n+\t\t\tstoredAs.outputFormat.unparse(writer, leftPrec, rightPrec);\n+\t\t}\n+\t}\n+\n+\tprivate void unparseRowFormat(SqlWriter writer, int leftPrec, int rightPrec) {\n+\t\tif (rowFormat == null) {\n+\t\t\treturn;\n+\t\t}\n+\t\twriter.newlineAndIndent();\n+\t\twriter.keyword(\"ROW FORMAT\");\n+\t\tif (rowFormat.serdeClass != null) {\n+\t\t\twriter.keyword(\"SERDE\");\n+\t\t\trowFormat.serdeClass.unparse(writer, leftPrec, rightPrec);\n+\t\t\tif (rowFormat.serdeProps != null) {\n+\t\t\t\twriter.keyword(\"WITH SERDEPROPERTIES\");\n+\t\t\t\tunparsePropList(rowFormat.serdeProps, writer, leftPrec, rightPrec);\n+\t\t\t}\n+\t\t} else {\n+\t\t\twriter.keyword(\"DELIMITED\");\n+\t\t\tSqlCharStringLiteral fieldDelim = rowFormat.delimitPropToValue.get(HiveTableRowFormat.FIELD_DELIM);\n+\t\t\tSqlCharStringLiteral escape = rowFormat.delimitPropToValue.get(HiveTableRowFormat.ESCAPE_CHAR);\n+\t\t\tif (fieldDelim != null) {\n+\t\t\t\twriter.keyword(\"FIELDS TERMINATED BY\");\n+\t\t\t\tfieldDelim.unparse(writer, leftPrec, rightPrec);\n+\t\t\t\tif (escape != null) {\n+\t\t\t\t\twriter.keyword(\"ESCAPED BY\");\n+\t\t\t\t\tescape.unparse(writer, leftPrec, rightPrec);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tSqlCharStringLiteral collectionDelim = rowFormat.delimitPropToValue.get(HiveTableRowFormat.COLLECTION_DELIM);\n+\t\t\tif (collectionDelim != null) {\n+\t\t\t\twriter.keyword(\"COLLECTION ITEMS TERMINATED BY\");\n+\t\t\t\tcollectionDelim.unparse(writer, leftPrec, rightPrec);\n+\t\t\t}\n+\t\t\tSqlCharStringLiteral mapKeyDelim = rowFormat.delimitPropToValue.get(HiveTableRowFormat.MAPKEY_DELIM);\n+\t\t\tif (mapKeyDelim != null) {\n+\t\t\t\twriter.keyword(\"MAP KEYS TERMINATED BY\");\n+\t\t\t\tmapKeyDelim.unparse(writer, leftPrec, rightPrec);\n+\t\t\t}\n+\t\t\tSqlCharStringLiteral lineDelim = rowFormat.delimitPropToValue.get(HiveTableRowFormat.LINE_DELIM);\n+\t\t\tif (lineDelim != null) {\n+\t\t\t\twriter.keyword(\"LINES TERMINATED BY\");\n+\t\t\t\tlineDelim.unparse(writer, leftPrec, rightPrec);\n+\t\t\t}\n+\t\t\tSqlCharStringLiteral nullAs = rowFormat.delimitPropToValue.get(HiveTableRowFormat.SERIALIZATION_NULL_FORMAT);\n+\t\t\tif (nullAs != null) {\n+\t\t\t\twriter.keyword(\"NULL DEFINED AS\");\n+\t\t\t\tnullAs.unparse(writer, leftPrec, rightPrec);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void unparsePropList(SqlNodeList propList, SqlWriter writer, int leftPrec, int rightPrec) {\n+\t\tSqlWriter.Frame withFrame = writer.startList(\"(\", \")\");\n+\t\tfor (SqlNode property : propList) {\n+\t\t\tprintIndent(writer);\n+\t\t\tproperty.unparse(writer, leftPrec, rightPrec);\n+\t\t}\n+\t\twriter.newlineAndIndent();\n+\t\twriter.endList(withFrame);\n+\t}\n+\n+\tprivate void unparseColumns(HiveTableCreationContext context, SqlNodeList columns,\n+\t\t\tSqlWriter writer, int leftPrec, int rightPrec) {\n+\t\tList<Byte> notNullTraits = context.notNullTraits;\n+\t\tint traitIndex = 0;\n+\t\tfor (SqlNode node : columns) {\n+\t\t\tprintIndent(writer);\n+\t\t\tSqlTableColumn column = (SqlTableColumn) node;\n+\t\t\tcolumn.getName().unparse(writer, leftPrec, rightPrec);\n+\t\t\twriter.print(\" \");\n+\t\t\tcolumn.getType().unparse(writer, leftPrec, rightPrec);\n+\t\t\tif (column.getType().getNullable() != null && !column.getType().getNullable()) {\n+\t\t\t\twriter.keyword(\"NOT NULL\");\n+\t\t\t\tHiveDDLUtils.unparseConstraintTrait(notNullTraits.get(traitIndex++), writer);\n+\t\t\t}\n+\t\t\tcolumn.getComment().ifPresent(c -> {\n+\t\t\t\twriter.keyword(\"COMMENT\");\n+\t\t\t\tc.unparse(writer, leftPrec, rightPrec);\n+\t\t\t});\n+\t\t}\n+\t}\n+\n+\t// Extract the identifiers from partition col list -- that's what SqlCreateTable expects for partition keys\n+\tprivate static SqlNodeList extractPartColIdentifiers(SqlNodeList partCols) {\n+\t\tif (partCols == null) {\n+\t\t\treturn null;\n+\t\t}\n+\t\tSqlNodeList res = new SqlNodeList(partCols.getParserPosition());\n+\t\tfor (SqlNode node : partCols) {\n+\t\t\tSqlTableColumn partCol = (SqlTableColumn) node;\n+\t\t\tres.add(partCol.getName());\n+\t\t}\n+\t\treturn res;\n+\t}\n+\n+\t/**\n+\t * Creation context for a Hive table.\n+\t */\n+\tpublic static class HiveTableCreationContext extends TableCreationContext {\n+\t\tpublic SqlIdentifier pkName = null;\n+\t\tpublic Byte pkTrait = null;\n+\t\tpublic SqlIdentifier ukName = null;\n+\t\tpublic List<Byte> notNullTraits = null;\n+\t}\n+\n+\t/**\n+\t * To represent STORED AS in CREATE TABLE DDL.\n+\t */\n+\tpublic static class HiveTableStoredAs {\n+\n+\t\tpublic static final String STORED_AS_FILE_FORMAT = \"hive.stored.as.file.format\";", "originalCommit": "062114a456dffdef8b5226105037c398c012a56d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzE4NDEyMw==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417184123", "bodyText": "delimiter?", "author": "JingsongLi", "createdAt": "2020-04-29T09:30:37Z", "path": "flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/ddl/SqlCreateHiveTable.java", "diffHunk": "@@ -0,0 +1,467 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive.ddl;\n+\n+import org.apache.flink.sql.parser.ddl.SqlCreateTable;\n+import org.apache.flink.sql.parser.ddl.SqlTableColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.sql.parser.hive.impl.ParseException;\n+import org.apache.flink.table.catalog.config.CatalogConfig;\n+\n+import org.apache.calcite.sql.SqlCharStringLiteral;\n+import org.apache.calcite.sql.SqlIdentifier;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.SqlWriter;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * CREATE Table DDL for Hive dialect.\n+ */\n+public class SqlCreateHiveTable extends SqlCreateTable {\n+\n+\tpublic static final String TABLE_LOCATION_URI = \"hive.table.location-uri\";\n+\tpublic static final String TABLE_IS_TEMPORARY = \"hive.table.is-temporary\";\n+\tpublic static final String TABLE_IS_EXTERNAL = \"hive.table.is-external\";\n+\tpublic static final String PK_CONSTRAINT_NAME = \"hive.pk.constraint.name\";\n+\tpublic static final String PK_CONSTRAINT_TRAIT = \"hive.pk.constraint.trait\";\n+\tpublic static final String NOT_NULL_CONSTRAINT_TRAITS = \"hive.not.null.constraint.traits\";\n+\n+\tprivate final HiveTableCreationContext creationContext;\n+\tprivate final SqlNodeList originPropList;\n+\tprivate final boolean isTemporary;\n+\tprivate final boolean isExternal;\n+\tprivate final HiveTableRowFormat rowFormat;\n+\tprivate final HiveTableStoredAs storedAs;\n+\tprivate final SqlCharStringLiteral location;\n+\n+\tpublic SqlCreateHiveTable(SqlParserPos pos, SqlIdentifier tableName, SqlNodeList columnList,\n+\t\t\tHiveTableCreationContext creationContext, SqlNodeList propertyList,\n+\t\t\tSqlNodeList partColList, @Nullable SqlCharStringLiteral comment, boolean isTemporary, boolean isExternal,\n+\t\t\tHiveTableRowFormat rowFormat, HiveTableStoredAs storedAs, SqlCharStringLiteral location) throws ParseException {\n+\n+\t\tsuper(pos, tableName, columnList, creationContext.primaryKeyList, creationContext.uniqueKeysList,\n+\t\t\t\tHiveDDLUtils.checkReservedTableProperties(propertyList), extractPartColIdentifiers(partColList), null,\n+\t\t\t\tcomment, null);\n+\n+\t\tHiveDDLUtils.convertDataTypes(columnList);\n+\t\tHiveDDLUtils.convertDataTypes(partColList);\n+\t\toriginPropList = new SqlNodeList(propertyList.getList(), propertyList.getParserPosition());\n+\t\t// mark it as a hive table\n+\t\tHiveDDLUtils.ensureNonGeneric(propertyList);\n+\t\tpropertyList.add(HiveDDLUtils.toTableOption(CatalogConfig.IS_GENERIC, \"false\", pos));\n+\t\t// set temporary\n+\t\tthis.isTemporary = isTemporary;\n+\t\tif (isTemporary) {\n+\t\t\tpropertyList.add(HiveDDLUtils.toTableOption(TABLE_IS_TEMPORARY, \"true\", pos));\n+\t\t}\n+\t\t// set external\n+\t\tthis.isExternal = isExternal;\n+\t\tif (isExternal) {\n+\t\t\tpropertyList.add(HiveDDLUtils.toTableOption(TABLE_IS_EXTERNAL, \"true\", pos));\n+\t\t}\n+\t\t// add partition cols to col list\n+\t\tif (partColList != null) {\n+\t\t\tfor (SqlNode partCol : partColList) {\n+\t\t\t\tcolumnList.add(partCol);\n+\t\t\t}\n+\t\t}\n+\t\t// set PRIMARY KEY\n+\t\tthis.creationContext = creationContext;\n+\t\tif (creationContext.primaryKeyList.size() > 0) {\n+\t\t\t// PK list is taken care of by super class, we need to set constraint name and trait here\n+\t\t\tSqlIdentifier pkName = creationContext.pkName;\n+\t\t\tif (pkName != null) {\n+\t\t\t\tpropertyList.add(HiveDDLUtils.toTableOption(\n+\t\t\t\t\t\tPK_CONSTRAINT_NAME, pkName.getSimple(), pkName.getParserPosition()));\n+\t\t\t}\n+\t\t\tpropertyList.add(HiveDDLUtils.toTableOption(\n+\t\t\t\t\tPK_CONSTRAINT_TRAIT, creationContext.pkTrait.toString(), propertyList.getParserPosition()));\n+\t\t}\n+\t\t// set NOT NULL\n+\t\tif (creationContext.notNullTraits != null) {\n+\t\t\t// NOT NULL cols are taken care of by super class, we need to set constraint traits here\n+\t\t\tString notNullTraits = creationContext.notNullTraits.stream()\n+\t\t\t\t\t.map(Object::toString).collect(Collectors.joining(HiveDDLUtils.COL_DELIMITER));\n+\t\t\tpropertyList.add(HiveDDLUtils.toTableOption(\n+\t\t\t\t\tNOT_NULL_CONSTRAINT_TRAITS, notNullTraits, propertyList.getParserPosition()));\n+\t\t}\n+\t\t// set row format\n+\t\tthis.rowFormat = rowFormat;\n+\t\tif (rowFormat != null) {\n+\t\t\tfor (SqlNode node : rowFormat.toPropList()) {\n+\t\t\t\tpropertyList.add(node);\n+\t\t\t}\n+\t\t}\n+\t\t// set stored as\n+\t\tthis.storedAs = storedAs;\n+\t\tif (storedAs != null) {\n+\t\t\tfor (SqlNode node : storedAs.toPropList()) {\n+\t\t\t\tpropertyList.add(node);\n+\t\t\t}\n+\t\t}\n+\t\t// set location\n+\t\tthis.location = location;\n+\t\tif (location != null) {\n+\t\t\tpropertyList.add(HiveDDLUtils.toTableOption(TABLE_LOCATION_URI, location, location.getParserPosition()));\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void unparse(SqlWriter writer, int leftPrec, int rightPrec) {\n+\t\twriter.keyword(\"CREATE\");\n+\t\tif (isTemporary) {\n+\t\t\twriter.keyword(\"TEMPORARY\");\n+\t\t}\n+\t\tif (isExternal) {\n+\t\t\twriter.keyword(\"EXTERNAL\");\n+\t\t}\n+\t\twriter.keyword(\"TABLE\");\n+\t\tif (ifNotExists) {\n+\t\t\twriter.keyword(\"IF NOT EXISTS\");\n+\t\t}\n+\t\tgetTableName().unparse(writer, leftPrec, rightPrec);\n+\t\t// columns\n+\t\tint numPartCol = getPartitionKeyList() == null ? 0 : getPartitionKeyList().size();\n+\t\tSqlWriter.Frame frame = writer.startList(SqlWriter.FrameTypeEnum.create(\"sds\"), \"(\", \")\");\n+\t\tSqlNodeList allCols = getColumnList();\n+\t\tunparseColumns(creationContext,\n+\t\t\t\tnew SqlNodeList(allCols.getList().subList(0, allCols.size() - numPartCol), allCols.getParserPosition()),\n+\t\t\t\twriter, leftPrec, rightPrec);\n+\t\tif (creationContext.primaryKeyList.size() > 0) {\n+\t\t\tprintIndent(writer);\n+\t\t\tif (creationContext.pkName != null) {\n+\t\t\t\twriter.keyword(\"CONSTRAINT\");\n+\t\t\t\tcreationContext.pkName.unparse(writer, leftPrec, rightPrec);\n+\t\t\t}\n+\t\t\twriter.keyword(\"PRIMARY KEY\");\n+\t\t\tSqlWriter.Frame pkFrame = writer.startList(\"(\", \")\");\n+\t\t\tcreationContext.primaryKeyList.unparse(writer, leftPrec, rightPrec);\n+\t\t\twriter.endList(pkFrame);\n+\t\t\tHiveDDLUtils.unparseConstraintTrait(creationContext.pkTrait, writer);\n+\t\t}\n+\t\twriter.newlineAndIndent();\n+\t\twriter.endList(frame);\n+\t\t// table comment\n+\t\tgetComment().ifPresent(c -> {\n+\t\t\twriter.keyword(\"COMMENT\");\n+\t\t\tc.unparse(writer, leftPrec, rightPrec);\n+\t\t});\n+\t\t// partitions\n+\t\tif (numPartCol > 0) {\n+\t\t\twriter.newlineAndIndent();\n+\t\t\twriter.keyword(\"PARTITIONED BY\");\n+\t\t\tSqlWriter.Frame partitionedByFrame = writer.startList(\"(\", \")\");\n+\t\t\tunparseColumns(creationContext,\n+\t\t\t\t\tnew SqlNodeList(allCols.getList().subList(allCols.size() - numPartCol, allCols.size()), allCols.getParserPosition()),\n+\t\t\t\t\twriter, leftPrec, rightPrec);\n+\t\t\twriter.newlineAndIndent();\n+\t\t\twriter.endList(partitionedByFrame);\n+\t\t}\n+\t\t// row format\n+\t\tunparseRowFormat(writer, leftPrec, rightPrec);\n+\t\t// stored as\n+\t\tunparseStoredAs(writer, leftPrec, rightPrec);\n+\t\t// location\n+\t\tif (location != null) {\n+\t\t\twriter.newlineAndIndent();\n+\t\t\twriter.keyword(\"LOCATION\");\n+\t\t\tlocation.unparse(writer, leftPrec, rightPrec);\n+\t\t}\n+\t\t// properties\n+\t\tif (originPropList.size() > 0) {\n+\t\t\twriter.newlineAndIndent();\n+\t\t\twriter.keyword(\"TBLPROPERTIES\");\n+\t\t\tunparsePropList(originPropList, writer, leftPrec, rightPrec);\n+\t\t}\n+\t}\n+\n+\tprivate void unparseStoredAs(SqlWriter writer, int leftPrec, int rightPrec) {\n+\t\tif (storedAs == null) {\n+\t\t\treturn;\n+\t\t}\n+\t\twriter.newlineAndIndent();\n+\t\twriter.keyword(\"STORED AS\");\n+\t\tif (storedAs.fileFormat != null) {\n+\t\t\tstoredAs.fileFormat.unparse(writer, leftPrec, rightPrec);\n+\t\t} else {\n+\t\t\twriter.keyword(\"INPUTFORMAT\");\n+\t\t\tstoredAs.intputFormat.unparse(writer, leftPrec, rightPrec);\n+\t\t\twriter.keyword(\"OUTPUTFORMAT\");\n+\t\t\tstoredAs.outputFormat.unparse(writer, leftPrec, rightPrec);\n+\t\t}\n+\t}\n+\n+\tprivate void unparseRowFormat(SqlWriter writer, int leftPrec, int rightPrec) {\n+\t\tif (rowFormat == null) {\n+\t\t\treturn;\n+\t\t}\n+\t\twriter.newlineAndIndent();\n+\t\twriter.keyword(\"ROW FORMAT\");\n+\t\tif (rowFormat.serdeClass != null) {\n+\t\t\twriter.keyword(\"SERDE\");\n+\t\t\trowFormat.serdeClass.unparse(writer, leftPrec, rightPrec);\n+\t\t\tif (rowFormat.serdeProps != null) {\n+\t\t\t\twriter.keyword(\"WITH SERDEPROPERTIES\");\n+\t\t\t\tunparsePropList(rowFormat.serdeProps, writer, leftPrec, rightPrec);\n+\t\t\t}\n+\t\t} else {\n+\t\t\twriter.keyword(\"DELIMITED\");\n+\t\t\tSqlCharStringLiteral fieldDelim = rowFormat.delimitPropToValue.get(HiveTableRowFormat.FIELD_DELIM);\n+\t\t\tSqlCharStringLiteral escape = rowFormat.delimitPropToValue.get(HiveTableRowFormat.ESCAPE_CHAR);\n+\t\t\tif (fieldDelim != null) {\n+\t\t\t\twriter.keyword(\"FIELDS TERMINATED BY\");\n+\t\t\t\tfieldDelim.unparse(writer, leftPrec, rightPrec);\n+\t\t\t\tif (escape != null) {\n+\t\t\t\t\twriter.keyword(\"ESCAPED BY\");\n+\t\t\t\t\tescape.unparse(writer, leftPrec, rightPrec);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tSqlCharStringLiteral collectionDelim = rowFormat.delimitPropToValue.get(HiveTableRowFormat.COLLECTION_DELIM);\n+\t\t\tif (collectionDelim != null) {\n+\t\t\t\twriter.keyword(\"COLLECTION ITEMS TERMINATED BY\");\n+\t\t\t\tcollectionDelim.unparse(writer, leftPrec, rightPrec);\n+\t\t\t}\n+\t\t\tSqlCharStringLiteral mapKeyDelim = rowFormat.delimitPropToValue.get(HiveTableRowFormat.MAPKEY_DELIM);\n+\t\t\tif (mapKeyDelim != null) {\n+\t\t\t\twriter.keyword(\"MAP KEYS TERMINATED BY\");\n+\t\t\t\tmapKeyDelim.unparse(writer, leftPrec, rightPrec);\n+\t\t\t}\n+\t\t\tSqlCharStringLiteral lineDelim = rowFormat.delimitPropToValue.get(HiveTableRowFormat.LINE_DELIM);\n+\t\t\tif (lineDelim != null) {\n+\t\t\t\twriter.keyword(\"LINES TERMINATED BY\");\n+\t\t\t\tlineDelim.unparse(writer, leftPrec, rightPrec);\n+\t\t\t}\n+\t\t\tSqlCharStringLiteral nullAs = rowFormat.delimitPropToValue.get(HiveTableRowFormat.SERIALIZATION_NULL_FORMAT);\n+\t\t\tif (nullAs != null) {\n+\t\t\t\twriter.keyword(\"NULL DEFINED AS\");\n+\t\t\t\tnullAs.unparse(writer, leftPrec, rightPrec);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void unparsePropList(SqlNodeList propList, SqlWriter writer, int leftPrec, int rightPrec) {\n+\t\tSqlWriter.Frame withFrame = writer.startList(\"(\", \")\");\n+\t\tfor (SqlNode property : propList) {\n+\t\t\tprintIndent(writer);\n+\t\t\tproperty.unparse(writer, leftPrec, rightPrec);\n+\t\t}\n+\t\twriter.newlineAndIndent();\n+\t\twriter.endList(withFrame);\n+\t}\n+\n+\tprivate void unparseColumns(HiveTableCreationContext context, SqlNodeList columns,\n+\t\t\tSqlWriter writer, int leftPrec, int rightPrec) {\n+\t\tList<Byte> notNullTraits = context.notNullTraits;\n+\t\tint traitIndex = 0;\n+\t\tfor (SqlNode node : columns) {\n+\t\t\tprintIndent(writer);\n+\t\t\tSqlTableColumn column = (SqlTableColumn) node;\n+\t\t\tcolumn.getName().unparse(writer, leftPrec, rightPrec);\n+\t\t\twriter.print(\" \");\n+\t\t\tcolumn.getType().unparse(writer, leftPrec, rightPrec);\n+\t\t\tif (column.getType().getNullable() != null && !column.getType().getNullable()) {\n+\t\t\t\twriter.keyword(\"NOT NULL\");\n+\t\t\t\tHiveDDLUtils.unparseConstraintTrait(notNullTraits.get(traitIndex++), writer);\n+\t\t\t}\n+\t\t\tcolumn.getComment().ifPresent(c -> {\n+\t\t\t\twriter.keyword(\"COMMENT\");\n+\t\t\t\tc.unparse(writer, leftPrec, rightPrec);\n+\t\t\t});\n+\t\t}\n+\t}\n+\n+\t// Extract the identifiers from partition col list -- that's what SqlCreateTable expects for partition keys\n+\tprivate static SqlNodeList extractPartColIdentifiers(SqlNodeList partCols) {\n+\t\tif (partCols == null) {\n+\t\t\treturn null;\n+\t\t}\n+\t\tSqlNodeList res = new SqlNodeList(partCols.getParserPosition());\n+\t\tfor (SqlNode node : partCols) {\n+\t\t\tSqlTableColumn partCol = (SqlTableColumn) node;\n+\t\t\tres.add(partCol.getName());\n+\t\t}\n+\t\treturn res;\n+\t}\n+\n+\t/**\n+\t * Creation context for a Hive table.\n+\t */\n+\tpublic static class HiveTableCreationContext extends TableCreationContext {\n+\t\tpublic SqlIdentifier pkName = null;\n+\t\tpublic Byte pkTrait = null;\n+\t\tpublic SqlIdentifier ukName = null;\n+\t\tpublic List<Byte> notNullTraits = null;\n+\t}\n+\n+\t/**\n+\t * To represent STORED AS in CREATE TABLE DDL.\n+\t */\n+\tpublic static class HiveTableStoredAs {\n+\n+\t\tpublic static final String STORED_AS_FILE_FORMAT = \"hive.stored.as.file.format\";\n+\t\tpublic static final String STORED_AS_INPUT_FORMAT = \"hive.stored.as.input.format\";\n+\t\tpublic static final String STORED_AS_OUTPUT_FORMAT = \"hive.stored.as.output.format\";\n+\n+\t\tprivate final SqlParserPos pos;\n+\t\tprivate final SqlIdentifier fileFormat;\n+\t\tprivate final SqlCharStringLiteral intputFormat;\n+\t\tprivate final SqlCharStringLiteral outputFormat;\n+\n+\t\tprivate HiveTableStoredAs(SqlParserPos pos, SqlIdentifier fileFormat, SqlCharStringLiteral intputFormat,\n+\t\t\t\tSqlCharStringLiteral outputFormat) throws ParseException {\n+\t\t\tthis.pos = pos;\n+\t\t\tthis.fileFormat = fileFormat;\n+\t\t\tthis.intputFormat = intputFormat;\n+\t\t\tthis.outputFormat = outputFormat;\n+\t\t\tvalidate();\n+\t\t}\n+\n+\t\tprivate void validate() throws ParseException {\n+\t\t\tif (fileFormat != null) {\n+\t\t\t\tif (intputFormat != null || outputFormat != null) {\n+\t\t\t\t\tthrow new ParseException(\"Both file format and input/output format are specified\");\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tif (intputFormat == null || outputFormat == null) {\n+\t\t\t\t\tthrow new ParseException(\"Neither file format nor input/output format is specified\");\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tpublic SqlNodeList toPropList() {\n+\t\t\tSqlNodeList res = new SqlNodeList(pos);\n+\t\t\tif (fileFormat != null) {\n+\t\t\t\tres.add(HiveDDLUtils.toTableOption(STORED_AS_FILE_FORMAT, fileFormat.getSimple(), fileFormat.getParserPosition()));\n+\t\t\t} else {\n+\t\t\t\tres.add(HiveDDLUtils.toTableOption(STORED_AS_INPUT_FORMAT, intputFormat, intputFormat.getParserPosition()));\n+\t\t\t\tres.add(HiveDDLUtils.toTableOption(STORED_AS_OUTPUT_FORMAT, outputFormat, outputFormat.getParserPosition()));\n+\t\t\t}\n+\t\t\treturn res;\n+\t\t}\n+\n+\t\tpublic static HiveTableStoredAs ofFileFormat(SqlParserPos pos, SqlIdentifier fileFormat) throws ParseException {\n+\t\t\treturn new HiveTableStoredAs(pos, fileFormat, null, null);\n+\t\t}\n+\n+\t\tpublic static HiveTableStoredAs ofInputOutputFormat(SqlParserPos pos, SqlCharStringLiteral intputFormat,\n+\t\t\t\tSqlCharStringLiteral outputFormat) throws ParseException {\n+\t\t\treturn new HiveTableStoredAs(pos, null, intputFormat, outputFormat);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * To represent ROW FORMAT in CREATE TABLE DDL.\n+\t */\n+\tpublic static class HiveTableRowFormat {\n+\n+\t\tpublic static final String SERDE_LIB_CLASS_NAME = \"hive.serde.lib.class.name\";\n+\t\tpublic static final String SERDE_INFO_PROP_PREFIX = \"hive.serde.info.prop.\";\n+\t\tprivate static final String FIELD_DELIM = SERDE_INFO_PROP_PREFIX + \"field.delim\";", "originalCommit": "062114a456dffdef8b5226105037c398c012a56d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzMwMDkyNA==", "url": "https://github.com/apache/flink/pull/11935#discussion_r417300924", "bodyText": "These properties are actually SerDe properties defined by Hive. So we can't choose our own name.", "author": "lirui-apache", "createdAt": "2020-04-29T13:09:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzE4NDEyMw=="}], "type": "inlineReview"}, {"oid": "515dd8d0816f25cf0eeb3c23e92cf6c72190e2d1", "url": "https://github.com/apache/flink/commit/515dd8d0816f25cf0eeb3c23e92cf6c72190e2d1", "message": "rebase", "committedDate": "2020-05-09T07:13:37Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU3NDY5OQ==", "url": "https://github.com/apache/flink/pull/11935#discussion_r422574699", "bodyText": "Compare directly with SqlTypeName is enough, it is a Enum.", "author": "danny0405", "createdAt": "2020-05-10T02:41:18Z", "path": "flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/ddl/HiveDDLUtils.java", "diffHunk": "@@ -91,4 +154,149 @@ public static SqlTableOption toTableOption(String key, SqlNode value, SqlParserP\n \tpublic static SqlTableOption toTableOption(String key, String value, SqlParserPos pos) {\n \t\treturn new SqlTableOption(SqlLiteral.createCharString(key, pos), SqlLiteral.createCharString(value, pos), pos);\n \t}\n+\n+\tpublic static void convertDataTypes(SqlNodeList columns) {\n+\t\tif (columns != null) {\n+\t\t\tfor (SqlNode node : columns) {\n+\t\t\t\tconvertDataTypes((SqlTableColumn) node);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// data types may need to be converted to comply with HiveQL, e.g. TIMESTAMP and BINARY\n+\tpublic static void convertDataTypes(SqlTableColumn column) {\n+\t\tcolumn.setType(convertDataTypes(column.getType()));\n+\t}\n+\n+\tprivate static SqlDataTypeSpec convertDataTypes(SqlDataTypeSpec typeSpec) {\n+\t\tSqlTypeNameSpec nameSpec = typeSpec.getTypeNameSpec();\n+\t\tSqlTypeNameSpec convertedNameSpec = convertDataTypes(nameSpec);\n+\t\tif (nameSpec != convertedNameSpec) {\n+\t\t\ttypeSpec = new SqlDataTypeSpec(convertedNameSpec, typeSpec.getTimeZone(), typeSpec.getNullable(),\n+\t\t\t\t\ttypeSpec.getParserPosition());\n+\t\t}\n+\t\treturn typeSpec;\n+\t}\n+\n+\tprivate static SqlTypeNameSpec convertDataTypes(SqlTypeNameSpec nameSpec) {\n+\t\tif (nameSpec instanceof SqlBasicTypeNameSpec) {\n+\t\t\tSqlBasicTypeNameSpec basicNameSpec = (SqlBasicTypeNameSpec) nameSpec;\n+\t\t\tif (basicNameSpec.getTypeName().getSimple().equalsIgnoreCase(SqlTypeName.TIMESTAMP.name())) {\n+\t\t\t\tif (basicNameSpec.getPrecision() < 0) {", "originalCommit": "71b8c98cbccd661f2b4f90e18d450199e860caa0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwMDA3OA==", "url": "https://github.com/apache/flink/pull/11935#discussion_r422600078", "bodyText": "But I can't get SqlTypeName from a SqlBasicTypeNameSpec", "author": "lirui-apache", "createdAt": "2020-05-10T07:12:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU3NDY5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU3NTI5Nw==", "url": "https://github.com/apache/flink/pull/11935#discussion_r422575297", "bodyText": "We can represent these constraint as a Literal and unparse them directly, just like SqlConstraintEnforcement. There is no need to do these complex transition.", "author": "danny0405", "createdAt": "2020-05-10T02:49:26Z", "path": "flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/ddl/HiveDDLUtils.java", "diffHunk": "@@ -91,4 +154,149 @@ public static SqlTableOption toTableOption(String key, SqlNode value, SqlParserP\n \tpublic static SqlTableOption toTableOption(String key, String value, SqlParserPos pos) {\n \t\treturn new SqlTableOption(SqlLiteral.createCharString(key, pos), SqlLiteral.createCharString(value, pos), pos);\n \t}\n+\n+\tpublic static void convertDataTypes(SqlNodeList columns) {\n+\t\tif (columns != null) {\n+\t\t\tfor (SqlNode node : columns) {\n+\t\t\t\tconvertDataTypes((SqlTableColumn) node);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// data types may need to be converted to comply with HiveQL, e.g. TIMESTAMP and BINARY\n+\tpublic static void convertDataTypes(SqlTableColumn column) {\n+\t\tcolumn.setType(convertDataTypes(column.getType()));\n+\t}\n+\n+\tprivate static SqlDataTypeSpec convertDataTypes(SqlDataTypeSpec typeSpec) {\n+\t\tSqlTypeNameSpec nameSpec = typeSpec.getTypeNameSpec();\n+\t\tSqlTypeNameSpec convertedNameSpec = convertDataTypes(nameSpec);\n+\t\tif (nameSpec != convertedNameSpec) {\n+\t\t\ttypeSpec = new SqlDataTypeSpec(convertedNameSpec, typeSpec.getTimeZone(), typeSpec.getNullable(),\n+\t\t\t\t\ttypeSpec.getParserPosition());\n+\t\t}\n+\t\treturn typeSpec;\n+\t}\n+\n+\tprivate static SqlTypeNameSpec convertDataTypes(SqlTypeNameSpec nameSpec) {\n+\t\tif (nameSpec instanceof SqlBasicTypeNameSpec) {\n+\t\t\tSqlBasicTypeNameSpec basicNameSpec = (SqlBasicTypeNameSpec) nameSpec;\n+\t\t\tif (basicNameSpec.getTypeName().getSimple().equalsIgnoreCase(SqlTypeName.TIMESTAMP.name())) {\n+\t\t\t\tif (basicNameSpec.getPrecision() < 0) {\n+\t\t\t\t\tnameSpec = new SqlBasicTypeNameSpec(SqlTypeName.TIMESTAMP, 9, basicNameSpec.getScale(),\n+\t\t\t\t\t\t\tbasicNameSpec.getCharSetName(), basicNameSpec.getParserPos());\n+\t\t\t\t}\n+\t\t\t} else if (basicNameSpec.getTypeName().getSimple().equalsIgnoreCase(SqlTypeName.BINARY.name())) {\n+\t\t\t\tif (basicNameSpec.getPrecision() < 0) {\n+\t\t\t\t\tnameSpec = new SqlBasicTypeNameSpec(SqlTypeName.VARBINARY, Integer.MAX_VALUE, basicNameSpec.getScale(),\n+\t\t\t\t\t\t\tbasicNameSpec.getCharSetName(), basicNameSpec.getParserPos());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else if (nameSpec instanceof ExtendedSqlCollectionTypeNameSpec) {\n+\t\t\tExtendedSqlCollectionTypeNameSpec collectionNameSpec = (ExtendedSqlCollectionTypeNameSpec) nameSpec;\n+\t\t\tSqlTypeNameSpec elementNameSpec = collectionNameSpec.getElementTypeName();\n+\t\t\tSqlTypeNameSpec convertedElementNameSpec = convertDataTypes(elementNameSpec);\n+\t\t\tif (convertedElementNameSpec != elementNameSpec) {\n+\t\t\t\tnameSpec = new ExtendedSqlCollectionTypeNameSpec(convertedElementNameSpec,\n+\t\t\t\t\t\tcollectionNameSpec.elementNullable(), collectionNameSpec.getCollectionTypeName(),\n+\t\t\t\t\t\tcollectionNameSpec.unparseAsStandard(), collectionNameSpec.getParserPos());\n+\t\t\t}\n+\t\t} else if (nameSpec instanceof SqlMapTypeNameSpec) {\n+\t\t\tSqlMapTypeNameSpec mapNameSpec = (SqlMapTypeNameSpec) nameSpec;\n+\t\t\tSqlDataTypeSpec keyTypeSpec = mapNameSpec.getKeyType();\n+\t\t\tSqlDataTypeSpec valTypeSpec = mapNameSpec.getValType();\n+\t\t\tSqlDataTypeSpec convertedKeyTypeSpec = convertDataTypes(keyTypeSpec);\n+\t\t\tSqlDataTypeSpec convertedValTypeSpec = convertDataTypes(valTypeSpec);\n+\t\t\tif (keyTypeSpec != convertedKeyTypeSpec || valTypeSpec != convertedValTypeSpec) {\n+\t\t\t\tnameSpec = new SqlMapTypeNameSpec(convertedKeyTypeSpec, convertedValTypeSpec, nameSpec.getParserPos());\n+\t\t\t}\n+\t\t} else if (nameSpec instanceof ExtendedSqlRowTypeNameSpec) {\n+\t\t\tExtendedSqlRowTypeNameSpec rowNameSpec = (ExtendedSqlRowTypeNameSpec) nameSpec;\n+\t\t\tList<SqlDataTypeSpec> fieldTypeSpecs = rowNameSpec.getFieldTypes();\n+\t\t\tList<SqlDataTypeSpec> convertedFieldTypeSpecs = new ArrayList<>(fieldTypeSpecs.size());\n+\t\t\tboolean updated = false;\n+\t\t\tfor (SqlDataTypeSpec fieldTypeSpec : fieldTypeSpecs) {\n+\t\t\t\tSqlDataTypeSpec convertedFieldTypeSpec = convertDataTypes(fieldTypeSpec);\n+\t\t\t\tif (fieldTypeSpec != convertedFieldTypeSpec) {\n+\t\t\t\t\tupdated = true;\n+\t\t\t\t}\n+\t\t\t\tconvertedFieldTypeSpecs.add(convertedFieldTypeSpec);\n+\t\t\t}\n+\t\t\tif (updated) {\n+\t\t\t\tnameSpec = new ExtendedSqlRowTypeNameSpec(nameSpec.getParserPos(), rowNameSpec.getFieldNames(),\n+\t\t\t\t\t\tconvertedFieldTypeSpecs, rowNameSpec.getComments(), rowNameSpec.unparseAsStandard());\n+\t\t\t}\n+\t\t}\n+\t\treturn nameSpec;\n+\t}\n+\n+\t// a constraint is by default ENABLE NOVALIDATE RELY\n+\tpublic static byte defaultTrait() {\n+\t\tbyte res = enableConstraint((byte) 0);\n+\t\tres = relyConstraint(res);\n+\t\treturn res;\n+\t}\n+\n+\t// returns a constraint trait that requires ENABLE\n+\tpublic static byte enableConstraint(byte trait) {\n+\t\treturn (byte) (trait | HIVE_CONSTRAINT_ENABLE);\n+\t}\n+\n+\t// returns a constraint trait that doesn't require ENABLE\n+\tpublic static byte disableConstraint(byte trait) {\n+\t\treturn (byte) (trait & (~HIVE_CONSTRAINT_ENABLE));\n+\t}\n+\n+\t// returns a constraint trait that requires VALIDATE\n+\tpublic static byte validateConstraint(byte trait) {\n+\t\treturn (byte) (trait | HIVE_CONSTRAINT_VALIDATE);", "originalCommit": "71b8c98cbccd661f2b4f90e18d450199e860caa0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU3NTQ5Nw==", "url": "https://github.com/apache/flink/pull/11935#discussion_r422575497", "bodyText": "Then the unparse of SqlCreateTable can also be simplified.", "author": "danny0405", "createdAt": "2020-05-10T02:51:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU3NTI5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYxNjgzMQ==", "url": "https://github.com/apache/flink/pull/11935#discussion_r422616831", "bodyText": "I added enums for these keywords and use them during unparse. I still encode them as bytes in the table properties to be consistent with how we deal with constraints in hive catalog currently.", "author": "lirui-apache", "createdAt": "2020-05-10T09:26:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU3NTI5Nw=="}], "type": "inlineReview"}, {"oid": "12972e78d80e1351d8644ae3119b7bb14abbd7d7", "url": "https://github.com/apache/flink/commit/12972e78d80e1351d8644ae3119b7bb14abbd7d7", "message": "store NN col names and add test", "committedDate": "2020-05-11T07:40:39Z", "type": "forcePushed"}, {"oid": "fd44e13b8aa343733328d4a356441604c5309f10", "url": "https://github.com/apache/flink/commit/fd44e13b8aa343733328d4a356441604c5309f10", "message": "disallow varchar w/o precision", "committedDate": "2020-05-11T09:56:09Z", "type": "forcePushed"}, {"oid": "61c782bea7d0f96ca0d71ef5384ae006c1795aaa", "url": "https://github.com/apache/flink/commit/61c782bea7d0f96ca0d71ef5384ae006c1795aaa", "message": "disallow varchar w/o precision", "committedDate": "2020-05-12T02:28:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQzNjE0Nw==", "url": "https://github.com/apache/flink/pull/11935#discussion_r423436147", "bodyText": "describe formatted tbl => DESCRIBE FORMATTED TBL`` ?", "author": "danny0405", "createdAt": "2020-05-12T03:10:31Z", "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -98,4 +98,105 @@ public void testDescribeDatabase() {\n \t\tsql(\"describe schema db1\").ok(\"DESCRIBE DATABASE `DB1`\");\n \t\tsql(\"describe database extended db1\").ok(\"DESCRIBE DATABASE EXTENDED `DB1`\");\n \t}\n+\n+\t@Test\n+\tpublic void testShowTables() {\n+\t\t// TODO: support SHOW TABLES IN 'db_name' 'regex_pattern'\n+\t\tsql(\"show tables\").ok(\"SHOW TABLES\");\n+\t}\n+\n+\t@Test\n+\tpublic void testDescribeTable() {\n+\t\t// TODO: support describe partition and columns\n+\t\tsql(\"describe tbl\").ok(\"DESCRIBE `TBL`\");\n+\t\tsql(\"describe extended tbl\").ok(\"DESCRIBE EXTENDED `TBL`\");\n+\t\tsql(\"describe formatted tbl\").ok(\"DESCRIBE EXTENDED `TBL`\");\n+\t}", "originalCommit": "61c782bea7d0f96ca0d71ef5384ae006c1795aaa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQzNjY5Mw==", "url": "https://github.com/apache/flink/pull/11935#discussion_r423436693", "bodyText": "Make a new line after each TERMINATED BY  _", "author": "danny0405", "createdAt": "2020-05-12T03:12:44Z", "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -98,4 +98,105 @@ public void testDescribeDatabase() {\n \t\tsql(\"describe schema db1\").ok(\"DESCRIBE DATABASE `DB1`\");\n \t\tsql(\"describe database extended db1\").ok(\"DESCRIBE DATABASE EXTENDED `DB1`\");\n \t}\n+\n+\t@Test\n+\tpublic void testShowTables() {\n+\t\t// TODO: support SHOW TABLES IN 'db_name' 'regex_pattern'\n+\t\tsql(\"show tables\").ok(\"SHOW TABLES\");\n+\t}\n+\n+\t@Test\n+\tpublic void testDescribeTable() {\n+\t\t// TODO: support describe partition and columns\n+\t\tsql(\"describe tbl\").ok(\"DESCRIBE `TBL`\");\n+\t\tsql(\"describe extended tbl\").ok(\"DESCRIBE EXTENDED `TBL`\");\n+\t\tsql(\"describe formatted tbl\").ok(\"DESCRIBE EXTENDED `TBL`\");\n+\t}\n+\n+\t@Test\n+\tpublic void testCreateTable() {\n+\t\tsql(\"create table tbl (x int) row format delimited fields terminated by ',' escaped by '\\\\' \" +\n+\t\t\t\t\"collection items terminated by ',' map keys terminated by ':' lines terminated by '\\n' \" +\n+\t\t\t\t\"null defined as 'null' location '/path/to/table'\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  INTEGER\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ESCAPED BY '\\\\' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' LINES TERMINATED BY '\\n\" +\n+\t\t\t\t\t\t\"' NULL DEFINED AS 'null'\\n\" +", "originalCommit": "61c782bea7d0f96ca0d71ef5384ae006c1795aaa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQzNzIyMA==", "url": "https://github.com/apache/flink/pull/11935#discussion_r423437220", "bodyText": "struct should still unparse as struct.", "author": "danny0405", "createdAt": "2020-05-12T03:14:54Z", "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -98,4 +98,105 @@ public void testDescribeDatabase() {\n \t\tsql(\"describe schema db1\").ok(\"DESCRIBE DATABASE `DB1`\");\n \t\tsql(\"describe database extended db1\").ok(\"DESCRIBE DATABASE EXTENDED `DB1`\");\n \t}\n+\n+\t@Test\n+\tpublic void testShowTables() {\n+\t\t// TODO: support SHOW TABLES IN 'db_name' 'regex_pattern'\n+\t\tsql(\"show tables\").ok(\"SHOW TABLES\");\n+\t}\n+\n+\t@Test\n+\tpublic void testDescribeTable() {\n+\t\t// TODO: support describe partition and columns\n+\t\tsql(\"describe tbl\").ok(\"DESCRIBE `TBL`\");\n+\t\tsql(\"describe extended tbl\").ok(\"DESCRIBE EXTENDED `TBL`\");\n+\t\tsql(\"describe formatted tbl\").ok(\"DESCRIBE EXTENDED `TBL`\");\n+\t}\n+\n+\t@Test\n+\tpublic void testCreateTable() {\n+\t\tsql(\"create table tbl (x int) row format delimited fields terminated by ',' escaped by '\\\\' \" +\n+\t\t\t\t\"collection items terminated by ',' map keys terminated by ':' lines terminated by '\\n' \" +\n+\t\t\t\t\"null defined as 'null' location '/path/to/table'\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  INTEGER\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ESCAPED BY '\\\\' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' LINES TERMINATED BY '\\n\" +\n+\t\t\t\t\t\t\"' NULL DEFINED AS 'null'\\n\" +\n+\t\t\t\t\t\t\"LOCATION '/path/to/table'\");\n+\t\tsql(\"create table tbl (x double) stored as orc tblproperties ('k1'='v1')\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  DOUBLE\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"STORED AS `ORC`\\n\" +\n+\t\t\t\t\t\t\"TBLPROPERTIES (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create table tbl (x decimal(5,2)) row format serde 'serde.class.name' with serdeproperties ('serde.k1'='v1')\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  DECIMAL(5, 2)\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"ROW FORMAT SERDE 'serde.class.name' WITH SERDEPROPERTIES (\\n\" +\n+\t\t\t\t\t\t\"  'serde.k1' = 'v1'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create table tbl (x date) row format delimited fields terminated by '\\u0001' \" +\n+\t\t\t\t\"stored as inputformat 'input.format.class' outputformat 'output.format.class'\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  DATE\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"ROW FORMAT DELIMITED FIELDS TERMINATED BY u&'\\\\0001'\\n\" +\n+\t\t\t\t\t\t\"STORED AS INPUTFORMAT 'input.format.class' OUTPUTFORMAT 'output.format.class'\");\n+\t\tsql(\"create table tbl (x struct<f1:timestamp,f2:int>) partitioned by (p1 string,p2 bigint) stored as rcfile\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  ROW< `F1` TIMESTAMP(9), `F2` INTEGER >\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"PARTITIONED BY (\\n\" +", "originalCommit": "61c782bea7d0f96ca0d71ef5384ae006c1795aaa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQzNzU1OA==", "url": "https://github.com/apache/flink/pull/11935#discussion_r423437558", "bodyText": "timestamp should still unparse as timestamp.", "author": "danny0405", "createdAt": "2020-05-12T03:16:10Z", "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -98,4 +98,105 @@ public void testDescribeDatabase() {\n \t\tsql(\"describe schema db1\").ok(\"DESCRIBE DATABASE `DB1`\");\n \t\tsql(\"describe database extended db1\").ok(\"DESCRIBE DATABASE EXTENDED `DB1`\");\n \t}\n+\n+\t@Test\n+\tpublic void testShowTables() {\n+\t\t// TODO: support SHOW TABLES IN 'db_name' 'regex_pattern'\n+\t\tsql(\"show tables\").ok(\"SHOW TABLES\");\n+\t}\n+\n+\t@Test\n+\tpublic void testDescribeTable() {\n+\t\t// TODO: support describe partition and columns\n+\t\tsql(\"describe tbl\").ok(\"DESCRIBE `TBL`\");\n+\t\tsql(\"describe extended tbl\").ok(\"DESCRIBE EXTENDED `TBL`\");\n+\t\tsql(\"describe formatted tbl\").ok(\"DESCRIBE EXTENDED `TBL`\");\n+\t}\n+\n+\t@Test\n+\tpublic void testCreateTable() {\n+\t\tsql(\"create table tbl (x int) row format delimited fields terminated by ',' escaped by '\\\\' \" +\n+\t\t\t\t\"collection items terminated by ',' map keys terminated by ':' lines terminated by '\\n' \" +\n+\t\t\t\t\"null defined as 'null' location '/path/to/table'\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  INTEGER\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ESCAPED BY '\\\\' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' LINES TERMINATED BY '\\n\" +\n+\t\t\t\t\t\t\"' NULL DEFINED AS 'null'\\n\" +\n+\t\t\t\t\t\t\"LOCATION '/path/to/table'\");\n+\t\tsql(\"create table tbl (x double) stored as orc tblproperties ('k1'='v1')\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  DOUBLE\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"STORED AS `ORC`\\n\" +\n+\t\t\t\t\t\t\"TBLPROPERTIES (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create table tbl (x decimal(5,2)) row format serde 'serde.class.name' with serdeproperties ('serde.k1'='v1')\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  DECIMAL(5, 2)\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"ROW FORMAT SERDE 'serde.class.name' WITH SERDEPROPERTIES (\\n\" +\n+\t\t\t\t\t\t\"  'serde.k1' = 'v1'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create table tbl (x date) row format delimited fields terminated by '\\u0001' \" +\n+\t\t\t\t\"stored as inputformat 'input.format.class' outputformat 'output.format.class'\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  DATE\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"ROW FORMAT DELIMITED FIELDS TERMINATED BY u&'\\\\0001'\\n\" +\n+\t\t\t\t\t\t\"STORED AS INPUTFORMAT 'input.format.class' OUTPUTFORMAT 'output.format.class'\");\n+\t\tsql(\"create table tbl (x struct<f1:timestamp,f2:int>) partitioned by (p1 string,p2 bigint) stored as rcfile\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  ROW< `F1` TIMESTAMP(9), `F2` INTEGER >\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"PARTITIONED BY (\\n\" +\n+\t\t\t\t\t\t\"  `P1`  STRING,\\n\" +\n+\t\t\t\t\t\t\"  `P2`  BIGINT\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"STORED AS `RCFILE`\");\n+\t\tsql(\"create external table tbl (x map<timestamp,array<timestamp>>) location '/table/path'\")\n+\t\t\t\t.ok(\"CREATE EXTERNAL TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  MAP< TIMESTAMP(9), ARRAY< TIMESTAMP(9) > >\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"LOCATION '/table/path'\");\n+\t\tsql(\"create temporary table tbl (x varchar(50)) partitioned by (p timestamp)\")\n+\t\t\t\t.ok(\"CREATE TEMPORARY TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  VARCHAR(50)\\n\" +\n+\t\t\t\t\t\t\")\\n\" +", "originalCommit": "61c782bea7d0f96ca0d71ef5384ae006c1795aaa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQzODIzMg==", "url": "https://github.com/apache/flink/pull/11935#discussion_r423438232", "bodyText": "All the default options should not be print out.", "author": "danny0405", "createdAt": "2020-05-12T03:18:48Z", "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -98,4 +98,105 @@ public void testDescribeDatabase() {\n \t\tsql(\"describe schema db1\").ok(\"DESCRIBE DATABASE `DB1`\");\n \t\tsql(\"describe database extended db1\").ok(\"DESCRIBE DATABASE EXTENDED `DB1`\");\n \t}\n+\n+\t@Test\n+\tpublic void testShowTables() {\n+\t\t// TODO: support SHOW TABLES IN 'db_name' 'regex_pattern'\n+\t\tsql(\"show tables\").ok(\"SHOW TABLES\");\n+\t}\n+\n+\t@Test\n+\tpublic void testDescribeTable() {\n+\t\t// TODO: support describe partition and columns\n+\t\tsql(\"describe tbl\").ok(\"DESCRIBE `TBL`\");\n+\t\tsql(\"describe extended tbl\").ok(\"DESCRIBE EXTENDED `TBL`\");\n+\t\tsql(\"describe formatted tbl\").ok(\"DESCRIBE EXTENDED `TBL`\");\n+\t}\n+\n+\t@Test\n+\tpublic void testCreateTable() {\n+\t\tsql(\"create table tbl (x int) row format delimited fields terminated by ',' escaped by '\\\\' \" +\n+\t\t\t\t\"collection items terminated by ',' map keys terminated by ':' lines terminated by '\\n' \" +\n+\t\t\t\t\"null defined as 'null' location '/path/to/table'\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  INTEGER\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ESCAPED BY '\\\\' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' LINES TERMINATED BY '\\n\" +\n+\t\t\t\t\t\t\"' NULL DEFINED AS 'null'\\n\" +\n+\t\t\t\t\t\t\"LOCATION '/path/to/table'\");\n+\t\tsql(\"create table tbl (x double) stored as orc tblproperties ('k1'='v1')\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  DOUBLE\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"STORED AS `ORC`\\n\" +\n+\t\t\t\t\t\t\"TBLPROPERTIES (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create table tbl (x decimal(5,2)) row format serde 'serde.class.name' with serdeproperties ('serde.k1'='v1')\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  DECIMAL(5, 2)\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"ROW FORMAT SERDE 'serde.class.name' WITH SERDEPROPERTIES (\\n\" +\n+\t\t\t\t\t\t\"  'serde.k1' = 'v1'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create table tbl (x date) row format delimited fields terminated by '\\u0001' \" +\n+\t\t\t\t\"stored as inputformat 'input.format.class' outputformat 'output.format.class'\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  DATE\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"ROW FORMAT DELIMITED FIELDS TERMINATED BY u&'\\\\0001'\\n\" +\n+\t\t\t\t\t\t\"STORED AS INPUTFORMAT 'input.format.class' OUTPUTFORMAT 'output.format.class'\");\n+\t\tsql(\"create table tbl (x struct<f1:timestamp,f2:int>) partitioned by (p1 string,p2 bigint) stored as rcfile\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  ROW< `F1` TIMESTAMP(9), `F2` INTEGER >\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"PARTITIONED BY (\\n\" +\n+\t\t\t\t\t\t\"  `P1`  STRING,\\n\" +\n+\t\t\t\t\t\t\"  `P2`  BIGINT\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"STORED AS `RCFILE`\");\n+\t\tsql(\"create external table tbl (x map<timestamp,array<timestamp>>) location '/table/path'\")\n+\t\t\t\t.ok(\"CREATE EXTERNAL TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  MAP< TIMESTAMP(9), ARRAY< TIMESTAMP(9) > >\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"LOCATION '/table/path'\");\n+\t\tsql(\"create temporary table tbl (x varchar(50)) partitioned by (p timestamp)\")\n+\t\t\t\t.ok(\"CREATE TEMPORARY TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  VARCHAR(50)\\n\" +\n+\t\t\t\t\t\t\")\\n\" +\n+\t\t\t\t\t\t\"PARTITIONED BY (\\n\" +\n+\t\t\t\t\t\t\"  `P`  TIMESTAMP(9)\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create table tbl (v varchar)\").fails(\"VARCHAR precision is mandatory\");\n+\t\t// TODO: support CLUSTERED BY, SKEWED BY, STORED BY, col constraints\n+\t}\n+\n+\t@Test\n+\tpublic void testConstraints() {\n+\t\tsql(\"create table tbl (x int not null enable rely, y string not null disable novalidate norely)\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  INTEGER NOT NULL ENABLE NOVALIDATE RELY,\\n\" +\n+\t\t\t\t\t\t\"  `Y`  STRING NOT NULL DISABLE NOVALIDATE NORELY\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create table tbl (x int,y timestamp not null,z string,primary key (x,z) disable novalidate rely)\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  INTEGER,\\n\" +\n+\t\t\t\t\t\t\"  `Y`  TIMESTAMP(9) NOT NULL ENABLE NOVALIDATE RELY,\\n\" +\n+\t\t\t\t\t\t\"  `Z`  STRING,\\n\" +\n+\t\t\t\t\t\t\"  PRIMARY KEY (`X`, `Z`) DISABLE NOVALIDATE RELY\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create table tbl (x binary,y date,z string,constraint pk_cons primary key(x))\")\n+\t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n+\t\t\t\t\t\t\"  `X`  VARBINARY(2147483647),\\n\" +\n+\t\t\t\t\t\t\"  `Y`  DATE,\\n\" +", "originalCommit": "61c782bea7d0f96ca0d71ef5384ae006c1795aaa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUyNzc3Mg==", "url": "https://github.com/apache/flink/pull/11935#discussion_r423527772", "bodyText": "\"'\\n\" + redundant new line.", "author": "danny0405", "createdAt": "2020-05-12T07:44:06Z", "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -121,8 +121,13 @@ public void testCreateTable() {\n \t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n \t\t\t\t\t\t\"  `X`  INTEGER\\n\" +\n \t\t\t\t\t\t\")\\n\" +\n-\t\t\t\t\t\t\"ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ESCAPED BY '\\\\' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' LINES TERMINATED BY '\\n\" +\n-\t\t\t\t\t\t\"' NULL DEFINED AS 'null'\\n\" +\n+\t\t\t\t\t\t\"ROW FORMAT DELIMITED\\n\" +\n+\t\t\t\t\t\t\"  FIELDS TERMINATED BY ',' ESCAPED BY '\\\\'\\n\" +\n+\t\t\t\t\t\t\"  COLLECTION ITEMS TERMINATED BY ','\\n\" +\n+\t\t\t\t\t\t\"  MAP KEYS TERMINATED BY ':'\\n\" +\n+\t\t\t\t\t\t\"  LINES TERMINATED BY '\\n\" +\n+\t\t\t\t\t\t\"'\\n\" +\n+\t\t\t\t\t\t\"  NULL DEFINED AS 'null'\\n\" +", "originalCommit": "206d429eda0f67e8545967829af4fe4b2e1a81f3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUzMjA0NA==", "url": "https://github.com/apache/flink/pull/11935#discussion_r423532044", "bodyText": "Does this mean we actually parse the type BINARY as VARBINARY(2147483647) ?", "author": "danny0405", "createdAt": "2020-05-12T07:51:16Z", "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -187,13 +187,13 @@ public void testConstraints() {\n \t\tsql(\"create table tbl (x int,y timestamp not null,z string,primary key (x,z) disable novalidate rely)\")\n \t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n \t\t\t\t\t\t\"  `X`  INTEGER,\\n\" +\n-\t\t\t\t\t\t\"  `Y`  TIMESTAMP(9) NOT NULL ENABLE NOVALIDATE RELY,\\n\" +\n+\t\t\t\t\t\t\"  `Y`  TIMESTAMP NOT NULL ENABLE NOVALIDATE RELY,\\n\" +\n \t\t\t\t\t\t\"  `Z`  STRING,\\n\" +\n \t\t\t\t\t\t\"  PRIMARY KEY (`X`, `Z`) DISABLE NOVALIDATE RELY\\n\" +\n \t\t\t\t\t\t\")\");\n \t\tsql(\"create table tbl (x binary,y date,z string,constraint pk_cons primary key(x))\")\n \t\t\t\t.ok(\"CREATE TABLE `TBL` (\\n\" +\n-\t\t\t\t\t\t\"  `X`  VARBINARY(2147483647),\\n\" +\n+\t\t\t\t\t\t\"  `X`  BINARY,\\n\" +\n \t\t\t\t\t\t\"  `Y`  DATE,\\n\" +", "originalCommit": "bafe9c7193f263b9ee720e342f4c5b91e8c82422", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU1NjQ3Nw==", "url": "https://github.com/apache/flink/pull/11935#discussion_r423556477", "bodyText": "Yes, it's the same as BYTES in Flink", "author": "lirui-apache", "createdAt": "2020-05-12T08:30:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUzMjA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU1ODcyMQ==", "url": "https://github.com/apache/flink/pull/11935#discussion_r423558721", "bodyText": "But Flink also has BINARY type, i think we should use that.", "author": "danny0405", "createdAt": "2020-05-12T08:34:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUzMjA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU2MTkwNQ==", "url": "https://github.com/apache/flink/pull/11935#discussion_r423561905", "bodyText": "No, Hive BINARY should be mapped to Flink BYTES, according to out type mappings:\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/hive/hive_catalog.html#supported-types", "author": "lirui-apache", "createdAt": "2020-05-12T08:39:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUzMjA0NA=="}], "type": "inlineReview"}, {"oid": "a1bab14d4c3c2f13e37bce567dfbacc136a55f0d", "url": "https://github.com/apache/flink/commit/a1bab14d4c3c2f13e37bce567dfbacc136a55f0d", "message": "[FLINK-17431][sql-parser-hive][hive] Implement table DDLs for Hive dialect part 1", "committedDate": "2020-05-12T11:42:54Z", "type": "commit"}, {"oid": "133161b7f414c601accf03df036a2b7acdc3201a", "url": "https://github.com/apache/flink/commit/133161b7f414c601accf03df036a2b7acdc3201a", "message": "address comments and fix collection delim", "committedDate": "2020-05-12T11:42:54Z", "type": "commit"}, {"oid": "07c59d47ab91d40dc60362b94bfe2e61354a48dd", "url": "https://github.com/apache/flink/commit/07c59d47ab91d40dc60362b94bfe2e61354a48dd", "message": "renaming", "committedDate": "2020-05-12T11:42:55Z", "type": "commit"}, {"oid": "26c8c4d434414a9ff81ee929979de6f983a8d98b", "url": "https://github.com/apache/flink/commit/26c8c4d434414a9ff81ee929979de6f983a8d98b", "message": "rebase", "committedDate": "2020-05-12T11:42:55Z", "type": "commit"}, {"oid": "169dae1322b847704a838a1082b2428d6b7af4c4", "url": "https://github.com/apache/flink/commit/169dae1322b847704a838a1082b2428d6b7af4c4", "message": "remove unused property", "committedDate": "2020-05-12T11:42:55Z", "type": "commit"}, {"oid": "a0fb3fba32733915999b7c4a1f8ef616123e007a", "url": "https://github.com/apache/flink/commit/a0fb3fba32733915999b7c4a1f8ef616123e007a", "message": "don't support bytes", "committedDate": "2020-05-12T11:42:55Z", "type": "commit"}, {"oid": "de61a577ae53b0110e2883dc42451e4462190d32", "url": "https://github.com/apache/flink/commit/de61a577ae53b0110e2883dc42451e4462190d32", "message": "add enums for hive constraint traits", "committedDate": "2020-05-12T11:42:55Z", "type": "commit"}, {"oid": "a644228cd855e3519e66eb13b9ee0f7bc2ec3a91", "url": "https://github.com/apache/flink/commit/a644228cd855e3519e66eb13b9ee0f7bc2ec3a91", "message": "store NN col names and add test", "committedDate": "2020-05-12T11:42:55Z", "type": "commit"}, {"oid": "4b5e67e6ba4ddc38d29fae79a6a06ae5ea661664", "url": "https://github.com/apache/flink/commit/4b5e67e6ba4ddc38d29fae79a6a06ae5ea661664", "message": "disallow varchar w/o precision", "committedDate": "2020-05-12T11:42:55Z", "type": "commit"}, {"oid": "48cd6963aeb6c672179d1d98145307161bd347ec", "url": "https://github.com/apache/flink/commit/48cd6963aeb6c672179d1d98145307161bd347ec", "message": "fix unparse for describe table", "committedDate": "2020-05-12T11:42:55Z", "type": "commit"}, {"oid": "51b54a309f0caf1417d7762e453b60195b9e7cf8", "url": "https://github.com/apache/flink/commit/51b54a309f0caf1417d7762e453b60195b9e7cf8", "message": "fix unparse row format", "committedDate": "2020-05-12T11:42:55Z", "type": "commit"}, {"oid": "ff267982efe548fc2cabd0bc7df35a1350403bff", "url": "https://github.com/apache/flink/commit/ff267982efe548fc2cabd0bc7df35a1350403bff", "message": "fix col type unparse", "committedDate": "2020-05-12T11:42:55Z", "type": "commit"}, {"oid": "792bbd1b48b7ff32a47bf3808290d3284bdc0a91", "url": "https://github.com/apache/flink/commit/792bbd1b48b7ff32a47bf3808290d3284bdc0a91", "message": "address comments", "committedDate": "2020-05-12T11:42:55Z", "type": "commit"}, {"oid": "792bbd1b48b7ff32a47bf3808290d3284bdc0a91", "url": "https://github.com/apache/flink/commit/792bbd1b48b7ff32a47bf3808290d3284bdc0a91", "message": "address comments", "committedDate": "2020-05-12T11:42:55Z", "type": "forcePushed"}]}