{"pr_number": 12120, "pr_title": "[FLINK-17547] Support unaligned checkpoints for records spilled to files", "pr_createdAt": "2020-05-13T10:30:23Z", "pr_url": "https://github.com/apache/flink/pull/12120", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQ5OTAyOA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424499028", "bodyText": "Shouldn't this be the other way around? throwable.addSuppressed(e)", "author": "pnowojski", "createdAt": "2020-05-13T14:49:41Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequest.java", "diffHunk": "@@ -46,8 +48,27 @@ static CheckpointInProgressRequest completeOutput(long checkpointId) {\n \t\treturn new CheckpointInProgressRequest(\"completeOutput\", checkpointId, ChannelStateCheckpointWriter::completeOutput, false);\n \t}\n \n-\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, Buffer... flinkBuffers) {\n-\t\treturn new CheckpointInProgressRequest(\"writeInput\", checkpointId, writer -> writer.writeInput(info, flinkBuffers), recycle(flinkBuffers), false);\n+\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, CloseableIterator<Buffer> iterator) {\n+\t\treturn new CheckpointInProgressRequest(\n+\t\t\t\"writeInput\",\n+\t\t\tcheckpointId,\n+\t\t\twriter -> {\n+\t\t\t\twhile (iterator.hasNext()) {\n+\t\t\t\t\tBuffer buffer = iterator.next();\n+\t\t\t\t\tcheckArgument(buffer.isBuffer());\n+\t\t\t\t\twriter.writeInput(info, buffer);\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\tthrowable -> {\n+\t\t\t\ttry {\n+\t\t\t\t\titerator.close();\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\tfinal RuntimeException re = new RuntimeException(\"unable to close iterator\", e);\n+\t\t\t\t\tre.addSuppressed(throwable);", "originalCommit": "75557c37eec1d0fddebf0f229b926fa884ab7379", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkxOTYwOQ==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424919609", "bodyText": "It depends on which exception is thrown afterward. Currently, it is re to which throwable is added as suppressed.\nDo you think we should throw throwable instead?\nIrrelevant after addressing the next comment.", "author": "rkhachatryan", "createdAt": "2020-05-14T07:16:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQ5OTAyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQ5OTI2Mg==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424499262", "bodyText": "Couldn't this be ThrowingConsumer so that we avoid RuntimeException?", "author": "pnowojski", "createdAt": "2020-05-13T14:49:56Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequest.java", "diffHunk": "@@ -46,8 +48,27 @@ static CheckpointInProgressRequest completeOutput(long checkpointId) {\n \t\treturn new CheckpointInProgressRequest(\"completeOutput\", checkpointId, ChannelStateCheckpointWriter::completeOutput, false);\n \t}\n \n-\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, Buffer... flinkBuffers) {\n-\t\treturn new CheckpointInProgressRequest(\"writeInput\", checkpointId, writer -> writer.writeInput(info, flinkBuffers), recycle(flinkBuffers), false);\n+\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, CloseableIterator<Buffer> iterator) {\n+\t\treturn new CheckpointInProgressRequest(\n+\t\t\t\"writeInput\",\n+\t\t\tcheckpointId,\n+\t\t\twriter -> {\n+\t\t\t\twhile (iterator.hasNext()) {\n+\t\t\t\t\tBuffer buffer = iterator.next();\n+\t\t\t\t\tcheckArgument(buffer.isBuffer());\n+\t\t\t\t\twriter.writeInput(info, buffer);\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\tthrowable -> {\n+\t\t\t\ttry {\n+\t\t\t\t\titerator.close();\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\tfinal RuntimeException re = new RuntimeException(\"unable to close iterator\", e);", "originalCommit": "75557c37eec1d0fddebf0f229b926fa884ab7379", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUwMDMwNg==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424500306", "bodyText": "I think inside ChannelStateWriteRequestExecutorImpl#run exceptions from this consumer, invoked via cleanupRequests() can be swallowed/ignored.", "author": "pnowojski", "createdAt": "2020-05-13T14:51:15Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequest.java", "diffHunk": "@@ -46,8 +48,27 @@ static CheckpointInProgressRequest completeOutput(long checkpointId) {\n \t\treturn new CheckpointInProgressRequest(\"completeOutput\", checkpointId, ChannelStateCheckpointWriter::completeOutput, false);\n \t}\n \n-\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, Buffer... flinkBuffers) {\n-\t\treturn new CheckpointInProgressRequest(\"writeInput\", checkpointId, writer -> writer.writeInput(info, flinkBuffers), recycle(flinkBuffers), false);\n+\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, CloseableIterator<Buffer> iterator) {\n+\t\treturn new CheckpointInProgressRequest(\n+\t\t\t\"writeInput\",\n+\t\t\tcheckpointId,\n+\t\t\twriter -> {\n+\t\t\t\twhile (iterator.hasNext()) {\n+\t\t\t\t\tBuffer buffer = iterator.next();\n+\t\t\t\t\tcheckArgument(buffer.isBuffer());\n+\t\t\t\t\twriter.writeInput(info, buffer);\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\tthrowable -> {", "originalCommit": "75557c37eec1d0fddebf0f229b926fa884ab7379", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUxNTcwMg==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424515702", "bodyText": "It looks like a bit of an overkill to introduce flatten for just this single use case. Especially that wrapInt produces unpooled buffer/segment (with no-op recycle).\nCan not we return wrapInt(recordLength) as first buffer from FileBasedBufferIterator?", "author": "pnowojski", "createdAt": "2020-05-13T15:10:53Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -183,22 +196,26 @@ private void updateLength(int length) throws IOException {\n \t@Override\n \tpublic CloseableIterator<Buffer> getUnconsumedSegment() throws IOException {\n \t\tif (isReadingLength()) {\n-\t\t\treturn singleBufferIterator(copyLengthBuffer());\n+\t\t\treturn singleBufferIterator(wrapCopy(lengthBuffer.array(), 0, lengthBuffer.position()));\n \t\t} else if (isAboveSpillingThreshold()) {\n-\t\t\tthrow new UnsupportedOperationException(\"Unaligned checkpoint currently do not support spilled records.\");\n+\t\t\treturn createSpilledDataIterator();\n \t\t} else if (recordLength == -1) {\n-\t\t\treturn CloseableIterator.empty(); // no remaining partial length or data\n+\t\t\treturn empty(); // no remaining partial length or data\n \t\t} else {\n \t\t\treturn singleBufferIterator(copyDataBuffer());\n \t\t}\n \t}\n \n-\tprivate MemorySegment copyLengthBuffer() {\n-\t\tint position = lengthBuffer.position();\n-\t\tMemorySegment segment = MemorySegmentFactory.allocateUnpooledSegment(position);\n-\t\tlengthBuffer.position(0);\n-\t\tsegment.put(0, lengthBuffer, position);\n-\t\treturn segment;\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate CloseableIterator<Buffer> createSpilledDataIterator() throws IOException {\n+\t\tif (spillingChannel != null) {\n+\t\t\tspillingChannel.force(false);\n+\t\t}\n+\t\treturn CloseableIterator.flatten(\n+\t\t\ttoSingleBufferIterator(wrapInt(recordLength)),\n+\t\t\tnew FileBasedBufferIterator(spillFile, min(accumulatedRecordBytes, recordLength), fileBufferSize),\n+\t\t\tleftOverData == null ? empty() : toSingleBufferIterator(wrapCopy(leftOverData.getArray(), leftOverStart, leftOverLimit))\n+\t\t);", "originalCommit": "91982f5d59af5b6540ffa4052050b18a56f92b25", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDkzODg2NA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424938864", "bodyText": "It flattens 3 iterators, one of which can be empty. I tried to put it all into FileBasedBufferIterator but it doesn't simplify anything (as there will be many checks there) and makes it more error-prone.", "author": "rkhachatryan", "createdAt": "2020-05-14T07:53:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUxNTcwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUxNzk5MQ==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424517991", "bodyText": "\ud83d\udc4d", "author": "pnowojski", "createdAt": "2020-05-13T15:13:58Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -183,22 +196,26 @@ private void updateLength(int length) throws IOException {\n \t@Override\n \tpublic CloseableIterator<Buffer> getUnconsumedSegment() throws IOException {\n \t\tif (isReadingLength()) {\n-\t\t\treturn singleBufferIterator(copyLengthBuffer());\n+\t\t\treturn singleBufferIterator(wrapCopy(lengthBuffer.array(), 0, lengthBuffer.position()));\n \t\t} else if (isAboveSpillingThreshold()) {\n-\t\t\tthrow new UnsupportedOperationException(\"Unaligned checkpoint currently do not support spilled records.\");\n+\t\t\treturn createSpilledDataIterator();\n \t\t} else if (recordLength == -1) {\n-\t\t\treturn CloseableIterator.empty(); // no remaining partial length or data\n+\t\t\treturn empty(); // no remaining partial length or data\n \t\t} else {\n \t\t\treturn singleBufferIterator(copyDataBuffer());\n \t\t}\n \t}\n \n-\tprivate MemorySegment copyLengthBuffer() {\n-\t\tint position = lengthBuffer.position();\n-\t\tMemorySegment segment = MemorySegmentFactory.allocateUnpooledSegment(position);\n-\t\tlengthBuffer.position(0);\n-\t\tsegment.put(0, lengthBuffer, position);\n-\t\treturn segment;\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate CloseableIterator<Buffer> createSpilledDataIterator() throws IOException {\n+\t\tif (spillingChannel != null) {\n+\t\t\tspillingChannel.force(false);", "originalCommit": "91982f5d59af5b6540ffa4052050b18a56f92b25", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzNDc1NA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424534754", "bodyText": "missing iterator.close() on the happy path?", "author": "pnowojski", "createdAt": "2020-05-13T15:36:04Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequest.java", "diffHunk": "@@ -46,8 +48,27 @@ static CheckpointInProgressRequest completeOutput(long checkpointId) {\n \t\treturn new CheckpointInProgressRequest(\"completeOutput\", checkpointId, ChannelStateCheckpointWriter::completeOutput, false);\n \t}\n \n-\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, Buffer... flinkBuffers) {\n-\t\treturn new CheckpointInProgressRequest(\"writeInput\", checkpointId, writer -> writer.writeInput(info, flinkBuffers), recycle(flinkBuffers), false);\n+\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, CloseableIterator<Buffer> iterator) {\n+\t\treturn new CheckpointInProgressRequest(\n+\t\t\t\"writeInput\",\n+\t\t\tcheckpointId,\n+\t\t\twriter -> {\n+\t\t\t\twhile (iterator.hasNext()) {\n+\t\t\t\t\tBuffer buffer = iterator.next();\n+\t\t\t\t\tcheckArgument(buffer.isBuffer());\n+\t\t\t\t\twriter.writeInput(info, buffer);\n+\t\t\t\t}", "originalCommit": "75557c37eec1d0fddebf0f229b926fa884ab7379", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzNjE1MQ==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424536151", "bodyText": "512KB? I would expect even smaller values to be working reasonably well, but below 1MB the memory consumption of the extra buffer will be less then 20% of the spilling buffer, while 2MB could increase memory consumption by ~40%.", "author": "pnowojski", "createdAt": "2020-05-13T15:37:45Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -43,15 +46,18 @@\n \n import static java.lang.Math.max;\n import static java.lang.Math.min;\n+import static org.apache.flink.core.memory.MemorySegmentFactory.wrapCopy;\n+import static org.apache.flink.core.memory.MemorySegmentFactory.wrapInt;\n import static org.apache.flink.runtime.io.network.api.serialization.NonSpanningWrapper.singleBufferIterator;\n import static org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.LENGTH_BYTES;\n+import static org.apache.flink.util.CloseableIterator.empty;\n import static org.apache.flink.util.FileUtils.writeCompletely;\n import static org.apache.flink.util.IOUtils.closeAllQuietly;\n \n final class SpanningWrapper implements BufferWrapper {\n \n-\tprivate static final int THRESHOLD_FOR_SPILLING = 5 * 1024 * 1024; // 5 MiBytes\n-\tprivate static final int FILE_BUFFER_SIZE = 2 * 1024 * 1024;\n+\tprivate static final int DEFAULT_THRESHOLD_FOR_SPILLING = 5 * 1024 * 1024; // 5 MiBytes\n+\tprivate static final int DEFAULT_FILE_BUFFER_SIZE = 2 * 1024 * 1024;", "originalCommit": "91982f5d59af5b6540ffa4052050b18a56f92b25", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk0ODI4Mw==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424948283", "bodyText": "I agree that it could be less but it's the same constant used for \"normal\" reading of \"spill\" file.\nI don't think it should be a different number of that we should change it in this PR (and without some experimentation).", "author": "rkhachatryan", "createdAt": "2020-05-14T08:09:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzNjE1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzODc4Nw==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424538787", "bodyText": "What's the point of this interface? (It's not being used anywhere)", "author": "pnowojski", "createdAt": "2020-05-13T15:41:20Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpillingAdaptiveSpanningRecordDeserializer.java", "diffHunk": "@@ -158,7 +113,10 @@ public void clear() {\n \n \t@Override\n \tpublic boolean hasUnfinishedData() {\n-\t\treturn this.nonSpanningWrapper.remaining() > 0 || this.spanningWrapper.getNumGatheredBytes() > 0;\n+\t\treturn this.nonSpanningWrapper.hasRemaining() || this.spanningWrapper.getNumGatheredBytes() > 0;\n \t}\n \n+\tinterface BufferWrapper {\n+\t\tOptional<MemorySegment> getUnconsumedSegment() throws IOException;\n+\t}", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk0OTk1Nw==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424949957", "bodyText": "It is implemented by both wrappers (used in SpillingAdaptiveSpanningRecordDeserializer).\nI think they could further be unified (unless we drop NonSpanningWrapper and merge\nSpillingAdaptiveSpanningRecordDeserializer with SpanningWrapper).", "author": "rkhachatryan", "createdAt": "2020-05-14T08:12:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzODc4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk5NDQxNw==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424994417", "bodyText": "I've seen it was implemented, but the interface itself was not being used anywhere, so I'm not sure if it has much value.", "author": "pnowojski", "createdAt": "2020-05-14T09:22:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzODc4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTA5NjcxMw==", "url": "https://github.com/apache/flink/pull/12120#discussion_r425096713", "bodyText": "Removed.", "author": "rkhachatryan", "createdAt": "2020-05-14T12:27:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzODc4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzOTcwNw==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424539707", "bodyText": "It's better if simpler/shorter branch goes first", "author": "pnowojski", "createdAt": "2020-05-13T15:42:33Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/NonSpanningWrapper.java", "diffHunk": "@@ -47,27 +64,33 @@ void clear() {\n \t\tthis.position = 0;\n \t}\n \n-\tvoid initializeFromMemorySegment(MemorySegment seg, int position, int leftOverLimit) {\n+\tvoid initializeFromMemorySegment(MemorySegment seg, int position, int limit) {\n \t\tthis.segment = seg;\n \t\tthis.position = position;\n-\t\tthis.limit = leftOverLimit;\n+\t\tthis.limit = limit;\n \t}\n \n-\tOptional<MemorySegment> getUnconsumedSegment() {\n-\t\tif (remaining() == 0) {\n+\t@Override\n+\tpublic Optional<MemorySegment> getUnconsumedSegment() {\n+\t\tif (hasRemaining()) {\n+\t\t\tMemorySegment target = MemorySegmentFactory.allocateUnpooledSegment(remaining());\n+\t\t\tsegment.copyTo(position, target, 0, remaining());\n+\t\t\treturn Optional.of(target);\n+\t\t} else {\n \t\t\treturn Optional.empty();", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk1MDU5MA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424950590", "bodyText": "The other point of view is that if shouldn't negate the condition :)\nWill swap it.", "author": "rkhachatryan", "createdAt": "2020-05-14T08:13:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzOTcwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU0OTYzNQ==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424549635", "bodyText": "This onPartialRecord is a bit of spaghetti dependency. Also it doesn't help that's it's a ThrowingConsumer so IDE support for finding implementations wouldn't work.\nWhy don't you return recordLen via some wrapper around DeserializationResult?", "author": "pnowojski", "createdAt": "2020-05-13T15:55:55Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/NonSpanningWrapper.java", "diffHunk": "@@ -290,4 +313,55 @@ public int read(byte[] b, int off, int len) {\n \tpublic int read(byte[] b) {\n \t\treturn read(b, 0, b.length);\n \t}\n+\n+\tTuple2<ByteBuffer, Integer> wrapIntoByteBuffer() {\n+\t\tint numBytes = remaining();\n+\t\treturn Tuple2.of(segment.wrap(position, numBytes), numBytes);\n+\t}\n+\n+\tint copyTo(byte[] dst) {\n+\t\tfinal int numBytesChunk = remaining();\n+\t\tsegment.get(position, dst, 0, numBytesChunk);\n+\t\treturn numBytesChunk;\n+\t}\n+\n+\t/**\n+\t * We have an incomplete length add our part of the length to the length buffer.\n+\t */\n+\tvoid transferTo(ByteBuffer dst) {\n+\t\tsegment.get(position, dst, remaining());\n+\t\tclear();\n+\t}\n+\n+\tprivate DeserializationResult readInto(IOReadableWritable target) throws IOException {\n+\t\ttarget.read(this);\n+\n+\t\tint remaining = remaining();\n+\t\tif (remaining > 0) {\n+\t\t\treturn INTERMEDIATE_RECORD_FROM_BUFFER;\n+\t\t} else if (remaining == 0) {\n+\t\t\treturn LAST_RECORD_FROM_BUFFER;\n+\t\t} else {\n+\t\t\tthrow new IOException(BROKEN_SERIALIZATION_ERROR_MESSAGE, new IndexOutOfBoundsException(\"Remaining = \" + remaining));\n+\t\t}\n+\t}\n+\n+\tDeserializationResult getNextRecord(IOReadableWritable target, ThrowingConsumer<Integer, IOException> onPartialRecord) throws IOException {\n+\t\tint recordLen = readInt();\n+\t\tif (canReadRecord(recordLen)) {\n+\t\t\treturn readInto(target);\n+\t\t} else {\n+\t\t\tonPartialRecord.accept(recordLen);", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU1NzYwMA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424557600", "bodyText": "Either the methods in the if conditions should be renamed to self-documenting ones, or the inlined comments should be preserved.\nBy looking at this right now, I'm not even sure which condition was mapped to which one (has the order of conditions been preserved?) after couple of minutes I figured it out, that the order was preserved, but since it wasn't obvious for me, it proves the point that something is missing here.", "author": "pnowojski", "createdAt": "2020-05-13T16:06:53Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpillingAdaptiveSpanningRecordDeserializer.java", "diffHunk": "@@ -91,62 +86,22 @@ public DeserializationResult getNextRecord(T target) throws IOException {\n \t\t// this should be the majority of the cases for small records\n \t\t// for large records, this portion of the work is very small in comparison anyways\n \n-\t\tint nonSpanningRemaining = this.nonSpanningWrapper.remaining();\n-\n-\t\t// check if we can get a full length;\n-\t\tif (nonSpanningRemaining >= 4) {\n-\t\t\tint len = this.nonSpanningWrapper.readInt();\n-\n-\t\t\tif (len <= nonSpanningRemaining - 4) {\n-\t\t\t\t// we can get a full record from here\n-\t\t\t\ttry {\n-\t\t\t\t\ttarget.read(this.nonSpanningWrapper);\n-\n-\t\t\t\t\tint remaining = this.nonSpanningWrapper.remaining();\n-\t\t\t\t\tif (remaining > 0) {\n-\t\t\t\t\t\treturn DeserializationResult.INTERMEDIATE_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse if (remaining == 0) {\n-\t\t\t\t\t\treturn DeserializationResult.LAST_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tthrow new IndexOutOfBoundsException(\"Remaining = \" + remaining);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tcatch (IndexOutOfBoundsException e) {\n-\t\t\t\t\tthrow new IOException(BROKEN_SERIALIZATION_ERROR_MESSAGE, e);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\t// we got the length, but we need the rest from the spanning deserializer\n-\t\t\t\t// and need to wait for more buffers\n-\t\t\t\tthis.spanningWrapper.initializeWithPartialRecord(this.nonSpanningWrapper, len);\n-\t\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t\t}\n-\t\t} else if (nonSpanningRemaining > 0) {\n-\t\t\t// we have an incomplete length\n-\t\t\t// add our part of the length to the length buffer\n-\t\t\tthis.spanningWrapper.initializeWithPartialLength(this.nonSpanningWrapper);\n-\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t}\n+\t\tif (nonSpanningWrapper.canReadLength()) {", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTA2OTMxMg==", "url": "https://github.com/apache/flink/pull/12120#discussion_r425069312", "bodyText": "Renamed as you suggested below.", "author": "rkhachatryan", "createdAt": "2020-05-14T11:35:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU1NzYwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU1OTUzMQ==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424559531", "bodyText": "nonSpanningWrapper.canReadLength() -> !nonSpanningWrapper.hasCompleteLength()?\nnonSpanningWrapper.canReadLength() -> nonSpanningWrapper.hasCompleteLength()?", "author": "pnowojski", "createdAt": "2020-05-13T16:09:45Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpillingAdaptiveSpanningRecordDeserializer.java", "diffHunk": "@@ -91,62 +86,22 @@ public DeserializationResult getNextRecord(T target) throws IOException {\n \t\t// this should be the majority of the cases for small records\n \t\t// for large records, this portion of the work is very small in comparison anyways\n \n-\t\tint nonSpanningRemaining = this.nonSpanningWrapper.remaining();\n-\n-\t\t// check if we can get a full length;\n-\t\tif (nonSpanningRemaining >= 4) {\n-\t\t\tint len = this.nonSpanningWrapper.readInt();\n-\n-\t\t\tif (len <= nonSpanningRemaining - 4) {\n-\t\t\t\t// we can get a full record from here\n-\t\t\t\ttry {\n-\t\t\t\t\ttarget.read(this.nonSpanningWrapper);\n-\n-\t\t\t\t\tint remaining = this.nonSpanningWrapper.remaining();\n-\t\t\t\t\tif (remaining > 0) {\n-\t\t\t\t\t\treturn DeserializationResult.INTERMEDIATE_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse if (remaining == 0) {\n-\t\t\t\t\t\treturn DeserializationResult.LAST_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tthrow new IndexOutOfBoundsException(\"Remaining = \" + remaining);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tcatch (IndexOutOfBoundsException e) {\n-\t\t\t\t\tthrow new IOException(BROKEN_SERIALIZATION_ERROR_MESSAGE, e);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\t// we got the length, but we need the rest from the spanning deserializer\n-\t\t\t\t// and need to wait for more buffers\n-\t\t\t\tthis.spanningWrapper.initializeWithPartialRecord(this.nonSpanningWrapper, len);\n-\t\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t\t}\n-\t\t} else if (nonSpanningRemaining > 0) {\n-\t\t\t// we have an incomplete length\n-\t\t\t// add our part of the length to the length buffer\n-\t\t\tthis.spanningWrapper.initializeWithPartialLength(this.nonSpanningWrapper);\n-\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t}\n+\t\tif (nonSpanningWrapper.canReadLength()) {", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU2MTc1Nw==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424561757", "bodyText": "Again it would help quite a lot with understanding the code, if the onPartialRecord would be handled here, instead of inside the getNextRecord(). As it is, it's strange that nonSpanningWrapper is implicitly, but directly using spanningWrapper. That's almost by definition purpose of ***SpanningRecordDeserializer class.", "author": "pnowojski", "createdAt": "2020-05-13T16:13:01Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpillingAdaptiveSpanningRecordDeserializer.java", "diffHunk": "@@ -91,62 +86,22 @@ public DeserializationResult getNextRecord(T target) throws IOException {\n \t\t// this should be the majority of the cases for small records\n \t\t// for large records, this portion of the work is very small in comparison anyways\n \n-\t\tint nonSpanningRemaining = this.nonSpanningWrapper.remaining();\n-\n-\t\t// check if we can get a full length;\n-\t\tif (nonSpanningRemaining >= 4) {\n-\t\t\tint len = this.nonSpanningWrapper.readInt();\n-\n-\t\t\tif (len <= nonSpanningRemaining - 4) {\n-\t\t\t\t// we can get a full record from here\n-\t\t\t\ttry {\n-\t\t\t\t\ttarget.read(this.nonSpanningWrapper);\n-\n-\t\t\t\t\tint remaining = this.nonSpanningWrapper.remaining();\n-\t\t\t\t\tif (remaining > 0) {\n-\t\t\t\t\t\treturn DeserializationResult.INTERMEDIATE_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse if (remaining == 0) {\n-\t\t\t\t\t\treturn DeserializationResult.LAST_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tthrow new IndexOutOfBoundsException(\"Remaining = \" + remaining);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tcatch (IndexOutOfBoundsException e) {\n-\t\t\t\t\tthrow new IOException(BROKEN_SERIALIZATION_ERROR_MESSAGE, e);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\t// we got the length, but we need the rest from the spanning deserializer\n-\t\t\t\t// and need to wait for more buffers\n-\t\t\t\tthis.spanningWrapper.initializeWithPartialRecord(this.nonSpanningWrapper, len);\n-\t\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t\t}\n-\t\t} else if (nonSpanningRemaining > 0) {\n-\t\t\t// we have an incomplete length\n-\t\t\t// add our part of the length to the length buffer\n-\t\t\tthis.spanningWrapper.initializeWithPartialLength(this.nonSpanningWrapper);\n-\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t}\n+\t\tif (nonSpanningWrapper.canReadLength()) {\n+\t\t\treturn nonSpanningWrapper.getNextRecord(\n+\t\t\t\ttarget,\n+\t\t\t\trecordLength -> spanningWrapper.transferFrom(nonSpanningWrapper, recordLength));", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU2NzM5OA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424567398", "bodyText": "hasPartialLength()?", "author": "pnowojski", "createdAt": "2020-05-13T16:21:35Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpillingAdaptiveSpanningRecordDeserializer.java", "diffHunk": "@@ -91,62 +86,22 @@ public DeserializationResult getNextRecord(T target) throws IOException {\n \t\t// this should be the majority of the cases for small records\n \t\t// for large records, this portion of the work is very small in comparison anyways\n \n-\t\tint nonSpanningRemaining = this.nonSpanningWrapper.remaining();\n-\n-\t\t// check if we can get a full length;\n-\t\tif (nonSpanningRemaining >= 4) {\n-\t\t\tint len = this.nonSpanningWrapper.readInt();\n-\n-\t\t\tif (len <= nonSpanningRemaining - 4) {\n-\t\t\t\t// we can get a full record from here\n-\t\t\t\ttry {\n-\t\t\t\t\ttarget.read(this.nonSpanningWrapper);\n-\n-\t\t\t\t\tint remaining = this.nonSpanningWrapper.remaining();\n-\t\t\t\t\tif (remaining > 0) {\n-\t\t\t\t\t\treturn DeserializationResult.INTERMEDIATE_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse if (remaining == 0) {\n-\t\t\t\t\t\treturn DeserializationResult.LAST_RECORD_FROM_BUFFER;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tthrow new IndexOutOfBoundsException(\"Remaining = \" + remaining);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tcatch (IndexOutOfBoundsException e) {\n-\t\t\t\t\tthrow new IOException(BROKEN_SERIALIZATION_ERROR_MESSAGE, e);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\t// we got the length, but we need the rest from the spanning deserializer\n-\t\t\t\t// and need to wait for more buffers\n-\t\t\t\tthis.spanningWrapper.initializeWithPartialRecord(this.nonSpanningWrapper, len);\n-\t\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t\t}\n-\t\t} else if (nonSpanningRemaining > 0) {\n-\t\t\t// we have an incomplete length\n-\t\t\t// add our part of the length to the length buffer\n-\t\t\tthis.spanningWrapper.initializeWithPartialLength(this.nonSpanningWrapper);\n-\t\t\tthis.nonSpanningWrapper.clear();\n-\t\t\treturn DeserializationResult.PARTIAL_RECORD;\n-\t\t}\n+\t\tif (nonSpanningWrapper.canReadLength()) {\n+\t\t\treturn nonSpanningWrapper.getNextRecord(\n+\t\t\t\ttarget,\n+\t\t\t\trecordLength -> spanningWrapper.transferFrom(nonSpanningWrapper, recordLength));\n \n-\t\t// spanning record case\n-\t\tif (this.spanningWrapper.hasFullRecord()) {\n-\t\t\t// get the full record\n-\t\t\ttarget.read(this.spanningWrapper.getInputView());\n+\t\t} else if (nonSpanningWrapper.hasRemaining()) {", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTA2ODM5OQ==", "url": "https://github.com/apache/flink/pull/12120#discussion_r425068399", "bodyText": "It would change the existing logic and make less readable IMO.", "author": "rkhachatryan", "createdAt": "2020-05-14T11:33:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU2NzM5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU3MTQ1Nw==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424571457", "bodyText": "Unnamed tuples are not very readable. I would say convert to a simple POJO class, but I think you do not need length here at all, right?\nByteBuffer buffer = partial.wrapIntoByteBuffer()\nint length = buffer.remaining()\nwriteCompletely(spillingChannel, buffer);\nreturn length;\n\n?", "author": "pnowojski", "createdAt": "2020-05-13T16:27:34Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -294,4 +290,15 @@ private static String randomString(Random random) {\n \t\trandom.nextBytes(bytes);\n \t\treturn StringUtils.byteToHexString(bytes);\n \t}\n+\n+\tprivate int spill(NonSpanningWrapper partial) throws IOException {\n+\t\tTuple2<ByteBuffer, Integer> bufferAndLen = partial.wrapIntoByteBuffer();\n+\t\twriteCompletely(spillingChannel, bufferAndLen.f0);\n+\t\treturn bufferAndLen.f1;\n+\t}", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU3NjM1NA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424576354", "bodyText": "copyTo -> copyPartialDataTo? -> copyContentTo?", "author": "pnowojski", "createdAt": "2020-05-13T16:35:18Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/NonSpanningWrapper.java", "diffHunk": "@@ -290,4 +313,55 @@ public int read(byte[] b, int off, int len) {\n \tpublic int read(byte[] b) {\n \t\treturn read(b, 0, b.length);\n \t}\n+\n+\tTuple2<ByteBuffer, Integer> wrapIntoByteBuffer() {\n+\t\tint numBytes = remaining();\n+\t\treturn Tuple2.of(segment.wrap(position, numBytes), numBytes);\n+\t}\n+\n+\tint copyTo(byte[] dst) {", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU3ODY4NQ==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424578685", "bodyText": "What is the difference between transferTo and copyTo? It's not clear from the signatures.\nAlso it's strange (and unnecessary?) that transferTo is calling clear() from within, but copyTo doesn't. I would unify this and pull the clear() call out of here.", "author": "pnowojski", "createdAt": "2020-05-13T16:38:44Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/NonSpanningWrapper.java", "diffHunk": "@@ -290,4 +313,55 @@ public int read(byte[] b, int off, int len) {\n \tpublic int read(byte[] b) {\n \t\treturn read(b, 0, b.length);\n \t}\n+\n+\tTuple2<ByteBuffer, Integer> wrapIntoByteBuffer() {\n+\t\tint numBytes = remaining();\n+\t\treturn Tuple2.of(segment.wrap(position, numBytes), numBytes);\n+\t}\n+\n+\tint copyTo(byte[] dst) {\n+\t\tfinal int numBytesChunk = remaining();\n+\t\tsegment.get(position, dst, 0, numBytesChunk);\n+\t\treturn numBytesChunk;\n+\t}\n+\n+\t/**\n+\t * We have an incomplete length add our part of the length to the length buffer.\n+\t */\n+\tvoid transferTo(ByteBuffer dst) {\n+\t\tsegment.get(position, dst, remaining());\n+\t\tclear();\n+\t}", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTA2NTg3MA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r425065870", "bodyText": "copyTo only copies data, while transferTo transfers \"ownership\" (and that's why it calls clear()). Added Javadoc for this method.", "author": "rkhachatryan", "createdAt": "2020-05-14T11:28:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU3ODY4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU3OTQzOQ==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424579439", "bodyText": "move this method below getNextRecord so the class could be read top to bottom?", "author": "pnowojski", "createdAt": "2020-05-13T16:39:56Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/NonSpanningWrapper.java", "diffHunk": "@@ -290,4 +313,55 @@ public int read(byte[] b, int off, int len) {\n \tpublic int read(byte[] b) {\n \t\treturn read(b, 0, b.length);\n \t}\n+\n+\tTuple2<ByteBuffer, Integer> wrapIntoByteBuffer() {\n+\t\tint numBytes = remaining();\n+\t\treturn Tuple2.of(segment.wrap(position, numBytes), numBytes);\n+\t}\n+\n+\tint copyTo(byte[] dst) {\n+\t\tfinal int numBytesChunk = remaining();\n+\t\tsegment.get(position, dst, 0, numBytesChunk);\n+\t\treturn numBytesChunk;\n+\t}\n+\n+\t/**\n+\t * We have an incomplete length add our part of the length to the length buffer.\n+\t */\n+\tvoid transferTo(ByteBuffer dst) {\n+\t\tsegment.get(position, dst, remaining());\n+\t\tclear();\n+\t}\n+\n+\tprivate DeserializationResult readInto(IOReadableWritable target) throws IOException {", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4MDUxOA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424580518", "bodyText": "startPosition (abbreviation \ud83d\udc4e )\nedit: using offset and numBytes in the arguments and position + remaining as local variables is confusing. It was less so in the old version, where it was obvious that those things are the same, but here this rename is mangled via readLengthIfNeeded - but that might be simplified if you return a single int from readLength (check comment below)", "author": "pnowojski", "createdAt": "2020-05-13T16:41:36Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -82,187 +94,171 @@ public SpanningWrapper(String[] tempDirs) {\n \t\tthis.buffer = initialBuffer;\n \t}\n \n-\tvoid initializeWithPartialRecord(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n-\t\t// set the length and copy what is available to the buffer\n-\t\tthis.recordLength = nextRecordLength;\n-\n-\t\tfinal int numBytesChunk = partial.remaining();\n-\n-\t\tif (nextRecordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t// create a spilling channel and put the data there\n-\t\t\tthis.spillingChannel = createSpillingChannel();\n-\n-\t\t\tByteBuffer toWrite = partial.segment.wrap(partial.position, numBytesChunk);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n-\t\t}\n-\t\telse {\n-\t\t\t// collect in memory\n-\t\t\tensureBufferCapacity(nextRecordLength);\n-\t\t\tpartial.segment.get(partial.position, buffer, 0, numBytesChunk);\n-\t\t}\n-\n-\t\tthis.accumulatedRecordBytes = numBytesChunk;\n+\t/**\n+\t * We got the length, but we need the rest from the spanning deserializer and need to wait for more buffers.\n+\t */\n+\tvoid transferFrom(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n+\t\tupdateLength(nextRecordLength);\n+\t\taccumulatedRecordBytes = isAboveSpillingThreshold() ? spill(partial) : partial.copyTo(buffer);\n+\t\tpartial.clear();\n \t}\n \n-\tvoid initializeWithPartialLength(NonSpanningWrapper partial) throws IOException {\n-\t\t// copy what we have to the length buffer\n-\t\tpartial.segment.get(partial.position, this.lengthBuffer, partial.remaining());\n+\tprivate boolean isAboveSpillingThreshold() {\n+\t\treturn recordLength > THRESHOLD_FOR_SPILLING;\n \t}\n \n \tvoid addNextChunkFromMemorySegment(MemorySegment segment, int offset, int numBytes) throws IOException {\n-\t\tint segmentPosition = offset;\n-\t\tint segmentRemaining = numBytes;\n-\t\t// check where to go. if we have a partial length, we need to complete it first\n-\t\tif (this.lengthBuffer.position() > 0) {\n-\t\t\tint toPut = Math.min(this.lengthBuffer.remaining(), segmentRemaining);\n-\t\t\tsegment.get(segmentPosition, this.lengthBuffer, toPut);\n-\t\t\t// did we complete the length?\n-\t\t\tif (this.lengthBuffer.hasRemaining()) {\n-\t\t\t\treturn;\n-\t\t\t} else {\n-\t\t\t\tthis.recordLength = this.lengthBuffer.getInt(0);\n-\n-\t\t\t\tthis.lengthBuffer.clear();\n-\t\t\t\tsegmentPosition += toPut;\n-\t\t\t\tsegmentRemaining -= toPut;\n-\t\t\t\tif (this.recordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t\t\tthis.spillingChannel = createSpillingChannel();\n-\t\t\t\t} else {\n-\t\t\t\t\tensureBufferCapacity(this.recordLength);\n-\t\t\t\t}\n-\t\t\t}\n+\t\tTuple2<Integer, Integer> positionAndRemaining = readLengthIfNeeded(segment, offset, numBytes);\n+\t\tint startPos = positionAndRemaining.f0;", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTA3MzczNA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r425073734", "bodyText": "startPosition  \ud83d\udc4e, but ok :)\n\nthose things are the same\n\nThey are not: we need to use old values to compute new leftOverLimit.", "author": "rkhachatryan", "createdAt": "2020-05-14T11:44:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4MDUxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4MjY1Mg==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424582652", "bodyText": "inline this method? One branch is a no-op just returning the arguments, which is quite strange. Also there would be one fewer place with unnamed Tupple2 (it's really difficult to understand what is this method doing looking at it, and the return value is a mystery)", "author": "pnowojski", "createdAt": "2020-05-13T16:45:05Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -82,187 +94,171 @@ public SpanningWrapper(String[] tempDirs) {\n \t\tthis.buffer = initialBuffer;\n \t}\n \n-\tvoid initializeWithPartialRecord(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n-\t\t// set the length and copy what is available to the buffer\n-\t\tthis.recordLength = nextRecordLength;\n-\n-\t\tfinal int numBytesChunk = partial.remaining();\n-\n-\t\tif (nextRecordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t// create a spilling channel and put the data there\n-\t\t\tthis.spillingChannel = createSpillingChannel();\n-\n-\t\t\tByteBuffer toWrite = partial.segment.wrap(partial.position, numBytesChunk);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n-\t\t}\n-\t\telse {\n-\t\t\t// collect in memory\n-\t\t\tensureBufferCapacity(nextRecordLength);\n-\t\t\tpartial.segment.get(partial.position, buffer, 0, numBytesChunk);\n-\t\t}\n-\n-\t\tthis.accumulatedRecordBytes = numBytesChunk;\n+\t/**\n+\t * We got the length, but we need the rest from the spanning deserializer and need to wait for more buffers.\n+\t */\n+\tvoid transferFrom(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n+\t\tupdateLength(nextRecordLength);\n+\t\taccumulatedRecordBytes = isAboveSpillingThreshold() ? spill(partial) : partial.copyTo(buffer);\n+\t\tpartial.clear();\n \t}\n \n-\tvoid initializeWithPartialLength(NonSpanningWrapper partial) throws IOException {\n-\t\t// copy what we have to the length buffer\n-\t\tpartial.segment.get(partial.position, this.lengthBuffer, partial.remaining());\n+\tprivate boolean isAboveSpillingThreshold() {\n+\t\treturn recordLength > THRESHOLD_FOR_SPILLING;\n \t}\n \n \tvoid addNextChunkFromMemorySegment(MemorySegment segment, int offset, int numBytes) throws IOException {\n-\t\tint segmentPosition = offset;\n-\t\tint segmentRemaining = numBytes;\n-\t\t// check where to go. if we have a partial length, we need to complete it first\n-\t\tif (this.lengthBuffer.position() > 0) {\n-\t\t\tint toPut = Math.min(this.lengthBuffer.remaining(), segmentRemaining);\n-\t\t\tsegment.get(segmentPosition, this.lengthBuffer, toPut);\n-\t\t\t// did we complete the length?\n-\t\t\tif (this.lengthBuffer.hasRemaining()) {\n-\t\t\t\treturn;\n-\t\t\t} else {\n-\t\t\t\tthis.recordLength = this.lengthBuffer.getInt(0);\n-\n-\t\t\t\tthis.lengthBuffer.clear();\n-\t\t\t\tsegmentPosition += toPut;\n-\t\t\t\tsegmentRemaining -= toPut;\n-\t\t\t\tif (this.recordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t\t\tthis.spillingChannel = createSpillingChannel();\n-\t\t\t\t} else {\n-\t\t\t\t\tensureBufferCapacity(this.recordLength);\n-\t\t\t\t}\n-\t\t\t}\n+\t\tTuple2<Integer, Integer> positionAndRemaining = readLengthIfNeeded(segment, offset, numBytes);\n+\t\tint startPos = positionAndRemaining.f0;\n+\t\tint remaining = positionAndRemaining.f1;\n+\t\tif (remaining == 0) {\n+\t\t\treturn;\n \t\t}\n \n-\t\t// copy as much as we need or can for this next spanning record\n-\t\tint needed = this.recordLength - this.accumulatedRecordBytes;\n-\t\tint toCopy = Math.min(needed, segmentRemaining);\n+\t\tint toCopy = min(recordLength - accumulatedRecordBytes, remaining);\n+\t\tif (toCopy > 0) {\n+\t\t\treadData(segment, startPos, toCopy);\n+\t\t}\n+\t\tif (remaining > toCopy) {\n+\t\t\tleftOverData = segment;\n+\t\t\tleftOverStart = startPos + toCopy;\n+\t\t\tleftOverLimit = offset + numBytes;\n+\t\t}\n+\t}\n \n-\t\tif (spillingChannel != null) {\n-\t\t\t// spill to file\n-\t\t\tByteBuffer toWrite = segment.wrap(segmentPosition, toCopy);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n+\tprivate void readData(MemorySegment segment, int offset, int length) throws IOException {\n+\t\tif (spillingChannel == null) {\n+\t\t\treadIntoBuffer(segment, offset, length);\n \t\t} else {\n-\t\t\tsegment.get(segmentPosition, buffer, this.accumulatedRecordBytes, toCopy);\n+\t\t\treadIntoFile(segment, offset, length);\n \t\t}\n+\t}\n \n-\t\tthis.accumulatedRecordBytes += toCopy;\n+\tprivate void readIntoFile(MemorySegment segment, int offset, int length) throws IOException {\n+\t\twriteCompletely(spillingChannel, segment.wrap(offset, length));\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tspillingChannel.close();\n+\t\t\tspillFileReader = new DataInputViewStreamWrapper(new BufferedInputStream(new FileInputStream(spillFile), FILE_BUFFER_SIZE));\n+\t\t}\n+\t}\n \n-\t\tif (toCopy < segmentRemaining) {\n-\t\t\t// there is more data in the segment\n-\t\t\tthis.leftOverData = segment;\n-\t\t\tthis.leftOverStart = segmentPosition + toCopy;\n-\t\t\tthis.leftOverLimit = numBytes + offset;\n+\tprivate void readIntoBuffer(MemorySegment segment, int offset, int length) {\n+\t\tsegment.get(offset, buffer, accumulatedRecordBytes, length);\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tserializationReadBuffer.setBuffer(buffer, 0, recordLength);\n \t\t}\n+\t}\n \n-\t\tif (accumulatedRecordBytes == recordLength) {\n-\t\t\t// we have the full record\n-\t\t\tif (spillingChannel == null) {\n-\t\t\t\tthis.serializationReadBuffer.setBuffer(buffer, 0, recordLength);\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tspillingChannel.close();\n+\tprivate Tuple2<Integer, Integer> readLengthIfNeeded(MemorySegment segment, int offset, int numBytes) throws IOException {\n+\t\tif (isReadingLength()) {\n+\t\t\treturn readLength(segment, offset, numBytes);\n+\t\t} else {\n+\t\t\treturn Tuple2.of(offset, numBytes);\n+\t\t}\n+\t}", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4NDc3MA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424584770", "bodyText": "Why is it a constant? It's not being used anywhere. It looks like the only value of it is to workaround the problem of unnamed fields of Tuple2?", "author": "pnowojski", "createdAt": "2020-05-13T16:48:17Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -82,187 +94,171 @@ public SpanningWrapper(String[] tempDirs) {\n \t\tthis.buffer = initialBuffer;\n \t}\n \n-\tvoid initializeWithPartialRecord(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n-\t\t// set the length and copy what is available to the buffer\n-\t\tthis.recordLength = nextRecordLength;\n-\n-\t\tfinal int numBytesChunk = partial.remaining();\n-\n-\t\tif (nextRecordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t// create a spilling channel and put the data there\n-\t\t\tthis.spillingChannel = createSpillingChannel();\n-\n-\t\t\tByteBuffer toWrite = partial.segment.wrap(partial.position, numBytesChunk);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n-\t\t}\n-\t\telse {\n-\t\t\t// collect in memory\n-\t\t\tensureBufferCapacity(nextRecordLength);\n-\t\t\tpartial.segment.get(partial.position, buffer, 0, numBytesChunk);\n-\t\t}\n-\n-\t\tthis.accumulatedRecordBytes = numBytesChunk;\n+\t/**\n+\t * We got the length, but we need the rest from the spanning deserializer and need to wait for more buffers.\n+\t */\n+\tvoid transferFrom(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n+\t\tupdateLength(nextRecordLength);\n+\t\taccumulatedRecordBytes = isAboveSpillingThreshold() ? spill(partial) : partial.copyTo(buffer);\n+\t\tpartial.clear();\n \t}\n \n-\tvoid initializeWithPartialLength(NonSpanningWrapper partial) throws IOException {\n-\t\t// copy what we have to the length buffer\n-\t\tpartial.segment.get(partial.position, this.lengthBuffer, partial.remaining());\n+\tprivate boolean isAboveSpillingThreshold() {\n+\t\treturn recordLength > THRESHOLD_FOR_SPILLING;\n \t}\n \n \tvoid addNextChunkFromMemorySegment(MemorySegment segment, int offset, int numBytes) throws IOException {\n-\t\tint segmentPosition = offset;\n-\t\tint segmentRemaining = numBytes;\n-\t\t// check where to go. if we have a partial length, we need to complete it first\n-\t\tif (this.lengthBuffer.position() > 0) {\n-\t\t\tint toPut = Math.min(this.lengthBuffer.remaining(), segmentRemaining);\n-\t\t\tsegment.get(segmentPosition, this.lengthBuffer, toPut);\n-\t\t\t// did we complete the length?\n-\t\t\tif (this.lengthBuffer.hasRemaining()) {\n-\t\t\t\treturn;\n-\t\t\t} else {\n-\t\t\t\tthis.recordLength = this.lengthBuffer.getInt(0);\n-\n-\t\t\t\tthis.lengthBuffer.clear();\n-\t\t\t\tsegmentPosition += toPut;\n-\t\t\t\tsegmentRemaining -= toPut;\n-\t\t\t\tif (this.recordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t\t\tthis.spillingChannel = createSpillingChannel();\n-\t\t\t\t} else {\n-\t\t\t\t\tensureBufferCapacity(this.recordLength);\n-\t\t\t\t}\n-\t\t\t}\n+\t\tTuple2<Integer, Integer> positionAndRemaining = readLengthIfNeeded(segment, offset, numBytes);\n+\t\tint startPos = positionAndRemaining.f0;\n+\t\tint remaining = positionAndRemaining.f1;\n+\t\tif (remaining == 0) {\n+\t\t\treturn;\n \t\t}\n \n-\t\t// copy as much as we need or can for this next spanning record\n-\t\tint needed = this.recordLength - this.accumulatedRecordBytes;\n-\t\tint toCopy = Math.min(needed, segmentRemaining);\n+\t\tint toCopy = min(recordLength - accumulatedRecordBytes, remaining);\n+\t\tif (toCopy > 0) {\n+\t\t\treadData(segment, startPos, toCopy);\n+\t\t}\n+\t\tif (remaining > toCopy) {\n+\t\t\tleftOverData = segment;\n+\t\t\tleftOverStart = startPos + toCopy;\n+\t\t\tleftOverLimit = offset + numBytes;\n+\t\t}\n+\t}\n \n-\t\tif (spillingChannel != null) {\n-\t\t\t// spill to file\n-\t\t\tByteBuffer toWrite = segment.wrap(segmentPosition, toCopy);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n+\tprivate void readData(MemorySegment segment, int offset, int length) throws IOException {\n+\t\tif (spillingChannel == null) {\n+\t\t\treadIntoBuffer(segment, offset, length);\n \t\t} else {\n-\t\t\tsegment.get(segmentPosition, buffer, this.accumulatedRecordBytes, toCopy);\n+\t\t\treadIntoFile(segment, offset, length);\n \t\t}\n+\t}\n \n-\t\tthis.accumulatedRecordBytes += toCopy;\n+\tprivate void readIntoFile(MemorySegment segment, int offset, int length) throws IOException {\n+\t\twriteCompletely(spillingChannel, segment.wrap(offset, length));\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tspillingChannel.close();\n+\t\t\tspillFileReader = new DataInputViewStreamWrapper(new BufferedInputStream(new FileInputStream(spillFile), FILE_BUFFER_SIZE));\n+\t\t}\n+\t}\n \n-\t\tif (toCopy < segmentRemaining) {\n-\t\t\t// there is more data in the segment\n-\t\t\tthis.leftOverData = segment;\n-\t\t\tthis.leftOverStart = segmentPosition + toCopy;\n-\t\t\tthis.leftOverLimit = numBytes + offset;\n+\tprivate void readIntoBuffer(MemorySegment segment, int offset, int length) {\n+\t\tsegment.get(offset, buffer, accumulatedRecordBytes, length);\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tserializationReadBuffer.setBuffer(buffer, 0, recordLength);\n \t\t}\n+\t}\n \n-\t\tif (accumulatedRecordBytes == recordLength) {\n-\t\t\t// we have the full record\n-\t\t\tif (spillingChannel == null) {\n-\t\t\t\tthis.serializationReadBuffer.setBuffer(buffer, 0, recordLength);\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tspillingChannel.close();\n+\tprivate Tuple2<Integer, Integer> readLengthIfNeeded(MemorySegment segment, int offset, int numBytes) throws IOException {\n+\t\tif (isReadingLength()) {\n+\t\t\treturn readLength(segment, offset, numBytes);\n+\t\t} else {\n+\t\t\treturn Tuple2.of(offset, numBytes);\n+\t\t}\n+\t}\n \n-\t\t\t\tBufferedInputStream inStream = new BufferedInputStream(new FileInputStream(spillFile), 2 * 1024 * 1024);\n-\t\t\t\tthis.spillFileReader = new DataInputViewStreamWrapper(inStream);\n-\t\t\t}\n+\tprivate Tuple2<Integer, Integer> readLength(MemorySegment segment, int segmentPosition, int segmentRemaining) throws IOException {\n+\t\tint numBytes = min(lengthBuffer.remaining(), segmentRemaining);\n+\t\tsegment.get(segmentPosition, lengthBuffer, numBytes);\n+\t\tif (lengthBuffer.hasRemaining()) {\n+\t\t\treturn POSITION_AND_SIZE_IF_LENGTH_NOT_FULLY_READ;", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk0MTg3Ng==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424941876", "bodyText": "To avoid unnecessary creation of the same Tuple2 each time (and make it more readable).", "author": "rkhachatryan", "createdAt": "2020-05-14T07:58:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU4NDc3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5MDU4MA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424590580", "bodyText": "If you return a single numBytes from this method, I think it would simplify the code. Returning number of bytes that was read/copied is quite standard practice and easy to explain/document/understand. While deciphering behaviour of this Tuple2 took me quite a bit of time.", "author": "pnowojski", "createdAt": "2020-05-13T16:57:31Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -82,187 +94,171 @@ public SpanningWrapper(String[] tempDirs) {\n \t\tthis.buffer = initialBuffer;\n \t}\n \n-\tvoid initializeWithPartialRecord(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n-\t\t// set the length and copy what is available to the buffer\n-\t\tthis.recordLength = nextRecordLength;\n-\n-\t\tfinal int numBytesChunk = partial.remaining();\n-\n-\t\tif (nextRecordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t// create a spilling channel and put the data there\n-\t\t\tthis.spillingChannel = createSpillingChannel();\n-\n-\t\t\tByteBuffer toWrite = partial.segment.wrap(partial.position, numBytesChunk);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n-\t\t}\n-\t\telse {\n-\t\t\t// collect in memory\n-\t\t\tensureBufferCapacity(nextRecordLength);\n-\t\t\tpartial.segment.get(partial.position, buffer, 0, numBytesChunk);\n-\t\t}\n-\n-\t\tthis.accumulatedRecordBytes = numBytesChunk;\n+\t/**\n+\t * We got the length, but we need the rest from the spanning deserializer and need to wait for more buffers.\n+\t */\n+\tvoid transferFrom(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n+\t\tupdateLength(nextRecordLength);\n+\t\taccumulatedRecordBytes = isAboveSpillingThreshold() ? spill(partial) : partial.copyTo(buffer);\n+\t\tpartial.clear();\n \t}\n \n-\tvoid initializeWithPartialLength(NonSpanningWrapper partial) throws IOException {\n-\t\t// copy what we have to the length buffer\n-\t\tpartial.segment.get(partial.position, this.lengthBuffer, partial.remaining());\n+\tprivate boolean isAboveSpillingThreshold() {\n+\t\treturn recordLength > THRESHOLD_FOR_SPILLING;\n \t}\n \n \tvoid addNextChunkFromMemorySegment(MemorySegment segment, int offset, int numBytes) throws IOException {\n-\t\tint segmentPosition = offset;\n-\t\tint segmentRemaining = numBytes;\n-\t\t// check where to go. if we have a partial length, we need to complete it first\n-\t\tif (this.lengthBuffer.position() > 0) {\n-\t\t\tint toPut = Math.min(this.lengthBuffer.remaining(), segmentRemaining);\n-\t\t\tsegment.get(segmentPosition, this.lengthBuffer, toPut);\n-\t\t\t// did we complete the length?\n-\t\t\tif (this.lengthBuffer.hasRemaining()) {\n-\t\t\t\treturn;\n-\t\t\t} else {\n-\t\t\t\tthis.recordLength = this.lengthBuffer.getInt(0);\n-\n-\t\t\t\tthis.lengthBuffer.clear();\n-\t\t\t\tsegmentPosition += toPut;\n-\t\t\t\tsegmentRemaining -= toPut;\n-\t\t\t\tif (this.recordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t\t\tthis.spillingChannel = createSpillingChannel();\n-\t\t\t\t} else {\n-\t\t\t\t\tensureBufferCapacity(this.recordLength);\n-\t\t\t\t}\n-\t\t\t}\n+\t\tTuple2<Integer, Integer> positionAndRemaining = readLengthIfNeeded(segment, offset, numBytes);\n+\t\tint startPos = positionAndRemaining.f0;\n+\t\tint remaining = positionAndRemaining.f1;\n+\t\tif (remaining == 0) {\n+\t\t\treturn;\n \t\t}\n \n-\t\t// copy as much as we need or can for this next spanning record\n-\t\tint needed = this.recordLength - this.accumulatedRecordBytes;\n-\t\tint toCopy = Math.min(needed, segmentRemaining);\n+\t\tint toCopy = min(recordLength - accumulatedRecordBytes, remaining);\n+\t\tif (toCopy > 0) {\n+\t\t\treadData(segment, startPos, toCopy);\n+\t\t}\n+\t\tif (remaining > toCopy) {\n+\t\t\tleftOverData = segment;\n+\t\t\tleftOverStart = startPos + toCopy;\n+\t\t\tleftOverLimit = offset + numBytes;\n+\t\t}\n+\t}\n \n-\t\tif (spillingChannel != null) {\n-\t\t\t// spill to file\n-\t\t\tByteBuffer toWrite = segment.wrap(segmentPosition, toCopy);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n+\tprivate void readData(MemorySegment segment, int offset, int length) throws IOException {\n+\t\tif (spillingChannel == null) {\n+\t\t\treadIntoBuffer(segment, offset, length);\n \t\t} else {\n-\t\t\tsegment.get(segmentPosition, buffer, this.accumulatedRecordBytes, toCopy);\n+\t\t\treadIntoFile(segment, offset, length);\n \t\t}\n+\t}\n \n-\t\tthis.accumulatedRecordBytes += toCopy;\n+\tprivate void readIntoFile(MemorySegment segment, int offset, int length) throws IOException {\n+\t\twriteCompletely(spillingChannel, segment.wrap(offset, length));\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tspillingChannel.close();\n+\t\t\tspillFileReader = new DataInputViewStreamWrapper(new BufferedInputStream(new FileInputStream(spillFile), FILE_BUFFER_SIZE));\n+\t\t}\n+\t}\n \n-\t\tif (toCopy < segmentRemaining) {\n-\t\t\t// there is more data in the segment\n-\t\t\tthis.leftOverData = segment;\n-\t\t\tthis.leftOverStart = segmentPosition + toCopy;\n-\t\t\tthis.leftOverLimit = numBytes + offset;\n+\tprivate void readIntoBuffer(MemorySegment segment, int offset, int length) {\n+\t\tsegment.get(offset, buffer, accumulatedRecordBytes, length);\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tserializationReadBuffer.setBuffer(buffer, 0, recordLength);\n \t\t}\n+\t}\n \n-\t\tif (accumulatedRecordBytes == recordLength) {\n-\t\t\t// we have the full record\n-\t\t\tif (spillingChannel == null) {\n-\t\t\t\tthis.serializationReadBuffer.setBuffer(buffer, 0, recordLength);\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tspillingChannel.close();\n+\tprivate Tuple2<Integer, Integer> readLengthIfNeeded(MemorySegment segment, int offset, int numBytes) throws IOException {\n+\t\tif (isReadingLength()) {\n+\t\t\treturn readLength(segment, offset, numBytes);\n+\t\t} else {\n+\t\t\treturn Tuple2.of(offset, numBytes);\n+\t\t}\n+\t}\n \n-\t\t\t\tBufferedInputStream inStream = new BufferedInputStream(new FileInputStream(spillFile), 2 * 1024 * 1024);\n-\t\t\t\tthis.spillFileReader = new DataInputViewStreamWrapper(inStream);\n-\t\t\t}\n+\tprivate Tuple2<Integer, Integer> readLength(MemorySegment segment, int segmentPosition, int segmentRemaining) throws IOException {\n+\t\tint numBytes = min(lengthBuffer.remaining(), segmentRemaining);\n+\t\tsegment.get(segmentPosition, lengthBuffer, numBytes);\n+\t\tif (lengthBuffer.hasRemaining()) {\n+\t\t\treturn POSITION_AND_SIZE_IF_LENGTH_NOT_FULLY_READ;\n+\t\t} else {\n+\t\t\tupdateLength(lengthBuffer.getInt(0));\n+\t\t\treturn Tuple2.of(segmentPosition + numBytes, segmentRemaining - numBytes);", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5MzIyMg==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424593222", "bodyText": "nit: numBytes has a very different meaning on the call stack in addNextChunkFromMemorySegment() method, so naming (collision?) is a bit unfortunate.", "author": "pnowojski", "createdAt": "2020-05-13T17:01:38Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -82,187 +94,171 @@ public SpanningWrapper(String[] tempDirs) {\n \t\tthis.buffer = initialBuffer;\n \t}\n \n-\tvoid initializeWithPartialRecord(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n-\t\t// set the length and copy what is available to the buffer\n-\t\tthis.recordLength = nextRecordLength;\n-\n-\t\tfinal int numBytesChunk = partial.remaining();\n-\n-\t\tif (nextRecordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t// create a spilling channel and put the data there\n-\t\t\tthis.spillingChannel = createSpillingChannel();\n-\n-\t\t\tByteBuffer toWrite = partial.segment.wrap(partial.position, numBytesChunk);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n-\t\t}\n-\t\telse {\n-\t\t\t// collect in memory\n-\t\t\tensureBufferCapacity(nextRecordLength);\n-\t\t\tpartial.segment.get(partial.position, buffer, 0, numBytesChunk);\n-\t\t}\n-\n-\t\tthis.accumulatedRecordBytes = numBytesChunk;\n+\t/**\n+\t * We got the length, but we need the rest from the spanning deserializer and need to wait for more buffers.\n+\t */\n+\tvoid transferFrom(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n+\t\tupdateLength(nextRecordLength);\n+\t\taccumulatedRecordBytes = isAboveSpillingThreshold() ? spill(partial) : partial.copyTo(buffer);\n+\t\tpartial.clear();\n \t}\n \n-\tvoid initializeWithPartialLength(NonSpanningWrapper partial) throws IOException {\n-\t\t// copy what we have to the length buffer\n-\t\tpartial.segment.get(partial.position, this.lengthBuffer, partial.remaining());\n+\tprivate boolean isAboveSpillingThreshold() {\n+\t\treturn recordLength > THRESHOLD_FOR_SPILLING;\n \t}\n \n \tvoid addNextChunkFromMemorySegment(MemorySegment segment, int offset, int numBytes) throws IOException {\n-\t\tint segmentPosition = offset;\n-\t\tint segmentRemaining = numBytes;\n-\t\t// check where to go. if we have a partial length, we need to complete it first\n-\t\tif (this.lengthBuffer.position() > 0) {\n-\t\t\tint toPut = Math.min(this.lengthBuffer.remaining(), segmentRemaining);\n-\t\t\tsegment.get(segmentPosition, this.lengthBuffer, toPut);\n-\t\t\t// did we complete the length?\n-\t\t\tif (this.lengthBuffer.hasRemaining()) {\n-\t\t\t\treturn;\n-\t\t\t} else {\n-\t\t\t\tthis.recordLength = this.lengthBuffer.getInt(0);\n-\n-\t\t\t\tthis.lengthBuffer.clear();\n-\t\t\t\tsegmentPosition += toPut;\n-\t\t\t\tsegmentRemaining -= toPut;\n-\t\t\t\tif (this.recordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t\t\tthis.spillingChannel = createSpillingChannel();\n-\t\t\t\t} else {\n-\t\t\t\t\tensureBufferCapacity(this.recordLength);\n-\t\t\t\t}\n-\t\t\t}\n+\t\tTuple2<Integer, Integer> positionAndRemaining = readLengthIfNeeded(segment, offset, numBytes);\n+\t\tint startPos = positionAndRemaining.f0;\n+\t\tint remaining = positionAndRemaining.f1;\n+\t\tif (remaining == 0) {\n+\t\t\treturn;\n \t\t}\n \n-\t\t// copy as much as we need or can for this next spanning record\n-\t\tint needed = this.recordLength - this.accumulatedRecordBytes;\n-\t\tint toCopy = Math.min(needed, segmentRemaining);\n+\t\tint toCopy = min(recordLength - accumulatedRecordBytes, remaining);\n+\t\tif (toCopy > 0) {\n+\t\t\treadData(segment, startPos, toCopy);\n+\t\t}\n+\t\tif (remaining > toCopy) {\n+\t\t\tleftOverData = segment;\n+\t\t\tleftOverStart = startPos + toCopy;\n+\t\t\tleftOverLimit = offset + numBytes;\n+\t\t}\n+\t}\n \n-\t\tif (spillingChannel != null) {\n-\t\t\t// spill to file\n-\t\t\tByteBuffer toWrite = segment.wrap(segmentPosition, toCopy);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n+\tprivate void readData(MemorySegment segment, int offset, int length) throws IOException {\n+\t\tif (spillingChannel == null) {\n+\t\t\treadIntoBuffer(segment, offset, length);\n \t\t} else {\n-\t\t\tsegment.get(segmentPosition, buffer, this.accumulatedRecordBytes, toCopy);\n+\t\t\treadIntoFile(segment, offset, length);\n \t\t}\n+\t}\n \n-\t\tthis.accumulatedRecordBytes += toCopy;\n+\tprivate void readIntoFile(MemorySegment segment, int offset, int length) throws IOException {\n+\t\twriteCompletely(spillingChannel, segment.wrap(offset, length));\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tspillingChannel.close();\n+\t\t\tspillFileReader = new DataInputViewStreamWrapper(new BufferedInputStream(new FileInputStream(spillFile), FILE_BUFFER_SIZE));\n+\t\t}\n+\t}\n \n-\t\tif (toCopy < segmentRemaining) {\n-\t\t\t// there is more data in the segment\n-\t\t\tthis.leftOverData = segment;\n-\t\t\tthis.leftOverStart = segmentPosition + toCopy;\n-\t\t\tthis.leftOverLimit = numBytes + offset;\n+\tprivate void readIntoBuffer(MemorySegment segment, int offset, int length) {\n+\t\tsegment.get(offset, buffer, accumulatedRecordBytes, length);\n+\t\taccumulatedRecordBytes += length;\n+\t\tif (hasFullRecord()) {\n+\t\t\tserializationReadBuffer.setBuffer(buffer, 0, recordLength);\n \t\t}\n+\t}\n \n-\t\tif (accumulatedRecordBytes == recordLength) {\n-\t\t\t// we have the full record\n-\t\t\tif (spillingChannel == null) {\n-\t\t\t\tthis.serializationReadBuffer.setBuffer(buffer, 0, recordLength);\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tspillingChannel.close();\n+\tprivate Tuple2<Integer, Integer> readLengthIfNeeded(MemorySegment segment, int offset, int numBytes) throws IOException {\n+\t\tif (isReadingLength()) {\n+\t\t\treturn readLength(segment, offset, numBytes);\n+\t\t} else {\n+\t\t\treturn Tuple2.of(offset, numBytes);\n+\t\t}\n+\t}\n \n-\t\t\t\tBufferedInputStream inStream = new BufferedInputStream(new FileInputStream(spillFile), 2 * 1024 * 1024);\n-\t\t\t\tthis.spillFileReader = new DataInputViewStreamWrapper(inStream);\n-\t\t\t}\n+\tprivate Tuple2<Integer, Integer> readLength(MemorySegment segment, int segmentPosition, int segmentRemaining) throws IOException {\n+\t\tint numBytes = min(lengthBuffer.remaining(), segmentRemaining);", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5NTM0OQ==", "url": "https://github.com/apache/flink/pull/12120#discussion_r424595349", "bodyText": "nit: readData -> copyFromSegment\nreadIntoBuffer -> copyIntoBuffer / copyFromSegmentIntoBuffer\nreadIntoFile -> copyIntoFile / copyFromSegmentIntoFile\n?\nIt wasn't obvious to me what's the direction and purpose of the plain \"readData\". (Inside readData it was already obvious)", "author": "pnowojski", "createdAt": "2020-05-13T17:05:07Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/SpanningWrapper.java", "diffHunk": "@@ -82,187 +94,171 @@ public SpanningWrapper(String[] tempDirs) {\n \t\tthis.buffer = initialBuffer;\n \t}\n \n-\tvoid initializeWithPartialRecord(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n-\t\t// set the length and copy what is available to the buffer\n-\t\tthis.recordLength = nextRecordLength;\n-\n-\t\tfinal int numBytesChunk = partial.remaining();\n-\n-\t\tif (nextRecordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t// create a spilling channel and put the data there\n-\t\t\tthis.spillingChannel = createSpillingChannel();\n-\n-\t\t\tByteBuffer toWrite = partial.segment.wrap(partial.position, numBytesChunk);\n-\t\t\tFileUtils.writeCompletely(this.spillingChannel, toWrite);\n-\t\t}\n-\t\telse {\n-\t\t\t// collect in memory\n-\t\t\tensureBufferCapacity(nextRecordLength);\n-\t\t\tpartial.segment.get(partial.position, buffer, 0, numBytesChunk);\n-\t\t}\n-\n-\t\tthis.accumulatedRecordBytes = numBytesChunk;\n+\t/**\n+\t * We got the length, but we need the rest from the spanning deserializer and need to wait for more buffers.\n+\t */\n+\tvoid transferFrom(NonSpanningWrapper partial, int nextRecordLength) throws IOException {\n+\t\tupdateLength(nextRecordLength);\n+\t\taccumulatedRecordBytes = isAboveSpillingThreshold() ? spill(partial) : partial.copyTo(buffer);\n+\t\tpartial.clear();\n \t}\n \n-\tvoid initializeWithPartialLength(NonSpanningWrapper partial) throws IOException {\n-\t\t// copy what we have to the length buffer\n-\t\tpartial.segment.get(partial.position, this.lengthBuffer, partial.remaining());\n+\tprivate boolean isAboveSpillingThreshold() {\n+\t\treturn recordLength > THRESHOLD_FOR_SPILLING;\n \t}\n \n \tvoid addNextChunkFromMemorySegment(MemorySegment segment, int offset, int numBytes) throws IOException {\n-\t\tint segmentPosition = offset;\n-\t\tint segmentRemaining = numBytes;\n-\t\t// check where to go. if we have a partial length, we need to complete it first\n-\t\tif (this.lengthBuffer.position() > 0) {\n-\t\t\tint toPut = Math.min(this.lengthBuffer.remaining(), segmentRemaining);\n-\t\t\tsegment.get(segmentPosition, this.lengthBuffer, toPut);\n-\t\t\t// did we complete the length?\n-\t\t\tif (this.lengthBuffer.hasRemaining()) {\n-\t\t\t\treturn;\n-\t\t\t} else {\n-\t\t\t\tthis.recordLength = this.lengthBuffer.getInt(0);\n-\n-\t\t\t\tthis.lengthBuffer.clear();\n-\t\t\t\tsegmentPosition += toPut;\n-\t\t\t\tsegmentRemaining -= toPut;\n-\t\t\t\tif (this.recordLength > THRESHOLD_FOR_SPILLING) {\n-\t\t\t\t\tthis.spillingChannel = createSpillingChannel();\n-\t\t\t\t} else {\n-\t\t\t\t\tensureBufferCapacity(this.recordLength);\n-\t\t\t\t}\n-\t\t\t}\n+\t\tTuple2<Integer, Integer> positionAndRemaining = readLengthIfNeeded(segment, offset, numBytes);\n+\t\tint startPos = positionAndRemaining.f0;\n+\t\tint remaining = positionAndRemaining.f1;\n+\t\tif (remaining == 0) {\n+\t\t\treturn;\n \t\t}\n \n-\t\t// copy as much as we need or can for this next spanning record\n-\t\tint needed = this.recordLength - this.accumulatedRecordBytes;\n-\t\tint toCopy = Math.min(needed, segmentRemaining);\n+\t\tint toCopy = min(recordLength - accumulatedRecordBytes, remaining);\n+\t\tif (toCopy > 0) {\n+\t\t\treadData(segment, startPos, toCopy);", "originalCommit": "670aab1cacb84a6403811dd959dffd2e1d03d889", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b9fe05dbed1b5e67d9495f3619886d7ffd5b101f", "url": "https://github.com/apache/flink/commit/b9fe05dbed1b5e67d9495f3619886d7ffd5b101f", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers (incomplete)", "committedDate": "2020-05-14T12:30:56Z", "type": "forcePushed"}, {"oid": "a83ee4ba9c88ba7a9d3ac8065cd056db57c5f65d", "url": "https://github.com/apache/flink/commit/a83ee4ba9c88ba7a9d3ac8065cd056db57c5f65d", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers (incomplete)", "committedDate": "2020-05-14T14:35:34Z", "type": "forcePushed"}, {"oid": "2b8d574b50b77c59aeee5f45c796eaa16ef13aeb", "url": "https://github.com/apache/flink/commit/2b8d574b50b77c59aeee5f45c796eaa16ef13aeb", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers", "committedDate": "2020-05-14T17:10:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM5MDYzMw==", "url": "https://github.com/apache/flink/pull/12120#discussion_r425390633", "bodyText": "are you sure this won't be called twice? Or if it will, that it will not double release the buffers/files?", "author": "pnowojski", "createdAt": "2020-05-14T19:50:10Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequest.java", "diffHunk": "@@ -46,8 +47,23 @@ static CheckpointInProgressRequest completeOutput(long checkpointId) {\n \t\treturn new CheckpointInProgressRequest(\"completeOutput\", checkpointId, ChannelStateCheckpointWriter::completeOutput, false);\n \t}\n \n-\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, Buffer... flinkBuffers) {\n-\t\treturn new CheckpointInProgressRequest(\"writeInput\", checkpointId, writer -> writer.writeInput(info, flinkBuffers), recycle(flinkBuffers), false);\n+\tstatic ChannelStateWriteRequest write(long checkpointId, InputChannelInfo info, CloseableIterator<Buffer> iterator) {\n+\t\treturn new CheckpointInProgressRequest(\n+\t\t\t\"writeInput\",\n+\t\t\tcheckpointId,\n+\t\t\twriter -> {\n+\t\t\t\ttry {\n+\t\t\t\t\twhile (iterator.hasNext()) {\n+\t\t\t\t\t\tBuffer buffer = iterator.next();\n+\t\t\t\t\t\tcheckArgument(buffer.isBuffer());\n+\t\t\t\t\t\twriter.writeInput(info, buffer);\n+\t\t\t\t\t}\n+\t\t\t\t} finally {\n+\t\t\t\t\titerator.close();\n+\t\t\t\t}\n+\t\t\t},\n+\t\t\tthrowable -> iterator.close(),", "originalCommit": "5c01c2037a94b4b236fba20c5facce73351dfd58", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "16e4c4a0a573d16eeb52692d8ab4af949c80170d", "url": "https://github.com/apache/flink/commit/16e4c4a0a573d16eeb52692d8ab4af949c80170d", "message": "fixup! [FLINK-17547][task] Use iterator for unconsumed buffers. Motivation: support spilled records Changes: 1. change SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer signature 2. adapt channel state persistence to new types", "committedDate": "2020-05-14T20:06:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU4MzA2Mg==", "url": "https://github.com/apache/flink/pull/12120#discussion_r425583062", "bodyText": "we had quite a bit of problems with error logging like that polluting the logs failing e2e tests. It would be better to add e as suppressed exception to the thrown, like org.apache.flink.streaming.runtime.tasks.StreamTask#invoke is doing.\n} catch (Exception e) {\n\tdispatcher.fail(ExceptionUtils.firstOrSuppressed(e, thrown));\n}\n\n?", "author": "pnowojski", "createdAt": "2020-05-15T06:00:39Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequestExecutorImpl.java", "diffHunk": "@@ -67,8 +70,11 @@ void run() {\n \t\t} catch (Exception ex) {\n \t\t\tthrown = ex;\n \t\t} finally {\n-\t\t\tcleanupRequests();\n-\t\t\tdispatcher.fail(thrown == null ? new CancellationException() : thrown);\n+\t\t\ttry {\n+\t\t\t\tcloseAll(this::cleanupRequests, () -> dispatcher.fail(thrown == null ? new CancellationException() : thrown));\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tLOG.error(\"unable to terminate properly\", e);\n+\t\t\t}", "originalCommit": "16e4c4a0a573d16eeb52692d8ab4af949c80170d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYwMDkzNQ==", "url": "https://github.com/apache/flink/pull/12120#discussion_r425600935", "bodyText": "I'll update the thrown field (dispatcher.fail is already called two lines above).", "author": "rkhachatryan", "createdAt": "2020-05-15T06:52:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU4MzA2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU4ODExOA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r425588118", "bodyText": "nit: by wrapper I meant a POJO not Tuple2, but this is probably fine.\nHowever now I'm a bit worried about performance implications of this - it's a critical/hot path, and we are allocating new object on it.", "author": "pnowojski", "createdAt": "2020-05-15T06:16:17Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/serialization/NonSpanningWrapper.java", "diffHunk": "@@ -290,4 +309,49 @@ public int read(byte[] b, int off, int len) {\n \tpublic int read(byte[] b) {\n \t\treturn read(b, 0, b.length);\n \t}\n+\n+\tByteBuffer wrapIntoByteBuffer() {\n+\t\treturn segment.wrap(position, remaining());\n+\t}\n+\n+\tint copyContentTo(byte[] dst) {\n+\t\tfinal int numBytesChunk = remaining();\n+\t\tsegment.get(position, dst, 0, numBytesChunk);\n+\t\treturn numBytesChunk;\n+\t}\n+\n+\t/**\n+\t * Copies the data and transfers the \"ownership\" (i.e. clears current wrapper).\n+\t */\n+\tvoid transferTo(ByteBuffer dst) {\n+\t\tsegment.get(position, dst, remaining());\n+\t\tclear();\n+\t}\n+\n+\tTuple2<DeserializationResult, Integer> getNextRecord(IOReadableWritable target) throws IOException {\n+\t\tint recordLen = readInt();\n+\t\tif (canReadRecord(recordLen)) {\n+\t\t\treturn readInto(target);\n+\t\t} else {\n+\t\t\treturn new Tuple2<>(PARTIAL_RECORD, recordLen);\n+\t\t}\n+\t}", "originalCommit": "2c35fca3a72306f9f19b85da09e400cd33896cb3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTYwNTAzMw==", "url": "https://github.com/apache/flink/pull/12120#discussion_r425605033", "bodyText": "I'll create a POJO and will reuse the instance (it's a single thread).", "author": "rkhachatryan", "createdAt": "2020-05-15T07:02:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU4ODExOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkxMDAwNw==", "url": "https://github.com/apache/flink/pull/12120#discussion_r425910007", "bodyText": "nit: This is a really hacky piece of code :)\nI'm fine with keeping it as it is to not trigger another re-build over a minor issue.", "author": "pnowojski", "createdAt": "2020-05-15T16:20:55Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriteRequestExecutorImpl.java", "diffHunk": "@@ -67,8 +70,12 @@ void run() {\n \t\t} catch (Exception ex) {\n \t\t\tthrown = ex;\n \t\t} finally {\n-\t\t\tcleanupRequests();\n-\t\t\tdispatcher.fail(thrown == null ? new CancellationException() : thrown);\n+\t\t\ttry {\n+\t\t\t\tcloseAll(this::cleanupRequests, () -> dispatcher.fail(thrown == null ? new CancellationException() : thrown));\n+\t\t\t} catch (Exception e) {\n+\t\t\t\t//noinspection NonAtomicOperationOnVolatileField\n+\t\t\t\tthrown = ExceptionUtils.firstOrSuppressed(e, thrown);\n+\t\t\t}", "originalCommit": "73e4094cc98004f8dcc51d4c495fceec3a35f964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3892c6442227362797a73e8cc0fbfb5b11fa7863", "url": "https://github.com/apache/flink/commit/3892c6442227362797a73e8cc0fbfb5b11fa7863", "message": "fixup! [FLINK-17547][task] Use iterator for unconsumed buffers. Motivation: support spilled records Changes: 1. change SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer signature 2. adapt channel state persistence to new types", "committedDate": "2020-05-15T20:27:10Z", "type": "forcePushed"}, {"oid": "4b453c84b4823034f4118e24191339851096097e", "url": "https://github.com/apache/flink/commit/4b453c84b4823034f4118e24191339851096097e", "message": "fixup! [FLINK-17547][task] Use iterator for unconsumed buffers. Motivation: support spilled records Changes: 1. change SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer signature 2. adapt channel state persistence to new types", "committedDate": "2020-05-15T20:32:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMTU5OA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r426121598", "bodyText": "Why are we recycling buffers at the end, on close and not on the fly? It can mean GBs of extra (unnecessary) memory usage if we keep all unconsumed buffers from different channels. The ownership of the buffers returned from the iterator should belong to the caller of next() method.", "author": "pnowojski", "createdAt": "2020-05-16T05:44:49Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/disk/FileBasedBufferIterator.java", "diffHunk": "@@ -85,6 +90,6 @@ private int read(byte[] buffer) {\n \n \t@Override\n \tpublic void close() throws Exception {\n-\t\tcloseAll(stream, file::release);\n+\t\tcloseAll(stream, file::release, () -> buffersToClose.forEach(Buffer::recycleBuffer));", "originalCommit": "4b453c84b4823034f4118e24191339851096097e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEzOTA3OQ==", "url": "https://github.com/apache/flink/pull/12120#discussion_r426139079", "bodyText": "I've changed it to recycle only the last buffer in close.\nI wanted to avoid extra complexity but it turned out not that complicated.\n(the reason why buffers need to be recycled by iterator rather than by client is that for in-memory collections we always want to recycle all the elements, even if iteration wasn't started)", "author": "rkhachatryan", "createdAt": "2020-05-16T10:00:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMTU5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjIyNjQxOQ==", "url": "https://github.com/apache/flink/pull/12120#discussion_r426226419", "bodyText": "I don't understand it how is it working right now. Where are the non last buffers recycled? And why do you have to handle the last buffer differently? As I mentioned before, I think:\n\nThe ownership of the buffers returned from the iterator should belong to the caller of next() method.\n\nso buffers' ownership should be handed over, and owner should be always responsible to recycle it when it's done using it. For in memory collections, all of the remaining buffers should be recycled via closing the iterator (as he would be the owner of not yet returned buffers).\nI guess we are avoiding memory leaks right now, only because the returned Buffers are not pooled and using simple FreeingBufferRecycler? If so, that's not how buffers should be used and can brake if someone else would pass buffers with a different recycler.", "author": "pnowojski", "createdAt": "2020-05-17T07:09:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMTU5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI2MzE1NQ==", "url": "https://github.com/apache/flink/pull/12120#discussion_r426263155", "bodyText": "I don't understand it how is it working right now. Where are the non last buffers recycled?\n\nIn the beginnning of FileBasedBufferIterator.next().\n\nAnd why do you have to handle the last buffer differently?\n\nBecause next() won't be called anymore.\n\nThe ownership of the buffers returned from the iterator should belong to the caller of next() method.\n\nThe client doesn't know whether the iterator is lazy or not. So if failure happens before iteration, it would have to read the whole file just to recycle the buffers.\n\nI guess we are avoiding memory leaks right now, only because the returned Buffers are not pooled and using simple FreeingBufferRecycler?\n\nNo, the buffers are recycled explicitly (see above).", "author": "rkhachatryan", "createdAt": "2020-05-17T13:41:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMTU5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQxMTQwNA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r426411404", "bodyText": "The client doesn't know whether the iterator is lazy or not. So if failure happens before iteration, it would have to read the whole file just to recycle the buffers.\n\nWhy? As I wrote before, the buffers that were not yet returned, should be recycled by it's current owner - in that case CloseableIterator. If the buffers were even not yet created, because it's a lazy iterator, even better - nothing to do in the close().\n\nIn the beginnning of FileBasedBufferIterator.next().\n\nIf buffers were returned from the iterator, iterator is no long it's owner and they shouldn't be recycled by the iterator.", "author": "pnowojski", "createdAt": "2020-05-18T07:11:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMTU5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjYxMjQ1NA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r426612454", "bodyText": "I think you are right, these ownership rules are more clear (though implementation will be a bit more complex and less optimal in collections iterators).\nI'll update the PR.", "author": "rkhachatryan", "createdAt": "2020-05-18T13:08:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMTU5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA4NDcxNA==", "url": "https://github.com/apache/flink/pull/12120#discussion_r427084714", "bodyText": "Done.", "author": "rkhachatryan", "createdAt": "2020-05-19T07:25:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMTU5OA=="}], "type": "inlineReview"}, {"oid": "979c1dc26503c67f39af742c46e6df241e5e5ab7", "url": "https://github.com/apache/flink/commit/979c1dc26503c67f39af742c46e6df241e5e5ab7", "message": "[FLINK-17547][task][hotfix] Improve error handling\n1 catch one more invalid input in DataOutputSerializer.write\n2 more informative error messages", "committedDate": "2020-05-16T16:38:34Z", "type": "commit"}, {"oid": "2472c7572e86b5c85d71d834a39c51da097fab6a", "url": "https://github.com/apache/flink/commit/2472c7572e86b5c85d71d834a39c51da097fab6a", "message": "[FLINK-17547][task][hotfix] Extract NonSpanningWrapper from\nSpillingAdaptiveSpanningRecordDeserializer (static inner class)\nAs it is, no logical changes.", "committedDate": "2020-05-16T16:38:34Z", "type": "commit"}, {"oid": "224c2077305b4930e4ff8776791a16f9fc118782", "url": "https://github.com/apache/flink/commit/224c2077305b4930e4ff8776791a16f9fc118782", "message": "[FLINK-17547][task][hotfix] Extract SpanningWrapper\nfrom SpillingAdaptiveSpanningRecordDeserializer (static inner class).\nAs it is, no logical changes.", "committedDate": "2020-05-16T16:38:34Z", "type": "commit"}, {"oid": "09f64114070ff7783e28d7cc273a31a54b1b7e28", "url": "https://github.com/apache/flink/commit/09f64114070ff7783e28d7cc273a31a54b1b7e28", "message": "[FLINK-17547][task][hotfix] Fix compiler warnings in NonSpanningWrapper", "committedDate": "2020-05-16T16:38:34Z", "type": "commit"}, {"oid": "eb63e8ca140a0d6f4a1fff1d4f8fe09fefc2c91f", "url": "https://github.com/apache/flink/commit/eb63e8ca140a0d6f4a1fff1d4f8fe09fefc2c91f", "message": "[FLINK-17547][task][hotfix] Extract methods from RecordsDeserializer", "committedDate": "2020-05-16T16:38:35Z", "type": "commit"}, {"oid": "95c57fc02f0f4c16e685df54b249f2386177126c", "url": "https://github.com/apache/flink/commit/95c57fc02f0f4c16e685df54b249f2386177126c", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers", "committedDate": "2020-05-16T17:36:12Z", "type": "forcePushed"}, {"oid": "91fd89dff9bdbe4286d64a3c015e891e2199f173", "url": "https://github.com/apache/flink/commit/91fd89dff9bdbe4286d64a3c015e891e2199f173", "message": "[FLINK-17547][task] Use iterator for unconsumed buffers.\nMotivation: support spilled records\nChanges:\n1. change SpillingAdaptiveSpanningRecordDeserializer.getUnconsumedBuffer\nsignature\n2. adapt channel state persistence to new types\n\nNo changes in existing logic.", "committedDate": "2020-05-18T18:29:05Z", "type": "commit"}, {"oid": "a15a76f78250e35a48975dc91748cff0ba13c999", "url": "https://github.com/apache/flink/commit/a15a76f78250e35a48975dc91748cff0ba13c999", "message": "[FLINK-17547][task][hotfix] Extract RefCountedFileWithStream from RefCountedFile\nMotivation: use RefCountedFile for reading as well.", "committedDate": "2020-05-18T18:29:14Z", "type": "commit"}, {"oid": "bca823195e558bf3e85ca39356b7075c0a8ef6ec", "url": "https://github.com/apache/flink/commit/bca823195e558bf3e85ca39356b7075c0a8ef6ec", "message": "[FLINK-17547][task][hotfix] Move RefCountedFile to flink-core\nto use it in SpanningWrapper", "committedDate": "2020-05-18T18:29:14Z", "type": "commit"}, {"oid": "a863cd18d1b42346eb8d826f8596d1755495b809", "url": "https://github.com/apache/flink/commit/a863cd18d1b42346eb8d826f8596d1755495b809", "message": "[FLINK-17547][task] Use RefCountedFile in SpanningWrapper (todo: merge\nwith next?)", "committedDate": "2020-05-18T18:29:14Z", "type": "commit"}, {"oid": "0242c584e370d24eca0329895e81b0a11a4ed7cd", "url": "https://github.com/apache/flink/commit/0242c584e370d24eca0329895e81b0a11a4ed7cd", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers", "committedDate": "2020-05-18T18:30:23Z", "type": "commit"}, {"oid": "0242c584e370d24eca0329895e81b0a11a4ed7cd", "url": "https://github.com/apache/flink/commit/0242c584e370d24eca0329895e81b0a11a4ed7cd", "message": "[FLINK-17547][task] Implement getUnconsumedSegment for spilled buffers", "committedDate": "2020-05-18T18:30:23Z", "type": "forcePushed"}]}