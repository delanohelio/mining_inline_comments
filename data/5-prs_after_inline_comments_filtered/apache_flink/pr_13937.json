{"pr_number": 13937, "pr_title": "[FLINK-19886][hive] Integrate file compaction to Hive connector", "pr_createdAt": "2020-11-05T08:40:47Z", "pr_url": "https://github.com/apache/flink/pull/13937", "timeline": [{"oid": "799ab6a369fcfaa8df1765c68bf77a960abd0977", "url": "https://github.com/apache/flink/commit/799ab6a369fcfaa8df1765c68bf77a960abd0977", "message": "[FLINK-19886][hive] Integrate file compaction to Hive connector", "committedDate": "2020-11-05T08:37:34Z", "type": "commit"}, {"oid": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88", "url": "https://github.com/apache/flink/commit/0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88", "message": "Remove restorePartitionValueFromType in HiveTableSource", "committedDate": "2020-11-05T08:41:03Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY3NjY3Mg==", "url": "https://github.com/apache/flink/pull/13937#discussion_r518676672", "bodyText": "Why do we need a CompactBulkReader? It seems all we need is just a CompactReader.Factory?", "author": "lirui-apache", "createdAt": "2020-11-06T10:59:02Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveCompactReader.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.compact.CompactBulkReader;\n+import org.apache.flink.table.filesystem.stream.compact.CompactContext;\n+import org.apache.flink.table.filesystem.stream.compact.CompactReader;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.connectors.hive.util.HivePartitionUtils.restorePartitionValueFromType;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+\n+/**\n+ * The {@link CompactReader} to delegate hive bulk format.\n+ */\n+public class HiveCompactReader extends CompactBulkReader<RowData> {", "originalCommit": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4MTczNg==", "url": "https://github.com/apache/flink/pull/13937#discussion_r518681736", "bodyText": "Is it possible that partition spec cannot be extracted from path? E.g. when we write to a static partition and the partition location is different from the default one?", "author": "lirui-apache", "createdAt": "2020-11-06T11:07:02Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveCompactReader.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.connectors.hive.JobConfWrapper;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.client.HiveShimLoader;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.stream.compact.CompactBulkReader;\n+import org.apache.flink.table.filesystem.stream.compact.CompactContext;\n+import org.apache.flink.table.filesystem.stream.compact.CompactReader;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.flink.connectors.hive.util.HivePartitionUtils.restorePartitionValueFromType;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionSpecFromPath;\n+\n+/**\n+ * The {@link CompactReader} to delegate hive bulk format.\n+ */\n+public class HiveCompactReader extends CompactBulkReader<RowData> {\n+\n+\tprivate HiveCompactReader(BulkFormat.Reader<RowData> reader) throws IOException {\n+\t\tsuper(reader);\n+\t}\n+\n+\tpublic static CompactReader.Factory<RowData> factory(\n+\t\t\tStorageDescriptor sd,\n+\t\t\tProperties properties,\n+\t\t\tJobConf jobConf,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tString hiveVersion,\n+\t\t\tRowType producedRowType,\n+\t\t\tboolean useMapRedReader) {\n+\t\treturn new Factory(\n+\t\t\t\tsd,\n+\t\t\t\tproperties,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tcatalogTable.getSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getSchema().getFieldDataTypes(),\n+\t\t\t\thiveVersion,\n+\t\t\t\tproducedRowType,\n+\t\t\t\tuseMapRedReader);\n+\t}\n+\n+\t/**\n+\t * Factory to create {@link HiveCompactReader}.\n+\t */\n+\tprivate static class Factory implements CompactReader.Factory<RowData> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final StorageDescriptor sd;\n+\t\tprivate final Properties properties;\n+\t\tprivate final JobConfWrapper jobConfWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String hiveVersion;\n+\t\tprivate final HiveShim shim;\n+\t\tprivate final RowType producedRowType;\n+\t\tprivate final boolean useMapRedReader;\n+\n+\t\tprivate Factory(\n+\t\t\t\tStorageDescriptor sd,\n+\t\t\t\tProperties properties,\n+\t\t\t\tJobConfWrapper jobConfWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString hiveVersion,\n+\t\t\t\tRowType producedRowType,\n+\t\t\t\tboolean useMapRedReader) {\n+\t\t\tthis.sd = sd;\n+\t\t\tthis.properties = properties;\n+\t\t\tthis.jobConfWrapper = jobConfWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.hiveVersion = hiveVersion;\n+\t\t\tthis.shim = HiveShimLoader.loadHiveShim(hiveVersion);\n+\t\t\tthis.producedRowType = producedRowType;\n+\t\t\tthis.useMapRedReader = useMapRedReader;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompactReader<RowData> create(CompactContext context) throws IOException {\n+\t\t\tHiveSourceSplit split = createSplit(context.getPath(), context.getFileSystem());\n+\t\t\tHiveBulkFormatAdapter format = new HiveBulkFormatAdapter(\n+\t\t\t\t\tjobConfWrapper, partitionKeys, fieldNames, fieldTypes, hiveVersion, producedRowType, useMapRedReader);\n+\t\t\tBulkFormat.Reader<RowData> reader = format.createReader(context.getConfig(), split);\n+\t\t\treturn new HiveCompactReader(reader);\n+\t\t}\n+\n+\t\tprivate HiveSourceSplit createSplit(Path path, FileSystem fs) throws IOException {\n+\t\t\tlong len = fs.getFileStatus(path).getLen();\n+\t\t\treturn new HiveSourceSplit(\"id\", path, 0, len, new String[0], null, createPartition(path));\n+\t\t}\n+\n+\t\tprivate HiveTablePartition createPartition(Path path) {\n+\t\t\tMap<String, Object> partitionSpec = new LinkedHashMap<>();\n+\t\t\tMap<String, DataType> nameToTypes = new HashMap<>();\n+\t\t\tfor (int i = 0; i < fieldNames.length; i++) {\n+\t\t\t\tnameToTypes.put(fieldNames[i], fieldTypes[i]);\n+\t\t\t}\n+\t\t\tfor (Map.Entry<String, String> entry : extractPartitionSpecFromPath(path).entrySet()) {", "originalCommit": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc5NDMzNA==", "url": "https://github.com/apache/flink/pull/13937#discussion_r518794334", "bodyText": "We don't support this, now the path must be come from table.getSd(), and the rule must be partitioned path.", "author": "JingsongLi", "createdAt": "2020-11-06T14:42:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4MTczNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA4Njc3NQ==", "url": "https://github.com/apache/flink/pull/13937#discussion_r519086775", "bodyText": "Let's then log a jira to track this. Although not popular, it's still a valid use case in hive.", "author": "lirui-apache", "createdAt": "2020-11-07T03:00:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4MTczNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA5NzM4NA==", "url": "https://github.com/apache/flink/pull/13937#discussion_r519097384", "bodyText": "https://issues.apache.org/jira/browse/FLINK-20040", "author": "JingsongLi", "createdAt": "2020-11-07T04:19:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY4MTczNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY5OTQwOA==", "url": "https://github.com/apache/flink/pull/13937#discussion_r518699408", "bodyText": "I think only HiveTableSource performs parallelism inference. But this test won't use HiveTableSource, right?", "author": "lirui-apache", "createdAt": "2020-11-06T11:43:50Z", "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveSinkCompactionITCase.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.table.api.SqlDialect;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.catalog.hive.HiveCatalog;\n+import org.apache.flink.table.catalog.hive.HiveTestUtils;\n+import org.apache.flink.table.planner.runtime.stream.sql.CompactionITCaseBase;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+\n+/**\n+ * Test sink file compaction of hive tables.\n+ */\n+@RunWith(Parameterized.class)\n+public class HiveSinkCompactionITCase extends CompactionITCaseBase {\n+\n+\t@Parameterized.Parameters(name = \"format = {0}\")\n+\tpublic static Collection<String> parameters() {\n+\t\treturn Arrays.asList(\"sequencefile\", \"parquet\");\n+\t}\n+\n+\t@Parameterized.Parameter\n+\tpublic String format;\n+\n+\tprivate HiveCatalog hiveCatalog;\n+\n+\t@Override\n+\t@Before\n+\tpublic void init() throws IOException {\n+\t\thiveCatalog = HiveTestUtils.createHiveCatalog();\n+\t\ttEnv().registerCatalog(hiveCatalog.getName(), hiveCatalog);\n+\t\ttEnv().useCatalog(hiveCatalog.getName());\n+\n+\t\t// avoid too large parallelism lead to scheduler dead lock in streaming mode\n+\t\ttEnv().getConfig().getConfiguration().set(\n+\t\t\t\tHiveOptions.TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM, false);", "originalCommit": "0d6b4e7fbd9948ee6c4db339bc49b93a823c3b88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc5NTAwNw==", "url": "https://github.com/apache/flink/pull/13937#discussion_r518795007", "bodyText": "We are using. There is select * from sink_table", "author": "JingsongLi", "createdAt": "2020-11-06T14:43:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY5OTQwOA=="}], "type": "inlineReview"}, {"oid": "ac28db597b7f045cd6e749baf6ea3ae86d2ad2a2", "url": "https://github.com/apache/flink/commit/ac28db597b7f045cd6e749baf6ea3ae86d2ad2a2", "message": "Address comments", "committedDate": "2020-11-06T14:43:23Z", "type": "commit"}]}