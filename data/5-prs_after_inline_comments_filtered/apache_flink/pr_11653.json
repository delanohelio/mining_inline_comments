{"pr_number": 11653, "pr_title": "[FLINK-16951][parquet] Integrate parquet to file system connector", "pr_createdAt": "2020-04-07T07:25:41Z", "pr_url": "https://github.com/apache/flink/pull/11653", "timeline": [{"oid": "b47050ed0f6a8474be297eabf078ef186e00ea4d", "url": "https://github.com/apache/flink/commit/b47050ed0f6a8474be297eabf078ef186e00ea4d", "message": "[FLINK-16951][parquet] Integrate parquet to file system connector", "committedDate": "2020-04-07T07:19:18Z", "type": "commit"}, {"oid": "24b9af733c519ceca287e189b2b290d3d6a0baf4", "url": "https://github.com/apache/flink/commit/24b9af733c519ceca287e189b2b290d3d6a0baf4", "message": "comments", "committedDate": "2020-04-08T08:27:43Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzM0MjExMQ==", "url": "https://github.com/apache/flink/pull/11653#discussion_r407342111", "bodyText": "use PARQUET_PROPERTIES_KEY here\uff1f", "author": "leonardBang", "createdAt": "2020-04-13T06:39:56Z", "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.parquet;\n+\n+import org.apache.flink.api.common.io.FileInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.parquet.row.ParquetRowDataBuilder;\n+import org.apache.flink.formats.parquet.utils.SerializableConfiguration;\n+import org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReader;\n+import org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.filesystem.FileSystemFormatFactory;\n+import org.apache.flink.table.filesystem.PartitionPathUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.parquet.hadoop.ParquetOutputFormat;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.dataformat.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.filesystem.RowPartitionComputer.restorePartValueFromType;\n+\n+/**\n+ * Parquet {@link FileSystemFormatFactory} for file system.\n+ */\n+public class ParquetFileSystemFormatFactory implements FileSystemFormatFactory {\n+\n+\tpublic static final ConfigOption<Boolean> UTC_TIMEZONE = key(\"format.utc-timezone\")\n+\t\t\t.booleanType()\n+\t\t\t.defaultValue(false)\n+\t\t\t.withDescription(\"Use UTC timezone or local timezone to the conversion between epoch\" +\n+\t\t\t\t\t\" time and LocalDateTime. Hive 0.x/1.x/2.x use local timezone. But Hive 3.x\" +\n+\t\t\t\t\t\" use UTC timezone\");\n+\n+\t/**\n+\t * Parquet properties, start with \"parquet\", see more in {@link ParquetOutputFormat}.\n+\t * - parquet.compression\n+\t * - parquet.block.size\n+\t * - parquet.page.size\n+\t * - parquet.dictionary.page.size\n+\t * - parquet.writer.max-padding\n+\t * - parquet.enable.dictionary\n+\t * - parquet.validation\n+\t * - parquet.writer.version\n+\t * ...\n+\t */\n+\tpublic static final String PARQUET_PROPERTIES = \"format.parquet\";", "originalCommit": "24b9af733c519ceca287e189b2b290d3d6a0baf4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzM3NjAyNA==", "url": "https://github.com/apache/flink/pull/11653#discussion_r407376024", "bodyText": "Just like FormatDescriptorValidator.FORMAT, I will add comments to say this is for prefix.", "author": "JingsongLi", "createdAt": "2020-04-13T08:28:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzM0MjExMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzM0MjM3MQ==", "url": "https://github.com/apache/flink/pull/11653#discussion_r407342371", "bodyText": "Call parent's function first in constructor?", "author": "leonardBang", "createdAt": "2020-04-13T06:40:56Z", "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.parquet;\n+\n+import org.apache.flink.api.common.io.FileInputFormat;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.parquet.row.ParquetRowDataBuilder;\n+import org.apache.flink.formats.parquet.utils.SerializableConfiguration;\n+import org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReader;\n+import org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.filesystem.FileSystemFormatFactory;\n+import org.apache.flink.table.filesystem.PartitionPathUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.parquet.hadoop.ParquetOutputFormat;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.dataformat.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.filesystem.RowPartitionComputer.restorePartValueFromType;\n+\n+/**\n+ * Parquet {@link FileSystemFormatFactory} for file system.\n+ */\n+public class ParquetFileSystemFormatFactory implements FileSystemFormatFactory {\n+\n+\tpublic static final ConfigOption<Boolean> UTC_TIMEZONE = key(\"format.utc-timezone\")\n+\t\t\t.booleanType()\n+\t\t\t.defaultValue(false)\n+\t\t\t.withDescription(\"Use UTC timezone or local timezone to the conversion between epoch\" +\n+\t\t\t\t\t\" time and LocalDateTime. Hive 0.x/1.x/2.x use local timezone. But Hive 3.x\" +\n+\t\t\t\t\t\" use UTC timezone\");\n+\n+\t/**\n+\t * Parquet properties, start with \"parquet\", see more in {@link ParquetOutputFormat}.\n+\t * - parquet.compression\n+\t * - parquet.block.size\n+\t * - parquet.page.size\n+\t * - parquet.dictionary.page.size\n+\t * - parquet.writer.max-padding\n+\t * - parquet.enable.dictionary\n+\t * - parquet.validation\n+\t * - parquet.writer.version\n+\t * ...\n+\t */\n+\tpublic static final String PARQUET_PROPERTIES = \"format.parquet\";\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"parquet\");\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\treturn Arrays.asList(\n+\t\t\t\tUTC_TIMEZONE.key(),\n+\t\t\t\tPARQUET_PROPERTIES + \".*\"\n+\t\t);\n+\t}\n+\n+\tprivate static boolean isUtcTimestamp(DescriptorProperties properties) {\n+\t\treturn properties.getOptionalBoolean(UTC_TIMEZONE.key())\n+\t\t\t\t.orElse(UTC_TIMEZONE.defaultValue());\n+\t}\n+\n+\tprivate static Configuration getParquetConfiguration(DescriptorProperties properties) {\n+\t\tConfiguration conf = new Configuration();\n+\t\tproperties.asMap().keySet()\n+\t\t\t\t.stream()\n+\t\t\t\t.filter(key -> key.startsWith(PARQUET_PROPERTIES))\n+\t\t\t\t.forEach(key -> {\n+\t\t\t\t\tString value = properties.getString(key);\n+\t\t\t\t\tString subKey = key.substring((FORMAT + '.').length());\n+\t\t\t\t\tconf.set(subKey, value);\n+\t\t\t\t});\n+\t\treturn conf;\n+\t}\n+\n+\t@Override\n+\tpublic InputFormat<BaseRow, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getFormatProperties());\n+\n+\t\treturn new ParquetInputFormat(\n+\t\t\t\tcontext.getPaths(),\n+\t\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\t\tcontext.getProjectFields(),\n+\t\t\t\tcontext.getDefaultPartName(),\n+\t\t\t\tcontext.getPushedDownLimit(),\n+\t\t\t\tgetParquetConfiguration(properties),\n+\t\t\t\tisUtcTimestamp(properties));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<BaseRow>> createBulkWriterFactory(WriterContext context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getFormatProperties());\n+\n+\t\treturn Optional.of(ParquetRowDataBuilder.createWriterFactory(\n+\t\t\t\tRowType.of(Arrays.stream(context.getFieldTypesWithoutPartKeys())\n+\t\t\t\t\t\t\t\t.map(DataType::getLogicalType)\n+\t\t\t\t\t\t\t\t.toArray(LogicalType[]::new),\n+\t\t\t\t\t\tcontext.getFieldNamesWithoutPartKeys()),\n+\t\t\t\tgetParquetConfiguration(properties),\n+\t\t\t\tisUtcTimestamp(properties)\n+\t\t));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<BaseRow>> createEncoder(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * An implementation of {@link ParquetInputFormat} to read {@link BaseRow} records\n+\t * from Parquet files.\n+\t */\n+\tpublic static class ParquetInputFormat extends FileInputFormat<BaseRow> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final String[] fullFieldNames;\n+\t\tprivate final DataType[] fullFieldTypes;\n+\t\tprivate final int[] selectedFields;\n+\t\tprivate final String partDefaultName;\n+\t\tprivate final boolean utcTimestamp;\n+\t\tprivate final SerializableConfiguration conf;\n+\t\tprivate final long limit;\n+\n+\t\tprivate transient ParquetColumnarRowSplitReader reader;\n+\t\tprivate transient long currentReadCount;\n+\n+\t\tpublic ParquetInputFormat(\n+\t\t\t\tPath[] paths,\n+\t\t\t\tString[] fullFieldNames,\n+\t\t\t\tDataType[] fullFieldTypes,\n+\t\t\t\tint[] selectedFields,\n+\t\t\t\tString partDefaultName,\n+\t\t\t\tlong limit,\n+\t\t\t\tConfiguration conf,\n+\t\t\t\tboolean utcTimestamp) {\n+\t\t\tthis.limit = limit;\n+\t\t\tsuper.setFilePaths(paths);", "originalCommit": "24b9af733c519ceca287e189b2b290d3d6a0baf4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "46f0d9c95d82b8f9dcd5e344f3cb98c2cb9995a5", "url": "https://github.com/apache/flink/commit/46f0d9c95d82b8f9dcd5e344f3cb98c2cb9995a5", "message": "Fix comments", "committedDate": "2020-04-13T08:31:07Z", "type": "commit"}]}