{"pr_number": 13479, "pr_title": "[FLINK-19414][parquet] Introduce ParquetColumnarRowInputFormat", "pr_createdAt": "2020-09-25T09:17:24Z", "pr_url": "https://github.com/apache/flink/pull/13479", "timeline": [{"oid": "be67383a2c22ee4fb6a5330510b4549dd029af51", "url": "https://github.com/apache/flink/commit/be67383a2c22ee4fb6a5330510b4549dd029af51", "message": "[FLINK-19414][parquet] Introduce ParquetColumnarRowInputFormat", "committedDate": "2020-10-10T03:10:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NzU4NA==", "url": "https://github.com/apache/flink/pull/13479#discussion_r502767584", "bodyText": "-1 can be used to represent NO_OFFSET, will that cause a problem here?", "author": "lirui-apache", "createdAt": "2020-10-10T09:07:17Z", "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetVectorizedInputFormat.java", "diffHunk": "@@ -0,0 +1,471 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.parquet;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.base.source.reader.SourceReaderOptions;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.CheckpointedPosition;\n+import org.apache.flink.connector.file.src.util.Pool;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.parquet.utils.SerializableConfiguration;\n+import org.apache.flink.formats.parquet.vector.ColumnBatchFactory;\n+import org.apache.flink.formats.parquet.vector.ParquetDecimalVector;\n+import org.apache.flink.formats.parquet.vector.reader.AbstractColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.ColumnReader;\n+import org.apache.flink.table.data.vector.ColumnVector;\n+import org.apache.flink.table.data.vector.VectorizedColumnBatch;\n+import org.apache.flink.table.data.vector.writable.WritableColumnVector;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+import org.apache.flink.util.FlinkRuntimeException;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.filter2.compat.FilterCompat;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.apache.parquet.schema.Types;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+import static org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createColumnReader;\n+import static org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createWritableColumnVector;\n+import static org.apache.parquet.filter2.compat.RowGroupFilter.filterRowGroups;\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.range;\n+import static org.apache.parquet.hadoop.ParquetFileReader.readFooter;\n+import static org.apache.parquet.hadoop.ParquetInputFormat.getFilter;\n+\n+/**\n+ * Parquet {@link BulkFormat} that reads data from the file to {@link VectorizedColumnBatch} in\n+ * vectorized mode.\n+ */\n+public abstract class ParquetVectorizedInputFormat<T> implements BulkFormat<T> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final SerializableConfiguration hadoopConfig;\n+\tprivate final String[] projectedFields;\n+\tprivate final LogicalType[] projectedTypes;\n+\tprivate final ColumnBatchFactory batchFactory;\n+\tprivate final int batchSize;\n+\tprivate final boolean isUtcTimestamp;\n+\tprivate final boolean isCaseSensitive;\n+\n+\tpublic ParquetVectorizedInputFormat(\n+\t\t\tSerializableConfiguration hadoopConfig,\n+\t\t\tString[] projectedFields,\n+\t\t\tLogicalType[] projectedTypes,\n+\t\t\tColumnBatchFactory batchFactory,\n+\t\t\tint batchSize,\n+\t\t\tboolean isUtcTimestamp,\n+\t\t\tboolean isCaseSensitive) {\n+\t\tPreconditions.checkArgument(\n+\t\t\t\tprojectedFields.length == projectedTypes.length,\n+\t\t\t\t\"The length(%s) of projectedFields should equal to the length(%s) projectedTypes\",\n+\t\t\t\tprojectedFields.length,\n+\t\t\t\tprojectedTypes.length);\n+\n+\t\tthis.hadoopConfig = hadoopConfig;\n+\t\tthis.projectedFields = projectedFields;\n+\t\tthis.projectedTypes = projectedTypes;\n+\t\tthis.batchFactory = batchFactory;\n+\t\tthis.batchSize = batchSize;\n+\t\tthis.isUtcTimestamp = isUtcTimestamp;\n+\t\tthis.isCaseSensitive = isCaseSensitive;\n+\t}\n+\n+\t@Override\n+\tpublic ParquetReader createReader(\n+\t\t\tConfiguration config,\n+\t\t\tPath filePath,\n+\t\t\tlong splitOffset,\n+\t\t\tlong splitLength) throws IOException {\n+\t\torg.apache.hadoop.fs.Path hadoopPath = new org.apache.hadoop.fs.Path(filePath.toUri());\n+\t\tParquetMetadata footer = readFooter(\n+\t\t\t\thadoopConfig.conf(), hadoopPath, range(splitOffset, splitOffset + splitLength));\n+\t\tMessageType fileSchema = footer.getFileMetaData().getSchema();\n+\t\tFilterCompat.Filter filter = getFilter(hadoopConfig.conf());\n+\t\tList<BlockMetaData> blocks = filterRowGroups(filter, footer.getBlocks(), fileSchema);\n+\n+\t\tMessageType requestedSchema = clipParquetSchema(fileSchema);\n+\t\tParquetFileReader reader = new ParquetFileReader(\n+\t\t\t\thadoopConfig.conf(),\n+\t\t\t\tfooter.getFileMetaData(),\n+\t\t\t\thadoopPath,\n+\t\t\t\tblocks,\n+\t\t\t\trequestedSchema.getColumns());\n+\n+\t\tlong totalRowCount = 0;\n+\t\tfor (BlockMetaData block : blocks) {\n+\t\t\ttotalRowCount += block.getRowCount();\n+\t\t}\n+\n+\t\tcheckSchema(fileSchema, requestedSchema);\n+\n+\t\tfinal int numBatchesToCirculate = config.getInteger(SourceReaderOptions.ELEMENT_QUEUE_CAPACITY) + 1;\n+\t\tfinal Pool<ParquetReaderBatch<T>> poolOfBatches =\n+\t\t\t\tcreatePoolOfBatches(filePath, requestedSchema, numBatchesToCirculate);\n+\n+\t\treturn new ParquetReader(reader, requestedSchema, totalRowCount, poolOfBatches);\n+\t}\n+\n+\t@Override\n+\tpublic ParquetReader restoreReader(\n+\t\t\tConfiguration config,\n+\t\t\tPath filePath,\n+\t\t\tlong splitOffset,\n+\t\t\tlong splitLength,\n+\t\t\tCheckpointedPosition checkpointedPosition) throws IOException {\n+\t\tParquetReader reader = createReader(config, filePath, splitOffset, splitLength);\n+\t\t// Offset is record count too.\n+\t\treader.seek(checkpointedPosition.getOffset() + checkpointedPosition.getRecordsAfterOffset());", "originalCommit": "6fc43dcfa07a2f1488f33d54d26ed8c4bdeff2ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzNDYyOA==", "url": "https://github.com/apache/flink/pull/13479#discussion_r503034628", "bodyText": "Good point", "author": "JingsongLi", "createdAt": "2020-10-12T04:38:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NzU4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA1NDY2NA==", "url": "https://github.com/apache/flink/pull/13479#discussion_r503054664", "bodyText": "I think we should only use recordsAfterOffset. Offset is useless. See comments in CheckpointedPosition.", "author": "JingsongLi", "createdAt": "2020-10-12T05:59:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NzU4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA2MDMyMw==", "url": "https://github.com/apache/flink/pull/13479#discussion_r503060323", "bodyText": "I wasn't using offset for my Hive source PoC. Honestly, I don't quite understand the difference between offset and recordsAfterOffset. Could you elaborate on these and how should they be used?", "author": "lirui-apache", "createdAt": "2020-10-12T06:18:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NzU4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA2NjExNA==", "url": "https://github.com/apache/flink/pull/13479#discussion_r503066114", "bodyText": "I think make sense to use offset as a binary offset. Otherwise, it looks strange.", "author": "JingsongLi", "createdAt": "2020-10-12T06:33:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NzU4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2OTcxOA==", "url": "https://github.com/apache/flink/pull/13479#discussion_r502769718", "bodyText": "Seems this check is unnecessary.", "author": "lirui-apache", "createdAt": "2020-10-10T09:29:54Z", "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetVectorizedInputFormat.java", "diffHunk": "@@ -0,0 +1,471 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.parquet;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.base.source.reader.SourceReaderOptions;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.connector.file.src.util.CheckpointedPosition;\n+import org.apache.flink.connector.file.src.util.Pool;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.parquet.utils.SerializableConfiguration;\n+import org.apache.flink.formats.parquet.vector.ColumnBatchFactory;\n+import org.apache.flink.formats.parquet.vector.ParquetDecimalVector;\n+import org.apache.flink.formats.parquet.vector.reader.AbstractColumnReader;\n+import org.apache.flink.formats.parquet.vector.reader.ColumnReader;\n+import org.apache.flink.table.data.vector.ColumnVector;\n+import org.apache.flink.table.data.vector.VectorizedColumnBatch;\n+import org.apache.flink.table.data.vector.writable.WritableColumnVector;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+import org.apache.flink.util.FlinkRuntimeException;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.filter2.compat.FilterCompat;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.apache.parquet.schema.Types;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+import static org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createColumnReader;\n+import static org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createWritableColumnVector;\n+import static org.apache.parquet.filter2.compat.RowGroupFilter.filterRowGroups;\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.range;\n+import static org.apache.parquet.hadoop.ParquetFileReader.readFooter;\n+import static org.apache.parquet.hadoop.ParquetInputFormat.getFilter;\n+\n+/**\n+ * Parquet {@link BulkFormat} that reads data from the file to {@link VectorizedColumnBatch} in\n+ * vectorized mode.\n+ */\n+public abstract class ParquetVectorizedInputFormat<T> implements BulkFormat<T> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final SerializableConfiguration hadoopConfig;\n+\tprivate final String[] projectedFields;\n+\tprivate final LogicalType[] projectedTypes;\n+\tprivate final ColumnBatchFactory batchFactory;\n+\tprivate final int batchSize;\n+\tprivate final boolean isUtcTimestamp;\n+\tprivate final boolean isCaseSensitive;\n+\n+\tpublic ParquetVectorizedInputFormat(\n+\t\t\tSerializableConfiguration hadoopConfig,\n+\t\t\tString[] projectedFields,\n+\t\t\tLogicalType[] projectedTypes,\n+\t\t\tColumnBatchFactory batchFactory,\n+\t\t\tint batchSize,\n+\t\t\tboolean isUtcTimestamp,\n+\t\t\tboolean isCaseSensitive) {\n+\t\tPreconditions.checkArgument(\n+\t\t\t\tprojectedFields.length == projectedTypes.length,\n+\t\t\t\t\"The length(%s) of projectedFields should equal to the length(%s) projectedTypes\",\n+\t\t\t\tprojectedFields.length,\n+\t\t\t\tprojectedTypes.length);\n+\n+\t\tthis.hadoopConfig = hadoopConfig;\n+\t\tthis.projectedFields = projectedFields;\n+\t\tthis.projectedTypes = projectedTypes;\n+\t\tthis.batchFactory = batchFactory;\n+\t\tthis.batchSize = batchSize;\n+\t\tthis.isUtcTimestamp = isUtcTimestamp;\n+\t\tthis.isCaseSensitive = isCaseSensitive;\n+\t}\n+\n+\t@Override\n+\tpublic ParquetReader createReader(\n+\t\t\tConfiguration config,\n+\t\t\tPath filePath,\n+\t\t\tlong splitOffset,\n+\t\t\tlong splitLength) throws IOException {\n+\t\torg.apache.hadoop.fs.Path hadoopPath = new org.apache.hadoop.fs.Path(filePath.toUri());\n+\t\tParquetMetadata footer = readFooter(\n+\t\t\t\thadoopConfig.conf(), hadoopPath, range(splitOffset, splitOffset + splitLength));\n+\t\tMessageType fileSchema = footer.getFileMetaData().getSchema();\n+\t\tFilterCompat.Filter filter = getFilter(hadoopConfig.conf());\n+\t\tList<BlockMetaData> blocks = filterRowGroups(filter, footer.getBlocks(), fileSchema);\n+\n+\t\tMessageType requestedSchema = clipParquetSchema(fileSchema);\n+\t\tParquetFileReader reader = new ParquetFileReader(\n+\t\t\t\thadoopConfig.conf(),\n+\t\t\t\tfooter.getFileMetaData(),\n+\t\t\t\thadoopPath,\n+\t\t\t\tblocks,\n+\t\t\t\trequestedSchema.getColumns());\n+\n+\t\tlong totalRowCount = 0;\n+\t\tfor (BlockMetaData block : blocks) {\n+\t\t\ttotalRowCount += block.getRowCount();\n+\t\t}\n+\n+\t\tcheckSchema(fileSchema, requestedSchema);\n+\n+\t\tfinal int numBatchesToCirculate = config.getInteger(SourceReaderOptions.ELEMENT_QUEUE_CAPACITY) + 1;\n+\t\tfinal Pool<ParquetReaderBatch<T>> poolOfBatches =\n+\t\t\t\tcreatePoolOfBatches(filePath, requestedSchema, numBatchesToCirculate);\n+\n+\t\treturn new ParquetReader(reader, requestedSchema, totalRowCount, poolOfBatches);\n+\t}\n+\n+\t@Override\n+\tpublic ParquetReader restoreReader(\n+\t\t\tConfiguration config,\n+\t\t\tPath filePath,\n+\t\t\tlong splitOffset,\n+\t\t\tlong splitLength,\n+\t\t\tCheckpointedPosition checkpointedPosition) throws IOException {\n+\t\tParquetReader reader = createReader(config, filePath, splitOffset, splitLength);\n+\t\t// Offset is record count too.\n+\t\treader.seek(checkpointedPosition.getOffset() + checkpointedPosition.getRecordsAfterOffset());\n+\t\treturn reader;\n+\t}\n+\n+\t@Override\n+\tpublic boolean isSplittable() {\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * Clips `parquetSchema` according to `fieldNames`.\n+\t */\n+\tprivate MessageType clipParquetSchema(GroupType parquetSchema) {\n+\t\tType[] types = new Type[projectedFields.length];\n+\t\tif (isCaseSensitive) {\n+\t\t\tfor (int i = 0; i < projectedFields.length; ++i) {\n+\t\t\t\tString fieldName = projectedFields[i];\n+\t\t\t\tif (parquetSchema.getFieldIndex(fieldName) < 0) {\n+\t\t\t\t\tthrow new IllegalArgumentException(fieldName + \" does not exist\");\n+\t\t\t\t}\n+\t\t\t\ttypes[i] = parquetSchema.getType(fieldName);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tMap<String, Type> caseInsensitiveFieldMap = new HashMap<>();\n+\t\t\tfor (Type type : parquetSchema.getFields()) {\n+\t\t\t\tcaseInsensitiveFieldMap.compute(type.getName().toLowerCase(Locale.ROOT),\n+\t\t\t\t\t\t(key, previousType) -> {\n+\t\t\t\t\t\t\tif (previousType != null) {\n+\t\t\t\t\t\t\t\tthrow new FlinkRuntimeException(\n+\t\t\t\t\t\t\t\t\t\t\"Parquet with case insensitive mode should have no duplicate key: \" + key);\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\treturn type;\n+\t\t\t\t\t\t});\n+\t\t\t}\n+\t\t\tfor (int i = 0; i < projectedFields.length; ++i) {\n+\t\t\t\tType type = caseInsensitiveFieldMap.get(projectedFields[i].toLowerCase(Locale.ROOT));\n+\t\t\t\tif (type == null) {\n+\t\t\t\t\tthrow new IllegalArgumentException(projectedFields[i] + \" does not exist\");\n+\t\t\t\t}\n+\t\t\t\t// TODO clip for array,map,row types.\n+\t\t\t\ttypes[i] = type;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn Types.buildMessage().addFields(types).named(\"flink-parquet\");\n+\t}\n+\n+\tprivate void checkSchema(\n+\t\t\tMessageType fileSchema,\n+\t\t\tMessageType requestedSchema) throws IOException, UnsupportedOperationException {\n+\t\tif (projectedFields.length != requestedSchema.getFieldCount()) {\n+\t\t\tthrow new RuntimeException(\"The quality of field type is incompatible with the request schema!\");\n+\t\t}\n+\n+\t\t/*\n+\t\t * Check that the requested schema is supported.\n+\t\t */\n+\t\tfor (int i = 0; i < requestedSchema.getFieldCount(); ++i) {\n+\t\t\tType t = requestedSchema.getFields().get(i);\n+\t\t\tif (!t.isPrimitive() || t.isRepetition(Type.Repetition.REPEATED)) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Complex types not supported.\");\n+\t\t\t}\n+\n+\t\t\tString[] colPath = requestedSchema.getPaths().get(i);\n+\t\t\tif (fileSchema.containsPath(colPath)) {\n+\t\t\t\tColumnDescriptor fd = fileSchema.getColumnDescription(colPath);\n+\t\t\t\tif (!fd.equals(requestedSchema.getColumns().get(i))) {\n+\t\t\t\t\tthrow new UnsupportedOperationException(\"Schema evolution not supported.\");\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tif (requestedSchema.getColumns().get(i).getMaxDefinitionLevel() == 0) {\n+\t\t\t\t\t// Column is missing in data but the required data is non-nullable. This file is invalid.\n+\t\t\t\t\tthrow new IOException(\"Required column is missing in data file. Col: \" + Arrays.toString(colPath));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Pool<ParquetReaderBatch<T>> createPoolOfBatches(\n+\t\t\tPath filePath, MessageType requestedSchema, int numBatches) {\n+\t\tfinal Pool<ParquetReaderBatch<T>> pool = new Pool<>(numBatches);\n+\n+\t\tfor (int i = 0; i < numBatches; i++) {\n+\t\t\tpool.add(createReaderBatch(filePath, requestedSchema, pool.recycler()));\n+\t\t}\n+\n+\t\treturn pool;\n+\t}\n+\n+\tprivate ParquetReaderBatch<T> createReaderBatch(\n+\t\t\tPath filePath,\n+\t\t\tMessageType requestedSchema,\n+\t\t\tPool.Recycler<ParquetReaderBatch<T>> recycler) {\n+\t\tWritableColumnVector[] writableVectors = createWritableVectors(requestedSchema);\n+\t\tVectorizedColumnBatch columnarBatch =\n+\t\t\t\tbatchFactory.create(filePath, createReadableVectors(writableVectors));\n+\t\treturn createReaderBatch(writableVectors, columnarBatch, recycler);\n+\t}\n+\n+\tprivate WritableColumnVector[] createWritableVectors(MessageType requestedSchema) {\n+\t\tWritableColumnVector[] columns = new WritableColumnVector[projectedTypes.length];\n+\t\tfor (int i = 0; i < projectedTypes.length; i++) {\n+\t\t\tcolumns[i] = createWritableColumnVector(\n+\t\t\t\t\tbatchSize,\n+\t\t\t\t\tprojectedTypes[i],\n+\t\t\t\t\trequestedSchema.getColumns().get(i).getPrimitiveType());\n+\t\t}\n+\t\treturn columns;\n+\t}\n+\n+\t/**\n+\t * Create readable vectors from writable vectors.\n+\t * Especially for decimal, see {@link ParquetDecimalVector}.\n+\t */\n+\tprivate ColumnVector[] createReadableVectors(WritableColumnVector[] writableVectors) {\n+\t\tColumnVector[] vectors = new ColumnVector[writableVectors.length];\n+\t\tfor (int i = 0; i < writableVectors.length; i++) {\n+\t\t\tvectors[i] = projectedTypes[i].getTypeRoot() == LogicalTypeRoot.DECIMAL ?\n+\t\t\t\t\tnew ParquetDecimalVector(writableVectors[i]) :\n+\t\t\t\t\twritableVectors[i];\n+\t\t}\n+\t\treturn vectors;\n+\t}\n+\n+\tprivate class ParquetReader implements BulkFormat.Reader<T> {\n+\n+\t\tprivate ParquetFileReader reader;\n+\n+\t\tprivate final MessageType requestedSchema;\n+\n+\t\t/**\n+\t\t * The total number of rows this RecordReader will eventually read. The sum of the rows of all\n+\t\t * the row groups.\n+\t\t */\n+\t\tprivate final long totalRowCount;\n+\n+\t\tprivate final Pool<ParquetReaderBatch<T>> pool;\n+\n+\t\t/**\n+\t\t * The number of rows that have been returned.\n+\t\t */\n+\t\tprivate long rowsReturned;\n+\n+\t\t/**\n+\t\t * The number of rows that have been reading, including the current in flight row group.\n+\t\t */\n+\t\tprivate long totalCountLoadedSoFar;\n+\n+\t\t/**\n+\t\t * For each request column, the reader to read this column. This is NULL if this column is\n+\t\t * missing from the file, in which case we populate the attribute with NULL.\n+\t\t */\n+\t\t@SuppressWarnings(\"rawtypes\")\n+\t\tprivate ColumnReader[] columnReaders;\n+\n+\t\tprivate long recordsToSkip;\n+\n+\t\tprivate ParquetReader(\n+\t\t\t\tParquetFileReader reader,\n+\t\t\t\tMessageType requestedSchema,\n+\t\t\t\tlong totalRowCount,\n+\t\t\t\tPool<ParquetReaderBatch<T>> pool) {\n+\t\t\tthis.reader = reader;\n+\t\t\tthis.requestedSchema = requestedSchema;\n+\t\t\tthis.totalRowCount = totalRowCount;\n+\t\t\tthis.pool = pool;\n+\t\t\tthis.rowsReturned = 0;\n+\t\t\tthis.totalCountLoadedSoFar = 0;\n+\t\t\tthis.recordsToSkip = 0;\n+\t\t}\n+\n+\t\t@Nullable\n+\t\t@Override\n+\t\tpublic RecordIterator<T> readBatch() throws IOException {\n+\t\t\tfinal ParquetReaderBatch<T> batch = getCachedEntry();\n+\n+\t\t\tfinal long startingRowNum = rowsReturned;\n+\t\t\tif (!nextBatch(batch)) {\n+\t\t\t\tbatch.recycle();\n+\t\t\t\treturn null;\n+\t\t\t}\n+\n+\t\t\tfinal RecordIterator<T> records = batch.convertAndGetIterator(startingRowNum);\n+\t\t\tif (recordsToSkip > 0) {", "originalCommit": "6fc43dcfa07a2f1488f33d54d26ed8c4bdeff2ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA0ODg4Nw==", "url": "https://github.com/apache/flink/pull/13479#discussion_r503048887", "bodyText": "It is necessary, It is possible to read only half of the batch data.", "author": "JingsongLi", "createdAt": "2020-10-12T05:39:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2OTcxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA2MTg3OQ==", "url": "https://github.com/apache/flink/pull/13479#discussion_r503061879", "bodyText": "I meant we can simply call skipRecord(records) here. It checks recordsToSkip > 0 by itself, no?", "author": "lirui-apache", "createdAt": "2020-10-12T06:22:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2OTcxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA3MTYyNQ==", "url": "https://github.com/apache/flink/pull/13479#discussion_r503071625", "bodyText": "I see", "author": "JingsongLi", "createdAt": "2020-10-12T06:47:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2OTcxOA=="}], "type": "inlineReview"}, {"oid": "ce8e39488eeb28a9b310bda4c1c93636bc7ae8f2", "url": "https://github.com/apache/flink/commit/ce8e39488eeb28a9b310bda4c1c93636bc7ae8f2", "message": "[FLINK-19414][parquet] Introduce ParquetColumnarRowInputFormat", "committedDate": "2020-10-12T06:58:21Z", "type": "forcePushed"}, {"oid": "701a6ec779ee2bf5cabbbb4a34c91db4efbdfb3e", "url": "https://github.com/apache/flink/commit/701a6ec779ee2bf5cabbbb4a34c91db4efbdfb3e", "message": "[FLINK-19414][table-runtime] Introduce ColumnarRowIterator", "committedDate": "2020-10-12T07:00:24Z", "type": "commit"}, {"oid": "cf56f3259a4e6f72c1974a8e323bcb0e90729f90", "url": "https://github.com/apache/flink/commit/cf56f3259a4e6f72c1974a8e323bcb0e90729f90", "message": "[FLINK-19414][connector-file] Introduce forEachRemaining util for BulkFormat.Reader", "committedDate": "2020-10-12T07:00:26Z", "type": "commit"}, {"oid": "66d13440e98f05d20e7351e22f1c0f4f2fca7e60", "url": "https://github.com/apache/flink/commit/66d13440e98f05d20e7351e22f1c0f4f2fca7e60", "message": "[FLINK-19414][parquet] Introduce ParquetColumnarRowInputFormat", "committedDate": "2020-10-12T07:02:33Z", "type": "forcePushed"}, {"oid": "8b7772756eb215c09ef7a77bec62a2e258661454", "url": "https://github.com/apache/flink/commit/8b7772756eb215c09ef7a77bec62a2e258661454", "message": "[FLINK-19414][parquet] Introduce ParquetColumnarRowInputFormat", "committedDate": "2020-10-12T07:06:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA4MTM4MQ==", "url": "https://github.com/apache/flink/pull/13479#discussion_r503081381", "bodyText": "This can be improved when it takes a Split in the create/restore method, not just the path and position. Then we can get partition directly.", "author": "JingsongLi", "createdAt": "2020-10-12T07:10:00Z", "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetColumnarRowInputFormat.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.parquet;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connector.file.src.util.Pool;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.parquet.utils.SerializableConfiguration;\n+import org.apache.flink.formats.parquet.vector.ColumnBatchFactory;\n+import org.apache.flink.table.data.ColumnarRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.vector.ColumnVector;\n+import org.apache.flink.table.data.vector.VectorizedColumnBatch;\n+import org.apache.flink.table.data.vector.writable.WritableColumnVector;\n+import org.apache.flink.table.filesystem.ColumnarRowIterator;\n+import org.apache.flink.table.filesystem.PartitionValueConverter;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createVectorFromConstant;\n+\n+/**\n+ * A {@link ParquetVectorizedInputFormat} to provide {@link RowData} iterator.\n+ * Using {@link ColumnarRowData} to provide a row view of column batch.\n+ */\n+public class ParquetColumnarRowInputFormat extends ParquetVectorizedInputFormat<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final RowType producedType;\n+\n+\t/**\n+\t * Constructor to create parquet format without other fields.\n+\t */\n+\tpublic ParquetColumnarRowInputFormat(\n+\t\t\tConfiguration hadoopConfig,\n+\t\t\tString[] projectedFields,\n+\t\t\tLogicalType[] projectedTypes,\n+\t\t\tint batchSize,\n+\t\t\tboolean isUtcTimestamp,\n+\t\t\tboolean isCaseSensitive) {\n+\t\tthis(\n+\t\t\t\thadoopConfig,\n+\t\t\t\tprojectedFields,\n+\t\t\t\tprojectedTypes,\n+\t\t\t\tRowType.of(projectedTypes, projectedFields),\n+\t\t\t\tColumnBatchFactory.DEFAULT,\n+\t\t\t\tbatchSize,\n+\t\t\t\tisUtcTimestamp,\n+\t\t\t\tisCaseSensitive);\n+\t}\n+\n+\t/**\n+\t * Constructor to create parquet format with other fields created by {@link ColumnBatchFactory}.\n+\t */\n+\tpublic ParquetColumnarRowInputFormat(\n+\t\t\tConfiguration hadoopConfig,\n+\t\t\tString[] projectedFields,\n+\t\t\tLogicalType[] projectedTypes,\n+\t\t\tRowType producedType,\n+\t\t\tColumnBatchFactory batchFactory,\n+\t\t\tint batchSize,\n+\t\t\tboolean isUtcTimestamp,\n+\t\t\tboolean isCaseSensitive) {\n+\t\tsuper(\n+\t\t\t\tnew SerializableConfiguration(hadoopConfig),\n+\t\t\t\tprojectedFields,\n+\t\t\t\tprojectedTypes,\n+\t\t\t\tbatchFactory,\n+\t\t\t\tbatchSize,\n+\t\t\t\tisUtcTimestamp,\n+\t\t\t\tisCaseSensitive);\n+\t\tthis.producedType = producedType;\n+\t}\n+\n+\t@Override\n+\tprotected ParquetReaderBatch<RowData> createReaderBatch(\n+\t\t\tWritableColumnVector[] writableVectors,\n+\t\t\tVectorizedColumnBatch columnarBatch,\n+\t\t\tPool.Recycler<ParquetReaderBatch<RowData>> recycler) {\n+\t\treturn new ColumnarRowReaderBatch(writableVectors, columnarBatch, recycler);\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn InternalTypeInfo.of(producedType);\n+\t}\n+\n+\tprivate static class ColumnarRowReaderBatch extends ParquetReaderBatch<RowData> {\n+\n+\t\tprivate final ColumnarRowIterator result;\n+\n+\t\tprivate ColumnarRowReaderBatch(\n+\t\t\t\tWritableColumnVector[] writableVectors,\n+\t\t\t\tVectorizedColumnBatch columnarBatch,\n+\t\t\t\tPool.Recycler<ParquetReaderBatch<RowData>> recycler) {\n+\t\t\tsuper(writableVectors, columnarBatch, recycler);\n+\t\t\tthis.result = new ColumnarRowIterator(new ColumnarRowData(columnarBatch), this::recycle);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RecordIterator<RowData> convertAndGetIterator(long rowsReturned) {\n+\t\t\tresult.set(columnarBatch.getNumRows(), rowsReturned);\n+\t\t\treturn result;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Create a partitioned {@link ParquetColumnarRowInputFormat}, the partition columns can be\n+\t * generated by {@link Path}.\n+\t */\n+\tpublic static ParquetColumnarRowInputFormat createPartitionedFormat(\n+\t\t\tConfiguration hadoopConfig,\n+\t\t\tRowType tableType,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartValue,\n+\t\t\tint[] selectedFields,\n+\t\t\tPartitionValueConverter valueConverter,", "originalCommit": "8b7772756eb215c09ef7a77bec62a2e258661454", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3fdab6dd18196b9307fe7e6eaad216549f626a3b", "url": "https://github.com/apache/flink/commit/3fdab6dd18196b9307fe7e6eaad216549f626a3b", "message": "[FLINK-19414][file connector] Extract FileSourceTextLinesITCase to AbstractFileSourceITCase", "committedDate": "2020-10-12T08:20:20Z", "type": "commit"}, {"oid": "acf0eed867c7b85fb44381802649d2ade720e942", "url": "https://github.com/apache/flink/commit/acf0eed867c7b85fb44381802649d2ade720e942", "message": "[FLINK-19414][parquet] Introduce ParquetColumnarRowInputFormat", "committedDate": "2020-10-12T08:20:21Z", "type": "forcePushed"}, {"oid": "0513fe1d2ce9768deaba3b7da20b9dff11746a0c", "url": "https://github.com/apache/flink/commit/0513fe1d2ce9768deaba3b7da20b9dff11746a0c", "message": "[FLINK-19414][parquet] Introduce ParquetColumnarRowInputFormat", "committedDate": "2020-10-12T11:35:20Z", "type": "commit"}, {"oid": "0513fe1d2ce9768deaba3b7da20b9dff11746a0c", "url": "https://github.com/apache/flink/commit/0513fe1d2ce9768deaba3b7da20b9dff11746a0c", "message": "[FLINK-19414][parquet] Introduce ParquetColumnarRowInputFormat", "committedDate": "2020-10-12T11:35:20Z", "type": "forcePushed"}]}