{"pr_number": 14464, "pr_title": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format", "pr_createdAt": "2020-12-22T13:24:34Z", "pr_url": "https://github.com/apache/flink/pull/14464", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ0MTA5NQ==", "url": "https://github.com/apache/flink/pull/14464#discussion_r548441095", "bodyText": "Add indent.", "author": "wuchong", "createdAt": "2020-12-24T08:12:24Z", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/canal/CanalJsonDecodingFormat.java", "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json.canal;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.formats.json.TimestampFormat;\n+import org.apache.flink.formats.json.canal.CanalJsonDeserializationSchema.MetadataConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.DecodingFormat;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.utils.DataTypeUtils;\n+import org.apache.flink.types.RowKind;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * {@link DecodingFormat} for Canal using JSON encoding.\n+ */\n+public class CanalJsonDecodingFormat implements DecodingFormat<DeserializationSchema<RowData>> {\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Mutable attributes\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate List<String> metadataKeys;\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Canal-specific attributes\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate final @Nullable String database;\n+\n+\tprivate final @Nullable String table;\n+\n+\tprivate final boolean ignoreParseErrors;\n+\n+\tprivate final TimestampFormat timestampFormat;\n+\n+\tpublic CanalJsonDecodingFormat(\n+\t\tString database,\n+\t\tString table,\n+\t\tboolean ignoreParseErrors,\n+\t\tTimestampFormat timestampFormat) {\n+\t\tthis.database = database;\n+\t\tthis.table = table;\n+\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\tthis.timestampFormat = timestampFormat;\n+\t\tthis.metadataKeys = Collections.emptyList();\n+\t}\n+\n+\t@Override\n+\tpublic DeserializationSchema<RowData> createRuntimeDecoder(\n+\t\tDynamicTableSource.Context context,\n+\t\tDataType physicalDataType) {", "originalCommit": "ddcee436fcf2904fb1b3241eb4eccfd36f6553be", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ0NjA4OA==", "url": "https://github.com/apache/flink/pull/14464#discussion_r548446088", "bodyText": "Add indent.", "author": "wuchong", "createdAt": "2020-12-24T08:28:35Z", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/canal/CanalJsonDecodingFormat.java", "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.json.canal;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.formats.json.TimestampFormat;\n+import org.apache.flink.formats.json.canal.CanalJsonDeserializationSchema.MetadataConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.DecodingFormat;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.utils.DataTypeUtils;\n+import org.apache.flink.types.RowKind;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * {@link DecodingFormat} for Canal using JSON encoding.\n+ */\n+public class CanalJsonDecodingFormat implements DecodingFormat<DeserializationSchema<RowData>> {\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Mutable attributes\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate List<String> metadataKeys;\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Canal-specific attributes\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate final @Nullable String database;\n+\n+\tprivate final @Nullable String table;\n+\n+\tprivate final boolean ignoreParseErrors;\n+\n+\tprivate final TimestampFormat timestampFormat;\n+\n+\tpublic CanalJsonDecodingFormat(\n+\t\tString database,\n+\t\tString table,\n+\t\tboolean ignoreParseErrors,\n+\t\tTimestampFormat timestampFormat) {\n+\t\tthis.database = database;\n+\t\tthis.table = table;\n+\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\tthis.timestampFormat = timestampFormat;\n+\t\tthis.metadataKeys = Collections.emptyList();\n+\t}\n+\n+\t@Override\n+\tpublic DeserializationSchema<RowData> createRuntimeDecoder(\n+\t\tDynamicTableSource.Context context,\n+\t\tDataType physicalDataType) {\n+\t\tfinal List<ReadableMetadata> readableMetadata = metadataKeys.stream()\n+\t\t\t.map(k ->\n+\t\t\t\tStream.of(ReadableMetadata.values())\n+\t\t\t\t\t.filter(rm -> rm.key.equals(k))\n+\t\t\t\t\t.findFirst()\n+\t\t\t\t\t.orElseThrow(IllegalStateException::new))\n+\t\t\t.collect(Collectors.toList());\n+\t\tfinal List<DataTypes.Field> metadataFields = readableMetadata.stream()\n+\t\t\t.map(m -> DataTypes.FIELD(m.key, m.dataType))\n+\t\t\t.collect(Collectors.toList());\n+\t\tfinal DataType producedDataType = DataTypeUtils.appendRowFields(physicalDataType, metadataFields);\n+\t\tfinal TypeInformation<RowData> producedTypeInfo = context.createTypeInformation(producedDataType);\n+\t\treturn CanalJsonDeserializationSchema.builder(physicalDataType, readableMetadata, producedTypeInfo)\n+\t\t\t.setDatabase(database)\n+\t\t\t.setTable(table)\n+\t\t\t.setIgnoreParseErrors(ignoreParseErrors)\n+\t\t\t.setTimestampFormat(timestampFormat)\n+\t\t\t.build();\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, DataType> listReadableMetadata() {\n+\t\tfinal Map<String, DataType> metadataMap = new LinkedHashMap<>();\n+\t\tStream.of(ReadableMetadata.values()).forEachOrdered(m -> metadataMap.put(m.key, m.dataType));\n+\t\treturn metadataMap;\n+\t}\n+\n+\t@Override\n+\tpublic void applyReadableMetadata(List<String> metadataKeys) {\n+\t\tthis.metadataKeys = metadataKeys;\n+\t}\n+\n+\t@Override\n+\tpublic ChangelogMode getChangelogMode() {\n+\t\treturn ChangelogMode.newBuilder()\n+\t\t\t.addContainedKind(RowKind.INSERT)\n+\t\t\t.addContainedKind(RowKind.UPDATE_BEFORE)\n+\t\t\t.addContainedKind(RowKind.UPDATE_AFTER)\n+\t\t\t.addContainedKind(RowKind.DELETE)\n+\t\t\t.build();\n+\t}\n+\n+\t// --------------------------------------------------------------------------------------------\n+\t// Metadata handling\n+\t// --------------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * List of metadata that can be read with this format.\n+\t */\n+\tenum ReadableMetadata {\n+\t\tDATABASE(\n+\t\t\t\"database\",\n+\t\t\tDataTypes.STRING().nullable(),\n+\t\t\tDataTypes.FIELD(\"database\", DataTypes.STRING()),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\treturn row.getString(pos);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t),\n+\n+\t\tTABLE(\n+\t\t\t\"table\",\n+\t\t\tDataTypes.STRING().nullable(),\n+\t\t\tDataTypes.FIELD(\"table\", DataTypes.STRING()),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\treturn row.getString(pos);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t),\n+\n+\t\tSQL_TYPE(\n+\t\t\t\"sql-type\",\n+\t\t\tDataTypes.STRING().nullable(),\n+\t\t\tDataTypes.FIELD(\"sqlType\", DataTypes.STRING()),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\treturn row.getString(pos);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t),\n+\n+\t\tPK_NAMES(\n+\t\t\t\"pk-names\",\n+\t\t\tDataTypes.STRING().nullable(),\n+\t\t\tDataTypes.FIELD(\"pkNames\", DataTypes.ARRAY(DataTypes.STRING())),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\treturn row.getArray(pos);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t),\n+\n+\t\tINGESTION_TIMESTAMP(\n+\t\t\t\"ingestion-timestamp\",\n+\t\t\tDataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE(3).nullable(),\n+\t\t\tDataTypes.FIELD(\"ts\", DataTypes.BIGINT()),\n+\t\t\tnew MetadataConverter() {\n+\t\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic Object convert(GenericRowData row, int pos) {\n+\t\t\t\t\tif (row.isNullAt(pos)) {\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn TimestampData.fromEpochMillis(row.getLong(pos));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t);\n+\n+\t\tfinal String key;\n+\n+\t\tfinal DataType dataType;\n+\n+\t\tfinal DataTypes.Field requiredJsonField;\n+\n+\t\tfinal MetadataConverter converter;\n+\n+\t\tReadableMetadata(\n+\t\t\tString key,\n+\t\t\tDataType dataType,\n+\t\t\tDataTypes.Field requiredJsonField,\n+\t\t\tMetadataConverter converter) {", "originalCommit": "ddcee436fcf2904fb1b3241eb4eccfd36f6553be", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ0NzY0Mg==", "url": "https://github.com/apache/flink/pull/14464#discussion_r548447642", "bodyText": "Add indent.", "author": "wuchong", "createdAt": "2020-12-24T08:33:45Z", "path": "flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/canal/CanalJsonDeserializationSchema.java", "diffHunk": "@@ -262,24 +296,71 @@ public boolean equals(Object o) {\n \t\t\treturn false;\n \t\t}\n \t\tCanalJsonDeserializationSchema that = (CanalJsonDeserializationSchema) o;\n-\t\treturn ignoreParseErrors == that.ignoreParseErrors &&\n-\t\t\tfieldCount == that.fieldCount &&\n-\t\t\tObjects.equals(jsonDeserializer, that.jsonDeserializer) &&\n-\t\t\tObjects.equals(resultTypeInfo, that.resultTypeInfo);\n+\t\treturn Objects.equals(jsonDeserializer, that.jsonDeserializer)\n+\t\t\t&& hasMetadata == that.hasMetadata\n+\t\t\t&& Objects.equals(producedTypeInfo, that.producedTypeInfo)\n+\t\t\t&& Objects.equals(database, that.database)\n+\t\t\t&& Objects.equals(table, that.table)\n+\t\t\t&& ignoreParseErrors == that.ignoreParseErrors\n+\t\t\t&& fieldCount == that.fieldCount;\n \t}\n \n \t@Override\n \tpublic int hashCode() {\n-\t\treturn Objects.hash(jsonDeserializer, resultTypeInfo, ignoreParseErrors, fieldCount);\n+\t\treturn Objects.hash(jsonDeserializer, hasMetadata, producedTypeInfo, database, table, ignoreParseErrors, fieldCount);\n \t}\n \n-\tprivate static RowType createJsonRowType(DataType databaseSchema) {\n+\t// --------------------------------------------------------------------------------------------\n+\n+\tprivate static RowType createJsonRowType(DataType physicalDataType, List<ReadableMetadata> readableMetadata) {\n \t\t// Canal JSON contains other information, e.g. \"ts\", \"sql\", but we don't need them\n-\t\treturn (RowType) DataTypes.ROW(\n-\t\t\tDataTypes.FIELD(\"data\", DataTypes.ARRAY(databaseSchema)),\n-\t\t\tDataTypes.FIELD(\"old\", DataTypes.ARRAY(databaseSchema)),\n+\t\tDataType root = DataTypes.ROW(\n+\t\t\tDataTypes.FIELD(\"data\", DataTypes.ARRAY(physicalDataType)),\n+\t\t\tDataTypes.FIELD(\"old\", DataTypes.ARRAY(physicalDataType)),\n \t\t\tDataTypes.FIELD(\"type\", DataTypes.STRING()),\n-\t\t\tDataTypes.FIELD(\"database\", DataTypes.STRING()),\n-\t\t\tDataTypes.FIELD(\"table\", DataTypes.STRING())).getLogicalType();\n+\t\t\tReadableMetadata.DATABASE.requiredJsonField,\n+\t\t\tReadableMetadata.TABLE.requiredJsonField);\n+\t\t// append fields that are required for reading metadata in the root\n+\t\tfinal List<DataTypes.Field> rootMetadataFields = readableMetadata.stream()\n+\t\t\t.filter(m -> m != ReadableMetadata.DATABASE && m != ReadableMetadata.TABLE)\n+\t\t\t.map(m -> m.requiredJsonField)\n+\t\t\t.distinct()\n+\t\t\t.collect(Collectors.toList());\n+\t\treturn (RowType) DataTypeUtils.appendRowFields(root, rootMetadataFields).getLogicalType();\n+\t}\n+\n+\tprivate static MetadataConverter[] createMetadataConverters(\n+\t\tRowType jsonRowType,\n+\t\tList<ReadableMetadata> requestedMetadata) {", "originalCommit": "ddcee436fcf2904fb1b3241eb4eccfd36f6553be", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9abb79a2bb6e487d3f1deb0cfc4ecfff74a7d780", "url": "https://github.com/apache/flink/commit/9abb79a2bb6e487d3f1deb0cfc4ecfff74a7d780", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format", "committedDate": "2020-12-27T23:18:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTIwNzI4MA==", "url": "https://github.com/apache/flink/pull/14464#discussion_r549207280", "bodyText": "Could you also test the Map and Array metadatas?", "author": "wuchong", "createdAt": "2020-12-28T04:05:48Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaChangelogTableITCase.java", "diffHunk": "@@ -198,4 +198,130 @@ public void testKafkaDebeziumChangelogSource() throws Exception {\n \t\ttableResult.getJobClient().get().cancel().get(); // stop the job\n \t\tdeleteTestTopic(topic);\n \t}\n+\n+\t@Test\n+\tpublic void testKafkaCanalChangelogSource() throws Exception {\n+\t\tfinal String topic = \"changelog_canal\";\n+\t\tcreateTestTopic(topic, 1, 1);\n+\n+\t\t// enables MiniBatch processing to verify MiniBatch + FLIP-95, see FLINK-18769\n+\t\tConfiguration tableConf = tEnv.getConfig().getConfiguration();\n+\t\ttableConf.setString(\"table.exec.mini-batch.enabled\", \"true\");\n+\t\ttableConf.setString(\"table.exec.mini-batch.allow-latency\", \"1s\");\n+\t\ttableConf.setString(\"table.exec.mini-batch.size\", \"5000\");\n+\t\ttableConf.setString(\"table.optimizer.agg-phase-strategy\", \"TWO_PHASE\");\n+\n+\t\t// ---------- Write the Canal json into Kafka -------------------\n+\t\tList<String> lines = readLines(\"canal-data.txt\");\n+\t\tDataStreamSource<String> stream = env.fromCollection(lines);\n+\t\tSerializationSchema<String> serSchema = new SimpleStringSchema();\n+\t\tFlinkKafkaPartitioner<String> partitioner = new FlinkFixedPartitioner<>();\n+\n+\t\t// the producer must not produce duplicates\n+\t\tProperties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n+\t\tproducerProperties.setProperty(\"retries\", \"0\");\n+\t\tproducerProperties.putAll(secureProps);\n+\t\tkafkaServer.produceIntoKafka(stream, topic, serSchema, producerProperties, partitioner);\n+\t\ttry {\n+\t\t\tenv.execute(\"Write sequence\");\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new Exception(\"Failed to write canal data to Kafka.\", e);\n+\t\t}\n+\n+\t\t// ---------- Produce an event time stream into Kafka -------------------\n+\t\tString bootstraps = standardProps.getProperty(\"bootstrap.servers\");\n+\t\tString sourceDDL = String.format(\n+\t\t\t\"CREATE TABLE canal_source (\" +\n+\t\t\t\t// test format metadata\n+\t\t\t\t\" origin_ts STRING METADATA FROM 'value.ingestion-timestamp' VIRTUAL,\" + // unused\n+\t\t\t\t\" origin_table STRING METADATA FROM 'value.table' VIRTUAL,\" +", "originalCommit": "9abb79a2bb6e487d3f1deb0cfc4ecfff74a7d780", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e81abd70d44d6ce7221d4cd2f44d31deab78ea9a", "url": "https://github.com/apache/flink/commit/e81abd70d44d6ce7221d4cd2f44d31deab78ea9a", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format", "committedDate": "2020-12-28T13:26:32Z", "type": "forcePushed"}, {"oid": "6956309e3928df5f4e7717a978bd91eee472f6ed", "url": "https://github.com/apache/flink/commit/6956309e3928df5f4e7717a978bd91eee472f6ed", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format", "committedDate": "2020-12-28T13:33:45Z", "type": "forcePushed"}, {"oid": "f868d49d3c71f7b55d1c6a3dc601ac1561bdd9e8", "url": "https://github.com/apache/flink/commit/f868d49d3c71f7b55d1c6a3dc601ac1561bdd9e8", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format", "committedDate": "2020-12-29T06:19:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU5MjE5Ng==", "url": "https://github.com/apache/flink/pull/14464#discussion_r549592196", "bodyText": "Could you beautify the format? The same to the debezium one.", "author": "wuchong", "createdAt": "2020-12-29T07:01:09Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaChangelogTableITCase.java", "diffHunk": "@@ -218,4 +218,168 @@ public void testKafkaDebeziumChangelogSource() throws Exception {\n         tableResult.getJobClient().get().cancel().get(); // stop the job\n         deleteTestTopic(topic);\n     }\n+\n+    @Test\n+    public void testKafkaCanalChangelogSource() throws Exception {\n+        final String topic = \"changelog_canal\";\n+        createTestTopic(topic, 1, 1);\n+\n+        // enables MiniBatch processing to verify MiniBatch + FLIP-95, see FLINK-18769\n+        Configuration tableConf = tEnv.getConfig().getConfiguration();\n+        tableConf.setString(\"table.exec.mini-batch.enabled\", \"true\");\n+        tableConf.setString(\"table.exec.mini-batch.allow-latency\", \"1s\");\n+        tableConf.setString(\"table.exec.mini-batch.size\", \"5000\");\n+        tableConf.setString(\"table.optimizer.agg-phase-strategy\", \"TWO_PHASE\");\n+\n+        // ---------- Write the Canal json into Kafka -------------------\n+        List<String> lines = readLines(\"canal-data.txt\");\n+        DataStreamSource<String> stream = env.fromCollection(lines);\n+        SerializationSchema<String> serSchema = new SimpleStringSchema();\n+        FlinkKafkaPartitioner<String> partitioner = new FlinkFixedPartitioner<>();\n+\n+        // the producer must not produce duplicates\n+        Properties producerProperties =\n+                FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);\n+        producerProperties.setProperty(\"retries\", \"0\");\n+        producerProperties.putAll(secureProps);\n+        kafkaServer.produceIntoKafka(stream, topic, serSchema, producerProperties, partitioner);\n+        try {\n+            env.execute(\"Write sequence\");\n+        } catch (Exception e) {\n+            throw new Exception(\"Failed to write canal data to Kafka.\", e);\n+        }\n+\n+        // ---------- Produce an event time stream into Kafka -------------------\n+        String bootstraps = standardProps.getProperty(\"bootstrap.servers\");\n+        String sourceDDL =\n+                String.format(\n+                        \"CREATE TABLE canal_source (\"\n+                                +\n+                                // test format metadata\n+                                \" origin_database STRING METADATA FROM 'value.database' VIRTUAL,\"\n+                                + \" origin_table STRING METADATA FROM 'value.table' VIRTUAL,\"\n+                                + \" origin_sql_type MAP<STRING, INT> METADATA FROM 'value.sql-type' VIRTUAL,\"\n+                                + \" origin_pk_names ARRAY<STRING> METADATA FROM 'value.pk-names' VIRTUAL,\"\n+                                + \" origin_ts TIMESTAMP(3) METADATA FROM 'value.ingestion-timestamp' VIRTUAL,\"\n+                                + \" id INT NOT NULL,\"\n+                                + \" name STRING,\"\n+                                + \" description STRING,\"\n+                                + \" weight DECIMAL(10,3),\"\n+                                +\n+                                // test connector metadata\n+                                \" origin_topic STRING METADATA FROM 'topic' VIRTUAL,\"\n+                                + \" origin_partition STRING METADATA FROM 'partition' VIRTUAL\"\n+                                + // unused\n+                                \") WITH (\"\n+                                + \" 'connector' = 'kafka',\"\n+                                + \" 'topic' = '%s',\"\n+                                + \" 'properties.bootstrap.servers' = '%s',\"\n+                                + \" 'scan.startup.mode' = 'earliest-offset',\"\n+                                + \" 'value.format' = 'canal-json'\"\n+                                + \")\",\n+                        topic, bootstraps);\n+        String sinkDDL =\n+                \"CREATE TABLE sink (\"\n+                        + \" origin_topic STRING,\"\n+                        + \" origin_database STRING,\"\n+                        + \" origin_table STRING,\"\n+                        + \" origin_sql_type MAP<STRING, INT>,\"\n+                        + \" origin_pk_names ARRAY<STRING>,\"\n+                        + \" origin_ts TIMESTAMP(3),\"\n+                        + \" name STRING,\"\n+                        + \" PRIMARY KEY (name) NOT ENFORCED\"\n+                        + \") WITH (\"\n+                        + \" 'connector' = 'values',\"\n+                        + \" 'sink-insert-only' = 'false'\"\n+                        + \")\";\n+        tEnv.executeSql(sourceDDL);\n+        tEnv.executeSql(sinkDDL);\n+        TableResult tableResult =\n+                tEnv.executeSql(\n+                        \"INSERT INTO sink \"\n+                                + \"SELECT origin_topic, origin_database, origin_table, origin_sql_type, \"\n+                                + \"origin_pk_names, origin_ts, name \"\n+                                + \"FROM canal_source\");\n+\n+        // Canal captures change data on the `products2` table:\n+        //\n+        // CREATE TABLE products2 (\n+        //  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,\n+        //  name VARCHAR(255),\n+        //  description VARCHAR(512),\n+        //  weight FLOAT\n+        // );\n+        // ALTER TABLE products2 AUTO_INCREMENT = 101;\n+        //\n+        // INSERT INTO products2\n+        // VALUES (default,\"scooter\",\"Small 2-wheel scooter\",3.14),\n+        //        (default,\"car battery\",\"12V car battery\",8.1),\n+        //        (default,\"12-pack drill bits\",\"12-pack of drill bits with sizes ranging from #40\n+        // to\n+        // #3\",0.8),\n+        //        (default,\"hammer\",\"12oz carpenter's hammer\",0.75),\n+        //        (default,\"hammer\",\"14oz carpenter's hammer\",0.875),\n+        //        (default,\"hammer\",\"16oz carpenter's hammer\",1.0),\n+        //        (default,\"rocks\",\"box of assorted rocks\",5.3),\n+        //        (default,\"jacket\",\"water resistent black wind breaker\",0.1),\n+        //        (default,\"spare tire\",\"24 inch spare tire\",22.2);\n+        // UPDATE products2 SET description='18oz carpenter hammer' WHERE id=106;\n+        // UPDATE products2 SET weight='5.1' WHERE id=107;\n+        // INSERT INTO products2 VALUES (default,\"jacket\",\"water resistent white wind breaker\",0.2);\n+        // INSERT INTO products2 VALUES (default,\"scooter\",\"Big 2-wheel scooter \",5.18);\n+        // UPDATE products2 SET description='new water resistent white wind breaker', weight='0.5'\n+        // WHERE\n+        // id=110;\n+        // UPDATE products2 SET weight='5.17' WHERE id=111;\n+        // DELETE FROM products2 WHERE id=111;\n+        // UPDATE products2 SET weight='5.17' WHERE id=102 or id = 101;\n+        // DELETE FROM products2 WHERE id=102 or id = 103;\n+        //\n+        // > SELECT * FROM products2;\n+        // +-----+--------------------+---------------------------------------------------------+--------+\n+        // | id  | name               | description                                             |\n+        // weight\n+        // |\n+        // +-----+--------------------+---------------------------------------------------------+--------+\n+        // | 101 | scooter            | Small 2-wheel scooter                                   |\n+        // 5.17\n+        // |\n+        // | 104 | hammer             | 12oz carpenter's hammer                                 |\n+        // 0.75\n+        // |\n+        // | 105 | hammer             | 14oz carpenter's hammer                                 |\n+        // 0.875\n+        // |\n+        // | 106 | hammer             | 18oz carpenter hammer                                   |\n+        //   1\n+        // |\n+        // | 107 | rocks              | box of assorted rocks                                   |\n+        // 5.1\n+        // |\n+        // | 108 | jacket             | water resistent black wind breaker                      |", "originalCommit": "f868d49d3c71f7b55d1c6a3dc601ac1561bdd9e8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "554e0a2c2ada42836d65397b163bad657806e8c2", "url": "https://github.com/apache/flink/commit/554e0a2c2ada42836d65397b163bad657806e8c2", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format", "committedDate": "2020-12-29T12:02:57Z", "type": "forcePushed"}, {"oid": "3f3b88d98c525b635179b6a9098a2634a1ffb42c", "url": "https://github.com/apache/flink/commit/3f3b88d98c525b635179b6a9098a2634a1ffb42c", "message": "[FLINK-20385][canal][json] Allow to read metadata for canal-json format", "committedDate": "2020-12-30T06:52:25Z", "type": "forcePushed"}, {"oid": "5a5ef1f0bb5e1dcc094ad5454890d13ddf55c646", "url": "https://github.com/apache/flink/commit/5a5ef1f0bb5e1dcc094ad5454890d13ddf55c646", "message": "[FLINK-20385][canal-json] Allow to read metadata for canal-json format\n\nThis closes #14464", "committedDate": "2020-12-30T07:33:03Z", "type": "commit"}, {"oid": "5a5ef1f0bb5e1dcc094ad5454890d13ddf55c646", "url": "https://github.com/apache/flink/commit/5a5ef1f0bb5e1dcc094ad5454890d13ddf55c646", "message": "[FLINK-20385][canal-json] Allow to read metadata for canal-json format\n\nThis closes #14464", "committedDate": "2020-12-30T07:33:03Z", "type": "forcePushed"}, {"oid": "d6ec8319ec7906aeda62d9c60887f6b9fb2f505f", "url": "https://github.com/apache/flink/commit/d6ec8319ec7906aeda62d9c60887f6b9fb2f505f", "message": "[FLINK-20385][canal-json] Allow to read metadata for canal-json format", "committedDate": "2020-12-30T10:58:13Z", "type": "commit"}]}