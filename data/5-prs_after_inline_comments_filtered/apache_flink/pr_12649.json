{"pr_number": 12649, "pr_title": "[FLINK-18286] Implement type inference for GET/FLATTEN ", "pr_createdAt": "2020-06-15T06:57:23Z", "pr_url": "https://github.com/apache/flink/pull/12649", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1MDk0OQ==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440250949", "bodyText": "remove false? the nullability is already excluded by LITERAL", "author": "twalthr", "createdAt": "2020-06-15T15:15:38Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/functions/BuiltInFunctionDefinitions.java", "diffHunk": "@@ -972,13 +972,28 @@\n \t\tnew BuiltInFunctionDefinition.Builder()\n \t\t\t.name(\"flatten\")\n \t\t\t.kind(OTHER)\n-\t\t\t.outputTypeStrategy(TypeStrategies.MISSING)\n+\t\t\t.inputTypeStrategy(sequence(InputTypeStrategies.COMPOSITE))\n+\t\t\t.outputTypeStrategy(callContext -> {\n+\t\t\t\tthrow new UnsupportedOperationException(\"FLATTEN should be resolved to GET expressions\");\n+\t\t\t})\n \t\t\t.build();\n \tpublic static final BuiltInFunctionDefinition GET =\n \t\tnew BuiltInFunctionDefinition.Builder()\n \t\t\t.name(\"get\")\n \t\t\t.kind(OTHER)\n-\t\t\t.outputTypeStrategy(TypeStrategies.MISSING)\n+\t\t\t.inputTypeStrategy(\n+\t\t\t\tsequence(\n+\t\t\t\t\tInputTypeStrategies.COMPOSITE,\n+\t\t\t\t\tand(\n+\t\t\t\t\t\tInputTypeStrategies.LITERAL,\n+\t\t\t\t\t\tor(\n+\t\t\t\t\t\t\tlogical(LogicalTypeRoot.INTEGER),\n+\t\t\t\t\t\t\tlogical(LogicalTypeFamily.CHARACTER_STRING, false)", "originalCommit": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1MjM2Mg==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440252362", "bodyText": "nit: move to separate class like all other argument strategies", "author": "twalthr", "createdAt": "2020-06-15T15:17:36Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java", "diffHunk": "@@ -198,6 +201,36 @@ public static InputTypeStrategy comparable(\n \t */\n \tpublic static final LiteralArgumentTypeStrategy LITERAL_OR_NULL = new LiteralArgumentTypeStrategy(true);\n \n+\t/**\n+\t * Strategy that checks that the argument has a composite type.\n+\t */\n+\tpublic static final ArgumentTypeStrategy COMPOSITE = new ArgumentTypeStrategy() {", "originalCommit": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1MzQ5MA==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440253490", "bodyText": "nit: wrong indention", "author": "twalthr", "createdAt": "2020-06-15T15:19:11Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java", "diffHunk": "@@ -198,6 +201,36 @@ public static InputTypeStrategy comparable(\n \t */\n \tpublic static final LiteralArgumentTypeStrategy LITERAL_OR_NULL = new LiteralArgumentTypeStrategy(true);\n \n+\t/**\n+\t * Strategy that checks that the argument has a composite type.\n+\t */\n+\tpublic static final ArgumentTypeStrategy COMPOSITE = new ArgumentTypeStrategy() {\n+\t\t@Override\n+\t\tpublic Optional<DataType> inferArgumentType(\n+\t\t\t\tCallContext callContext,\n+\t\t\t\tint argumentPos,\n+\t\t\t\tboolean throwOnFailure) {\n+\t\t\t\tDataType dataType = callContext.getArgumentDataTypes().get(argumentPos);", "originalCommit": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1OTA5Nw==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440259097", "bodyText": "I don't like that we use TableSchema here. This class doesn't belong here. Furthermore, it does not support distinct type yet. Maybe we can just introduce a DataTypeUtils.getFieldType(int) and DataTypeUtils.getFieldType(name). Can we make it support distinct type (an AtomicDataType)?", "author": "twalthr", "createdAt": "2020-06-15T15:27:02Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/TypeStrategies.java", "diffHunk": "@@ -315,6 +317,36 @@ public static TypeStrategy nullable(TypeStrategy initialStrategy) {\n \t\treturn Optional.of(fromLogicalToDataType(inferredType));\n \t};\n \n+\t/**\n+\t * Type strategy that returns a type of a field nested inside a composite type that is described by the second argument.\n+\t * The second argument must be a literal that describes either the nested field name or index.\n+\t */\n+\tpublic static final TypeStrategy GET = callContext -> {\n+\t\tList<DataType> argumentDataTypes = callContext.getArgumentDataTypes();\n+\t\tDataType rowDataType = argumentDataTypes.get(0);\n+\t\tTableSchema nestedSchema = DataTypeUtils.expandCompositeTypeToSchema(rowDataType);", "originalCommit": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDg5NDEzOQ==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440894139", "bodyText": "I think it does already support the DISTINCT type. Moreover AtomicDataType can not hold a composite type. AtomicDataType can not hold the bridging classes of nested fields. IMO Distinct type can be any of FieldsDataType, AtomicDataType, 'KeyValueDataType` etc.\nThe way we support DISTINCT type is that we do not check the LogicalTypeRoot in expandCompositeTypeToSchema, but we check only for the FieldsDataType.", "author": "dawidwys", "createdAt": "2020-06-16T14:27:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1OTA5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI2MjI0Nw==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440262247", "bodyText": "shall we upgrade the test base to accept AbstractDataType similar to DataStructureConverterTest? it would make the code more readable.", "author": "twalthr", "createdAt": "2020-06-15T15:31:30Z", "path": "flink-table/flink-table-common/src/test/java/org/apache/flink/table/types/inference/InputTypeStrategiesTest.java", "diffHunk": "@@ -582,7 +587,43 @@\n \t\t\t\t\t\t\t\t\"My constraint says %s must be nullable.\",\n \t\t\t\t\t\t\t\targs -> args.get(0).getLogicalType().isNullable()))))\n \t\t\t\t.calledWithArgumentTypes(DataTypes.BOOLEAN().notNull())\n-\t\t\t\t.expectErrorMessage(\"My constraint says BOOLEAN NOT NULL must be nullable.\")\n+\t\t\t\t.expectErrorMessage(\"My constraint says BOOLEAN NOT NULL must be nullable.\"),\n+\n+\t\t\tTestSpec\n+\t\t\t\t.forStrategy(\n+\t\t\t\t\t\"Composite type strategy with ROW\",\n+\t\t\t\t\tsequence(InputTypeStrategies.COMPOSITE)\n+\t\t\t\t)\n+\t\t\t\t.calledWithArgumentTypes(DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.BIGINT())))\n+\t\t\t\t.expectSignature(\"f(<COMPOSITE>)\")\n+\t\t\t\t.expectArgumentTypes(DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.BIGINT()))),\n+\n+\t\t\tTestSpec\n+\t\t\t\t.forStrategy(\n+\t\t\t\t\t\"Composite type strategy with STRUCTURED type\",\n+\t\t\t\t\tsequence(InputTypeStrategies.COMPOSITE)\n+\t\t\t\t)\n+\t\t\t\t.calledWithArgumentTypes(new FieldsDataType(", "originalCommit": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc3MTcwMA==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440771700", "bodyText": "use the newly introduced testResult such that a result must be defined only once", "author": "twalthr", "createdAt": "2020-06-16T11:12:43Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.expressions;\n+\n+import org.apache.flink.table.annotation.DataTypeHint;\n+import org.apache.flink.table.annotation.FunctionHint;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.functions.BuiltInFunctionDefinitions;\n+import org.apache.flink.table.functions.ScalarFunction;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Suite;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.singletonMap;\n+import static org.apache.flink.table.api.DataTypes.ARRAY;\n+import static org.apache.flink.table.api.DataTypes.BIGINT;\n+import static org.apache.flink.table.api.DataTypes.FIELD;\n+import static org.apache.flink.table.api.DataTypes.MAP;\n+import static org.apache.flink.table.api.DataTypes.ROW;\n+import static org.apache.flink.table.api.DataTypes.STRING;\n+import static org.apache.flink.table.api.Expressions.$;\n+import static org.apache.flink.table.api.Expressions.call;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for functions that access nested fields/elements of composite/collection types.\n+ */\n+@RunWith(Suite.class)\n+@Suite.SuiteClasses(\n+\t{\n+\t\tCompositeTypeAccessExpressionITCase.TableFieldAccess.class,\n+\t\tCompositeTypeAccessExpressionITCase.CallFieldAccess.class\n+\t}\n+)\n+public class CompositeTypeAccessExpressionITCase {\n+\n+\t/**\n+\t * Regular tests. See also {@link CallFieldAccess} for tests that access a nested field of an expression or\n+\t * for {@link BuiltInFunctionDefinitions#FLATTEN} which produces multiple columns from a single one.\n+\t */\n+\tpublic static class TableFieldAccess extends BuiltInFunctionTestBase {\n+\t\t@Parameterized.Parameters(name = \"{index}: {0}\")\n+\t\tpublic static List<TestSpec> testData() {\n+\t\t\treturn Arrays.asList(\n+\n+\t\t\t\t// Actually in case of SQL it does not use the GET method, but\n+\t\t\t\t// a custom logic for accessing nested fields of a Table.\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.GET)\n+\t\t\t\t\t.onFieldsWithData(null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t.testTableApiResult($(\"f0\").get(\"nested\"), null, BIGINT().nullable())", "originalCommit": "b3dd92d0c41cee2472bb521eec89741c42bc05cd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc3MjgyNg==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440772826", "bodyText": "should we port AT as well or in a separate PR?", "author": "twalthr", "createdAt": "2020-06-16T11:15:04Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.expressions;\n+\n+import org.apache.flink.table.annotation.DataTypeHint;\n+import org.apache.flink.table.annotation.FunctionHint;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.functions.BuiltInFunctionDefinitions;\n+import org.apache.flink.table.functions.ScalarFunction;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Suite;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.singletonMap;\n+import static org.apache.flink.table.api.DataTypes.ARRAY;\n+import static org.apache.flink.table.api.DataTypes.BIGINT;\n+import static org.apache.flink.table.api.DataTypes.FIELD;\n+import static org.apache.flink.table.api.DataTypes.MAP;\n+import static org.apache.flink.table.api.DataTypes.ROW;\n+import static org.apache.flink.table.api.DataTypes.STRING;\n+import static org.apache.flink.table.api.Expressions.$;\n+import static org.apache.flink.table.api.Expressions.call;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for functions that access nested fields/elements of composite/collection types.\n+ */\n+@RunWith(Suite.class)\n+@Suite.SuiteClasses(\n+\t{\n+\t\tCompositeTypeAccessExpressionITCase.TableFieldAccess.class,\n+\t\tCompositeTypeAccessExpressionITCase.CallFieldAccess.class\n+\t}\n+)\n+public class CompositeTypeAccessExpressionITCase {\n+\n+\t/**\n+\t * Regular tests. See also {@link CallFieldAccess} for tests that access a nested field of an expression or\n+\t * for {@link BuiltInFunctionDefinitions#FLATTEN} which produces multiple columns from a single one.\n+\t */\n+\tpublic static class TableFieldAccess extends BuiltInFunctionTestBase {\n+\t\t@Parameterized.Parameters(name = \"{index}: {0}\")\n+\t\tpublic static List<TestSpec> testData() {\n+\t\t\treturn Arrays.asList(\n+\n+\t\t\t\t// Actually in case of SQL it does not use the GET method, but\n+\t\t\t\t// a custom logic for accessing nested fields of a Table.\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.GET)\n+\t\t\t\t\t.onFieldsWithData(null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t.testTableApiResult($(\"f0\").get(\"nested\"), null, BIGINT().nullable())\n+\t\t\t\t\t.testTableApiResult($(\"f1\").get(\"nested\"), 1L, BIGINT().notNull())\n+\t\t\t\t\t.testSqlResult(\"f0.nested\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f1.nested\", 1L, BIGINT().notNull()),\n+\n+\t\t\t\t// In Calcite it maps to FlinkSqlOperatorTable.ITEM\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.AT)\n+\t\t\t\t\t.onFieldsWithData(null, new int[] {1}, null, singletonMap(\"nested\", 1), null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tARRAY(BIGINT().notNull()).nullable(),\n+\t\t\t\t\t\tARRAY(BIGINT().notNull()).notNull(),\n+\t\t\t\t\t\tMAP(STRING(), BIGINT().notNull()).nullable(),\n+\t\t\t\t\t\tMAP(STRING(), BIGINT().notNull()).notNull(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t// accessing elements of MAP or ARRAY is a runtime operations,\n+\t\t\t\t\t// we do not know about the size or contents during the inference\n+\t\t\t\t\t// therefore the results are always nullable\n+\t\t\t\t\t.testSqlResult(\"f0[1]\", null, BIGINT().nullable())", "originalCommit": "b3dd92d0c41cee2472bb521eec89741c42bc05cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5NDc3Nw==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440794777", "bodyText": "I have it in a different branch. I found this PR already quite big.", "author": "dawidwys", "createdAt": "2020-06-16T11:59:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc3MjgyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc3NjQ4Mg==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440776482", "bodyText": "nit: very good tests in general but can we rename the two suits to FieldAccessAfterCall and FieldAccessFromTable or similar to highlight the difference?", "author": "twalthr", "createdAt": "2020-06-16T11:22:32Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.expressions;\n+\n+import org.apache.flink.table.annotation.DataTypeHint;\n+import org.apache.flink.table.annotation.FunctionHint;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.functions.BuiltInFunctionDefinitions;\n+import org.apache.flink.table.functions.ScalarFunction;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Suite;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.singletonMap;\n+import static org.apache.flink.table.api.DataTypes.ARRAY;\n+import static org.apache.flink.table.api.DataTypes.BIGINT;\n+import static org.apache.flink.table.api.DataTypes.FIELD;\n+import static org.apache.flink.table.api.DataTypes.MAP;\n+import static org.apache.flink.table.api.DataTypes.ROW;\n+import static org.apache.flink.table.api.DataTypes.STRING;\n+import static org.apache.flink.table.api.Expressions.$;\n+import static org.apache.flink.table.api.Expressions.call;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for functions that access nested fields/elements of composite/collection types.\n+ */\n+@RunWith(Suite.class)\n+@Suite.SuiteClasses(\n+\t{\n+\t\tCompositeTypeAccessExpressionITCase.TableFieldAccess.class,\n+\t\tCompositeTypeAccessExpressionITCase.CallFieldAccess.class\n+\t}\n+)\n+public class CompositeTypeAccessExpressionITCase {\n+\n+\t/**\n+\t * Regular tests. See also {@link CallFieldAccess} for tests that access a nested field of an expression or\n+\t * for {@link BuiltInFunctionDefinitions#FLATTEN} which produces multiple columns from a single one.\n+\t */\n+\tpublic static class TableFieldAccess extends BuiltInFunctionTestBase {\n+\t\t@Parameterized.Parameters(name = \"{index}: {0}\")\n+\t\tpublic static List<TestSpec> testData() {\n+\t\t\treturn Arrays.asList(\n+\n+\t\t\t\t// Actually in case of SQL it does not use the GET method, but\n+\t\t\t\t// a custom logic for accessing nested fields of a Table.\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.GET)\n+\t\t\t\t\t.onFieldsWithData(null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t.testTableApiResult($(\"f0\").get(\"nested\"), null, BIGINT().nullable())\n+\t\t\t\t\t.testTableApiResult($(\"f1\").get(\"nested\"), 1L, BIGINT().notNull())\n+\t\t\t\t\t.testSqlResult(\"f0.nested\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f1.nested\", 1L, BIGINT().notNull()),\n+\n+\t\t\t\t// In Calcite it maps to FlinkSqlOperatorTable.ITEM\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.AT)\n+\t\t\t\t\t.onFieldsWithData(null, new int[] {1}, null, singletonMap(\"nested\", 1), null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tARRAY(BIGINT().notNull()).nullable(),\n+\t\t\t\t\t\tARRAY(BIGINT().notNull()).notNull(),\n+\t\t\t\t\t\tMAP(STRING(), BIGINT().notNull()).nullable(),\n+\t\t\t\t\t\tMAP(STRING(), BIGINT().notNull()).notNull(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t// accessing elements of MAP or ARRAY is a runtime operations,\n+\t\t\t\t\t// we do not know about the size or contents during the inference\n+\t\t\t\t\t// therefore the results are always nullable\n+\t\t\t\t\t.testSqlResult(\"f0[1]\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f1[1]\", 1L, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f2['nested']\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f3['nested']\", 1L, BIGINT().nullable())\n+\n+\t\t\t\t\t// we know all the fields of a type up front, therefore we can\n+\t\t\t\t\t// derive more accurate types during the inference\n+\t\t\t\t\t.testSqlResult(\"f4['nested']\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f5['nested']\", 1L, BIGINT().notNull())\n+\t\t\t);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A class for customized tests.\n+\t */\n+\tpublic static class CallFieldAccess {", "originalCommit": "b3dd92d0c41cee2472bb521eec89741c42bc05cd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "326222e8eb85270297876eba22ffda57ebe592c3", "url": "https://github.com/apache/flink/commit/326222e8eb85270297876eba22ffda57ebe592c3", "message": "Comments addressed", "committedDate": "2020-06-16T14:32:47Z", "type": "forcePushed"}, {"oid": "67905cf92819be26e49afbcf38794e340cca8e9c", "url": "https://github.com/apache/flink/commit/67905cf92819be26e49afbcf38794e340cca8e9c", "message": "Comments addressed", "committedDate": "2020-06-17T07:16:46Z", "type": "forcePushed"}, {"oid": "4588093f97836f4f04c7c2c9992ea5049bf71c4c", "url": "https://github.com/apache/flink/commit/4588093f97836f4f04c7c2c9992ea5049bf71c4c", "message": "Comments addressed", "committedDate": "2020-06-17T08:59:37Z", "type": "forcePushed"}, {"oid": "43596996d023eb47c9589f95d8baf8a3cee83016", "url": "https://github.com/apache/flink/commit/43596996d023eb47c9589f95d8baf8a3cee83016", "message": "[FLINK-18286] Implement type inference for GET/FLATTEN", "committedDate": "2020-06-18T09:16:36Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzUyODI4MA==", "url": "https://github.com/apache/flink/pull/12649#discussion_r443528280", "bodyText": "In Calcite, when the record type is nullable, the attributes are always nullable, so, this fix is not that necessary from the Calcite side.", "author": "danny0405", "createdAt": "2020-06-22T12:39:36Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/calcite/sql/fun/SqlDotOperator.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to you under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.calcite.sql.fun;\n+\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rel.type.RelDataTypeFactory;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.sql.SqlCall;\n+import org.apache.calcite.sql.SqlCallBinding;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlOperandCountRange;\n+import org.apache.calcite.sql.SqlOperatorBinding;\n+import org.apache.calcite.sql.SqlSpecialOperator;\n+import org.apache.calcite.sql.SqlUtil;\n+import org.apache.calcite.sql.SqlWriter;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+import org.apache.calcite.sql.type.OperandTypes;\n+import org.apache.calcite.sql.type.SqlOperandCountRanges;\n+import org.apache.calcite.sql.type.SqlSingleOperandTypeChecker;\n+import org.apache.calcite.sql.type.SqlTypeFamily;\n+import org.apache.calcite.sql.type.SqlTypeName;\n+import org.apache.calcite.sql.util.SqlBasicVisitor;\n+import org.apache.calcite.sql.util.SqlVisitor;\n+import org.apache.calcite.sql.validate.SqlValidator;\n+import org.apache.calcite.sql.validate.SqlValidatorScope;\n+import org.apache.calcite.sql.validate.SqlValidatorUtil;\n+import org.apache.calcite.util.Litmus;\n+import org.apache.calcite.util.Static;\n+\n+import java.util.Arrays;\n+\n+/**\n+ * The dot operator {@code .}, used to access a field of a\n+ * record. For example, {@code a.b}.\n+ *\n+ * <p>This class was copied over from Calcite to fix the derived type.\n+ * If the ROW is nullable force the accessed field to be nullable as well.\n+ */\n+public class SqlDotOperator extends SqlSpecialOperator {\n+  SqlDotOperator() {\n+    super(\"DOT\", SqlKind.DOT, 100, true, null, null, null);\n+  }\n+\n+  @Override public ReduceResult reduceExpr(int ordinal, TokenSequence list) {\n+    SqlNode left = list.node(ordinal - 1);\n+    SqlNode right = list.node(ordinal + 1);\n+    return new ReduceResult(ordinal - 1,\n+        ordinal + 2,\n+        createCall(\n+            SqlParserPos.sum(\n+                Arrays.asList(left.getParserPosition(),\n+                    right.getParserPosition(),\n+                    list.pos(ordinal))),\n+            left,\n+            right));\n+  }\n+\n+  @Override public void unparse(SqlWriter writer, SqlCall call, int leftPrec,\n+      int rightPrec) {\n+    final SqlWriter.Frame frame =\n+        writer.startList(SqlWriter.FrameTypeEnum.IDENTIFIER);\n+    call.operand(0).unparse(writer, leftPrec, 0);\n+    writer.sep(\".\");\n+    call.operand(1).unparse(writer, 0, 0);\n+    writer.endList(frame);\n+  }\n+\n+  @Override public SqlOperandCountRange getOperandCountRange() {\n+    return SqlOperandCountRanges.of(2);\n+  }\n+\n+  @Override public <R> void acceptCall(SqlVisitor<R> visitor, SqlCall call,\n+      boolean onlyExpressions, SqlBasicVisitor.ArgHandler<R> argHandler) {\n+    if (onlyExpressions) {\n+      // Do not visit operands[1] -- it is not an expression.\n+      argHandler.visitChild(visitor, call, 0, call.operand(0));\n+    } else {\n+      super.acceptCall(visitor, call, onlyExpressions, argHandler);\n+    }\n+  }\n+\n+  @Override public RelDataType deriveType(SqlValidator validator,\n+      SqlValidatorScope scope, SqlCall call) {\n+    final SqlNode operand = call.getOperandList().get(0);\n+    final RelDataType nodeType =\n+        validator.deriveType(scope, operand);\n+    assert nodeType != null;\n+    if (!nodeType.isStruct()) {\n+      throw SqlUtil.newContextException(operand.getParserPosition(),\n+          Static.RESOURCE.incompatibleTypes());\n+    }\n+\n+    final SqlNode fieldId = call.operand(1);\n+    final String fieldName = fieldId.toString();\n+    final RelDataTypeField field =\n+        nodeType.getField(fieldName, false, false);\n+    if (field == null) {\n+      throw SqlUtil.newContextException(fieldId.getParserPosition(),\n+          Static.RESOURCE.unknownField(fieldName));\n+    }\n+    RelDataType type = field.getType();\n+    if (nodeType.isNullable() && !type.isNullable()) {\n+        type = validator.getTypeFactory().createTypeWithNullability(type, true);\n+    }", "originalCommit": "43596996d023eb47c9589f95d8baf8a3cee83016", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "cdbf1943f1d1d221a527d8b1701213a25870c710", "url": "https://github.com/apache/flink/commit/cdbf1943f1d1d221a527d8b1701213a25870c710", "message": "[hotfix] Suppport ITEM for ROW types.", "committedDate": "2020-07-16T12:03:54Z", "type": "commit"}, {"oid": "4dd633a9c0621b97d075c386d77b69abfaf408fe", "url": "https://github.com/apache/flink/commit/4dd633a9c0621b97d075c386d77b69abfaf408fe", "message": "[FLINK-18286] Fix type inference for GET & AT Calcite functions\n\nThis commit fixes how Calcite infers/derives types for accessing nested columns of a ROW type.", "committedDate": "2020-07-16T12:08:19Z", "type": "commit"}, {"oid": "6ba85a29c018561157e8591f4c4713ea12ae5c5b", "url": "https://github.com/apache/flink/commit/6ba85a29c018561157e8591f4c4713ea12ae5c5b", "message": "[hotfix] Reuse a Flink cluster for expressions tests.", "committedDate": "2020-07-16T12:08:22Z", "type": "commit"}, {"oid": "93f58e49ad79331331d4d85bbe7720ecfe39f968", "url": "https://github.com/apache/flink/commit/93f58e49ad79331331d4d85bbe7720ecfe39f968", "message": "[FLINK-18286] Implement type inference for GET/FLATTEN", "committedDate": "2020-07-16T12:12:31Z", "type": "commit"}, {"oid": "93f58e49ad79331331d4d85bbe7720ecfe39f968", "url": "https://github.com/apache/flink/commit/93f58e49ad79331331d4d85bbe7720ecfe39f968", "message": "[FLINK-18286] Implement type inference for GET/FLATTEN", "committedDate": "2020-07-16T12:12:31Z", "type": "forcePushed"}]}