{"pr_number": 11962, "pr_title": "[FLINK-17462][format][csv] Support CSV serialization and deseriazation schema for RowData type", "pr_createdAt": "2020-04-30T12:56:25Z", "pr_url": "https://github.com/apache/flink/pull/11962", "timeline": [{"oid": "98481e8219841500289cabd8d3e7b1b0787207ea", "url": "https://github.com/apache/flink/commit/98481e8219841500289cabd8d3e7b1b0787207ea", "message": "[FLINK-17462][format][csv] Support CSV serialization and deseriazation schema for RowData type", "committedDate": "2020-04-30T12:54:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk5OTcyMw==", "url": "https://github.com/apache/flink/pull/11962#discussion_r417999723", "bodyText": "Just use jsonNode.isBoolean() ? jsonNode.asBoolean() : Boolean.parseBoolean(jsonNode.asText().trim())?\nSame below.", "author": "JingsongLi", "createdAt": "2020-04-30T13:14:49Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericArrayData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectReader;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ArrayNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.lang.reflect.Array;\n+import java.math.BigDecimal;\n+import java.sql.Date;\n+import java.sql.Time;\n+import java.sql.Timestamp;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+/**\n+ * Deserialization schema from CSV to Flink Table & SQL internal data structures.\n+ *\n+ * <p>Deserializes a <code>byte[]</code> message as a {@link JsonNode} and\n+ * converts it to {@link RowData}.\n+ *\n+ * <p>Failure during deserialization are forwarded as wrapped {@link IOException}s.\n+ */\n+@Internal\n+public final class CsvRowDataDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** Type information describing the result type. */\n+\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\n+\t/** Runtime instance that performs the actual work. */\n+\tprivate final DeserializationRuntimeConverter runtimeConverter;\n+\n+\t/** Schema describing the input CSV data. */\n+\tprivate final CsvSchema csvSchema;\n+\n+\t/** Object reader used to read rows. It is configured by {@link CsvSchema}. */\n+\tprivate final ObjectReader objectReader;\n+\n+\t/** Flag indicating whether to ignore invalid fields/rows (default: throw an exception). */\n+\tprivate final boolean ignoreParseErrors;\n+\n+\tprivate CsvRowDataDeserializationSchema(\n+\t\t\tRowType rowType,\n+\t\t\tTypeInformation<RowData> resultTypeInfo,\n+\t\t\tCsvSchema csvSchema,\n+\t\t\tboolean ignoreParseErrors) {\n+\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\tthis.runtimeConverter = createRowConverter(rowType, true);\n+\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\tthis.objectReader = new CsvMapper().readerFor(JsonNode.class).with(csvSchema);\n+\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t}\n+\n+\t/**\n+\t * A builder for creating a {@link CsvRowDataDeserializationSchema}.\n+\t */\n+\t@Internal\n+\tpublic static class Builder {\n+\n+\t\tprivate final RowType rowType;\n+\t\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\t\tprivate CsvSchema csvSchema;\n+\t\tprivate boolean ignoreParseErrors;\n+\n+\t\t/**\n+\t\t * Creates a CSV deserialization schema for the given {@link TypeInformation} with\n+\t\t * optional parameters.\n+\t\t */\n+\t\tpublic Builder(RowType rowType, TypeInformation<RowData> resultTypeInfo) {\n+\t\t\tPreconditions.checkNotNull(rowType, \"RowType must not be null.\");\n+\t\t\tPreconditions.checkNotNull(resultTypeInfo, \"Result type information must not be null.\");\n+\t\t\tthis.rowType = rowType;\n+\t\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t}\n+\n+\t\tpublic Builder setFieldDelimiter(char delimiter) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setColumnSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setAllowComments(boolean allowComments) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setAllowComments(allowComments).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setArrayElementDelimiter(String delimiter) {\n+\t\t\tPreconditions.checkNotNull(delimiter, \"Array element delimiter must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setArrayElementSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setQuoteCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setQuoteChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setEscapeCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setEscapeChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setNullLiteral(String nullLiteral) {\n+\t\t\tPreconditions.checkNotNull(nullLiteral, \"Null literal must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setNullValue(nullLiteral).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setIgnoreParseErrors(boolean ignoreParseErrors) {\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic CsvRowDataDeserializationSchema build() {\n+\t\t\treturn new CsvRowDataDeserializationSchema(\n+\t\t\t\trowType,\n+\t\t\t\tresultTypeInfo,\n+\t\t\t\tcsvSchema,\n+\t\t\t\tignoreParseErrors);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\ttry {\n+\t\t\tfinal JsonNode root = objectReader.readValue(message);\n+\t\t\treturn (RowData) runtimeConverter.convert(root);\n+\t\t} catch (Throwable t) {\n+\t\t\tif (ignoreParseErrors) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\tthrow new IOException(\"Failed to deserialize CSV row '\" + new String(message) + \"'.\", t);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn resultTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || o.getClass() != this.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tfinal CsvRowDataDeserializationSchema that = (CsvRowDataDeserializationSchema) o;\n+\t\tfinal CsvSchema otherSchema = that.csvSchema;\n+\n+\t\treturn resultTypeInfo.equals(that.resultTypeInfo) &&\n+\t\t\tignoreParseErrors == that.ignoreParseErrors &&\n+\t\t\tcsvSchema.getColumnSeparator() == otherSchema.getColumnSeparator() &&\n+\t\t\tcsvSchema.allowsComments() == otherSchema.allowsComments() &&\n+\t\t\tcsvSchema.getArrayElementSeparator().equals(otherSchema.getArrayElementSeparator()) &&\n+\t\t\tcsvSchema.getQuoteChar() == otherSchema.getQuoteChar() &&\n+\t\t\tcsvSchema.getEscapeChar() == otherSchema.getEscapeChar() &&\n+\t\t\tArrays.equals(csvSchema.getNullValue(), otherSchema.getNullValue());\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(\n+\t\t\tresultTypeInfo,\n+\t\t\tignoreParseErrors,\n+\t\t\tcsvSchema.getColumnSeparator(),\n+\t\t\tcsvSchema.allowsComments(),\n+\t\t\tcsvSchema.getArrayElementSeparator(),\n+\t\t\tcsvSchema.getQuoteChar(),\n+\t\t\tcsvSchema.getEscapeChar(),\n+\t\t\tcsvSchema.getNullValue());\n+\t}\n+\n+\t// -------------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// -------------------------------------------------------------------------------------\n+\n+\t// -------------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// -------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that converts {@link JsonNode}s into objects of Flink Table & SQL\n+\t * internal data structures.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\t\tObject convert(JsonNode jsonNode);\n+\t}\n+\n+\tprivate DeserializationRuntimeConverter createRowConverter(RowType rowType, boolean isTopLevel) {\n+\t\tfinal DeserializationRuntimeConverter[] fieldConverters = rowType.getFields().stream()\n+\t\t\t.map(RowType.RowField::getType)\n+\t\t\t.map(this::createNullableConverter)\n+\t\t\t.toArray(DeserializationRuntimeConverter[]::new);\n+\t\tfinal String[] fieldNames = rowType.getFieldNames().toArray(new String[0]);\n+\t\tfinal int arity = fieldNames.length;\n+\n+\t\treturn jsonNode -> {\n+\t\t\tint nodeSize = jsonNode.size();\n+\n+\t\t\tvalidateArity(arity, nodeSize, ignoreParseErrors);\n+\n+\t\t\tGenericRowData row = new GenericRowData(arity);\n+\t\t\tfor (int i = 0; i < arity; i++) {\n+\t\t\t\tJsonNode field;\n+\t\t\t\t// Jackson only supports mapping by name in the first level\n+\t\t\t\tif (isTopLevel) {\n+\t\t\t\t\tfield = jsonNode.get(fieldNames[i]);\n+\t\t\t\t} else {\n+\t\t\t\t\tfield = jsonNode.get(i);\n+\t\t\t\t}\n+\t\t\t\tif (field == null) {\n+\t\t\t\t\trow.setField(i, null);\n+\t\t\t\t} else {\n+\t\t\t\t\trow.setField(i, fieldConverters[i].convert(field));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn row;\n+\t\t};\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter which is null safe.\n+\t */\n+\tprivate DeserializationRuntimeConverter createNullableConverter(LogicalType type) {\n+\t\tfinal DeserializationRuntimeConverter converter = createConverter(type);\n+\t\treturn jsonNode -> {\n+\t\t\tif (jsonNode == null || jsonNode.isNull()) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\treturn converter.convert(jsonNode);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!ignoreParseErrors) {\n+\t\t\t\t\tthrow t;\n+\t\t\t\t}\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter which assuming input object is not null.\n+\t */\n+\tprivate DeserializationRuntimeConverter createConverter(LogicalType type) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase NULL:\n+\t\t\t\treturn jsonNode -> null;\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn this::convertToBoolean;\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn jsonNode -> Byte.parseByte(jsonNode.asText().trim());\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn jsonNode -> Short.parseShort(jsonNode.asText().trim());\n+\t\t\tcase INTEGER:\n+\t\t\tcase INTERVAL_YEAR_MONTH:\n+\t\t\t\treturn this::convertToInt;\n+\t\t\tcase BIGINT:\n+\t\t\tcase INTERVAL_DAY_TIME:\n+\t\t\t\treturn this::convertToLong;\n+\t\t\tcase DATE:\n+\t\t\t\treturn this::convertToDate;\n+\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n+\t\t\t\treturn this::convertToTime;\n+\t\t\tcase TIMESTAMP_WITH_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n+\t\t\t\treturn this::convertToTimestamp;\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn this::convertToFloat;\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn this::convertToDouble;\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn this::convertToString;\n+\t\t\tcase BINARY:\n+\t\t\tcase VARBINARY:\n+\t\t\t\treturn this::convertToBytes;\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn createDecimalConverter((DecimalType) type);\n+\t\t\tcase ARRAY:\n+\t\t\t\treturn createArrayConverter((ArrayType) type);\n+\t\t\tcase ROW:\n+\t\t\t\treturn createRowConverter((RowType) type, false);\n+\t\t\tcase MAP:\n+\t\t\tcase MULTISET:\n+\t\t\tcase RAW:\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"Unsupported type: \" + type);\n+\t\t}\n+\t}\n+\n+\tprivate boolean convertToBoolean(JsonNode jsonNode) {\n+\t\tif (jsonNode.isBoolean()) {", "originalCommit": "98481e8219841500289cabd8d3e7b1b0787207ea", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNTc2Ng==", "url": "https://github.com/apache/flink/pull/11962#discussion_r418125766", "bodyText": "Sorry, I don't think such a long line code is better for readability and performance. And it's not possible to add inline comment.", "author": "wuchong", "createdAt": "2020-04-30T16:11:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk5OTcyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk5OTg4OQ==", "url": "https://github.com/apache/flink/pull/11962#discussion_r417999889", "bodyText": "Why we can't use node.asBoolean() directly?", "author": "JingsongLi", "createdAt": "2020-04-30T13:15:06Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericArrayData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectReader;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ArrayNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.lang.reflect.Array;\n+import java.math.BigDecimal;\n+import java.sql.Date;\n+import java.sql.Time;\n+import java.sql.Timestamp;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+/**\n+ * Deserialization schema from CSV to Flink Table & SQL internal data structures.\n+ *\n+ * <p>Deserializes a <code>byte[]</code> message as a {@link JsonNode} and\n+ * converts it to {@link RowData}.\n+ *\n+ * <p>Failure during deserialization are forwarded as wrapped {@link IOException}s.\n+ */\n+@Internal\n+public final class CsvRowDataDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** Type information describing the result type. */\n+\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\n+\t/** Runtime instance that performs the actual work. */\n+\tprivate final DeserializationRuntimeConverter runtimeConverter;\n+\n+\t/** Schema describing the input CSV data. */\n+\tprivate final CsvSchema csvSchema;\n+\n+\t/** Object reader used to read rows. It is configured by {@link CsvSchema}. */\n+\tprivate final ObjectReader objectReader;\n+\n+\t/** Flag indicating whether to ignore invalid fields/rows (default: throw an exception). */\n+\tprivate final boolean ignoreParseErrors;\n+\n+\tprivate CsvRowDataDeserializationSchema(\n+\t\t\tRowType rowType,\n+\t\t\tTypeInformation<RowData> resultTypeInfo,\n+\t\t\tCsvSchema csvSchema,\n+\t\t\tboolean ignoreParseErrors) {\n+\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\tthis.runtimeConverter = createRowConverter(rowType, true);\n+\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\tthis.objectReader = new CsvMapper().readerFor(JsonNode.class).with(csvSchema);\n+\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t}\n+\n+\t/**\n+\t * A builder for creating a {@link CsvRowDataDeserializationSchema}.\n+\t */\n+\t@Internal\n+\tpublic static class Builder {\n+\n+\t\tprivate final RowType rowType;\n+\t\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\t\tprivate CsvSchema csvSchema;\n+\t\tprivate boolean ignoreParseErrors;\n+\n+\t\t/**\n+\t\t * Creates a CSV deserialization schema for the given {@link TypeInformation} with\n+\t\t * optional parameters.\n+\t\t */\n+\t\tpublic Builder(RowType rowType, TypeInformation<RowData> resultTypeInfo) {\n+\t\t\tPreconditions.checkNotNull(rowType, \"RowType must not be null.\");\n+\t\t\tPreconditions.checkNotNull(resultTypeInfo, \"Result type information must not be null.\");\n+\t\t\tthis.rowType = rowType;\n+\t\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t}\n+\n+\t\tpublic Builder setFieldDelimiter(char delimiter) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setColumnSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setAllowComments(boolean allowComments) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setAllowComments(allowComments).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setArrayElementDelimiter(String delimiter) {\n+\t\t\tPreconditions.checkNotNull(delimiter, \"Array element delimiter must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setArrayElementSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setQuoteCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setQuoteChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setEscapeCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setEscapeChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setNullLiteral(String nullLiteral) {\n+\t\t\tPreconditions.checkNotNull(nullLiteral, \"Null literal must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setNullValue(nullLiteral).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setIgnoreParseErrors(boolean ignoreParseErrors) {\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic CsvRowDataDeserializationSchema build() {\n+\t\t\treturn new CsvRowDataDeserializationSchema(\n+\t\t\t\trowType,\n+\t\t\t\tresultTypeInfo,\n+\t\t\t\tcsvSchema,\n+\t\t\t\tignoreParseErrors);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\ttry {\n+\t\t\tfinal JsonNode root = objectReader.readValue(message);\n+\t\t\treturn (RowData) runtimeConverter.convert(root);\n+\t\t} catch (Throwable t) {\n+\t\t\tif (ignoreParseErrors) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\tthrow new IOException(\"Failed to deserialize CSV row '\" + new String(message) + \"'.\", t);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn resultTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || o.getClass() != this.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tfinal CsvRowDataDeserializationSchema that = (CsvRowDataDeserializationSchema) o;\n+\t\tfinal CsvSchema otherSchema = that.csvSchema;\n+\n+\t\treturn resultTypeInfo.equals(that.resultTypeInfo) &&\n+\t\t\tignoreParseErrors == that.ignoreParseErrors &&\n+\t\t\tcsvSchema.getColumnSeparator() == otherSchema.getColumnSeparator() &&\n+\t\t\tcsvSchema.allowsComments() == otherSchema.allowsComments() &&\n+\t\t\tcsvSchema.getArrayElementSeparator().equals(otherSchema.getArrayElementSeparator()) &&\n+\t\t\tcsvSchema.getQuoteChar() == otherSchema.getQuoteChar() &&\n+\t\t\tcsvSchema.getEscapeChar() == otherSchema.getEscapeChar() &&\n+\t\t\tArrays.equals(csvSchema.getNullValue(), otherSchema.getNullValue());\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(\n+\t\t\tresultTypeInfo,\n+\t\t\tignoreParseErrors,\n+\t\t\tcsvSchema.getColumnSeparator(),\n+\t\t\tcsvSchema.allowsComments(),\n+\t\t\tcsvSchema.getArrayElementSeparator(),\n+\t\t\tcsvSchema.getQuoteChar(),\n+\t\t\tcsvSchema.getEscapeChar(),\n+\t\t\tcsvSchema.getNullValue());\n+\t}\n+\n+\t// -------------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// -------------------------------------------------------------------------------------\n+\n+\t// -------------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// -------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that converts {@link JsonNode}s into objects of Flink Table & SQL\n+\t * internal data structures.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\t\tObject convert(JsonNode jsonNode);\n+\t}\n+\n+\tprivate DeserializationRuntimeConverter createRowConverter(RowType rowType, boolean isTopLevel) {\n+\t\tfinal DeserializationRuntimeConverter[] fieldConverters = rowType.getFields().stream()\n+\t\t\t.map(RowType.RowField::getType)\n+\t\t\t.map(this::createNullableConverter)\n+\t\t\t.toArray(DeserializationRuntimeConverter[]::new);\n+\t\tfinal String[] fieldNames = rowType.getFieldNames().toArray(new String[0]);\n+\t\tfinal int arity = fieldNames.length;\n+\n+\t\treturn jsonNode -> {\n+\t\t\tint nodeSize = jsonNode.size();\n+\n+\t\t\tvalidateArity(arity, nodeSize, ignoreParseErrors);\n+\n+\t\t\tGenericRowData row = new GenericRowData(arity);\n+\t\t\tfor (int i = 0; i < arity; i++) {\n+\t\t\t\tJsonNode field;\n+\t\t\t\t// Jackson only supports mapping by name in the first level\n+\t\t\t\tif (isTopLevel) {\n+\t\t\t\t\tfield = jsonNode.get(fieldNames[i]);\n+\t\t\t\t} else {\n+\t\t\t\t\tfield = jsonNode.get(i);\n+\t\t\t\t}\n+\t\t\t\tif (field == null) {\n+\t\t\t\t\trow.setField(i, null);\n+\t\t\t\t} else {\n+\t\t\t\t\trow.setField(i, fieldConverters[i].convert(field));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn row;\n+\t\t};\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter which is null safe.\n+\t */\n+\tprivate DeserializationRuntimeConverter createNullableConverter(LogicalType type) {\n+\t\tfinal DeserializationRuntimeConverter converter = createConverter(type);\n+\t\treturn jsonNode -> {\n+\t\t\tif (jsonNode == null || jsonNode.isNull()) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\treturn converter.convert(jsonNode);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!ignoreParseErrors) {\n+\t\t\t\t\tthrow t;\n+\t\t\t\t}\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter which assuming input object is not null.\n+\t */\n+\tprivate DeserializationRuntimeConverter createConverter(LogicalType type) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase NULL:\n+\t\t\t\treturn jsonNode -> null;\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn this::convertToBoolean;\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn jsonNode -> Byte.parseByte(jsonNode.asText().trim());\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn jsonNode -> Short.parseShort(jsonNode.asText().trim());\n+\t\t\tcase INTEGER:\n+\t\t\tcase INTERVAL_YEAR_MONTH:\n+\t\t\t\treturn this::convertToInt;\n+\t\t\tcase BIGINT:\n+\t\t\tcase INTERVAL_DAY_TIME:\n+\t\t\t\treturn this::convertToLong;\n+\t\t\tcase DATE:\n+\t\t\t\treturn this::convertToDate;\n+\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n+\t\t\t\treturn this::convertToTime;\n+\t\t\tcase TIMESTAMP_WITH_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n+\t\t\t\treturn this::convertToTimestamp;\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn this::convertToFloat;\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn this::convertToDouble;\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn this::convertToString;\n+\t\t\tcase BINARY:\n+\t\t\tcase VARBINARY:\n+\t\t\t\treturn this::convertToBytes;\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn createDecimalConverter((DecimalType) type);\n+\t\t\tcase ARRAY:\n+\t\t\t\treturn createArrayConverter((ArrayType) type);\n+\t\t\tcase ROW:\n+\t\t\t\treturn createRowConverter((RowType) type, false);\n+\t\t\tcase MAP:\n+\t\t\tcase MULTISET:\n+\t\t\tcase RAW:\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"Unsupported type: \" + type);\n+\t\t}\n+\t}\n+\n+\tprivate boolean convertToBoolean(JsonNode jsonNode) {\n+\t\tif (jsonNode.isBoolean()) {\n+\t\t\t// avoid redundant toString and parseBoolean, for better performance", "originalCommit": "98481e8219841500289cabd8d3e7b1b0787207ea", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNDcxOA==", "url": "https://github.com/apache/flink/pull/11962#discussion_r418124718", "bodyText": "We want to support this case {\"f0\": \"true\"} where the value is a string node, not boolean node. This is also how CsvRowDeserializationSchema process boolean and numeric types. I just add a fast path (avoid toString and parse if it's a boolean node).", "author": "wuchong", "createdAt": "2020-04-30T16:09:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk5OTg4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTkyNTQ0MA==", "url": "https://github.com/apache/flink/pull/11962#discussion_r419925440", "bodyText": "TextNode also has a correct implementation for asBoolean.", "author": "JingsongLi", "createdAt": "2020-05-05T07:53:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk5OTg4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA2NzkzNA==", "url": "https://github.com/apache/flink/pull/11962#discussion_r420067934", "bodyText": "The problem of asBoolean, asLong is that they eat the parse exception and fall back to the default value (0 for Long, and false for Boolean). But we want to keep the parse excepton to let users be aware of it, and can use ignore-parse-error option to turn it off.", "author": "wuchong", "createdAt": "2020-05-05T12:24:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk5OTg4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODAwMTk2OQ==", "url": "https://github.com/apache/flink/pull/11962#discussion_r418001969", "bodyText": "Loose mills precision here, use toNanoOfDay?\nBTW, add tests for this.", "author": "JingsongLi", "createdAt": "2020-04-30T13:18:12Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericArrayData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectReader;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ArrayNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.lang.reflect.Array;\n+import java.math.BigDecimal;\n+import java.sql.Date;\n+import java.sql.Time;\n+import java.sql.Timestamp;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+/**\n+ * Deserialization schema from CSV to Flink Table & SQL internal data structures.\n+ *\n+ * <p>Deserializes a <code>byte[]</code> message as a {@link JsonNode} and\n+ * converts it to {@link RowData}.\n+ *\n+ * <p>Failure during deserialization are forwarded as wrapped {@link IOException}s.\n+ */\n+@Internal\n+public final class CsvRowDataDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** Type information describing the result type. */\n+\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\n+\t/** Runtime instance that performs the actual work. */\n+\tprivate final DeserializationRuntimeConverter runtimeConverter;\n+\n+\t/** Schema describing the input CSV data. */\n+\tprivate final CsvSchema csvSchema;\n+\n+\t/** Object reader used to read rows. It is configured by {@link CsvSchema}. */\n+\tprivate final ObjectReader objectReader;\n+\n+\t/** Flag indicating whether to ignore invalid fields/rows (default: throw an exception). */\n+\tprivate final boolean ignoreParseErrors;\n+\n+\tprivate CsvRowDataDeserializationSchema(\n+\t\t\tRowType rowType,\n+\t\t\tTypeInformation<RowData> resultTypeInfo,\n+\t\t\tCsvSchema csvSchema,\n+\t\t\tboolean ignoreParseErrors) {\n+\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\tthis.runtimeConverter = createRowConverter(rowType, true);\n+\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\tthis.objectReader = new CsvMapper().readerFor(JsonNode.class).with(csvSchema);\n+\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t}\n+\n+\t/**\n+\t * A builder for creating a {@link CsvRowDataDeserializationSchema}.\n+\t */\n+\t@Internal\n+\tpublic static class Builder {\n+\n+\t\tprivate final RowType rowType;\n+\t\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\t\tprivate CsvSchema csvSchema;\n+\t\tprivate boolean ignoreParseErrors;\n+\n+\t\t/**\n+\t\t * Creates a CSV deserialization schema for the given {@link TypeInformation} with\n+\t\t * optional parameters.\n+\t\t */\n+\t\tpublic Builder(RowType rowType, TypeInformation<RowData> resultTypeInfo) {\n+\t\t\tPreconditions.checkNotNull(rowType, \"RowType must not be null.\");\n+\t\t\tPreconditions.checkNotNull(resultTypeInfo, \"Result type information must not be null.\");\n+\t\t\tthis.rowType = rowType;\n+\t\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t}\n+\n+\t\tpublic Builder setFieldDelimiter(char delimiter) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setColumnSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setAllowComments(boolean allowComments) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setAllowComments(allowComments).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setArrayElementDelimiter(String delimiter) {\n+\t\t\tPreconditions.checkNotNull(delimiter, \"Array element delimiter must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setArrayElementSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setQuoteCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setQuoteChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setEscapeCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setEscapeChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setNullLiteral(String nullLiteral) {\n+\t\t\tPreconditions.checkNotNull(nullLiteral, \"Null literal must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setNullValue(nullLiteral).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setIgnoreParseErrors(boolean ignoreParseErrors) {\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic CsvRowDataDeserializationSchema build() {\n+\t\t\treturn new CsvRowDataDeserializationSchema(\n+\t\t\t\trowType,\n+\t\t\t\tresultTypeInfo,\n+\t\t\t\tcsvSchema,\n+\t\t\t\tignoreParseErrors);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\ttry {\n+\t\t\tfinal JsonNode root = objectReader.readValue(message);\n+\t\t\treturn (RowData) runtimeConverter.convert(root);\n+\t\t} catch (Throwable t) {\n+\t\t\tif (ignoreParseErrors) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\tthrow new IOException(\"Failed to deserialize CSV row '\" + new String(message) + \"'.\", t);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn resultTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || o.getClass() != this.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tfinal CsvRowDataDeserializationSchema that = (CsvRowDataDeserializationSchema) o;\n+\t\tfinal CsvSchema otherSchema = that.csvSchema;\n+\n+\t\treturn resultTypeInfo.equals(that.resultTypeInfo) &&\n+\t\t\tignoreParseErrors == that.ignoreParseErrors &&\n+\t\t\tcsvSchema.getColumnSeparator() == otherSchema.getColumnSeparator() &&\n+\t\t\tcsvSchema.allowsComments() == otherSchema.allowsComments() &&\n+\t\t\tcsvSchema.getArrayElementSeparator().equals(otherSchema.getArrayElementSeparator()) &&\n+\t\t\tcsvSchema.getQuoteChar() == otherSchema.getQuoteChar() &&\n+\t\t\tcsvSchema.getEscapeChar() == otherSchema.getEscapeChar() &&\n+\t\t\tArrays.equals(csvSchema.getNullValue(), otherSchema.getNullValue());\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(\n+\t\t\tresultTypeInfo,\n+\t\t\tignoreParseErrors,\n+\t\t\tcsvSchema.getColumnSeparator(),\n+\t\t\tcsvSchema.allowsComments(),\n+\t\t\tcsvSchema.getArrayElementSeparator(),\n+\t\t\tcsvSchema.getQuoteChar(),\n+\t\t\tcsvSchema.getEscapeChar(),\n+\t\t\tcsvSchema.getNullValue());\n+\t}\n+\n+\t// -------------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// -------------------------------------------------------------------------------------\n+\n+\t// -------------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// -------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that converts {@link JsonNode}s into objects of Flink Table & SQL\n+\t * internal data structures.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\t\tObject convert(JsonNode jsonNode);\n+\t}\n+\n+\tprivate DeserializationRuntimeConverter createRowConverter(RowType rowType, boolean isTopLevel) {\n+\t\tfinal DeserializationRuntimeConverter[] fieldConverters = rowType.getFields().stream()\n+\t\t\t.map(RowType.RowField::getType)\n+\t\t\t.map(this::createNullableConverter)\n+\t\t\t.toArray(DeserializationRuntimeConverter[]::new);\n+\t\tfinal String[] fieldNames = rowType.getFieldNames().toArray(new String[0]);\n+\t\tfinal int arity = fieldNames.length;\n+\n+\t\treturn jsonNode -> {\n+\t\t\tint nodeSize = jsonNode.size();\n+\n+\t\t\tvalidateArity(arity, nodeSize, ignoreParseErrors);\n+\n+\t\t\tGenericRowData row = new GenericRowData(arity);\n+\t\t\tfor (int i = 0; i < arity; i++) {\n+\t\t\t\tJsonNode field;\n+\t\t\t\t// Jackson only supports mapping by name in the first level\n+\t\t\t\tif (isTopLevel) {\n+\t\t\t\t\tfield = jsonNode.get(fieldNames[i]);\n+\t\t\t\t} else {\n+\t\t\t\t\tfield = jsonNode.get(i);\n+\t\t\t\t}\n+\t\t\t\tif (field == null) {\n+\t\t\t\t\trow.setField(i, null);\n+\t\t\t\t} else {\n+\t\t\t\t\trow.setField(i, fieldConverters[i].convert(field));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn row;\n+\t\t};\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter which is null safe.\n+\t */\n+\tprivate DeserializationRuntimeConverter createNullableConverter(LogicalType type) {\n+\t\tfinal DeserializationRuntimeConverter converter = createConverter(type);\n+\t\treturn jsonNode -> {\n+\t\t\tif (jsonNode == null || jsonNode.isNull()) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\treturn converter.convert(jsonNode);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tif (!ignoreParseErrors) {\n+\t\t\t\t\tthrow t;\n+\t\t\t\t}\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter which assuming input object is not null.\n+\t */\n+\tprivate DeserializationRuntimeConverter createConverter(LogicalType type) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase NULL:\n+\t\t\t\treturn jsonNode -> null;\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn this::convertToBoolean;\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn jsonNode -> Byte.parseByte(jsonNode.asText().trim());\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn jsonNode -> Short.parseShort(jsonNode.asText().trim());\n+\t\t\tcase INTEGER:\n+\t\t\tcase INTERVAL_YEAR_MONTH:\n+\t\t\t\treturn this::convertToInt;\n+\t\t\tcase BIGINT:\n+\t\t\tcase INTERVAL_DAY_TIME:\n+\t\t\t\treturn this::convertToLong;\n+\t\t\tcase DATE:\n+\t\t\t\treturn this::convertToDate;\n+\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n+\t\t\t\treturn this::convertToTime;\n+\t\t\tcase TIMESTAMP_WITH_TIME_ZONE:\n+\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n+\t\t\t\treturn this::convertToTimestamp;\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn this::convertToFloat;\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn this::convertToDouble;\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn this::convertToString;\n+\t\t\tcase BINARY:\n+\t\t\tcase VARBINARY:\n+\t\t\t\treturn this::convertToBytes;\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn createDecimalConverter((DecimalType) type);\n+\t\t\tcase ARRAY:\n+\t\t\t\treturn createArrayConverter((ArrayType) type);\n+\t\t\tcase ROW:\n+\t\t\t\treturn createRowConverter((RowType) type, false);\n+\t\t\tcase MAP:\n+\t\t\tcase MULTISET:\n+\t\t\tcase RAW:\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"Unsupported type: \" + type);\n+\t\t}\n+\t}\n+\n+\tprivate boolean convertToBoolean(JsonNode jsonNode) {\n+\t\tif (jsonNode.isBoolean()) {\n+\t\t\t// avoid redundant toString and parseBoolean, for better performance\n+\t\t\treturn jsonNode.asBoolean();\n+\t\t} else {\n+\t\t\treturn Boolean.parseBoolean(jsonNode.asText().trim());\n+\t\t}\n+\t}\n+\n+\tprivate int convertToInt(JsonNode jsonNode) {\n+\t\tif (jsonNode.canConvertToInt()) {\n+\t\t\t// avoid redundant toString and parseInt, for better performance\n+\t\t\treturn jsonNode.asInt();\n+\t\t} else {\n+\t\t\treturn Integer.parseInt(jsonNode.asText().trim());\n+\t\t}\n+\t}\n+\n+\tprivate long convertToLong(JsonNode jsonNode) {\n+\t\tif (jsonNode.canConvertToLong()) {\n+\t\t\t// avoid redundant toString and parseLong, for better performance\n+\t\t\treturn jsonNode.asLong();\n+\t\t} else {\n+\t\t\treturn Long.parseLong(jsonNode.asText().trim());\n+\t\t}\n+\t}\n+\n+\tprivate double convertToDouble(JsonNode jsonNode) {\n+\t\tif (jsonNode.isDouble()) {\n+\t\t\t// avoid redundant toString and parseDouble, for better performance\n+\t\t\treturn jsonNode.asDouble();\n+\t\t} else {\n+\t\t\treturn Double.parseDouble(jsonNode.asText().trim());\n+\t\t}\n+\t}\n+\n+\tprivate float convertToFloat(JsonNode jsonNode) {\n+\t\tif (jsonNode.isDouble()) {\n+\t\t\t// avoid redundant toString and parseDouble, for better performance\n+\t\t\treturn (float) jsonNode.asDouble();\n+\t\t} else {\n+\t\t\treturn Float.parseFloat(jsonNode.asText().trim());\n+\t\t}\n+\t}\n+\n+\tprivate int convertToDate(JsonNode jsonNode) {\n+\t\t// csv currently is using Date.valueOf() to parse date string\n+\t\treturn (int) Date.valueOf(jsonNode.asText()).toLocalDate().toEpochDay();\n+\t}\n+\n+\tprivate int convertToTime(JsonNode jsonNode) {\n+\t\t// csv currently is using Time.valueOf() to parse time string\n+\t\tLocalTime localTime = Time.valueOf(jsonNode.asText()).toLocalTime();\n+\t\t// get number of milliseconds of the day\n+\t\treturn localTime.toSecondOfDay() * 1000;", "originalCommit": "98481e8219841500289cabd8d3e7b1b0787207ea", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNDcwNg==", "url": "https://github.com/apache/flink/pull/11962#discussion_r418124706", "bodyText": "This is also aligned with previous behavior. Time.valueOf() doesn't support to parse milli/nano seconds.", "author": "wuchong", "createdAt": "2020-04-30T16:09:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODAwMTk2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTkyMzk3NQ==", "url": "https://github.com/apache/flink/pull/11962#discussion_r419923975", "bodyText": "Previous behavior is because old planner only support seconds, but blink support mills and more.\nWe are using LocalTime instead of Time now. If you don't want change it you should add a TODO and create a JIRA.", "author": "JingsongLi", "createdAt": "2020-05-05T07:50:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODAwMTk2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MDcxOA==", "url": "https://github.com/apache/flink/pull/11962#discussion_r420070718", "bodyText": "Thanks @JingsongLi , I created FLINK-17525 to track this.", "author": "wuchong", "createdAt": "2020-05-05T12:29:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODAwMTk2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODAxMDg4OQ==", "url": "https://github.com/apache/flink/pull/11962#discussion_r418010889", "bodyText": "I think you should use something like ParquetRowDataWriter.FieldWriter.", "author": "JingsongLi", "createdAt": "2020-04-30T13:31:13Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataSerializationSchema.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.annotation.PublicEvolving;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectWriter;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ArrayNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ContainerNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ObjectNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.Serializable;\n+import java.math.BigDecimal;\n+import java.time.LocalDate;\n+import java.time.LocalTime;\n+import java.time.format.DateTimeFormatter;\n+import java.time.format.DateTimeFormatterBuilder;\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+import static java.time.format.DateTimeFormatter.ISO_LOCAL_DATE;\n+import static java.time.format.DateTimeFormatter.ISO_LOCAL_TIME;\n+\n+/**\n+ * Serialization schema that serializes an object of Flink Table & SQL internal data structure\n+ * into a CSV bytes.\n+ *\n+ * <p>Serializes the input row into a {@link JsonNode} and\n+ * converts it into <code>byte[]</code>.\n+ *\n+ * <p>Result <code>byte[]</code> messages can be deserialized using {@link CsvRowDataDeserializationSchema}.\n+ */\n+@PublicEvolving\n+public final class CsvRowDataSerializationSchema implements SerializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/** Logical row type describing the input CSV data. */\n+\tprivate final RowType rowType;\n+\n+\t/** Runtime instance that performs the actual work. */\n+\tprivate final SerializationRuntimeConverter runtimeConverter;\n+\n+\t/** CsvMapper used to write {@link JsonNode} into bytes. */\n+\tprivate final CsvMapper csvMapper;\n+\n+\t/** Schema describing the input CSV data. */\n+\tprivate final CsvSchema csvSchema;\n+\n+\t/** Object writer used to write rows. It is configured by {@link CsvSchema}. */\n+\tprivate final ObjectWriter objectWriter;\n+\n+\t/** Reusable object node. */\n+\tprivate transient ObjectNode root;\n+\n+\tprivate CsvRowDataSerializationSchema(\n+\t\t\tRowType rowType,\n+\t\t\tCsvSchema csvSchema) {\n+\t\tthis.rowType = rowType;\n+\t\tthis.runtimeConverter = createRowConverter(rowType, true);\n+\t\tthis.csvMapper = new CsvMapper();\n+\t\tthis.csvSchema = csvSchema;\n+\t\tthis.objectWriter = csvMapper.writer(csvSchema);\n+\t}\n+\n+\t/**\n+\t * A builder for creating a {@link CsvRowDataSerializationSchema}.\n+\t */\n+\t@PublicEvolving\n+\tpublic static class Builder {\n+\n+\t\tprivate final RowType rowType;\n+\t\tprivate CsvSchema csvSchema;\n+\n+\t\t/**\n+\t\t * Creates a {@link CsvRowDataSerializationSchema} expecting the given {@link RowType}.\n+\t\t *\n+\t\t * @param rowType logical row type used to create schema.\n+\t\t */\n+\t\tpublic Builder(RowType rowType) {\n+\t\t\tPreconditions.checkNotNull(rowType, \"Row type must not be null.\");\n+\n+\t\t\tthis.rowType = rowType;\n+\t\t\tthis.csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t}\n+\n+\t\tpublic Builder setFieldDelimiter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setColumnSeparator(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setLineDelimiter(String delimiter) {\n+\t\t\tPreconditions.checkNotNull(delimiter, \"Delimiter must not be null.\");\n+\t\t\tif (!delimiter.equals(\"\\n\") && !delimiter.equals(\"\\r\") && !delimiter.equals(\"\\r\\n\") && !delimiter.equals(\"\")) {\n+\t\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\t\"Unsupported new line delimiter. Only \\\\n, \\\\r, \\\\r\\\\n, or empty string are supported.\");\n+\t\t\t}\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setLineSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setArrayElementDelimiter(String delimiter) {\n+\t\t\tPreconditions.checkNotNull(delimiter, \"Delimiter must not be null.\");\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setArrayElementSeparator(delimiter).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder disableQuoteCharacter() {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().disableQuoteChar().build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setQuoteCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setQuoteChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setEscapeCharacter(char c) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setEscapeChar(c).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic Builder setNullLiteral(String s) {\n+\t\t\tthis.csvSchema = this.csvSchema.rebuild().setNullValue(s).build();\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tpublic CsvRowDataSerializationSchema build() {\n+\t\t\treturn new CsvRowDataSerializationSchema(\n+\t\t\t\trowType,\n+\t\t\t\tcsvSchema);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(RowData row) {\n+\t\tif (root == null) {\n+\t\t\troot = csvMapper.createObjectNode();\n+\t\t}\n+\t\ttry {\n+\t\t\truntimeConverter.convert(csvMapper, root, row);\n+\t\t\treturn objectWriter.writeValueAsBytes(root);\n+\t\t} catch (Throwable t) {\n+\t\t\tthrow new RuntimeException(\"Could not serialize row '\" + row + \"'.\", t);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (o == null || o.getClass() != this.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tfinal CsvRowDataSerializationSchema that = (CsvRowDataSerializationSchema) o;\n+\t\tfinal CsvSchema otherSchema = that.csvSchema;\n+\n+\t\treturn rowType.equals(that.rowType) &&\n+\t\t\tcsvSchema.getColumnSeparator() == otherSchema.getColumnSeparator() &&\n+\t\t\tArrays.equals(csvSchema.getLineSeparator(), otherSchema.getLineSeparator()) &&\n+\t\t\tcsvSchema.getArrayElementSeparator().equals(otherSchema.getArrayElementSeparator()) &&\n+\t\t\tcsvSchema.getQuoteChar() == otherSchema.getQuoteChar() &&\n+\t\t\tcsvSchema.getEscapeChar() == otherSchema.getEscapeChar() &&\n+\t\t\tArrays.equals(csvSchema.getNullValue(), otherSchema.getNullValue());\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(\n+\t\t\trowType,\n+\t\t\tcsvSchema.getColumnSeparator(),\n+\t\t\tcsvSchema.getLineSeparator(),\n+\t\t\tcsvSchema.getArrayElementSeparator(),\n+\t\t\tcsvSchema.getQuoteChar(),\n+\t\t\tcsvSchema.getEscapeChar(),\n+\t\t\tcsvSchema.getNullValue());\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Runtime Converters\n+\t// --------------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that converts objects of Flink Table & SQL internal data structures\n+\t * to corresponding {@link JsonNode}s.\n+\t */\n+\tprivate interface SerializationRuntimeConverter extends Serializable {", "originalCommit": "98481e8219841500289cabd8d3e7b1b0787207ea", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNDY5NA==", "url": "https://github.com/apache/flink/pull/11962#discussion_r418124694", "bodyText": "Do you mean the naming? I just want to follow the current naming style in csv and json formats.", "author": "wuchong", "createdAt": "2020-04-30T16:09:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODAxMDg4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyODYyNg==", "url": "https://github.com/apache/flink/pull/11962#discussion_r418128626", "bodyText": "Or do you mean accessing fields of RowData using getters instead of RowData.get utiltiy?\nI thought about this, but due to we don't have TypedGetters interface for RowData and ArrayData, we have to implement two writers for each type. You can have a look at org.apache.flink.table.runtime.arrow.writers.IntWriter. This will be much more complex and we should come up with an idea how to reduce duplicate code. So I think that can be a future work.", "author": "wuchong", "createdAt": "2020-04-30T16:15:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODAxMDg4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTkyNzQ3MQ==", "url": "https://github.com/apache/flink/pull/11962#discussion_r419927471", "bodyText": "I don't think we should use RowData.get(RowData row, int pos, LogicalType fieldType) if we can have a better choice.\nIt is not only for the performance of csv, but also an important example for users.\nA strong purpose of RowData is to recommend users to use specific get instead of inefficient and generic get.\nWe need to have a better way before the 1.11 release.", "author": "JingsongLi", "createdAt": "2020-05-05T07:57:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODAxMDg4OQ=="}], "type": "inlineReview"}, {"oid": "676245d476a27b37ecb0701333ff5e410a5f44c0", "url": "https://github.com/apache/flink/commit/676245d476a27b37ecb0701333ff5e410a5f44c0", "message": "Use getters instead of RowData.get() utility in serialize schema", "committedDate": "2020-05-05T14:35:18Z", "type": "commit"}]}