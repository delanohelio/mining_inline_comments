{"pr_number": 12919, "pr_title": "[FLINK-16048][avro] Support read/write confluent schema registry avro\u2026", "pr_createdAt": "2020-07-17T07:18:46Z", "pr_url": "https://github.com/apache/flink/pull/12919", "timeline": [{"oid": "48d837f15c74d134f2ba8b8e8f7ea27e6b62299f", "url": "https://github.com/apache/flink/commit/48d837f15c74d134f2ba8b8e8f7ea27e6b62299f", "message": "Fix the review comments", "committedDate": "2020-07-20T05:47:08Z", "type": "forcePushed"}, {"oid": "edb952c0f8ae4394b7f5238f4fea39878106a775", "url": "https://github.com/apache/flink/commit/edb952c0f8ae4394b7f5238f4fea39878106a775", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka", "committedDate": "2020-07-21T06:16:14Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzIxNDgyNQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r457214825", "bodyText": "default scope? + @Internal", "author": "dawidwys", "createdAt": "2020-07-20T09:24:34Z", "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/CachedSchemaCoderProvider.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.formats.avro.SchemaCoder;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.Objects;\n+\n+/** A {@link SchemaCoder.SchemaCoderProvider} that uses a cached schema registry\n+ * client underlying. **/\n+public class CachedSchemaCoderProvider implements SchemaCoder.SchemaCoderProvider {", "originalCommit": "48d837f15c74d134f2ba8b8e8f7ea27e6b62299f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d1e4ba7690be134e21d193fbc1cb01aa51aaeb9b", "url": "https://github.com/apache/flink/commit/d1e4ba7690be134e21d193fbc1cb01aa51aaeb9b", "message": "Fix the review comments", "committedDate": "2020-07-22T03:51:41Z", "type": "forcePushed"}, {"oid": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "url": "https://github.com/apache/flink/commit/f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka", "committedDate": "2020-07-22T06:28:45Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU4NTMyMQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458585321", "bodyText": "Do we still need to extract this class after the latest changes?", "author": "dawidwys", "createdAt": "2020-07-22T07:19:28Z", "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/CachedSchemaCoderProvider.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.formats.avro.SchemaCoder;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.Objects;\n+\n+/** A {@link SchemaCoder.SchemaCoderProvider} that uses a cached schema registry\n+ * client underlying. **/\n+@Internal\n+class CachedSchemaCoderProvider implements SchemaCoder.SchemaCoderProvider {", "originalCommit": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY2NDYyNA==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458664624", "bodyText": "Personally i prefer normal interface than static inner one, and also we can reuse this data structure.", "author": "danny0405", "createdAt": "2020-07-22T09:35:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU4NTMyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY2ODczOQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458668739", "bodyText": "Where can we reuse it?", "author": "dawidwys", "createdAt": "2020-07-22T09:42:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU4NTMyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcyOTgyNQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458729825", "bodyText": "We only need one class for the serialize/deserialize schema.", "author": "danny0405", "createdAt": "2020-07-22T11:41:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU4NTMyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU4NTQwMQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458585401", "bodyText": "ditto", "author": "dawidwys", "createdAt": "2020-07-22T07:19:38Z", "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/ConfluentRegistryAvroDeserializationSchema.java", "diffHunk": "@@ -114,23 +113,4 @@ private ConfluentRegistryAvroDeserializationSchema(Class<T> recordClazz, @Nullab\n \t\t\tnew CachedSchemaCoderProvider(url, identityMapCapacity)\n \t\t);\n \t}\n-\n-\tprivate static class CachedSchemaCoderProvider implements SchemaCoder.SchemaCoderProvider {", "originalCommit": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU4NTQ2MQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458585461", "bodyText": "ditto", "author": "dawidwys", "createdAt": "2020-07-22T07:19:44Z", "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/ConfluentRegistryAvroSerializationSchema.java", "diffHunk": "@@ -92,25 +91,4 @@ private ConfluentRegistryAvroSerializationSchema(Class<T> recordClazz, Schema sc\n \t\t\tnew CachedSchemaCoderProvider(subject, schemaRegistryUrl, DEFAULT_IDENTITY_MAP_CAPACITY)\n \t\t);\n \t}\n-\n-\tprivate static class CachedSchemaCoderProvider implements SchemaCoder.SchemaCoderProvider {", "originalCommit": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYwNDg0MQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458604841", "bodyText": "Why do we need that in the @Before block? Can't we just initialize it statically?", "author": "dawidwys", "createdAt": "2020-07-22T07:55:03Z", "path": "flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroFormatFactoryTest.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroRowDataSerializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.RowDataToAvroConverters;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.TestDynamicTableFactory;\n+import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.core.testutils.FlinkMatchers.containsCause;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Tests for the {@link RegistryAvroFormatFactory}.\n+ */\n+public class RegistryAvroFormatFactoryTest {\n+\tprivate TableSchema schema;\n+\tprivate RowType rowType;\n+\tprivate String subject;\n+\tprivate String registryURL;\n+\n+\t@Rule\n+\tpublic ExpectedException thrown = ExpectedException.none();\n+\n+\t@Before\n+\tpublic void before() {", "originalCommit": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY2NDkwOQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458664909", "bodyText": "We can, make them static also works.", "author": "danny0405", "createdAt": "2020-07-22T09:35:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYwNDg0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYxMTA3Nw==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458611077", "bodyText": "Please don't use assert. I can't think of a reason to use an assert in a test. assert is an assertion you can disable via a compiler flag. Why would you want to disable assertions in tests? If you want to check the type of actualSource use e.g.\nassertThat(actualSink, instanceOf(TestDynamicTableFactory.DynamicTableSinkMock.class));", "author": "dawidwys", "createdAt": "2020-07-22T08:05:51Z", "path": "flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroFormatFactoryTest.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroRowDataSerializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.RowDataToAvroConverters;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.TestDynamicTableFactory;\n+import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.core.testutils.FlinkMatchers.containsCause;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Tests for the {@link RegistryAvroFormatFactory}.\n+ */\n+public class RegistryAvroFormatFactoryTest {\n+\tprivate TableSchema schema;\n+\tprivate RowType rowType;\n+\tprivate String subject;\n+\tprivate String registryURL;\n+\n+\t@Rule\n+\tpublic ExpectedException thrown = ExpectedException.none();\n+\n+\t@Before\n+\tpublic void before() {\n+\t\tthis.schema = TableSchema.builder()\n+\t\t\t\t.field(\"a\", DataTypes.STRING())\n+\t\t\t\t.field(\"b\", DataTypes.INT())\n+\t\t\t\t.field(\"c\", DataTypes.BOOLEAN())\n+\t\t\t\t.build();\n+\t\tthis.rowType = (RowType) schema.toRowDataType().getLogicalType();\n+\t\tthis.subject = \"test-subject\";\n+\t\tthis.registryURL = \"http://localhost:8081\";\n+\t}\n+\n+\t@Test\n+\tpublic void testSeDeSchema() {\n+\t\tfinal AvroRowDataDeserializationSchema expectedDeser =\n+\t\t\t\tnew AvroRowDataDeserializationSchema(\n+\t\t\t\t\t\tConfluentRegistryAvroDeserializationSchema.forGeneric(\n+\t\t\t\t\t\t\t\tAvroSchemaConverter.convertToSchema(rowType),\n+\t\t\t\t\t\t\t\tregistryURL),\n+\t\t\t\t\t\tAvroToRowDataConverters.createRowConverter(rowType),\n+\t\t\t\t\t\tInternalTypeInfo.of(rowType));\n+\n+\t\tfinal Map<String, String> options = getAllOptions();\n+\n+\t\tfinal DynamicTableSource actualSource = createTableSource(options);\n+\t\tassert actualSource instanceof TestDynamicTableFactory.DynamicTableSourceMock;", "originalCommit": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYyNjg3NA==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458626874", "bodyText": "You have two completely independent tests in the single method. Please split it into two separate tests. We should always aim to test a single thing at a time. The benefits are:\n\nBoth tests are always executed. Independent of the result of the other.\nIt's easier to debug. You don't need to run the first case if the second fails.", "author": "dawidwys", "createdAt": "2020-07-22T08:32:21Z", "path": "flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroFormatFactoryTest.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroRowDataSerializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.RowDataToAvroConverters;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.TestDynamicTableFactory;\n+import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.core.testutils.FlinkMatchers.containsCause;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Tests for the {@link RegistryAvroFormatFactory}.\n+ */\n+public class RegistryAvroFormatFactoryTest {\n+\tprivate TableSchema schema;\n+\tprivate RowType rowType;\n+\tprivate String subject;\n+\tprivate String registryURL;\n+\n+\t@Rule\n+\tpublic ExpectedException thrown = ExpectedException.none();\n+\n+\t@Before\n+\tpublic void before() {\n+\t\tthis.schema = TableSchema.builder()\n+\t\t\t\t.field(\"a\", DataTypes.STRING())\n+\t\t\t\t.field(\"b\", DataTypes.INT())\n+\t\t\t\t.field(\"c\", DataTypes.BOOLEAN())\n+\t\t\t\t.build();\n+\t\tthis.rowType = (RowType) schema.toRowDataType().getLogicalType();\n+\t\tthis.subject = \"test-subject\";\n+\t\tthis.registryURL = \"http://localhost:8081\";\n+\t}\n+\n+\t@Test\n+\tpublic void testSeDeSchema() {", "originalCommit": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYyODMzNQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458628335", "bodyText": "getAllOptions -> getDefaultOptions", "author": "dawidwys", "createdAt": "2020-07-22T08:34:52Z", "path": "flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroFormatFactoryTest.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroRowDataSerializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.RowDataToAvroConverters;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.TestDynamicTableFactory;\n+import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.core.testutils.FlinkMatchers.containsCause;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Tests for the {@link RegistryAvroFormatFactory}.\n+ */\n+public class RegistryAvroFormatFactoryTest {\n+\tprivate TableSchema schema;\n+\tprivate RowType rowType;\n+\tprivate String subject;\n+\tprivate String registryURL;\n+\n+\t@Rule\n+\tpublic ExpectedException thrown = ExpectedException.none();\n+\n+\t@Before\n+\tpublic void before() {\n+\t\tthis.schema = TableSchema.builder()\n+\t\t\t\t.field(\"a\", DataTypes.STRING())\n+\t\t\t\t.field(\"b\", DataTypes.INT())\n+\t\t\t\t.field(\"c\", DataTypes.BOOLEAN())\n+\t\t\t\t.build();\n+\t\tthis.rowType = (RowType) schema.toRowDataType().getLogicalType();\n+\t\tthis.subject = \"test-subject\";\n+\t\tthis.registryURL = \"http://localhost:8081\";\n+\t}\n+\n+\t@Test\n+\tpublic void testSeDeSchema() {\n+\t\tfinal AvroRowDataDeserializationSchema expectedDeser =\n+\t\t\t\tnew AvroRowDataDeserializationSchema(\n+\t\t\t\t\t\tConfluentRegistryAvroDeserializationSchema.forGeneric(\n+\t\t\t\t\t\t\t\tAvroSchemaConverter.convertToSchema(rowType),\n+\t\t\t\t\t\t\t\tregistryURL),\n+\t\t\t\t\t\tAvroToRowDataConverters.createRowConverter(rowType),\n+\t\t\t\t\t\tInternalTypeInfo.of(rowType));\n+\n+\t\tfinal Map<String, String> options = getAllOptions();\n+\n+\t\tfinal DynamicTableSource actualSource = createTableSource(options);\n+\t\tassert actualSource instanceof TestDynamicTableFactory.DynamicTableSourceMock;\n+\t\tTestDynamicTableFactory.DynamicTableSourceMock scanSourceMock =\n+\t\t\t\t(TestDynamicTableFactory.DynamicTableSourceMock) actualSource;\n+\n+\t\tDeserializationSchema<RowData> actualDeser = scanSourceMock.valueFormat\n+\t\t\t\t.createRuntimeDecoder(\n+\t\t\t\t\t\tScanRuntimeProviderContext.INSTANCE,\n+\t\t\t\t\t\tschema.toRowDataType());\n+\n+\t\tassertEquals(expectedDeser, actualDeser);\n+\n+\t\tfinal AvroRowDataSerializationSchema expectedSer =\n+\t\t\t\tnew AvroRowDataSerializationSchema(\n+\t\t\t\t\t\trowType,\n+\t\t\t\t\t\tConfluentRegistryAvroSerializationSchema.forGeneric(\n+\t\t\t\t\t\t\t\tsubject,\n+\t\t\t\t\t\t\t\tAvroSchemaConverter.convertToSchema(rowType),\n+\t\t\t\t\t\t\t\tregistryURL),\n+\t\t\t\t\t\tRowDataToAvroConverters.createRowConverter(rowType));\n+\n+\t\tfinal DynamicTableSink actualSink = createTableSink(options);\n+\t\tassert actualSink instanceof TestDynamicTableFactory.DynamicTableSinkMock;\n+\t\tTestDynamicTableFactory.DynamicTableSinkMock sinkMock =\n+\t\t\t\t(TestDynamicTableFactory.DynamicTableSinkMock) actualSink;\n+\n+\t\tSerializationSchema<RowData> actualSer = sinkMock.valueFormat\n+\t\t\t\t.createRuntimeEncoder(\n+\t\t\t\t\t\tnull,\n+\t\t\t\t\t\tschema.toRowDataType());\n+\n+\t\tassertEquals(expectedSer, actualSer);\n+\t}\n+\n+\t@Test\n+\tpublic void testMissingSubjectForSink() {\n+\t\tthrown.expect(ValidationException.class);\n+\t\tthrown.expect(\n+\t\t\t\tcontainsCause(\n+\t\t\t\t\t\tnew ValidationException(\"Option avro-sr.schema-registry.subject \"\n+\t\t\t\t\t\t\t\t+ \"is required for serialization\")));\n+\n+\t\tfinal Map<String, String> options =\n+\t\t\t\tgetModifiedOptions(opts -> opts.remove(\"avro-sr.schema-registry.subject\"));\n+\n+\t\tcreateTableSink(options);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  Utilities\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Returns the full options modified by the given consumer {@code optionModifier}.\n+\t *\n+\t * @param optionModifier Consumer to modify the options\n+\t */\n+\tprivate Map<String, String> getModifiedOptions(Consumer<Map<String, String>> optionModifier) {\n+\t\tMap<String, String> options = getAllOptions();\n+\t\toptionModifier.accept(options);\n+\t\treturn options;\n+\t}\n+\n+\tprivate Map<String, String> getAllOptions() {", "originalCommit": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYzMTUxNA==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458631514", "bodyText": "There is no point in using a large number here. Use  2L here.", "author": "dawidwys", "createdAt": "2020-07-22T08:40:05Z", "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDataDeserializationSchema.java", "diffHunk": "@@ -75,17 +44,10 @@\n @PublicEvolving\n public class AvroRowDataDeserializationSchema implements DeserializationSchema<RowData> {\n \n-\tprivate static final long serialVersionUID = 1L;\n-\n-\t/**\n-\t * Used for converting Date type.\n-\t */\n-\tprivate static final int MILLIS_PER_DAY = 86400_000;\n+\tprivate static final long serialVersionUID = 9055890466043022732L;", "originalCommit": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY0NTU0OQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458645549", "bodyText": "Those tests have nothing to do with schema registry.\nThey test the same logic as in AvroRowDataDeSerializationSchemaTest", "author": "dawidwys", "createdAt": "2020-07-22T09:02:59Z", "path": "flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/RegistryAvroRowDataSeDeSchemaTest.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro;\n+\n+import org.apache.flink.formats.avro.generated.Address;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.formats.avro.utils.TestDataGenerator;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Random;\n+\n+import static org.apache.flink.formats.avro.utils.AvroTestUtils.writeRecord;\n+import static org.hamcrest.core.Is.is;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for {@link AvroRowDataDeserializationSchema} and\n+ * {@link AvroRowDataSerializationSchema} for schema registry avro.\n+ */\n+public class RegistryAvroRowDataSeDeSchemaTest {", "originalCommit": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3MjQ0MA==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458672440", "bodyText": "Yes, we can remove it.", "author": "danny0405", "createdAt": "2020-07-22T09:48:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY0NTU0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1MDE1MQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458650151", "bodyText": "@Internal", "author": "dawidwys", "createdAt": "2020-07-22T09:10:45Z", "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroToRowDataConverters.java", "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro;\n+\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericArrayData;\n+import org.apache.flink.table.data.GenericMapData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;\n+\n+import org.apache.avro.generic.GenericFixed;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.joda.time.DateTime;\n+import org.joda.time.DateTimeFieldType;\n+import org.joda.time.LocalDate;\n+\n+import java.io.Serializable;\n+import java.lang.reflect.Array;\n+import java.nio.ByteBuffer;\n+import java.sql.Timestamp;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.formats.avro.typeutils.AvroSchemaConverter.extractValueTypeToAvroMap;\n+import static org.joda.time.DateTimeConstants.MILLIS_PER_DAY;\n+\n+/** Tool class used to convert from Avro {@link GenericRecord} to {@link RowData}. **/\n+public class AvroToRowDataConverters {", "originalCommit": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1MzMwMQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458653301", "bodyText": "Could we add tests for this method?", "author": "dawidwys", "createdAt": "2020-07-22T09:16:10Z", "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java", "diffHunk": "@@ -169,6 +171,121 @@ private AvroSchemaConverter() {\n \t\tthrow new IllegalArgumentException(\"Unsupported Avro type '\" + schema.getType() + \"'.\");\n \t}\n \n+\t/**\n+\t * Converts an Avro schema string into a nested row structure with deterministic field order and data\n+\t * types that are compatible with Flink's Table & SQL API.\n+\t *\n+\t * @param avroSchemaString Avro schema definition string\n+\t *\n+\t * @return data type matching the schema\n+\t */\n+\tpublic static DataType convertToDataType(String avroSchemaString) {", "originalCommit": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1NTE3Nw==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458655177", "bodyText": "How about we remove this ctor? IMO the logic from this ctor should be only in the factory. I know that this class in theory is PublicEvolving but practically it is only usable from Table API through the factory. Therefore in my opinion it is safe to drop this ctor.\nThe same applies to SerializationSchema.", "author": "dawidwys", "createdAt": "2020-07-22T09:19:23Z", "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDataSerializationSchema.java", "diffHunk": "@@ -64,61 +41,59 @@\n \n \tprivate static final long serialVersionUID = 1L;\n \n+\t/** Nested schema to serialize the {@link GenericRecord} into bytes. **/\n+\tprivate final SerializationSchema<GenericRecord> nestedSchema;\n+\n \t/**\n \t * Logical type describing the input type.\n \t */\n \tprivate final RowType rowType;\n \n-\t/**\n-\t * Runtime instance that performs the actual work.\n-\t */\n-\tprivate final SerializationRuntimeConverter runtimeConverter;\n-\n \t/**\n \t * Avro serialization schema.\n \t */\n \tprivate transient Schema schema;\n \n \t/**\n-\t * Writer to serialize Avro record into a Avro bytes.\n-\t */\n-\tprivate transient DatumWriter<IndexedRecord> datumWriter;\n-\n-\t/**\n-\t * Output stream to serialize records into byte array.\n+\t * Runtime instance that performs the actual work.\n \t */\n-\tprivate transient ByteArrayOutputStream arrayOutputStream;\n+\tprivate final RowDataToAvroConverters.RowDataToAvroConverter runtimeConverter;\n \n \t/**\n-\t * Low-level class for serialization of Avro values.\n+\t * Creates an Avro serialization schema with the given record row type.\n \t */\n-\tprivate transient Encoder encoder;\n+\tpublic AvroRowDataSerializationSchema(RowType rowType) {", "originalCommit": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3MDk4MQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458670981", "bodyText": "I would prefer a constructor with default implementation.", "author": "danny0405", "createdAt": "2020-07-22T09:46:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1NTE3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY4NDM1MA==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458684350", "bodyText": "Why?", "author": "dawidwys", "createdAt": "2020-07-22T10:09:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1NTE3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcyNDAxNA==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458724014", "bodyText": "Why not, the avro row data format is default to SE/DE avro without schema registry, if other formats what to customize something, use the correct constructor. Write the same code everywhere just makes no sense.", "author": "danny0405", "createdAt": "2020-07-22T11:29:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1NTE3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1NjQ1Nw==", "url": "https://github.com/apache/flink/pull/12919#discussion_r458656457", "bodyText": "IMO we should add a simple test for serializing and deserializing using schema registry. It does not need to be very in depth, but  so that it checks that everything is well connected.", "author": "dawidwys", "createdAt": "2020-07-22T09:21:25Z", "path": "flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/RegistryAvroRowDataSeDeSchemaTest.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro;\n+\n+import org.apache.flink.formats.avro.generated.Address;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.formats.avro.utils.TestDataGenerator;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.Random;\n+\n+import static org.apache.flink.formats.avro.utils.AvroTestUtils.writeRecord;\n+import static org.hamcrest.core.Is.is;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for {@link AvroRowDataDeserializationSchema} and\n+ * {@link AvroRowDataSerializationSchema} for schema registry avro.\n+ */\n+public class RegistryAvroRowDataSeDeSchemaTest {\n+\tprivate static final String ADDRESS_SCHEMA = \"\" +", "originalCommit": "f0da3cee91d22ec20cbba1b6c5be45da1440cf05", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI3MzczNA==", "url": "https://github.com/apache/flink/pull/12919#discussion_r459273734", "bodyText": "I have added a schema registry server there, and test the new format SE/DE with connection to the schema registry service. See the new RegistryAvroRowDataSeDeSchemaTest.", "author": "danny0405", "createdAt": "2020-07-23T07:54:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1NjQ1Nw=="}], "type": "inlineReview"}, {"oid": "9104e12b0394cd6d578d2380ca4554b75e6e00f9", "url": "https://github.com/apache/flink/commit/9104e12b0394cd6d578d2380ca4554b75e6e00f9", "message": "Fix the review comment address", "committedDate": "2020-07-22T11:31:27Z", "type": "forcePushed"}, {"oid": "870bafa5f797f69a43f713b554355838b2447cf8", "url": "https://github.com/apache/flink/commit/870bafa5f797f69a43f713b554355838b2447cf8", "message": "Add schema registry serve in test and modify the tests", "committedDate": "2020-07-23T10:51:20Z", "type": "forcePushed"}, {"oid": "9b557c718fe731e8d5c58e7c5d9c3452a245ee5a", "url": "https://github.com/apache/flink/commit/9b557c718fe731e8d5c58e7c5d9c3452a245ee5a", "message": "Add schema registry serve in test and modify the tests", "committedDate": "2020-07-23T10:57:08Z", "type": "forcePushed"}, {"oid": "71218ee49095663a641e56889831536a2a2e69ea", "url": "https://github.com/apache/flink/commit/71218ee49095663a641e56889831536a2a2e69ea", "message": "Add schema registry serve in test and modify the tests", "committedDate": "2020-07-23T12:03:52Z", "type": "forcePushed"}, {"oid": "6839c54eedcdca926b8304782fabcb0dc529c5a6", "url": "https://github.com/apache/flink/commit/6839c54eedcdca926b8304782fabcb0dc529c5a6", "message": "Add schema registry serve in test and modify the tests", "committedDate": "2020-07-24T02:26:47Z", "type": "forcePushed"}, {"oid": "9d8870894b4d9d434c45b58339985aed3b76a8be", "url": "https://github.com/apache/flink/commit/9d8870894b4d9d434c45b58339985aed3b76a8be", "message": "Add schema registry serve in test and modify the tests", "committedDate": "2020-07-24T04:20:21Z", "type": "forcePushed"}, {"oid": "bc53f692ab7edf110e6c7c39202e50ed1ec0c05d", "url": "https://github.com/apache/flink/commit/bc53f692ab7edf110e6c7c39202e50ed1ec0c05d", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka", "committedDate": "2020-07-24T06:30:03Z", "type": "forcePushed"}, {"oid": "0d49193a68ad28f6d2965bc8bfd57c59b2b21935", "url": "https://github.com/apache/flink/commit/0d49193a68ad28f6d2965bc8bfd57c59b2b21935", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka", "committedDate": "2020-07-24T07:17:11Z", "type": "forcePushed"}, {"oid": "810321d988a8284eb54c2963f22a049dc06ac8aa", "url": "https://github.com/apache/flink/commit/810321d988a8284eb54c2963f22a049dc06ac8aa", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka", "committedDate": "2020-07-24T07:55:09Z", "type": "commit"}, {"oid": "810321d988a8284eb54c2963f22a049dc06ac8aa", "url": "https://github.com/apache/flink/commit/810321d988a8284eb54c2963f22a049dc06ac8aa", "message": "[FLINK-16048][avro] Support read/write confluent schema registry avro data from Kafka", "committedDate": "2020-07-24T07:55:09Z", "type": "forcePushed"}, {"oid": "6bd8c02de778a8ff2f34a19d5beee414beac3f69", "url": "https://github.com/apache/flink/commit/6bd8c02de778a8ff2f34a19d5beee414beac3f69", "message": "Fix the review comments", "committedDate": "2020-07-24T10:34:03Z", "type": "commit"}, {"oid": "6bd8c02de778a8ff2f34a19d5beee414beac3f69", "url": "https://github.com/apache/flink/commit/6bd8c02de778a8ff2f34a19d5beee414beac3f69", "message": "Fix the review comments", "committedDate": "2020-07-24T10:34:03Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDAyMTc5NA==", "url": "https://github.com/apache/flink/pull/12919#discussion_r460021794", "bodyText": "nit: use SUBJECT", "author": "dawidwys", "createdAt": "2020-07-24T12:28:59Z", "path": "flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/RegistryAvroRowDataSeDeSchemaTest.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent;\n+\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroRowDataSerializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.RegistryAvroDeserializationSchema;\n+import org.apache.flink.formats.avro.RegistryAvroSerializationSchema;\n+import org.apache.flink.formats.avro.RowDataToAvroConverters;\n+import org.apache.flink.formats.avro.generated.Address;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.formats.avro.utils.TestDataGenerator;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.binary.BinaryStringData;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import static org.apache.flink.core.testutils.FlinkMatchers.containsCause;\n+import static org.apache.flink.formats.avro.utils.AvroTestUtils.writeRecord;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.core.Is.is;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for {@link AvroRowDataDeserializationSchema} and\n+ * {@link AvroRowDataSerializationSchema} for schema registry avro.\n+ */\n+public class RegistryAvroRowDataSeDeSchemaTest {\n+\tprivate static final Schema ADDRESS_SCHEMA = Address.getClassSchema();\n+\n+\tprivate static final Schema ADDRESS_SCHEMA_COMPATIBLE = new Schema.Parser().parse(\n+\t\t\t\"\" +\n+\t\t\t\t\t\"{\\\"namespace\\\": \\\"org.apache.flink.formats.avro.generated\\\",\\n\" +\n+\t\t\t\t\t\" \\\"type\\\": \\\"record\\\",\\n\" +\n+\t\t\t\t\t\" \\\"name\\\": \\\"Address\\\",\\n\" +\n+\t\t\t\t\t\" \\\"fields\\\": [\\n\" +\n+\t\t\t\t\t\"     {\\\"name\\\": \\\"num\\\", \\\"type\\\": \\\"int\\\"},\\n\" +\n+\t\t\t\t\t\"     {\\\"name\\\": \\\"street\\\", \\\"type\\\": \\\"string\\\"}\\n\" +\n+\t\t\t\t\t\"  ]\\n\" +\n+\t\t\t\t\t\"}\");\n+\n+\tprivate static final String SUBJECT = \"address-value\";\n+\n+\tprivate static SchemaRegistryClient client;\n+\n+\tprivate Address address;\n+\n+\t@Rule\n+\tpublic ExpectedException expectedEx = ExpectedException.none();\n+\n+\t@BeforeClass\n+\tpublic static void beforeClass() {\n+\t\tclient = new MockSchemaRegistryClient();\n+\t}\n+\n+\t@Before\n+\tpublic void before() {\n+\t\tthis.address = TestDataGenerator.generateRandomAddress(new Random());\n+\t}\n+\n+\t@After\n+\tpublic void after() throws IOException, RestClientException {\n+\t\tclient.deleteSubject(SUBJECT);\n+\t}\n+\n+\t@Test\n+\tpublic void testRowDataWriteReadWithFullSchema() throws Exception {\n+\t\ttestRowDataWriteReadWithSchema(ADDRESS_SCHEMA);\n+\t}\n+\n+\t@Test\n+\tpublic void testRowDataWriteReadWithCompatibleSchema() throws Exception {\n+\t\ttestRowDataWriteReadWithSchema(ADDRESS_SCHEMA_COMPATIBLE);\n+\t\t// Validates new schema has been registered.\n+\t\tassertThat(client.getAllVersions(\"address-value\").size(), is(1));", "originalCommit": "6bd8c02de778a8ff2f34a19d5beee414beac3f69", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDAyMjgzMQ==", "url": "https://github.com/apache/flink/pull/12919#discussion_r460022831", "bodyText": "Remove this method? If you prefer to keep it add missing javadoc.", "author": "dawidwys", "createdAt": "2020-07-24T12:31:17Z", "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/RegistryAvroDeserializationSchema.java", "diffHunk": "@@ -52,13 +53,22 @@\n \t * @param schemaCoderProvider schema provider that allows instantiation of {@link SchemaCoder} that will be used for\n \t *                            schema reading\n \t */\n-\tprotected RegistryAvroDeserializationSchema(Class<T> recordClazz, @Nullable Schema reader,\n+\tpublic RegistryAvroDeserializationSchema(Class<T> recordClazz, @Nullable Schema reader,\n \t\t\tSchemaCoder.SchemaCoderProvider schemaCoderProvider) {\n \t\tsuper(recordClazz, reader);\n \t\tthis.schemaCoderProvider = schemaCoderProvider;\n \t\tthis.schemaCoder = schemaCoderProvider.get();\n \t}\n \n+\tpublic static RegistryAvroDeserializationSchema<GenericRecord> forGeneric(", "originalCommit": "6bd8c02de778a8ff2f34a19d5beee414beac3f69", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "52518eecfce65f5adceda689fa720f15c85413b6", "url": "https://github.com/apache/flink/commit/52518eecfce65f5adceda689fa720f15c85413b6", "message": "Fix the review comments", "committedDate": "2020-07-25T00:04:55Z", "type": "commit"}]}