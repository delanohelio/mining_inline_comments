{"pr_number": 12803, "pr_title": "[FLINK-18461][table-planner-blink] Fix Changelog source can't be insert into upsert sink", "pr_createdAt": "2020-07-02T03:48:08Z", "pr_url": "https://github.com/apache/flink/pull/12803", "timeline": [{"oid": "95f8aa6e2dde098b77298dca13b736bbe742e09a", "url": "https://github.com/apache/flink/commit/95f8aa6e2dde098b77298dca13b736bbe742e09a", "message": "[FLINK-18461][table-planner-blink] Fix Changelog source can't be insert into upsert sink", "committedDate": "2020-07-02T03:42:44Z", "type": "commit"}, {"oid": "0198d4276d4971f31ec872e185d0261310e3396b", "url": "https://github.com/apache/flink/commit/0198d4276d4971f31ec872e185d0261310e3396b", "message": "address comments.", "committedDate": "2020-07-02T07:40:54Z", "type": "commit"}, {"oid": "250275b97b69730087e23586a56ae92bca71b7f6", "url": "https://github.com/apache/flink/commit/250275b97b69730087e23586a56ae92bca71b7f6", "message": "address comment", "committedDate": "2020-07-02T08:12:14Z", "type": "commit"}, {"oid": "1179adabba184f1203465cd89bbbb055b8fd9c04", "url": "https://github.com/apache/flink/commit/1179adabba184f1203465cd89bbbb055b8fd9c04", "message": "fix jdbc bug", "committedDate": "2020-07-02T17:35:24Z", "type": "commit"}, {"oid": "533492d8b403e3532079bba154f960f8dd66df31", "url": "https://github.com/apache/flink/commit/533492d8b403e3532079bba154f960f8dd66df31", "message": "fix tests", "committedDate": "2020-07-03T04:44:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQwNDk1Ng==", "url": "https://github.com/apache/flink/pull/12803#discussion_r449404956", "bodyText": "We can omit  this local variable?\nreduceBuffer.put(keyExtractor.apply(record),\n                 Tuple2.of(changeFlag(record.getRowKind()), recordExtractor.apply(record)));", "author": "leonardBang", "createdAt": "2020-07-03T06:45:55Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.jdbc.internal.executor;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.types.RowKind;\n+\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Function;\n+\n+/**\n+ * Currently, this statement executor is only used for table/sql to buffer insert/update/delete\n+ * events, and reduce them in buffer before submit to external database.\n+ */\n+public class BufferReduceStatementExecutor implements JdbcBatchStatementExecutor<RowData> {\n+\n+\tprivate final JdbcBatchStatementExecutor<RowData> upsertExecutor;\n+\tprivate final JdbcBatchStatementExecutor<RowData> deleteExecutor;\n+\tprivate final Function<RowData, RowData> keyExtractor;\n+\tprivate final Function<RowData, RowData> valueTransform;\n+\t// the mapping is [KEY, <+/-, VALUE>]\n+\tprivate final Map<RowData, Tuple2<Boolean, RowData>> reduceBuffer = new HashMap<>();\n+\n+\tpublic BufferReduceStatementExecutor(\n+\t\t\tJdbcBatchStatementExecutor<RowData> upsertExecutor,\n+\t\t\tJdbcBatchStatementExecutor<RowData> deleteExecutor,\n+\t\t\tFunction<RowData, RowData> keyExtractor,\n+\t\t\tFunction<RowData, RowData> valueTransform) {\n+\t\tthis.upsertExecutor = upsertExecutor;\n+\t\tthis.deleteExecutor = deleteExecutor;\n+\t\tthis.keyExtractor = keyExtractor;\n+\t\tthis.valueTransform = valueTransform;\n+\t}\n+\n+\t@Override\n+\tpublic void prepareStatements(Connection connection) throws SQLException {\n+\t\tupsertExecutor.prepareStatements(connection);\n+\t\tdeleteExecutor.prepareStatements(connection);\n+\t}\n+\n+\t@Override\n+\tpublic void addToBatch(RowData record) throws SQLException {\n+\t\tRowData key = keyExtractor.apply(record);", "originalCommit": "1179adabba184f1203465cd89bbbb055b8fd9c04", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQwNTQzNA==", "url": "https://github.com/apache/flink/pull/12803#discussion_r449405434", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t * Returns true if it is INSERT or UPDATE_AFTER, returns false if it is DELETE or UPDATE_BEFORE.\n          \n          \n            \n            \t * Returns true if the row kind is INSERT or UPDATE_AFTER, returns false if the row kind is DELETE or UPDATE_BEFORE.", "author": "leonardBang", "createdAt": "2020-07-03T06:47:09Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.jdbc.internal.executor;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.types.RowKind;\n+\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Function;\n+\n+/**\n+ * Currently, this statement executor is only used for table/sql to buffer insert/update/delete\n+ * events, and reduce them in buffer before submit to external database.\n+ */\n+public class BufferReduceStatementExecutor implements JdbcBatchStatementExecutor<RowData> {\n+\n+\tprivate final JdbcBatchStatementExecutor<RowData> upsertExecutor;\n+\tprivate final JdbcBatchStatementExecutor<RowData> deleteExecutor;\n+\tprivate final Function<RowData, RowData> keyExtractor;\n+\tprivate final Function<RowData, RowData> valueTransform;\n+\t// the mapping is [KEY, <+/-, VALUE>]\n+\tprivate final Map<RowData, Tuple2<Boolean, RowData>> reduceBuffer = new HashMap<>();\n+\n+\tpublic BufferReduceStatementExecutor(\n+\t\t\tJdbcBatchStatementExecutor<RowData> upsertExecutor,\n+\t\t\tJdbcBatchStatementExecutor<RowData> deleteExecutor,\n+\t\t\tFunction<RowData, RowData> keyExtractor,\n+\t\t\tFunction<RowData, RowData> valueTransform) {\n+\t\tthis.upsertExecutor = upsertExecutor;\n+\t\tthis.deleteExecutor = deleteExecutor;\n+\t\tthis.keyExtractor = keyExtractor;\n+\t\tthis.valueTransform = valueTransform;\n+\t}\n+\n+\t@Override\n+\tpublic void prepareStatements(Connection connection) throws SQLException {\n+\t\tupsertExecutor.prepareStatements(connection);\n+\t\tdeleteExecutor.prepareStatements(connection);\n+\t}\n+\n+\t@Override\n+\tpublic void addToBatch(RowData record) throws SQLException {\n+\t\tRowData key = keyExtractor.apply(record);\n+\t\treduceBuffer.put(key, Tuple2.of(changeFlag(record.getRowKind()), valueTransform.apply(record)));\n+\t}\n+\n+\t/**\n+\t * Returns true if it is INSERT or UPDATE_AFTER, returns false if it is DELETE or UPDATE_BEFORE.", "originalCommit": "1179adabba184f1203465cd89bbbb055b8fd9c04", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQwNzM5MQ==", "url": "https://github.com/apache/flink/pull/12803#discussion_r449407391", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t * Creates a {@link JdbcStatementBuilder} for {@link Row} using the provided SQL types array.\n          \n          \n            \n            \t * Create a {@link JdbcStatementBuilder} for {@link RowData} using the provided SQL types array.", "author": "leonardBang", "createdAt": "2020-07-03T06:52:25Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatBuilder.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.jdbc.table;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.connector.jdbc.JdbcExecutionOptions;\n+import org.apache.flink.connector.jdbc.JdbcStatementBuilder;\n+import org.apache.flink.connector.jdbc.dialect.JdbcDialect;\n+import org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat;\n+import org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider;\n+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;\n+import org.apache.flink.connector.jdbc.internal.executor.BufferReduceStatementExecutor;\n+import org.apache.flink.connector.jdbc.internal.executor.InsertOrUpdateJdbcExecutor;\n+import org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor;\n+import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;\n+import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;\n+import org.apache.flink.connector.jdbc.utils.JdbcUtils;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.Row;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.function.Function;\n+\n+import static org.apache.flink.table.data.RowData.createFieldGetter;\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Builder for {@link JdbcBatchingOutputFormat} for Table/SQL.\n+ */\n+public class JdbcDynamicOutputFormatBuilder implements Serializable {\n+\n+\tprivate JdbcOptions jdbcOptions;\n+\tprivate JdbcExecutionOptions executionOptions;\n+\tprivate JdbcDmlOptions dmlOptions;\n+\tprivate TypeInformation<RowData> rowDataTypeInformation;\n+\tprivate DataType[] fieldDataTypes;\n+\n+\tpublic JdbcDynamicOutputFormatBuilder() {\n+\t}\n+\n+\tpublic JdbcDynamicOutputFormatBuilder setJdbcOptions(JdbcOptions jdbcOptions) {\n+\t\tthis.jdbcOptions = jdbcOptions;\n+\t\treturn this;\n+\t}\n+\n+\tpublic JdbcDynamicOutputFormatBuilder setJdbcExecutionOptions(JdbcExecutionOptions executionOptions) {\n+\t\tthis.executionOptions = executionOptions;\n+\t\treturn this;\n+\t}\n+\n+\tpublic JdbcDynamicOutputFormatBuilder setJdbcDmlOptions(JdbcDmlOptions dmlOptions) {\n+\t\tthis.dmlOptions = dmlOptions;\n+\t\treturn this;\n+\t}\n+\n+\tpublic JdbcDynamicOutputFormatBuilder setRowDataTypeInfo(TypeInformation<RowData> rowDataTypeInfo) {\n+\t\tthis.rowDataTypeInformation = rowDataTypeInfo;\n+\t\treturn this;\n+\t}\n+\n+\tpublic JdbcDynamicOutputFormatBuilder setFieldDataTypes(DataType[] fieldDataTypes) {\n+\t\tthis.fieldDataTypes = fieldDataTypes;\n+\t\treturn this;\n+\t}\n+\n+\tpublic JdbcBatchingOutputFormat<RowData, ?, ?> build() {\n+\t\tcheckNotNull(jdbcOptions, \"jdbc options can not be null\");\n+\t\tcheckNotNull(dmlOptions, \"jdbc dml options can not be null\");\n+\t\tcheckNotNull(executionOptions, \"jdbc execution options can not be null\");\n+\n+\t\tfinal LogicalType[] logicalTypes = Arrays.stream(fieldDataTypes)\n+\t\t\t.map(DataType::getLogicalType)\n+\t\t\t.toArray(LogicalType[]::new);\n+\t\tif (dmlOptions.getKeyFields().isPresent() && dmlOptions.getKeyFields().get().length > 0) {\n+\t\t\t//upsert query\n+\t\t\treturn new JdbcBatchingOutputFormat<>(\n+\t\t\t\tnew SimpleJdbcConnectionProvider(jdbcOptions),\n+\t\t\t\texecutionOptions,\n+\t\t\t\tctx -> createBufferReduceExecutor(dmlOptions, ctx, rowDataTypeInformation, logicalTypes),\n+\t\t\t\tJdbcBatchingOutputFormat.RecordExtractor.identity());\n+\t\t} else {\n+\t\t\t// append only query\n+\t\t\tfinal String sql = dmlOptions\n+\t\t\t\t.getDialect()\n+\t\t\t\t.getInsertIntoStatement(dmlOptions.getTableName(), dmlOptions.getFieldNames());\n+\t\t\treturn new JdbcBatchingOutputFormat<>(\n+\t\t\t\tnew SimpleJdbcConnectionProvider(jdbcOptions),\n+\t\t\t\texecutionOptions,\n+\t\t\t\tctx -> createSimpleRowDataExecutor(dmlOptions.getDialect(), sql, logicalTypes, ctx, rowDataTypeInformation),\n+\t\t\t\tJdbcBatchingOutputFormat.RecordExtractor.identity());\n+\t\t}\n+\t}\n+\n+\tprivate static JdbcBatchStatementExecutor<RowData> createKeyedRowExecutor(\n+\t\tJdbcDialect dialect,\n+\t\tint[] pkFields,\n+\t\tLogicalType[] pkTypes,\n+\t\tString sql,\n+\t\tLogicalType[] logicalTypes) {\n+\t\tfinal JdbcRowConverter rowConverter = dialect.getRowConverter(RowType.of(pkTypes));\n+\t\tfinal Function<RowData, RowData> keyExtractor = createRowKeyExtractor(logicalTypes, pkFields);\n+\t\treturn JdbcBatchStatementExecutor.keyed(\n+\t\t\tsql,\n+\t\t\tkeyExtractor,\n+\t\t\t(st, record) -> rowConverter.toExternal(keyExtractor.apply(record), st));\n+\t}\n+\n+\tprivate static JdbcBatchStatementExecutor<RowData> createBufferReduceExecutor(\n+\t\tJdbcDmlOptions opt,\n+\t\tRuntimeContext ctx,\n+\t\tTypeInformation<RowData> rowDataTypeInfo,\n+\t\tLogicalType[] fieldTypes) {\n+\t\tcheckArgument(opt.getKeyFields().isPresent());\n+\t\tint[] pkFields = Arrays.stream(opt.getKeyFields().get()).mapToInt(Arrays.asList(opt.getFieldNames())::indexOf).toArray();\n+\t\tLogicalType[] pkTypes = Arrays.stream(pkFields).mapToObj(f -> fieldTypes[f]).toArray(LogicalType[]::new);\n+\t\tfinal TypeSerializer<RowData> typeSerializer = rowDataTypeInfo.createSerializer(ctx.getExecutionConfig());\n+\t\tfinal Function<RowData, RowData> valueTransform = ctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : Function.identity();\n+\n+\t\tJdbcBatchStatementExecutor<RowData> upsertExecutor = createUpsertRowExecutor(opt, ctx, rowDataTypeInfo, pkFields, pkTypes, fieldTypes, valueTransform);\n+\t\tJdbcBatchStatementExecutor<RowData> deleteExecutor = createDeleteExecutor(opt, pkFields, pkTypes, fieldTypes);\n+\n+\t\treturn new BufferReduceStatementExecutor(\n+\t\t\tupsertExecutor,\n+\t\t\tdeleteExecutor,\n+\t\t\tcreateRowKeyExtractor(fieldTypes, pkFields),\n+\t\t\tvalueTransform);\n+\t}\n+\n+\tprivate static JdbcBatchStatementExecutor<RowData> createUpsertRowExecutor(\n+\t\tJdbcDmlOptions opt,\n+\t\tRuntimeContext ctx,\n+\t\tTypeInformation<RowData> rowDataTypeInfo,\n+\t\tint[] pkFields,\n+\t\tLogicalType[] pkTypes,\n+\t\tLogicalType[] fieldTypes,\n+\t\tFunction<RowData, RowData> valueTransform) {\n+\t\tcheckArgument(opt.getKeyFields().isPresent());\n+\t\tJdbcDialect dialect = opt.getDialect();\n+\t\treturn opt.getDialect()\n+\t\t\t.getUpsertStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get())\n+\t\t\t.map(sql -> createSimpleRowDataExecutor(dialect, sql, fieldTypes, ctx, rowDataTypeInfo))\n+\t\t\t.orElseGet(() ->\n+\t\t\t\tnew InsertOrUpdateJdbcExecutor<>(\n+\t\t\t\t\topt.getDialect().getRowExistsStatement(opt.getTableName(), opt.getKeyFields().get()),\n+\t\t\t\t\topt.getDialect().getInsertIntoStatement(opt.getTableName(), opt.getFieldNames()),\n+\t\t\t\t\topt.getDialect().getUpdateStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get()),\n+\t\t\t\t\tcreateRowDataJdbcStatementBuilder(dialect, pkTypes),\n+\t\t\t\t\tcreateRowDataJdbcStatementBuilder(dialect, fieldTypes),\n+\t\t\t\t\tcreateRowDataJdbcStatementBuilder(dialect, fieldTypes),\n+\t\t\t\t\tcreateRowKeyExtractor(fieldTypes, pkFields),\n+\t\t\t\t\tvalueTransform));\n+\t}\n+\n+\tprivate static JdbcBatchStatementExecutor<RowData> createDeleteExecutor(\n+\t\tJdbcDmlOptions dmlOptions,\n+\t\tint[] pkFields,\n+\t\tLogicalType[] pkTypes,\n+\t\tLogicalType[] fieldTypes) {\n+\t\tcheckArgument(dmlOptions.getKeyFields().isPresent());\n+\t\tString[] pkNames = Arrays.stream(pkFields).mapToObj(k -> dmlOptions.getFieldNames()[k]).toArray(String[]::new);\n+\t\tString deleteSql = dmlOptions.getDialect().getDeleteStatement(dmlOptions.getTableName(), pkNames);\n+\t\treturn createKeyedRowExecutor(dmlOptions.getDialect(), pkFields, pkTypes, deleteSql, fieldTypes);\n+\t}\n+\n+\tprivate static Function<RowData, RowData> createRowKeyExtractor(LogicalType[] logicalTypes, int[] pkFields) {\n+\t\tfinal RowData.FieldGetter[] fieldGetters = new RowData.FieldGetter[pkFields.length];\n+\t\tfor (int i = 0; i < pkFields.length; i++) {\n+\t\t\tfieldGetters[i] = createFieldGetter(logicalTypes[pkFields[i]], pkFields[i]);\n+\t\t}\n+\t\treturn row -> getPrimaryKey(row, fieldGetters);\n+\t}\n+\n+\tprivate static JdbcBatchStatementExecutor<RowData> createSimpleRowDataExecutor(JdbcDialect dialect, String sql, LogicalType[] fieldTypes, RuntimeContext ctx, TypeInformation<RowData> rowDataTypeInfo) {\n+\t\tfinal TypeSerializer<RowData> typeSerializer = rowDataTypeInfo.createSerializer(ctx.getExecutionConfig());\n+\t\treturn JdbcBatchStatementExecutor.simple(\n+\t\t\tsql,\n+\t\t\tcreateRowDataJdbcStatementBuilder(dialect, fieldTypes),\n+\t\t\tctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : Function.identity());\n+\t}\n+\n+\t/**\n+\t * Creates a {@link JdbcStatementBuilder} for {@link Row} using the provided SQL types array.", "originalCommit": "1179adabba184f1203465cd89bbbb055b8fd9c04", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3b212fec4e0c4453bc6da26180643487c01071d0", "url": "https://github.com/apache/flink/commit/3b212fec4e0c4453bc6da26180643487c01071d0", "message": "address review comments", "committedDate": "2020-07-03T07:20:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDAxOTgzOQ==", "url": "https://github.com/apache/flink/pull/12803#discussion_r580019839", "bodyText": "Hi @wuchong , may I know the reason why \"deleteExecutor.executeBatch()\" is placed after \"upsertExecutor.executeBatch()\"? I have found that the deleteExecutor would accidentally delete previously updated records, resulting in loss of data.", "author": "kylemeow", "createdAt": "2021-02-22T06:58:38Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.jdbc.internal.executor;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.types.RowKind;\n+\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Function;\n+\n+/**\n+ * Currently, this statement executor is only used for table/sql to buffer insert/update/delete\n+ * events, and reduce them in buffer before submit to external database.\n+ */\n+public class BufferReduceStatementExecutor implements JdbcBatchStatementExecutor<RowData> {\n+\n+\tprivate final JdbcBatchStatementExecutor<RowData> upsertExecutor;\n+\tprivate final JdbcBatchStatementExecutor<RowData> deleteExecutor;\n+\tprivate final Function<RowData, RowData> keyExtractor;\n+\tprivate final Function<RowData, RowData> valueTransform;\n+\t// the mapping is [KEY, <+/-, VALUE>]\n+\tprivate final Map<RowData, Tuple2<Boolean, RowData>> reduceBuffer = new HashMap<>();\n+\n+\tpublic BufferReduceStatementExecutor(\n+\t\t\tJdbcBatchStatementExecutor<RowData> upsertExecutor,\n+\t\t\tJdbcBatchStatementExecutor<RowData> deleteExecutor,\n+\t\t\tFunction<RowData, RowData> keyExtractor,\n+\t\t\tFunction<RowData, RowData> valueTransform) {\n+\t\tthis.upsertExecutor = upsertExecutor;\n+\t\tthis.deleteExecutor = deleteExecutor;\n+\t\tthis.keyExtractor = keyExtractor;\n+\t\tthis.valueTransform = valueTransform;\n+\t}\n+\n+\t@Override\n+\tpublic void prepareStatements(Connection connection) throws SQLException {\n+\t\tupsertExecutor.prepareStatements(connection);\n+\t\tdeleteExecutor.prepareStatements(connection);\n+\t}\n+\n+\t@Override\n+\tpublic void addToBatch(RowData record) throws SQLException {\n+\t\tRowData key = keyExtractor.apply(record);\n+\t\tboolean flag = changeFlag(record.getRowKind());\n+\t\tRowData value = valueTransform.apply(record); // copy or not\n+\t\treduceBuffer.put(key, Tuple2.of(flag, value));\n+\t}\n+\n+\t/**\n+\t * Returns true if the row kind is INSERT or UPDATE_AFTER,\n+\t * returns false if the row kind is DELETE or UPDATE_BEFORE.\n+\t */\n+\tprivate boolean changeFlag(RowKind rowKind) {\n+\t\tswitch (rowKind) {\n+\t\t\tcase INSERT:\n+\t\t\tcase UPDATE_AFTER:\n+\t\t\t\treturn true;\n+\t\t\tcase DELETE:\n+\t\t\tcase UPDATE_BEFORE:\n+\t\t\t\treturn false;\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\n+\t\t\t\t\tString.format(\"Unknown row kind, the supported row kinds is: INSERT, UPDATE_BEFORE, UPDATE_AFTER,\" +\n+\t\t\t\t\t\t\" DELETE, but get: %s.\", rowKind));\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void executeBatch() throws SQLException {\n+\t\t// TODO: reuse the extracted key and avoid value copy in the future\n+\t\tfor (Tuple2<Boolean, RowData> tuple : reduceBuffer.values()) {\n+\t\t\tif (tuple.f0) {\n+\t\t\t\tupsertExecutor.addToBatch(tuple.f1);\n+\t\t\t} else {\n+\t\t\t\tdeleteExecutor.addToBatch(tuple.f1);\n+\t\t\t}\n+\t\t}\n+\t\tupsertExecutor.executeBatch();\n+\t\tdeleteExecutor.executeBatch();", "originalCommit": "3b212fec4e0c4453bc6da26180643487c01071d0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDAyMzkxMQ==", "url": "https://github.com/apache/flink/pull/12803#discussion_r580023911", "bodyText": "There is no matter which one is executed first, because there is no duplicate elements between upsertExecutor and deleteExecutor.", "author": "wuchong", "createdAt": "2021-02-22T07:10:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDAxOTgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDA0MzQyNQ==", "url": "https://github.com/apache/flink/pull/12803#discussion_r580043425", "bodyText": "Thanks @wuchong for the timely comments. However, I have observed that some records in deleteExecutor contains the same primary key as the records in upsertExecutor, and so they are eventually deleted from the database and never appear thereafter.  This problem only happens when \"connector.write.flush.max-rows\" is larger than 1, so I wonder if this could be a bug related to the bulk flush logic?\nThanks again for your attention : )", "author": "kylemeow", "createdAt": "2021-02-22T07:56:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDAxOTgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDA1MDMzNg==", "url": "https://github.com/apache/flink/pull/12803#discussion_r580050336", "bodyText": "And by the way, by exchanging the order of the two lines of code mentioned above, the data loss problem also disappears, which looks really strange.", "author": "kylemeow", "createdAt": "2021-02-22T08:10:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDAxOTgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDA2Mjc1NQ==", "url": "https://github.com/apache/flink/pull/12803#discussion_r580062755", "bodyText": "However, I have observed that some records in deleteExecutor contains the same primary key as the records in upsertExecutor.\n\nAre you sure about this? Because the delete records and upsert records are deduplicated by keys in a Map<KEY, Tuple> structure. It's hard to believe there are duplicate records with same key.\nBtw, Did you try Flink v1.12? We have fixed a problem with upsert statement in FLINK-15728.", "author": "wuchong", "createdAt": "2021-02-22T08:33:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDAxOTgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDk2NjU5OQ==", "url": "https://github.com/apache/flink/pull/12803#discussion_r580966599", "bodyText": "Hi @wuchong , glad to see the reply : )\nAfter careful verification, I have found you are right that the two lines are actually not the culprit. Rather, the problem disappeared beause I previously filtered (removed) all of the DELETE messages from the connector, as they are always misplaced at the end of all records during bulk operation, thus causing data loss.\nWhich is to say, when connector.write.flush.max-rows is set to 0 or 1, the DELETE messages are correctly put ahead of corresponding UPDATE messages. But as the bulk increases, DELETE messages are executed AFTER all UPDATE messages, and it seems that this reorder issue is not related to FLINK-15728 , so may I know if there are other reasons that might cause this abnormality?\nThanks : )", "author": "kylemeow", "createdAt": "2021-02-23T11:39:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDAxOTgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDk3NDQzNw==", "url": "https://github.com/apache/flink/pull/12803#discussion_r580974437", "bodyText": "When the bulk increase, DELETE messages and UPDATE messages will be merged into the reducedBuffer first. So there is no matter which one is executed first", "author": "wuchong", "createdAt": "2021-02-23T11:53:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDAxOTgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTU4ODUxNQ==", "url": "https://github.com/apache/flink/pull/12803#discussion_r581588515", "bodyText": "Yes, and eventually I realized that legacy connector options introduced in Flink 1.10  (i.e. 'connector.type' ) could not enable this feature, instead Flink would use JdbcBatchingOutputFormat which is the reason why the data is incorrect. After replacing the WITH parameters with current ones, the problem solved.\nThanks again for your great effort : )", "author": "kylemeow", "createdAt": "2021-02-24T03:18:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDAxOTgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTU4OTMyNA==", "url": "https://github.com/apache/flink/pull/12803#discussion_r581589324", "bodyText": "Yes. 'connector.type' is the legacy connector and shouldn't be used anymore.", "author": "wuchong", "createdAt": "2021-02-24T03:20:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDAxOTgzOQ=="}], "type": "inlineReview"}]}