{"pr_number": 13331, "pr_title": "[FLINK-19079][table-runtime] Import rowtime deduplicate operator", "pr_createdAt": "2020-09-05T10:00:41Z", "pr_url": "https://github.com/apache/flink/pull/13331", "timeline": [{"oid": "f37fa30b387d0c443139107f7b2a8dbdc6a8364d", "url": "https://github.com/apache/flink/commit/f37fa30b387d0c443139107f7b2a8dbdc6a8364d", "message": "[FLINK-19079][table-runtime] Import rowtime deduplicate operator", "committedDate": "2020-09-06T11:11:57Z", "type": "forcePushed"}, {"oid": "6a79badae175128326ed4e77708b48488009731c", "url": "https://github.com/apache/flink/commit/6a79badae175128326ed4e77708b48488009731c", "message": "[FLINK-19079][table-runtime] Import rowtime deduplicate operator", "committedDate": "2020-09-07T01:12:13Z", "type": "forcePushed"}, {"oid": "6988d189034d8ad024e5db27962cf589273a5534", "url": "https://github.com/apache/flink/commit/6988d189034d8ad024e5db27962cf589273a5534", "message": "[FLINK-19079][table-runtime] Import rowtime deduplicate operator", "committedDate": "2020-09-07T03:35:52Z", "type": "forcePushed"}, {"oid": "61470e6514fd5cb80250ed641e9e7e293c97fa07", "url": "https://github.com/apache/flink/commit/61470e6514fd5cb80250ed641e9e7e293c97fa07", "message": "[FLINK-19079][table-runtime] Import rowtime deduplicate operator", "committedDate": "2020-09-07T09:11:18Z", "type": "forcePushed"}, {"oid": "baed656a328a88791c638d87ac3b15091bc43408", "url": "https://github.com/apache/flink/commit/baed656a328a88791c638d87ac3b15091bc43408", "message": "[FLINK-19079][table-runtime] Import rowtime deduplicate operator", "committedDate": "2020-09-09T07:44:39Z", "type": "forcePushed"}, {"oid": "5e0c14bcf30a4640f90210be692e922a11d3dd1c", "url": "https://github.com/apache/flink/commit/5e0c14bcf30a4640f90210be692e922a11d3dd1c", "message": "[FLINK-19079][table-runtime] Import rowtime deduplicate operator", "committedDate": "2020-10-19T09:43:01Z", "type": "forcePushed"}, {"oid": "b4a2d274bd70b8006c267be02ee48ee5b9def3bb", "url": "https://github.com/apache/flink/commit/b4a2d274bd70b8006c267be02ee48ee5b9def3bb", "message": "[FLINK-19079][table-runtime] Import rowtime deduplicate operator", "committedDate": "2020-10-19T10:33:42Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk2NzI3NA==", "url": "https://github.com/apache/flink/pull/13331#discussion_r508967274", "bodyText": "Do not need copy.", "author": "danny0405", "createdAt": "2020-10-21T03:28:58Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateFunctionHelper.java", "diffHunk": "@@ -93,6 +95,239 @@ static void processFirstRow(\n \t\tout.collect(currentRow);\n \t}\n \n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last row,\n+\t * retracts previous element if needed.\n+\t *\n+\t * @param state state of function\n+\t * @param currentRow latest row received by deduplicate function\n+\t * @param serializer serializer to serialize the data\n+\t * @param out underlying collector\n+\t * @param rowtimeIndex index of row time field\n+\t * @param generateUpdateBefore flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert flag to gennerate INSERT message or not\n+\t */\n+\tstatic void processLastRowOnRowtime(\n+\t\t\tValueState<RowData> state,\n+\t\t\tRowData currentRow,\n+\t\t\tTypeSerializer<RowData> serializer,\n+\t\t\tCollector<RowData> out,\n+\t\t\tint rowtimeIndex,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert) throws Exception {\n+\n+\t\tcheckInsertOnly(currentRow);\n+\t\tRowData prevRow = state.value();\n+\t\tif (!isLastRow(prevRow, currentRow, rowtimeIndex)) {\n+\t\t\treturn;\n+\t\t}\n+\t\tstate.update(currentRow);\n+\n+\t\t// store all needed data to state\n+\t\tif (generateUpdateBefore || generateInsert) {\n+\t\t\tif (prevRow == null) {\n+\t\t\t\t// the first row, send INSERT message\n+\t\t\t\tcurrentRow.setRowKind(RowKind.INSERT);\n+\t\t\t\tout.collect(currentRow);\n+\t\t\t} else {\n+\t\t\t\tif (generateUpdateBefore) {\n+\t\t\t\t\tRowData copyRow = serializer.copy(prevRow);\n+\t\t\t\t\tcopyRow.setRowKind(RowKind.UPDATE_BEFORE);", "originalCommit": "b4a2d274bd70b8006c267be02ee48ee5b9def3bb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk2NzQyMw==", "url": "https://github.com/apache/flink/pull/13331#discussion_r508967423", "bodyText": "Can reuse the code in\nstatic void processLastRowOnRowtime(\n\t\t\tValueState<RowData> state,\n\t\t\tRowData currentRow,\n\t\t\tTypeSerializer<RowData> serializer,\n\t\t\tCollector<RowData> out,\n\t\t\tint rowtimeIndex,\n\t\t\tboolean generateUpdateBefore,\n\t\t\tboolean generateInsert)", "author": "danny0405", "createdAt": "2020-10-21T03:29:22Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateFunctionHelper.java", "diffHunk": "@@ -93,6 +95,239 @@ static void processFirstRow(\n \t\tout.collect(currentRow);\n \t}\n \n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last row,\n+\t * retracts previous element if needed.\n+\t *\n+\t * @param state state of function\n+\t * @param currentRow latest row received by deduplicate function\n+\t * @param serializer serializer to serialize the data\n+\t * @param out underlying collector\n+\t * @param rowtimeIndex index of row time field\n+\t * @param generateUpdateBefore flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert flag to gennerate INSERT message or not\n+\t */\n+\tstatic void processLastRowOnRowtime(\n+\t\t\tValueState<RowData> state,\n+\t\t\tRowData currentRow,\n+\t\t\tTypeSerializer<RowData> serializer,\n+\t\t\tCollector<RowData> out,\n+\t\t\tint rowtimeIndex,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert) throws Exception {\n+\n+\t\tcheckInsertOnly(currentRow);\n+\t\tRowData prevRow = state.value();\n+\t\tif (!isLastRow(prevRow, currentRow, rowtimeIndex)) {\n+\t\t\treturn;\n+\t\t}\n+\t\tstate.update(currentRow);\n+\n+\t\t// store all needed data to state\n+\t\tif (generateUpdateBefore || generateInsert) {\n+\t\t\tif (prevRow == null) {\n+\t\t\t\t// the first row, send INSERT message\n+\t\t\t\tcurrentRow.setRowKind(RowKind.INSERT);\n+\t\t\t\tout.collect(currentRow);\n+\t\t\t} else {\n+\t\t\t\tif (generateUpdateBefore) {\n+\t\t\t\t\tRowData copyRow = serializer.copy(prevRow);\n+\t\t\t\t\tcopyRow.setRowKind(RowKind.UPDATE_BEFORE);\n+\t\t\t\t\tout.collect(copyRow);\n+\t\t\t\t}\n+\t\t\t\tcurrentRow.setRowKind(RowKind.UPDATE_AFTER);\n+\t\t\t\tout.collect(currentRow);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tcurrentRow.setRowKind(RowKind.UPDATE_AFTER);\n+\t\t\tout.collect(currentRow);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last row,\n+\t * retracts previous element if needed.\n+\t *\n+\t * @param state state of function\n+\t * @param bufferedRows latest rows received by deduplicate function\n+\t * @param serializer serializer to serialize the data\n+\t * @param out underlying collector\n+\t * @param rowtimeIndex index of row time field\n+\t * @param generateUpdateBefore flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert flag to gennerate INSERT message or not\n+\t */\n+\tstatic void processLastRowOnRowtime(\n+\t\tValueState<RowData> state,\n+\t\tList<RowData> bufferedRows,\n+\t\tTypeSerializer<RowData> serializer,\n+\t\tCollector<RowData> out,\n+\t\tint rowtimeIndex,\n+\t\tboolean generateUpdateBefore,\n+\t\tboolean generateInsert) throws Exception {\n+\n+\t\tif (bufferedRows == null) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tRowData prevRow = state.value();\n+\t\tfor (RowData currentRow: bufferedRows) {\n+\t\t\tcheckInsertOnly(currentRow);\n+\t\t\tif (!isLastRow(prevRow, currentRow, rowtimeIndex)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t// store all needed data to state\n+\t\t\tif (generateUpdateBefore || generateInsert) {\n+\t\t\t\tif (prevRow == null) {\n+\t\t\t\t\t// the first row, send INSERT message\n+\t\t\t\t\tcurrentRow.setRowKind(RowKind.INSERT);\n+\t\t\t\t\tout.collect(currentRow);\n+\t\t\t\t} else {\n+\t\t\t\t\tif (generateUpdateBefore) {\n+\t\t\t\t\t\tRowData copyRow = serializer.copy(prevRow);\n+\t\t\t\t\t\tcopyRow.setRowKind(RowKind.UPDATE_BEFORE);\n+\t\t\t\t\t\tout.collect(copyRow);\n+\t\t\t\t\t}", "originalCommit": "b4a2d274bd70b8006c267be02ee48ee5b9def3bb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "42f18261f80da589a648414d6bd81c20730e0cc0", "url": "https://github.com/apache/flink/commit/42f18261f80da589a648414d6bd81c20730e0cc0", "message": "address comments", "committedDate": "2020-10-23T16:47:28Z", "type": "forcePushed"}, {"oid": "008e4ae2c2e10b712d84a4f324634a71fe5f84ac", "url": "https://github.com/apache/flink/commit/008e4ae2c2e10b712d84a4f324634a71fe5f84ac", "message": "[FLINK-19079][table-runtime] Import rowtime deduplicate operator", "committedDate": "2020-10-25T08:56:48Z", "type": "forcePushed"}, {"oid": "115ad9e4e8c7258d5831848a7144bfbb46a5b6f9", "url": "https://github.com/apache/flink/commit/115ad9e4e8c7258d5831848a7144bfbb46a5b6f9", "message": "[FLINK-19079][table-runtime] Introduce row time deduplicate operator", "committedDate": "2020-10-28T07:44:52Z", "type": "forcePushed"}, {"oid": "f3b2192b6cc216c691e833e40e52dfc64807e0a0", "url": "https://github.com/apache/flink/commit/f3b2192b6cc216c691e833e40e52dfc64807e0a0", "message": "[FLINK-19079][table-runtime] Introduce row time deduplicate operator", "committedDate": "2020-10-30T03:34:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkxNTI4Ng==", "url": "https://github.com/apache/flink/pull/13331#discussion_r514915286", "bodyText": "The code is almost same with processMiniBatchFirstRowOnRowtime, we can abstract the common code out here.", "author": "danny0405", "createdAt": "2020-10-30T07:34:29Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateFunctionHelper.java", "diffHunk": "@@ -147,6 +149,226 @@ static void processFirstRow(\n \t\tout.collect(currentRow);\n \t}\n \n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last row,\n+\t * retracts previous element if needed.\n+\t *\n+\t * @param state                state of function\n+\t * @param currentRow           latest row received by deduplicate function\n+\t * @param out                  underlying collector\n+\t * @param rowtimeIndex         index of row time field\n+\t * @param generateUpdateBefore flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert       flag to gennerate INSERT message or not\n+\t */\n+\tstatic void processLastRowOnRowtime(\n+\t\t\tValueState<RowData> state,\n+\t\t\tRowData currentRow,\n+\t\t\tCollector<RowData> out,\n+\t\t\tint rowtimeIndex,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert) throws Exception {\n+\n+\t\tcheckInsertOnly(currentRow);\n+\t\tRowData prevRow = state.value();\n+\t\tif (!isLastRow(prevRow, currentRow, rowtimeIndex)) {\n+\t\t\treturn;\n+\t\t}\n+\t\tstate.update(currentRow);\n+\n+\t\t// store all needed data to state\n+\t\tcollectRetractResult(\n+\t\t\t\tgenerateUpdateBefore,\n+\t\t\t\tgenerateInsert,\n+\t\t\t\tprevRow,\n+\t\t\t\tcurrentRow,\n+\t\t\t\tout,\n+\t\t\t\tnull\n+\t\t);\n+\t}\n+\n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last row,\n+\t * retracts previous element if needed.\n+\t *\n+\t * @param state                state of function\n+\t * @param bufferedRows         latest rows received by deduplicate function\n+\t * @param serializer           serializer to serialize the data\n+\t * @param out                  underlying collector\n+\t * @param rowtimeIndex         index of row time field\n+\t * @param generateUpdateBefore flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert       flag to gennerate INSERT message or not\n+\t */\n+\tstatic void processMiniBatchLastRowOnRowtime(\n+\t\t\tValueState<RowData> state,\n+\t\t\tList<RowData> bufferedRows,\n+\t\t\tTypeSerializer<RowData> serializer,\n+\t\t\tCollector<RowData> out,\n+\t\t\tint rowtimeIndex,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert) throws Exception {\n+\n+\t\tif (bufferedRows == null) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tRowData preRow = state.value();\n+\t\tfor (RowData currentRow : bufferedRows) {\n+\t\t\tcheckInsertOnly(currentRow);\n+\t\t\tif (!isLastRow(preRow, currentRow, rowtimeIndex)) {", "originalCommit": "f3b2192b6cc216c691e833e40e52dfc64807e0a0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkxNTY1Nw==", "url": "https://github.com/apache/flink/pull/13331#discussion_r514915657", "bodyText": "Both addInput and finishBundle can be moved to the base class.", "author": "danny0405", "createdAt": "2020-10-30T07:35:33Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/RowTimeMiniBatchDeduplicateKeepFirstRowFunction.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.deduplicate;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.util.Collector;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionHelper.processMiniBatchFirstRowOnRowTime;\n+\n+/**\n+ * This function is used to get the first row for every key partition in miniBatch mode.\n+ */\n+public class RowTimeMiniBatchDeduplicateKeepFirstRowFunction\n+\t\textends MiniBatchDeduplicateFunctionBase<RowData, RowData, List<RowData>, RowData, RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final int rowtimeIndex;\n+\tprivate final boolean generateUpdateBefore;\n+\tprivate final boolean generateInsert;\n+\n+\tpublic RowTimeMiniBatchDeduplicateKeepFirstRowFunction(\n+\t\t\tInternalTypeInfo<RowData> typeInfo,\n+\t\t\tTypeSerializer<RowData> serializer,\n+\t\t\tlong minRetentionTime,\n+\t\t\tint rowtimeIndex,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert) {\n+\t\tsuper(typeInfo, serializer, minRetentionTime);\n+\t\tthis.rowtimeIndex = rowtimeIndex;\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.generateInsert = generateInsert;\n+\t}\n+\n+\t@Override\n+\tpublic List<RowData> addInput(@Nullable List<RowData> value, RowData input) throws Exception {\n+\t\tif (value == null) {\n+\t\t\tvalue = new ArrayList<>();\n+\t\t}\n+\t\tvalue.add(serializer.copy(input));\n+\t\treturn value;\n+\t}\n+\n+\t@Override\n+\tpublic void finishBundle(Map<RowData, List<RowData>> buffer, Collector<RowData> out) throws Exception {\n+\t\tfor (Map.Entry<RowData, List<RowData>> entry : buffer.entrySet()) {\n+\t\t\tRowData currentKey = entry.getKey();\n+\t\t\tList<RowData> bufferedRows = entry.getValue();", "originalCommit": "f3b2192b6cc216c691e833e40e52dfc64807e0a0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc5MzQ2Nw==", "url": "https://github.com/apache/flink/pull/13331#discussion_r515793467", "bodyText": "It is only used in RowTimeMiniBatchDeduplicateFunction. We don't need to put it in the util class.", "author": "wuchong", "createdAt": "2020-11-02T08:00:20Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateFunctionHelper.java", "diffHunk": "@@ -147,6 +150,148 @@ static void processFirstRow(\n \t\tout.collect(currentRow);\n \t}\n \n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last\n+\t * or first row, retracts previous element if needed.\n+\t *\n+\t * @param state                 state of function\n+\t * @param currentRow            latest row received by deduplicate function\n+\t * @param out                   underlying collector\n+\t * @param generateUpdateBefore  flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert        flag to gennerate INSERT message or not\n+\t * @param orderFunctionProvider provider that provides an order function to judge first or last\n+\t */\n+\tpublic static void deduplicateOnRowTime(\n+\t\t\tValueState<RowData> state,\n+\t\t\tRowData currentRow,\n+\t\t\tCollector<RowData> out,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert,\n+\t\t\tDeduplicateOrderFunctionProvider orderFunctionProvider) throws Exception {\n+\n+\t\tcheckInsertOnly(currentRow);\n+\t\tRowData preRow = state.value();\n+\t\tif (!orderFunctionProvider.matches(preRow, currentRow)) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tcollectRetractResult(\n+\t\t\t\tgenerateUpdateBefore,\n+\t\t\t\tgenerateInsert,\n+\t\t\t\tpreRow,\n+\t\t\t\tcurrentRow,\n+\t\t\t\tout,\n+\t\t\t\tnull\n+\t\t);\n+\t\tstate.update(currentRow);\n+\t}\n+\n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last\n+\t * or first row, retracts previous element if needed.\n+\t *\n+\t * @param state                 state of function\n+\t * @param bufferedRows          latest row received by deduplicate function\n+\t * @param serializer            serializer to serialize the data\n+\t * @param out                   underlying collector\n+\t * @param generateUpdateBefore  flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert        flag to gennerate INSERT message or not\n+\t * @param orderFunctionProvider provider that provides an order function to judge first or last\n+\t */\n+\tpublic static void miniBatchDeduplicateOnRowTime(", "originalCommit": "0fcea2d9a006b405865e558b2bdb07311b87c2a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc5MzczMg==", "url": "https://github.com/apache/flink/pull/13331#discussion_r515793732", "bodyText": "I think bufferedRows would never be null, but maybe empty.", "author": "wuchong", "createdAt": "2020-11-02T08:00:55Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateFunctionHelper.java", "diffHunk": "@@ -147,6 +150,148 @@ static void processFirstRow(\n \t\tout.collect(currentRow);\n \t}\n \n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last\n+\t * or first row, retracts previous element if needed.\n+\t *\n+\t * @param state                 state of function\n+\t * @param currentRow            latest row received by deduplicate function\n+\t * @param out                   underlying collector\n+\t * @param generateUpdateBefore  flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert        flag to gennerate INSERT message or not\n+\t * @param orderFunctionProvider provider that provides an order function to judge first or last\n+\t */\n+\tpublic static void deduplicateOnRowTime(\n+\t\t\tValueState<RowData> state,\n+\t\t\tRowData currentRow,\n+\t\t\tCollector<RowData> out,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert,\n+\t\t\tDeduplicateOrderFunctionProvider orderFunctionProvider) throws Exception {\n+\n+\t\tcheckInsertOnly(currentRow);\n+\t\tRowData preRow = state.value();\n+\t\tif (!orderFunctionProvider.matches(preRow, currentRow)) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tcollectRetractResult(\n+\t\t\t\tgenerateUpdateBefore,\n+\t\t\t\tgenerateInsert,\n+\t\t\t\tpreRow,\n+\t\t\t\tcurrentRow,\n+\t\t\t\tout,\n+\t\t\t\tnull\n+\t\t);\n+\t\tstate.update(currentRow);\n+\t}\n+\n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last\n+\t * or first row, retracts previous element if needed.\n+\t *\n+\t * @param state                 state of function\n+\t * @param bufferedRows          latest row received by deduplicate function\n+\t * @param serializer            serializer to serialize the data\n+\t * @param out                   underlying collector\n+\t * @param generateUpdateBefore  flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert        flag to gennerate INSERT message or not\n+\t * @param orderFunctionProvider provider that provides an order function to judge first or last\n+\t */\n+\tpublic static void miniBatchDeduplicateOnRowTime(\n+\t\t\tValueState<RowData> state,\n+\t\t\tList<RowData> bufferedRows,\n+\t\t\tTypeSerializer<RowData> serializer,\n+\t\t\tCollector<RowData> out,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert,\n+\t\t\tDeduplicateOrderFunctionProvider orderFunctionProvider) throws Exception {\n+\t\tif (bufferedRows == null) {", "originalCommit": "0fcea2d9a006b405865e558b2bdb07311b87c2a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc5OTA0MA==", "url": "https://github.com/apache/flink/pull/13331#discussion_r515799040", "bodyText": "It would be better to remove this serializer, as it is never used in the base class. It confuses what's this used for, and it's error-prone, becuase it's nullable.", "author": "wuchong", "createdAt": "2020-11-02T08:13:03Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/MiniBatchDeduplicateFunctionBase.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.deduplicate;\n+\n+import org.apache.flink.api.common.state.StateTtlConfig;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.table.runtime.context.ExecutionContext;\n+import org.apache.flink.table.runtime.operators.bundle.MapBundleFunction;\n+\n+import static org.apache.flink.table.runtime.util.StateTtlConfigUtil.createTtlConfig;\n+\n+/**\n+ * Base class for miniBatch deduplicate function.\n+ * @param <T>   The type of the value in the state.\n+ * @param <K>   The type of the key in the bundle map.\n+ * @param <V>   The type of the value in the bundle map.\n+ * @param <IN>  Type of the input elements.\n+ * @param <OUT> Type of the returned elements.\n+ */\n+abstract class MiniBatchDeduplicateFunctionBase<T, K, V, IN, OUT> extends MapBundleFunction<K, V, IN, OUT> {\n+\n+\tprotected final TypeInformation<T> typeInfo;\n+\tprotected final TypeSerializer<OUT> serializer;\n+\tprotected final long minRetentionTime;\n+\t// state stores previous message under the key.\n+\tprotected ValueState<T> state;\n+\n+\tpublic MiniBatchDeduplicateFunctionBase(\n+\t\t\tTypeInformation<T> typeInfo,\n+\t\t\tTypeSerializer<OUT> serializer,", "originalCommit": "0fcea2d9a006b405865e558b2bdb07311b87c2a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc5OTE3MQ==", "url": "https://github.com/apache/flink/pull/13331#discussion_r515799171", "bodyText": "typeInfo -> stateType to be more specifically.", "author": "wuchong", "createdAt": "2020-11-02T08:13:19Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/MiniBatchDeduplicateFunctionBase.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.deduplicate;\n+\n+import org.apache.flink.api.common.state.StateTtlConfig;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.table.runtime.context.ExecutionContext;\n+import org.apache.flink.table.runtime.operators.bundle.MapBundleFunction;\n+\n+import static org.apache.flink.table.runtime.util.StateTtlConfigUtil.createTtlConfig;\n+\n+/**\n+ * Base class for miniBatch deduplicate function.\n+ * @param <T>   The type of the value in the state.\n+ * @param <K>   The type of the key in the bundle map.\n+ * @param <V>   The type of the value in the bundle map.\n+ * @param <IN>  Type of the input elements.\n+ * @param <OUT> Type of the returned elements.\n+ */\n+abstract class MiniBatchDeduplicateFunctionBase<T, K, V, IN, OUT> extends MapBundleFunction<K, V, IN, OUT> {\n+\n+\tprotected final TypeInformation<T> typeInfo;\n+\tprotected final TypeSerializer<OUT> serializer;\n+\tprotected final long minRetentionTime;\n+\t// state stores previous message under the key.\n+\tprotected ValueState<T> state;\n+\n+\tpublic MiniBatchDeduplicateFunctionBase(\n+\t\t\tTypeInformation<T> typeInfo,", "originalCommit": "0fcea2d9a006b405865e558b2bdb07311b87c2a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTc5OTQzOA==", "url": "https://github.com/apache/flink/pull/13331#discussion_r515799438", "bodyText": "Add serialVersionUID.", "author": "wuchong", "createdAt": "2020-11-02T08:13:54Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/MiniBatchDeduplicateFunctionBase.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.deduplicate;\n+\n+import org.apache.flink.api.common.state.StateTtlConfig;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.table.runtime.context.ExecutionContext;\n+import org.apache.flink.table.runtime.operators.bundle.MapBundleFunction;\n+\n+import static org.apache.flink.table.runtime.util.StateTtlConfigUtil.createTtlConfig;\n+\n+/**\n+ * Base class for miniBatch deduplicate function.\n+ * @param <T>   The type of the value in the state.\n+ * @param <K>   The type of the key in the bundle map.\n+ * @param <V>   The type of the value in the bundle map.\n+ * @param <IN>  Type of the input elements.\n+ * @param <OUT> Type of the returned elements.\n+ */\n+abstract class MiniBatchDeduplicateFunctionBase<T, K, V, IN, OUT> extends MapBundleFunction<K, V, IN, OUT> {", "originalCommit": "0fcea2d9a006b405865e558b2bdb07311b87c2a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgwMTYyMA==", "url": "https://github.com/apache/flink/pull/13331#discussion_r515801620", "bodyText": "Why we have to do a deep copy here?\nIf it is for not affecting the original preRow, we can set back RowKind after collecting preRow (non-minibatch mode also has this problem).", "author": "wuchong", "createdAt": "2020-11-02T08:18:33Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateFunctionHelper.java", "diffHunk": "@@ -147,6 +150,148 @@ static void processFirstRow(\n \t\tout.collect(currentRow);\n \t}\n \n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last\n+\t * or first row, retracts previous element if needed.\n+\t *\n+\t * @param state                 state of function\n+\t * @param currentRow            latest row received by deduplicate function\n+\t * @param out                   underlying collector\n+\t * @param generateUpdateBefore  flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert        flag to gennerate INSERT message or not\n+\t * @param orderFunctionProvider provider that provides an order function to judge first or last\n+\t */\n+\tpublic static void deduplicateOnRowTime(\n+\t\t\tValueState<RowData> state,\n+\t\t\tRowData currentRow,\n+\t\t\tCollector<RowData> out,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert,\n+\t\t\tDeduplicateOrderFunctionProvider orderFunctionProvider) throws Exception {\n+\n+\t\tcheckInsertOnly(currentRow);\n+\t\tRowData preRow = state.value();\n+\t\tif (!orderFunctionProvider.matches(preRow, currentRow)) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tcollectRetractResult(\n+\t\t\t\tgenerateUpdateBefore,\n+\t\t\t\tgenerateInsert,\n+\t\t\t\tpreRow,\n+\t\t\t\tcurrentRow,\n+\t\t\t\tout,\n+\t\t\t\tnull\n+\t\t);\n+\t\tstate.update(currentRow);\n+\t}\n+\n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last\n+\t * or first row, retracts previous element if needed.\n+\t *\n+\t * @param state                 state of function\n+\t * @param bufferedRows          latest row received by deduplicate function\n+\t * @param serializer            serializer to serialize the data\n+\t * @param out                   underlying collector\n+\t * @param generateUpdateBefore  flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert        flag to gennerate INSERT message or not\n+\t * @param orderFunctionProvider provider that provides an order function to judge first or last\n+\t */\n+\tpublic static void miniBatchDeduplicateOnRowTime(\n+\t\t\tValueState<RowData> state,\n+\t\t\tList<RowData> bufferedRows,\n+\t\t\tTypeSerializer<RowData> serializer,\n+\t\t\tCollector<RowData> out,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert,\n+\t\t\tDeduplicateOrderFunctionProvider orderFunctionProvider) throws Exception {\n+\t\tif (bufferedRows == null) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tRowData preRow = state.value();\n+\t\tfor (RowData currentRow : bufferedRows) {\n+\t\t\tcheckInsertOnly(currentRow);\n+\t\t\tif (!orderFunctionProvider.matches(preRow, currentRow)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tcollectRetractResult(\n+\t\t\t\t\tgenerateUpdateBefore,\n+\t\t\t\t\tgenerateInsert,\n+\t\t\t\t\tpreRow,\n+\t\t\t\t\tcurrentRow,\n+\t\t\t\t\tout,\n+\t\t\t\t\tserializer\n+\t\t\t);\n+\t\t\tpreRow = currentRow;\n+\t\t}\n+\t\tstate.update(preRow);\n+\t}\n+\n+\tprivate static void collectRetractResult(\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert,\n+\t\t\tRowData preRow,\n+\t\t\tRowData currentRow,\n+\t\t\tCollector<RowData> out,\n+\t\t\tTypeSerializer<RowData> serializer) {\n+\t\tif (generateUpdateBefore || generateInsert) {\n+\t\t\tif (preRow == null) {\n+\t\t\t\t// the first row, send INSERT message\n+\t\t\t\tcurrentRow.setRowKind(RowKind.INSERT);\n+\t\t\t\tout.collect(currentRow);\n+\t\t\t} else {\n+\t\t\t\tif (generateUpdateBefore) {\n+\t\t\t\t\tRowData copyRow;\n+\t\t\t\t\t// when miniBatch enabled, do a copy here, the serializer is not null\n+\t\t\t\t\tif (serializer != null) {\n+\t\t\t\t\t\tcopyRow = serializer.copy(preRow);", "originalCommit": "0fcea2d9a006b405865e558b2bdb07311b87c2a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgwODQzMw==", "url": "https://github.com/apache/flink/pull/13331#discussion_r515808433", "bodyText": "If this copying can be removed, the implementations of collectRetractResult and processLastRowOnProcTime are totally the same. I feel that collectRetractResult is not suitable, because it not always generates retractions.\nWhat about combine them in the the following new method?\nstatic void updateDeduplicateResult(\n\t\t\tRowData currentRow,  // new row result of deduplicate under current key\n\t\t\tValueState<RowData> state, // the state stores the previous collected row under current key\n\t\t\tboolean generateUpdateBefore,           \n\t\t\tboolean generateInsert,\n\t\t\tCollector<RowData> out) throws Exception {", "author": "wuchong", "createdAt": "2020-11-02T08:31:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgwMTYyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIzOTk5MQ==", "url": "https://github.com/apache/flink/pull/13331#discussion_r517239991", "bodyText": "the state update logic is a little different, I skip this one", "author": "leonardBang", "createdAt": "2020-11-04T10:23:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgwMTYyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgwOTY2Mg==", "url": "https://github.com/apache/flink/pull/13331#discussion_r515809662", "bodyText": "If this method can be moved in RowTimeMiniBatchDeduplicateFunction,  I think we don't need the special DeduplicateOrderFunctionProvider parameter then, because we have keepLastRow flag.", "author": "wuchong", "createdAt": "2020-11-02T08:34:29Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateFunctionHelper.java", "diffHunk": "@@ -147,6 +150,148 @@ static void processFirstRow(\n \t\tout.collect(currentRow);\n \t}\n \n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last\n+\t * or first row, retracts previous element if needed.\n+\t *\n+\t * @param state                 state of function\n+\t * @param currentRow            latest row received by deduplicate function\n+\t * @param out                   underlying collector\n+\t * @param generateUpdateBefore  flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert        flag to gennerate INSERT message or not\n+\t * @param orderFunctionProvider provider that provides an order function to judge first or last\n+\t */\n+\tpublic static void deduplicateOnRowTime(\n+\t\t\tValueState<RowData> state,\n+\t\t\tRowData currentRow,\n+\t\t\tCollector<RowData> out,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert,\n+\t\t\tDeduplicateOrderFunctionProvider orderFunctionProvider) throws Exception {\n+\n+\t\tcheckInsertOnly(currentRow);\n+\t\tRowData preRow = state.value();\n+\t\tif (!orderFunctionProvider.matches(preRow, currentRow)) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tcollectRetractResult(\n+\t\t\t\tgenerateUpdateBefore,\n+\t\t\t\tgenerateInsert,\n+\t\t\t\tpreRow,\n+\t\t\t\tcurrentRow,\n+\t\t\t\tout,\n+\t\t\t\tnull\n+\t\t);\n+\t\tstate.update(currentRow);\n+\t}\n+\n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last\n+\t * or first row, retracts previous element if needed.\n+\t *\n+\t * @param state                 state of function\n+\t * @param bufferedRows          latest row received by deduplicate function\n+\t * @param serializer            serializer to serialize the data\n+\t * @param out                   underlying collector\n+\t * @param generateUpdateBefore  flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert        flag to gennerate INSERT message or not\n+\t * @param orderFunctionProvider provider that provides an order function to judge first or last\n+\t */\n+\tpublic static void miniBatchDeduplicateOnRowTime(\n+\t\t\tValueState<RowData> state,\n+\t\t\tList<RowData> bufferedRows,\n+\t\t\tTypeSerializer<RowData> serializer,\n+\t\t\tCollector<RowData> out,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert,\n+\t\t\tDeduplicateOrderFunctionProvider orderFunctionProvider) throws Exception {", "originalCommit": "0fcea2d9a006b405865e558b2bdb07311b87c2a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgwOTk2MQ==", "url": "https://github.com/apache/flink/pull/13331#discussion_r515809961", "bodyText": "ditto.", "author": "wuchong", "createdAt": "2020-11-02T08:35:04Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateFunctionHelper.java", "diffHunk": "@@ -147,6 +150,148 @@ static void processFirstRow(\n \t\tout.collect(currentRow);\n \t}\n \n+\t/**\n+\t * Processes element to deduplicate on keys with row time semantic, sends current element if it is last\n+\t * or first row, retracts previous element if needed.\n+\t *\n+\t * @param state                 state of function\n+\t * @param currentRow            latest row received by deduplicate function\n+\t * @param out                   underlying collector\n+\t * @param generateUpdateBefore  flag to generate UPDATE_BEFORE message or not\n+\t * @param generateInsert        flag to gennerate INSERT message or not\n+\t * @param orderFunctionProvider provider that provides an order function to judge first or last\n+\t */\n+\tpublic static void deduplicateOnRowTime(", "originalCommit": "0fcea2d9a006b405865e558b2bdb07311b87c2a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxMTUwMg==", "url": "https://github.com/apache/flink/pull/13331#discussion_r515811502", "bodyText": "Miss @Test?", "author": "wuchong", "createdAt": "2020-11-02T08:37:55Z", "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/deduplicate/RowTimeDeduplicateFunctionTest.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.deduplicate;\n+\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.api.operators.KeyedProcessOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.data.RowData;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+\n+import static org.apache.flink.table.runtime.util.StreamRecordUtils.insertRecord;\n+\n+/**\n+ * Tests for {@link RowTimeDeduplicateFunction}.\n+ */\n+@RunWith(Parameterized.class)\n+public class RowTimeDeduplicateFunctionTest extends RowTimeDeduplicateFunctionTestBase {\n+\n+\tprivate final boolean generateUpdateBefore;\n+\tprivate final boolean generateInsert;\n+\n+\tpublic RowTimeDeduplicateFunctionTest(boolean generateUpdateBefore, boolean generateInsert) {\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.generateInsert = generateInsert;\n+\t}\n+\n+\t@Test\n+\tpublic void testRowTimeDeduplicateKeepFirstRow() throws Exception {\n+\t\tfinal boolean keepLastRow = false;\n+\t\tRowTimeDeduplicateFunction func = new RowTimeDeduplicateFunction(\n+\t\t\t\tinputRowType,\n+\t\t\t\tminTtlTime.toMilliseconds(),\n+\t\t\t\trowTimeIndex,\n+\t\t\t\tgenerateUpdateBefore,\n+\t\t\t\tgenerateInsert,\n+\t\t\t\tkeepLastRow);\n+\t\tKeyedProcessOperator<RowData, RowData, RowData> operator = new KeyedProcessOperator<>(func);\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness = createTestHarness(operator);\n+\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 13, 99L));\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 13, 99L));\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 12, 100L));\n+\t\ttestHarness.processElement(insertRecord(\"key2\", 11, 101L));\n+\n+\t\t// test 1: keep first row with row time\n+\t\ttestHarness.processWatermark(new Watermark(102));\n+\t\tassertor.assertOutputEqualsSorted(\"output wrong.\", getExpectOutput(keepLastRow, 1), testHarness.getOutput());\n+\n+\t\t// do a snapshot, close and restore again\n+\t\tOperatorSubtaskState snapshot = testHarness.snapshot(0L, 0);\n+\t\ttestHarness.close();\n+\n+\t\ttestHarness = createTestHarness(operator);\n+\t\ttestHarness.setup();\n+\t\ttestHarness.initializeState(snapshot);\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 12, 300L));\n+\t\ttestHarness.processElement(insertRecord(\"key2\", 11, 301L));\n+\t\ttestHarness.processElement(insertRecord(\"key3\", 5, 299L));\n+\n+\t\t// test 2: load snapshot state\n+\t\ttestHarness.processWatermark(new Watermark(302));\n+\n+\t\tassertor.assertOutputEqualsSorted(\"output wrong.\", getExpectOutput(keepLastRow, 2), testHarness.getOutput());\n+\n+\t\t// test 3: expire the state\n+\t\ttestHarness.setStateTtlProcessingTime(minTtlTime.toMilliseconds() + 1);\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 12, 400L));\n+\t\ttestHarness.processElement(insertRecord(\"key2\", 11, 401L));\n+\t\ttestHarness.processWatermark(402);\n+\n+\t\t// (\"key1\", 13, 99L) and (\"key2\", 11, 101L) had retired, thus output (\"key1\", 12, 200L),(\"key2\", 11, 201L)\n+\t\tassertor.assertOutputEqualsSorted(\"output wrong.\", getExpectOutput(keepLastRow, 3), testHarness.getOutput());\n+\t\ttestHarness.close();\n+\t}\n+\n+\tpublic void testRowTimeDeduplicateKeepLastRow() throws Exception {", "originalCommit": "0fcea2d9a006b405865e558b2bdb07311b87c2a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTgxNDcxNw==", "url": "https://github.com/apache/flink/pull/13331#discussion_r515814717", "bodyText": "It's super hard to understand what the expect result is.\nI think if the result are different, then we shouldn't parameterize them into one test. We can follow the way of TemporalRowTimeJoinOperatorTest that resue the same test data in a common private method, but delcare differenet expected output in different test methods.\nBesides, I think we can't share the expected output for both minibatch and non-minibatch mode?", "author": "wuchong", "createdAt": "2020-11-02T08:43:46Z", "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/deduplicate/RowTimeDeduplicateFunctionTest.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.deduplicate;\n+\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.api.operators.KeyedProcessOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.data.RowData;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+\n+import static org.apache.flink.table.runtime.util.StreamRecordUtils.insertRecord;\n+\n+/**\n+ * Tests for {@link RowTimeDeduplicateFunction}.\n+ */\n+@RunWith(Parameterized.class)\n+public class RowTimeDeduplicateFunctionTest extends RowTimeDeduplicateFunctionTestBase {\n+\n+\tprivate final boolean generateUpdateBefore;\n+\tprivate final boolean generateInsert;\n+\n+\tpublic RowTimeDeduplicateFunctionTest(boolean generateUpdateBefore, boolean generateInsert) {\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.generateInsert = generateInsert;\n+\t}\n+\n+\t@Test\n+\tpublic void testRowTimeDeduplicateKeepFirstRow() throws Exception {\n+\t\tfinal boolean keepLastRow = false;\n+\t\tRowTimeDeduplicateFunction func = new RowTimeDeduplicateFunction(\n+\t\t\t\tinputRowType,\n+\t\t\t\tminTtlTime.toMilliseconds(),\n+\t\t\t\trowTimeIndex,\n+\t\t\t\tgenerateUpdateBefore,\n+\t\t\t\tgenerateInsert,\n+\t\t\t\tkeepLastRow);\n+\t\tKeyedProcessOperator<RowData, RowData, RowData> operator = new KeyedProcessOperator<>(func);\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness = createTestHarness(operator);\n+\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 13, 99L));\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 13, 99L));\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 12, 100L));\n+\t\ttestHarness.processElement(insertRecord(\"key2\", 11, 101L));\n+\n+\t\t// test 1: keep first row with row time\n+\t\ttestHarness.processWatermark(new Watermark(102));\n+\t\tassertor.assertOutputEqualsSorted(\"output wrong.\", getExpectOutput(keepLastRow, 1), testHarness.getOutput());\n+\n+\t\t// do a snapshot, close and restore again\n+\t\tOperatorSubtaskState snapshot = testHarness.snapshot(0L, 0);\n+\t\ttestHarness.close();\n+\n+\t\ttestHarness = createTestHarness(operator);\n+\t\ttestHarness.setup();\n+\t\ttestHarness.initializeState(snapshot);\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 12, 300L));\n+\t\ttestHarness.processElement(insertRecord(\"key2\", 11, 301L));\n+\t\ttestHarness.processElement(insertRecord(\"key3\", 5, 299L));\n+\n+\t\t// test 2: load snapshot state\n+\t\ttestHarness.processWatermark(new Watermark(302));\n+\n+\t\tassertor.assertOutputEqualsSorted(\"output wrong.\", getExpectOutput(keepLastRow, 2), testHarness.getOutput());\n+\n+\t\t// test 3: expire the state\n+\t\ttestHarness.setStateTtlProcessingTime(minTtlTime.toMilliseconds() + 1);\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 12, 400L));\n+\t\ttestHarness.processElement(insertRecord(\"key2\", 11, 401L));\n+\t\ttestHarness.processWatermark(402);\n+\n+\t\t// (\"key1\", 13, 99L) and (\"key2\", 11, 101L) had retired, thus output (\"key1\", 12, 200L),(\"key2\", 11, 201L)\n+\t\tassertor.assertOutputEqualsSorted(\"output wrong.\", getExpectOutput(keepLastRow, 3), testHarness.getOutput());\n+\t\ttestHarness.close();\n+\t}\n+\n+\tpublic void testRowTimeDeduplicateKeepLastRow() throws Exception {\n+\t\tfinal boolean keepLastRow = true;\n+\t\tRowTimeDeduplicateFunction func = new RowTimeDeduplicateFunction(\n+\t\t\t\tinputRowType,\n+\t\t\t\tminTtlTime.toMilliseconds(),\n+\t\t\t\trowTimeIndex,\n+\t\t\t\tgenerateUpdateBefore,\n+\t\t\t\tgenerateInsert,\n+\t\t\t\ttrue);\n+\t\tKeyedProcessOperator<RowData, RowData, RowData> operator = new KeyedProcessOperator<>(func);\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness = createTestHarness(operator);\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 13, 99L));\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 12, 100L));\n+\t\ttestHarness.processElement(insertRecord(\"key2\", 11, 101L));\n+\n+\t\t// test 1: keep last row with row time\n+\t\ttestHarness.processWatermark(new Watermark(102));\n+\t\tassertor.assertOutputEqualsSorted(\"output wrong.\", getExpectOutput(keepLastRow, 1), testHarness.getOutput());\n+\n+\t\t// do a snapshot, close and restore again\n+\t\tOperatorSubtaskState snapshot = testHarness.snapshot(0L, 0);\n+\t\ttestHarness.close();\n+\n+\t\ttestHarness = createTestHarness(operator);\n+\t\ttestHarness.setup();\n+\t\ttestHarness.initializeState(snapshot);\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 12, 300L));\n+\t\ttestHarness.processElement(insertRecord(\"key2\", 11, 301L));\n+\t\ttestHarness.processElement(insertRecord(\"key3\", 5, 299L));\n+\n+\t\t// test 2: load snapshot state\n+\t\ttestHarness.processWatermark(new Watermark(302));\n+\t\tassertor.assertOutputEqualsSorted(\"output wrong.\", getExpectOutput(keepLastRow, 2), testHarness.getOutput());\n+\n+\t\t// test 3: expire the state\n+\t\ttestHarness.setStateTtlProcessingTime(minTtlTime.toMilliseconds() + 1);\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 12, 400L));\n+\t\ttestHarness.processElement(insertRecord(\"key2\", 11, 401L));\n+\t\ttestHarness.processWatermark(402);\n+\n+\t\t// all state has expired, so the record (\"key1\", 12, 400L), (\"key2\", 12, 401L) will be INSERT message\n+\t\tassertor.assertOutputEqualsSorted(\"output wrong.\", getExpectOutput(keepLastRow, 3), testHarness.getOutput());", "originalCommit": "0fcea2d9a006b405865e558b2bdb07311b87c2a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2797547877ee7c77a07baa21e86bae69276258af", "url": "https://github.com/apache/flink/commit/2797547877ee7c77a07baa21e86bae69276258af", "message": "address jark's comments", "committedDate": "2020-11-04T10:22:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzg1NDkyMg==", "url": "https://github.com/apache/flink/pull/13331#discussion_r517854922", "bodyText": "nit: would be better to call RowTimeDeduplicateFunctionTest, minibatch is the parameter.", "author": "wuchong", "createdAt": "2020-11-05T08:02:08Z", "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/deduplicate/RowTimeMiniBatchDeduplicateFunctionTest.java", "diffHunk": "@@ -0,0 +1,363 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.deduplicate;\n+\n+import org.apache.flink.api.common.time.Time;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.api.operators.KeyedProcessOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.operators.bundle.KeyedMapBundleOperator;\n+import org.apache.flink.table.runtime.operators.bundle.trigger.CountBundleTrigger;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.util.BinaryRowDataKeySelector;\n+import org.apache.flink.table.runtime.util.GenericRowRecordSortComparator;\n+import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.IntType;\n+import org.apache.flink.table.types.logical.VarCharType;\n+import org.apache.flink.types.RowKind;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+\n+import static org.apache.flink.table.runtime.util.StreamRecordUtils.insertRecord;\n+import static org.apache.flink.table.runtime.util.StreamRecordUtils.record;\n+\n+/**\n+ * Harness tests for {@link RowTimeDeduplicateFunction} and {@link RowTimeMiniBatchDeduplicateFunction}.\n+ */\n+@RunWith(Parameterized.class)\n+public class RowTimeMiniBatchDeduplicateFunctionTest {", "originalCommit": "b3252a0d7e4481aebb849b2777ae544209cff1e8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzg1NTE5Mg==", "url": "https://github.com/apache/flink/pull/13331#discussion_r517855192", "bodyText": "This adds duplicate result into the output.", "author": "wuchong", "createdAt": "2020-11-05T08:02:45Z", "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/deduplicate/RowTimeMiniBatchDeduplicateFunctionTest.java", "diffHunk": "@@ -0,0 +1,363 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.deduplicate;\n+\n+import org.apache.flink.api.common.time.Time;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n+import org.apache.flink.streaming.api.operators.KeyedProcessOperator;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.operators.bundle.KeyedMapBundleOperator;\n+import org.apache.flink.table.runtime.operators.bundle.trigger.CountBundleTrigger;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.util.BinaryRowDataKeySelector;\n+import org.apache.flink.table.runtime.util.GenericRowRecordSortComparator;\n+import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.IntType;\n+import org.apache.flink.table.types.logical.VarCharType;\n+import org.apache.flink.types.RowKind;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+\n+import static org.apache.flink.table.runtime.util.StreamRecordUtils.insertRecord;\n+import static org.apache.flink.table.runtime.util.StreamRecordUtils.record;\n+\n+/**\n+ * Harness tests for {@link RowTimeDeduplicateFunction} and {@link RowTimeMiniBatchDeduplicateFunction}.\n+ */\n+@RunWith(Parameterized.class)\n+public class RowTimeMiniBatchDeduplicateFunctionTest {\n+\n+\tprivate final long miniBatchSize = 4L;\n+\tprivate Time minTtlTime = Time.milliseconds(10);\n+\tprivate InternalTypeInfo inputRowType = InternalTypeInfo.ofFields(\n+\t\t\tnew VarCharType(VarCharType.MAX_LENGTH),\n+\t\t\tnew IntType(),\n+\t\t\tnew BigIntType());\n+\tprivate TypeSerializer<RowData> serializer = inputRowType.toSerializer();\n+\tprivate int rowTimeIndex = 2;\n+\tprivate int rowKeyIndex = 0;\n+\tprivate BinaryRowDataKeySelector rowKeySelector = new BinaryRowDataKeySelector(\n+\t\t\tnew int[]{rowKeyIndex},\n+\t\t\tinputRowType.toRowFieldTypes());\n+\tprivate RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(\n+\t\t\tinputRowType.toRowFieldTypes(),\n+\t\t\tnew GenericRowRecordSortComparator(rowKeyIndex, inputRowType.toRowFieldTypes()[rowKeyIndex]));\n+\n+\tprivate final boolean miniBatchEnable;\n+\n+\tpublic RowTimeMiniBatchDeduplicateFunctionTest(boolean miniBacthEnable) {\n+\t\tthis.miniBatchEnable = miniBacthEnable;\n+\t}\n+\n+\t@Test\n+\tpublic void testRowTimeDeduplicateKeepFirstRow() throws Exception {\n+\t\tList<Object> expectedOutput = new ArrayList<>();\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key1\", 13, 99L));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key2\", 11, 101L));\n+\t\texpectedOutput.add(new Watermark(102));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key3\", 5, 299L));\n+\t\texpectedOutput.add(new Watermark(302));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key3\", 5, 299L));\n+\t\texpectedOutput.add(new Watermark(302));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key1\", 12, 400L));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key2\", 11, 401L));\n+\t\texpectedOutput.add(new Watermark(402));\n+\n+\t\t// generateUpdateBefore: true, generateInsert: true\n+\t\ttestRowTimeDeduplicateKeepFirstRow(true, true, expectedOutput);\n+\n+\t\t// generateUpdateBefore: true, generateInsert: false\n+\t\ttestRowTimeDeduplicateKeepFirstRow(true, false, expectedOutput);\n+\n+\t\t// generateUpdateBefore: false, generateInsert: true\n+\t\ttestRowTimeDeduplicateKeepFirstRow(false, true, expectedOutput);\n+\n+\t\t// generateUpdateBefore: false, generateInsert: false\n+\t\texpectedOutput.clear();\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key1\", 13, 99L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key2\", 11, 101L));\n+\t\texpectedOutput.add(new Watermark(102));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key3\", 5, 299L));\n+\t\texpectedOutput.add(new Watermark(302));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key3\", 5, 299L));\n+\t\texpectedOutput.add(new Watermark(302));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key1\", 12, 400L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key2\", 11, 401L));\n+\t\texpectedOutput.add(new Watermark(402));\n+\t\ttestRowTimeDeduplicateKeepFirstRow(false, false, expectedOutput);\n+\n+\t}\n+\n+\t@Test\n+\tpublic void testRowTimeDeduplicateKeepLastRow() throws Exception {\n+\t\tList<Object> expectedOutput = new ArrayList<>();\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key1\", 13, 99L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_BEFORE, \"key1\", 13, 99L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key1\", 12, 100L));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key2\", 11, 101L));\n+\t\texpectedOutput.add(new Watermark(102));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_BEFORE, \"key1\", 12, 100L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key1\", 12, 300L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_BEFORE, \"key2\", 11, 101L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key2\", 11, 301L));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key3\", 5, 299L));\n+\t\texpectedOutput.add(new Watermark(302));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_BEFORE, \"key1\", 12, 100L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key1\", 12, 300L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_BEFORE, \"key2\", 11, 101L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key2\", 11, 301L));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key3\", 5, 299L));\n+\t\texpectedOutput.add(new Watermark(302));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key1\", 12, 400L));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key2\", 11, 401L));\n+\t\texpectedOutput.add(new Watermark(402));\n+\n+\t\t// generateUpdateBefore: true, generateInsert: true\n+\t\ttestRowTimeDeduplicateKeepLastRow(true, true, expectedOutput);\n+\n+\t\t// generateUpdateBefore: true, generateInsert: false\n+\t\ttestRowTimeDeduplicateKeepLastRow(true, false, expectedOutput);\n+\n+\t\t// generateUpdateBefore: false, generateInsert: true\n+\t\texpectedOutput.clear();\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key1\", 13, 99L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key1\", 12, 100L));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key2\", 11, 101L));\n+\t\texpectedOutput.add(new Watermark(102));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key1\", 12, 300L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key2\", 11, 301L));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key3\", 5, 299L));\n+\t\texpectedOutput.add(new Watermark(302));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key1\", 12, 300L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key2\", 11, 301L));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key3\", 5, 299L));\n+\t\texpectedOutput.add(new Watermark(302));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key1\", 12, 400L));\n+\t\texpectedOutput.add(record(RowKind.INSERT, \"key2\", 11, 401L));\n+\t\texpectedOutput.add(new Watermark(402));\n+\t\ttestRowTimeDeduplicateKeepLastRow(false, true, expectedOutput);\n+\n+\t\t// generateUpdateBefore: false, generateInsert: false\n+\t\texpectedOutput.clear();\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key1\", 13, 99L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key1\", 12, 100L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key2\", 11, 101L));\n+\t\texpectedOutput.add(new Watermark(102));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key1\", 12, 300L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key2\", 11, 301L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key3\", 5, 299L));\n+\t\texpectedOutput.add(new Watermark(302));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key1\", 12, 300L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key2\", 11, 301L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key3\", 5, 299L));\n+\t\texpectedOutput.add(new Watermark(302));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key1\", 12, 400L));\n+\t\texpectedOutput.add(record(RowKind.UPDATE_AFTER, \"key2\", 11, 401L));\n+\t\texpectedOutput.add(new Watermark(402));\n+\t\ttestRowTimeDeduplicateKeepLastRow(false, false, expectedOutput);\n+\t}\n+\n+\tprivate void testRowTimeDeduplicateKeepFirstRow(\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tboolean generateInsert,\n+\t\t\tList<Object> expectedOutput) throws Exception {\n+\t\tfinal boolean keepLastRow = false;\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness;\n+\t\tKeyedMapBundleOperator<RowData, RowData, RowData, RowData> keyedMapBundleOperator = null;\n+\t\tKeyedProcessOperator keyedProcessOperator = null;\n+\t\tif (miniBatchEnable) {\n+\t\t\tRowTimeMiniBatchDeduplicateFunction func = new RowTimeMiniBatchDeduplicateFunction(\n+\t\t\t\t\tinputRowType,\n+\t\t\t\t\tserializer,\n+\t\t\t\t\tminTtlTime.toMilliseconds(),\n+\t\t\t\t\trowTimeIndex,\n+\t\t\t\t\tgenerateUpdateBefore,\n+\t\t\t\t\tgenerateInsert,\n+\t\t\t\t\tkeepLastRow);\n+\t\t\tCountBundleTrigger trigger = new CountBundleTrigger<RowData>(miniBatchSize);\n+\t\t\tkeyedMapBundleOperator = new KeyedMapBundleOperator(func, trigger);\n+\t\t\ttestHarness = createTestHarness(keyedMapBundleOperator);\n+\t\t} else {\n+\t\t\tRowTimeDeduplicateFunction func = new RowTimeDeduplicateFunction(\n+\t\t\t\t\tinputRowType,\n+\t\t\t\t\tminTtlTime.toMilliseconds(),\n+\t\t\t\t\trowTimeIndex,\n+\t\t\t\t\tgenerateUpdateBefore,\n+\t\t\t\t\tgenerateInsert,\n+\t\t\t\t\tkeepLastRow);\n+\t\t\tkeyedProcessOperator = new KeyedProcessOperator<>(func);\n+\t\t\ttestHarness = createTestHarness(keyedProcessOperator);\n+\t\t}\n+\n+\t\tList<Object> actualOutput = new ArrayList<>();\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 13, 99L));\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 13, 99L));\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 12, 100L));\n+\t\ttestHarness.processElement(insertRecord(\"key2\", 11, 101L));\n+\n+\t\t// test 1: keep first row with row time\n+\t\ttestHarness.processWatermark(new Watermark(102));\n+\t\tactualOutput.addAll(testHarness.getOutput());\n+\n+\t\t// do a snapshot, close and restore again\n+\t\tOperatorSubtaskState snapshot = testHarness.snapshot(0L, 0);\n+\t\ttestHarness.close();\n+\n+\t\tif (miniBatchEnable) {\n+\t\t\ttestHarness = createTestHarness(keyedMapBundleOperator);\n+\t\t} else {\n+\t\t\ttestHarness = createTestHarness(keyedProcessOperator);\n+\t\t}\n+\n+\t\ttestHarness.setup();\n+\t\ttestHarness.initializeState(snapshot);\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(insertRecord(\"key1\", 12, 300L));\n+\t\ttestHarness.processElement(insertRecord(\"key2\", 11, 301L));\n+\t\ttestHarness.processElement(insertRecord(\"key3\", 5, 299L));\n+\n+\t\t// test 2:  load snapshot state\n+\t\ttestHarness.processWatermark(new Watermark(302));\n+\t\tactualOutput.addAll(testHarness.getOutput());", "originalCommit": "b3252a0d7e4481aebb849b2777ae544209cff1e8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1755f4c73e181e591f39adaddd10413f75d6989e", "url": "https://github.com/apache/flink/commit/1755f4c73e181e591f39adaddd10413f75d6989e", "message": "[FLINK-19079][table-runtime] Introduce row time deduplicate operator", "committedDate": "2020-11-05T08:15:13Z", "type": "commit"}, {"oid": "1755f4c73e181e591f39adaddd10413f75d6989e", "url": "https://github.com/apache/flink/commit/1755f4c73e181e591f39adaddd10413f75d6989e", "message": "[FLINK-19079][table-runtime] Introduce row time deduplicate operator", "committedDate": "2020-11-05T08:15:13Z", "type": "forcePushed"}]}