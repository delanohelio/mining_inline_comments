{"pr_number": 11602, "pr_title": "[FLINK-16912][parquet] Introduce table row write support for parquet writer", "pr_createdAt": "2020-04-01T12:59:23Z", "pr_url": "https://github.com/apache/flink/pull/11602", "timeline": [{"oid": "b28a0ffe70a414addafd0d3b481292a2831d4e10", "url": "https://github.com/apache/flink/commit/b28a0ffe70a414addafd0d3b481292a2831d4e10", "message": "[FLINK-16912][parquet] Introduce table row write support for parquet writer", "committedDate": "2020-04-01T12:50:45Z", "type": "commit"}, {"oid": "3f7c4f6dd3c6f9cdb66d9d5a1dcb456b567436c3", "url": "https://github.com/apache/flink/commit/3f7c4f6dd3c6f9cdb66d9d5a1dcb456b567436c3", "message": "Remove is legacy", "committedDate": "2020-04-02T07:59:41Z", "type": "commit"}, {"oid": "95f0deb44dfae1be800260732e47bf8ceaf9ef67", "url": "https://github.com/apache/flink/commit/95f0deb44dfae1be800260732e47bf8ceaf9ef67", "message": "extract SerializableConfiguration to util", "committedDate": "2020-04-02T08:58:25Z", "type": "commit"}, {"oid": "5dce0e57680a2e3b34f27ec2fac8ed7a59d00dbe", "url": "https://github.com/apache/flink/commit/5dce0e57680a2e3b34f27ec2fac8ed7a59d00dbe", "message": "checkstyle", "committedDate": "2020-04-03T02:04:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc1NjY5OQ==", "url": "https://github.com/apache/flink/pull/11602#discussion_r402756699", "bodyText": "I think #createDecimalWriter is better to align #createWriter", "author": "leonardBang", "createdAt": "2020-04-03T06:12:53Z", "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/row/ParquetRowDataWriter.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.parquet.row;\n+\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.Decimal;\n+import org.apache.flink.table.dataformat.SqlTimestamp;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.LocalZonedTimestampType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TimestampType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.io.api.RecordConsumer;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.Type;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.sql.Timestamp;\n+import java.util.Arrays;\n+\n+import static org.apache.flink.formats.parquet.utils.ParquetSchemaConverter.computeMinBytesForDecimalPrecision;\n+import static org.apache.flink.formats.parquet.vector.reader.TimestampColumnReader.JULIAN_EPOCH_OFFSET_DAYS;\n+import static org.apache.flink.formats.parquet.vector.reader.TimestampColumnReader.MILLIS_IN_DAY;\n+import static org.apache.flink.formats.parquet.vector.reader.TimestampColumnReader.NANOS_PER_MILLISECOND;\n+import static org.apache.flink.formats.parquet.vector.reader.TimestampColumnReader.NANOS_PER_SECOND;\n+\n+/**\n+ * Writes a record to the Parquet API with the expected schema in order to be written to a file.\n+ */\n+public class ParquetRowDataWriter {\n+\n+\tprivate final RecordConsumer recordConsumer;\n+\tprivate final boolean utcTimestamp;\n+\n+\tprivate final FieldWriter[] filedWriters;\n+\tprivate final String[] fieldNames;\n+\n+\tpublic ParquetRowDataWriter(\n+\t\t\tRecordConsumer recordConsumer,\n+\t\t\tRowType rowType,\n+\t\t\tGroupType schema,\n+\t\t\tboolean utcTimestamp) {\n+\t\tthis.recordConsumer = recordConsumer;\n+\t\tthis.utcTimestamp = utcTimestamp;\n+\n+\t\tthis.filedWriters = new FieldWriter[rowType.getFieldCount()];\n+\t\tthis.fieldNames = rowType.getFieldNames().toArray(new String[0]);\n+\t\tfor (int i = 0; i < rowType.getFieldCount(); i++) {\n+\t\t\tthis.filedWriters[i] = createWriter(rowType.getTypeAt(i), schema.getType(i));\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * It writes a record to Parquet.\n+\t *\n+\t * @param record Contains the record that is going to be written.\n+\t */\n+\tpublic void write(final BaseRow record) {\n+\t\trecordConsumer.startMessage();\n+\t\tfor (int i = 0; i < filedWriters.length; i++) {\n+\t\t\tif (!record.isNullAt(i)) {\n+\t\t\t\tString fieldName = fieldNames[i];\n+\t\t\t\tFieldWriter writer = filedWriters[i];\n+\n+\t\t\t\trecordConsumer.startField(fieldName, i);\n+\t\t\t\twriter.write(record, i);\n+\t\t\t\trecordConsumer.endField(fieldName, i);\n+\t\t\t}\n+\t\t}\n+\t\trecordConsumer.endMessage();\n+\t}\n+\n+\tprivate FieldWriter createWriter(LogicalType t, Type type) {\n+\t\tif (type.isPrimitive()) {\n+\t\t\tswitch (t.getTypeRoot()) {\n+\t\t\t\tcase CHAR:\n+\t\t\t\tcase VARCHAR:\n+\t\t\t\t\treturn new StringWriter();\n+\t\t\t\tcase BOOLEAN:\n+\t\t\t\t\treturn new BooleanWriter();\n+\t\t\t\tcase BINARY:\n+\t\t\t\tcase VARBINARY:\n+\t\t\t\t\treturn new BinaryWriter();\n+\t\t\t\tcase DECIMAL:\n+\t\t\t\t\tDecimalType decimalType = (DecimalType) t;\n+\t\t\t\t\treturn makeDecimalWriter(decimalType.getPrecision(), decimalType.getScale());\n+\t\t\t\tcase TINYINT:\n+\t\t\t\t\treturn new ByteWriter();\n+\t\t\t\tcase SMALLINT:\n+\t\t\t\t\treturn new ShortWriter();\n+\t\t\t\tcase DATE:\n+\t\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n+\t\t\t\tcase INTEGER:\n+\t\t\t\t\treturn new IntWriter();\n+\t\t\t\tcase BIGINT:\n+\t\t\t\t\treturn new LongWriter();\n+\t\t\t\tcase FLOAT:\n+\t\t\t\t\treturn new FloatWriter();\n+\t\t\t\tcase DOUBLE:\n+\t\t\t\t\treturn new DoubleWriter();\n+\t\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n+\t\t\t\t\tTimestampType timestampType = (TimestampType) t;\n+\t\t\t\t\treturn new TimestampWriter(timestampType.getPrecision());\n+\t\t\t\tcase TIMESTAMP_WITH_LOCAL_TIME_ZONE:\n+\t\t\t\t\tLocalZonedTimestampType localZonedTimestampType = (LocalZonedTimestampType) t;\n+\t\t\t\t\treturn new TimestampWriter(localZonedTimestampType.getPrecision());\n+\t\t\t\tdefault:\n+\t\t\t\t\tthrow new UnsupportedOperationException(\"Unsupported type: \" + type);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tthrow new IllegalArgumentException(\"Unsupported  data type: \" + t);\n+\t\t}\n+\t}\n+\n+\tprivate interface FieldWriter {\n+\n+\t\tvoid write(BaseRow row, int ordinal);\n+\t}\n+\n+\tprivate class BooleanWriter implements FieldWriter {\n+\n+\t\t@Override\n+\t\tpublic void write(BaseRow row, int ordinal) {\n+\t\t\trecordConsumer.addBoolean(row.getBoolean(ordinal));\n+\t\t}\n+\t}\n+\n+\tprivate class ByteWriter implements FieldWriter {\n+\n+\t\t@Override\n+\t\tpublic void write(BaseRow row, int ordinal) {\n+\t\t\trecordConsumer.addInteger(row.getByte(ordinal));\n+\t\t}\n+\t}\n+\n+\tprivate class ShortWriter implements FieldWriter {\n+\n+\t\t@Override\n+\t\tpublic void write(BaseRow row, int ordinal) {\n+\t\t\trecordConsumer.addInteger(row.getShort(ordinal));\n+\t\t}\n+\t}\n+\n+\tprivate class LongWriter implements FieldWriter {\n+\n+\t\t@Override\n+\t\tpublic void write(BaseRow row, int ordinal) {\n+\t\t\trecordConsumer.addLong(row.getLong(ordinal));\n+\t\t}\n+\t}\n+\n+\tprivate class FloatWriter implements FieldWriter {\n+\n+\t\t@Override\n+\t\tpublic void write(BaseRow row, int ordinal) {\n+\t\t\trecordConsumer.addFloat(row.getFloat(ordinal));\n+\t\t}\n+\t}\n+\n+\tprivate class DoubleWriter implements FieldWriter {\n+\n+\t\t@Override\n+\t\tpublic void write(BaseRow row, int ordinal) {\n+\t\t\trecordConsumer.addDouble(row.getDouble(ordinal));\n+\t\t}\n+\t}\n+\n+\tprivate class StringWriter implements FieldWriter {\n+\n+\t\t@Override\n+\t\tpublic void write(BaseRow row, int ordinal) {\n+\t\t\trecordConsumer.addBinary(\n+\t\t\t\tBinary.fromReusedByteArray(row.getString(ordinal).getBytes()));\n+\t\t}\n+\t}\n+\n+\tprivate class BinaryWriter implements FieldWriter {\n+\n+\t\t@Override\n+\t\tpublic void write(BaseRow row, int ordinal) {\n+\t\t\trecordConsumer.addBinary(\n+\t\t\t\t\tBinary.fromReusedByteArray(row.getBinary(ordinal)));\n+\t\t}\n+\t}\n+\n+\tprivate class IntWriter implements FieldWriter {\n+\n+\t\t@Override\n+\t\tpublic void write(BaseRow row, int ordinal) {\n+\t\t\trecordConsumer.addInteger(row.getInt(ordinal));\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * We only support INT96 bytes now, julianDay(4) + nanosOfDay(8).\n+\t * See https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#timestamp\n+\t * TIMESTAMP_MILLIS and TIMESTAMP_MICROS are the deprecated ConvertedType.\n+\t */\n+\tprivate class TimestampWriter implements FieldWriter {\n+\n+\t\tprivate final int precision;\n+\n+\t\tprivate TimestampWriter(int precision) {\n+\t\t\tthis.precision = precision;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void write(BaseRow row, int ordinal) {\n+\t\t\trecordConsumer.addBinary(timestampToInt96(row.getTimestamp(ordinal, precision)));\n+\t\t}\n+\t}\n+\n+\tprivate Binary timestampToInt96(SqlTimestamp sqlTimestamp) {\n+\t\tint julianDay;\n+\t\tlong nanosOfDay;\n+\t\tif (utcTimestamp) {\n+\t\t\tlong mills = sqlTimestamp.getMillisecond();\n+\t\t\tjulianDay = (int) ((mills / MILLIS_IN_DAY) + JULIAN_EPOCH_OFFSET_DAYS);\n+\t\t\tnanosOfDay = (mills % MILLIS_IN_DAY) * NANOS_PER_MILLISECOND + sqlTimestamp.getNanoOfMillisecond();\n+\t\t} else {\n+\t\t\tTimestamp timestamp = sqlTimestamp.toTimestamp();\n+\t\t\tlong mills = timestamp.getTime();\n+\t\t\tjulianDay = (int) ((mills / MILLIS_IN_DAY) + JULIAN_EPOCH_OFFSET_DAYS);\n+\t\t\tnanosOfDay = ((mills % MILLIS_IN_DAY) / 1000) * NANOS_PER_SECOND + timestamp.getNanos();\n+\t\t}\n+\n+\t\tByteBuffer buf = ByteBuffer.allocate(12);\n+\t\tbuf.order(ByteOrder.LITTLE_ENDIAN);\n+\t\tbuf.putLong(nanosOfDay);\n+\t\tbuf.putInt(julianDay);\n+\t\tbuf.flip();\n+\t\treturn Binary.fromConstantByteBuffer(buf);\n+\t}\n+\n+\tprivate FieldWriter makeDecimalWriter(int precision, int scale) {", "originalCommit": "5dce0e57680a2e3b34f27ec2fac8ed7a59d00dbe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc1ODA1NA==", "url": "https://github.com/apache/flink/pull/11602#discussion_r402758054", "bodyText": "the 10000 looks like a wrong digit.", "author": "leonardBang", "createdAt": "2020-04-03T06:17:17Z", "path": "flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/row/ParquetRowDataWriterTest.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.parquet.row;\n+\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.parquet.ParquetWriterFactory;\n+import org.apache.flink.formats.parquet.vector.ParquetColumnarRowSplitReader;\n+import org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.DataFormatConverters;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.BooleanType;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.DoubleType;\n+import org.apache.flink.table.types.logical.FloatType;\n+import org.apache.flink.table.types.logical.IntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.SmallIntType;\n+import org.apache.flink.table.types.logical.TimestampType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.table.types.logical.VarBinaryType;\n+import org.apache.flink.table.types.logical.VarCharType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.parquet.hadoop.ParquetOutputFormat;\n+import org.junit.Assert;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.nio.charset.StandardCharsets;\n+import java.time.LocalDateTime;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Test for {@link ParquetRowDataBuilder} and {@link ParquetRowDataWriter}.\n+ */\n+public class ParquetRowDataWriterTest {\n+\n+\t@ClassRule\n+\tpublic static final TemporaryFolder TEMPORARY_FOLDER = new TemporaryFolder();\n+\n+\tprivate static final RowType ROW_TYPE = RowType.of(\n+\t\t\tnew VarCharType(VarCharType.MAX_LENGTH),\n+\t\t\tnew VarBinaryType(VarBinaryType.MAX_LENGTH),\n+\t\t\tnew BooleanType(),\n+\t\t\tnew TinyIntType(),\n+\t\t\tnew SmallIntType(),\n+\t\t\tnew IntType(),\n+\t\t\tnew BigIntType(),\n+\t\t\tnew FloatType(),\n+\t\t\tnew DoubleType(),\n+\t\t\tnew TimestampType(9),\n+\t\t\tnew DecimalType(5, 0),\n+\t\t\tnew DecimalType(15, 0),\n+\t\t\tnew DecimalType(20, 0));\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate static final DataFormatConverters.DataFormatConverter<BaseRow, Row> CONVERTER =\n+\t\t\tDataFormatConverters.getConverterForDataType(\n+\t\t\t\t\tTypeConversions.fromLogicalToDataType(ROW_TYPE));\n+\n+\t@Test\n+\tpublic void testTypes() throws IOException {\n+\t\tConfiguration conf = new Configuration();\n+\t\tinnerTest(conf, true);\n+\t\tinnerTest(conf, false);\n+\t}\n+\n+\t@Test\n+\tpublic void testCompression() throws IOException {\n+\t\tConfiguration conf = new Configuration();\n+\t\tconf.set(ParquetOutputFormat.COMPRESSION, \"GZIP\");\n+\t\tinnerTest(conf, true);\n+\t\tinnerTest(conf, false);\n+\t}\n+\n+\tprivate void innerTest(\n+\t\t\tConfiguration conf,\n+\t\t\tboolean utcTimestamp) throws IOException {\n+\t\tPath path = new Path(TEMPORARY_FOLDER.newFolder().getPath(), UUID.randomUUID().toString());\n+\t\tint number = 1000;\n+\t\tList<Row> rows = new ArrayList<>(number);\n+\t\tfor (int i = 0; i < number; i++) {\n+\t\t\tInteger v = i;\n+\t\t\trows.add(Row.of(\n+\t\t\t\t\tString.valueOf(v),\n+\t\t\t\t\tString.valueOf(v).getBytes(StandardCharsets.UTF_8),\n+\t\t\t\t\tv % 2 == 0,\n+\t\t\t\t\tv.byteValue(),\n+\t\t\t\t\tv.shortValue(),\n+\t\t\t\t\tv,\n+\t\t\t\t\tv.longValue(),\n+\t\t\t\t\tv.floatValue(),\n+\t\t\t\t\tv.doubleValue(),\n+\t\t\t\t\ttoDateTime(v),\n+\t\t\t\t\tBigDecimal.valueOf(v),\n+\t\t\t\t\tBigDecimal.valueOf(v),\n+\t\t\t\t\tBigDecimal.valueOf(v)));\n+\t\t}\n+\n+\t\tParquetWriterFactory<BaseRow> factory = ParquetRowDataBuilder.createWriterFactory(\n+\t\t\t\tROW_TYPE, conf, utcTimestamp);\n+\t\tBulkWriter<BaseRow> writer = factory.create(path.getFileSystem().create(\n+\t\t\t\tpath, FileSystem.WriteMode.OVERWRITE));\n+\t\tfor (int i = 0; i < number; i++) {\n+\t\t\twriter.addElement(CONVERTER.toInternal(rows.get(i)));\n+\t\t}\n+\t\twriter.flush();\n+\t\twriter.finish();\n+\n+\t\t// verify\n+\t\tParquetColumnarRowSplitReader reader = ParquetSplitReaderUtil.genPartColumnarRowReader(\n+\t\t\t\tutcTimestamp,\n+\t\t\t\tconf,\n+\t\t\t\tROW_TYPE.getFieldNames().toArray(new String[0]),\n+\t\t\t\tROW_TYPE.getChildren().stream()\n+\t\t\t\t\t\t.map(TypeConversions::fromLogicalToDataType)\n+\t\t\t\t\t\t.toArray(DataType[]::new),\n+\t\t\t\tnew HashMap<>(),\n+\t\t\t\tIntStream.range(0, ROW_TYPE.getFieldCount()).toArray(),\n+\t\t\t\t50,\n+\t\t\t\tpath,\n+\t\t\t\t0,\n+\t\t\t\tLong.MAX_VALUE);\n+\t\tint cnt = 0;\n+\t\twhile (!reader.reachedEnd()) {\n+\t\t\tRow row = CONVERTER.toExternal(reader.nextRecord());\n+\t\t\tAssert.assertEquals(rows.get(cnt), row);\n+\t\t\tcnt++;\n+\t\t}\n+\t\tAssert.assertEquals(number, cnt);\n+\t}\n+\n+\tprivate LocalDateTime toDateTime(Integer v) {\n+\t\tv = (v > 0 ? v : -v) % 10000;", "originalCommit": "5dce0e57680a2e3b34f27ec2fac8ed7a59d00dbe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "39d9b1a9c461fc940a660b95a4764f9982ba1599", "url": "https://github.com/apache/flink/commit/39d9b1a9c461fc940a660b95a4764f9982ba1599", "message": "Fix comments", "committedDate": "2020-04-03T06:37:32Z", "type": "commit"}]}