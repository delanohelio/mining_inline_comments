{"pr_number": 13499, "pr_title": " [FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.", "pr_createdAt": "2020-09-28T07:53:14Z", "pr_url": "https://github.com/apache/flink/pull/13499", "timeline": [{"oid": "378555e68fc64c708f033109422f8701013fdaed", "url": "https://github.com/apache/flink/commit/378555e68fc64c708f033109422f8701013fdaed", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement.", "committedDate": "2020-09-28T10:25:27Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTk3MDgwMA==", "url": "https://github.com/apache/flink/pull/13499#discussion_r495970800", "bodyText": "Have you checked if the BufferPoolOwner is not part of our shuffle service API? Maybe there can be some 3rd party shuffle services using it?\n@zhijiangW seemed to be fine with removing it in the ticket, so I guess that's not an issue (he was involved in the plugable shuffle service story).", "author": "pnowojski", "createdAt": "2020-09-28T14:12:36Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/BufferPoolFactory.java", "diffHunk": "@@ -48,8 +48,6 @@\n \t * \t\tminimum number of network buffers in this pool\n \t * @param maxUsedBuffers\n \t * \t\tmaximum number of network buffers this pool offers\n-\t * @param bufferPoolOwner", "originalCommit": "68b0c710d4850f79920a584097c0218a1ace6ca8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ2OTU1NA==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496469554", "bodyText": "I just looked into the respective package and nothing buffer related is directly exposed in the API.", "author": "AHeise", "createdAt": "2020-09-29T07:16:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTk3MDgwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTk5MzE0OQ==", "url": "https://github.com/apache/flink/pull/13499#discussion_r495993149", "bodyText": "What's the purpose of this change? Is it actually working? What if getInputOutputJointFuture returns completed future, but it become unavailable during the input recovery?\nAlso, it's missing a test coverage.", "author": "pnowojski", "createdAt": "2020-09-28T14:43:15Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamTask.java", "diffHunk": "@@ -518,6 +518,11 @@ private void readRecoveredChannelState() throws IOException, InterruptedExceptio\n \t\t\t\t\t\t\t\"Cannot restore state to a non-checkpointable partition type: \" + writer);\n \t\t\t\t}\n \t\t\t}\n+\n+\t\t\tif (!recordWriter.isAvailable()) {\n+\t\t\t\tMailboxDefaultAction.Suspension suspendedDefaultAction = mailboxProcessor.suspendDefaultAction();\n+\t\t\t\tgetInputOutputJointFuture(InputStatus.NOTHING_AVAILABLE).thenRun(suspendedDefaultAction::resume);\n+\t\t\t}", "originalCommit": "a59d85b1e23a1c3c904c36c63f6dddae8ccc691c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ3NTA2OA==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496475068", "bodyText": "I didn't manage to create a unit test, so I will probably add an ITCase. I'm extending commit message to state \"Currently, task thread blocks if all output buffers are taken during recovery: The default action is only suspended after calling StreamTask#processInput once, which will block as soon as one element is emitted. With this fix, the task thread suspends input processing if all output buffers are taken during recovery.\"\n\nWhat if getInputOutputJointFuture returns completed future, but it become unavailable during the input recovery?\n\nThis is the current behavior: input processing is enabled by default. What happens is that the first call to #processInput blocks and sets the future correctly as soon as one output buffer has been processed. Note that the input availability should not be set at this point. It may only happen when the first input is recovered.\n\nIs it actually working?\n\nYes, but I can only merge the corresponding test after we allow concurrent checkpoints or else we run into live locks: Recovery of input channels on a non-rescaling case can only happen if #processInput is called once because of EndOfChannelStateEvent being an extra buffer that is only polled when more input channels are available.", "author": "AHeise", "createdAt": "2020-09-29T07:26:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTk5MzE0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk5NjcwMg==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496996702", "bodyText": "I dropped the commit for now, as it would only be relevant for fully blocked output (input selection) or checkpointing during recovery.", "author": "AHeise", "createdAt": "2020-09-29T19:36:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTk5MzE0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAwNTA0Mw==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496005043", "bodyText": "why assert and not checkState()?", "author": "pnowojski", "createdAt": "2020-09-28T14:56:08Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -190,6 +205,12 @@\n \t\t\tsubpartitionBufferRecyclers[i] = new SubpartitionBufferRecycler(i, this);\n \t\t}\n \t\tthis.maxBuffersPerChannel = maxBuffersPerChannel;\n+\n+\t\tif (checkAvailability()) {\n+\t\t\tavailabilityHelper.resetAvailable();\n+\t\t}\n+\n+\t\tassert hasConsistentAvailability();", "originalCommit": "378555e68fc64c708f033109422f8701013fdaed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ3NjgwNg==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496476806", "bodyText": "Similar to NetworkBufferPool#redistributeBuffers these assertions are just to ease debugging during test execution. The few cycles needed to conduct the test are skipped on production. If you rather like to see them being executed for every run, we probably need to align it also in NetworkBufferPool to keep it consistent.", "author": "AHeise", "createdAt": "2020-09-29T07:29:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAwNTA0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ4NDgyNA==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496484824", "bodyText": "I mean I do not see a performance benefit of avoiding a couple of checkStates on per buffer path, while it could provide some help, if bug will occur on some production setup only.\nIn per record paths, I guess I would agree more with asserts", "author": "pnowojski", "createdAt": "2020-09-29T07:44:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAwNTA0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAzNzI3Ng==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496037276", "bodyText": "doing this under the lock would look more consistent? As it is, can not it yield false results? All of the modifications to the availabilityHelper  are happening under the availableMemorySegments lock , so after moving there it should be perfectly accurate.", "author": "pnowojski", "createdAt": "2020-09-28T15:23:57Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -322,23 +350,71 @@ private MemorySegment requestMemorySegment() {\n \t\treturn requestMemorySegment(UNKNOWN_CHANNEL);\n \t}\n \n-\t@Nullable\n-\tprivate MemorySegment requestMemorySegmentFromGlobal() {\n-\t\tassert Thread.holdsLock(availableMemorySegments);\n+\tprivate boolean requestMemorySegmentFromGlobal() {\n+\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tMemorySegment segment = networkBufferPool.requestMemorySegment();\n+\t\tif (segment != null) {\n+\t\t\tavailableMemorySegments.add(segment);\n+\t\t\tnumberOfRequestedMemorySegments++;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}\n \n-\t\tif (isDestroyed) {\n-\t\t\tthrow new IllegalStateException(\"Buffer pool is destroyed.\");\n+\t/**\n+\t * Tries to obtain a buffer from global pool as soon as one pool is available. Note that multiple\n+\t * {@link LocalBufferPool}s might wait on the future of the global pool, hence this method double-check if a new\n+\t * buffer is really needed at the time it becomes available.\n+\t */\n+\tprivate void eagerlyRequestMemorySegmentFromGlobal() {\n+\t\tif (eagerlyRequesting) {\n+\t\t\treturn;\n \t\t}\n+\t\teagerlyRequesting = true;\n+\t\tnetworkBufferPool.getAvailableFuture().thenRun(() -> {\n+\t\t\teagerlyRequesting = false;\n+\t\t\tif (availabilityHelper.isAvailable()) {\n+\t\t\t\t// there is currently no benefit for this pool to obtain buffer from global; give other pools precedent\n+\t\t\t\treturn;\n+\t\t\t}", "originalCommit": "378555e68fc64c708f033109422f8701013fdaed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ3NzEzOQ==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496477139", "bodyText": "I was trying to optimize, but I guess it's premature. Segment reallocation is rather rare anyways afaik.", "author": "AHeise", "createdAt": "2020-09-29T07:30:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAzNzI3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAzODQwOA==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496038408", "bodyText": "does it need to be volatile if we already have @GuardedBy(\"availableMemorySegments\")? Adding another point of synchronisation makes it more difficult to reason about the concurrency model.", "author": "pnowojski", "createdAt": "2020-09-28T15:25:09Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -92,16 +102,21 @@\n \n \tprivate final int maxBuffersPerChannel;\n \n+\t@GuardedBy(\"availableMemorySegments\")\n \tprivate final int[] subpartitionBuffersCount;\n \n \tprivate final BufferRecycler[] subpartitionBufferRecyclers;\n \n+\t@GuardedBy(\"availableMemorySegments\")\n \tprivate int unavailableSubpartitionsCount = 0;\n \n \tprivate boolean isDestroyed;\n \n+\t@GuardedBy(\"availableMemorySegments\")\n \tprivate final AvailabilityHelper availabilityHelper = new AvailabilityHelper();\n \n+\tprivate volatile boolean eagerlyRequesting;", "originalCommit": "378555e68fc64c708f033109422f8701013fdaed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ3NzUwNg==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496477506", "bodyText": "I'll try to get rid of it. Probably also premature optimization (cheap read-lock).", "author": "AHeise", "createdAt": "2020-09-29T07:30:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjAzODQwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA0MTgxNQ==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496041815", "bodyText": "nit: for (future) consistency and self documenting code, maybe extract this condition to a private method isRequestedSizeReached()?", "author": "pnowojski", "createdAt": "2020-09-28T15:29:47Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -322,23 +350,71 @@ private MemorySegment requestMemorySegment() {\n \t\treturn requestMemorySegment(UNKNOWN_CHANNEL);\n \t}\n \n-\t@Nullable\n-\tprivate MemorySegment requestMemorySegmentFromGlobal() {\n-\t\tassert Thread.holdsLock(availableMemorySegments);\n+\tprivate boolean requestMemorySegmentFromGlobal() {\n+\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tMemorySegment segment = networkBufferPool.requestMemorySegment();\n+\t\tif (segment != null) {\n+\t\t\tavailableMemorySegments.add(segment);\n+\t\t\tnumberOfRequestedMemorySegments++;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}\n \n-\t\tif (isDestroyed) {\n-\t\t\tthrow new IllegalStateException(\"Buffer pool is destroyed.\");\n+\t/**\n+\t * Tries to obtain a buffer from global pool as soon as one pool is available. Note that multiple\n+\t * {@link LocalBufferPool}s might wait on the future of the global pool, hence this method double-check if a new\n+\t * buffer is really needed at the time it becomes available.\n+\t */\n+\tprivate void eagerlyRequestMemorySegmentFromGlobal() {\n+\t\tif (eagerlyRequesting) {\n+\t\t\treturn;\n \t\t}\n+\t\teagerlyRequesting = true;\n+\t\tnetworkBufferPool.getAvailableFuture().thenRun(() -> {\n+\t\t\teagerlyRequesting = false;\n+\t\t\tif (availabilityHelper.isAvailable()) {\n+\t\t\t\t// there is currently no benefit for this pool to obtain buffer from global; give other pools precedent\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tCompletableFuture<?> toNotify = null;\n+\t\t\tsynchronized (availableMemorySegments) {\n+\t\t\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {", "originalCommit": "378555e68fc64c708f033109422f8701013fdaed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ3ODI1NA==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496478254", "bodyText": "I had various helper methods in-between and in the end inlined them all as it didn't make them much easier to read imho. But I'll try your suggestion; the semantics is easy enough that you do not need to look into the implementation.", "author": "AHeise", "createdAt": "2020-09-29T07:32:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA0MTgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ4NTg2NA==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496485864", "bodyText": "It's not only that, but also the check is appearing in multiple places. If it changes in one place, it should also change in the others.", "author": "pnowojski", "createdAt": "2020-09-29T07:45:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA0MTgxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2MzM5OQ==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496063399", "bodyText": "I guess it's not \"eagerly\" after all? Eagerly would mean to me something like request them upon construction, or something like that?\nHere you mean, request the buffers first, before making LocalBufferPool available?", "author": "pnowojski", "createdAt": "2020-09-28T16:01:16Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -322,23 +350,71 @@ private MemorySegment requestMemorySegment() {\n \t\treturn requestMemorySegment(UNKNOWN_CHANNEL);\n \t}\n \n-\t@Nullable\n-\tprivate MemorySegment requestMemorySegmentFromGlobal() {\n-\t\tassert Thread.holdsLock(availableMemorySegments);\n+\tprivate boolean requestMemorySegmentFromGlobal() {\n+\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tMemorySegment segment = networkBufferPool.requestMemorySegment();\n+\t\tif (segment != null) {\n+\t\t\tavailableMemorySegments.add(segment);\n+\t\t\tnumberOfRequestedMemorySegments++;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}\n \n-\t\tif (isDestroyed) {\n-\t\t\tthrow new IllegalStateException(\"Buffer pool is destroyed.\");\n+\t/**\n+\t * Tries to obtain a buffer from global pool as soon as one pool is available. Note that multiple\n+\t * {@link LocalBufferPool}s might wait on the future of the global pool, hence this method double-check if a new\n+\t * buffer is really needed at the time it becomes available.\n+\t */\n+\tprivate void eagerlyRequestMemorySegmentFromGlobal() {", "originalCommit": "378555e68fc64c708f033109422f8701013fdaed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ3OTIxNQ==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496479215", "bodyText": "No, eagerly means that we poll the new segment as soon as we run out of local segments. In the old way, we would only poll lazily when a new local segment is requested. Further, this method even polls the segment as soon as it is returned to the network buffer from a different pool (through avail future).", "author": "AHeise", "createdAt": "2020-09-29T07:34:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2MzM5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk5MTQ4Mw==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496991483", "bodyText": "I actually renamed to requestMemorySegmentFromGlobalWhenAvailable.", "author": "AHeise", "createdAt": "2020-09-29T19:29:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2MzM5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2NDEyNw==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496064127", "bodyText": "does it mean we can keep requesting segments despite reaching per sub-partition limit? Is it a pre-existing behaviour? (if so, we could leave it as a future improvement)", "author": "pnowojski", "createdAt": "2020-09-28T16:02:23Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -322,23 +350,71 @@ private MemorySegment requestMemorySegment() {\n \t\treturn requestMemorySegment(UNKNOWN_CHANNEL);\n \t}\n \n-\t@Nullable\n-\tprivate MemorySegment requestMemorySegmentFromGlobal() {\n-\t\tassert Thread.holdsLock(availableMemorySegments);\n+\tprivate boolean requestMemorySegmentFromGlobal() {\n+\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tMemorySegment segment = networkBufferPool.requestMemorySegment();\n+\t\tif (segment != null) {\n+\t\t\tavailableMemorySegments.add(segment);\n+\t\t\tnumberOfRequestedMemorySegments++;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}\n \n-\t\tif (isDestroyed) {\n-\t\t\tthrow new IllegalStateException(\"Buffer pool is destroyed.\");\n+\t/**\n+\t * Tries to obtain a buffer from global pool as soon as one pool is available. Note that multiple\n+\t * {@link LocalBufferPool}s might wait on the future of the global pool, hence this method double-check if a new\n+\t * buffer is really needed at the time it becomes available.\n+\t */\n+\tprivate void eagerlyRequestMemorySegmentFromGlobal() {\n+\t\tif (eagerlyRequesting) {\n+\t\t\treturn;\n \t\t}\n+\t\teagerlyRequesting = true;\n+\t\tnetworkBufferPool.getAvailableFuture().thenRun(() -> {\n+\t\t\teagerlyRequesting = false;\n+\t\t\tif (availabilityHelper.isAvailable()) {\n+\t\t\t\t// there is currently no benefit for this pool to obtain buffer from global; give other pools precedent\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tCompletableFuture<?> toNotify = null;\n+\t\t\tsynchronized (availableMemorySegments) {\n+\t\t\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\n+\t\t\t\t// fetch a segment from global pool\n+\t\t\t\tif (requestMemorySegmentFromGlobal()) {\n+\t\t\t\t\ttoNotify = availabilityHelper.getUnavailableToResetAvailable();\n+\t\t\t\t} else {\n+\t\t\t\t\t// segment probably taken by other pool, so retry later\n+\t\t\t\t\teagerlyRequestMemorySegmentFromGlobal();\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tmayNotifyAvailable(toNotify);\n+\t\t});\n+\t}\n \n+\tprivate boolean checkAvailability() {\n+\t\tif (!availableMemorySegments.isEmpty()) {\n+\t\t\treturn unavailableSubpartitionsCount == 0;\n+\t\t}\n \t\tif (numberOfRequestedMemorySegments < currentPoolSize) {\n-\t\t\tfinal MemorySegment segment = networkBufferPool.requestMemorySegment();\n-\t\t\tif (segment != null) {\n-\t\t\t\tnumberOfRequestedMemorySegments++;\n-\t\t\t\treturn segment;\n+\t\t\tif (requestMemorySegmentFromGlobal()) {\n+\t\t\t\treturn unavailableSubpartitionsCount == 0;", "originalCommit": "378555e68fc64c708f033109422f8701013fdaed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ4MjA3NA==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496482074", "bodyText": "Yes.\nI initially didn't poll it and it would only affect one more test case, however, I decided against it. First of all, it's much easier to reason without having to worry about unavailableSubpartitionsCount: If availableSegments are empty and the pool is allowed to take more buffers, it takes one.\nIt also helps to reach the equilibrium much quicker: every local pool has the segments it is assigned.\nThe clear downside is that if two local buffer pools are competing and one of them reached the quota, the wrong one polls first, keeping both unavailable. I favored simplicity and quicker equilibrium over this edge case, but can also revert to the earlier version if you think that the edge case is very common.", "author": "AHeise", "createdAt": "2020-09-29T07:39:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2NDEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU3MDg2Nw==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496570867", "bodyText": "I don't think this is a big issue. Just wanted to make sure it's not a regression (that would be an issue)", "author": "pnowojski", "createdAt": "2020-09-29T09:24:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2NDEyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2NjY2OQ==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496066669", "bodyText": "subpartitionBuffersCount[targetChannel]++ vs ++subpartitionBuffersCount[targetChannel], isn't it changing the semantic a bit?", "author": "pnowojski", "createdAt": "2020-09-28T16:06:26Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -292,27 +313,34 @@ private MemorySegment requestMemorySegmentBlocking(int targetChannel) throws Int\n \n \t@Nullable\n \tprivate MemorySegment requestMemorySegment(int targetChannel) {\n-\t\tMemorySegment segment = null;\n+\t\tMemorySegment segment;\n \t\tsynchronized (availableMemorySegments) {\n-\t\t\treturnExcessMemorySegments();\n-\n-\t\t\tif (availableMemorySegments.isEmpty()) {\n-\t\t\t\tsegment = requestMemorySegmentFromGlobal();\n+\t\t\tif (isDestroyed) {\n+\t\t\t\tthrow new IllegalStateException(\"Buffer pool is destroyed.\");\n \t\t\t}\n-\t\t\t// segment may have been released by buffer pool owner\n-\t\t\tif (segment == null) {\n-\t\t\t\tsegment = availableMemorySegments.poll();\n+\n+\t\t\t// target channel over quota; do not return a segment\n+\t\t\tif (targetChannel != UNKNOWN_CHANNEL && subpartitionBuffersCount[targetChannel] >= maxBuffersPerChannel) {\n+\t\t\t\treturn null;\n \t\t\t}\n+\n+\t\t\tsegment = availableMemorySegments.poll();\n+\n \t\t\tif (segment == null) {\n-\t\t\t\tavailabilityHelper.resetUnavailable();\n+\t\t\t\treturn null;\n \t\t\t}\n \n-\t\t\tif (segment != null && targetChannel != UNKNOWN_CHANNEL) {\n-\t\t\t\tif (subpartitionBuffersCount[targetChannel]++ == maxBuffersPerChannel) {\n+\t\t\tif (targetChannel != UNKNOWN_CHANNEL) {\n+\t\t\t\tif (++subpartitionBuffersCount[targetChannel] == maxBuffersPerChannel) {", "originalCommit": "378555e68fc64c708f033109422f8701013fdaed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ4MzMyNQ==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496483325", "bodyText": "Yes, I should probably pull it out. Imho it was wrong before. For me valid subpartitionBuffersCounts are [0, maxBuffersPerChannel], but it used to be [0, maxBuffersPerChannel + 1]. In the end, maxBuffersPerChannel is rather arbitrary, so it shouldn't have a any impact on production applications, but for me, it's also important to have a clear semantics.", "author": "AHeise", "createdAt": "2020-09-29T07:41:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2NjY2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU3MTUyNQ==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496571525", "bodyText": "Yes, it would be better to move it to a separate commit. I agree that [0, maxBuffersPerChannel] makes more sense.", "author": "pnowojski", "createdAt": "2020-09-29T09:25:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA2NjY2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU2NTMzMw==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496565333", "bodyText": "maybe change it to while (numberOfRequestedMemorySegments < currentPoolSize) loop? (as a follow up commit?)", "author": "pnowojski", "createdAt": "2020-09-29T09:16:13Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -322,23 +350,71 @@ private MemorySegment requestMemorySegment() {\n \t\treturn requestMemorySegment(UNKNOWN_CHANNEL);\n \t}\n \n-\t@Nullable\n-\tprivate MemorySegment requestMemorySegmentFromGlobal() {\n-\t\tassert Thread.holdsLock(availableMemorySegments);\n+\tprivate boolean requestMemorySegmentFromGlobal() {\n+\t\tif (numberOfRequestedMemorySegments >= currentPoolSize) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tMemorySegment segment = networkBufferPool.requestMemorySegment();\n+\t\tif (segment != null) {\n+\t\t\tavailableMemorySegments.add(segment);\n+\t\t\tnumberOfRequestedMemorySegments++;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}", "originalCommit": "378555e68fc64c708f033109422f8701013fdaed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk5OTU0Mg==", "url": "https://github.com/apache/flink/pull/13499#discussion_r496999542", "bodyText": "Is your intent to reach equilibrium much quicker? If so, I like the idea. (If not, then I haven't understood)\nMy main concern is that it means that the first buffer pool would potentially take all available segments while the last buffer pool gets nothing although each of them could take some buffers. However, I must admit that I have not fully understood when excess buffers actually occur in reality. I'd assume that during start of an application all pools are created and exclusive segments are acquired more or less simultaneously and handed out a bit later to the writer/input channels, such that excess buffers are close to non-existant.", "author": "AHeise", "createdAt": "2020-09-29T19:42:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU2NTMzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQxMDAwNQ==", "url": "https://github.com/apache/flink/pull/13499#discussion_r497410005", "bodyText": "As discussed offline, let's keep it as it is for now.", "author": "pnowojski", "createdAt": "2020-09-30T10:36:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjU2NTMzMw=="}], "type": "inlineReview"}, {"oid": "f09cce84837e90a6617a4f2d22ad96258f98427b", "url": "https://github.com/apache/flink/commit/f09cce84837e90a6617a4f2d22ad96258f98427b", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement.", "committedDate": "2020-09-29T19:43:21Z", "type": "forcePushed"}, {"oid": "f298e8b85569242501d871e326cce21695582caa", "url": "https://github.com/apache/flink/commit/f298e8b85569242501d871e326cce21695582caa", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement.", "committedDate": "2020-09-29T19:56:00Z", "type": "forcePushed"}, {"oid": "df08363e00e50b79ff2492f344a4cef335699b17", "url": "https://github.com/apache/flink/commit/df08363e00e50b79ff2492f344a4cef335699b17", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement.", "committedDate": "2020-09-30T07:19:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQxMTEzNw==", "url": "https://github.com/apache/flink/pull/13499#discussion_r497411137", "bodyText": "Shouldn't this commit change some tests?", "author": "pnowojski", "createdAt": "2020-09-30T10:39:11Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -296,6 +296,11 @@ private MemorySegment requestMemorySegment(int targetChannel) {\n \t\tsynchronized (availableMemorySegments) {\n \t\t\treturnExcessMemorySegments();\n \n+\t\t\t// target channel over quota; do not return a segment", "originalCommit": "0db12e7ec1a83349104a4f6556096f9997c03805", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQ4MTUyNg==", "url": "https://github.com/apache/flink/pull/13499#discussion_r497481526", "bodyText": "Yes it changes LocalBufferPoolTest#testMaxBuffersPerChannelAndAvailability. Other test played nicely (only took buffer they should have taken).", "author": "AHeise", "createdAt": "2020-09-30T12:48:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQxMTEzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgxNzY2OQ==", "url": "https://github.com/apache/flink/pull/13499#discussion_r497817669", "bodyText": "Pulled out the test changes into this commit.", "author": "AHeise", "createdAt": "2020-09-30T21:42:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQxMTEzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQxMjY4Ng==", "url": "https://github.com/apache/flink/pull/13499#discussion_r497412686", "bodyText": "nit: there are still two places doing numberOfRequestedMemorySegments > currentPoolSize, but I guess they are off by one in the comparison and can not be easily migrated to isRequestedSizeReached()?", "author": "pnowojski", "createdAt": "2020-09-30T10:42:04Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/buffer/LocalBufferPool.java", "diffHunk": "@@ -327,23 +358,83 @@ private MemorySegment requestMemorySegment() {\n \t\treturn requestMemorySegment(UNKNOWN_CHANNEL);\n \t}\n \n-\t@Nullable\n-\tprivate MemorySegment requestMemorySegmentFromGlobal() {\n+\tprivate boolean requestMemorySegmentFromGlobal() {\n+\t\tassert Thread.holdsLock(availableMemorySegments);\n+\n+\t\tif (isRequestedSizeReached()) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\tMemorySegment segment = networkBufferPool.requestMemorySegment();\n+\t\tif (segment != null) {\n+\t\t\tavailableMemorySegments.add(segment);\n+\t\t\tnumberOfRequestedMemorySegments++;\n+\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}\n+\n+\tprivate boolean isRequestedSizeReached() {\n+\t\treturn numberOfRequestedMemorySegments >= currentPoolSize;\n+\t}", "originalCommit": "df08363e00e50b79ff2492f344a4cef335699b17", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQ4MzIwOA==", "url": "https://github.com/apache/flink/pull/13499#discussion_r497483208", "bodyText": "I didn't find a good way, but extracted them into hasExcessBuffers for now.", "author": "AHeise", "createdAt": "2020-09-30T12:50:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQxMjY4Ng=="}], "type": "inlineReview"}, {"oid": "8cc24be5102c67d50c2f3555195cab48210339d4", "url": "https://github.com/apache/flink/commit/8cc24be5102c67d50c2f3555195cab48210339d4", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement.", "committedDate": "2020-09-30T12:51:48Z", "type": "forcePushed"}, {"oid": "159747dae3d424c8dfa0a7b6286d446e1a111c61", "url": "https://github.com/apache/flink/commit/159747dae3d424c8dfa0a7b6286d446e1a111c61", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement.", "committedDate": "2020-09-30T14:04:20Z", "type": "forcePushed"}, {"oid": "01cedbe068a6c964e003e4d5d6b2e179eadd5365", "url": "https://github.com/apache/flink/commit/01cedbe068a6c964e003e4d5d6b2e179eadd5365", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement.", "committedDate": "2020-09-30T14:05:39Z", "type": "forcePushed"}, {"oid": "16935a2bb218146d4f6880f314844cc607197b8b", "url": "https://github.com/apache/flink/commit/16935a2bb218146d4f6880f314844cc607197b8b", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement.", "committedDate": "2020-09-30T17:22:22Z", "type": "forcePushed"}, {"oid": "6f8aa8fb436ef7a136003389f006a2d507a4314e", "url": "https://github.com/apache/flink/commit/6f8aa8fb436ef7a136003389f006a2d507a4314e", "message": "[FLINK-19400][network] Removed legacy BufferPoolOwner.\n\nAll implementations are doing only noop operations and it makes implementation of LocalBufferPool seemingly harder.\nThe removal of BufferPoolOwner also eliminates the only source of IOException during NetworkBufferPool#redistributeBuffers and as such respective tests are also removed. The next commit cleans up exception handling.", "committedDate": "2020-09-30T21:43:36Z", "type": "commit"}, {"oid": "6eefd3c7e625b1aac0ca9168b5d9e33caa219d53", "url": "https://github.com/apache/flink/commit/6eefd3c7e625b1aac0ca9168b5d9e33caa219d53", "message": "[FLINK-19400][network] Remove superfluous IOExceptions.", "committedDate": "2020-09-30T21:43:36Z", "type": "commit"}, {"oid": "72507dd1819b1f04b6b7c9c7ef2b43582aa4b134", "url": "https://github.com/apache/flink/commit/72507dd1819b1f04b6b7c9c7ef2b43582aa4b134", "message": "[FLINK-16972][network] Correctly enforcing subpartition quota in LocalBufferPool.\n\nPreviously, it was possible for a subpartition to acquire (maxBuffersPerChannel+1) buffers, before LocalBufferPool became unavailable.\nAlso, requestMemorySegment does not return a buffer for a channel over quota at all, making blocking requests blocking for respective subpartitions.", "committedDate": "2020-09-30T21:43:36Z", "type": "commit"}, {"oid": "fce4a20dcb376d6c0df3671c228a872045f64606", "url": "https://github.com/apache/flink/commit/fce4a20dcb376d6c0df3671c228a872045f64606", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement.", "committedDate": "2020-09-30T21:44:47Z", "type": "commit"}, {"oid": "fce4a20dcb376d6c0df3671c228a872045f64606", "url": "https://github.com/apache/flink/commit/fce4a20dcb376d6c0df3671c228a872045f64606", "message": "[FLINK-16972][network] LocalBufferPool eagerly fetches global segments to ensure proper availability.\n\nBefore this commit, availability of LocalBufferPool depended on a the availability of a shared NetworkBufferPool. However, if multiple LocalBufferPools simultaneously are available only because the NetworkBufferPool becomes available with one segment, only one of the LocalBufferPools is truly available (the one that actually acquires this segment).\n\nThe solution in this commit is to define availability only through the guaranteed ability to provide a memory segment to the consumer. If a LocalBufferPool runs out of local segments it will become unavailable until it receives a segment from the NetworkBufferPool. To minimize unavailability, LocalBufferPool first tries to eagerly fetch new segments before declaring unavailability and if that fails, the local pool subscribes to the availability to the network pool to restore availability asap.\n\nAdditionally, LocalBufferPool would switch to unavailable only after it could not serve a requested memory segment. For requestBufferBuilderBlocking that is too late as it entered the blocking loop already.\n\nFinally, LocalBufferPool now permanently holds at least one buffer. To reflect that, the number of required segments needs to be at least one, which matches all usages in production code. A few test needed to be adjusted to properly capture the new requirement.", "committedDate": "2020-09-30T21:44:47Z", "type": "forcePushed"}]}