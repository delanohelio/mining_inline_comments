{"pr_number": 13909, "pr_title": "[FLINK-14356][table][formats] Introduce \"raw\" format to (de)serialize message to a single field", "pr_createdAt": "2020-11-03T16:44:15Z", "pr_url": "https://github.com/apache/flink/pull/13909", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAxNjAyMg==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518016022", "bodyText": "nit: call thisproducedTypeInfo for consistency\nfinal?", "author": "twalthr", "createdAt": "2020-11-05T12:32:36Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.singlefield;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.BooleanSerializer;\n+import org.apache.flink.api.common.typeutils.base.ByteSerializer;\n+import org.apache.flink.api.common.typeutils.base.DoubleSerializer;\n+import org.apache.flink.api.common.typeutils.base.FloatSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.ShortSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from single field data to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class SingleFieldDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType fieldType;\n+\tprivate final DeserializationRuntimeConverter converter;\n+\tprivate TypeInformation<RowData> typeInfo;", "originalCommit": "8de5115aa0bbc4bc486dced3d2bc67d34823659f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAxNjI2MA==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518016260", "bodyText": "nit: call this deserializedType?", "author": "twalthr", "createdAt": "2020-11-05T12:33:03Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.singlefield;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.BooleanSerializer;\n+import org.apache.flink.api.common.typeutils.base.ByteSerializer;\n+import org.apache.flink.api.common.typeutils.base.DoubleSerializer;\n+import org.apache.flink.api.common.typeutils.base.FloatSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.ShortSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from single field data to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class SingleFieldDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType fieldType;", "originalCommit": "8de5115aa0bbc4bc486dced3d2bc67d34823659f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAyMzM3Ng==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518023376", "bodyText": "Would we have a performance benefit of implementing the deserialization logic ourselves? instead of setting fields and delegating to other classes.", "author": "twalthr", "createdAt": "2020-11-05T12:45:30Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.singlefield;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.BooleanSerializer;\n+import org.apache.flink.api.common.typeutils.base.ByteSerializer;\n+import org.apache.flink.api.common.typeutils.base.DoubleSerializer;\n+import org.apache.flink.api.common.typeutils.base.FloatSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.ShortSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from single field data to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class SingleFieldDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType fieldType;\n+\tprivate final DeserializationRuntimeConverter converter;\n+\tprivate TypeInformation<RowData> typeInfo;\n+\n+\tprivate transient GenericRowData reuse;\n+\n+\tpublic SingleFieldDeserializationSchema(\n+\t\t\tLogicalType fieldType,\n+\t\t\tTypeInformation<RowData> resultTypeInfo) {\n+\t\tthis.fieldType = checkNotNull(fieldType);\n+\t\tthis.typeInfo = checkNotNull(resultTypeInfo);\n+\t\tthis.converter = createConverter(fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic void open(InitializationContext context) throws Exception {\n+\t\treuse = new GenericRowData(1);\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\treuse.setField(0, converter.convert(message));\n+\t\treturn reuse;\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn typeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tSingleFieldDeserializationSchema that = (SingleFieldDeserializationSchema) o;\n+\t\treturn typeInfo.equals(that.typeInfo) && fieldType.equals(that.fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(typeInfo, fieldType);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert byte[] to internal data structure object.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\t\tObject convert(byte[] data) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate static DeserializationRuntimeConverter createConverter(LogicalType type) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn data -> {\n+\t\t\t\t\t// put null check logic in the lambda instead of wrapping outside\n+\t\t\t\t\t// to avoid virtual method invoking.\n+\t\t\t\t\tif (data == null) {\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn StringData.fromBytes(data);\n+\t\t\t\t};\n+\t\t\tcase VARBINARY:\n+\t\t\tcase BINARY:\n+\t\t\t\treturn data -> data;\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn createConverterUsingSerializer(ByteSerializer.INSTANCE);\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn createConverterUsingSerializer(ShortSerializer.INSTANCE);\n+\t\t\tcase INTEGER:\n+\t\t\t\treturn createConverterUsingSerializer(IntSerializer.INSTANCE);\n+\t\t\tcase BIGINT:\n+\t\t\t\treturn createConverterUsingSerializer(LongSerializer.INSTANCE);\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn createConverterUsingSerializer(FloatSerializer.INSTANCE);\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn createConverterUsingSerializer(DoubleSerializer.INSTANCE);\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn createConverterUsingSerializer(BooleanSerializer.INSTANCE);\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"'single-format' currently doesn't support type: \" + type);\n+\t\t}\n+\t}\n+\n+\tprivate static DeserializationRuntimeConverter createConverterUsingSerializer(\n+\t\t\tfinal TypeSerializer<?> serializer) {\n+\t\treturn new DelegatingDeserializationConverter(serializer);\n+\t}\n+\n+\tprivate static final class DelegatingDeserializationConverter\n+\t\timplements DeserializationRuntimeConverter {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final DataInputDeserializer source = new DataInputDeserializer();\n+\t\tprivate final TypeSerializer<?> serializer;\n+\n+\t\tprotected DelegatingDeserializationConverter(TypeSerializer<?> serializer) {\n+\t\t\tthis.serializer = serializer;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\tif (data == null) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\tsource.setBuffer(data);", "originalCommit": "8de5115aa0bbc4bc486dced3d2bc67d34823659f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAyNDY0NA==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518024644", "bodyText": "Same comment as before, this looks overly complicated to just convert a a couple of data types to bytes. We should think about doing it manually.", "author": "twalthr", "createdAt": "2020-11-05T12:47:44Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldSerializationSchema.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.singlefield;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.BooleanSerializer;\n+import org.apache.flink.api.common.typeutils.base.ByteSerializer;\n+import org.apache.flink.api.common.typeutils.base.DoubleSerializer;\n+import org.apache.flink.api.common.typeutils.base.FloatSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.ShortSerializer;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.RowData.FieldGetter;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+\n+/**\n+ * Serialization schema that serializes an {@link RowData} object into a single field bytes.\n+ */\n+@Internal\n+public class SingleFieldSerializationSchema implements SerializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType fieldType;\n+\n+\tprivate final SerializationRuntimeConverter converter;\n+\n+\tprivate final FieldGetter fieldGetter;\n+\n+\tpublic SingleFieldSerializationSchema(LogicalType fieldType) {\n+\t\tthis.fieldType = fieldType;\n+\t\tthis.fieldGetter = RowData.createFieldGetter(fieldType, 0);\n+\t\tthis.converter = createConverter(fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(RowData row) {\n+\t\ttry {\n+\t\t\treturn converter.convert(fieldGetter.getFieldOrNull(row));\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(\"Could not serialize row '\" + row + \"'. \", e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tSingleFieldSerializationSchema that = (SingleFieldSerializationSchema) o;\n+\t\treturn fieldType.equals(that.fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(fieldType);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert an object of internal data structure to byte[].\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface SerializationRuntimeConverter extends Serializable {\n+\t\tbyte[] convert(Object value) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate SerializationRuntimeConverter createConverter(LogicalType type) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn value -> {\n+\t\t\t\t\t// put null check logic in the lambda instead of wrapping outside\n+\t\t\t\t\t// to avoid virtual method invoking.\n+\t\t\t\t\tif (value == null) {\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn ((StringData) value).toBytes();\n+\t\t\t\t};\n+\t\t\tcase VARBINARY:\n+\t\t\tcase BINARY:\n+\t\t\t\treturn value -> {\n+\t\t\t\t\tif (value == null) {\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn (byte[]) value;\n+\t\t\t\t};\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn createConverterUsingSerializer(ByteSerializer.INSTANCE);\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn createConverterUsingSerializer(ShortSerializer.INSTANCE);\n+\t\t\tcase INTEGER:\n+\t\t\t\treturn createConverterUsingSerializer(IntSerializer.INSTANCE);\n+\t\t\tcase BIGINT:\n+\t\t\t\treturn createConverterUsingSerializer(LongSerializer.INSTANCE);\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn createConverterUsingSerializer(FloatSerializer.INSTANCE);\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn createConverterUsingSerializer(DoubleSerializer.INSTANCE);\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn createConverterUsingSerializer(BooleanSerializer.INSTANCE);\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"'single-format' currently doesn't support type: \" + type);\n+\t\t}\n+\t}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate static SerializationRuntimeConverter createConverterUsingSerializer(\n+\t\t\tTypeSerializer<?> serializer) {\n+\t\treturn new DelegatingSerializationConverter((TypeSerializer<Object>) serializer);\n+\t}\n+\n+\tprivate static final class DelegatingSerializationConverter\n+\t\timplements SerializationRuntimeConverter {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final DataOutputSerializer dos = new DataOutputSerializer(16);\n+\t\tprivate final TypeSerializer<Object> delegatingSerializer;\n+\n+\t\tprotected DelegatingSerializationConverter(TypeSerializer<Object> delegatingSerializer) {\n+\t\t\tthis.delegatingSerializer = delegatingSerializer;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic byte[] convert(Object value) throws IOException {\n+\t\t\tif (value == null) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t\tdelegatingSerializer.serialize(value, dos);\n+\t\t\tbyte[] ret = dos.getCopyOfBuffer();", "originalCommit": "8de5115aa0bbc4bc486dced3d2bc67d34823659f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAyNzQ5Mg==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518027492", "bodyText": "Is this Flink's representation of strings in bytes? I could imagine that most people assume \"string\".getBytes() semantics. Is the string length included in the bytes are as well?", "author": "twalthr", "createdAt": "2020-11-05T12:52:48Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/singlefield/SingleFieldSerializationSchema.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.singlefield;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.common.typeutils.base.BooleanSerializer;\n+import org.apache.flink.api.common.typeutils.base.ByteSerializer;\n+import org.apache.flink.api.common.typeutils.base.DoubleSerializer;\n+import org.apache.flink.api.common.typeutils.base.FloatSerializer;\n+import org.apache.flink.api.common.typeutils.base.IntSerializer;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.common.typeutils.base.ShortSerializer;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.RowData.FieldGetter;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+\n+/**\n+ * Serialization schema that serializes an {@link RowData} object into a single field bytes.\n+ */\n+@Internal\n+public class SingleFieldSerializationSchema implements SerializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType fieldType;\n+\n+\tprivate final SerializationRuntimeConverter converter;\n+\n+\tprivate final FieldGetter fieldGetter;\n+\n+\tpublic SingleFieldSerializationSchema(LogicalType fieldType) {\n+\t\tthis.fieldType = fieldType;\n+\t\tthis.fieldGetter = RowData.createFieldGetter(fieldType, 0);\n+\t\tthis.converter = createConverter(fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(RowData row) {\n+\t\ttry {\n+\t\t\treturn converter.convert(fieldGetter.getFieldOrNull(row));\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(\"Could not serialize row '\" + row + \"'. \", e);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tSingleFieldSerializationSchema that = (SingleFieldSerializationSchema) o;\n+\t\treturn fieldType.equals(that.fieldType);\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(fieldType);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert an object of internal data structure to byte[].\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface SerializationRuntimeConverter extends Serializable {\n+\t\tbyte[] convert(Object value) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate SerializationRuntimeConverter createConverter(LogicalType type) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn value -> {\n+\t\t\t\t\t// put null check logic in the lambda instead of wrapping outside\n+\t\t\t\t\t// to avoid virtual method invoking.\n+\t\t\t\t\tif (value == null) {\n+\t\t\t\t\t\treturn null;\n+\t\t\t\t\t}\n+\t\t\t\t\treturn ((StringData) value).toBytes();", "originalCommit": "8de5115aa0bbc4bc486dced3d2bc67d34823659f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODEzNjMzNQ==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518136335", "bodyText": "I'm sure this is the same semantic of \"string\".getBytes(), becuase we are using StringData.fromBytes(stringBytes) to save decoding overhead (stringBytes to String) in the sources.\nThe string length is not included in the bytes. The length is appended by the StringDataSerializer.", "author": "wuchong", "createdAt": "2020-11-05T15:26:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAyNzQ5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODAzMjkxMA==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518032910", "bodyText": "Use a JUnit @Parameterized instead?", "author": "twalthr", "createdAt": "2020-11-05T13:01:38Z", "path": "flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/formats/singlefield/SingleFieldSerDeSchemaTest.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.singlefield;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.types.Row;\n+\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.apache.flink.table.api.DataTypes.BIGINT;\n+import static org.apache.flink.table.api.DataTypes.BINARY;\n+import static org.apache.flink.table.api.DataTypes.BOOLEAN;\n+import static org.apache.flink.table.api.DataTypes.BYTES;\n+import static org.apache.flink.table.api.DataTypes.DOUBLE;\n+import static org.apache.flink.table.api.DataTypes.FIELD;\n+import static org.apache.flink.table.api.DataTypes.FLOAT;\n+import static org.apache.flink.table.api.DataTypes.INT;\n+import static org.apache.flink.table.api.DataTypes.ROW;\n+import static org.apache.flink.table.api.DataTypes.SMALLINT;\n+import static org.apache.flink.table.api.DataTypes.STRING;\n+import static org.apache.flink.table.api.DataTypes.TINYINT;\n+import static org.apache.flink.table.api.DataTypes.VARCHAR;\n+import static org.junit.Assert.assertEquals;\n+import static org.mockito.Mockito.mock;\n+\n+/**\n+ * Tests for {@link SingleFieldDeserializationSchema} {@link SingleFieldSerializationSchema}.\n+ */\n+public class SingleFieldSerDeSchemaTest {\n+\n+\t@Test\n+\tpublic void testSerializationAndDeserialization() throws Exception {\n+\t\tfor (TestSpec testSpec : testData) {\n+\t\t\trunTest(testSpec);\n+\t\t}\n+\t}\n+\n+\tprivate void runTest(TestSpec testSpec) throws Exception {\n+\t\tSingleFieldDeserializationSchema deserializationSchema = new SingleFieldDeserializationSchema(\n+\t\t\ttestSpec.type.getLogicalType(), TypeInformation.of(RowData.class));\n+\t\tSingleFieldSerializationSchema serializationSchema = new SingleFieldSerializationSchema(\n+\t\t\ttestSpec.type.getLogicalType());\n+\t\tdeserializationSchema.open(mock(DeserializationSchema.InitializationContext.class));\n+\t\tserializationSchema.open(mock(SerializationSchema.InitializationContext.class));\n+\n+\t\tRow row = Row.of(testSpec.value);\n+\t\tDataStructureConverter<Object, Object> converter = DataStructureConverters.getConverter(\n+\t\t\tROW(FIELD(\"single\", testSpec.type)));\n+\t\tRowData originalRowData = (RowData) converter.toInternal(row);\n+\n+\t\tbyte[] serializedBytes = serializationSchema.serialize(originalRowData);\n+\t\tRowData deserializeRowData = deserializationSchema.deserialize(serializedBytes);\n+\n+\t\tRow actual = (Row) converter.toExternal(deserializeRowData);\n+\t\tassertEquals(row, actual);\n+\t}\n+\n+\tprivate static List<TestSpec> testData = Arrays.asList(", "originalCommit": "8de5115aa0bbc4bc486dced3d2bc67d34823659f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwMDc1MA==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518800750", "bodyText": "can't we use RawValueData.fromBytes here and do lazy deserialization?", "author": "twalthr", "createdAt": "2020-11-06T14:51:56Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.raw;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.HeapMemorySegment;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RawType;\n+import org.apache.flink.types.DeserializationException;\n+import org.apache.flink.util.function.FunctionUtils;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Objects;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from raw (byte based) value to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class RawFormatDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType deserializedType;\n+\n+\tprivate final DeserializationRuntimeConverter converter;\n+\n+\tprivate final TypeInformation<RowData> producedTypeInfo;\n+\n+\tprivate final String charsetName;\n+\n+\tprivate final boolean isBigEndian;\n+\n+\tprivate transient GenericRowData reuse;\n+\n+\tpublic RawFormatDeserializationSchema(\n+\t\t\tLogicalType deserializedType,\n+\t\t\tTypeInformation<RowData> producedTypeInfo,\n+\t\t\tString charsetName,\n+\t\t\tboolean isBigEndian) {\n+\t\tthis.deserializedType = checkNotNull(deserializedType);\n+\t\tthis.producedTypeInfo = checkNotNull(producedTypeInfo);\n+\t\tthis.converter = createConverter(deserializedType, charsetName, isBigEndian);\n+\t\tthis.charsetName = charsetName;\n+\t\tthis.isBigEndian = isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic void open(InitializationContext context) throws Exception {\n+\t\treuse = new GenericRowData(1);\n+\t\tconverter.open();\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\treuse.setField(0, converter.convert(message));\n+\t\treturn reuse;\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn producedTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tRawFormatDeserializationSchema that = (RawFormatDeserializationSchema) o;\n+\t\treturn producedTypeInfo.equals(that.producedTypeInfo) &&\n+\t\t\tdeserializedType.equals(that.deserializedType) &&\n+\t\t\tcharsetName.equals(that.charsetName) &&\n+\t\t\tisBigEndian == that.isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(producedTypeInfo, deserializedType, charsetName, isBigEndian);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert byte[] to internal data structure object.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\n+\t\tdefault void open() {}\n+\n+\t\tObject convert(byte[] data) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate static DeserializationRuntimeConverter createConverter(\n+\t\t\tLogicalType type, String charsetName, boolean isBigEndian) {\n+\t\tfinal DeserializationRuntimeConverter converter = createNotNullConverter(type, charsetName, isBigEndian);\n+\t\tfinal Consumer<byte[]> validator = createDataLengthValidator(type);\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t@Override\n+\t\t\tpublic void open() {\n+\t\t\t\tconverter.open();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\t\tif (data == null) {\n+\t\t\t\t\treturn null;\n+\t\t\t\t}\n+\t\t\t\tvalidator.accept(data);\n+\t\t\t\treturn converter.convert(data);\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\tprivate static DeserializationRuntimeConverter createNotNullConverter(\n+\t\t\tLogicalType type, String charsetName, boolean isBigEndian) {\n+\t\tswitch (type.getTypeRoot()) {\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn createStringConverter(charsetName);\n+\n+\t\t\tcase VARBINARY:\n+\t\t\tcase BINARY:\n+\t\t\t\treturn data -> data;\n+\n+\t\t\tcase RAW:\n+\t\t\t\treturn createRawValueConverter((RawType<?>) type);\n+\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn data -> data[0] != 0;\n+\n+\t\t\tcase TINYINT:\n+\t\t\t\treturn data -> data[0];\n+\n+\t\t\tcase SMALLINT:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getShortBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getShortLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tcase INTEGER:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getIntBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getIntLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tcase BIGINT:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getLongBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getLongLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getFloatBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getFloatLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn createEndiannessAwareConverter(\n+\t\t\t\t\tisBigEndian,\n+\t\t\t\t\tsegment -> segment.getDoubleBigEndian(0),\n+\t\t\t\t\tsegment -> segment.getDoubleLittleEndian(0)\n+\t\t\t\t);\n+\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"'raw' format currently doesn't support type: \" + type);\n+\t\t}\n+\t}\n+\n+\tprivate static DeserializationRuntimeConverter createStringConverter(final String charsetName) {\n+\t\t// this also checks the charsetName is valid\n+\t\tCharset charset = Charset.forName(charsetName);\n+\t\tif (charset == StandardCharsets.UTF_8) {\n+\t\t\t// avoid UTF-8 decoding if the given charset is UTF-8\n+\t\t\t// because the underlying bytes of StringData is in UTF-8 encoding\n+\t\t\treturn StringData::fromBytes;\n+\t\t}\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\t\t\tprivate transient Charset charset;\n+\t\t\t@Override\n+\t\t\tpublic void open() {\n+\t\t\t\tcharset = Charset.forName(charsetName);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) {\n+\t\t\t\tString str = new String(data, charset);\n+\t\t\t\treturn StringData.fromString(str);\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\tprivate static DeserializationRuntimeConverter createRawValueConverter(RawType<?> rawType) {\n+\t\tfinal TypeSerializer<?> serializer = rawType.getTypeSerializer();\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\t\t\tprivate final DataInputDeserializer source = new DataInputDeserializer();\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\t\tsource.setBuffer(data);\n+\t\t\t\treturn RawValueData.fromObject(serializer.deserialize(source));", "originalCommit": "d60a354989d3e3a3e6cea3fcce6903a35fa72965", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgzMzg1NQ==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518833855", "bodyText": "Yes. I think we can.", "author": "wuchong", "createdAt": "2020-11-06T15:41:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwMDc1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwNTAzNw==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518805037", "bodyText": "nit: Defines the endianness for bytes of numeric values.", "author": "twalthr", "createdAt": "2020-11-06T14:58:23Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatFactory.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.raw;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.DecodingFormat;\n+import org.apache.flink.table.connector.format.EncodingFormat;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.DeserializationFormatFactory;\n+import org.apache.flink.table.factories.DynamicTableFactory.Context;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.SerializationFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Format factory for raw format which allows to read and write raw (byte based) values as a single column.\n+ */\n+public class RawFormatFactory implements DeserializationFormatFactory, SerializationFormatFactory {\n+\n+\tpublic static final String IDENTIFIER = \"raw\";\n+\tprivate static final String BIG_ENDIAN = \"big-endian\";\n+\tprivate static final String LITTLE_ENDIAN = \"little-endian\";\n+\n+\tpublic static final ConfigOption<String> ENDIANNESS = ConfigOptions\n+\t\t.key(\"endianness\")\n+\t\t.stringType()\n+\t\t.defaultValue(BIG_ENDIAN)\n+\t\t.withDescription(\"Defines the endianness of the bytes of the numeric value.\");", "originalCommit": "d60a354989d3e3a3e6cea3fcce6903a35fa72965", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwNjI2Ng==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518806266", "bodyText": "shall we perform real type validation here? e.g. TIMESTAMP or NULL would still pass", "author": "twalthr", "createdAt": "2020-11-06T15:00:13Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatFactory.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.raw;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.DecodingFormat;\n+import org.apache.flink.table.connector.format.EncodingFormat;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.DeserializationFormatFactory;\n+import org.apache.flink.table.factories.DynamicTableFactory.Context;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.SerializationFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Format factory for raw format which allows to read and write raw (byte based) values as a single column.\n+ */\n+public class RawFormatFactory implements DeserializationFormatFactory, SerializationFormatFactory {\n+\n+\tpublic static final String IDENTIFIER = \"raw\";\n+\tprivate static final String BIG_ENDIAN = \"big-endian\";\n+\tprivate static final String LITTLE_ENDIAN = \"little-endian\";\n+\n+\tpublic static final ConfigOption<String> ENDIANNESS = ConfigOptions\n+\t\t.key(\"endianness\")\n+\t\t.stringType()\n+\t\t.defaultValue(BIG_ENDIAN)\n+\t\t.withDescription(\"Defines the endianness of the bytes of the numeric value.\");\n+\n+\tpublic static final ConfigOption<String> CHARSET = ConfigOptions\n+\t\t.key(\"charset\")\n+\t\t.stringType()\n+\t\t.defaultValue(StandardCharsets.UTF_8.displayName())\n+\t\t.withDescription(\"Defines the string charset.\");\n+\n+\t@Override\n+\tpublic String factoryIdentifier() {\n+\t\treturn IDENTIFIER;\n+\t}\n+\n+\t@Override\n+\tpublic Set<ConfigOption<?>> requiredOptions() {\n+\t\treturn Collections.emptySet();\n+\t}\n+\n+\t@Override\n+\tpublic Set<ConfigOption<?>> optionalOptions() {\n+\t\tSet<ConfigOption<?>> options = new HashSet<>();\n+\t\toptions.add(ENDIANNESS);\n+\t\toptions.add(CHARSET);\n+\t\treturn options;\n+\t}\n+\n+\t@Override\n+\tpublic DecodingFormat<DeserializationSchema<RowData>> createDecodingFormat(\n+\t\t\tContext context,\n+\t\t\tReadableConfig formatOptions) {\n+\t\tFactoryUtil.validateFactoryOptions(this, formatOptions);\n+\t\tfinal String charsetName = validateAndGetCharsetName(formatOptions);\n+\t\tfinal boolean isBigEndian = isBigEndian(formatOptions);\n+\n+\t\treturn new DecodingFormat<DeserializationSchema<RowData>>() {\n+\t\t\t@Override\n+\t\t\tpublic DeserializationSchema<RowData> createRuntimeDecoder(\n+\t\t\t\t\tDynamicTableSource.Context context,\n+\t\t\t\t\tDataType producedDataType) {\n+\t\t\t\tfinal RowType physicalRowType = (RowType) producedDataType.getLogicalType();\n+\t\t\t\tfinal LogicalType fieldType = validateAndExtractSingleField(physicalRowType);\n+\t\t\t\tfinal TypeInformation<RowData> producedTypeInfo = context.createTypeInformation(producedDataType);\n+\t\t\t\treturn new RawFormatDeserializationSchema(fieldType, producedTypeInfo, charsetName, isBigEndian);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic ChangelogMode getChangelogMode() {\n+\t\t\t\treturn ChangelogMode.insertOnly();\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\t@Override\n+\tpublic EncodingFormat<SerializationSchema<RowData>> createEncodingFormat(\n+\t\t\tContext context, ReadableConfig formatOptions) {\n+\t\tFactoryUtil.validateFactoryOptions(this, formatOptions);\n+\t\tfinal String charsetName = validateAndGetCharsetName(formatOptions);\n+\t\tfinal boolean isBigEndian = isBigEndian(formatOptions);\n+\n+\t\treturn new EncodingFormat<SerializationSchema<RowData>>() {\n+\t\t\t@Override\n+\t\t\tpublic SerializationSchema<RowData> createRuntimeEncoder(\n+\t\t\t\t\tDynamicTableSink.Context context,\n+\t\t\t\t\tDataType consumedDataType) {\n+\t\t\t\tfinal RowType physicalRowType = (RowType) consumedDataType.getLogicalType();\n+\t\t\t\tfinal LogicalType fieldType = validateAndExtractSingleField(physicalRowType);\n+\t\t\t\treturn new RawFormatSerializationSchema(fieldType, charsetName, isBigEndian);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic ChangelogMode getChangelogMode() {\n+\t\t\t\treturn ChangelogMode.insertOnly();\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\t// ------------------------------------------------------------------------------------------\n+\n+\t/**\n+\t * Validates and extract the single field type from the given physical row schema.\n+\t */\n+\tprivate static LogicalType validateAndExtractSingleField(RowType physicalRowType) {\n+\t\tif (physicalRowType.getFieldCount() != 1) {", "originalCommit": "d60a354989d3e3a3e6cea3fcce6903a35fa72965", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwODgxMA==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518808810", "bodyText": "I think data is never null at this location.", "author": "twalthr", "createdAt": "2020-11-06T15:04:12Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/formats/raw/RawFormatDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,343 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.formats.raw;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.HeapMemorySegment;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RawType;\n+import org.apache.flink.types.DeserializationException;\n+import org.apache.flink.util.function.FunctionUtils;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Objects;\n+import java.util.function.Consumer;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Deserialization schema from raw (byte based) value to Flink Table/SQL internal data structure {@link RowData}.\n+ */\n+@Internal\n+public class RawFormatDeserializationSchema implements DeserializationSchema<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final LogicalType deserializedType;\n+\n+\tprivate final DeserializationRuntimeConverter converter;\n+\n+\tprivate final TypeInformation<RowData> producedTypeInfo;\n+\n+\tprivate final String charsetName;\n+\n+\tprivate final boolean isBigEndian;\n+\n+\tprivate transient GenericRowData reuse;\n+\n+\tpublic RawFormatDeserializationSchema(\n+\t\t\tLogicalType deserializedType,\n+\t\t\tTypeInformation<RowData> producedTypeInfo,\n+\t\t\tString charsetName,\n+\t\t\tboolean isBigEndian) {\n+\t\tthis.deserializedType = checkNotNull(deserializedType);\n+\t\tthis.producedTypeInfo = checkNotNull(producedTypeInfo);\n+\t\tthis.converter = createConverter(deserializedType, charsetName, isBigEndian);\n+\t\tthis.charsetName = charsetName;\n+\t\tthis.isBigEndian = isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic void open(InitializationContext context) throws Exception {\n+\t\treuse = new GenericRowData(1);\n+\t\tconverter.open();\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\treuse.setField(0, converter.convert(message));\n+\t\treturn reuse;\n+\t}\n+\n+\t@Override\n+\tpublic boolean isEndOfStream(RowData nextElement) {\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn producedTypeInfo;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tRawFormatDeserializationSchema that = (RawFormatDeserializationSchema) o;\n+\t\treturn producedTypeInfo.equals(that.producedTypeInfo) &&\n+\t\t\tdeserializedType.equals(that.deserializedType) &&\n+\t\t\tcharsetName.equals(that.charsetName) &&\n+\t\t\tisBigEndian == that.isBigEndian;\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(producedTypeInfo, deserializedType, charsetName, isBigEndian);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * Runtime converter that convert byte[] to internal data structure object.\n+\t */\n+\t@FunctionalInterface\n+\tprivate interface DeserializationRuntimeConverter extends Serializable {\n+\n+\t\tdefault void open() {}\n+\n+\t\tObject convert(byte[] data) throws IOException;\n+\t}\n+\n+\t/**\n+\t * Creates a runtime converter.\n+\t */\n+\tprivate static DeserializationRuntimeConverter createConverter(\n+\t\t\tLogicalType type, String charsetName, boolean isBigEndian) {\n+\t\tfinal DeserializationRuntimeConverter converter = createNotNullConverter(type, charsetName, isBigEndian);\n+\t\tfinal Consumer<byte[]> validator = createDataLengthValidator(type);\n+\n+\t\treturn new DeserializationRuntimeConverter() {\n+\t\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t\t@Override\n+\t\t\tpublic void open() {\n+\t\t\t\tconverter.open();\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Object convert(byte[] data) throws IOException {\n+\t\t\t\tif (data == null) {", "originalCommit": "d60a354989d3e3a3e6cea3fcce6903a35fa72965", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgzMzczMA==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518833730", "bodyText": "Why? The Kafka message value might be null.", "author": "wuchong", "createdAt": "2020-11-06T15:41:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwODgxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg1MTg2OQ==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518851869", "bodyText": "at other locations we also don't check that:\nJsonRowDataDeserializationSchema#deserialize or CsvRowDataDeserializationSchema#deserialize\nthe JavaDocs of DeserializationSchema don't mention that either", "author": "twalthr", "createdAt": "2020-11-06T16:10:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwODgxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg1MjE4OA==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518852188", "bodyText": "but let's keep the null check for now", "author": "twalthr", "createdAt": "2020-11-06T16:10:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwODgxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg1NjM4Ng==", "url": "https://github.com/apache/flink/pull/13909#discussion_r518856386", "bodyText": "I think both JSON and CSV format didn't implement correctly. There will be NPE if encountering tombstone messages. That can be skipped when ignoreParseErrors is enabled, but I think the format should handle tombstone messages.", "author": "wuchong", "createdAt": "2020-11-06T16:17:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwODgxMA=="}], "type": "inlineReview"}, {"oid": "8a450577f3654c53d2e435b85a33c6504dd6a444", "url": "https://github.com/apache/flink/commit/8a450577f3654c53d2e435b85a33c6504dd6a444", "message": "[FLINK-14356][table][formats] Introduce \"single-value\" format to (de)serialize message for single field", "committedDate": "2020-11-06T16:06:03Z", "type": "commit"}, {"oid": "13cf67f4b76c0d67346e760d23d2f9b7c96be5f0", "url": "https://github.com/apache/flink/commit/13cf67f4b76c0d67346e760d23d2f9b7c96be5f0", "message": "[FLINK-14356][table][formats] Refactor and rename \"single-value\" to \"raw\" format\n\nThis closes #13909", "committedDate": "2020-11-06T16:06:46Z", "type": "commit"}, {"oid": "13cf67f4b76c0d67346e760d23d2f9b7c96be5f0", "url": "https://github.com/apache/flink/commit/13cf67f4b76c0d67346e760d23d2f9b7c96be5f0", "message": "[FLINK-14356][table][formats] Refactor and rename \"single-value\" to \"raw\" format\n\nThis closes #13909", "committedDate": "2020-11-06T16:06:46Z", "type": "forcePushed"}]}