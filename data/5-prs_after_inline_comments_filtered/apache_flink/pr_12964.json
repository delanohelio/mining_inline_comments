{"pr_number": 12964, "pr_title": "[FLINK-17426][blink planner] Dynamic Source supportsLimit pushdown", "pr_createdAt": "2020-07-23T02:20:05Z", "pr_url": "https://github.com/apache/flink/pull/12964", "timeline": [{"oid": "12d602633d870ed63020ac0a79d8f3add99ec020", "url": "https://github.com/apache/flink/commit/12d602633d870ed63020ac0a79d8f3add99ec020", "message": "[FLINK-17426][blink-planner] supportsLimitPushDown rule.", "committedDate": "2020-07-22T03:50:17Z", "type": "commit"}, {"oid": "65f49baddd16ead7030e56ed19322c067e91cbe0", "url": "https://github.com/apache/flink/commit/65f49baddd16ead7030e56ed19322c067e91cbe0", "message": "[FLINK-17426][blink-planner] DynamicSource supports SupportsLimitPushDown.", "committedDate": "2020-07-22T04:22:38Z", "type": "commit"}, {"oid": "4eadef4cfc64e49d0b71c914a375818fb29557db", "url": "https://github.com/apache/flink/commit/4eadef4cfc64e49d0b71c914a375818fb29557db", "message": "[FLINK-17426][blink-planner] fix checkstyle of line too long.", "committedDate": "2020-07-22T04:22:41Z", "type": "commit"}, {"oid": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56", "url": "https://github.com/apache/flink/commit/984b8a182e5a5cc64a9e1957710e9b9fd4768a56", "message": "[FLINK-17425][blink-planner] rewrite the limit pushdown.", "committedDate": "2020-07-23T02:11:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3MzAxMA==", "url": "https://github.com/apache/flink/pull/12964#discussion_r459873010", "bodyText": "nit: simpler way: use noneMatch instead of ! + anyMatch.\nbtw, it's better we can also update the matches method of PushFilterIntoTableSourceScanRule", "author": "godfreyhe", "createdAt": "2020-07-24T06:28:44Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(str -> str.startsWith(\"limit=[\"));", "originalCommit": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTkxMzQ2NA==", "url": "https://github.com/apache/flink/pull/12964#discussion_r459913464", "bodyText": "ok", "author": "liuyongvs", "createdAt": "2020-07-24T08:16:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3MzAxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3NDY3OQ==", "url": "https://github.com/apache/flink/pull/12964#discussion_r459874679", "bodyText": "we should update the digest anyway, otherwise the rule will be applied endless loop if limit is 0.", "author": "godfreyhe", "createdAt": "2020-07-24T06:34:37Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tcall.transformTo(sort.copy(sort.getTraitSet(), Collections.singletonList(newScan)));\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[0];\n+\t\tif (limit > 0) {", "originalCommit": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTkxNjEyMA==", "url": "https://github.com/apache/flink/pull/12964#discussion_r459916120", "bodyText": "But it will look strange, like this limit=[0] And this rule FlinkLimit0RemoveRule will remove it.", "author": "liuyongvs", "createdAt": "2020-07-24T08:21:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3NDY3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk1OTMyNg==", "url": "https://github.com/apache/flink/pull/12964#discussion_r459959326", "bodyText": "We can't rely on other rules to ensure the correctness of this rule, we must make sure each rule itself is correct.", "author": "godfreyhe", "createdAt": "2020-07-24T09:54:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3NDY3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3NTI2OQ==", "url": "https://github.com/apache/flink/pull/12964#discussion_r459875269", "bodyText": "It's better we can put sort.copy(sort.getTraitSet(), Collections.singletonList(newScan)) in a single line, which could make debugging more easy.", "author": "godfreyhe", "createdAt": "2020-07-24T06:36:49Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& !Arrays.stream(tableSourceTable.extraDigests()).anyMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tcall.transformTo(sort.copy(sort.getTraitSet(), Collections.singletonList(newScan)));", "originalCommit": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTkxMzU1Mg==", "url": "https://github.com/apache/flink/pull/12964#discussion_r459913552", "bodyText": "ok", "author": "liuyongvs", "createdAt": "2020-07-24T08:16:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3NTI2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3Njc2Nw==", "url": "https://github.com/apache/flink/pull/12964#discussion_r459876767", "bodyText": "this line is too long...", "author": "godfreyhe", "createdAt": "2020-07-24T06:40:42Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -359,7 +361,7 @@ private ChangelogMode parseChangelogMode(String string) {\n \t/**\n \t * Values {@link DynamicTableSource} for testing.\n \t */\n-\tprivate static class TestValuesTableSource implements ScanTableSource, LookupTableSource, SupportsProjectionPushDown, SupportsFilterPushDown {\n+\tprivate static class TestValuesTableSource implements ScanTableSource, LookupTableSource, SupportsProjectionPushDown, SupportsFilterPushDown, SupportsLimitPushDown {", "originalCommit": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTkxMzYwMQ==", "url": "https://github.com/apache/flink/pull/12964#discussion_r459913601", "bodyText": "ok", "author": "liuyongvs", "createdAt": "2020-07-24T08:16:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3Njc2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3ODk3Mg==", "url": "https://github.com/apache/flink/pull/12964#discussion_r459878972", "bodyText": "please add a rule test to verify this rule\uff0c just like PushFilterIntoTableSourceScanRuleTest", "author": "godfreyhe", "createdAt": "2020-07-24T06:48:25Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {", "originalCommit": "984b8a182e5a5cc64a9e1957710e9b9fd4768a56", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTkxMzkyOA==", "url": "https://github.com/apache/flink/pull/12964#discussion_r459913928", "bodyText": "the unit test is LimitTest.", "author": "liuyongvs", "createdAt": "2020-07-24T08:16:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3ODk3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk2MTA2NQ==", "url": "https://github.com/apache/flink/pull/12964#discussion_r459961065", "bodyText": "Rule test is different from plan test. Rule test focuses on the new rule you implement and only a few must-involved rules can be added to the rule set to help the test. Plan test will verify the plan based on whole rule sets.", "author": "godfreyhe", "createdAt": "2020-07-24T09:58:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg3ODk3Mg=="}], "type": "inlineReview"}, {"oid": "0324c22a9b2c85f04481966b2a78f72e92b9c88c", "url": "https://github.com/apache/flink/commit/0324c22a9b2c85f04481966b2a78f72e92b9c88c", "message": "[FLINK-17426][blink-planner] add unit tests.", "committedDate": "2020-07-27T08:24:13Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQzOTQ3OQ==", "url": "https://github.com/apache/flink/pull/12964#discussion_r461439479", "bodyText": "how about we remove the if ?", "author": "godfreyhe", "createdAt": "2020-07-28T09:16:37Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& Arrays.stream(tableSourceTable.extraDigests()).noneMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tSort newSort = sort.copy(sort.getTraitSet(), Collections.singletonList(newScan));\n+\t\tcall.transformTo(newSort);\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[0];\n+\t\tif (limit >= 0) {", "originalCommit": "0324c22a9b2c85f04481966b2a78f72e92b9c88c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ4ODU1NA==", "url": "https://github.com/apache/flink/pull/12964#discussion_r461488554", "bodyText": "I think it can not be removed. Although, it doesn't have problem now. But if the calcite supports this syntax limit x,y, which is the mysql limit offset syntax. y represents limit, which can be negative, such as -1 , you can refer this https://www.cnblogs.com/acm-bingzi/p/msqlLimit.html.\nAnd you can also read the code of calcite FlinkSqlParserImpl.OrderedQueryOrExpr, where has some comments.\nThat is the reason why i add this test testMysqlLimit.", "author": "liuyongvs", "createdAt": "2020-07-28T10:45:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQzOTQ3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjEwNzUwNw==", "url": "https://github.com/apache/flink/pull/12964#discussion_r462107507", "bodyText": "why not we add limitation limit >= 0 in matches method ?\nbtw, LIMIT x,y can be expressed as LIMIT y OFFSET x in sql standard.", "author": "godfreyhe", "createdAt": "2020-07-29T07:52:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQzOTQ3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjc4MzgyNg==", "url": "https://github.com/apache/flink/pull/12964#discussion_r462783826", "bodyText": "Yeap, calcite fetch member is null when it doesn't have limit currently. And calcite doesn't support mysql  limit x,y syntax.  If it supports, limit 5, -1. The -1 represents the end. That is to say [offset5, end).\nSo the calcite may transform -1 to fetch null when it supports in parser. And we don't need to limit >=0.\nSo i just do as you say, remove the limit >= 0 and  don't add limitation in matches.", "author": "liuyongvs", "createdAt": "2020-07-30T07:00:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQzOTQ3OQ=="}], "type": "inlineReview"}, {"oid": "141bd8e09efe79e55398adbdd0916592231fdfbf", "url": "https://github.com/apache/flink/commit/141bd8e09efe79e55398adbdd0916592231fdfbf", "message": "[FLINK-17426][blink-planner] fix reviews to unify the unit test.", "committedDate": "2020-07-28T10:49:54Z", "type": "commit"}, {"oid": "34dd37d992e9d3c3a0bf92e660575571d83b407c", "url": "https://github.com/apache/flink/commit/34dd37d992e9d3c3a0bf92e660575571d83b407c", "message": "[FLINK-17426][blink-planner] remove the redundant content.", "committedDate": "2020-07-30T06:49:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjgxMzU1Ng==", "url": "https://github.com/apache/flink/pull/12964#discussion_r462813556", "bodyText": "nit:  -> .tableStats(new TableStats(newRowCount))", "author": "godfreyhe", "createdAt": "2020-07-30T07:49:37Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& Arrays.stream(tableSourceTable.extraDigests()).noneMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tSort newSort = sort.copy(sort.getTraitSet(), Collections.singletonList(newScan));\n+\t\tcall.transformTo(newSort);\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)", "originalCommit": "34dd37d992e9d3c3a0bf92e660575571d83b407c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjgxNDI0NA==", "url": "https://github.com/apache/flink/pull/12964#discussion_r462814244", "bodyText": "nit: newExtraDigests is only used once, just move new String[] {\"limit=[\" + limit + \"]\"} into here.", "author": "godfreyhe", "createdAt": "2020-07-30T07:50:49Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushLimitIntoTableSourceScanRule.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.rules.logical;\n+\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n+import org.apache.flink.table.plan.stats.TableStats;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalSort;\n+import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.schema.FlinkPreparingTableBase;\n+import org.apache.flink.table.planner.plan.schema.TableSourceTable;\n+import org.apache.flink.table.planner.plan.stats.FlinkStatistic;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexLiteral;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+\n+/**\n+ * Planner rule that tries to push limit into a {@link FlinkLogicalTableSourceScan},\n+ * which table is a {@link TableSourceTable}. And the table source in the table is a {@link SupportsLimitPushDown}.\n+ * The original limit will still be retained.\n+ * The reasons why the limit still be retained:\n+ * 1.If the source is required to return the exact number of limit number, the implementation\n+ * of the source is highly required. The source is required to accurately control the record\n+ * number of split, and the parallelism setting also need to be adjusted accordingly.\n+ * 2.When remove the limit, maybe filter will be pushed down to the source after limit pushed\n+ * down. The source need know it should do limit first and do the filter later, it is hard to\n+ * implement.\n+ * 3.We can support limit with offset, we can push down offset + fetch to table source.\n+ */\n+public class PushLimitIntoTableSourceScanRule extends RelOptRule {\n+\tpublic static final PushLimitIntoTableSourceScanRule INSTANCE = new PushLimitIntoTableSourceScanRule();\n+\n+\tpublic PushLimitIntoTableSourceScanRule() {\n+\t\tsuper(operand(FlinkLogicalSort.class,\n+\t\t\toperand(FlinkLogicalTableSourceScan.class, none())),\n+\t\t\t\"PushLimitIntoTableSourceScanRule\");\n+\t}\n+\n+\t@Override\n+\tpublic boolean matches(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tTableSourceTable tableSourceTable = call.rel(1).getTable().unwrap(TableSourceTable.class);\n+\n+\t\t// a limit can be pushed down only if it satisfies the two conditions: 1) do not have order by keys, 2) have limit.\n+\t\tboolean onlyLimit = sort.getCollation().getFieldCollations().isEmpty() && sort.fetch != null;\n+\t\treturn onlyLimit\n+\t\t\t&& tableSourceTable != null\n+\t\t\t&& tableSourceTable.tableSource() instanceof SupportsLimitPushDown\n+\t\t\t&& Arrays.stream(tableSourceTable.extraDigests()).noneMatch(str -> str.startsWith(\"limit=[\"));\n+\t}\n+\n+\t@Override\n+\tpublic void onMatch(RelOptRuleCall call) {\n+\t\tSort sort = call.rel(0);\n+\t\tFlinkLogicalTableSourceScan scan = call.rel(1);\n+\t\tTableSourceTable tableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tint offset = sort.offset == null ? 0 : RexLiteral.intValue(sort.offset);\n+\t\tint limit = offset + RexLiteral.intValue(sort.fetch);\n+\n+\t\tTableSourceTable newTableSourceTable = applyLimit(limit, tableSourceTable);\n+\n+\t\tFlinkLogicalTableSourceScan newScan = FlinkLogicalTableSourceScan.create(scan.getCluster(), newTableSourceTable);\n+\t\tSort newSort = sort.copy(sort.getTraitSet(), Collections.singletonList(newScan));\n+\t\tcall.transformTo(newSort);\n+\t}\n+\n+\tprivate TableSourceTable applyLimit(\n+\t\tlong limit,\n+\t\tFlinkPreparingTableBase relOptTable) {\n+\t\tTableSourceTable oldTableSourceTable = relOptTable.unwrap(TableSourceTable.class);\n+\t\tDynamicTableSource newTableSource = oldTableSourceTable.tableSource().copy();\n+\t\t((SupportsLimitPushDown) newTableSource).applyLimit(limit);\n+\n+\t\tFlinkStatistic statistic = relOptTable.getStatistic();\n+\t\tlong newRowCount = 0;\n+\t\tif (statistic.getRowCount() != null) {\n+\t\t\tnewRowCount = Math.min(limit, statistic.getRowCount().longValue());\n+\t\t} else {\n+\t\t\tnewRowCount = limit;\n+\t\t}\n+\t\t// update TableStats after limit push down\n+\t\tTableStats newTableStats = new TableStats(newRowCount);\n+\t\tFlinkStatistic newStatistic = FlinkStatistic.builder()\n+\t\t\t.statistic(statistic)\n+\t\t\t.tableStats(newTableStats)\n+\t\t\t.build();\n+\n+\t\t// update extraDigests\n+\t\tString[] newExtraDigests = new String[]{\"limit=[\" + limit + \"]\"};\n+\n+\t\treturn oldTableSourceTable.copy(\n+\t\t\tnewTableSource,\n+\t\t\tnewStatistic,\n+\t\t\tnewExtraDigests", "originalCommit": "34dd37d992e9d3c3a0bf92e660575571d83b407c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}