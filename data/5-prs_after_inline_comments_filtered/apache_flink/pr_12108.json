{"pr_number": 12108, "pr_title": "[FLINK-17448][sql-parser][table-api-java][table-planner-blink][hive] Implement table DDLs for Hive dialect part2", "pr_createdAt": "2020-05-13T07:08:39Z", "pr_url": "https://github.com/apache/flink/pull/12108", "timeline": [{"oid": "66bbeb2e19f810744287740e7a691da17ebb8b0c", "url": "https://github.com/apache/flink/commit/66bbeb2e19f810744287740e7a691da17ebb8b0c", "message": "fix hive parser", "committedDate": "2020-05-15T05:27:07Z", "type": "forcePushed"}, {"oid": "ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "url": "https://github.com/apache/flink/commit/ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "message": "fix hive parser", "committedDate": "2020-05-15T08:20:45Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgxMjM5Mg==", "url": "https://github.com/apache/flink/pull/12108#discussion_r425812392", "bodyText": "Move them to a type parser class.", "author": "JingsongLi", "createdAt": "2020-05-15T13:45:54Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -138,6 +158,14 @@\n \tprivate static final String FLINK_FUNCTION_PREFIX = \"flink:\";\n \tprivate static final String FLINK_PYTHON_FUNCTION_PREFIX = FLINK_FUNCTION_PREFIX + \"python:\";\n \n+\tprivate static final Pattern DECIMAL_PATTERN = Pattern.compile(\"^decimal(\\\\((\\\\d+),\\\\s*(\\\\d+)\\\\))?$\");", "originalCommit": "ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgxMjQ2NA==", "url": "https://github.com/apache/flink/pull/12108#discussion_r425812464", "bodyText": "Move them to a type parser class.", "author": "JingsongLi", "createdAt": "2020-05-15T13:46:01Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1450,4 +1489,253 @@ static boolean isGenericForGet(Map<String, String> properties) {\n \t\treturn properties != null && Boolean.parseBoolean(properties.getOrDefault(CatalogConfig.IS_GENERIC, \"false\"));\n \t}\n \n+\tpublic static void disallowChangeIsGeneric(boolean oldIsGeneric, boolean newIsGeneric) {\n+\t\tcheckArgument(oldIsGeneric == newIsGeneric, \"Changing whether a metadata object is generic is not allowed\");\n+\t}\n+\n+\tprivate static AlterTableOp extractAlterTableOp(Map<String, String> props) {\n+\t\tString opStr = props.remove(ALTER_TABLE_OP);\n+\t\tif (opStr != null) {\n+\t\t\treturn AlterTableOp.valueOf(opStr);\n+\t\t}\n+\t\treturn null;\n+\t}\n+\n+\tprivate Table alterTableViaCatalogBaseTable(ObjectPath tablePath, CatalogBaseTable baseTable, Table oldHiveTable) {\n+\t\tTable newHiveTable = instantiateHiveTable(tablePath, baseTable, hiveConf);\n+\t\t// client.alter_table() requires a valid location\n+\t\t// thus, if new table doesn't have that, it reuses location of the old table\n+\t\tif (!newHiveTable.getSd().isSetLocation()) {\n+\t\t\tnewHiveTable.getSd().setLocation(oldHiveTable.getSd().getLocation());\n+\t\t}\n+\t\treturn newHiveTable;\n+\t}\n+\n+\tprivate void alterTableViaProperties(AlterTableOp alterOp, Table hiveTable, Map<String, String> oldProps,\n+\t\t\tMap<String, String> newProps, StorageDescriptor sd) {\n+\t\tswitch (alterOp) {\n+\t\t\tcase CHANGE_TBL_PROPS:\n+\t\t\t\toldProps.putAll(newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_LOCATION:\n+\t\t\t\tHiveTableUtil.extractLocation(sd, newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_FILE_FORMAT:\n+\t\t\t\tString newFileFormat = newProps.remove(STORED_AS_FILE_FORMAT);\n+\t\t\t\tHiveTableUtil.setStorageFormat(sd, newFileFormat, hiveConf);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_SERDE_PROPS:\n+\t\t\t\tHiveTableUtil.extractRowFormat(sd, newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase ADD_COLUMNS:\n+\t\t\tcase REPLACE_COLUMNS:\n+\t\t\t\tboolean cascade = Boolean.parseBoolean(newProps.remove(ALTER_COL_CASCADE));\n+\t\t\t\tensureCascadeOnPartitionedTable(hiveTable, cascade);\n+\t\t\t\tString[] names = newProps.remove(ADD_REPLACE_COL_NAMES).split(HiveDDLUtils.COL_DELIMITER);\n+\t\t\t\tString[] types = newProps.remove(ADD_REPLACE_COL_TYPES).split(HiveDDLUtils.COL_DELIMITER);\n+\t\t\t\tcheckArgument(names.length == types.length);\n+\t\t\t\tList<FieldSchema> columns = new ArrayList<>(names.length);\n+\t\t\t\tfor (int i = 0; i < names.length; i++) {\n+\t\t\t\t\tString comment = newProps.remove(String.format(ADD_REPLACE_COL_COMMENT_FORMAT, i));\n+\t\t\t\t\tcolumns.add(new FieldSchema(names[i], calTypeStrToHiveTypeInfo(types[i].toLowerCase()).getTypeName(), comment));\n+\t\t\t\t}\n+\t\t\t\taddReplaceColumns(sd, columns, alterOp == REPLACE_COLUMNS);\n+\t\t\t\tif (cascade) {\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tfor (CatalogPartitionSpec spec : listPartitions(new ObjectPath(hiveTable.getDbName(), hiveTable.getTableName()))) {\n+\t\t\t\t\t\t\tPartition partition = getHivePartition(hiveTable, spec);\n+\t\t\t\t\t\t\taddReplaceColumns(partition.getSd(), columns, alterOp == REPLACE_COLUMNS);\n+\t\t\t\t\t\t\tclient.alter_partition(hiveTable.getDbName(), hiveTable.getTableName(), partition);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\tthrow new CatalogException(\"Failed to cascade add/replace columns to partitions\", e);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_COLUMN:\n+\t\t\t\tcascade = Boolean.parseBoolean(newProps.remove(ALTER_COL_CASCADE));\n+\t\t\t\tensureCascadeOnPartitionedTable(hiveTable, cascade);\n+\t\t\t\tString oldName = newProps.remove(CHANGE_COL_OLD_NAME).toLowerCase();\n+\t\t\t\tString newName = newProps.remove(CHANGE_COL_NEW_NAME).toLowerCase();\n+\t\t\t\tString newType = newProps.remove(CHANGE_COL_NEW_TYPE).toLowerCase();\n+\t\t\t\tString colComment = newProps.remove(CHANGE_COL_COMMENT);\n+\t\t\t\tboolean first = Boolean.parseBoolean(newProps.remove(CHANGE_COL_FIRST));\n+\t\t\t\tString after = newProps.remove(CHANGE_COL_AFTER);\n+\t\t\t\tif (after != null) {\n+\t\t\t\t\tafter = after.toLowerCase();\n+\t\t\t\t}\n+\t\t\t\tchangeColumn(oldName, newName, newType, colComment, first, after, sd);\n+\t\t\t\tif (cascade) {\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tfor (CatalogPartitionSpec spec : listPartitions(new ObjectPath(hiveTable.getDbName(), hiveTable.getTableName()))) {\n+\t\t\t\t\t\t\tPartition partition = getHivePartition(hiveTable, spec);\n+\t\t\t\t\t\t\tchangeColumn(oldName, newName, newType, colComment, first, after, partition.getSd());\n+\t\t\t\t\t\t\tclient.alter_partition(hiveTable.getDbName(), hiveTable.getTableName(), partition);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\tthrow new CatalogException(\"Failed to cascade change column to partitions\", e);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tdefault:\n+\t\t\t\tthrow new CatalogException(\"Unsupported alter table operation \" + alterOp);\n+\t\t}\n+\t}\n+\n+\tprivate static void ensureCascadeOnPartitionedTable(Table hiveTable, boolean cascade) {\n+\t\tif (cascade) {\n+\t\t\tif (hiveTable == null) {\n+\t\t\t\tthrow new CatalogException(\"Alter columns cascade for a partition\");\n+\t\t\t}\n+\t\t\tif (!isTablePartitioned(hiveTable)) {\n+\t\t\t\tthrow new CatalogException(\"Alter columns cascade for non-partitioned table\");\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void changeColumn(String oldName, String newName, String newType, String comment, boolean first,\n+\t\t\tString after, StorageDescriptor sd) {\n+\t\tif (first && after != null) {\n+\t\t\tthrow new CatalogException(\"Both first and after specified for CHANGE COLUMN\");\n+\t\t}\n+\t\tTypeInfo newTypeInfo = calTypeStrToHiveTypeInfo(newType);\n+\t\tList<String> oldNames = getFieldNames(sd.getCols());\n+\t\tint oldIndex = oldNames.indexOf(oldName);\n+\t\tif (oldIndex < 0) {\n+\t\t\tthrow new CatalogException(String.format(\"Old column %s not found for CHANGE COLUMN\", oldName));\n+\t\t}\n+\t\tFieldSchema newField = new FieldSchema(newName, newTypeInfo.getTypeName(), comment);\n+\t\tif ((!first && after == null) || oldName.equals(after)) {\n+\t\t\tsd.getCols().set(oldIndex, newField);\n+\t\t} else {\n+\t\t\t// need change column position\n+\t\t\tsd.getCols().remove(oldIndex);\n+\t\t\tif (first) {\n+\t\t\t\tsd.getCols().add(0, newField);\n+\t\t\t} else {\n+\t\t\t\tint newIndex = oldNames.indexOf(after);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new CatalogException(String.format(\"After column %s not found for CHANGE COLUMN\", after));\n+\t\t\t\t}\n+\t\t\t\tsd.getCols().add(++newIndex, newField);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void addReplaceColumns(StorageDescriptor sd, List<FieldSchema> columns, boolean replace) {\n+\t\tif (replace) {\n+\t\t\tsd.setCols(columns);\n+\t\t} else {\n+\t\t\tsd.getCols().addAll(columns);\n+\t\t}\n+\t}\n+\n+\tprivate static TypeInfo calTypeStrToHiveTypeInfo(String typeStr) {", "originalCommit": "ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgxMjcxNw==", "url": "https://github.com/apache/flink/pull/12108#discussion_r425812717", "bodyText": "Is there no util from hive or flink-sql-parser-hive?", "author": "JingsongLi", "createdAt": "2020-05-15T13:46:25Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1450,4 +1489,253 @@ static boolean isGenericForGet(Map<String, String> properties) {\n \t\treturn properties != null && Boolean.parseBoolean(properties.getOrDefault(CatalogConfig.IS_GENERIC, \"false\"));\n \t}\n \n+\tpublic static void disallowChangeIsGeneric(boolean oldIsGeneric, boolean newIsGeneric) {\n+\t\tcheckArgument(oldIsGeneric == newIsGeneric, \"Changing whether a metadata object is generic is not allowed\");\n+\t}\n+\n+\tprivate static AlterTableOp extractAlterTableOp(Map<String, String> props) {\n+\t\tString opStr = props.remove(ALTER_TABLE_OP);\n+\t\tif (opStr != null) {\n+\t\t\treturn AlterTableOp.valueOf(opStr);\n+\t\t}\n+\t\treturn null;\n+\t}\n+\n+\tprivate Table alterTableViaCatalogBaseTable(ObjectPath tablePath, CatalogBaseTable baseTable, Table oldHiveTable) {\n+\t\tTable newHiveTable = instantiateHiveTable(tablePath, baseTable, hiveConf);\n+\t\t// client.alter_table() requires a valid location\n+\t\t// thus, if new table doesn't have that, it reuses location of the old table\n+\t\tif (!newHiveTable.getSd().isSetLocation()) {\n+\t\t\tnewHiveTable.getSd().setLocation(oldHiveTable.getSd().getLocation());\n+\t\t}\n+\t\treturn newHiveTable;\n+\t}\n+\n+\tprivate void alterTableViaProperties(AlterTableOp alterOp, Table hiveTable, Map<String, String> oldProps,\n+\t\t\tMap<String, String> newProps, StorageDescriptor sd) {\n+\t\tswitch (alterOp) {\n+\t\t\tcase CHANGE_TBL_PROPS:\n+\t\t\t\toldProps.putAll(newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_LOCATION:\n+\t\t\t\tHiveTableUtil.extractLocation(sd, newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_FILE_FORMAT:\n+\t\t\t\tString newFileFormat = newProps.remove(STORED_AS_FILE_FORMAT);\n+\t\t\t\tHiveTableUtil.setStorageFormat(sd, newFileFormat, hiveConf);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_SERDE_PROPS:\n+\t\t\t\tHiveTableUtil.extractRowFormat(sd, newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase ADD_COLUMNS:\n+\t\t\tcase REPLACE_COLUMNS:\n+\t\t\t\tboolean cascade = Boolean.parseBoolean(newProps.remove(ALTER_COL_CASCADE));\n+\t\t\t\tensureCascadeOnPartitionedTable(hiveTable, cascade);\n+\t\t\t\tString[] names = newProps.remove(ADD_REPLACE_COL_NAMES).split(HiveDDLUtils.COL_DELIMITER);\n+\t\t\t\tString[] types = newProps.remove(ADD_REPLACE_COL_TYPES).split(HiveDDLUtils.COL_DELIMITER);\n+\t\t\t\tcheckArgument(names.length == types.length);\n+\t\t\t\tList<FieldSchema> columns = new ArrayList<>(names.length);\n+\t\t\t\tfor (int i = 0; i < names.length; i++) {\n+\t\t\t\t\tString comment = newProps.remove(String.format(ADD_REPLACE_COL_COMMENT_FORMAT, i));\n+\t\t\t\t\tcolumns.add(new FieldSchema(names[i], calTypeStrToHiveTypeInfo(types[i].toLowerCase()).getTypeName(), comment));\n+\t\t\t\t}\n+\t\t\t\taddReplaceColumns(sd, columns, alterOp == REPLACE_COLUMNS);\n+\t\t\t\tif (cascade) {\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tfor (CatalogPartitionSpec spec : listPartitions(new ObjectPath(hiveTable.getDbName(), hiveTable.getTableName()))) {\n+\t\t\t\t\t\t\tPartition partition = getHivePartition(hiveTable, spec);\n+\t\t\t\t\t\t\taddReplaceColumns(partition.getSd(), columns, alterOp == REPLACE_COLUMNS);\n+\t\t\t\t\t\t\tclient.alter_partition(hiveTable.getDbName(), hiveTable.getTableName(), partition);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\tthrow new CatalogException(\"Failed to cascade add/replace columns to partitions\", e);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_COLUMN:\n+\t\t\t\tcascade = Boolean.parseBoolean(newProps.remove(ALTER_COL_CASCADE));\n+\t\t\t\tensureCascadeOnPartitionedTable(hiveTable, cascade);\n+\t\t\t\tString oldName = newProps.remove(CHANGE_COL_OLD_NAME).toLowerCase();\n+\t\t\t\tString newName = newProps.remove(CHANGE_COL_NEW_NAME).toLowerCase();\n+\t\t\t\tString newType = newProps.remove(CHANGE_COL_NEW_TYPE).toLowerCase();\n+\t\t\t\tString colComment = newProps.remove(CHANGE_COL_COMMENT);\n+\t\t\t\tboolean first = Boolean.parseBoolean(newProps.remove(CHANGE_COL_FIRST));\n+\t\t\t\tString after = newProps.remove(CHANGE_COL_AFTER);\n+\t\t\t\tif (after != null) {\n+\t\t\t\t\tafter = after.toLowerCase();\n+\t\t\t\t}\n+\t\t\t\tchangeColumn(oldName, newName, newType, colComment, first, after, sd);\n+\t\t\t\tif (cascade) {\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tfor (CatalogPartitionSpec spec : listPartitions(new ObjectPath(hiveTable.getDbName(), hiveTable.getTableName()))) {\n+\t\t\t\t\t\t\tPartition partition = getHivePartition(hiveTable, spec);\n+\t\t\t\t\t\t\tchangeColumn(oldName, newName, newType, colComment, first, after, partition.getSd());\n+\t\t\t\t\t\t\tclient.alter_partition(hiveTable.getDbName(), hiveTable.getTableName(), partition);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\tthrow new CatalogException(\"Failed to cascade change column to partitions\", e);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tdefault:\n+\t\t\t\tthrow new CatalogException(\"Unsupported alter table operation \" + alterOp);\n+\t\t}\n+\t}\n+\n+\tprivate static void ensureCascadeOnPartitionedTable(Table hiveTable, boolean cascade) {\n+\t\tif (cascade) {\n+\t\t\tif (hiveTable == null) {\n+\t\t\t\tthrow new CatalogException(\"Alter columns cascade for a partition\");\n+\t\t\t}\n+\t\t\tif (!isTablePartitioned(hiveTable)) {\n+\t\t\t\tthrow new CatalogException(\"Alter columns cascade for non-partitioned table\");\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void changeColumn(String oldName, String newName, String newType, String comment, boolean first,\n+\t\t\tString after, StorageDescriptor sd) {\n+\t\tif (first && after != null) {\n+\t\t\tthrow new CatalogException(\"Both first and after specified for CHANGE COLUMN\");\n+\t\t}\n+\t\tTypeInfo newTypeInfo = calTypeStrToHiveTypeInfo(newType);\n+\t\tList<String> oldNames = getFieldNames(sd.getCols());\n+\t\tint oldIndex = oldNames.indexOf(oldName);\n+\t\tif (oldIndex < 0) {\n+\t\t\tthrow new CatalogException(String.format(\"Old column %s not found for CHANGE COLUMN\", oldName));\n+\t\t}\n+\t\tFieldSchema newField = new FieldSchema(newName, newTypeInfo.getTypeName(), comment);\n+\t\tif ((!first && after == null) || oldName.equals(after)) {\n+\t\t\tsd.getCols().set(oldIndex, newField);\n+\t\t} else {\n+\t\t\t// need change column position\n+\t\t\tsd.getCols().remove(oldIndex);\n+\t\t\tif (first) {\n+\t\t\t\tsd.getCols().add(0, newField);\n+\t\t\t} else {\n+\t\t\t\tint newIndex = oldNames.indexOf(after);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new CatalogException(String.format(\"After column %s not found for CHANGE COLUMN\", after));\n+\t\t\t\t}\n+\t\t\t\tsd.getCols().add(++newIndex, newField);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void addReplaceColumns(StorageDescriptor sd, List<FieldSchema> columns, boolean replace) {\n+\t\tif (replace) {\n+\t\t\tsd.setCols(columns);\n+\t\t} else {\n+\t\t\tsd.getCols().addAll(columns);\n+\t\t}\n+\t}\n+\n+\tprivate static TypeInfo calTypeStrToHiveTypeInfo(String typeStr) {", "originalCommit": "ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgxNDEwOA==", "url": "https://github.com/apache/flink/pull/12108#discussion_r425814108", "bodyText": "Move them to an alter util?\nOr maybe a AlterTable, and have two implementation: ObjectAlterTable and PropertiesAlterTable.", "author": "JingsongLi", "createdAt": "2020-05-15T13:48:41Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -484,18 +512,24 @@ public void alterTable(ObjectPath tablePath, CatalogBaseTable newCatalogTable, b\n \t\t\t\t\texistingTable.getClass().getName(), newCatalogTable.getClass().getName()));\n \t\t}\n \n-\t\tTable newTable = instantiateHiveTable(tablePath, newCatalogTable, hiveConf);\n-\n-\t\t// client.alter_table() requires a valid location\n-\t\t// thus, if new table doesn't have that, it reuses location of the old table\n-\t\tif (!newTable.getSd().isSetLocation()) {\n-\t\t\tnewTable.getSd().setLocation(hiveTable.getSd().getLocation());\n+\t\tboolean isGeneric = isGenericForGet(hiveTable.getParameters());\n+\t\tif (isGeneric) {\n+\t\t\thiveTable = alterTableViaCatalogBaseTable(tablePath, newCatalogTable, hiveTable);\n+\t\t} else {\n+\t\t\tAlterTableOp op = extractAlterTableOp(newCatalogTable.getOptions());", "originalCommit": "ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0NTc3NQ==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426145775", "bodyText": "I can extract some methods to the util class, but it's difficulty to move all of them. The alterTableViaProperties needs to access the HMS client to do the cascade ALTER COLUMNS. So I don't think we should make it static.", "author": "lirui-apache", "createdAt": "2020-05-16T11:34:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgxNDEwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEwOTMxNA==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426109314", "bodyText": "Add a AlterTableColumnTypeOperation?", "author": "JingsongLi", "createdAt": "2020-05-16T02:34:42Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1450,4 +1489,253 @@ static boolean isGenericForGet(Map<String, String> properties) {\n \t\treturn properties != null && Boolean.parseBoolean(properties.getOrDefault(CatalogConfig.IS_GENERIC, \"false\"));\n \t}\n \n+\tpublic static void disallowChangeIsGeneric(boolean oldIsGeneric, boolean newIsGeneric) {\n+\t\tcheckArgument(oldIsGeneric == newIsGeneric, \"Changing whether a metadata object is generic is not allowed\");\n+\t}\n+\n+\tprivate static AlterTableOp extractAlterTableOp(Map<String, String> props) {\n+\t\tString opStr = props.remove(ALTER_TABLE_OP);\n+\t\tif (opStr != null) {\n+\t\t\treturn AlterTableOp.valueOf(opStr);\n+\t\t}\n+\t\treturn null;\n+\t}\n+\n+\tprivate Table alterTableViaCatalogBaseTable(ObjectPath tablePath, CatalogBaseTable baseTable, Table oldHiveTable) {\n+\t\tTable newHiveTable = instantiateHiveTable(tablePath, baseTable, hiveConf);\n+\t\t// client.alter_table() requires a valid location\n+\t\t// thus, if new table doesn't have that, it reuses location of the old table\n+\t\tif (!newHiveTable.getSd().isSetLocation()) {\n+\t\t\tnewHiveTable.getSd().setLocation(oldHiveTable.getSd().getLocation());\n+\t\t}\n+\t\treturn newHiveTable;\n+\t}\n+\n+\tprivate void alterTableViaProperties(AlterTableOp alterOp, Table hiveTable, Map<String, String> oldProps,\n+\t\t\tMap<String, String> newProps, StorageDescriptor sd) {\n+\t\tswitch (alterOp) {\n+\t\t\tcase CHANGE_TBL_PROPS:\n+\t\t\t\toldProps.putAll(newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_LOCATION:\n+\t\t\t\tHiveTableUtil.extractLocation(sd, newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_FILE_FORMAT:\n+\t\t\t\tString newFileFormat = newProps.remove(STORED_AS_FILE_FORMAT);\n+\t\t\t\tHiveTableUtil.setStorageFormat(sd, newFileFormat, hiveConf);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_SERDE_PROPS:\n+\t\t\t\tHiveTableUtil.extractRowFormat(sd, newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase ADD_COLUMNS:\n+\t\t\tcase REPLACE_COLUMNS:\n+\t\t\t\tboolean cascade = Boolean.parseBoolean(newProps.remove(ALTER_COL_CASCADE));\n+\t\t\t\tensureCascadeOnPartitionedTable(hiveTable, cascade);\n+\t\t\t\tString[] names = newProps.remove(ADD_REPLACE_COL_NAMES).split(HiveDDLUtils.COL_DELIMITER);\n+\t\t\t\tString[] types = newProps.remove(ADD_REPLACE_COL_TYPES).split(HiveDDLUtils.COL_DELIMITER);\n+\t\t\t\tcheckArgument(names.length == types.length);\n+\t\t\t\tList<FieldSchema> columns = new ArrayList<>(names.length);\n+\t\t\t\tfor (int i = 0; i < names.length; i++) {\n+\t\t\t\t\tString comment = newProps.remove(String.format(ADD_REPLACE_COL_COMMENT_FORMAT, i));\n+\t\t\t\t\tcolumns.add(new FieldSchema(names[i], calTypeStrToHiveTypeInfo(types[i].toLowerCase()).getTypeName(), comment));\n+\t\t\t\t}\n+\t\t\t\taddReplaceColumns(sd, columns, alterOp == REPLACE_COLUMNS);\n+\t\t\t\tif (cascade) {\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tfor (CatalogPartitionSpec spec : listPartitions(new ObjectPath(hiveTable.getDbName(), hiveTable.getTableName()))) {\n+\t\t\t\t\t\t\tPartition partition = getHivePartition(hiveTable, spec);\n+\t\t\t\t\t\t\taddReplaceColumns(partition.getSd(), columns, alterOp == REPLACE_COLUMNS);\n+\t\t\t\t\t\t\tclient.alter_partition(hiveTable.getDbName(), hiveTable.getTableName(), partition);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\tthrow new CatalogException(\"Failed to cascade add/replace columns to partitions\", e);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_COLUMN:\n+\t\t\t\tcascade = Boolean.parseBoolean(newProps.remove(ALTER_COL_CASCADE));\n+\t\t\t\tensureCascadeOnPartitionedTable(hiveTable, cascade);\n+\t\t\t\tString oldName = newProps.remove(CHANGE_COL_OLD_NAME).toLowerCase();\n+\t\t\t\tString newName = newProps.remove(CHANGE_COL_NEW_NAME).toLowerCase();\n+\t\t\t\tString newType = newProps.remove(CHANGE_COL_NEW_TYPE).toLowerCase();\n+\t\t\t\tString colComment = newProps.remove(CHANGE_COL_COMMENT);\n+\t\t\t\tboolean first = Boolean.parseBoolean(newProps.remove(CHANGE_COL_FIRST));\n+\t\t\t\tString after = newProps.remove(CHANGE_COL_AFTER);\n+\t\t\t\tif (after != null) {\n+\t\t\t\t\tafter = after.toLowerCase();\n+\t\t\t\t}\n+\t\t\t\tchangeColumn(oldName, newName, newType, colComment, first, after, sd);\n+\t\t\t\tif (cascade) {\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tfor (CatalogPartitionSpec spec : listPartitions(new ObjectPath(hiveTable.getDbName(), hiveTable.getTableName()))) {\n+\t\t\t\t\t\t\tPartition partition = getHivePartition(hiveTable, spec);\n+\t\t\t\t\t\t\tchangeColumn(oldName, newName, newType, colComment, first, after, partition.getSd());\n+\t\t\t\t\t\t\tclient.alter_partition(hiveTable.getDbName(), hiveTable.getTableName(), partition);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\tthrow new CatalogException(\"Failed to cascade change column to partitions\", e);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tdefault:\n+\t\t\t\tthrow new CatalogException(\"Unsupported alter table operation \" + alterOp);\n+\t\t}\n+\t}\n+\n+\tprivate static void ensureCascadeOnPartitionedTable(Table hiveTable, boolean cascade) {\n+\t\tif (cascade) {\n+\t\t\tif (hiveTable == null) {\n+\t\t\t\tthrow new CatalogException(\"Alter columns cascade for a partition\");\n+\t\t\t}\n+\t\t\tif (!isTablePartitioned(hiveTable)) {\n+\t\t\t\tthrow new CatalogException(\"Alter columns cascade for non-partitioned table\");\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void changeColumn(String oldName, String newName, String newType, String comment, boolean first,\n+\t\t\tString after, StorageDescriptor sd) {\n+\t\tif (first && after != null) {\n+\t\t\tthrow new CatalogException(\"Both first and after specified for CHANGE COLUMN\");\n+\t\t}\n+\t\tTypeInfo newTypeInfo = calTypeStrToHiveTypeInfo(newType);\n+\t\tList<String> oldNames = getFieldNames(sd.getCols());\n+\t\tint oldIndex = oldNames.indexOf(oldName);\n+\t\tif (oldIndex < 0) {\n+\t\t\tthrow new CatalogException(String.format(\"Old column %s not found for CHANGE COLUMN\", oldName));\n+\t\t}\n+\t\tFieldSchema newField = new FieldSchema(newName, newTypeInfo.getTypeName(), comment);\n+\t\tif ((!first && after == null) || oldName.equals(after)) {\n+\t\t\tsd.getCols().set(oldIndex, newField);\n+\t\t} else {\n+\t\t\t// need change column position\n+\t\t\tsd.getCols().remove(oldIndex);\n+\t\t\tif (first) {\n+\t\t\t\tsd.getCols().add(0, newField);\n+\t\t\t} else {\n+\t\t\t\tint newIndex = oldNames.indexOf(after);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new CatalogException(String.format(\"After column %s not found for CHANGE COLUMN\", after));\n+\t\t\t\t}\n+\t\t\t\tsd.getCols().add(++newIndex, newField);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void addReplaceColumns(StorageDescriptor sd, List<FieldSchema> columns, boolean replace) {\n+\t\tif (replace) {\n+\t\t\tsd.setCols(columns);\n+\t\t} else {\n+\t\t\tsd.getCols().addAll(columns);\n+\t\t}\n+\t}\n+\n+\tprivate static TypeInfo calTypeStrToHiveTypeInfo(String typeStr) {", "originalCommit": "ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ba3e0e5ab6fff290c83bcbcb1c2dc94e14a5f4f9", "url": "https://github.com/apache/flink/commit/ba3e0e5ab6fff290c83bcbcb1c2dc94e14a5f4f9", "message": "add operations for ALTER COLUMNS", "committedDate": "2020-05-16T09:46:30Z", "type": "forcePushed"}, {"oid": "d263c2535994e33dedb8bf0548e533f7e6049af9", "url": "https://github.com/apache/flink/commit/d263c2535994e33dedb8bf0548e533f7e6049af9", "message": "[FLINK-17448][sql-parser][table-api-java][table-planner-blink][hive] Implement table DDLs for Hive dialect part2", "committedDate": "2020-05-16T09:54:49Z", "type": "commit"}, {"oid": "91df094c8c723370106dac5606db3689de0953e8", "url": "https://github.com/apache/flink/commit/91df094c8c723370106dac5606db3689de0953e8", "message": "remove unused change", "committedDate": "2020-05-16T09:54:49Z", "type": "commit"}, {"oid": "86a531ee0e322ddab3a3a5e23a1098b62081e1d5", "url": "https://github.com/apache/flink/commit/86a531ee0e322ddab3a3a5e23a1098b62081e1d5", "message": "fix unparse", "committedDate": "2020-05-16T09:54:50Z", "type": "commit"}, {"oid": "8d938603ec47386fcf16e1e509751948bc44b8dc", "url": "https://github.com/apache/flink/commit/8d938603ec47386fcf16e1e509751948bc44b8dc", "message": "fix hive parser", "committedDate": "2020-05-16T09:54:50Z", "type": "commit"}, {"oid": "5d12339609095aaa590b5168239934468a2002ac", "url": "https://github.com/apache/flink/commit/5d12339609095aaa590b5168239934468a2002ac", "message": "add operations for ALTER COLUMNS", "committedDate": "2020-05-16T09:54:50Z", "type": "commit"}, {"oid": "5d12339609095aaa590b5168239934468a2002ac", "url": "https://github.com/apache/flink/commit/5d12339609095aaa590b5168239934468a2002ac", "message": "add operations for ALTER COLUMNS", "committedDate": "2020-05-16T09:54:50Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0MTUzMw==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426141533", "bodyText": "Add comments to public method.", "author": "JingsongLi", "createdAt": "2020-05-16T10:34:40Z", "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogManager.java", "diffHunk": "@@ -328,6 +329,17 @@ public CatalogBaseTable getTable() {\n \t\treturn Optional.empty();\n \t}\n \n+\tpublic Optional<CatalogPartition> getPartition(ObjectIdentifier tableIdentifier, CatalogPartitionSpec partitionSpec) {", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0NzcxMA==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426147710", "bodyText": "one parameter one line.", "author": "JingsongLi", "createdAt": "2020-05-16T12:04:30Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/utils/OperationConverterUtils.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.utils;\n+\n+import org.apache.flink.sql.parser.ddl.SqlAddReplaceColumns;\n+import org.apache.flink.sql.parser.ddl.SqlChangeColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.api.WatermarkSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.operations.Operation;\n+import org.apache.flink.table.operations.ddl.AlterTableSchemaOperation;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+\n+import org.apache.calcite.sql.SqlDataTypeSpec;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.validate.SqlValidator;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utils methods for converting sql to operations.\n+ */\n+public class OperationConverterUtils {\n+\n+\tprivate OperationConverterUtils() {\n+\t}\n+\n+\tpublic static Operation convertAddReplaceColumns(ObjectIdentifier tableIdentifier,\n+\t\t\tSqlAddReplaceColumns addReplaceColumns, CatalogTable catalogTable, SqlValidator sqlValidator) {", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0Nzc4NQ==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426147785", "bodyText": "This is not flink partition style, this is hive partition style", "author": "JingsongLi", "createdAt": "2020-05-16T12:05:16Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/utils/OperationConverterUtils.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.utils;\n+\n+import org.apache.flink.sql.parser.ddl.SqlAddReplaceColumns;\n+import org.apache.flink.sql.parser.ddl.SqlChangeColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.api.WatermarkSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.operations.Operation;\n+import org.apache.flink.table.operations.ddl.AlterTableSchemaOperation;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+\n+import org.apache.calcite.sql.SqlDataTypeSpec;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.validate.SqlValidator;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utils methods for converting sql to operations.\n+ */\n+public class OperationConverterUtils {\n+\n+\tprivate OperationConverterUtils() {\n+\t}\n+\n+\tpublic static Operation convertAddReplaceColumns(ObjectIdentifier tableIdentifier,\n+\t\t\tSqlAddReplaceColumns addReplaceColumns, CatalogTable catalogTable, SqlValidator sqlValidator) {\n+\t\t// verify partitions columns appear last in the schema\n+\t\tTableSchema oldSchema = catalogTable.getSchema();\n+\t\tint numPartCol = catalogTable.getPartitionKeys().size();", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE1NTEyMg==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426155122", "bodyText": "I have added some explanations in the comments.", "author": "lirui-apache", "createdAt": "2020-05-16T13:47:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0Nzc4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0Nzg1Nw==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426147857", "bodyText": "getPartitionSpec?", "author": "JingsongLi", "createdAt": "2020-05-16T12:06:14Z", "path": "flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/SqlPartitionUtils.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser;\n+\n+import org.apache.calcite.sql.SqlLiteral;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.util.NlsString;\n+\n+import java.util.LinkedHashMap;\n+\n+/**\n+ * Utils methods for partition DDLs.\n+ */\n+public class SqlPartitionUtils {\n+\n+\tprivate SqlPartitionUtils() {\n+\t}\n+\n+\t/** Get static partition key value pair as strings.\n+\t *\n+\t * <p>For character literals we return the unquoted and unescaped values.\n+\t * For other types we use {@link SqlLiteral#toString()} to get\n+\t * the string format of the value literal.\n+\t *\n+\t * @return the mapping of column names to values of partition specifications,\n+\t * returns an empty map if there is no partition specifications.\n+\t */\n+\tpublic static LinkedHashMap<String, String> getPartitionKVs(SqlNodeList partitionSpec) {", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE1MzY2MQ==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426153661", "bodyText": "In our name convention, a partition spec is usually a SqlNodeList. And this is following the example of RichSqlInsert, whose getStaticPartitionKVs returns a map.", "author": "lirui-apache", "createdAt": "2020-05-16T13:28:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0Nzg1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0NzkyMA==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426147920", "bodyText": "else if", "author": "JingsongLi", "createdAt": "2020-05-16T12:07:06Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java", "diffHunk": "@@ -231,82 +239,102 @@ private Operation convertDropTable(SqlDropTable sqlDropTable) {\n \tprivate Operation convertAlterTable(SqlAlterTable sqlAlterTable) {\n \t\tUnresolvedIdentifier unresolvedIdentifier = UnresolvedIdentifier.of(sqlAlterTable.fullTableName());\n \t\tObjectIdentifier tableIdentifier = catalogManager.qualifyIdentifier(unresolvedIdentifier);\n+\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n+\t\tif (!optionalCatalogTable.isPresent() || optionalCatalogTable.get().isTemporary()) {\n+\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n+\t\t\t\t\ttableIdentifier.toString()));\n+\t\t}\n+\t\tCatalogBaseTable baseTable = optionalCatalogTable.get().getTable();\n \t\tif (sqlAlterTable instanceof SqlAlterTableRename) {\n \t\t\tUnresolvedIdentifier newUnresolvedIdentifier =\n \t\t\t\tUnresolvedIdentifier.of(((SqlAlterTableRename) sqlAlterTable).fullNewTableName());\n \t\t\tObjectIdentifier newTableIdentifier = catalogManager.qualifyIdentifier(newUnresolvedIdentifier);\n \t\t\treturn new AlterTableRenameOperation(tableIdentifier, newTableIdentifier);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableProperties) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tCatalogTable originalCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tMap<String, String> properties = new HashMap<>(originalCatalogTable.getOptions());\n-\t\t\t\t((SqlAlterTableProperties) sqlAlterTable).getPropertyList().getList().forEach(p ->\n-\t\t\t\t\tproperties.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n-\t\t\t\tCatalogTable catalogTable = new CatalogTableImpl(\n-\t\t\t\t\toriginalCatalogTable.getSchema(),\n-\t\t\t\t\toriginalCatalogTable.getPartitionKeys(),\n-\t\t\t\t\tproperties,\n-\t\t\t\t\toriginalCatalogTable.getComment());\n-\t\t\t\treturn new AlterTablePropertiesOperation(tableIdentifier, catalogTable);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\ttableIdentifier.toString()));\n-\t\t\t}\n+\t\t\treturn convertAlterTableProperties(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tbaseTable,\n+\t\t\t\t\t(SqlAlterTableProperties) sqlAlterTable);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableAddConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n-\t\t\t\t\t\t.getConstraint();\n-\t\t\t\tvalidateTableConstraint(constraint);\n-\t\t\t\tTableSchema oriSchema = optionalCatalogTable.get().getTable().getSchema();\n-\t\t\t\t// Sanity check for constraint.\n-\t\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n-\t\t\t\tif (constraint.getConstraintName().isPresent()) {\n-\t\t\t\t\tbuilder.primaryKey(\n-\t\t\t\t\t\t\tconstraint.getConstraintName().get(),\n-\t\t\t\t\t\t\tconstraint.getColumnNames());\n-\t\t\t\t} else {\n-\t\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n-\t\t\t\t}\n-\t\t\t\tbuilder.build();\n-\t\t\t\treturn new AlterTableAddConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n+\t\t\t\t\t.getConstraint();\n+\t\t\tvalidateTableConstraint(constraint);\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\t// Sanity check for constraint.\n+\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n+\t\t\tif (constraint.getConstraintName().isPresent()) {\n+\t\t\t\tbuilder.primaryKey(\n+\t\t\t\t\t\tconstraint.getConstraintName().get(),\n \t\t\t\t\t\tconstraint.getColumnNames());\n \t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n \t\t\t}\n+\t\t\tbuilder.build();\n+\t\t\treturn new AlterTableAddConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\t\t\tconstraint.getColumnNames());\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableDropConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n-\t\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n-\t\t\t\tCatalogTable oriCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tTableSchema oriSchema = oriCatalogTable.getSchema();\n-\t\t\t\tif (!oriSchema.getPrimaryKey()\n-\t\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n-\t\t\t\t\t\t.isPresent()) {\n-\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n-\t\t\t\t}\n-\t\t\t\treturn new AlterTableDropConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraintName);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n+\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\tif (!oriSchema.getPrimaryKey()\n+\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n+\t\t\t\t\t.isPresent()) {\n+\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n \t\t\t}\n+\t\t\treturn new AlterTableDropConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraintName);\n+\t\t} else if (sqlAlterTable instanceof SqlAddReplaceColumns) {\n+\t\t\treturn OperationConverterUtils.convertAddReplaceColumns(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlAddReplaceColumns) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n+\t\t} else if (sqlAlterTable instanceof SqlChangeColumn) {\n+\t\t\treturn OperationConverterUtils.convertChangeColumn(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlChangeColumn) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n \t\t} else {\n \t\t\tthrow new ValidationException(\n \t\t\t\t\tString.format(\"[%s] needs to implement\",\n \t\t\t\t\t\t\tsqlAlterTable.toSqlString(CalciteSqlDialect.DEFAULT)));\n \t\t}\n \t}\n \n+\tprivate Operation convertAlterTableProperties(ObjectIdentifier tableIdentifier, CatalogBaseTable baseTable,\n+\t\t\tSqlAlterTableProperties alterTableProperties) {\n+\t\tLinkedHashMap<String, String> partitionKVs = alterTableProperties.getPartitionKVs();\n+\t\t// it's altering partitions\n+\t\tif (partitionKVs != null) {\n+\t\t\tCatalogPartitionSpec partitionSpec = new CatalogPartitionSpec(partitionKVs);\n+\t\t\tCatalogPartition catalogPartition = catalogManager.getPartition(tableIdentifier, partitionSpec)\n+\t\t\t\t\t.orElseThrow(() -> new ValidationException(String.format(\"Partition %s of table %s doesn't exist\",\n+\t\t\t\t\t\t\tpartitionSpec.getPartitionSpec(), tableIdentifier)));\n+\t\t\tMap<String, String> props = catalogPartition.getProperties();\n+\t\t\talterTableProperties.getPropertyList().getList().forEach(p ->\n+\t\t\t\t\tprops.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n+\t\t\treturn new AlterPartitionPropertiesOperation(tableIdentifier, partitionSpec, catalogPartition);\n+\t\t}\n+\t\t// it's altering a table\n+\t\tif (baseTable instanceof CatalogTable) {", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0NzkyNQ==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426147925", "bodyText": "else", "author": "JingsongLi", "createdAt": "2020-05-16T12:07:11Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java", "diffHunk": "@@ -231,82 +239,102 @@ private Operation convertDropTable(SqlDropTable sqlDropTable) {\n \tprivate Operation convertAlterTable(SqlAlterTable sqlAlterTable) {\n \t\tUnresolvedIdentifier unresolvedIdentifier = UnresolvedIdentifier.of(sqlAlterTable.fullTableName());\n \t\tObjectIdentifier tableIdentifier = catalogManager.qualifyIdentifier(unresolvedIdentifier);\n+\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n+\t\tif (!optionalCatalogTable.isPresent() || optionalCatalogTable.get().isTemporary()) {\n+\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n+\t\t\t\t\ttableIdentifier.toString()));\n+\t\t}\n+\t\tCatalogBaseTable baseTable = optionalCatalogTable.get().getTable();\n \t\tif (sqlAlterTable instanceof SqlAlterTableRename) {\n \t\t\tUnresolvedIdentifier newUnresolvedIdentifier =\n \t\t\t\tUnresolvedIdentifier.of(((SqlAlterTableRename) sqlAlterTable).fullNewTableName());\n \t\t\tObjectIdentifier newTableIdentifier = catalogManager.qualifyIdentifier(newUnresolvedIdentifier);\n \t\t\treturn new AlterTableRenameOperation(tableIdentifier, newTableIdentifier);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableProperties) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tCatalogTable originalCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tMap<String, String> properties = new HashMap<>(originalCatalogTable.getOptions());\n-\t\t\t\t((SqlAlterTableProperties) sqlAlterTable).getPropertyList().getList().forEach(p ->\n-\t\t\t\t\tproperties.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n-\t\t\t\tCatalogTable catalogTable = new CatalogTableImpl(\n-\t\t\t\t\toriginalCatalogTable.getSchema(),\n-\t\t\t\t\toriginalCatalogTable.getPartitionKeys(),\n-\t\t\t\t\tproperties,\n-\t\t\t\t\toriginalCatalogTable.getComment());\n-\t\t\t\treturn new AlterTablePropertiesOperation(tableIdentifier, catalogTable);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\ttableIdentifier.toString()));\n-\t\t\t}\n+\t\t\treturn convertAlterTableProperties(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tbaseTable,\n+\t\t\t\t\t(SqlAlterTableProperties) sqlAlterTable);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableAddConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n-\t\t\t\t\t\t.getConstraint();\n-\t\t\t\tvalidateTableConstraint(constraint);\n-\t\t\t\tTableSchema oriSchema = optionalCatalogTable.get().getTable().getSchema();\n-\t\t\t\t// Sanity check for constraint.\n-\t\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n-\t\t\t\tif (constraint.getConstraintName().isPresent()) {\n-\t\t\t\t\tbuilder.primaryKey(\n-\t\t\t\t\t\t\tconstraint.getConstraintName().get(),\n-\t\t\t\t\t\t\tconstraint.getColumnNames());\n-\t\t\t\t} else {\n-\t\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n-\t\t\t\t}\n-\t\t\t\tbuilder.build();\n-\t\t\t\treturn new AlterTableAddConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n+\t\t\t\t\t.getConstraint();\n+\t\t\tvalidateTableConstraint(constraint);\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\t// Sanity check for constraint.\n+\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n+\t\t\tif (constraint.getConstraintName().isPresent()) {\n+\t\t\t\tbuilder.primaryKey(\n+\t\t\t\t\t\tconstraint.getConstraintName().get(),\n \t\t\t\t\t\tconstraint.getColumnNames());\n \t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n \t\t\t}\n+\t\t\tbuilder.build();\n+\t\t\treturn new AlterTableAddConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\t\t\tconstraint.getColumnNames());\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableDropConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n-\t\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n-\t\t\t\tCatalogTable oriCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tTableSchema oriSchema = oriCatalogTable.getSchema();\n-\t\t\t\tif (!oriSchema.getPrimaryKey()\n-\t\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n-\t\t\t\t\t\t.isPresent()) {\n-\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n-\t\t\t\t}\n-\t\t\t\treturn new AlterTableDropConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraintName);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n+\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\tif (!oriSchema.getPrimaryKey()\n+\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n+\t\t\t\t\t.isPresent()) {\n+\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n \t\t\t}\n+\t\t\treturn new AlterTableDropConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraintName);\n+\t\t} else if (sqlAlterTable instanceof SqlAddReplaceColumns) {\n+\t\t\treturn OperationConverterUtils.convertAddReplaceColumns(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlAddReplaceColumns) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n+\t\t} else if (sqlAlterTable instanceof SqlChangeColumn) {\n+\t\t\treturn OperationConverterUtils.convertChangeColumn(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlChangeColumn) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n \t\t} else {\n \t\t\tthrow new ValidationException(\n \t\t\t\t\tString.format(\"[%s] needs to implement\",\n \t\t\t\t\t\t\tsqlAlterTable.toSqlString(CalciteSqlDialect.DEFAULT)));\n \t\t}\n \t}\n \n+\tprivate Operation convertAlterTableProperties(ObjectIdentifier tableIdentifier, CatalogBaseTable baseTable,\n+\t\t\tSqlAlterTableProperties alterTableProperties) {\n+\t\tLinkedHashMap<String, String> partitionKVs = alterTableProperties.getPartitionKVs();\n+\t\t// it's altering partitions\n+\t\tif (partitionKVs != null) {\n+\t\t\tCatalogPartitionSpec partitionSpec = new CatalogPartitionSpec(partitionKVs);\n+\t\t\tCatalogPartition catalogPartition = catalogManager.getPartition(tableIdentifier, partitionSpec)\n+\t\t\t\t\t.orElseThrow(() -> new ValidationException(String.format(\"Partition %s of table %s doesn't exist\",\n+\t\t\t\t\t\t\tpartitionSpec.getPartitionSpec(), tableIdentifier)));\n+\t\t\tMap<String, String> props = catalogPartition.getProperties();\n+\t\t\talterTableProperties.getPropertyList().getList().forEach(p ->\n+\t\t\t\t\tprops.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n+\t\t\treturn new AlterPartitionPropertiesOperation(tableIdentifier, partitionSpec, catalogPartition);\n+\t\t}\n+\t\t// it's altering a table\n+\t\tif (baseTable instanceof CatalogTable) {\n+\t\t\tCatalogTable oldTable = (CatalogTable) baseTable;\n+\t\t\tMap<String, String> newProperties = new HashMap<>(oldTable.getProperties());\n+\t\t\tnewProperties.putAll(OperationConverterUtils.extractProperties(alterTableProperties.getPropertyList()));\n+\t\t\tCatalogTable newTable = new CatalogTableImpl(\n+\t\t\t\t\toldTable.getSchema(),\n+\t\t\t\t\toldTable.getPartitionKeys(),\n+\t\t\t\t\tnewProperties,\n+\t\t\t\t\toldTable.getComment());\n+\t\t\treturn new AlterTablePropertiesOperation(tableIdentifier, newTable);\n+\t\t}\n+\t\tthrow new ValidationException(\"Unsupported CatalogBaseTable type: \" + baseTable.getClass().getName());", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0ODAyMg==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426148022", "bodyText": "oldTable.copy", "author": "JingsongLi", "createdAt": "2020-05-16T12:08:02Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java", "diffHunk": "@@ -231,82 +239,102 @@ private Operation convertDropTable(SqlDropTable sqlDropTable) {\n \tprivate Operation convertAlterTable(SqlAlterTable sqlAlterTable) {\n \t\tUnresolvedIdentifier unresolvedIdentifier = UnresolvedIdentifier.of(sqlAlterTable.fullTableName());\n \t\tObjectIdentifier tableIdentifier = catalogManager.qualifyIdentifier(unresolvedIdentifier);\n+\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n+\t\tif (!optionalCatalogTable.isPresent() || optionalCatalogTable.get().isTemporary()) {\n+\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n+\t\t\t\t\ttableIdentifier.toString()));\n+\t\t}\n+\t\tCatalogBaseTable baseTable = optionalCatalogTable.get().getTable();\n \t\tif (sqlAlterTable instanceof SqlAlterTableRename) {\n \t\t\tUnresolvedIdentifier newUnresolvedIdentifier =\n \t\t\t\tUnresolvedIdentifier.of(((SqlAlterTableRename) sqlAlterTable).fullNewTableName());\n \t\t\tObjectIdentifier newTableIdentifier = catalogManager.qualifyIdentifier(newUnresolvedIdentifier);\n \t\t\treturn new AlterTableRenameOperation(tableIdentifier, newTableIdentifier);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableProperties) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tCatalogTable originalCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tMap<String, String> properties = new HashMap<>(originalCatalogTable.getOptions());\n-\t\t\t\t((SqlAlterTableProperties) sqlAlterTable).getPropertyList().getList().forEach(p ->\n-\t\t\t\t\tproperties.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n-\t\t\t\tCatalogTable catalogTable = new CatalogTableImpl(\n-\t\t\t\t\toriginalCatalogTable.getSchema(),\n-\t\t\t\t\toriginalCatalogTable.getPartitionKeys(),\n-\t\t\t\t\tproperties,\n-\t\t\t\t\toriginalCatalogTable.getComment());\n-\t\t\t\treturn new AlterTablePropertiesOperation(tableIdentifier, catalogTable);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\ttableIdentifier.toString()));\n-\t\t\t}\n+\t\t\treturn convertAlterTableProperties(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tbaseTable,\n+\t\t\t\t\t(SqlAlterTableProperties) sqlAlterTable);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableAddConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n-\t\t\t\t\t\t.getConstraint();\n-\t\t\t\tvalidateTableConstraint(constraint);\n-\t\t\t\tTableSchema oriSchema = optionalCatalogTable.get().getTable().getSchema();\n-\t\t\t\t// Sanity check for constraint.\n-\t\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n-\t\t\t\tif (constraint.getConstraintName().isPresent()) {\n-\t\t\t\t\tbuilder.primaryKey(\n-\t\t\t\t\t\t\tconstraint.getConstraintName().get(),\n-\t\t\t\t\t\t\tconstraint.getColumnNames());\n-\t\t\t\t} else {\n-\t\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n-\t\t\t\t}\n-\t\t\t\tbuilder.build();\n-\t\t\t\treturn new AlterTableAddConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n+\t\t\t\t\t.getConstraint();\n+\t\t\tvalidateTableConstraint(constraint);\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\t// Sanity check for constraint.\n+\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n+\t\t\tif (constraint.getConstraintName().isPresent()) {\n+\t\t\t\tbuilder.primaryKey(\n+\t\t\t\t\t\tconstraint.getConstraintName().get(),\n \t\t\t\t\t\tconstraint.getColumnNames());\n \t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n \t\t\t}\n+\t\t\tbuilder.build();\n+\t\t\treturn new AlterTableAddConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\t\t\tconstraint.getColumnNames());\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableDropConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n-\t\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n-\t\t\t\tCatalogTable oriCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tTableSchema oriSchema = oriCatalogTable.getSchema();\n-\t\t\t\tif (!oriSchema.getPrimaryKey()\n-\t\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n-\t\t\t\t\t\t.isPresent()) {\n-\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n-\t\t\t\t}\n-\t\t\t\treturn new AlterTableDropConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraintName);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n+\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\tif (!oriSchema.getPrimaryKey()\n+\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n+\t\t\t\t\t.isPresent()) {\n+\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n \t\t\t}\n+\t\t\treturn new AlterTableDropConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraintName);\n+\t\t} else if (sqlAlterTable instanceof SqlAddReplaceColumns) {\n+\t\t\treturn OperationConverterUtils.convertAddReplaceColumns(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlAddReplaceColumns) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n+\t\t} else if (sqlAlterTable instanceof SqlChangeColumn) {\n+\t\t\treturn OperationConverterUtils.convertChangeColumn(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlChangeColumn) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n \t\t} else {\n \t\t\tthrow new ValidationException(\n \t\t\t\t\tString.format(\"[%s] needs to implement\",\n \t\t\t\t\t\t\tsqlAlterTable.toSqlString(CalciteSqlDialect.DEFAULT)));\n \t\t}\n \t}\n \n+\tprivate Operation convertAlterTableProperties(ObjectIdentifier tableIdentifier, CatalogBaseTable baseTable,\n+\t\t\tSqlAlterTableProperties alterTableProperties) {\n+\t\tLinkedHashMap<String, String> partitionKVs = alterTableProperties.getPartitionKVs();\n+\t\t// it's altering partitions\n+\t\tif (partitionKVs != null) {\n+\t\t\tCatalogPartitionSpec partitionSpec = new CatalogPartitionSpec(partitionKVs);\n+\t\t\tCatalogPartition catalogPartition = catalogManager.getPartition(tableIdentifier, partitionSpec)\n+\t\t\t\t\t.orElseThrow(() -> new ValidationException(String.format(\"Partition %s of table %s doesn't exist\",\n+\t\t\t\t\t\t\tpartitionSpec.getPartitionSpec(), tableIdentifier)));\n+\t\t\tMap<String, String> props = catalogPartition.getProperties();\n+\t\t\talterTableProperties.getPropertyList().getList().forEach(p ->\n+\t\t\t\t\tprops.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n+\t\t\treturn new AlterPartitionPropertiesOperation(tableIdentifier, partitionSpec, catalogPartition);\n+\t\t}\n+\t\t// it's altering a table\n+\t\tif (baseTable instanceof CatalogTable) {\n+\t\t\tCatalogTable oldTable = (CatalogTable) baseTable;\n+\t\t\tMap<String, String> newProperties = new HashMap<>(oldTable.getProperties());\n+\t\t\tnewProperties.putAll(OperationConverterUtils.extractProperties(alterTableProperties.getPropertyList()));\n+\t\t\tCatalogTable newTable = new CatalogTableImpl(", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0ODE1Nw==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426148157", "bodyText": "Should create a new CatalogPartition with a new Map.", "author": "JingsongLi", "createdAt": "2020-05-16T12:10:13Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java", "diffHunk": "@@ -231,82 +239,102 @@ private Operation convertDropTable(SqlDropTable sqlDropTable) {\n \tprivate Operation convertAlterTable(SqlAlterTable sqlAlterTable) {\n \t\tUnresolvedIdentifier unresolvedIdentifier = UnresolvedIdentifier.of(sqlAlterTable.fullTableName());\n \t\tObjectIdentifier tableIdentifier = catalogManager.qualifyIdentifier(unresolvedIdentifier);\n+\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n+\t\tif (!optionalCatalogTable.isPresent() || optionalCatalogTable.get().isTemporary()) {\n+\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n+\t\t\t\t\ttableIdentifier.toString()));\n+\t\t}\n+\t\tCatalogBaseTable baseTable = optionalCatalogTable.get().getTable();\n \t\tif (sqlAlterTable instanceof SqlAlterTableRename) {\n \t\t\tUnresolvedIdentifier newUnresolvedIdentifier =\n \t\t\t\tUnresolvedIdentifier.of(((SqlAlterTableRename) sqlAlterTable).fullNewTableName());\n \t\t\tObjectIdentifier newTableIdentifier = catalogManager.qualifyIdentifier(newUnresolvedIdentifier);\n \t\t\treturn new AlterTableRenameOperation(tableIdentifier, newTableIdentifier);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableProperties) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tCatalogTable originalCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tMap<String, String> properties = new HashMap<>(originalCatalogTable.getOptions());\n-\t\t\t\t((SqlAlterTableProperties) sqlAlterTable).getPropertyList().getList().forEach(p ->\n-\t\t\t\t\tproperties.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n-\t\t\t\tCatalogTable catalogTable = new CatalogTableImpl(\n-\t\t\t\t\toriginalCatalogTable.getSchema(),\n-\t\t\t\t\toriginalCatalogTable.getPartitionKeys(),\n-\t\t\t\t\tproperties,\n-\t\t\t\t\toriginalCatalogTable.getComment());\n-\t\t\t\treturn new AlterTablePropertiesOperation(tableIdentifier, catalogTable);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\ttableIdentifier.toString()));\n-\t\t\t}\n+\t\t\treturn convertAlterTableProperties(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tbaseTable,\n+\t\t\t\t\t(SqlAlterTableProperties) sqlAlterTable);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableAddConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n-\t\t\t\t\t\t.getConstraint();\n-\t\t\t\tvalidateTableConstraint(constraint);\n-\t\t\t\tTableSchema oriSchema = optionalCatalogTable.get().getTable().getSchema();\n-\t\t\t\t// Sanity check for constraint.\n-\t\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n-\t\t\t\tif (constraint.getConstraintName().isPresent()) {\n-\t\t\t\t\tbuilder.primaryKey(\n-\t\t\t\t\t\t\tconstraint.getConstraintName().get(),\n-\t\t\t\t\t\t\tconstraint.getColumnNames());\n-\t\t\t\t} else {\n-\t\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n-\t\t\t\t}\n-\t\t\t\tbuilder.build();\n-\t\t\t\treturn new AlterTableAddConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n+\t\t\t\t\t.getConstraint();\n+\t\t\tvalidateTableConstraint(constraint);\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\t// Sanity check for constraint.\n+\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n+\t\t\tif (constraint.getConstraintName().isPresent()) {\n+\t\t\t\tbuilder.primaryKey(\n+\t\t\t\t\t\tconstraint.getConstraintName().get(),\n \t\t\t\t\t\tconstraint.getColumnNames());\n \t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n \t\t\t}\n+\t\t\tbuilder.build();\n+\t\t\treturn new AlterTableAddConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\t\t\tconstraint.getColumnNames());\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableDropConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n-\t\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n-\t\t\t\tCatalogTable oriCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tTableSchema oriSchema = oriCatalogTable.getSchema();\n-\t\t\t\tif (!oriSchema.getPrimaryKey()\n-\t\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n-\t\t\t\t\t\t.isPresent()) {\n-\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n-\t\t\t\t}\n-\t\t\t\treturn new AlterTableDropConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraintName);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n+\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\tif (!oriSchema.getPrimaryKey()\n+\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n+\t\t\t\t\t.isPresent()) {\n+\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n \t\t\t}\n+\t\t\treturn new AlterTableDropConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraintName);\n+\t\t} else if (sqlAlterTable instanceof SqlAddReplaceColumns) {\n+\t\t\treturn OperationConverterUtils.convertAddReplaceColumns(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlAddReplaceColumns) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n+\t\t} else if (sqlAlterTable instanceof SqlChangeColumn) {\n+\t\t\treturn OperationConverterUtils.convertChangeColumn(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlChangeColumn) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n \t\t} else {\n \t\t\tthrow new ValidationException(\n \t\t\t\t\tString.format(\"[%s] needs to implement\",\n \t\t\t\t\t\t\tsqlAlterTable.toSqlString(CalciteSqlDialect.DEFAULT)));\n \t\t}\n \t}\n \n+\tprivate Operation convertAlterTableProperties(ObjectIdentifier tableIdentifier, CatalogBaseTable baseTable,\n+\t\t\tSqlAlterTableProperties alterTableProperties) {\n+\t\tLinkedHashMap<String, String> partitionKVs = alterTableProperties.getPartitionKVs();\n+\t\t// it's altering partitions\n+\t\tif (partitionKVs != null) {\n+\t\t\tCatalogPartitionSpec partitionSpec = new CatalogPartitionSpec(partitionKVs);\n+\t\t\tCatalogPartition catalogPartition = catalogManager.getPartition(tableIdentifier, partitionSpec)\n+\t\t\t\t\t.orElseThrow(() -> new ValidationException(String.format(\"Partition %s of table %s doesn't exist\",\n+\t\t\t\t\t\t\tpartitionSpec.getPartitionSpec(), tableIdentifier)));\n+\t\t\tMap<String, String> props = catalogPartition.getProperties();", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "21988e7157e176b1d75a16dbfd8016075ad51968", "url": "https://github.com/apache/flink/commit/21988e7157e176b1d75a16dbfd8016075ad51968", "message": "address comments", "committedDate": "2020-05-16T12:14:04Z", "type": "commit"}, {"oid": "05368729e02dc0fad72c783a85cf951418ef901a", "url": "https://github.com/apache/flink/commit/05368729e02dc0fad72c783a85cf951418ef901a", "message": "address comments", "committedDate": "2020-05-16T13:45:55Z", "type": "commit"}]}