{"pr_number": 12594, "pr_title": "[FLINK-18072][hbase] HBaseLookupFunction can not work with new internal data structure RowData", "pr_createdAt": "2020-06-11T07:32:41Z", "pr_url": "https://github.com/apache/flink/pull/12594", "timeline": [{"oid": "5be824fed4684ac3078b929bb7e5eecb7c976ce6", "url": "https://github.com/apache/flink/commit/5be824fed4684ac3078b929bb7e5eecb7c976ce6", "message": "[FLINK-18072][hbase] HBaseLookupFunction can not work with new internal data structure RowData", "committedDate": "2020-06-11T07:22:13Z", "type": "commit"}, {"oid": "0341f1b2975e50cae4afc99173f0a1b7a27ed870", "url": "https://github.com/apache/flink/commit/0341f1b2975e50cae4afc99173f0a1b7a27ed870", "message": "minor", "committedDate": "2020-06-11T13:09:58Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTI2MzU2Mg==", "url": "https://github.com/apache/flink/pull/12594#discussion_r439263562", "bodyText": "Do not use HBaseReadWriteHelper. It uses legacy type. You can add a new method createGet in HbaseSerDe.", "author": "wuchong", "createdAt": "2020-06-12T07:50:46Z", "path": "flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseRowDataLookupFunction.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.connector.hbase.util.HBaseConfigurationUtil;\n+import org.apache.flink.connector.hbase.util.HBaseReadWriteHelper;\n+import org.apache.flink.connector.hbase.util.HBaseSerde;\n+import org.apache.flink.connector.hbase.util.HBaseTableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.util.StringUtils;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.TableNotFoundException;\n+import org.apache.hadoop.hbase.client.Connection;\n+import org.apache.hadoop.hbase.client.ConnectionFactory;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+/**\n+ * The HBaseRowDataLookupFunction is a standard user-defined table function, it can be used in tableAPI\n+ * and also useful for temporal table join plan in SQL. It looks up the result as {@link RowData}.\n+ */\n+@Internal\n+public class HBaseRowDataLookupFunction extends TableFunction<RowData> {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(HBaseRowDataLookupFunction.class);\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final String hTableName;\n+\tprivate final byte[] serializedConfig;\n+\tprivate final HBaseTableSchema hbaseTableSchema;\n+\tprivate final String nullStringLiteral;\n+\n+\tprivate transient HBaseReadWriteHelper readHelper;\n+\tprivate transient Connection hConnection;\n+\tprivate transient HTable table;\n+\tprivate transient HBaseSerde serde;\n+\n+\tpublic HBaseRowDataLookupFunction(\n+\t\t\tConfiguration configuration,\n+\t\t\tString hTableName,\n+\t\t\tHBaseTableSchema hbaseTableSchema,\n+\t\t\tString nullStringLiteral) {\n+\t\tthis.serializedConfig = HBaseConfigurationUtil.serializeConfiguration(configuration);\n+\t\tthis.hTableName = hTableName;\n+\t\tthis.hbaseTableSchema = hbaseTableSchema;\n+\t\tthis.nullStringLiteral = nullStringLiteral;\n+\t}\n+\n+\t/**\n+\t * The invoke entry point of lookup function.\n+\t * @param rowKey the lookup key. Currently only support single rowkey.\n+\t */\n+\tpublic void eval(Object rowKey) throws IOException {\n+\t\t// fetch result\n+\t\tResult result = table.get(readHelper.createGet(rowKey));", "originalCommit": "0341f1b2975e50cae4afc99173f0a1b7a27ed870", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTI2OTM3OQ==", "url": "https://github.com/apache/flink/pull/12594#discussion_r439269379", "bodyText": "Could you add a DECIMAL to the lookup table? It is the other error-prone type.", "author": "wuchong", "createdAt": "2020-06-12T08:02:59Z", "path": "flink-connectors/flink-connector-hbase/src/test/java/org/apache/flink/connector/hbase/HBaseConnectorITCase.java", "diffHunk": "@@ -558,71 +539,71 @@ public void testHBaseLookupTableSource() throws Exception {\n \t\t}\n \t\tStreamExecutionEnvironment streamEnv = StreamExecutionEnvironment.getExecutionEnvironment();\n \t\tStreamTableEnvironment streamTableEnv = StreamTableEnvironment.create(streamEnv, streamSettings);\n+\n+\t\t// prepare dimension table data.\n+\t\tDataStream<Row> ds = streamEnv.fromCollection(testData1).returns(testTypeInfo1);\n+\t\tstreamTableEnv.createTemporaryView(\"testData\", ds);\n+\t\tString ddl = getDDLForTestTable3();\n+\t\tstreamTableEnv.executeSql(ddl);\n+\n+\t\tString query = \"INSERT INTO hbase \" +\n+\t\t\t\"SELECT rowkey, ROW(f1c1), ROW(f2c1, f2c2), ROW(f3c1, f3c2, f3c3), ROW(f4c1, f4c2, f4c3) \" +\n+\t\t\t\"FROM testData\";\n+\t\tTableEnvUtil.execInsertSqlAndWaitResult(streamTableEnv, query);\n \t\tStreamITCase.clear();\n \n \t\t// prepare a source table\n \t\tString srcTableName = \"src\";\n-\t\tDataStream<Row> ds = streamEnv.fromCollection(testData2).returns(testTypeInfo2);\n-\t\tTable in = streamTableEnv.fromDataStream(ds, $(\"a\"), $(\"b\"), $(\"c\"), $(\"proc\").proctime());\n+\t\tDataStream<Row> srcDs = streamEnv.fromCollection(testData2).returns(testTypeInfo2);\n+\t\tTable in = streamTableEnv.fromDataStream(srcDs, $(\"a\"), $(\"b\"), $(\"c\"), $(\"proc\").proctime());\n \t\tstreamTableEnv.registerTable(srcTableName, in);\n \n-\t\tif (isLegacyConnector) {\n-\t\t\tMap<String, String> tableProperties = hbaseTableProperties();\n-\t\t\tTableSource<?> source = TableFactoryService\n-\t\t\t\t.find(HBaseTableFactory.class, tableProperties)\n-\t\t\t\t.createTableSource(tableProperties);\n-\t\t\t((TableEnvironmentInternal) streamTableEnv).registerTableSourceInternal(\"hbaseLookup\", source);\n-\t\t} else {\n-\t\t\tstreamTableEnv.executeSql(\n-\t\t\t\t\t\"CREATE TABLE hbaseLookup (\" +\n-\t\t\t\t\t\" family1 ROW<col1 INT>,\" +\n-\t\t\t\t\t\" rk INT,\" +\n-\t\t\t\t\t\" family2 ROW<col1 STRING, col2 BIGINT>,\" +\n-\t\t\t\t\t\" family3 ROW<col1 DOUBLE, col2 BOOLEAN, col3 STRING>\" +\n-\t\t\t\t\t\") WITH (\" +\n-\t\t\t\t\t\" 'connector' = 'hbase-1.4',\" +\n-\t\t\t\t\t\" 'table-name' = '\" + TEST_TABLE_1 + \"',\" +\n-\t\t\t\t\t\" 'zookeeper.quorum' = '\" + getZookeeperQuorum() + \"'\" +\n-\t\t\t\t\t\")\");\n-\t\t}\n \t\t// perform a temporal table join query\n-\t\tString query = \"SELECT a,family1.col1, family3.col3 FROM src \" +\n-\t\t\t\"JOIN hbaseLookup FOR SYSTEM_TIME AS OF src.proc as h ON src.a = h.rk\";\n-\t\tTable result = streamTableEnv.sqlQuery(query);\n+\t\tString dimJoinQuery = \"SELECT\" +\n+\t\t\t\" a,\" +\n+\t\t\t\" b,\" +\n+\t\t\t\" family1.col1,\" +\n+\t\t\t\" family2.col1,\" +\n+\t\t\t\" family2.col2,\" +\n+\t\t\t\" family3.col1,\" +\n+\t\t\t\" family3.col2,\" +\n+\t\t\t\" family3.col3,\" +\n+\t\t\t\" family4.col1,\" +\n+\t\t\t\" family4.col2,\" +\n+\t\t\t\" family4.col3\" +\n+\t\t\t\" FROM src JOIN hbase FOR SYSTEM_TIME AS OF src.proc as h ON src.a = h.rowkey\";\n+\t\tIterator<Row> collected = streamTableEnv.executeSql(dimJoinQuery).collect();\n+\t\tList<String> result = Lists.newArrayList(collected).stream()\n+\t\t\t.map(Row::toString)\n+\t\t\t.sorted()\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\t// check result, the time type in collected result is LOCAL_DATE_TIME, LOCAL_DATE, LOCAL_TIME\n+\t\tList<String> expected = new ArrayList<>();\n+\t\texpected.add(\"1,1,10,Hello-1,100,1.01,false,Welt-1,2019-08-18T19:00,2019-08-18,19:00\");\n+\t\texpected.add(\"2,2,20,Hello-2,200,2.02,true,Welt-2,2019-08-18T19:01,2019-08-18,19:01\");\n+\t\texpected.add(\"3,2,30,Hello-3,300,3.03,false,Welt-3,2019-08-18T19:02,2019-08-18,19:02\");\n+\t\texpected.add(\"3,3,30,Hello-3,300,3.03,false,Welt-3,2019-08-18T19:02,2019-08-18,19:02\");", "originalCommit": "0341f1b2975e50cae4afc99173f0a1b7a27ed870", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "34bcc247a9818e01531c1a118104ac935ae5ae02", "url": "https://github.com/apache/flink/commit/34bcc247a9818e01531c1a118104ac935ae5ae02", "message": "address comments and timestamp/time/date/decimal test in ITCase", "committedDate": "2020-06-15T12:52:44Z", "type": "commit"}, {"oid": "b6e4be8ec670abd7490d4077b162451cbff47902", "url": "https://github.com/apache/flink/commit/b6e4be8ec670abd7490d4077b162451cbff47902", "message": "format", "committedDate": "2020-06-15T14:27:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI2Mjg2NA==", "url": "https://github.com/apache/flink/pull/12594#discussion_r440262864", "bodyText": "The returned Get may be null.", "author": "wuchong", "createdAt": "2020-06-15T15:32:30Z", "path": "flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/source/HBaseRowDataLookupFunction.java", "diffHunk": "@@ -79,7 +77,7 @@ public HBaseRowDataLookupFunction(\n \t */\n \tpublic void eval(Object rowKey) throws IOException {\n \t\t// fetch result\n-\t\tResult result = table.get(readHelper.createGet(rowKey));\n+\t\tResult result = table.get(serde.createGet(rowKey));", "originalCommit": "34bcc247a9818e01531c1a118104ac935ae5ae02", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY4NDM3NQ==", "url": "https://github.com/apache/flink/pull/12594#discussion_r440684375", "bodyText": "We can reuse the GenericRowData.", "author": "wuchong", "createdAt": "2020-06-16T08:40:32Z", "path": "flink-connectors/flink-connector-hbase/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java", "diffHunk": "@@ -190,6 +191,30 @@ public Scan createScan() {\n \t\treturn scan;\n \t}\n \n+\t/**\n+\t * Returns an instance of Get that retrieves the matches records from the HBase table.\n+\t *\n+\t * @return The appropriate instance of Get for this use case.\n+\t */\n+\tpublic Get createGet(Object rowKey) {\n+\t\tcheckArgument(keyEncoder != null, \"row key is not set.\");\n+\t\tGenericRowData rowData = new GenericRowData(1);", "originalCommit": "b6e4be8ec670abd7490d4077b162451cbff47902", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY5ODY4OQ==", "url": "https://github.com/apache/flink/pull/12594#discussion_r440698689", "bodyText": "Can we also migrate this to use DDL?", "author": "wuchong", "createdAt": "2020-06-16T09:02:40Z", "path": "flink-connectors/flink-connector-hbase/src/test/java/org/apache/flink/connector/hbase/HBaseConnectorITCase.java", "diffHunk": "@@ -377,57 +344,28 @@ public void testTableSink() throws Exception {\n \t\t\tTableSink tableSink = TableFactoryService\n \t\t\t\t.find(HBaseTableFactory.class, descriptorProperties.asMap())\n \t\t\t\t.createTableSink(descriptorProperties.asMap());\n-\t\t\t((TableEnvironmentInternal) tEnv).registerTableSinkInternal(\"hbase\", tableSink);\n+\t\t\t((TableEnvironmentInternal) tEnv).registerTableSinkInternal(TEST_TABLE_2, tableSink);", "originalCommit": "b6e4be8ec670abd7490d4077b162451cbff47902", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDcwNDA5OA==", "url": "https://github.com/apache/flink/pull/12594#discussion_r440704098", "bodyText": "This test code is really hard to maintain and read. I would suggest only test for blink planner and new connector for simplification.", "author": "wuchong", "createdAt": "2020-06-16T09:11:34Z", "path": "flink-connectors/flink-connector-hbase/src/test/java/org/apache/flink/connector/hbase/HBaseConnectorITCase.java", "diffHunk": "@@ -455,99 +392,82 @@ public void testTableSink() throws Exception {\n \n \t@Test\n \tpublic void testTableSourceSinkWithDDL() throws Exception {\n+\t\t// only test TIMESTAMP/DATE/TIME/DECIMAL for new connector(using blink-planner), because new connector encodes\n+\t\t// DATE/TIME to int, the old one encodes to long, and DECIMAL with precision works well in new connector.\n+\t\tfinal boolean testTimeAndDecimalTypes = BLINK_PLANNER.equals(planner) && !isLegacyConnector;", "originalCommit": "b6e4be8ec670abd7490d4077b162451cbff47902", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a6358e3178bb96500b37a7f0e209845cd4822622", "url": "https://github.com/apache/flink/commit/a6358e3178bb96500b37a7f0e209845cd4822622", "message": "address comments", "committedDate": "2020-06-16T17:07:30Z", "type": "commit"}, {"oid": "79249151cc48e2711a224e5ca943c41ae434ce19", "url": "https://github.com/apache/flink/commit/79249151cc48e2711a224e5ca943c41ae434ce19", "message": "minor", "committedDate": "2020-06-17T02:37:44Z", "type": "commit"}]}