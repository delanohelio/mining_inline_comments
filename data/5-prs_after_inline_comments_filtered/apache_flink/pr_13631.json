{"pr_number": 13631, "pr_title": "[FLINK-19639][table sql/planner]Support SupportsNestedProjectionPushD\u2026", "pr_createdAt": "2020-10-14T10:39:52Z", "pr_url": "https://github.com/apache/flink/pull/13631", "timeline": [{"oid": "cb66dc97754abc1468b03508a41a4f3a34ce9572", "url": "https://github.com/apache/flink/commit/cb66dc97754abc1468b03508a41a4f3a34ce9572", "message": "[FLINK-19693][table sql/planner]Support SupportsNestedProjectionPushDown in planner", "committedDate": "2020-10-14T11:39:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTQxODI3OA==", "url": "https://github.com/apache/flink/pull/13631#discussion_r505418278", "bodyText": "nit: add a test about complex expressions, such as  deepNested.nested1.name + nested.value", "author": "godfreyhe", "createdAt": "2020-10-15T10:00:29Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java", "diffHunk": "@@ -107,4 +96,13 @@ private void testNestedProject(boolean nestedProjectionSupported) {\n \t\tutil().verifyPlan(sqlQuery);\n \t}\n \n+\t@Test\n+\tpublic void testComplicatedNestedProject() {\n+\t\tString sqlQuery = \"SELECT id,\" +\n+\t\t\t\t\"    deepNested.nested1.name AS nestedName,\\n\" +\n+\t\t\t\t\"    deepNested.nested2 AS nested2,\\n\" +\n+\t\t\t\t\"    deepNested.nested2.num AS nestedNum\\n\" +\n+\t\t\t\t\"FROM NestedTable\";\n+\t\tutil().verifyPlan(sqlQuery);", "originalCommit": "cb66dc97754abc1468b03508a41a4f3a34ce9572", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjAzNzEyOA==", "url": "https://github.com/apache/flink/pull/13631#discussion_r506037128", "bodyText": "Add test with calculation in query.", "author": "fsk119", "createdAt": "2020-10-16T03:59:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTQxODI3OA=="}], "type": "inlineReview"}, {"oid": "4b3b9898ceeebc31777e2aa96d04b445cf578d87", "url": "https://github.com/apache/flink/commit/4b3b9898ceeebc31777e2aa96d04b445cf578d87", "message": "fix godfrey's comment:\n1. use qualified name list to get the projectedFields and build new projections;\n2. add more tests", "committedDate": "2020-10-16T03:56:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjkxNzM5Ng==", "url": "https://github.com/apache/flink/pull/13631#discussion_r506917396", "bodyText": "how about resolve the conflicts through adding postfix ?", "author": "godfreyhe", "createdAt": "2020-10-17T09:53:50Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/TableSchemaUtils.java", "diffHunk": "@@ -75,10 +79,46 @@ public static TableSchema projectSchema(TableSchema tableSchema, int[][] project\n \t\tcheckArgument(containsPhysicalColumnsOnly(tableSchema), \"Projection is only supported for physical columns.\");\n \t\tTableSchema.Builder schemaBuilder = TableSchema.builder();\n \t\tList<TableColumn> tableColumns = tableSchema.getTableColumns();\n+\t\tMap<String, String> nameDomain = new HashMap<>();\n+\t\tString exceptionTemplate = \"Get name conflicts for origin fields %s and %s with new name `%s`. \" +\n+\t\t\t\t\"When pushing projection into scan, we will concatenate top level names with delimiter '_'. \" +\n+\t\t\t\t\"Please rename the origin field names when creating table.\";\n+\t\tString originFullyQualifiedName;\n+\t\tString newName;\n \t\tfor (int[] fieldPath : projectedFields) {\n-\t\t\tcheckArgument(fieldPath.length == 1, \"Nested projection push down is not supported yet.\");\n-\t\t\tTableColumn column = tableColumns.get(fieldPath[0]);\n-\t\t\tschemaBuilder.field(column.getName(), column.getType());\n+\t\t\tif (fieldPath.length == 1) {\n+\t\t\t\tTableColumn column = tableColumns.get(fieldPath[0]);\n+\t\t\t\tnewName = column.getName();\n+\t\t\t\toriginFullyQualifiedName = String.format(\"`%s`\", column.getName());\n+\t\t\t\tif (nameDomain.containsKey(column.getName())) {\n+\t\t\t\t\tthrow new TableException(", "originalCommit": "d1b917bb33f837bf04e8351affe7e108911e79b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjkxOTYyNQ==", "url": "https://github.com/apache/flink/pull/13631#discussion_r506919625", "bodyText": "how about keep the original test, and add a new test case the verify the case which field name contain dot", "author": "godfreyhe", "createdAt": "2020-10-17T09:59:09Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java", "diffHunk": "@@ -70,41 +69,40 @@ public void setup() {\n \t\t\t\t\t\t\" 'bounded' = 'true'\\n\" +\n \t\t\t\t\t\t\")\";\n \t\tutil().tableEnv().executeSql(ddl2);\n-\t}\n-\n-\t@Override\n-\tpublic void testNestedProject() {\n-\t\texpectedException().expect(TableException.class);\n-\t\texpectedException().expectMessage(\"Nested projection push down is unsupported now.\");\n-\t\ttestNestedProject(true);\n-\t}\n \n-\t@Test\n-\tpublic void testNestedProjectDisabled() {\n-\t\ttestNestedProject(false);\n-\t}\n-\n-\tprivate void testNestedProject(boolean nestedProjectionSupported) {\n-\t\tString ddl =\n+\t\tString ddl3 =\n \t\t\t\t\"CREATE TABLE NestedTable (\\n\" +\n \t\t\t\t\t\t\"  id int,\\n\" +\n-\t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, nested2 row<num int, flag boolean>>,\\n\" +\n-\t\t\t\t\t\t\"  nested row<name string, `value` int>,\\n\" +\n+\t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, `nested2.` row<num int, flag boolean>>,\\n\" +\n+\t\t\t\t\t\t\"  nested row<name string, `value.` int>,\\n\" +\n \t\t\t\t\t\t\"  name string\\n\" +\n \t\t\t\t\t\t\") WITH (\\n\" +\n \t\t\t\t\t\t\" 'connector' = 'values',\\n\" +\n-\t\t\t\t\t\t\" 'nested-projection-supported' = '\" + nestedProjectionSupported + \"',\\n\" +\n+\t\t\t\t\t\t\" 'nested-projection-supported' = 'true',\" +\n \t\t\t\t\t\t\"  'bounded' = 'true'\\n\" +\n \t\t\t\t\t\t\")\";\n-\t\tutil().tableEnv().executeSql(ddl);\n+\t\tutil().tableEnv().executeSql(ddl3);\n+\t}\n \n+\t@Override\n+\t@Test\n+\tpublic void testNestedProject() {\n \t\tString sqlQuery = \"SELECT id,\\n\" +\n \t\t\t\t\"    deepNested.nested1.name AS nestedName,\\n\" +\n-\t\t\t\t\"    nested.`value` AS nestedValue,\\n\" +\n-\t\t\t\t\"    deepNested.nested2.flag AS nestedFlag,\\n\" +\n-\t\t\t\t\"    deepNested.nested2.num AS nestedNum\\n\" +\n+\t\t\t\t\"    nested.`value.` AS nestedValue,\\n\" +\n+\t\t\t\t\"    deepNested.`nested2.`.flag AS nestedFlag,\\n\" +", "originalCommit": "d1b917bb33f837bf04e8351affe7e108911e79b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a7fe7dec5d7c869ccbc644bba45df241dd05e953", "url": "https://github.com/apache/flink/commit/a7fe7dec5d7c869ccbc644bba45df241dd05e953", "message": "1. use postfix \"$%d\" to resolve the name conflicts\n2. rewrite the rule to support metadata push down:\n2.1 we will check the source and extract the physical part of the schema. If the source supports nested projection push down, we use `RexNodeExtractor.extractRefNestedInputFields` to extract data else we add the physical part info into the coordinates info.\n2.2 If the source supports metadata push down, we add the metadata info into the coordinates.\n2.3 with the final coordinates, we write the projection.", "committedDate": "2020-10-26T02:44:09Z", "type": "forcePushed"}, {"oid": "1697a20356799dd5e2f8863110de1f4626664f12", "url": "https://github.com/apache/flink/commit/1697a20356799dd5e2f8863110de1f4626664f12", "message": "fix test", "committedDate": "2020-10-28T13:40:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIwNDU2MA==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514204560", "bodyText": "nit: how about adding  _ before \"$\"", "author": "godfreyhe", "createdAt": "2020-10-29T12:00:49Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/DataTypeUtils.java", "diffHunk": "@@ -74,42 +76,46 @@\n \t *\n \t * <p>Note: Index paths allow for arbitrary deep nesting. For example, {@code [[0, 2, 1], ...]}\n \t * specifies to include the 2nd field of the 3rd field of the 1st field in the top-level row.\n+\t * Sometimes, it may get name conflicts when extract fields from the row field. Considering the\n+\t * the path is unique to extract fields, it makes sense to use the path to the fields with\n+\t * delimiter `_` as the new name of the field. For example, the new name of the field `b` in\n+\t * the row `a` is `a_b` rather than `b`. But it may still gets name conflicts in some situation,\n+\t * such as the field `a_b` in the top level schema. In such situation, it will use the postfix\n+\t * in the format '$%d' to resolve the name conflicts.\n \t */\n \tpublic static DataType projectRow(DataType dataType, int[][] indexPaths) {\n \t\tfinal List<RowField> updatedFields = new ArrayList<>();\n \t\tfinal List<DataType> updatedChildren = new ArrayList<>();\n+\t\tSet<String> nameDomain = new HashSet<>();\n+\t\tint duplicateCount = 0;\n \t\tfor (int[] indexPath : indexPaths) {\n-\t\t\tupdatedFields.add(selectChild(dataType.getLogicalType(), indexPath, 0));\n-\t\t\tupdatedChildren.add(selectChild(dataType, indexPath, 0));\n+\t\t\tDataType fieldType = dataType.getChildren().get(indexPath[0]);\n+\t\t\tLogicalType fieldLogicalType = fieldType.getLogicalType();\n+\t\t\tStringBuilder builder =\n+\t\t\t\t\tnew StringBuilder(((RowType) dataType.getLogicalType()).getFieldNames().get(indexPath[0]));\n+\t\t\tfor (int index = 1; index < indexPath.length; index++) {\n+\t\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\thasRoot(fieldLogicalType, LogicalTypeRoot.ROW),\n+\t\t\t\t\t\t\"Row data type expected.\");\n+\t\t\t\tRowType rowtype = ((RowType) fieldLogicalType);\n+\t\t\t\tbuilder.append(\"_\").append(rowtype.getFieldNames().get(indexPath[index]));\n+\t\t\t\tfieldLogicalType = rowtype.getFields().get(indexPath[index]).getType();\n+\t\t\t\tfieldType = fieldType.getChildren().get(indexPath[index]);\n+\t\t\t}\n+\t\t\tString path = builder.toString();\n+\t\t\twhile (nameDomain.contains(path)) {\n+\t\t\t\tpath = builder.append(\"$\").append(duplicateCount++).toString();", "originalCommit": "1697a20356799dd5e2f8863110de1f4626664f12", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIyMDQ3OQ==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514220479", "bodyText": "what if the metadata columns have nested fields ?", "author": "godfreyhe", "createdAt": "2020-10-29T12:30:03Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -109,73 +112,67 @@ public void onMatch(RelOptRuleCall call) {\n \t\t\tusedFields = refFields;\n \t\t}\n \t\t// if no fields can be projected, we keep the original plan.\n-\t\tif (usedFields.length == fieldCount) {\n+\t\tif (!supportsNestedProjection && usedFields.length == fieldCount) {\n \t\t\treturn;\n \t\t}\n \n-\t\tfinal List<String> projectedFieldNames = IntStream.of(usedFields)\n-\t\t\t.mapToObj(fieldNames::get)\n-\t\t\t.collect(Collectors.toList());\n-\n \t\tfinal TableSchema oldSchema = oldTableSourceTable.catalogTable().getSchema();\n \t\tfinal DynamicTableSource oldSource = oldTableSourceTable.tableSource();\n \t\tfinal List<String> metadataKeys = DynamicSourceUtils.createRequiredMetadataKeys(oldSchema, oldSource);\n \t\tfinal int physicalFieldCount = fieldCount - metadataKeys.size();\n \t\tfinal DynamicTableSource newSource = oldSource.copy();\n \n-\t\t// remove metadata columns from the projection push down and store it in a separate list\n-\t\t// the projection push down itself happens purely on physical columns\n-\t\tfinal int[] usedPhysicalFields;\n-\t\tfinal List<String> usedMetadataKeys;\n-\t\tif (newSource instanceof SupportsReadingMetadata) {\n-\t\t\tusedPhysicalFields = IntStream.of(usedFields)\n-\t\t\t\t// select only physical columns\n-\t\t\t\t.filter(i -> i < physicalFieldCount)\n-\t\t\t\t.toArray();\n-\t\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n-\t\t\t\t// select only metadata columns\n-\t\t\t\t.filter(i -> i >= physicalFieldCount)\n-\t\t\t\t// map the indices to keys\n-\t\t\t\t.mapToObj(i -> metadataKeys.get(fieldCount - i - 1))\n-\t\t\t\t.collect(Collectors.toList());\n-\t\t\t// order the keys according to the source's declaration\n-\t\t\tusedMetadataKeys = metadataKeys\n-\t\t\t\t.stream()\n-\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n-\t\t\t\t.collect(Collectors.toList());\n+\t\tfinal List<List<Integer>> usedFieldsCoordinates = new ArrayList<>();\n+\t\tfinal Map<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder = new HashMap<>();\n+\n+\t\tif (supportsNestedProjection) {\n+\t\t\tgetCoordinatesAndMappingOfPhysicalColumnWithNestedProjection(\n+\t\t\t\t\tproject, oldSchema, usedFields, physicalFieldCount, usedFieldsCoordinates, fieldCoordinatesToOrder);\n \t\t} else {\n-\t\t\tusedPhysicalFields = usedFields;\n-\t\t\tusedMetadataKeys = Collections.emptyList();\n+\t\t\tfor (int usedField : usedFields) {\n+\t\t\t\t// filter metadata columns", "originalCommit": "1697a20356799dd5e2f8863110de1f4626664f12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg4NzkzNg==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514887936", "bodyText": "add TODO.\nHere we only project the top level of the fields.", "author": "fsk119", "createdAt": "2020-10-30T06:03:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIyMDQ3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIyNDc3OQ==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514224779", "bodyText": "the method name is too long, change to getExpandedFieldsAndOrderMapping ? add some comments the explain the arguments", "author": "godfreyhe", "createdAt": "2020-10-29T12:37:15Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -190,35 +187,82 @@ public void onMatch(RelOptRuleCall call) {\n \t\t}\n \t}\n \n-\tprivate void applyUpdatedMetadata(\n-\t\t\tDynamicTableSource oldSource,\n-\t\t\tTableSchema oldSchema,\n+\tprivate DataType applyUpdateMetadataAndGetNewDataType(\n \t\t\tDynamicTableSource newSource,\n+\t\t\tDataType producedDataType,\n \t\t\tList<String> metadataKeys,\n-\t\t\tList<String> usedMetadataKeys,\n+\t\t\tint[] usedFields,\n \t\t\tint physicalFieldCount,\n-\t\t\tint[][] projectedPhysicalFields) {\n-\t\tif (newSource instanceof SupportsReadingMetadata) {\n-\t\t\tfinal DataType producedDataType = TypeConversions.fromLogicalToDataType(\n-\t\t\t\tDynamicSourceUtils.createProducedType(oldSchema, oldSource));\n+\t\t\tList<List<Integer>> usedFieldsCoordinates,\n+\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n+\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n+\t\t\t\t// select only metadata columns\n+\t\t\t\t.filter(i -> i >= physicalFieldCount)\n+\t\t\t\t// map the indices to keys\n+\t\t\t\t.mapToObj(i -> metadataKeys.get(i - physicalFieldCount))\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t// order the keys according to the source's declaration\n+\t\tfinal List<String> usedMetadataKeys = metadataKeys\n+\t\t\t\t.stream()\n+\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n+\t\t\t\t.collect(Collectors.toList());\n \n-\t\t\tfinal int[][] projectedMetadataFields = usedMetadataKeys\n+\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n \t\t\t\t.stream()\n \t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> new int[]{ physicalFieldCount + i })\n-\t\t\t\t.toArray(int[][]::new);\n+\t\t\t\t.map(i -> {\n+\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n+\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tusedFieldsCoordinates.addAll(projectedMetadataFields);\n \n-\t\t\tfinal int[][] projectedFields = Stream\n-\t\t\t\t.concat(\n-\t\t\t\t\tStream.of(projectedPhysicalFields),\n-\t\t\t\t\tStream.of(projectedMetadataFields)\n-\t\t\t\t)\n+\t\tint[][] allFields = usedFieldsCoordinates\n+\t\t\t\t.stream()\n+\t\t\t\t.map(coordinates -> coordinates.stream().mapToInt(i -> i).toArray())\n \t\t\t\t.toArray(int[][]::new);\n \n-\t\t\t// create a new, final data type that includes all projections\n-\t\t\tfinal DataType newProducedDataType = DataTypeUtils.projectRow(producedDataType, projectedFields);\n+\t\tDataType newProducedDataType = DataTypeUtils.projectRow(producedDataType, allFields);\n \n-\t\t\t((SupportsReadingMetadata) newSource).applyReadableMetadata(usedMetadataKeys, newProducedDataType);\n+\t\t((SupportsReadingMetadata) newSource).applyReadableMetadata(usedMetadataKeys, newProducedDataType);\n+\t\treturn newProducedDataType;\n+\t}\n+\n+\tprivate void getCoordinatesAndMappingOfPhysicalColumnWithNestedProjection(", "originalCommit": "1697a20356799dd5e2f8863110de1f4626664f12", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIyOTcxNg==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514229716", "bodyText": "nit: It is better to close the position of the defined field to the position in which the field is used.", "author": "godfreyhe", "createdAt": "2020-10-29T12:45:34Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -74,28 +79,26 @@ public boolean matches(RelOptRuleCall call) {\n \t\tif (tableSourceTable == null || !(tableSourceTable.tableSource() instanceof SupportsProjectionPushDown)) {\n \t\t\treturn false;\n \t\t}\n-\t\tSupportsProjectionPushDown pushDownSource = (SupportsProjectionPushDown) tableSourceTable.tableSource();\n-\t\tif (pushDownSource.supportsNestedProjection()) {\n-\t\t\tthrow new TableException(\"Nested projection push down is unsupported now. \\n\" +\n-\t\t\t\t\t\"Please disable nested projection (SupportsProjectionPushDown#supportsNestedProjection returns false), \" +\n-\t\t\t\t\t\"planner will push down the top-level columns.\");\n-\t\t} else {\n-\t\t\treturn true;\n-\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"project=[\"));\n \t}\n \n \t@Override\n \tpublic void onMatch(RelOptRuleCall call) {\n \t\tfinal LogicalProject project = call.rel(0);\n \t\tfinal LogicalTableScan scan = call.rel(1);\n \n+\t\tTableSourceTable oldTableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\tfinal boolean supportsNestedProjection =\n+\t\t\t\t((SupportsProjectionPushDown) oldTableSourceTable.tableSource()).supportsNestedProjection();\n+\t\tfinal boolean supportsReadingMetaData = oldTableSourceTable.tableSource() instanceof SupportsReadingMetadata;", "originalCommit": "1697a20356799dd5e2f8863110de1f4626664f12", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIzMjU1Mw==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514232553", "bodyText": "use for to make it clearer", "author": "godfreyhe", "createdAt": "2020-10-29T12:50:16Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -190,35 +187,82 @@ public void onMatch(RelOptRuleCall call) {\n \t\t}\n \t}\n \n-\tprivate void applyUpdatedMetadata(\n-\t\t\tDynamicTableSource oldSource,\n-\t\t\tTableSchema oldSchema,\n+\tprivate DataType applyUpdateMetadataAndGetNewDataType(\n \t\t\tDynamicTableSource newSource,\n+\t\t\tDataType producedDataType,\n \t\t\tList<String> metadataKeys,\n-\t\t\tList<String> usedMetadataKeys,\n+\t\t\tint[] usedFields,\n \t\t\tint physicalFieldCount,\n-\t\t\tint[][] projectedPhysicalFields) {\n-\t\tif (newSource instanceof SupportsReadingMetadata) {\n-\t\t\tfinal DataType producedDataType = TypeConversions.fromLogicalToDataType(\n-\t\t\t\tDynamicSourceUtils.createProducedType(oldSchema, oldSource));\n+\t\t\tList<List<Integer>> usedFieldsCoordinates,\n+\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n+\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n+\t\t\t\t// select only metadata columns\n+\t\t\t\t.filter(i -> i >= physicalFieldCount)\n+\t\t\t\t// map the indices to keys\n+\t\t\t\t.mapToObj(i -> metadataKeys.get(i - physicalFieldCount))\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t// order the keys according to the source's declaration\n+\t\tfinal List<String> usedMetadataKeys = metadataKeys\n+\t\t\t\t.stream()\n+\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n+\t\t\t\t.collect(Collectors.toList());\n \n-\t\t\tfinal int[][] projectedMetadataFields = usedMetadataKeys\n+\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n \t\t\t\t.stream()\n \t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> new int[]{ physicalFieldCount + i })\n-\t\t\t\t.toArray(int[][]::new);\n+\t\t\t\t.map(i -> {\n+\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n+\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tusedFieldsCoordinates.addAll(projectedMetadataFields);", "originalCommit": "1697a20356799dd5e2f8863110de1f4626664f12", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIzMzM5NA==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514233394", "bodyText": "allFields -> projectedFields", "author": "godfreyhe", "createdAt": "2020-10-29T12:51:37Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -190,35 +187,82 @@ public void onMatch(RelOptRuleCall call) {\n \t\t}\n \t}\n \n-\tprivate void applyUpdatedMetadata(\n-\t\t\tDynamicTableSource oldSource,\n-\t\t\tTableSchema oldSchema,\n+\tprivate DataType applyUpdateMetadataAndGetNewDataType(\n \t\t\tDynamicTableSource newSource,\n+\t\t\tDataType producedDataType,\n \t\t\tList<String> metadataKeys,\n-\t\t\tList<String> usedMetadataKeys,\n+\t\t\tint[] usedFields,\n \t\t\tint physicalFieldCount,\n-\t\t\tint[][] projectedPhysicalFields) {\n-\t\tif (newSource instanceof SupportsReadingMetadata) {\n-\t\t\tfinal DataType producedDataType = TypeConversions.fromLogicalToDataType(\n-\t\t\t\tDynamicSourceUtils.createProducedType(oldSchema, oldSource));\n+\t\t\tList<List<Integer>> usedFieldsCoordinates,\n+\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n+\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n+\t\t\t\t// select only metadata columns\n+\t\t\t\t.filter(i -> i >= physicalFieldCount)\n+\t\t\t\t// map the indices to keys\n+\t\t\t\t.mapToObj(i -> metadataKeys.get(i - physicalFieldCount))\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t// order the keys according to the source's declaration\n+\t\tfinal List<String> usedMetadataKeys = metadataKeys\n+\t\t\t\t.stream()\n+\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n+\t\t\t\t.collect(Collectors.toList());\n \n-\t\t\tfinal int[][] projectedMetadataFields = usedMetadataKeys\n+\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n \t\t\t\t.stream()\n \t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> new int[]{ physicalFieldCount + i })\n-\t\t\t\t.toArray(int[][]::new);\n+\t\t\t\t.map(i -> {\n+\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n+\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tusedFieldsCoordinates.addAll(projectedMetadataFields);\n \n-\t\t\tfinal int[][] projectedFields = Stream\n-\t\t\t\t.concat(\n-\t\t\t\t\tStream.of(projectedPhysicalFields),\n-\t\t\t\t\tStream.of(projectedMetadataFields)\n-\t\t\t\t)\n+\t\tint[][] allFields = usedFieldsCoordinates", "originalCommit": "1697a20356799dd5e2f8863110de1f4626664f12", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwNzMzOQ==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514907339", "bodyText": "This line is unnecessary, use usedFieldsCoordinates.add(Collections.singletonList(physicalFieldCount + index)) at line 217", "author": "godfreyhe", "createdAt": "2020-10-30T07:11:30Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -207,27 +208,35 @@ private DataType applyUpdateMetadataAndGetNewDataType(\n \t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n \t\t\t\t.collect(Collectors.toList());\n \n-\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n-\t\t\t\t.stream()\n-\t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> {\n-\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n-\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n-\t\t\t\t.collect(Collectors.toList());\n+\t\tfinal List<List<Integer>> projectedMetadataFields = new ArrayList<>(usedMetadataKeys.size());", "originalCommit": "a5194ee25cf0c08ecd1d1e484b3f2e335ffb7656", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk1MjUxNw==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514952517", "bodyText": "MyTable does not support nested-projection-supported", "author": "godfreyhe", "createdAt": "2020-10-30T08:57:52Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java", "diffHunk": "@@ -70,34 +70,47 @@ public void setup() {\n \t\t\t\t\t\t\" 'bounded' = 'true'\\n\" +\n \t\t\t\t\t\t\")\";\n \t\tutil().tableEnv().executeSql(ddl2);\n-\t}\n-\n-\t@Override\n-\tpublic void testNestedProject() {\n-\t\texpectedException().expect(TableException.class);\n-\t\texpectedException().expectMessage(\"Nested projection push down is unsupported now.\");\n-\t\ttestNestedProject(true);\n-\t}\n-\n-\t@Test\n-\tpublic void testNestedProjectDisabled() {\n-\t\ttestNestedProject(false);\n-\t}\n \n-\tprivate void testNestedProject(boolean nestedProjectionSupported) {\n-\t\tString ddl =\n+\t\tString ddl3 =\n \t\t\t\t\"CREATE TABLE NestedTable (\\n\" +\n \t\t\t\t\t\t\"  id int,\\n\" +\n \t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, nested2 row<num int, flag boolean>>,\\n\" +\n \t\t\t\t\t\t\"  nested row<name string, `value` int>,\\n\" +\n+\t\t\t\t\t\t\"  `deepNestedWith.` row<`.value` int, nested row<name string, `.value` int>>,\\n\" +\n \t\t\t\t\t\t\"  name string\\n\" +\n \t\t\t\t\t\t\") WITH (\\n\" +\n \t\t\t\t\t\t\" 'connector' = 'values',\\n\" +\n-\t\t\t\t\t\t\" 'nested-projection-supported' = '\" + nestedProjectionSupported + \"',\\n\" +\n-\t\t\t\t\t\t\"  'bounded' = 'true'\\n\" +\n+\t\t\t\t\t\t\" 'nested-projection-supported' = 'true',\" +\n+\t\t\t\t\t\t\" 'bounded' = 'true'\\n\" +\n+\t\t\t\t\t\t\")\";\n+\t\tutil().tableEnv().executeSql(ddl3);\n+\n+\t\tString ddl4 =\n+\t\t\t\t\"CREATE TABLE MetadataTable(\\n\" +\n+\t\t\t\t\t\t\"  id int,\\n\" +\n+\t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, nested2 row<num int, flag boolean>>,\\n\" +\n+\t\t\t\t\t\t\"  metadata_1 int metadata,\\n\" +\n+\t\t\t\t\t\t\"  metadata_2 string metadata\\n\" +\n+\t\t\t\t\t\t\") WITH (\" +\n+\t\t\t\t\t\t\" 'connector' = 'values',\" +\n+\t\t\t\t\t\t\" 'nested-projection-supported' = 'true',\" +\n+\t\t\t\t\t\t\" 'bounded' = 'true',\\n\" +\n+\t\t\t\t\t\t\" 'readable-metadata' = 'metadata_1:INT, metadata_2:STRING, metadata_3:BIGINT'\" +\n \t\t\t\t\t\t\")\";\n-\t\tutil().tableEnv().executeSql(ddl);\n+\t\tutil().tableEnv().executeSql(ddl4);\n+\t}\n \n+\t@Test\n+\tpublic void testProjectWithMapType() {\n+\t\tString sqlQuery =\n+\t\t\t\t\"SELECT a, d['e']\\n\" +\n+\t\t\t\t\t\t\"FROM MyTable\";", "originalCommit": "9898b59d09c7483e3e0c034372a4875c46844841", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "cb980798a022a35131fccd5c536a5f5519f38cba", "url": "https://github.com/apache/flink/commit/cb980798a022a35131fccd5c536a5f5519f38cba", "message": "1. introduce the new extractor and rewriter:\n2. fix failed test: the order of fileds in the new schema is determined by the hashmap rather that the order in the projections.", "committedDate": "2020-11-01T02:14:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1NjM4Mg==", "url": "https://github.com/apache/flink/pull/13631#discussion_r515656382", "bodyText": "which plans are affected?", "author": "godfreyhe", "createdAt": "2020-11-01T18:40:30Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -74,108 +76,114 @@ public boolean matches(RelOptRuleCall call) {\n \t\tif (tableSourceTable == null || !(tableSourceTable.tableSource() instanceof SupportsProjectionPushDown)) {\n \t\t\treturn false;\n \t\t}\n-\t\tSupportsProjectionPushDown pushDownSource = (SupportsProjectionPushDown) tableSourceTable.tableSource();\n-\t\tif (pushDownSource.supportsNestedProjection()) {\n-\t\t\tthrow new TableException(\"Nested projection push down is unsupported now. \\n\" +\n-\t\t\t\t\t\"Please disable nested projection (SupportsProjectionPushDown#supportsNestedProjection returns false), \" +\n-\t\t\t\t\t\"planner will push down the top-level columns.\");\n-\t\t} else {\n-\t\t\treturn true;\n-\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"project=[\"));\n \t}\n \n \t@Override\n \tpublic void onMatch(RelOptRuleCall call) {\n \t\tfinal LogicalProject project = call.rel(0);\n \t\tfinal LogicalTableScan scan = call.rel(1);\n \n-\t\tfinal List<String> fieldNames = scan.getRowType().getFieldNames();\n-\t\tfinal int fieldCount = fieldNames.size();\n-\n \t\tfinal int[] refFields = RexNodeExtractor.extractRefInputFields(project.getProjects());\n-\t\tfinal int[] usedFields;\n-\n \t\tTableSourceTable oldTableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n-\t\tif (isUpsertSource(oldTableSourceTable)) {\n-\t\t\t// primary key fields are needed for upsert source\n-\t\t\tList<String> keyFields = oldTableSourceTable.catalogTable().getSchema()\n-\t\t\t\t.getPrimaryKey().get().getColumns();\n-\t\t\t// we should get source fields from scan node instead of CatalogTable,\n-\t\t\t// because projection may have been pushed down\n-\t\t\tList<String> sourceFields = scan.getRowType().getFieldNames();\n-\t\t\tint[] primaryKey = ScanUtil.getPrimaryKeyIndices(sourceFields, keyFields);\n-\t\t\tusedFields = mergeFields(refFields, primaryKey);\n-\t\t} else {\n-\t\t\tusedFields = refFields;\n-\t\t}\n-\t\t// if no fields can be projected, we keep the original plan.\n-\t\tif (usedFields.length == fieldCount) {\n+\t\tfinal TableSchema oldSchema = oldTableSourceTable.catalogTable().getSchema();\n+\t\tfinal DynamicTableSource oldSource = oldTableSourceTable.tableSource();\n+\n+\t\tfinal boolean supportsNestedProjection =\n+\t\t\t\t((SupportsProjectionPushDown) oldTableSourceTable.tableSource()).supportsNestedProjection();\n+\t\tList<String> fieldNames = scan.getRowType().getFieldNames();\n+\n+\t\tif (!supportsNestedProjection && refFields.length == fieldNames.size()) {\n+\t\t\t// just keep as same as the old plan\n+\t\t\t// TODO: refactor the affected plan", "originalCommit": "aa094733a6678ec72712da928dd457e79f3f81a9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTcyNDcxOA==", "url": "https://github.com/apache/flink/pull/13631#discussion_r515724718", "bodyText": "The main problem is we will add digest in this situation. Maybe we can regard as an improvement for furture.", "author": "fsk119", "createdAt": "2020-11-02T03:33:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1NjM4Mg=="}], "type": "inlineReview"}, {"oid": "34b48e349af82ba98ca546d4d3c8798b0521a1e7", "url": "https://github.com/apache/flink/commit/34b48e349af82ba98ca546d4d3c8798b0521a1e7", "message": "[FLINK-19639][table sql/planner]Support SupportsNestedProjectionPushDown in planner", "committedDate": "2020-11-03T05:43:58Z", "type": "commit"}, {"oid": "1a6af35c7056199c7f925687e7cff8dc525a1071", "url": "https://github.com/apache/flink/commit/1a6af35c7056199c7f925687e7cff8dc525a1071", "message": "fix godfrey's comment:\n1. use qualified name list to get the projectedFields and build new projections;\n2. add more tests", "committedDate": "2020-11-03T05:43:58Z", "type": "commit"}, {"oid": "679a5eb6a3e649aa2dceadef8a4e6c9455b91983", "url": "https://github.com/apache/flink/commit/679a5eb6a3e649aa2dceadef8a4e6c9455b91983", "message": "minor fix", "committedDate": "2020-11-03T05:43:58Z", "type": "commit"}, {"oid": "c624950f08de80ee7dc1e0c23d85fc832f779ad2", "url": "https://github.com/apache/flink/commit/c624950f08de80ee7dc1e0c23d85fc832f779ad2", "message": "fix godfrey's comment:\n1. use qualified name as the projected column name\n2. fix suggestions", "committedDate": "2020-11-03T05:43:58Z", "type": "commit"}, {"oid": "62edbb5ff58d100105d1bb1f85e44ef11d04cf05", "url": "https://github.com/apache/flink/commit/62edbb5ff58d100105d1bb1f85e44ef11d04cf05", "message": "use postfix to solve name conflicts.", "committedDate": "2020-11-03T05:43:58Z", "type": "commit"}, {"oid": "7ac21c27e3ba812bc0fc73941a494638d8cc6e80", "url": "https://github.com/apache/flink/commit/7ac21c27e3ba812bc0fc73941a494638d8cc6e80", "message": "1. use postfix \"$%d\" to resolve the name conflicts\n2. rewrite the rule to support metadata push down:\n2.1 we will check the source and extract the physical part of the schema. If the source supports nested projection push down, we use `RexNodeExtractor.extractRefNestedInputFields` to extract data else we add the physical part info into the coordinates info.\n2.2 If the source supports metadata push down, we add the metadata info into the coordinates.\n2.3 with the final coordinates, we write the projection.", "committedDate": "2020-11-03T05:48:43Z", "type": "commit"}, {"oid": "2e8ef24b27c38b03f0e6917972a4ab3f14947f04", "url": "https://github.com/apache/flink/commit/2e8ef24b27c38b03f0e6917972a4ab3f14947f04", "message": "fix line too long", "committedDate": "2020-11-03T05:48:47Z", "type": "commit"}, {"oid": "503bf618e2ea4cb46c1b929f6e5c1bbdcc5ab170", "url": "https://github.com/apache/flink/commit/503bf618e2ea4cb46c1b929f6e5c1bbdcc5ab170", "message": "fix test", "committedDate": "2020-11-03T05:48:47Z", "type": "commit"}, {"oid": "1c148a77f2f5b68a80dc7e7ecb9a6014d9e760ea", "url": "https://github.com/apache/flink/commit/1c148a77f2f5b68a80dc7e7ecb9a6014d9e760ea", "message": "address feedback:\n1. rename the func name and add comments;\n2. add test: projection push down with map type;\n3. other minor fix;", "committedDate": "2020-11-03T05:48:47Z", "type": "commit"}, {"oid": "e24f5d693ec0ad3f687fcb622d64ec9f78d20f11", "url": "https://github.com/apache/flink/commit/e24f5d693ec0ad3f687fcb622d64ec9f78d20f11", "message": "delete unused method", "committedDate": "2020-11-03T05:48:47Z", "type": "commit"}, {"oid": "9515a90a341fdb1173d20025f6ad2915364489b6", "url": "https://github.com/apache/flink/commit/9515a90a341fdb1173d20025f6ad2915364489b6", "message": "1. introduce the new extractor and rewriter:\n2. fix failed test: the order of fileds in the new schema is determined by the hashmap rather that the order in the projections.", "committedDate": "2020-11-03T05:48:48Z", "type": "commit"}, {"oid": "40d2a12d81458dc803fe9cfd6eba45d0f3098a8a", "url": "https://github.com/apache/flink/commit/40d2a12d81458dc803fe9cfd6eba45d0f3098a8a", "message": "fix failed test", "committedDate": "2020-11-03T05:48:48Z", "type": "commit"}, {"oid": "ad207e1b987cee8de52adac5e3dd2c8eee96d185", "url": "https://github.com/apache/flink/commit/ad207e1b987cee8de52adac5e3dd2c8eee96d185", "message": "use LinkedHashMap to reduce the cost of the reorder and roll back the modification of the test.", "committedDate": "2020-11-03T05:48:48Z", "type": "commit"}, {"oid": "421d5bb5e7f784a38064941dc2bc3fdc6b1f2ed7", "url": "https://github.com/apache/flink/commit/421d5bb5e7f784a38064941dc2bc3fdc6b1f2ed7", "message": "fix test", "committedDate": "2020-11-03T05:48:48Z", "type": "commit"}, {"oid": "95f0efef8a8dbbd0caf5e90cce810b448fbe5c74", "url": "https://github.com/apache/flink/commit/95f0efef8a8dbbd0caf5e90cce810b448fbe5c74", "message": "fix test and address the feedbacks.", "committedDate": "2020-11-03T05:48:48Z", "type": "commit"}, {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b", "url": "https://github.com/apache/flink/commit/0707f7489676ee1a7e709479e47c6a3da5dd0e8b", "message": "address godfrey's comment:\n1. add RexNodeNestedFields that works as tableschema\n2. fix failed test and some small problems", "committedDate": "2020-11-03T05:51:50Z", "type": "commit"}, {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b", "url": "https://github.com/apache/flink/commit/0707f7489676ee1a7e709479e47c6a3da5dd0e8b", "message": "address godfrey's comment:\n1. add RexNodeNestedFields that works as tableschema\n2. fix failed test and some small problems", "committedDate": "2020-11-03T05:51:50Z", "type": "forcePushed"}, {"oid": "4d844d385b7a556ee1f3f53b36b0b1211136e5b3", "url": "https://github.com/apache/flink/commit/4d844d385b7a556ee1f3f53b36b0b1211136e5b3", "message": "address feedback: refactor the NestedSchema && NestedColumn", "committedDate": "2020-11-03T12:19:24Z", "type": "commit"}, {"oid": "4bc536663a8a89fee69b58494c76b8e6f5e98c25", "url": "https://github.com/apache/flink/commit/4bc536663a8a89fee69b58494c76b8e6f5e98c25", "message": "minor fix", "committedDate": "2020-11-03T12:25:04Z", "type": "commit"}, {"oid": "95b1f48e2fa7c49f8193ef35738787e01bdf2fb5", "url": "https://github.com/apache/flink/commit/95b1f48e2fa7c49f8193ef35738787e01bdf2fb5", "message": "minor fix", "committedDate": "2020-11-03T13:18:01Z", "type": "commit"}, {"oid": "5a3617336c68ff9311adf5d93a07c9eb5ff39dbf", "url": "https://github.com/apache/flink/commit/5a3617336c68ff9311adf5d93a07c9eb5ff39dbf", "message": "minor fix", "committedDate": "2020-11-03T13:43:49Z", "type": "commit"}, {"oid": "bb84157d91d0f564a199c6af00209db8f51a7dda", "url": "https://github.com/apache/flink/commit/bb84157d91d0f564a199c6af00209db8f51a7dda", "message": "delete JLong", "committedDate": "2020-11-03T13:53:45Z", "type": "commit"}, {"oid": "fd8961181c3838b57b78d28c22029d581031763e", "url": "https://github.com/apache/flink/commit/fd8961181c3838b57b78d28c22029d581031763e", "message": "fix tab problems", "committedDate": "2020-11-03T15:29:11Z", "type": "commit"}, {"oid": "5f9d0abfa8ebf2ca3f29d1eeb69f898a6b4de915", "url": "https://github.com/apache/flink/commit/5f9d0abfa8ebf2ca3f29d1eeb69f898a6b4de915", "message": "rename the file and class", "committedDate": "2020-11-04T02:16:44Z", "type": "commit"}]}