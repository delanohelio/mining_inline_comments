{"pr_number": 11574, "pr_title": "[FLINK-16859][table-runtime] Introduce file system table factory", "pr_createdAt": "2020-03-31T07:23:52Z", "pr_url": "https://github.com/apache/flink/pull/11574", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNDQ2MA==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402034460", "bodyText": "using two optional interface looks strange\uff0ccould we make them to one like createWriter(WriterContext) ?", "author": "leonardBang", "createdAt": "2020-04-02T03:40:03Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.factories.TableFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+/**\n+ * File system format factory for creating configured instances of reader and writer.\n+ */\n+@Internal\n+public interface FileSystemFormatFactory extends TableFormatFactory<BaseRow> {\n+\n+\t/**\n+\t * Create {@link InputFormat} reader.\n+\t */\n+\tInputFormat<BaseRow, ?> createReader(ReaderContext context);\n+\n+\t/**\n+\t * Create {@link Encoder} writer.\n+\t */\n+\tOptional<Encoder<BaseRow>> createEncoder(WriterContext context);\n+\n+\t/**\n+\t * Create {@link BulkWriter.Factory} writer.\n+\t */\n+\tOptional<BulkWriter.Factory<BaseRow>> createBulkWriterFactory(WriterContext context);\n+", "originalCommit": "cd8a10137fa50eb11035082c95878fb6aa6a66c6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA2Mzg2Mg==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402063862", "bodyText": "I have tried to unify them to a method, but return value is hard to define, if I just define a generic T, this should be every object. So I define two methods to formats.", "author": "JingsongLi", "createdAt": "2020-04-02T05:43:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNDQ2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNDc2MA==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402034760", "bodyText": "use lowercase <p>", "author": "leonardBang", "createdAt": "2020-04-02T03:41:18Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:", "originalCommit": "cd8a10137fa50eb11035082c95878fb6aa6a66c6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNTUxOA==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402035518", "bodyText": "statement * 1. The partition ... is not a support but an agreement, we can place in new paragraph.", "author": "leonardBang", "createdAt": "2020-04-02T03:44:37Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:\n+ * 1.The partition information should be in the file system path, whether it's a temporary", "originalCommit": "cd8a10137fa50eb11035082c95878fb6aa6a66c6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA2NDU4Ng==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402064586", "bodyText": "I remove \"File system support\"", "author": "JingsongLi", "createdAt": "2020-04-02T05:46:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNTUxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzODU1OA==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402038558", "bodyText": "Here we change the CONNECTOR_VALUE\u3001PATH different with FileSystemValidator,\n public static final String CONNECTOR_TYPE_VALUE = \"filesystem\"; public static final String CONNECTOR_PATH = \"connector.path\";\nIIUC, you want to avoid conflict with current CsvTableSinkFactoryBase/CsvTableSourceFactoryBase, so if user use  path can route to this factory, use connector.path will route to oldfactory(i.e.CsvTableSinkFactoryBase/CsvTableSourceFactoryBase )\nThis is a new feature and face to user, I think we'd better add docs.", "author": "leonardBang", "createdAt": "2020-04-02T03:58:21Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:\n+ * 1.The partition information should be in the file system path, whether it's a temporary\n+ * table or a catalog table.\n+ * 2.Support insert into (append) and insert overwrite.\n+ * 3.Support static and dynamic partition inserting.\n+ */\n+public class FileSystemTableFactory implements\n+\t\tTableSourceFactory<BaseRow>,\n+\t\tTableSinkFactory<BaseRow> {\n+\n+\tpublic static final String CONNECTOR_VALUE = \"filesystem\";\n+\tpublic static final String PATH = \"path\";\n+", "originalCommit": "cd8a10137fa50eb11035082c95878fb6aa6a66c6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzODg2Nw==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402038867", "bodyText": "Do some basic validate properties here to align with other tableSource/Sink Factory?\nI think we can do validate in FileSystemValidator\uff0c and also move fields CONNECTOR_VALUE\u3001PATH\u3001PARTITION_DEFAULT_NAME to FileSystemValidator`\uff0c maybe this will make the class more clear?", "author": "leonardBang", "createdAt": "2020-04-02T03:59:38Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <P>File system support:\n+ * 1.The partition information should be in the file system path, whether it's a temporary\n+ * table or a catalog table.\n+ * 2.Support insert into (append) and insert overwrite.\n+ * 3.Support static and dynamic partition inserting.\n+ */\n+public class FileSystemTableFactory implements\n+\t\tTableSourceFactory<BaseRow>,\n+\t\tTableSinkFactory<BaseRow> {\n+\n+\tpublic static final String CONNECTOR_VALUE = \"filesystem\";\n+\tpublic static final String PATH = \"path\";\n+\n+\tpublic static final ConfigOption<String> PARTITION_DEFAULT_NAME = key(\"partition.default-name\")\n+\t\t\t.stringType()\n+\t\t\t.defaultValue(\"__DEFAULT_PARTITION__\")\n+\t\t\t.withDescription(\"The default partition name in case the dynamic partition\" +\n+\t\t\t\t\t\" column value is null/empty string\");\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(CONNECTOR, CONNECTOR_VALUE);\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tList<String> properties = new ArrayList<>();\n+\n+\t\t// path\n+\t\tproperties.add(PATH);\n+\n+\t\t// schema\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_DATA_TYPE);\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_NAME);\n+\n+\t\tproperties.add(PARTITION_DEFAULT_NAME.key());\n+\n+\t\t// format\n+\t\tproperties.add(FORMAT);\n+\t\tproperties.add(FORMAT + \".*\");\n+\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic TableSource<BaseRow> createTableSource(TableSourceFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());", "originalCommit": "cd8a10137fa50eb11035082c95878fb6aa6a66c6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA2Nzk5NQ==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402067995", "bodyText": "We can, but not now, after https://issues.apache.org/jira/browse/FLINK-16904 , we can move them to validator.", "author": "JingsongLi", "createdAt": "2020-04-02T05:57:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzODg2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0MjU1Ng==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402042556", "bodyText": "could we rename to BaseRowPartitionComputer before FLIP-95? otherwise the note will not match the name.", "author": "leonardBang", "createdAt": "2020-04-02T04:16:54Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/RowDataPartitionComputer.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.dataformat.TypeGetterSetters;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * {@link PartitionComputer} for {@link BaseRow}.\n+ */\n+@Internal\n+public class RowDataPartitionComputer implements PartitionComputer<BaseRow> {", "originalCommit": "cd8a10137fa50eb11035082c95878fb6aa6a66c6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA2ODUwNw==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402068507", "bodyText": "I don't want to another rename... I think even RowData for now, it can explain the behavior.", "author": "JingsongLi", "createdAt": "2020-04-02T05:59:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0MjU1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0MzMzMA==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402043330", "bodyText": "The value only from  partition column, use Unsupported partition type to make it clear?", "author": "leonardBang", "createdAt": "2020-04-02T04:20:55Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/utils/TestRowDataCsvInputFormat.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.utils;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.io.RowCsvInputFormat;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.BinaryString;\n+import org.apache.flink.table.dataformat.DataFormatConverters;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.filesystem.PartitionPathUtils;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * The {@link InputFormat} that output {@link BaseRow}.\n+ */\n+public class TestRowDataCsvInputFormat extends RichInputFormat<BaseRow, FileInputSplit> {\n+\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String defaultPartValue;\n+\tprivate final int[] selectFields;\n+\tprivate final long limit;\n+\tprivate final RowCsvInputFormat inputFormat;\n+\tprivate final List<TypeInformation> fieldTypes;\n+\tprivate final List<String> fieldNames;\n+\tprivate final List<DataFormatConverters.DataFormatConverter> csvSelectConverters;\n+\tprivate final int[] csvFieldMapping;\n+\n+\tprivate transient Row csvRow;\n+\tprivate transient GenericRow row;\n+\tprivate transient long emitted;\n+\n+\tpublic TestRowDataCsvInputFormat(\n+\t\t\tPath[] paths,\n+\t\t\tTableSchema schema,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartValue,\n+\t\t\tint[] selectFields,\n+\t\t\tlong limit) {\n+\t\tthis.partitionKeys = partitionKeys;\n+\t\tthis.defaultPartValue = defaultPartValue;\n+\t\tthis.selectFields = selectFields;\n+\t\tthis.limit = limit;\n+\t\tRowTypeInfo rowType = (RowTypeInfo) schema.toRowType();\n+\t\tthis.fieldTypes = Arrays.asList(rowType.getFieldTypes());\n+\t\tthis.fieldNames = Arrays.asList(rowType.getFieldNames());\n+\n+\t\tList<String> csvFieldNames = fieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\n+\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvSelectFieldNames = selectFieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\tList<TypeInformation> csvSelectTypes = csvSelectFieldNames.stream()\n+\t\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).collect(Collectors.toList());\n+\t\tthis.csvSelectConverters = csvSelectTypes.stream()\n+\t\t\t\t.map(TypeConversions::fromLegacyInfoToDataType)\n+\t\t\t\t.map(DataFormatConverters::getConverterForDataType)\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tint[] csvSelectFields = csvSelectFieldNames.stream().mapToInt(csvFieldNames::indexOf).toArray();\n+\t\tthis.inputFormat = new RowCsvInputFormat(\n+\t\t\t\tnull, csvSelectTypes.toArray(new TypeInformation[0]), csvSelectFields);\n+\t\tthis.inputFormat.setFilePaths(paths);\n+\n+\t\tthis.csvFieldMapping = csvSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\t\tthis.emitted = 0;\n+\t}\n+\n+\t@Override\n+\tpublic void configure(Configuration parameters) {\n+\t\tinputFormat.configure(parameters);\n+\t}\n+\n+\t@Override\n+\tpublic BaseStatistics getStatistics(BaseStatistics cachedStatistics) throws IOException {\n+\t\treturn inputFormat.getStatistics(cachedStatistics);\n+\t}\n+\n+\t@Override\n+\tpublic FileInputSplit[] createInputSplits(int minNumSplits) throws IOException {\n+\t\treturn inputFormat.createInputSplits(minNumSplits);\n+\t}\n+\n+\t@Override\n+\tpublic InputSplitAssigner getInputSplitAssigner(FileInputSplit[] inputSplits) {\n+\t\treturn inputFormat.getInputSplitAssigner(inputSplits);\n+\t}\n+\n+\t@Override\n+\tpublic void open(FileInputSplit split) throws IOException {\n+\t\tinputFormat.open(split);\n+\t\tPath path = split.getPath();\n+\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(path);\n+\t\tthis.row = new GenericRow(selectFields.length);\n+\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\tint selectField = selectFields[i];\n+\t\t\tString name = fieldNames.get(selectField);\n+\t\t\tif (partitionKeys.contains(name)) {\n+\t\t\t\tString value = partSpec.get(name);\n+\t\t\t\tvalue = defaultPartValue.equals(value) ? null : value;\n+\t\t\t\tthis.row.setField(\n+\t\t\t\t\t\ti, convertStringToInternal(value, fieldTypes.get(selectField)));\n+\t\t\t}\n+\t\t}\n+\t\tthis.csvRow = new Row(csvSelectConverters.size());\n+\t}\n+\n+\tprivate Object convertStringToInternal(String value, TypeInformation type) {\n+\t\tif (type.equals(Types.INT)) {\n+\t\t\treturn Integer.parseInt(value);\n+\t\t} else if (type.equals(Types.LONG)) {\n+\t\t\treturn Long.parseLong(value);\n+\t\t} else if (type.equals(Types.STRING)) {\n+\t\t\treturn BinaryString.fromString(value);\n+\t\t} else {\n+\t\t\tthrow new UnsupportedOperationException(\"Unsupported type: \" + type);", "originalCommit": "cd8a10137fa50eb11035082c95878fb6aa6a66c6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0NDEzNA==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402044134", "bodyText": "Add a serialVersionUID?", "author": "leonardBang", "createdAt": "2020-04-02T04:24:40Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.OutputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FSDataOutputStream;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.OverwritableTableSink;\n+import org.apache.flink.table.sinks.PartitionableTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+/**\n+ * File system {@link TableSink}.\n+ */\n+public class FileSystemTableSink implements\n+\t\tAppendStreamTableSink<BaseRow>,\n+\t\tPartitionableTableSink,\n+\t\tOverwritableTableSink {\n+\n+\tprivate final TableSchema schema;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final Path path;\n+\tprivate final String defaultPartName;\n+\tprivate final FileSystemFormatFactory formatFactory;\n+\n+\tprivate boolean overwrite = false;\n+\tprivate boolean dynamicGrouping = false;\n+\tprivate LinkedHashMap<String, String> staticPartitions = new LinkedHashMap<>();\n+\n+\t/**\n+\t * Construct a file system table sink.\n+\t *\n+\t * @param schema schema of the table.\n+\t * @param path directory path of the file system table.\n+\t * @param partitionKeys partition keys of the table.\n+\t * @param defaultPartName The default partition name in case the dynamic partition column value\n+\t *                        is null/empty string.\n+\t * @param formatFactory format factory to create reader.\n+\t */\n+\tpublic FileSystemTableSink(\n+\t\t\tTableSchema schema,\n+\t\t\tPath path,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartName,\n+\t\t\tFileSystemFormatFactory formatFactory) {\n+\t\tthis.schema = schema;\n+\t\tthis.path = path;\n+\t\tthis.defaultPartName = defaultPartName;\n+\t\tthis.formatFactory = formatFactory;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t}\n+\n+\t@Override\n+\tpublic final DataStreamSink<BaseRow> consumeDataStream(DataStream<BaseRow> dataStream) {\n+\t\tRowDataPartitionComputer computer = new RowDataPartitionComputer(\n+\t\t\t\tdefaultPartName,\n+\t\t\t\tschema.getFieldNames(),\n+\t\t\t\tschema.getFieldDataTypes(),\n+\t\t\t\tpartitionKeys.toArray(new String[0]));\n+\n+\t\tFileSystemOutputFormat.Builder<BaseRow> builder = new FileSystemOutputFormat.Builder<>();\n+\t\tbuilder.setPartitionComputer(computer);\n+\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n+\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n+\t\tbuilder.setFormatFactory(createOutputFormatFactory(\n+\t\t\t\tformatFactory, this::getNonPartitionTypes));\n+\t\tbuilder.setMetaStoreFactory(createTableMetaStoreFactory(path));\n+\t\tbuilder.setOverwrite(overwrite);\n+\t\tbuilder.setStaticPartitions(staticPartitions);\n+\t\tbuilder.setTempPath(toStagingPath());\n+\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n+\t\t\t\t.setParallelism(dataStream.getParallelism());\n+\t}\n+\n+\tprivate DataType[] getNonPartitionTypes() {\n+\t\treturn Arrays.stream(schema.getFieldNames())\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name))\n+\t\t\t\t.map(name -> schema.getFieldDataType(name).get())\n+\t\t\t\t.toArray(DataType[]::new);\n+\t}\n+\n+\tprivate Path toStagingPath() {\n+\t\tPath stagingDir = new Path(path, \".staging_\" + System.currentTimeMillis());\n+\t\ttry {\n+\t\t\tFileSystem fs = stagingDir.getFileSystem();\n+\t\t\tPreconditions.checkState(\n+\t\t\t\t\tfs.exists(stagingDir) || fs.mkdirs(stagingDir),\n+\t\t\t\t\t\"Failed to create staging dir \" + stagingDir);\n+\t\t\treturn stagingDir;\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(e);\n+\t\t}\n+\t}\n+\n+\tprivate static OutputFormatFactory<BaseRow> createOutputFormatFactory(\n+\t\t\tFileSystemFormatFactory formatFactory,\n+\t\t\tFileSystemFormatFactory.WriterContext context) {\n+\t\tOptional<Encoder<BaseRow>> encoder = formatFactory.createEncoder(context);\n+\t\tOptional<BulkWriter.Factory<BaseRow>> bulk = formatFactory.createBulkWriterFactory(context);\n+\n+\t\tif (!encoder.isPresent() && !bulk.isPresent()) {\n+\t\t\tthrow new TableException(\n+\t\t\t\t\tformatFactory + \" format should implement at least one Encoder or BulkWriter\");\n+\t\t}\n+\t\treturn encoder\n+\t\t\t\t.<OutputFormatFactory<BaseRow>>map(en -> path -> createEncoderOutputFormat(en, path))\n+\t\t\t\t.orElseGet(() -> path -> createBulkWriterOutputFormat(bulk.get(), path));\n+\t}\n+\n+\tprivate static OutputFormat<BaseRow> createBulkWriterOutputFormat(\n+\t\t\tBulkWriter.Factory<BaseRow> factory,\n+\t\t\tPath path) {\n+\t\treturn new OutputFormat<BaseRow>() {\n+\n+\t\t\tprivate BulkWriter<BaseRow> writer;\n+\n+\t\t\t@Override\n+\t\t\tpublic void configure(Configuration parameters) {\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void open(int taskNumber, int numTasks) throws IOException {\n+\t\t\t\tthis.writer = factory.create(path.getFileSystem()\n+\t\t\t\t\t\t.create(path, FileSystem.WriteMode.OVERWRITE));\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void writeRecord(BaseRow record) throws IOException {\n+\t\t\t\twriter.addElement(record);\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic void close() throws IOException {\n+\t\t\t\twriter.flush();\n+\t\t\t\twriter.finish();\n+\t\t\t}\n+\t\t};\n+\t}\n+\n+\tprivate static OutputFormat<BaseRow> createEncoderOutputFormat(\n+\t\t\tEncoder<BaseRow> encoder,\n+\t\t\tPath path) {\n+\t\treturn new OutputFormat<BaseRow>() {\n+\n+\t\t\tprivate FSDataOutputStream output;", "originalCommit": "cd8a10137fa50eb11035082c95878fb6aa6a66c6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA0NDE4MA==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402044180", "bodyText": "Add a serialVersionUID?", "author": "leonardBang", "createdAt": "2020-04-02T04:24:50Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.OutputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FSDataOutputStream;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.OverwritableTableSink;\n+import org.apache.flink.table.sinks.PartitionableTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+/**\n+ * File system {@link TableSink}.\n+ */\n+public class FileSystemTableSink implements\n+\t\tAppendStreamTableSink<BaseRow>,\n+\t\tPartitionableTableSink,\n+\t\tOverwritableTableSink {\n+\n+\tprivate final TableSchema schema;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final Path path;\n+\tprivate final String defaultPartName;\n+\tprivate final FileSystemFormatFactory formatFactory;\n+\n+\tprivate boolean overwrite = false;\n+\tprivate boolean dynamicGrouping = false;\n+\tprivate LinkedHashMap<String, String> staticPartitions = new LinkedHashMap<>();\n+\n+\t/**\n+\t * Construct a file system table sink.\n+\t *\n+\t * @param schema schema of the table.\n+\t * @param path directory path of the file system table.\n+\t * @param partitionKeys partition keys of the table.\n+\t * @param defaultPartName The default partition name in case the dynamic partition column value\n+\t *                        is null/empty string.\n+\t * @param formatFactory format factory to create reader.\n+\t */\n+\tpublic FileSystemTableSink(\n+\t\t\tTableSchema schema,\n+\t\t\tPath path,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartName,\n+\t\t\tFileSystemFormatFactory formatFactory) {\n+\t\tthis.schema = schema;\n+\t\tthis.path = path;\n+\t\tthis.defaultPartName = defaultPartName;\n+\t\tthis.formatFactory = formatFactory;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t}\n+\n+\t@Override\n+\tpublic final DataStreamSink<BaseRow> consumeDataStream(DataStream<BaseRow> dataStream) {\n+\t\tRowDataPartitionComputer computer = new RowDataPartitionComputer(\n+\t\t\t\tdefaultPartName,\n+\t\t\t\tschema.getFieldNames(),\n+\t\t\t\tschema.getFieldDataTypes(),\n+\t\t\t\tpartitionKeys.toArray(new String[0]));\n+\n+\t\tFileSystemOutputFormat.Builder<BaseRow> builder = new FileSystemOutputFormat.Builder<>();\n+\t\tbuilder.setPartitionComputer(computer);\n+\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n+\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n+\t\tbuilder.setFormatFactory(createOutputFormatFactory(\n+\t\t\t\tformatFactory, this::getNonPartitionTypes));\n+\t\tbuilder.setMetaStoreFactory(createTableMetaStoreFactory(path));\n+\t\tbuilder.setOverwrite(overwrite);\n+\t\tbuilder.setStaticPartitions(staticPartitions);\n+\t\tbuilder.setTempPath(toStagingPath());\n+\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n+\t\t\t\t.setParallelism(dataStream.getParallelism());\n+\t}\n+\n+\tprivate DataType[] getNonPartitionTypes() {\n+\t\treturn Arrays.stream(schema.getFieldNames())\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name))\n+\t\t\t\t.map(name -> schema.getFieldDataType(name).get())\n+\t\t\t\t.toArray(DataType[]::new);\n+\t}\n+\n+\tprivate Path toStagingPath() {\n+\t\tPath stagingDir = new Path(path, \".staging_\" + System.currentTimeMillis());\n+\t\ttry {\n+\t\t\tFileSystem fs = stagingDir.getFileSystem();\n+\t\t\tPreconditions.checkState(\n+\t\t\t\t\tfs.exists(stagingDir) || fs.mkdirs(stagingDir),\n+\t\t\t\t\t\"Failed to create staging dir \" + stagingDir);\n+\t\t\treturn stagingDir;\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(e);\n+\t\t}\n+\t}\n+\n+\tprivate static OutputFormatFactory<BaseRow> createOutputFormatFactory(\n+\t\t\tFileSystemFormatFactory formatFactory,\n+\t\t\tFileSystemFormatFactory.WriterContext context) {\n+\t\tOptional<Encoder<BaseRow>> encoder = formatFactory.createEncoder(context);\n+\t\tOptional<BulkWriter.Factory<BaseRow>> bulk = formatFactory.createBulkWriterFactory(context);\n+\n+\t\tif (!encoder.isPresent() && !bulk.isPresent()) {\n+\t\t\tthrow new TableException(\n+\t\t\t\t\tformatFactory + \" format should implement at least one Encoder or BulkWriter\");\n+\t\t}\n+\t\treturn encoder\n+\t\t\t\t.<OutputFormatFactory<BaseRow>>map(en -> path -> createEncoderOutputFormat(en, path))\n+\t\t\t\t.orElseGet(() -> path -> createBulkWriterOutputFormat(bulk.get(), path));\n+\t}\n+\n+\tprivate static OutputFormat<BaseRow> createBulkWriterOutputFormat(\n+\t\t\tBulkWriter.Factory<BaseRow> factory,\n+\t\t\tPath path) {\n+\t\treturn new OutputFormat<BaseRow>() {\n+", "originalCommit": "cd8a10137fa50eb11035082c95878fb6aa6a66c6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc0ODYxMg==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402748612", "bodyText": "Should we filter FORMAT.* properties here? otherwise we get all table connector properties.", "author": "leonardBang", "createdAt": "2020-04-03T05:45:23Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableFactory.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.TableFactory;\n+import org.apache.flink.table.factories.TableFactoryService;\n+import org.apache.flink.table.factories.TableSinkFactory;\n+import org.apache.flink.table.factories.TableSourceFactory;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.sources.TableSource;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.configuration.ConfigOptions.key;\n+import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+\n+/**\n+ * File system {@link TableFactory}.\n+ *\n+ * <p>1.The partition information should be in the file system path, whether it's a temporary\n+ * table or a catalog table.\n+ * 2.Support insert into (append) and insert overwrite.\n+ * 3.Support static and dynamic partition inserting.\n+ *\n+ * <p>Migrate to new source/sink interface after FLIP-95 is ready.\n+ */\n+public class FileSystemTableFactory implements\n+\t\tTableSourceFactory<BaseRow>,\n+\t\tTableSinkFactory<BaseRow> {\n+\n+\tpublic static final String CONNECTOR_VALUE = \"filesystem\";\n+\n+\t/**\n+\t * Not use \"connector.path\" because:\n+\t * 1.Using \"connector.path\" will conflict with current batch csv source and batch csv sink.\n+\t * 2.This is compatible with FLIP-122.\n+\t */\n+\tpublic static final String PATH = \"path\";\n+\n+\t/**\n+\t * Move these properties to validator after FLINK-16904.\n+\t */\n+\tpublic static final ConfigOption<String> PARTITION_DEFAULT_NAME = key(\"partition.default-name\")\n+\t\t\t.stringType()\n+\t\t\t.defaultValue(\"__DEFAULT_PARTITION__\")\n+\t\t\t.withDescription(\"The default partition name in case the dynamic partition\" +\n+\t\t\t\t\t\" column value is null/empty string\");\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(CONNECTOR, CONNECTOR_VALUE);\n+\t\treturn context;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tList<String> properties = new ArrayList<>();\n+\n+\t\t// path\n+\t\tproperties.add(PATH);\n+\n+\t\t// schema\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_DATA_TYPE);\n+\t\tproperties.add(SCHEMA + \".#.\" + DescriptorProperties.TABLE_SCHEMA_NAME);\n+\n+\t\tproperties.add(PARTITION_DEFAULT_NAME.key());\n+\n+\t\t// format\n+\t\tproperties.add(FORMAT);\n+\t\tproperties.add(FORMAT + \".*\");\n+\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic TableSource<BaseRow> createTableSource(TableSourceFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());\n+\n+\t\treturn new FileSystemTableSource(\n+\t\t\t\tcontext.getTable().getSchema(),\n+\t\t\t\tnew Path(properties.getString(PATH)),\n+\t\t\t\tcontext.getTable().getPartitionKeys(),\n+\t\t\t\tgetPartitionDefaultName(properties),\n+\t\t\t\tgetFormatProperties(context.getTable().getProperties()));\n+\t}\n+\n+\t@Override\n+\tpublic TableSink<BaseRow> createTableSink(TableSinkFactory.Context context) {\n+\t\tDescriptorProperties properties = new DescriptorProperties();\n+\t\tproperties.putProperties(context.getTable().getProperties());\n+\n+\t\treturn new FileSystemTableSink(\n+\t\t\t\tcontext.getTable().getSchema(),\n+\t\t\t\tnew Path(properties.getString(PATH)),\n+\t\t\t\tcontext.getTable().getPartitionKeys(),\n+\t\t\t\tgetPartitionDefaultName(properties),\n+\t\t\t\tgetFormatProperties(context.getTable().getProperties()));\n+\t}\n+\n+\tprivate static Map<String, String> getFormatProperties(Map<String, String> tableProperties) {", "originalCommit": "ad9d0b168f308a5205430b2dcf85e680f20449b3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc1MzY5OA==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402753698", "bodyText": "Don't need do this now, you can see there is a magic in TableFactoryService.filterSupportedPropertiesFactorySpecific.", "author": "JingsongLi", "createdAt": "2020-04-03T06:03:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc0ODYxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc2MTU2Ng==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402761566", "bodyText": "Oh, you're wright, I missed this logic.", "author": "leonardBang", "createdAt": "2020-04-03T06:27:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc0ODYxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc0ODkyNw==", "url": "https://github.com/apache/flink/pull/11574#discussion_r402748927", "bodyText": "how about name to bulkWriterFactory?", "author": "leonardBang", "createdAt": "2020-04-03T05:46:39Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -0,0 +1,289 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.api.common.io.OutputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FSDataOutputStream;\n+import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableException;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.OverwritableTableSink;\n+import org.apache.flink.table.sinks.PartitionableTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static org.apache.flink.table.filesystem.FileSystemTableFactory.createFormatFactory;\n+\n+/**\n+ * File system {@link TableSink}.\n+ */\n+public class FileSystemTableSink implements\n+\t\tAppendStreamTableSink<BaseRow>,\n+\t\tPartitionableTableSink,\n+\t\tOverwritableTableSink {\n+\n+\tprivate final TableSchema schema;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final Path path;\n+\tprivate final String defaultPartName;\n+\tprivate final Map<String, String> formatProperties;\n+\n+\tprivate boolean overwrite = false;\n+\tprivate boolean dynamicGrouping = false;\n+\tprivate LinkedHashMap<String, String> staticPartitions = new LinkedHashMap<>();\n+\n+\t/**\n+\t * Construct a file system table sink.\n+\t *\n+\t * @param schema schema of the table.\n+\t * @param path directory path of the file system table.\n+\t * @param partitionKeys partition keys of the table.\n+\t * @param defaultPartName The default partition name in case the dynamic partition column value\n+\t *                        is null/empty string.\n+\t * @param formatProperties format properties.\n+\t */\n+\tpublic FileSystemTableSink(\n+\t\t\tTableSchema schema,\n+\t\t\tPath path,\n+\t\t\tList<String> partitionKeys,\n+\t\t\tString defaultPartName,\n+\t\t\tMap<String, String> formatProperties) {\n+\t\tthis.schema = schema;\n+\t\tthis.path = path;\n+\t\tthis.defaultPartName = defaultPartName;\n+\t\tthis.formatProperties = formatProperties;\n+\t\tthis.partitionKeys = partitionKeys;\n+\t}\n+\n+\t@Override\n+\tpublic final DataStreamSink<BaseRow> consumeDataStream(DataStream<BaseRow> dataStream) {\n+\t\tRowDataPartitionComputer computer = new RowDataPartitionComputer(\n+\t\t\t\tdefaultPartName,\n+\t\t\t\tschema.getFieldNames(),\n+\t\t\t\tschema.getFieldDataTypes(),\n+\t\t\t\tpartitionKeys.toArray(new String[0]));\n+\n+\t\tFileSystemOutputFormat.Builder<BaseRow> builder = new FileSystemOutputFormat.Builder<>();\n+\t\tbuilder.setPartitionComputer(computer);\n+\t\tbuilder.setDynamicGrouped(dynamicGrouping);\n+\t\tbuilder.setPartitionColumns(partitionKeys.toArray(new String[0]));\n+\t\tbuilder.setFormatFactory(createOutputFormatFactory());\n+\t\tbuilder.setMetaStoreFactory(createTableMetaStoreFactory(path));\n+\t\tbuilder.setOverwrite(overwrite);\n+\t\tbuilder.setStaticPartitions(staticPartitions);\n+\t\tbuilder.setTempPath(toStagingPath());\n+\t\treturn dataStream.writeUsingOutputFormat(builder.build())\n+\t\t\t\t.setParallelism(dataStream.getParallelism());\n+\t}\n+\n+\tprivate Path toStagingPath() {\n+\t\tPath stagingDir = new Path(path, \".staging_\" + System.currentTimeMillis());\n+\t\ttry {\n+\t\t\tFileSystem fs = stagingDir.getFileSystem();\n+\t\t\tPreconditions.checkState(\n+\t\t\t\t\tfs.exists(stagingDir) || fs.mkdirs(stagingDir),\n+\t\t\t\t\t\"Failed to create staging dir \" + stagingDir);\n+\t\t\treturn stagingDir;\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new RuntimeException(e);\n+\t\t}\n+\t}\n+\n+\tprivate OutputFormatFactory<BaseRow> createOutputFormatFactory() {\n+\t\tFileSystemFormatFactory formatFactory = createFormatFactory(formatProperties);\n+\t\tFileSystemFormatFactory.WriterContext context = new FileSystemFormatFactory.WriterContext() {\n+\n+\t\t\t@Override\n+\t\t\tpublic TableSchema getSchema() {\n+\t\t\t\treturn schema;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic Map<String, String> getFormatProperties() {\n+\t\t\t\treturn formatProperties;\n+\t\t\t}\n+\n+\t\t\t@Override\n+\t\t\tpublic List<String> getPartitionKeys() {\n+\t\t\t\treturn partitionKeys;\n+\t\t\t}\n+\t\t};\n+\n+\t\tOptional<Encoder<BaseRow>> encoder = formatFactory.createEncoder(context);\n+\t\tOptional<BulkWriter.Factory<BaseRow>> bulk = formatFactory.createBulkWriterFactory(context);\n+\n+\t\tif (!encoder.isPresent() && !bulk.isPresent()) {\n+\t\t\tthrow new TableException(\n+\t\t\t\t\tformatFactory + \" format should implement at least one Encoder or BulkWriter\");\n+\t\t}\n+\t\treturn encoder\n+\t\t\t\t.<OutputFormatFactory<BaseRow>>map(en -> path -> createEncoderOutputFormat(en, path))\n+\t\t\t\t.orElseGet(() -> {\n+\t\t\t\t\t// Optional is not serializable.\n+\t\t\t\t\tBulkWriter.Factory<BaseRow> bulkWriter = bulk.get();", "originalCommit": "ad9d0b168f308a5205430b2dcf85e680f20449b3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "74f87e4c4a2cd7536d63b00f04635ceb8fd649b0", "url": "https://github.com/apache/flink/commit/74f87e4c4a2cd7536d63b00f04635ceb8fd649b0", "message": "[FLINK-16859][table-runtime] Introduce FileSystemTableFactory", "committedDate": "2020-04-07T03:12:04Z", "type": "commit"}, {"oid": "23b9b078c98c858127cb12564567b2cd739fa357", "url": "https://github.com/apache/flink/commit/23b9b078c98c858127cb12564567b2cd739fa357", "message": "Fix keys", "committedDate": "2020-04-07T03:12:04Z", "type": "commit"}, {"oid": "00e45724fcc6d83e5019a3c6aa1d41c040e02989", "url": "https://github.com/apache/flink/commit/00e45724fcc6d83e5019a3c6aa1d41c040e02989", "message": "Fix comments", "committedDate": "2020-04-07T03:12:04Z", "type": "commit"}, {"oid": "cdcee42ce8162ad2b98384e5acd862d9154e66fe", "url": "https://github.com/apache/flink/commit/cdcee42ce8162ad2b98384e5acd862d9154e66fe", "message": "Add getFormatProperties to context", "committedDate": "2020-04-07T03:12:05Z", "type": "commit"}, {"oid": "c1caed477e925f4af664db5e251a11e293d05659", "url": "https://github.com/apache/flink/commit/c1caed477e925f4af664db5e251a11e293d05659", "message": "enhance WriterContext", "committedDate": "2020-04-07T03:12:05Z", "type": "commit"}, {"oid": "716a90009e5fd994ebb3f1fed729a20b49398e01", "url": "https://github.com/apache/flink/commit/716a90009e5fd994ebb3f1fed729a20b49398e01", "message": "ITCase use array instead of scala seq", "committedDate": "2020-04-07T03:12:05Z", "type": "commit"}, {"oid": "ff086f2fa377e6950aca8a959c0cae50ab0558c6", "url": "https://github.com/apache/flink/commit/ff086f2fa377e6950aca8a959c0cae50ab0558c6", "message": "Add bulk writer test and fix bug", "committedDate": "2020-04-07T03:12:05Z", "type": "commit"}, {"oid": "a2008dc6560833b9df4a4faae6d2b783803a9c2a", "url": "https://github.com/apache/flink/commit/a2008dc6560833b9df4a4faae6d2b783803a9c2a", "message": "Fix comments", "committedDate": "2020-04-07T03:12:05Z", "type": "commit"}, {"oid": "a2008dc6560833b9df4a4faae6d2b783803a9c2a", "url": "https://github.com/apache/flink/commit/a2008dc6560833b9df4a4faae6d2b783803a9c2a", "message": "Fix comments", "committedDate": "2020-04-07T03:12:05Z", "type": "forcePushed"}]}