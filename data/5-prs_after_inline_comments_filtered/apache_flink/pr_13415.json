{"pr_number": 13415, "pr_title": "[FLINK-19277][python] Introduce BatchArrowPythonGroupWindowAggregateFunctionOperator", "pr_createdAt": "2020-09-18T05:05:10Z", "pr_url": "https://github.com/apache/flink/pull/13415", "timeline": [{"oid": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1", "url": "https://github.com/apache/flink/commit/4a182a499c3867765b6ef3c3957fbe4c63c2dce1", "message": "[FLINK-19277][python] Introduce BatchArrowPythonGroupWindowAggregateFunctionOperator", "committedDate": "2020-09-18T05:00:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxMTExNg==", "url": "https://github.com/apache/flink/pull/13415#discussion_r490711116", "bodyText": "Putting super.open() at the end of this method? Although it doesn't affect the correctness, it makes the code more readable.", "author": "dianfu", "createdAt": "2020-09-18T05:31:02Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/arrow/batch/BatchArrowPythonGroupWindowAggregateFunctionOperator.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.JoinedRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.data.binary.BinaryRowData;\n+import org.apache.flink.table.functions.AggregateFunction;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.operators.window.TimeWindow;\n+import org.apache.flink.table.runtime.operators.window.grouping.HeapWindowsGrouping;\n+import org.apache.flink.table.runtime.operators.window.grouping.WindowsGrouping;\n+import org.apache.flink.table.runtime.util.RowIterator;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import java.util.LinkedList;\n+\n+/**\n+ * The Batch Arrow Python {@link AggregateFunction} Operator for Group Window Aggregation.\n+ */\n+@Internal\n+public class BatchArrowPythonGroupWindowAggregateFunctionOperator\n+\textends AbstractBatchArrowPythonAggregateFunctionOperator {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/**\n+\t * The Infos of the Window.\n+\t * 0 -> start of the Window.\n+\t * 1 -> end of the Window.\n+\t * 2 -> row time of the Window.\n+\t */\n+\tprivate final int[] namedProperties;\n+\n+\t/**\n+\t * The row time index of the input data.\n+\t */\n+\tprivate final int inputTimeFieldIndex;\n+\n+\t/**\n+\t * The window elements buffer size limit used in group window agg operator.\n+\t */\n+\tprivate final int maxLimitSize;\n+\n+\t/**\n+\t * The window size of the window.\n+\t */\n+\tprivate final long windowSize;\n+\n+\t/**\n+\t * The sliding size of the sliding window.\n+\t */\n+\tprivate final long slideSize;\n+\n+\tprivate transient WindowsGrouping windowsGrouping;\n+\n+\t/**\n+\t * The GenericRowData reused holding the property of the window, such as window start, window\n+\t * end and window time.\n+\t */\n+\tprivate transient GenericRowData windowProperty;\n+\n+\t/**\n+\t * The JoinedRowData reused holding the window agg execution result.\n+\t */\n+\tprivate transient JoinedRowData windowAggResult;\n+\n+\t/**\n+\t * The queue holding the input groupSet with the TimeWindow for which the execution results\n+\t * have not been received.\n+\t */\n+\tprivate transient LinkedList<Tuple2<RowData, TimeWindow>> inputKeyAndWindow;\n+\n+\tpublic BatchArrowPythonGroupWindowAggregateFunctionOperator(\n+\t\tConfiguration config,\n+\t\tPythonFunctionInfo[] pandasAggFunctions,\n+\t\tRowType inputType,\n+\t\tRowType outputType,\n+\t\tint inputTimeFieldIndex,\n+\t\tint maxLimitSize,\n+\t\tlong windowSize,\n+\t\tlong slideSize,\n+\t\tint[] namedProperties,\n+\t\tint[] groupKey,\n+\t\tint[] groupingSet,\n+\t\tint[] udafInputOffsets) {\n+\t\tsuper(config, pandasAggFunctions, inputType, outputType, groupKey, groupingSet, udafInputOffsets);\n+\t\tthis.namedProperties = namedProperties;\n+\t\tthis.inputTimeFieldIndex = inputTimeFieldIndex;\n+\t\tthis.maxLimitSize = maxLimitSize;\n+\t\tthis.windowSize = windowSize;\n+\t\tthis.slideSize = slideSize;\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tuserDefinedFunctionOutputType = new RowType(\n+\t\t\toutputType.getFields().subList(groupingSet.length, outputType.getFieldCount() - namedProperties.length));\n+\t\tsuper.open();", "originalCommit": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxNDEwNw==", "url": "https://github.com/apache/flink/pull/13415#discussion_r490714107", "bodyText": "Could we refactor the following code a bit to avoid duplication?", "author": "dianfu", "createdAt": "2020-09-18T05:41:11Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/arrow/batch/BatchArrowPythonGroupWindowAggregateFunctionOperator.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.JoinedRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.data.binary.BinaryRowData;\n+import org.apache.flink.table.functions.AggregateFunction;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.operators.window.TimeWindow;\n+import org.apache.flink.table.runtime.operators.window.grouping.HeapWindowsGrouping;\n+import org.apache.flink.table.runtime.operators.window.grouping.WindowsGrouping;\n+import org.apache.flink.table.runtime.util.RowIterator;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import java.util.LinkedList;\n+\n+/**\n+ * The Batch Arrow Python {@link AggregateFunction} Operator for Group Window Aggregation.\n+ */\n+@Internal\n+public class BatchArrowPythonGroupWindowAggregateFunctionOperator\n+\textends AbstractBatchArrowPythonAggregateFunctionOperator {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/**\n+\t * The Infos of the Window.\n+\t * 0 -> start of the Window.\n+\t * 1 -> end of the Window.\n+\t * 2 -> row time of the Window.\n+\t */\n+\tprivate final int[] namedProperties;\n+\n+\t/**\n+\t * The row time index of the input data.\n+\t */\n+\tprivate final int inputTimeFieldIndex;\n+\n+\t/**\n+\t * The window elements buffer size limit used in group window agg operator.\n+\t */\n+\tprivate final int maxLimitSize;\n+\n+\t/**\n+\t * The window size of the window.\n+\t */\n+\tprivate final long windowSize;\n+\n+\t/**\n+\t * The sliding size of the sliding window.\n+\t */\n+\tprivate final long slideSize;\n+\n+\tprivate transient WindowsGrouping windowsGrouping;\n+\n+\t/**\n+\t * The GenericRowData reused holding the property of the window, such as window start, window\n+\t * end and window time.\n+\t */\n+\tprivate transient GenericRowData windowProperty;\n+\n+\t/**\n+\t * The JoinedRowData reused holding the window agg execution result.\n+\t */\n+\tprivate transient JoinedRowData windowAggResult;\n+\n+\t/**\n+\t * The queue holding the input groupSet with the TimeWindow for which the execution results\n+\t * have not been received.\n+\t */\n+\tprivate transient LinkedList<Tuple2<RowData, TimeWindow>> inputKeyAndWindow;\n+\n+\tpublic BatchArrowPythonGroupWindowAggregateFunctionOperator(\n+\t\tConfiguration config,\n+\t\tPythonFunctionInfo[] pandasAggFunctions,\n+\t\tRowType inputType,\n+\t\tRowType outputType,\n+\t\tint inputTimeFieldIndex,\n+\t\tint maxLimitSize,\n+\t\tlong windowSize,\n+\t\tlong slideSize,\n+\t\tint[] namedProperties,\n+\t\tint[] groupKey,\n+\t\tint[] groupingSet,\n+\t\tint[] udafInputOffsets) {\n+\t\tsuper(config, pandasAggFunctions, inputType, outputType, groupKey, groupingSet, udafInputOffsets);\n+\t\tthis.namedProperties = namedProperties;\n+\t\tthis.inputTimeFieldIndex = inputTimeFieldIndex;\n+\t\tthis.maxLimitSize = maxLimitSize;\n+\t\tthis.windowSize = windowSize;\n+\t\tthis.slideSize = slideSize;\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tuserDefinedFunctionOutputType = new RowType(\n+\t\t\toutputType.getFields().subList(groupingSet.length, outputType.getFieldCount() - namedProperties.length));\n+\t\tsuper.open();\n+\t\tinputKeyAndWindow = new LinkedList<>();\n+\t\twindowProperty = new GenericRowData(namedProperties.length);\n+\t\twindowAggResult = new JoinedRowData();\n+\t\twindowsGrouping = new HeapWindowsGrouping(\n+\t\t\tmaxLimitSize, windowSize, slideSize, inputTimeFieldIndex, false);\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\tsuper.close();\n+\t\twindowsGrouping.close();\n+\t}\n+\n+\t@Override\n+\tpublic void bufferInput(RowData input) throws Exception {\n+\t\t// always copy the projection result as the generated Projection reuses the projection result\n+\t\tBinaryRowData currentKey = groupKeyProjection.apply(input).copy();\n+\t\tcurrentKey.setRowKind(input.getRowKind());", "originalCommit": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxNjcwOA==", "url": "https://github.com/apache/flink/pull/13415#discussion_r490716708", "bodyText": "Move this line inside if (currentBatchCount > 0)?", "author": "dianfu", "createdAt": "2020-09-18T05:50:28Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/arrow/batch/BatchArrowPythonGroupWindowAggregateFunctionOperator.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.JoinedRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.data.binary.BinaryRowData;\n+import org.apache.flink.table.functions.AggregateFunction;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.operators.window.TimeWindow;\n+import org.apache.flink.table.runtime.operators.window.grouping.HeapWindowsGrouping;\n+import org.apache.flink.table.runtime.operators.window.grouping.WindowsGrouping;\n+import org.apache.flink.table.runtime.util.RowIterator;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import java.util.LinkedList;\n+\n+/**\n+ * The Batch Arrow Python {@link AggregateFunction} Operator for Group Window Aggregation.\n+ */\n+@Internal\n+public class BatchArrowPythonGroupWindowAggregateFunctionOperator\n+\textends AbstractBatchArrowPythonAggregateFunctionOperator {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/**\n+\t * The Infos of the Window.\n+\t * 0 -> start of the Window.\n+\t * 1 -> end of the Window.\n+\t * 2 -> row time of the Window.\n+\t */\n+\tprivate final int[] namedProperties;\n+\n+\t/**\n+\t * The row time index of the input data.\n+\t */\n+\tprivate final int inputTimeFieldIndex;\n+\n+\t/**\n+\t * The window elements buffer size limit used in group window agg operator.\n+\t */\n+\tprivate final int maxLimitSize;\n+\n+\t/**\n+\t * The window size of the window.\n+\t */\n+\tprivate final long windowSize;\n+\n+\t/**\n+\t * The sliding size of the sliding window.\n+\t */\n+\tprivate final long slideSize;\n+\n+\tprivate transient WindowsGrouping windowsGrouping;\n+\n+\t/**\n+\t * The GenericRowData reused holding the property of the window, such as window start, window\n+\t * end and window time.\n+\t */\n+\tprivate transient GenericRowData windowProperty;\n+\n+\t/**\n+\t * The JoinedRowData reused holding the window agg execution result.\n+\t */\n+\tprivate transient JoinedRowData windowAggResult;\n+\n+\t/**\n+\t * The queue holding the input groupSet with the TimeWindow for which the execution results\n+\t * have not been received.\n+\t */\n+\tprivate transient LinkedList<Tuple2<RowData, TimeWindow>> inputKeyAndWindow;\n+\n+\tpublic BatchArrowPythonGroupWindowAggregateFunctionOperator(\n+\t\tConfiguration config,\n+\t\tPythonFunctionInfo[] pandasAggFunctions,\n+\t\tRowType inputType,\n+\t\tRowType outputType,\n+\t\tint inputTimeFieldIndex,\n+\t\tint maxLimitSize,\n+\t\tlong windowSize,\n+\t\tlong slideSize,\n+\t\tint[] namedProperties,\n+\t\tint[] groupKey,\n+\t\tint[] groupingSet,\n+\t\tint[] udafInputOffsets) {\n+\t\tsuper(config, pandasAggFunctions, inputType, outputType, groupKey, groupingSet, udafInputOffsets);\n+\t\tthis.namedProperties = namedProperties;\n+\t\tthis.inputTimeFieldIndex = inputTimeFieldIndex;\n+\t\tthis.maxLimitSize = maxLimitSize;\n+\t\tthis.windowSize = windowSize;\n+\t\tthis.slideSize = slideSize;\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tuserDefinedFunctionOutputType = new RowType(\n+\t\t\toutputType.getFields().subList(groupingSet.length, outputType.getFieldCount() - namedProperties.length));\n+\t\tsuper.open();\n+\t\tinputKeyAndWindow = new LinkedList<>();\n+\t\twindowProperty = new GenericRowData(namedProperties.length);\n+\t\twindowAggResult = new JoinedRowData();\n+\t\twindowsGrouping = new HeapWindowsGrouping(\n+\t\t\tmaxLimitSize, windowSize, slideSize, inputTimeFieldIndex, false);\n+\t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\tsuper.close();\n+\t\twindowsGrouping.close();\n+\t}\n+\n+\t@Override\n+\tpublic void bufferInput(RowData input) throws Exception {\n+\t\t// always copy the projection result as the generated Projection reuses the projection result\n+\t\tBinaryRowData currentKey = groupKeyProjection.apply(input).copy();\n+\t\tcurrentKey.setRowKind(input.getRowKind());\n+\t\tif (lastGroupKey == null) {\n+\t\t\tlastGroupKey = currentKey;\n+\t\t\tlastGroupSet = groupSetProjection.apply(input).copy();\n+\t\t} else if (isNewKey(currentKey)) {\n+\t\t\tinvokeCurrentBatch();\n+\t\t\tlastGroupKey = currentKey;\n+\t\t\tlastGroupSet = groupSetProjection.apply(input).copy();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tprotected void invokeCurrentBatch() throws Exception {\n+\t\twindowsGrouping.advanceWatermarkToTriggerAllWindows();\n+\t\ttriggerWindowProcess();\n+\t\twindowsGrouping.reset();\n+\t}\n+\n+\t@Override\n+\tpublic void processElementInternal(RowData value) throws Exception {\n+\t\twindowsGrouping.addInputToBuffer((BinaryRowData) value);\n+\t\ttriggerWindowProcess();\n+\t}\n+\n+\t@Override\n+\t@SuppressWarnings(\"ConstantConditions\")\n+\tpublic void emitResult(Tuple2<byte[], Integer> resultTuple) throws Exception {\n+\t\tbyte[] udafResult = resultTuple.f0;\n+\t\tint length = resultTuple.f1;\n+\t\tbais.setBuffer(udafResult, 0, length);\n+\t\tint rowCount = arrowSerializer.load();\n+\t\tfor (int i = 0; i < rowCount; i++) {\n+\t\t\tTuple2<RowData, TimeWindow> input = inputKeyAndWindow.poll();\n+\t\t\tRowData key = input.f0;\n+\t\t\tTimeWindow window = input.f1;\n+\t\t\tsetWindowProperty(window);\n+\t\t\twindowAggResult.replace(key, arrowSerializer.read(i));\n+\t\t\trowDataWrapper.collect(reuseJoinedRow.replace(windowAggResult, windowProperty));\n+\t\t}\n+\t}\n+\n+\tprivate void triggerWindowProcess() throws Exception {\n+\t\twhile (windowsGrouping.hasTriggerWindow()) {\n+\t\t\tRowIterator<BinaryRowData> elementIterator =\n+\t\t\t\twindowsGrouping.buildTriggerWindowElementsIterator();\n+\t\t\tTimeWindow currentWindow = windowsGrouping.getTriggerWindow();", "originalCommit": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxODQ4NA==", "url": "https://github.com/apache/flink/pull/13415#discussion_r490718484", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * Test for {@link BatchArrowPythonGroupWindowAggregateFunctionOperatorTest}. These test that:\n          \n          \n            \n             * Test for {@link BatchArrowPythonGroupWindowAggregateFunctionOperator}. These test that:", "author": "dianfu", "createdAt": "2020-09-18T05:56:25Z", "path": "flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/aggregate/arrow/batch/BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java", "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.python.PythonOptions;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator;\n+import org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase;\n+import org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner;\n+import org.apache.flink.table.runtime.utils.PythonTestUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TimestampType;\n+import org.apache.flink.table.types.logical.VarCharType;\n+\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+\n+/**\n+ * Test for {@link BatchArrowPythonGroupWindowAggregateFunctionOperatorTest}. These test that:", "originalCommit": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxOTA3Nw==", "url": "https://github.com/apache/flink/pull/13415#discussion_r490719077", "bodyText": "BatchArrowPythonGroupWindowAggregateFunctionOperator is running in batch, so I guess there is no checkpoint?", "author": "dianfu", "createdAt": "2020-09-18T05:58:21Z", "path": "flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/aggregate/arrow/batch/BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java", "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.python.PythonOptions;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator;\n+import org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase;\n+import org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner;\n+import org.apache.flink.table.runtime.utils.PythonTestUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TimestampType;\n+import org.apache.flink.table.types.logical.VarCharType;\n+\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+\n+/**\n+ * Test for {@link BatchArrowPythonGroupWindowAggregateFunctionOperatorTest}. These test that:\n+ *\n+ * <ul>\n+ * <li>FinishBundle is called when checkpoint is encountered</li>\n+ * <li>Watermarks are buffered and only sent to downstream when finishedBundle is triggered</li>\n+ * </ul>\n+ */\n+public class BatchArrowPythonGroupWindowAggregateFunctionOperatorTest extends ArrowPythonAggregateFunctionOperatorTestBase {\n+\t@Test\n+\tpublic void testGroupAggregateFunction() throws Exception {\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness = getTestHarness(\n+\t\t\tnew Configuration());\n+\t\tlong initialTime = 0L;\n+\t\tConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();\n+\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c2\", 0L, 0L), initialTime + 1));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c4\", 1L, 6000L), initialTime + 2));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c6\", 2L, 10000L), initialTime + 3));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c2\", \"c8\", 3L, 0L), initialTime + 3));\n+\n+\t\ttestHarness.close();\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 1L, TimestampData.fromEpochMillis(5000L), TimestampData.fromEpochMillis(15000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 2L, TimestampData.fromEpochMillis(10000L), TimestampData.fromEpochMillis(20000L))));\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\t}\n+\n+\t@Test\n+\tpublic void testFinishBundleTriggeredOnCheckpoint() throws Exception {", "originalCommit": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDcxOTIzOA==", "url": "https://github.com/apache/flink/pull/13415#discussion_r490719238", "bodyText": "BatchArrowPythonGroupWindowAggregateFunctionOperator is running in batch and so I guess we don't need to consider watermark?", "author": "dianfu", "createdAt": "2020-09-18T05:58:57Z", "path": "flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/aggregate/arrow/batch/BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java", "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.python.PythonOptions;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator;\n+import org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase;\n+import org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner;\n+import org.apache.flink.table.runtime.utils.PythonTestUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TimestampType;\n+import org.apache.flink.table.types.logical.VarCharType;\n+\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+\n+/**\n+ * Test for {@link BatchArrowPythonGroupWindowAggregateFunctionOperatorTest}. These test that:\n+ *\n+ * <ul>\n+ * <li>FinishBundle is called when checkpoint is encountered</li>\n+ * <li>Watermarks are buffered and only sent to downstream when finishedBundle is triggered</li>\n+ * </ul>\n+ */\n+public class BatchArrowPythonGroupWindowAggregateFunctionOperatorTest extends ArrowPythonAggregateFunctionOperatorTestBase {\n+\t@Test\n+\tpublic void testGroupAggregateFunction() throws Exception {\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness = getTestHarness(\n+\t\t\tnew Configuration());\n+\t\tlong initialTime = 0L;\n+\t\tConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();\n+\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c2\", 0L, 0L), initialTime + 1));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c4\", 1L, 6000L), initialTime + 2));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c6\", 2L, 10000L), initialTime + 3));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c2\", \"c8\", 3L, 0L), initialTime + 3));\n+\n+\t\ttestHarness.close();\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 1L, TimestampData.fromEpochMillis(5000L), TimestampData.fromEpochMillis(15000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 2L, TimestampData.fromEpochMillis(10000L), TimestampData.fromEpochMillis(20000L))));\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\t}\n+\n+\t@Test\n+\tpublic void testFinishBundleTriggeredOnCheckpoint() throws Exception {\n+\t\tConfiguration conf = new Configuration();\n+\t\tconf.setInteger(PythonOptions.MAX_BUNDLE_SIZE, 10);\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness = getTestHarness(conf);\n+\n+\t\tlong initialTime = 0L;\n+\t\tConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();\n+\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c2\", 0L, 0L), initialTime + 1));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c4\", 1L, 6000L), initialTime + 2));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c6\", 2L, 10000L), initialTime + 3));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c2\", \"c8\", 3L, 0L), initialTime + 3));\n+\t\t// checkpoint trigger finishBundle\n+\t\ttestHarness.prepareSnapshotPreBarrier(0L);\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 1L, TimestampData.fromEpochMillis(5000L), TimestampData.fromEpochMillis(15000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 2L, TimestampData.fromEpochMillis(10000L), TimestampData.fromEpochMillis(20000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\n+\t\ttestHarness.close();\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\t}\n+\n+\t@Test\n+\tpublic void testFinishBundleTriggeredByCount() throws Exception {\n+\t\tConfiguration conf = new Configuration();\n+\t\tconf.setInteger(PythonOptions.MAX_BUNDLE_SIZE, 6);\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness = getTestHarness(conf);\n+\n+\t\tlong initialTime = 0L;\n+\t\tConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();\n+\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c2\", 0L, 0L), initialTime + 1));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c4\", 1L, 6000L), initialTime + 2));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c6\", 2L, 10000L), initialTime + 3));\n+\n+\t\tassertOutputEquals(\"FinishBundle should not be triggered.\", expectedOutput, testHarness.getOutput());\n+\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c2\", \"c8\", 3L, 0L), initialTime + 3));\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 1L, TimestampData.fromEpochMillis(5000L), TimestampData.fromEpochMillis(15000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 2L, TimestampData.fromEpochMillis(10000L), TimestampData.fromEpochMillis(20000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\n+\t\ttestHarness.close();\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\t}\n+\n+\t@Test\n+\tpublic void testFinishBundleTriggeredByTime() throws Exception {\n+\t\tConfiguration conf = new Configuration();\n+\t\tconf.setInteger(PythonOptions.MAX_BUNDLE_SIZE, 10);\n+\t\tconf.setLong(PythonOptions.MAX_BUNDLE_TIME_MILLS, 1000L);\n+\t\tOneInputStreamOperatorTestHarness<RowData, RowData> testHarness = getTestHarness(conf);\n+\n+\t\tlong initialTime = 0L;\n+\t\tConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();\n+\n+\t\ttestHarness.open();\n+\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c2\", 0L, 0L), initialTime + 1));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c4\", 1L, 6000L), initialTime + 2));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c1\", \"c6\", 2L, 10000L), initialTime + 3));\n+\t\ttestHarness.processElement(new StreamRecord<>(newBinaryRow(true, \"c2\", \"c8\", 3L, 0L), initialTime + 3));\n+\t\tassertOutputEquals(\"FinishBundle should not be triggered.\", expectedOutput, testHarness.getOutput());\n+\n+\t\ttestHarness.setProcessingTime(1000L);\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 0L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 1L, TimestampData.fromEpochMillis(5000L), TimestampData.fromEpochMillis(15000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c1\", 2L, TimestampData.fromEpochMillis(10000L), TimestampData.fromEpochMillis(20000L))));\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\n+\t\ttestHarness.close();\n+\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(-5000L), TimestampData.fromEpochMillis(5000L))));\n+\t\texpectedOutput.add(new StreamRecord<>(\n+\t\t\tnewRow(true, \"c2\", 3L, TimestampData.fromEpochMillis(0L), TimestampData.fromEpochMillis(10000L))));\n+\n+\t\tassertOutputEquals(\"Output was not correct.\", expectedOutput, testHarness.getOutput());\n+\t}\n+\n+\t@Test\n+\tpublic void testWatermarkProcessedOnFinishBundle() throws Exception {", "originalCommit": "4a182a499c3867765b6ef3c3957fbe4c63c2dce1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "026263400c716dc9e84d56e93af2b83cc8fe0c06", "url": "https://github.com/apache/flink/commit/026263400c716dc9e84d56e93af2b83cc8fe0c06", "message": "address comments", "committedDate": "2020-09-18T06:48:14Z", "type": "commit"}]}