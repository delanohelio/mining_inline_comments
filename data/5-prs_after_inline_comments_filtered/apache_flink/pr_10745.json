{"pr_number": 10745, "pr_title": "[FLINK-15445][connectors/jdbc] JDBC Table Source didn't work for Type\u2026", "pr_createdAt": "2020-01-02T09:24:49Z", "pr_url": "https://github.com/apache/flink/pull/10745", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQxMTYzNA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362411634", "bodyText": "This is wrong fix, you should modify returnType.", "author": "JingsongLi", "createdAt": "2020-01-02T09:31:49Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -112,6 +113,11 @@ public boolean isBounded() {\n \t\treturn returnType;\n \t}\n \n+\t@Override\n+\tpublic DataType getProducedDataType() {\n+\t\treturn schema.toRowDataType();", "originalCommit": "0c00c793d00c60c7dafc5b4f8209459a0dcc940b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQyMTcyOA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362421728", "bodyText": "updated", "author": "docete", "createdAt": "2020-01-02T10:08:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQxMTYzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTE5Ng==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362465196", "bodyText": "Please remove the overrided implementation of getReturnType()", "author": "wuchong", "createdAt": "2020-01-02T13:01:52Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -112,6 +120,11 @@ public boolean isBounded() {\n \t\treturn returnType;\n \t}\n \n+\t@Override\n+\tpublic DataType getProducedDataType() {", "originalCommit": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MTM3MQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362691371", "bodyText": "Sure", "author": "docete", "createdAt": "2020-01-03T02:17:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTE5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTQyOA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362465428", "bodyText": "Use returnType.getFieldNames instead reconstruct the selected field names again?", "author": "wuchong", "createdAt": "2020-01-02T13:02:52Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -180,6 +193,20 @@ public boolean equals(Object o) {\n \t\t}\n \t}\n \n+\t@Override\n+\tpublic String explainSource() {\n+\t\tif (selectFields == null) {\n+\t\t\treturn String.format(\n+\t\t\t\t\"JDBCTableSource(read fields: %s)\", String.join(\", \", schema.getFieldNames()));\n+\t\t} else {\n+\t\t\tString[] fields = new String[selectFields.length];\n+\t\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\t\tfields[i] = schema.getFieldName(selectFields[i]).get();\n+\t\t\t}\n+\t\t\treturn String.format(\"JDBCTableSource(read fields: %s)\", String.join(\", \", fields));", "originalCommit": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MTM4NQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362691385", "bodyText": "Sure", "author": "docete", "createdAt": "2020-01-03T02:17:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTQyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MzczNw==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362693737", "bodyText": "Could you add full list of types to have a full coverage? For example, add TIMESTAMP, TIMESTAMP(9), DECIMAL(38, 18), DECIMAL, FLOAT (we have a bug for float before), etc...\nI would also suggest to combine source integrate tests and sink integrate tests, e.g.  read from collections and write into jdbc using SQL, and read from JDBC to verify the result.", "author": "wuchong", "createdAt": "2020-01-03T02:40:52Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.java.io.jdbc;\n+\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.java.StreamTableEnvironment;\n+import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+\n+/**\n+ * ITCase for {@link JDBCTableSource}.\n+ */\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIME('15:35:00'), 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIME('15:36:01'), 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +", "originalCommit": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDU0ODEwMA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r374548100", "bodyText": "Have add more types for derby. and the combining of source and sink integrate tests will postpone to since the sinks did not support new type system.", "author": "docete", "createdAt": "2020-02-04T09:08:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MzczNw=="}], "type": "inlineReview"}, {"oid": "f0a7af07324182da2121f873e8e857459f425194", "url": "https://github.com/apache/flink/commit/f0a7af07324182da2121f873e8e857459f425194", "message": "[FLINK-15445][connectors/jdbc] JDBC Table Source didn't work for Types with precision (or/and scale)", "committedDate": "2020-02-04T07:56:32Z", "type": "commit"}, {"oid": "4a2e1789331ad3d6b7ed5a34f2a64c7893528a1b", "url": "https://github.com/apache/flink/commit/4a2e1789331ad3d6b7ed5a34f2a64c7893528a1b", "message": "fixup: adjust dependencies", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "24f4f9782b3332d62bfb0933b423ae3a5843318b", "url": "https://github.com/apache/flink/commit/24f4f9782b3332d62bfb0933b423ae3a5843318b", "message": "fixup: fix projection push-down", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "a22c3c94d4e6323543ca9b23254bcea343625f5d", "url": "https://github.com/apache/flink/commit/a22c3c94d4e6323543ca9b23254bcea343625f5d", "message": "[FLINK-15445][connectors/jdbc] JDBCTableSource should override and change explainSource() API to explain the pushdown applied", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "a3c34fd3dc54726056e63db06136917a566068a4", "url": "https://github.com/apache/flink/commit/a3c34fd3dc54726056e63db06136917a566068a4", "message": "fixup: address Jark's comments", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "5858540ad796f272248b09f6b65ea9f2dd402a6f", "url": "https://github.com/apache/flink/commit/5858540ad796f272248b09f6b65ea9f2dd402a6f", "message": "fixup: test more data types", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "5ed269626192adabb37b5481c21c91ad4c892592", "url": "https://github.com/apache/flink/commit/5ed269626192adabb37b5481c21c91ad4c892592", "message": "[FLINK-15445][connectors/jdbc] validate if the dialect instance support a specific data type", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "e8d921d91449b3c3c8afea064af7ec99daf8d976", "url": "https://github.com/apache/flink/commit/e8d921d91449b3c3c8afea064af7ec99daf8d976", "message": "fixup: derby only support decimal with precision [1,31]", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9", "url": "https://github.com/apache/flink/commit/f7456ac7e14681e18199941e0962b207eb99bef9", "message": "fixup: rebase to resolve conflicts", "committedDate": "2020-02-04T08:59:05Z", "type": "commit"}, {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9", "url": "https://github.com/apache/flink/commit/f7456ac7e14681e18199941e0962b207eb99bef9", "message": "fixup: rebase to resolve conflicts", "committedDate": "2020-02-04T08:59:05Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5NTI0NQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376195245", "bodyText": "Can we remove the returnType member field?\nIt's error-prone to maintain two objects. The returnType is only used in getDataStream and can be derived via TypeConversions.fromDataTypeToLegacyInfo(producedDataType).", "author": "wuchong", "createdAt": "2020-02-07T03:10:06Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -73,17 +75,23 @@ private JDBCTableSource(\n \t\tthis.selectFields = selectFields;\n \n \t\tfinal TypeInformation<?>[] schemaTypeInfos = schema.getFieldTypes();\n+\t\tfinal DataType[] schemaDataTypes = schema.getFieldDataTypes();\n \t\tfinal String[] schemaFieldNames = schema.getFieldNames();\n \t\tif (selectFields != null) {\n \t\t\tTypeInformation<?>[] typeInfos = new TypeInformation[selectFields.length];\n-\t\t\tString[] typeNames = new String[selectFields.length];\n+\t\t\tDataType[] dataTypes = new DataType[selectFields.length];\n+\t\t\tString[] fieldNames = new String[selectFields.length];\n \t\t\tfor (int i = 0; i < selectFields.length; i++) {\n \t\t\t\ttypeInfos[i] = schemaTypeInfos[selectFields[i]];\n-\t\t\t\ttypeNames[i] = schemaFieldNames[selectFields[i]];\n+\t\t\t\tdataTypes[i] = schemaDataTypes[selectFields[i]];\n+\t\t\t\tfieldNames[i] = schemaFieldNames[selectFields[i]];\n \t\t\t}\n-\t\t\tthis.returnType = new RowTypeInfo(typeInfos, typeNames);\n+\t\t\tthis.returnType = new RowTypeInfo(typeInfos, fieldNames);", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjI2Njc4NA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376266784", "bodyText": "Make sense.", "author": "docete", "createdAt": "2020-02-07T08:30:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5NTI0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5NjEzMw==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376196133", "bodyText": "I think it would be better to move the validation logic into JDBCValidator.\nhttps://github.com/apache/flink/blob/master/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/table/descriptors/JDBCValidator.java#L73", "author": "wuchong", "createdAt": "2020-02-07T03:14:13Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceSinkFactory.java", "diffHunk": "@@ -130,8 +130,11 @@\n \t\tTableSchema schema = TableSchemaUtils.getPhysicalSchema(\n \t\t\tdescriptorProperties.getTableSchema(SCHEMA));\n \n+\t\tJDBCOptions options = getJDBCOptions(descriptorProperties);\n+\t\toptions.getDialect().validate(schema);", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5ODM0NQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376198345", "bodyText": "You can create a unit test JDBCDataTypeTest to verify all the types with different precision with different dialects. This doesn't involve a job submission, and is a lightweight unit test. You can take FlinkDDLDataTypeTest as an example.", "author": "wuchong", "createdAt": "2020-02-07T03:25:04Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n \t}\n \n \t@Test\n-\tpublic void testFieldsProjection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"double_col DOUBLE,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ID_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT * FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001\");\n-\t\texpected.add(\"1002\");\n-\t\texpected.add(\"1003\");\n-\t\texpected.add(\"1004\");\n-\t\texpected.add(\"1005\");\n-\t\texpected.add(\"1006\");\n-\t\texpected.add(\"1007\");\n-\t\texpected.add(\"1008\");\n-\t\texpected.add(\"1009\");\n-\t\texpected.add(\"1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234\",\n+\t\t\t\t\"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n \t@Test\n-\tpublic void testAllFieldsSelection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testProjectableJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ALL_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT timestamp6_col, decimal_col FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001,Java public for dummies,Tan Ah Teck,11.11,11\");\n-\t\texpected.add(\"1002,More Java for dummies,Tan Ah Teck,22.22,22\");\n-\t\texpected.add(\"1003,More Java for more dummies,Mohammad Ali,33.33,33\");\n-\t\texpected.add(\"1004,A Cup of Java,Kumar,44.44,44\");\n-\t\texpected.add(\"1005,A Teaspoon of Java,Kevin Jones,55.55,55\");\n-\t\texpected.add(\"1006,A Teaspoon of Java 1.4,Kevin Jones,66.66,66\");\n-\t\texpected.add(\"1007,A Teaspoon of Java 1.5,Kevin Jones,77.77,77\");\n-\t\texpected.add(\"1008,A Teaspoon of Java 1.6,Kevin Jones,88.88,88\");\n-\t\texpected.add(\"1009,A Teaspoon of Java 1.7,Kevin Jones,99.99,99\");\n-\t\texpected.add(\"1010,A Teaspoon of Java 1.8,Kevin Jones,null,1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"2020-01-01T15:35:00.123456,100.1234\",\n+\t\t\t\t\"2020-01-01T15:36:01.123456,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n+\t@Test(expected = TableException.class)\n+\tpublic void testInvalidPrecisionOfJDBCSource() throws Exception {", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ1MDQzNw==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377450437", "bodyText": "I think we don't need this test any more, because is already covered by JDBCDataTypeTest.", "author": "wuchong", "createdAt": "2020-02-11T05:25:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5ODM0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTA2OQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376199069", "bodyText": "throws ValidationException on the method signature. And please add a description about the exception in the javadoc.", "author": "wuchong", "createdAt": "2020-02-07T03:28:03Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "diffHunk": "@@ -35,6 +37,15 @@\n \t */\n \tboolean canHandle(String url);\n \n+\t/**\n+\t * Check if this dialect instance support a specific data type in table schema.\n+\t *\n+\t * @param schema the table schema\n+\t */\n+\tdefault void validate(TableSchema schema) {", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MDYwNw==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376680607", "bodyText": "ValidationException is a RuntimeException, adding it to method signature won' t force caller the check it. Maybe we just need to add it in java doc like LogicalTypeParser.parse and FieldInfoUtils.validateInputTypeInfo ?", "author": "libenchao", "createdAt": "2020-02-08T02:34:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTA2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTA5OA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376199098", "bodyText": "remove emtpy line.", "author": "wuchong", "createdAt": "2020-02-07T03:28:10Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "diffHunk": "@@ -35,6 +37,15 @@\n \t */\n \tboolean canHandle(String url);\n \n+\t/**\n+\t * Check if this dialect instance support a specific data type in table schema.\n+\t *", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwMjUwNQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376202505", "bodyText": "We don't need to return if the return type is void.", "author": "wuchong", "createdAt": "2020-02-07T03:46:24Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "diffHunk": "@@ -35,6 +37,15 @@\n \t */\n \tboolean canHandle(String url);\n \n+\t/**\n+\t * Check if this dialect instance support a specific data type in table schema.\n+\t *\n+\t * @param schema the table schema\n+\t */\n+\tdefault void validate(TableSchema schema) {\n+\t\treturn;", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwMzcwNQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376203705", "bodyText": "It seems that the validation logic is the same, maybe we can refactor it a bit more to have a AbstractJDBCDialect which implements JDBCDialect.\n\tprivate abstract static class AbstractDialect implements JDBCDialect {\n\n\t\t@Override\n\t\tpublic void validate(TableSchema schema) throws ValidationException {\n\t\t\t// implement the common validation logic here\n\t\t}\n\t\t\n\t\tpublic abstract int maxDecimalPrecision();\n\t\t\n\t\tpublic abstract int minDecimalPrecision();\n\t\t\n\t\tpublic abstract int maxTimestampPrecision();\n\t\t\n\t\tpublic abstract int minTimestampPrecision();\n\t\t\n\t\tpublic abstract List<LogicalTypeRoot> unsupportedTypes();\n\t}", "author": "wuchong", "createdAt": "2020-02-07T03:52:30Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -70,11 +105,33 @@ public String quoteIdentifier(String identifier) {\n \n \t\tprivate static final long serialVersionUID = 1L;\n \n+\t\tprivate static final int MAX_MYSQL_TIMESTAMP_PRECISION = 6;\n+\n+\t\tprivate static final int MIN_MYSQL_TIMESTAMP_PRECISION = 0;\n+\n \t\t@Override\n \t\tpublic boolean canHandle(String url) {\n \t\t\treturn url.startsWith(\"jdbc:mysql:\");\n \t\t}\n \n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\t\t\t\tif (TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > MAX_MYSQL_TIMESTAMP_PRECISION) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of range [%d, %d].\",\n+\t\t\t\t\t\t\t\t\t\tfieldName,\n+\t\t\t\t\t\t\t\t\t\tMIN_MYSQL_TIMESTAMP_PRECISION,\n+\t\t\t\t\t\t\t\t\t\tMAX_MYSQL_TIMESTAMP_PRECISION));\n+\t\t\t\t\t}\n+\t\t\t\t}", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNDE1NA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376204154", "bodyText": "Could you add a comment above these constants that includes documentation link describes the precision? So that we can have the single truth.", "author": "wuchong", "createdAt": "2020-02-07T03:55:16Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -50,11 +60,36 @@\n \n \t\tprivate static final long serialVersionUID = 1L;\n \n+\t\tprivate static final int MAX_DERBY_DECIMAL_PRECISION = 31;\n+\n+\t\tprivate static final int MIN_DERBY_DECIMAL_PRECISION = 1;", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNTUwNQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376205505", "bodyText": "Should we use stat.executeUdpate?", "author": "wuchong", "createdAt": "2020-02-07T04:03:04Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg5MzA5OA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376893098", "bodyText": "execute can be used for any SQL statement, but executeUpdate is more clearly here. I will fix this.", "author": "docete", "createdAt": "2020-02-10T07:02:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNTUwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNjQzOA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376206438", "bodyText": "Do we really need this? Is there any error messages thrown when run these tests?", "author": "wuchong", "createdAt": "2020-02-07T04:08:47Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg5NDE2OQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376894169", "bodyText": "We add this to get rid of derby.log or the UT will end up with the derby.log file in the root of the project (flink-jdbc).", "author": "docete", "createdAt": "2020-02-10T07:07:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNjQzOA=="}], "type": "inlineReview"}, {"oid": "905972ad292b8657c04b5c4165371f0d8088f824", "url": "https://github.com/apache/flink/commit/905972ad292b8657c04b5c4165371f0d8088f824", "message": "fixup: remove the old style RowTypeInfo", "committedDate": "2020-02-07T08:21:35Z", "type": "commit"}, {"oid": "1aa14851215234acf47801d002c436bfd51588c2", "url": "https://github.com/apache/flink/commit/1aa14851215234acf47801d002c436bfd51588c2", "message": "fixup: remove useless code", "committedDate": "2020-02-07T08:28:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MDk1OQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376680959", "bodyText": "seems indent is not correct.", "author": "libenchao", "createdAt": "2020-02-08T02:39:41Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n \t}\n \n \t@Test\n-\tpublic void testFieldsProjection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"double_col DOUBLE,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ID_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT * FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001\");\n-\t\texpected.add(\"1002\");\n-\t\texpected.add(\"1003\");\n-\t\texpected.add(\"1004\");\n-\t\texpected.add(\"1005\");\n-\t\texpected.add(\"1006\");\n-\t\texpected.add(\"1007\");\n-\t\texpected.add(\"1008\");\n-\t\texpected.add(\"1009\");\n-\t\texpected.add(\"1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234\",\n+\t\t\t\t\"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n \t@Test\n-\tpublic void testAllFieldsSelection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testProjectableJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ALL_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT timestamp6_col, decimal_col FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001,Java public for dummies,Tan Ah Teck,11.11,11\");\n-\t\texpected.add(\"1002,More Java for dummies,Tan Ah Teck,22.22,22\");\n-\t\texpected.add(\"1003,More Java for more dummies,Mohammad Ali,33.33,33\");\n-\t\texpected.add(\"1004,A Cup of Java,Kumar,44.44,44\");\n-\t\texpected.add(\"1005,A Teaspoon of Java,Kevin Jones,55.55,55\");\n-\t\texpected.add(\"1006,A Teaspoon of Java 1.4,Kevin Jones,66.66,66\");\n-\t\texpected.add(\"1007,A Teaspoon of Java 1.5,Kevin Jones,77.77,77\");\n-\t\texpected.add(\"1008,A Teaspoon of Java 1.6,Kevin Jones,88.88,88\");\n-\t\texpected.add(\"1009,A Teaspoon of Java 1.7,Kevin Jones,99.99,99\");\n-\t\texpected.add(\"1010,A Teaspoon of Java 1.8,Kevin Jones,null,1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"2020-01-01T15:35:00.123456,100.1234\",\n+\t\t\t\t\"2020-01-01T15:36:01.123456,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n+\t@Test(expected = TableException.class)\n+\tpublic void testInvalidPrecisionOfJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg5NzcwOA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376897708", "bodyText": "nice catch", "author": "docete", "createdAt": "2020-02-10T07:20:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MDk1OQ=="}], "type": "inlineReview"}, {"oid": "a12d3d5cee04047c93c01119ad07332a23b7deb0", "url": "https://github.com/apache/flink/commit/a12d3d5cee04047c93c01119ad07332a23b7deb0", "message": "[FLINK-15445][connectors/jdbc] Refactor the validation of data types for dialects", "committedDate": "2020-02-10T06:48:07Z", "type": "commit"}, {"oid": "6336abbade5c71df3bd26fad40209e89642810d8", "url": "https://github.com/apache/flink/commit/6336abbade5c71df3bd26fad40209e89642810d8", "message": "fixup: address Jark&benchao's comments", "committedDate": "2020-02-10T07:23:18Z", "type": "commit"}, {"oid": "330adf7ac3582db6b47a093e6f342351a3ba7e43", "url": "https://github.com/apache/flink/commit/330adf7ac3582db6b47a093e6f342351a3ba7e43", "message": "fixup: checkstyle", "committedDate": "2020-02-10T08:50:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDA1MQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377440051", "bodyText": "Could we just simply dt.getLogicalType() instanceof VarBinaryType  to match it is a VarBinaryType? I think currently there isn't a LegacyTypeInformationType which is VARBINARY.  The same to the below if branches.", "author": "wuchong", "createdAt": "2020-02-11T04:13:56Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()", "originalCommit": "330adf7ac3582db6b47a093e6f342351a3ba7e43", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzU4NTIzNw==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377585237", "bodyText": "Yes, I think so. Will update soon.", "author": "docete", "createdAt": "2020-02-11T11:44:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDA1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDYzMg==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377440632", "bodyText": "Improve the error message a bit more:\nString.format(\"The precision of filed '%s' is out of the TIMESTAMP precision range [%d, %d] supported by the %s dialect.\",\nfieldName,\nminTimestampPrecision(),\nmaxTimestampPrecision(),\ndialectName);", "author": "wuchong", "createdAt": "2020-02-11T04:18:24Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n+\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n+\t\t\t\t\t\t&& DECIMAL == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((DecimalType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > maxDecimalPrecision()\n+\t\t\t\t\t\t\t|| precision < minDecimalPrecision()) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n+\t\t\t\t\t\t\t\t\t\tfieldName,\n+\t\t\t\t\t\t\t\t\t\tminDecimalPrecision(),\n+\t\t\t\t\t\t\t\t\t\tmaxDecimalPrecision()));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n+\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n+\t\t\t\t\t\t&& TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > maxTimestampPrecision()\n+\t\t\t\t\t\t\t|| precision < minTimestampPrecision()) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",", "originalCommit": "330adf7ac3582db6b47a093e6f342351a3ba7e43", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDcwOA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377440708", "bodyText": "The same to the DECIMAL type.", "author": "wuchong", "createdAt": "2020-02-11T04:19:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDYzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ1MDUzOA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377450538", "bodyText": "String.format(\"The %s dialect doesn't support type: %s.\", dialectName, dt.toString())", "author": "wuchong", "createdAt": "2020-02-11T05:25:55Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));", "originalCommit": "330adf7ac3582db6b47a093e6f342351a3ba7e43", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5e538c4875e6241d494585c6e2a8f586ac078131", "url": "https://github.com/apache/flink/commit/5e538c4875e6241d494585c6e2a8f586ac078131", "message": "fixup: address jark's comments about improving error msg", "committedDate": "2020-02-11T12:30:59Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzYxOTE1NA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377619154", "bodyText": "Nit: don't -> doesn't", "author": "wuchong", "createdAt": "2020-02-11T13:01:23Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -71,43 +71,48 @@ public void validate(TableSchema schema) throws ValidationException {\n \t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n \t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n \t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n-\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n-\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n-\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\t\t(dt.getLogicalType() instanceof VarBinaryType\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength())) {\n \t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t\t\t\tString.format(\"The %s dialect don't support type: %s.\",", "originalCommit": "5e538c4875e6241d494585c6e2a8f586ac078131", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2f27266467ee029955ac26e1acc0e64628d8f2dd", "url": "https://github.com/apache/flink/commit/2f27266467ee029955ac26e1acc0e64628d8f2dd", "message": "fixup: checkstyle", "committedDate": "2020-02-12T02:54:08Z", "type": "commit"}, {"oid": "5f30317b81fbc0dcf2d518adb8a0ddde1847bfe6", "url": "https://github.com/apache/flink/commit/5f30317b81fbc0dcf2d518adb8a0ddde1847bfe6", "message": "fixup: Nit don't -> doesn't", "committedDate": "2020-02-12T03:14:58Z", "type": "commit"}, {"oid": "7790797e5e90af02bc94b48f5326077573600f25", "url": "https://github.com/apache/flink/commit/7790797e5e90af02bc94b48f5326077573600f25", "message": "fixup: checkstyle", "committedDate": "2020-02-12T03:17:38Z", "type": "commit"}]}