{"pr_number": 13296, "pr_title": "[FLINK-18774][format][debezium] Support debezium-avro format", "pr_createdAt": "2020-09-01T08:39:54Z", "pr_url": "https://github.com/apache/flink/pull/13296", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDg0MTE2Nw==", "url": "https://github.com/apache/flink/pull/13296#discussion_r490841167", "bodyText": "Remove.", "author": "wuchong", "createdAt": "2020-09-18T10:04:03Z", "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/debezium/DebeziumAvroDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent.debezium;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.registry.confluent.ConfluentRegistryAvroDeserializationSchema;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.RowKind;\n+import org.apache.flink.util.Collector;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static java.lang.String.format;\n+import static org.apache.flink.table.types.utils.TypeConversions.fromLogicalToDataType;\n+\n+/**\n+ * Deserialization schema from Debezium Avro to Flink Table/SQL internal data structure {@link RowData}.\n+ * The deserialization schema knows Debezium's schema definition and can extract the database data\n+ * and convert into {@link RowData} with {@link RowKind}.\n+ * <p>", "originalCommit": "8003ac0791570594085b8672d4d47c33fc2bbaf2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDg0MTcyNQ==", "url": "https://github.com/apache/flink/pull/13296#discussion_r490841725", "bodyText": "Add . at the end.\nPlease check the checkstyle agian. The compile build is failed.", "author": "wuchong", "createdAt": "2020-09-18T10:05:03Z", "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/debezium/DebeziumAvroDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent.debezium;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.registry.confluent.ConfluentRegistryAvroDeserializationSchema;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.RowKind;\n+import org.apache.flink.util.Collector;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static java.lang.String.format;\n+import static org.apache.flink.table.types.utils.TypeConversions.fromLogicalToDataType;\n+\n+/**\n+ * Deserialization schema from Debezium Avro to Flink Table/SQL internal data structure {@link RowData}.\n+ * The deserialization schema knows Debezium's schema definition and can extract the database data\n+ * and convert into {@link RowData} with {@link RowKind}.\n+ * <p>\n+ * <p>Deserializes a <code>byte[]</code> message as a JSON object and reads\n+ * the specified fields.\n+ * <p>\n+ * <p>Failures during deserialization are forwarded as wrapped IOExceptions.\n+ *\n+ * @see <a href=\"https://debezium.io/\">Debezium</a>\n+ */\n+@Internal\n+public final class DebeziumAvroDeserializationSchema implements DeserializationSchema<RowData> {\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/**\n+\t * snapshot read", "originalCommit": "8003ac0791570594085b8672d4d47c33fc2bbaf2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDg0MjE1NQ==", "url": "https://github.com/apache/flink/pull/13296#discussion_r490842155", "bodyText": "Add one more indent for method arguments. Please check other methods.", "author": "wuchong", "createdAt": "2020-09-18T10:05:54Z", "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/debezium/DebeziumAvroFormatFactory.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent.debezium;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.DecodingFormat;\n+import org.apache.flink.table.connector.format.EncodingFormat;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.DeserializationFormatFactory;\n+import org.apache.flink.table.factories.DynamicTableFactory;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.SerializationFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.RowKind;\n+\n+import java.util.HashSet;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static org.apache.flink.formats.avro.registry.confluent.RegistryAvroOptions.SCHEMA_REGISTRY_SUBJECT;\n+import static org.apache.flink.formats.avro.registry.confluent.RegistryAvroOptions.SCHEMA_REGISTRY_URL;\n+\n+/**\n+ * Format factory for providing configured instances of Debezium Avro to RowData {@link DeserializationSchema}.\n+ */\n+public class DebeziumAvroFormatFactory implements DeserializationFormatFactory, SerializationFormatFactory {\n+\n+\tpublic static final String IDENTIFIER = \"debezium-avro-confluent\";\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\t@Override\n+\tpublic DecodingFormat<DeserializationSchema<RowData>> createDecodingFormat(\n+\t\tDynamicTableFactory.Context context,", "originalCommit": "8003ac0791570594085b8672d4d47c33fc2bbaf2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM0NDE0MQ==", "url": "https://github.com/apache/flink/pull/13296#discussion_r496344141", "bodyText": "checkstyle  done.", "author": "caozhen1937", "createdAt": "2020-09-29T02:38:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDg0MjE1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDg0OTc3Ng==", "url": "https://github.com/apache/flink/pull/13296#discussion_r490849776", "bodyText": "We should also know the index of before and after. Shouldn't get the index dynamically.\nSee the implementation of debezium-json.", "author": "wuchong", "createdAt": "2020-09-18T10:21:41Z", "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/debezium/DebeziumAvroDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent.debezium;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.registry.confluent.ConfluentRegistryAvroDeserializationSchema;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.RowKind;\n+import org.apache.flink.util.Collector;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static java.lang.String.format;\n+import static org.apache.flink.table.types.utils.TypeConversions.fromLogicalToDataType;\n+\n+/**\n+ * Deserialization schema from Debezium Avro to Flink Table/SQL internal data structure {@link RowData}.\n+ * The deserialization schema knows Debezium's schema definition and can extract the database data\n+ * and convert into {@link RowData} with {@link RowKind}.\n+ * <p>\n+ * <p>Deserializes a <code>byte[]</code> message as a JSON object and reads\n+ * the specified fields.\n+ * <p>\n+ * <p>Failures during deserialization are forwarded as wrapped IOExceptions.\n+ *\n+ * @see <a href=\"https://debezium.io/\">Debezium</a>\n+ */\n+@Internal\n+public final class DebeziumAvroDeserializationSchema implements DeserializationSchema<RowData> {\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/**\n+\t * snapshot read\n+\t */\n+\tprivate static final String OP_READ = \"r\";\n+\t/**\n+\t * insert operation\n+\t */\n+\tprivate static final String OP_CREATE = \"c\";\n+\t/**\n+\t * update operation\n+\t */\n+\tprivate static final String OP_UPDATE = \"u\";\n+\t/**\n+\t * delete operation\n+\t */\n+\tprivate static final String OP_DELETE = \"d\";\n+\n+\tprivate static final String REPLICA_IDENTITY_EXCEPTION = \"The \\\"before\\\" field of %s message is null, \" +\n+\t\t\"if you are using Debezium Postgres Connector, \" +\n+\t\t\"please check the Postgres table has been set REPLICA IDENTITY to FULL level.\";\n+\n+\t/**\n+\t * The deserializer to deserialize Debezium Avro data.\n+\t */\n+\tprivate final AvroRowDataDeserializationSchema avroDeserializer;\n+\n+\t/**\n+\t * TypeInformation of the produced {@link RowData}.\n+\t **/\n+\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\n+\t/**\n+\t * Debezium Avro data rowType\n+\t */\n+\tprivate final RowType rowType;\n+\n+\n+\tpublic DebeziumAvroDeserializationSchema(\n+\t\t\tRowType rowType,\n+\t\t\tTypeInformation<RowData> resultTypeInfo,\n+\t\t\tString schemaRegistryUrl) {\n+\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\tthis.rowType = rowType;\n+\t\tRowType debeziumAvroRowType = createDebeziumAvroRowType(fromLogicalToDataType(rowType));\n+\n+\t\tthis.avroDeserializer = new AvroRowDataDeserializationSchema(\n+\t\t\tConfluentRegistryAvroDeserializationSchema.forGeneric(\n+\t\t\t\tAvroSchemaConverter.convertToSchema(debeziumAvroRowType),\n+\t\t\t\tschemaRegistryUrl),\n+\t\t\tAvroToRowDataConverters.createRowConverter(debeziumAvroRowType),\n+\t\t\tresultTypeInfo);\n+\t}\n+\n+\tpublic DebeziumAvroDeserializationSchema(\n+\t\t\tRowType rowType,\n+\t\t\tTypeInformation<RowData> resultTypeInfo,\n+\t\t\tAvroRowDataDeserializationSchema avroDeserializer) {\n+\t\tthis.rowType = rowType;\n+\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\tthis.avroDeserializer = avroDeserializer;\n+\t}\n+\n+\t@Override\n+\tpublic RowData deserialize(byte[] message) throws IOException {\n+\t\tthrow new RuntimeException(\n+\t\t\t\"Please invoke DeserializationSchema#deserialize(byte[], Collector<RowData>) instead.\");\n+\t}\n+\n+\t@Override\n+\tpublic void deserialize(byte[] message, Collector<RowData> out) throws IOException {\n+\n+\t\tif (message == null || message.length == 0) {\n+\t\t\t// skip tombstone messages\n+\t\t\treturn;\n+\t\t}\n+\t\ttry {\n+\t\t\tGenericRowData row = (GenericRowData) avroDeserializer.deserialize(message);\n+\n+\t\t\tGenericRowData before = null;\n+\t\t\tint index = rowType.getFieldIndex(\"before\");", "originalCommit": "8003ac0791570594085b8672d4d47c33fc2bbaf2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDg1MjY5NQ==", "url": "https://github.com/apache/flink/pull/13296#discussion_r490852695", "bodyText": "We need \"before\", because the DELETE should encode data into \"before\".", "author": "wuchong", "createdAt": "2020-09-18T10:27:09Z", "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/debezium/DebeziumAvroSerializationSchema.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent.debezium;\n+\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.formats.avro.AvroRowDataSerializationSchema;\n+import org.apache.flink.formats.avro.RowDataToAvroConverters;\n+import org.apache.flink.formats.avro.registry.confluent.ConfluentRegistryAvroSerializationSchema;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.RowKind;\n+\n+import java.util.Objects;\n+\n+import static org.apache.flink.table.types.utils.TypeConversions.fromLogicalToDataType;\n+\n+/**\n+ * Serialization schema from Flink Table/SQL internal data structure {@link RowData} to Debezium Avro.\n+ */\n+public class DebeziumAvroSerializationSchema implements SerializationSchema<RowData> {\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/**\n+\t * insert operation\n+\t */\n+\tprivate static final StringData OP_CREATE = StringData.fromString(\"c\");\n+\t/**\n+\t * delete operation\n+\t */\n+\tprivate static final StringData OP_DELETE = StringData.fromString(\"d\");\n+\n+\t/**\n+\t * The deserializer to deserialize Debezium Avro data.\n+\t */\n+\tprivate final AvroRowDataSerializationSchema avroSerializer;\n+\n+\tprivate transient GenericRowData reuse = new GenericRowData(2);\n+\n+\tpublic DebeziumAvroSerializationSchema(\n+\t\t\tRowType rowType,\n+\t\t\tString schemaRegistryUrl,\n+\t\t\tString schemaRegistrySubject) {\n+\t\tRowType debeziumAvroRowType = createDebeziumAvroRowType(fromLogicalToDataType(rowType));\n+\n+\t\tthis.avroSerializer = new AvroRowDataSerializationSchema(\n+\t\t\tdebeziumAvroRowType,\n+\t\t\tConfluentRegistryAvroSerializationSchema.forGeneric(\n+\t\t\t\tschemaRegistrySubject,\n+\t\t\t\tAvroSchemaConverter.convertToSchema(debeziumAvroRowType),\n+\t\t\t\tschemaRegistryUrl),\n+\t\t\tRowDataToAvroConverters.createRowConverter(debeziumAvroRowType));\n+\t}\n+\n+\tpublic DebeziumAvroSerializationSchema(AvroRowDataSerializationSchema avroSerializer) {\n+\t\tthis.avroSerializer = avroSerializer;\n+\t}\n+\n+\t@Override\n+\tpublic void open(InitializationContext context) throws Exception {\n+\t}\n+\n+\t@Override\n+\tpublic byte[] serialize(RowData element) {\n+\t\treuse.setField(0, element);\n+\t\treuse.setField(1, rowKind2String(element.getRowKind()));\n+\t\treturn avroSerializer.serialize(reuse);\n+\t}\n+\n+\tprivate StringData rowKind2String(RowKind rowKind) {\n+\t\tswitch (rowKind) {\n+\t\t\tcase INSERT:\n+\t\t\tcase UPDATE_AFTER:\n+\t\t\t\treturn OP_CREATE;\n+\t\t\tcase UPDATE_BEFORE:\n+\t\t\tcase DELETE:\n+\t\t\t\treturn OP_DELETE;\n+\t\t\tdefault:\n+\t\t\t\tthrow new UnsupportedOperationException(\"Unsupported operation '\" + rowKind + \"' for row kind.\");\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif (this == o) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tif (o == null || getClass() != o.getClass()) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tDebeziumAvroSerializationSchema that = (DebeziumAvroSerializationSchema) o;\n+\t\treturn Objects.equals(avroSerializer, that.avroSerializer);\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(avroSerializer);\n+\t}\n+\n+\tpublic static RowType createDebeziumAvroRowType(DataType dataType) {\n+\t\t// Debezium Avro contains other information, e.g. \"source\", \"ts_ms\"\n+\t\t// but we don't need them\n+\t\t// and we don't need \"before\" , because can not support UPDATE_BEFORE,UPDATE_AFTER", "originalCommit": "8003ac0791570594085b8672d4d47c33fc2bbaf2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTIxNzk0OA==", "url": "https://github.com/apache/flink/pull/13296#discussion_r499217948", "bodyText": "@wuchong  I found that when Using canal-json,debezium-json,maxwell-json serialization, the RowData of DELETE was not put old data into JSON message. I will open an issue to track it.\nhttps://issues.apache.org/jira/browse/FLINK-19500\nSorry,not a problem,I got it wrong.", "author": "caozhen1937", "createdAt": "2020-10-04T08:04:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDg1MjY5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDg1NDE4MA==", "url": "https://github.com/apache/flink/pull/13296#discussion_r490854180", "bodyText": "Use REGISTRY_URL instead of extract the value from options. This is used for asserting which should be a constant value.", "author": "wuchong", "createdAt": "2020-09-18T10:29:43Z", "path": "flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/debezium/DebeziumAvroFormatFactoryTest.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent.debezium;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.formats.avro.registry.confluent.RegistryAvroOptions;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.TestDynamicTableFactory;\n+import org.apache.flink.table.runtime.connector.sink.SinkRuntimeProviderContext;\n+import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.TestLogger;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+\n+import static junit.framework.TestCase.assertEquals;\n+import static org.apache.flink.core.testutils.FlinkMatchers.containsCause;\n+\n+/**\n+ * Tests for {@link DebeziumAvroFormatFactory}.\n+ */\n+public class DebeziumAvroFormatFactoryTest extends TestLogger {\n+\t@Rule\n+\tpublic ExpectedException thrown = ExpectedException.none();\n+\n+\tprivate static final TableSchema SCHEMA = TableSchema.builder()\n+\t\t.field(\"a\", DataTypes.STRING())\n+\t\t.field(\"b\", DataTypes.INT())\n+\t\t.field(\"c\", DataTypes.BOOLEAN())\n+\t\t.build();\n+\n+\tprivate static final RowType ROW_TYPE = (RowType) SCHEMA.toRowDataType().getLogicalType();\n+\n+\tprivate static final String SUBJECT = \"test-debezium-avro\";\n+\tprivate static final String REGISTRY_URL = \"http://localhost:8081\";\n+\n+\t@Test\n+\tpublic void testSeDeSchema() {\n+\t\tfinal Map<String, String> options = getAllOptions();\n+\t\tString schemaRegistryUrl = options.get(DebeziumAvroFormatFactory.IDENTIFIER + \".\" + RegistryAvroOptions.SCHEMA_REGISTRY_URL.key());\n+\t\tString schemaRegistrySubject = options.get(DebeziumAvroFormatFactory.IDENTIFIER + \".\" + RegistryAvroOptions.SCHEMA_REGISTRY_SUBJECT.key());\n+\n+\t\tfinal DebeziumAvroDeserializationSchema expectedDeser = new DebeziumAvroDeserializationSchema(\n+\t\t\tROW_TYPE,\n+\t\t\tInternalTypeInfo.of(ROW_TYPE),\n+\t\t\tschemaRegistryUrl);", "originalCommit": "8003ac0791570594085b8672d4d47c33fc2bbaf2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDg1NDM2OQ==", "url": "https://github.com/apache/flink/pull/13296#discussion_r490854369", "bodyText": "Use SUBJECT instead.", "author": "wuchong", "createdAt": "2020-09-18T10:30:08Z", "path": "flink-formats/flink-avro-confluent-registry/src/test/java/org/apache/flink/formats/avro/registry/confluent/debezium/DebeziumAvroFormatFactoryTest.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent.debezium;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.serialization.SerializationSchema;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.formats.avro.registry.confluent.RegistryAvroOptions;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.FactoryUtil;\n+import org.apache.flink.table.factories.TestDynamicTableFactory;\n+import org.apache.flink.table.runtime.connector.sink.SinkRuntimeProviderContext;\n+import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.TestLogger;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+\n+import static junit.framework.TestCase.assertEquals;\n+import static org.apache.flink.core.testutils.FlinkMatchers.containsCause;\n+\n+/**\n+ * Tests for {@link DebeziumAvroFormatFactory}.\n+ */\n+public class DebeziumAvroFormatFactoryTest extends TestLogger {\n+\t@Rule\n+\tpublic ExpectedException thrown = ExpectedException.none();\n+\n+\tprivate static final TableSchema SCHEMA = TableSchema.builder()\n+\t\t.field(\"a\", DataTypes.STRING())\n+\t\t.field(\"b\", DataTypes.INT())\n+\t\t.field(\"c\", DataTypes.BOOLEAN())\n+\t\t.build();\n+\n+\tprivate static final RowType ROW_TYPE = (RowType) SCHEMA.toRowDataType().getLogicalType();\n+\n+\tprivate static final String SUBJECT = \"test-debezium-avro\";\n+\tprivate static final String REGISTRY_URL = \"http://localhost:8081\";\n+\n+\t@Test\n+\tpublic void testSeDeSchema() {\n+\t\tfinal Map<String, String> options = getAllOptions();\n+\t\tString schemaRegistryUrl = options.get(DebeziumAvroFormatFactory.IDENTIFIER + \".\" + RegistryAvroOptions.SCHEMA_REGISTRY_URL.key());\n+\t\tString schemaRegistrySubject = options.get(DebeziumAvroFormatFactory.IDENTIFIER + \".\" + RegistryAvroOptions.SCHEMA_REGISTRY_SUBJECT.key());\n+\n+\t\tfinal DebeziumAvroDeserializationSchema expectedDeser = new DebeziumAvroDeserializationSchema(\n+\t\t\tROW_TYPE,\n+\t\t\tInternalTypeInfo.of(ROW_TYPE),\n+\t\t\tschemaRegistryUrl);\n+\n+\t\tfinal DynamicTableSource actualSource = createTableSource(options);\n+\t\tassert actualSource instanceof TestDynamicTableFactory.DynamicTableSourceMock;\n+\t\tTestDynamicTableFactory.DynamicTableSourceMock scanSourceMock = (TestDynamicTableFactory.DynamicTableSourceMock) actualSource;\n+\n+\t\tDeserializationSchema<RowData> actualDeser = scanSourceMock.valueFormat\n+\t\t\t.createRuntimeDecoder(\n+\t\t\t\tScanRuntimeProviderContext.INSTANCE,\n+\t\t\t\tSCHEMA.toRowDataType());\n+\n+\t\tassertEquals(expectedDeser, actualDeser);\n+\n+\n+\t\tfinal DebeziumAvroSerializationSchema expectedSer = new DebeziumAvroSerializationSchema(\n+\t\t\tROW_TYPE,\n+\t\t\tschemaRegistryUrl,\n+\t\t\tschemaRegistrySubject", "originalCommit": "8003ac0791570594085b8672d4d47c33fc2bbaf2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDg2MjAwNA==", "url": "https://github.com/apache/flink/pull/13296#discussion_r490862004", "bodyText": "Why we need such a constructor? The avroDeserializer should be created by the debezium deserialization schema, otherwise it's error-prone.", "author": "wuchong", "createdAt": "2020-09-18T10:46:39Z", "path": "flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/debezium/DebeziumAvroDeserializationSchema.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.avro.registry.confluent.debezium;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.formats.avro.AvroRowDataDeserializationSchema;\n+import org.apache.flink.formats.avro.AvroToRowDataConverters;\n+import org.apache.flink.formats.avro.registry.confluent.ConfluentRegistryAvroDeserializationSchema;\n+import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.RowKind;\n+import org.apache.flink.util.Collector;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static java.lang.String.format;\n+import static org.apache.flink.table.types.utils.TypeConversions.fromLogicalToDataType;\n+\n+/**\n+ * Deserialization schema from Debezium Avro to Flink Table/SQL internal data structure {@link RowData}.\n+ * The deserialization schema knows Debezium's schema definition and can extract the database data\n+ * and convert into {@link RowData} with {@link RowKind}.\n+ * <p>\n+ * <p>Deserializes a <code>byte[]</code> message as a JSON object and reads\n+ * the specified fields.\n+ * <p>\n+ * <p>Failures during deserialization are forwarded as wrapped IOExceptions.\n+ *\n+ * @see <a href=\"https://debezium.io/\">Debezium</a>\n+ */\n+@Internal\n+public final class DebeziumAvroDeserializationSchema implements DeserializationSchema<RowData> {\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\t/**\n+\t * snapshot read\n+\t */\n+\tprivate static final String OP_READ = \"r\";\n+\t/**\n+\t * insert operation\n+\t */\n+\tprivate static final String OP_CREATE = \"c\";\n+\t/**\n+\t * update operation\n+\t */\n+\tprivate static final String OP_UPDATE = \"u\";\n+\t/**\n+\t * delete operation\n+\t */\n+\tprivate static final String OP_DELETE = \"d\";\n+\n+\tprivate static final String REPLICA_IDENTITY_EXCEPTION = \"The \\\"before\\\" field of %s message is null, \" +\n+\t\t\"if you are using Debezium Postgres Connector, \" +\n+\t\t\"please check the Postgres table has been set REPLICA IDENTITY to FULL level.\";\n+\n+\t/**\n+\t * The deserializer to deserialize Debezium Avro data.\n+\t */\n+\tprivate final AvroRowDataDeserializationSchema avroDeserializer;\n+\n+\t/**\n+\t * TypeInformation of the produced {@link RowData}.\n+\t **/\n+\tprivate final TypeInformation<RowData> resultTypeInfo;\n+\n+\t/**\n+\t * Debezium Avro data rowType\n+\t */\n+\tprivate final RowType rowType;\n+\n+\n+\tpublic DebeziumAvroDeserializationSchema(\n+\t\t\tRowType rowType,\n+\t\t\tTypeInformation<RowData> resultTypeInfo,\n+\t\t\tString schemaRegistryUrl) {\n+\t\tthis.resultTypeInfo = resultTypeInfo;\n+\t\tthis.rowType = rowType;\n+\t\tRowType debeziumAvroRowType = createDebeziumAvroRowType(fromLogicalToDataType(rowType));\n+\n+\t\tthis.avroDeserializer = new AvroRowDataDeserializationSchema(\n+\t\t\tConfluentRegistryAvroDeserializationSchema.forGeneric(\n+\t\t\t\tAvroSchemaConverter.convertToSchema(debeziumAvroRowType),\n+\t\t\t\tschemaRegistryUrl),\n+\t\t\tAvroToRowDataConverters.createRowConverter(debeziumAvroRowType),\n+\t\t\tresultTypeInfo);\n+\t}\n+\n+\tpublic DebeziumAvroDeserializationSchema(\n+\t\t\tRowType rowType,\n+\t\t\tTypeInformation<RowData> resultTypeInfo,\n+\t\t\tAvroRowDataDeserializationSchema avroDeserializer) {", "originalCommit": "8003ac0791570594085b8672d4d47c33fc2bbaf2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM0MTQyMQ==", "url": "https://github.com/apache/flink/pull/13296#discussion_r496341421", "bodyText": "This constructor is used for unit testing because schemaRegistryUrl cannot be used to retrieve the schema.", "author": "caozhen1937", "createdAt": "2020-09-29T02:27:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDg2MjAwNA=="}], "type": "inlineReview"}, {"oid": "53b4b43767a3140a1a736aa41cb6da0d1e2113b0", "url": "https://github.com/apache/flink/commit/53b4b43767a3140a1a736aa41cb6da0d1e2113b0", "message": "[FLINK-19684] fix AvroSchemaConverter#convertToSchema after merge", "committedDate": "2020-11-06T08:16:44Z", "type": "forcePushed"}, {"oid": "c57d805bea826af82626be82c5487e7259d3eaf9", "url": "https://github.com/apache/flink/commit/c57d805bea826af82626be82c5487e7259d3eaf9", "message": "[FLINK-18774][debezium-avro] Improve debezium-avro format implementation", "committedDate": "2020-11-07T13:52:02Z", "type": "forcePushed"}, {"oid": "3eb68994c05caed91cef945800d7cb586d617663", "url": "https://github.com/apache/flink/commit/3eb68994c05caed91cef945800d7cb586d617663", "message": "[FLINK-18774][debezium-avro] Improve debezium-avro format implementation", "committedDate": "2020-11-08T02:20:42Z", "type": "forcePushed"}, {"oid": "93471464568f5427eacac3f8e4100e8514b87099", "url": "https://github.com/apache/flink/commit/93471464568f5427eacac3f8e4100e8514b87099", "message": "[FLINK-18774][debezium-avro] Support debezium avro format\n\nThis closes #13296", "committedDate": "2020-11-08T03:40:56Z", "type": "commit"}, {"oid": "1d4db76b21faf3bb2835be6e529f1d2478237272", "url": "https://github.com/apache/flink/commit/1d4db76b21faf3bb2835be6e529f1d2478237272", "message": "[FLINK-18774][debezium-avro] Improve debezium-avro format implementation", "committedDate": "2020-11-08T03:41:57Z", "type": "commit"}, {"oid": "1d4db76b21faf3bb2835be6e529f1d2478237272", "url": "https://github.com/apache/flink/commit/1d4db76b21faf3bb2835be6e529f1d2478237272", "message": "[FLINK-18774][debezium-avro] Improve debezium-avro format implementation", "committedDate": "2020-11-08T03:41:57Z", "type": "forcePushed"}]}