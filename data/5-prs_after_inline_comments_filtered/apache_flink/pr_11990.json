{"pr_number": 11990, "pr_title": "[FLINK-17387][hive] Implement LookupableTableSource for Hive connector", "pr_createdAt": "2020-05-05T07:29:14Z", "pr_url": "https://github.com/apache/flink/pull/11990", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI5NDA5Ng==", "url": "https://github.com/apache/flink/pull/11990#discussion_r422294096", "bodyText": "does it mean it caches all rows of a hive file?", "author": "bowenli86", "createdAt": "2020-05-08T18:15:41Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final HiveTableInputFormat inputFormat;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\tprivate transient Map<RowData, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long cacheExpire;\n+\tprivate final Duration cacheTTL = Duration.ofHours(1);\n+\n+\tpublic HiveTableLookupFunction(HiveTableInputFormat inputFormat, String[] lookupKeys) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tString[] allFields = inputFormat.getFieldNames();\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, allFields.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> allFields[i], i -> i));\n+\t\tList<Integer> selectedIndices = Arrays.stream(inputFormat.getSelectedFields()).boxed().collect(Collectors.toList());\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not found in table schema\", Arrays.toString(lookupKeys));\n+\t\t\tindex = selectedIndices.indexOf(index);\n+\t\t\tPreconditions.checkArgument(index >= 0, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\tString[] allNames = inputFormat.getFieldNames();\n+\t\tDataType[] allTypes = inputFormat.getFieldTypes();\n+\t\tint[] selected = inputFormat.getSelectedFields();\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allTypes[i].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allNames[i]).toArray(String[]::new));\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tcacheExpire = -1;\n+\t}\n+\n+\tpublic void eval(Object... values) {\n+\t\tPreconditions.checkArgument(values.length == lookupCols.length, \"Number of values and lookup keys mismatch\");\n+\t\treloadCache();\n+\t\tRowData probeKey = GenericRowData.of(values);\n+\t\tList<RowData> matchedRows = cache.get(probeKey);\n+\t\tif (matchedRows != null) {\n+\t\t\tfor (RowData matchedRow : matchedRows) {\n+\t\t\t\tcollect(matchedRow);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void reloadCache() {", "originalCommit": "9a940870922bfb7e92ea199e3affd893cbadd745", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MjY5OA==", "url": "https://github.com/apache/flink/pull/11990#discussion_r422442698", "bodyText": "Not all rows of a hive file, but all rows of a hive table. If we only cache part of the data, we have to do full table scan whenever we have a cache miss. So I guess we might as well cache all the data.", "author": "lirui-apache", "createdAt": "2020-05-09T02:24:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI5NDA5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ1NDg0Ng==", "url": "https://github.com/apache/flink/pull/11990#discussion_r422454846", "bodyText": "I'm really concerned about caching the whole table, especially every parallelism does so.\nWe'd better add explicit documentation to warn users potential consequences and best practices.", "author": "bowenli86", "createdAt": "2020-05-09T05:24:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI5NDA5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ2NDI2Ng==", "url": "https://github.com/apache/flink/pull/11990#discussion_r422464266", "bodyText": "And your concern is valid. We will add explicit doc for that.\nBTW, in our internal implementation, we can do a partitioned lookup join, so that each task only has to cache data that belongs to a specific shuffle partition which reduces memory consumption. But I don' think that optimization can make it for 1.11.", "author": "lirui-apache", "createdAt": "2020-05-09T07:32:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI5NDA5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4Mzk3OQ==", "url": "https://github.com/apache/flink/pull/11990#discussion_r422583979", "bodyText": "@lirui-apache thanks for the explanation!", "author": "bowenli86", "createdAt": "2020-05-10T04:45:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI5NDA5Ng=="}], "type": "inlineReview"}, {"oid": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "url": "https://github.com/apache/flink/commit/c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "message": "make cache ttl configurable", "committedDate": "2020-05-09T03:07:23Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3OTExNg==", "url": "https://github.com/apache/flink/pull/11990#discussion_r423479116", "bodyText": "throw new UnsupportedOperationException", "author": "JingsongLi", "createdAt": "2020-05-12T05:58:08Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -382,4 +390,25 @@ public String explainSource() {\n \t\t}\n \t\treturn TableConnectorUtils.generateRuntimeName(getClass(), getTableSchema().getFieldNames()) + explain;\n \t}\n+\n+\t@Override\n+\tpublic TableFunction<RowData> getLookupFunction(String[] lookupKeys) {\n+\t\t// always use MR reader for the lookup function\n+\t\tList<HiveTablePartition> allPartitions = initAllPartitions();\n+\t\tDuration cacheTTL = Duration.ofMinutes(\n+\t\t\t\thiveTableCacheTTL != null ?\n+\t\t\t\t\t\tLong.parseLong(hiveTableCacheTTL) :\n+\t\t\t\t\t\tHiveOptions.LOOKUP_JOIN_CACHE_TTL.defaultValue());\n+\t\treturn new HiveTableLookupFunction(getInputFormat(allPartitions, true), lookupKeys, cacheTTL);\n+\t}\n+\n+\t@Override\n+\tpublic AsyncTableFunction<RowData> getAsyncLookupFunction(String[] lookupKeys) {\n+\t\treturn null;", "originalCommit": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3OTYzNQ==", "url": "https://github.com/apache/flink/pull/11990#discussion_r423479635", "bodyText": "move to FileSystemOptions", "author": "JingsongLi", "createdAt": "2020-05-12T05:59:41Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveOptions.java", "diffHunk": "@@ -45,4 +45,10 @@\n \t\t\tkey(\"table.exec.hive.infer-source-parallelism.max\")\n \t\t\t\t\t.defaultValue(1000)\n \t\t\t\t\t.withDescription(\"Sets max infer parallelism for source operator.\");\n+\n+\tpublic static final ConfigOption<Integer> LOOKUP_JOIN_CACHE_TTL =\n+\t\t\tkey(\"lookup.join.cache.ttl\")", "originalCommit": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3OTc1OA==", "url": "https://github.com/apache/flink/pull/11990#discussion_r423479758", "bodyText": "Duration", "author": "JingsongLi", "createdAt": "2020-05-12T06:00:03Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveOptions.java", "diffHunk": "@@ -45,4 +45,10 @@\n \t\t\tkey(\"table.exec.hive.infer-source-parallelism.max\")\n \t\t\t\t\t.defaultValue(1000)\n \t\t\t\t\t.withDescription(\"Sets max infer parallelism for source operator.\");\n+\n+\tpublic static final ConfigOption<Integer> LOOKUP_JOIN_CACHE_TTL =\n+\t\t\tkey(\"lookup.join.cache.ttl\")\n+\t\t\t\t\t.intType()", "originalCommit": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3OTk2Mg==", "url": "https://github.com/apache/flink/pull/11990#discussion_r423479962", "bodyText": "defaultValue should init here.\nCan construct a flink Configuration here.", "author": "JingsongLi", "createdAt": "2020-05-12T06:00:35Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -286,6 +293,7 @@ public boolean isLimitPushedDown() {\n \t\t\tList<String> partitionColNames = catalogTable.getPartitionKeys();\n \t\t\tTable hiveTable = client.getTable(dbName, tableName);\n \t\t\tProperties tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\t\thiveTableCacheTTL = tableProps.getProperty(HiveOptions.LOOKUP_JOIN_CACHE_TTL.key());", "originalCommit": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MDQxNw==", "url": "https://github.com/apache/flink/pull/11990#discussion_r423480417", "bodyText": "Can you move this class to table, and rename to FileSystemLookupFunction?", "author": "JingsongLi", "createdAt": "2020-05-12T06:02:05Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {", "originalCommit": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MTAyMA==", "url": "https://github.com/apache/flink/pull/11990#discussion_r423481020", "bodyText": "nextLoadTime?", "author": "JingsongLi", "createdAt": "2020-05-12T06:03:46Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final HiveTableInputFormat inputFormat;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\tprivate transient Map<RowData, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long cacheExpire;", "originalCommit": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MTIwMQ==", "url": "https://github.com/apache/flink/pull/11990#discussion_r423481201", "bodyText": "checkCacheReload?", "author": "JingsongLi", "createdAt": "2020-05-12T06:04:19Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final HiveTableInputFormat inputFormat;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\tprivate transient Map<RowData, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long cacheExpire;\n+\tprivate final Duration cacheTTL;\n+\n+\tpublic HiveTableLookupFunction(HiveTableInputFormat inputFormat, String[] lookupKeys, Duration cacheTTL) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tString[] allFields = inputFormat.getFieldNames();\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, allFields.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> allFields[i], i -> i));\n+\t\tList<Integer> selectedIndices = Arrays.stream(inputFormat.getSelectedFields()).boxed().collect(Collectors.toList());\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not found in table schema\", Arrays.toString(lookupKeys));\n+\t\t\tindex = selectedIndices.indexOf(index);\n+\t\t\tPreconditions.checkArgument(index >= 0, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.cacheTTL = cacheTTL;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\tString[] allNames = inputFormat.getFieldNames();\n+\t\tDataType[] allTypes = inputFormat.getFieldTypes();\n+\t\tint[] selected = inputFormat.getSelectedFields();\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allTypes[i].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allNames[i]).toArray(String[]::new));\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tcacheExpire = -1;\n+\t}\n+\n+\tpublic void eval(Object... values) {\n+\t\tPreconditions.checkArgument(values.length == lookupCols.length, \"Number of values and lookup keys mismatch\");\n+\t\treloadCache();", "originalCommit": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MjAzNg==", "url": "https://github.com/apache/flink/pull/11990#discussion_r423482036", "bodyText": "Maybe not GenericRowData.", "author": "JingsongLi", "createdAt": "2020-05-12T06:06:41Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final HiveTableInputFormat inputFormat;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\tprivate transient Map<RowData, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long cacheExpire;\n+\tprivate final Duration cacheTTL;\n+\n+\tpublic HiveTableLookupFunction(HiveTableInputFormat inputFormat, String[] lookupKeys, Duration cacheTTL) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tString[] allFields = inputFormat.getFieldNames();\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, allFields.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> allFields[i], i -> i));\n+\t\tList<Integer> selectedIndices = Arrays.stream(inputFormat.getSelectedFields()).boxed().collect(Collectors.toList());\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not found in table schema\", Arrays.toString(lookupKeys));\n+\t\t\tindex = selectedIndices.indexOf(index);\n+\t\t\tPreconditions.checkArgument(index >= 0, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.cacheTTL = cacheTTL;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\tString[] allNames = inputFormat.getFieldNames();\n+\t\tDataType[] allTypes = inputFormat.getFieldTypes();\n+\t\tint[] selected = inputFormat.getSelectedFields();\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allTypes[i].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allNames[i]).toArray(String[]::new));\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tcacheExpire = -1;\n+\t}\n+\n+\tpublic void eval(Object... values) {\n+\t\tPreconditions.checkArgument(values.length == lookupCols.length, \"Number of values and lookup keys mismatch\");\n+\t\treloadCache();\n+\t\tRowData probeKey = GenericRowData.of(values);\n+\t\tList<RowData> matchedRows = cache.get(probeKey);\n+\t\tif (matchedRows != null) {\n+\t\t\tfor (RowData matchedRow : matchedRows) {\n+\t\t\t\tcollect(matchedRow);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@VisibleForTesting\n+\tpublic Duration getCacheTTL() {\n+\t\treturn cacheTTL;\n+\t}\n+\n+\tprivate void reloadCache() {\n+\t\tif (cacheExpire > System.currentTimeMillis()) {\n+\t\t\treturn;\n+\t\t}\n+\t\tcache.clear();\n+\t\ttry {\n+\t\t\tHiveTableInputSplit[] inputSplits = inputFormat.createInputSplits(1);\n+\t\t\tGenericRowData reuse = new GenericRowData(inputFormat.getSelectedFields().length);\n+\t\t\tfor (HiveTableInputSplit split : inputSplits) {\n+\t\t\t\tinputFormat.open(split);\n+\t\t\t\twhile (!inputFormat.reachedEnd()) {\n+\t\t\t\t\tGenericRowData row = (GenericRowData) inputFormat.nextRecord(reuse);", "originalCommit": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MjE4Mw==", "url": "https://github.com/apache/flink/pull/11990#discussion_r423482183", "bodyText": "You should use type serializer to copy.", "author": "JingsongLi", "createdAt": "2020-05-12T06:07:09Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final HiveTableInputFormat inputFormat;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\tprivate transient Map<RowData, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long cacheExpire;\n+\tprivate final Duration cacheTTL;\n+\n+\tpublic HiveTableLookupFunction(HiveTableInputFormat inputFormat, String[] lookupKeys, Duration cacheTTL) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tString[] allFields = inputFormat.getFieldNames();\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, allFields.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> allFields[i], i -> i));\n+\t\tList<Integer> selectedIndices = Arrays.stream(inputFormat.getSelectedFields()).boxed().collect(Collectors.toList());\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not found in table schema\", Arrays.toString(lookupKeys));\n+\t\t\tindex = selectedIndices.indexOf(index);\n+\t\t\tPreconditions.checkArgument(index >= 0, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.cacheTTL = cacheTTL;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\tString[] allNames = inputFormat.getFieldNames();\n+\t\tDataType[] allTypes = inputFormat.getFieldTypes();\n+\t\tint[] selected = inputFormat.getSelectedFields();\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allTypes[i].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allNames[i]).toArray(String[]::new));\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tcacheExpire = -1;\n+\t}\n+\n+\tpublic void eval(Object... values) {\n+\t\tPreconditions.checkArgument(values.length == lookupCols.length, \"Number of values and lookup keys mismatch\");\n+\t\treloadCache();\n+\t\tRowData probeKey = GenericRowData.of(values);\n+\t\tList<RowData> matchedRows = cache.get(probeKey);\n+\t\tif (matchedRows != null) {\n+\t\t\tfor (RowData matchedRow : matchedRows) {\n+\t\t\t\tcollect(matchedRow);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@VisibleForTesting\n+\tpublic Duration getCacheTTL() {\n+\t\treturn cacheTTL;\n+\t}\n+\n+\tprivate void reloadCache() {\n+\t\tif (cacheExpire > System.currentTimeMillis()) {\n+\t\t\treturn;\n+\t\t}\n+\t\tcache.clear();\n+\t\ttry {\n+\t\t\tHiveTableInputSplit[] inputSplits = inputFormat.createInputSplits(1);\n+\t\t\tGenericRowData reuse = new GenericRowData(inputFormat.getSelectedFields().length);\n+\t\t\tfor (HiveTableInputSplit split : inputSplits) {\n+\t\t\t\tinputFormat.open(split);\n+\t\t\t\twhile (!inputFormat.reachedEnd()) {\n+\t\t\t\t\tGenericRowData row = (GenericRowData) inputFormat.nextRecord(reuse);\n+\t\t\t\t\tGenericRowData key = extractKey(row);\n+\t\t\t\t\tList<RowData> rows = cache.computeIfAbsent(key, k -> new ArrayList<>());\n+\t\t\t\t\trows.add(copyReference(row));\n+\t\t\t\t}\n+\t\t\t\tinputFormat.close();\n+\t\t\t}\n+\t\t\tcacheExpire = System.currentTimeMillis() + cacheTTL.toMillis();\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new FlinkHiveException(\"Failed to load hive table into cache\", e);\n+\t\t}\n+\t}\n+\n+\tprivate GenericRowData extractKey(GenericRowData row) {\n+\t\tGenericRowData key = new GenericRowData(lookupCols.length);\n+\t\tfor (int i = 0; i < lookupCols.length; i++) {\n+\t\t\tkey.setField(i, row.getField(lookupCols[i]));\n+\t\t}\n+\t\treturn key;\n+\t}\n+\n+\tprivate static GenericRowData copyReference(GenericRowData rowData) {", "originalCommit": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MjMxNg==", "url": "https://github.com/apache/flink/pull/11990#discussion_r423482316", "bodyText": "It is external objects, instead of internal.", "author": "JingsongLi", "createdAt": "2020-05-12T06:07:31Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableLookupFunction.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connectors.hive.FlinkHiveException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for Hive tables.\n+ */\n+public class HiveTableLookupFunction extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final HiveTableInputFormat inputFormat;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\tprivate transient Map<RowData, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long cacheExpire;\n+\tprivate final Duration cacheTTL;\n+\n+\tpublic HiveTableLookupFunction(HiveTableInputFormat inputFormat, String[] lookupKeys, Duration cacheTTL) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tString[] allFields = inputFormat.getFieldNames();\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, allFields.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> allFields[i], i -> i));\n+\t\tList<Integer> selectedIndices = Arrays.stream(inputFormat.getSelectedFields()).boxed().collect(Collectors.toList());\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not found in table schema\", Arrays.toString(lookupKeys));\n+\t\t\tindex = selectedIndices.indexOf(index);\n+\t\t\tPreconditions.checkArgument(index >= 0, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.cacheTTL = cacheTTL;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\tString[] allNames = inputFormat.getFieldNames();\n+\t\tDataType[] allTypes = inputFormat.getFieldTypes();\n+\t\tint[] selected = inputFormat.getSelectedFields();\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allTypes[i].getLogicalType()).toArray(LogicalType[]::new),\n+\t\t\t\tArrays.stream(selected).mapToObj(i -> allNames[i]).toArray(String[]::new));\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tcacheExpire = -1;\n+\t}\n+\n+\tpublic void eval(Object... values) {", "originalCommit": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ4MjQ0Ng==", "url": "https://github.com/apache/flink/pull/11990#discussion_r423482446", "bodyText": "Please add test for string key.", "author": "JingsongLi", "createdAt": "2020-05-12T06:07:50Z", "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveLookupJoinTest.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.connectors.hive.read.HiveTableLookupFunction;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableUtils;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.hive.HiveCatalog;\n+import org.apache.flink.table.catalog.hive.HiveTestUtils;\n+import org.apache.flink.table.factories.TableSourceFactoryContextImpl;\n+import org.apache.flink.table.planner.factories.utils.TestCollectionTableFactory;\n+import org.apache.flink.types.Row;\n+\n+import com.klarna.hiverunner.HiveShell;\n+import com.klarna.hiverunner.annotations.HiveSQL;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import java.time.Duration;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test lookup join of hive tables.\n+ */\n+@RunWith(FlinkStandaloneHiveRunner.class)\n+public class HiveLookupJoinTest {\n+\n+\t@HiveSQL(files = {})\n+\tprivate static HiveShell hiveShell;\n+\n+\t@Test\n+\tpublic void test() throws Exception {\n+\t\tEnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();\n+\t\tTableEnvironment tableEnv = TableEnvironment.create(settings);\n+\t\tHiveCatalog hiveCatalog = HiveTestUtils.createHiveCatalog(hiveShell.getHiveConf());\n+\t\ttableEnv.registerCatalog(hiveCatalog.getName(), hiveCatalog);\n+\t\ttableEnv.useCatalog(hiveCatalog.getName());\n+\n+\t\thiveShell.execute(String.format(\"create table build (x int,y string,z int) tblproperties ('%s'='5')\",\n+\t\t\t\tHiveOptions.LOOKUP_JOIN_CACHE_TTL.key()));\n+\n+\t\t// verify we properly configured the cache TTL\n+\t\tObjectIdentifier tableIdentifier = ObjectIdentifier.of(hiveCatalog.getName(), \"default\", \"build\");\n+\t\tCatalogTable catalogTable = (CatalogTable) hiveCatalog.getTable(tableIdentifier.toObjectPath());\n+\t\tHiveTableSource hiveTableSource = (HiveTableSource) ((HiveTableFactory) hiveCatalog.getTableFactory().get()).createTableSource(\n+\t\t\t\tnew TableSourceFactoryContextImpl(tableIdentifier, catalogTable, tableEnv.getConfig().getConfiguration()));\n+\t\tHiveTableLookupFunction lookupFunction = (HiveTableLookupFunction) hiveTableSource.getLookupFunction(new String[]{\"x\"});\n+\t\tassertEquals(Duration.ofMinutes(5), lookupFunction.getCacheTTL());\n+\n+\t\ttry {\n+\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"default\", \"build\")\n+\t\t\t\t\t.addRow(new Object[]{1, \"a\", 10})\n+\t\t\t\t\t.addRow(new Object[]{2, \"a\", 21})\n+\t\t\t\t\t.addRow(new Object[]{2, \"b\", 22})\n+\t\t\t\t\t.addRow(new Object[]{3, \"c\", 33})\n+\t\t\t\t\t.commit();\n+\n+\t\t\tTestCollectionTableFactory.initData(Arrays.asList(Row.of(1, 1), Row.of(1, 0), Row.of(2, 1), Row.of(2, 3), Row.of(3, 1), Row.of(4, 4)));\n+\t\t\ttableEnv.sqlUpdate(\"create table default_catalog.default_database.probe (x int,y int,p as proctime()) with ('connector'='COLLECTION','is-bounded' = 'false')\");\n+\n+\t\t\tList<Row> results = TableUtils.collectToList(tableEnv.sqlQuery(\"select p.x,p.y from default_catalog.default_database.probe as p join \" +", "originalCommit": "c9a2b1d53715e766991efb85ec98ba96c14f0f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3591a7e419816e0dc6a13110e180df331726fa44", "url": "https://github.com/apache/flink/commit/3591a7e419816e0dc6a13110e180df331726fa44", "message": "address comments", "committedDate": "2020-05-12T15:41:57Z", "type": "forcePushed"}, {"oid": "56807c3ce9342de2d968ebc87dc08526d41a34b3", "url": "https://github.com/apache/flink/commit/56807c3ce9342de2d968ebc87dc08526d41a34b3", "message": "[FLINK-17387][hive] Implement LookupableTableSource for Hive connector", "committedDate": "2020-05-14T09:09:26Z", "type": "commit"}, {"oid": "68710aef41a6071c54efa41b27688d48d58aceae", "url": "https://github.com/apache/flink/commit/68710aef41a6071c54efa41b27688d48d58aceae", "message": "make cache ttl configurable", "committedDate": "2020-05-14T09:09:26Z", "type": "commit"}, {"oid": "050f12452d8acee1c8d1caea478a036fcc32722d", "url": "https://github.com/apache/flink/commit/050f12452d8acee1c8d1caea478a036fcc32722d", "message": "address comments", "committedDate": "2020-05-14T09:09:26Z", "type": "commit"}, {"oid": "660e6d3b46147c9f3d4aec7d095c38d16766aa73", "url": "https://github.com/apache/flink/commit/660e6d3b46147c9f3d4aec7d095c38d16766aa73", "message": "address comments", "committedDate": "2020-05-14T09:09:26Z", "type": "commit"}, {"oid": "362c7705fb7e930c5de9a4e384cf6dc368b4e9d3", "url": "https://github.com/apache/flink/commit/362c7705fb7e930c5de9a4e384cf6dc368b4e9d3", "message": "address comments", "committedDate": "2020-05-14T09:09:26Z", "type": "commit"}, {"oid": "897828752903134fb9bf2173b787c453774c4580", "url": "https://github.com/apache/flink/commit/897828752903134fb9bf2173b787c453774c4580", "message": "address comments", "committedDate": "2020-05-14T09:09:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk3OTQ2Mw==", "url": "https://github.com/apache/flink/pull/11990#discussion_r424979463", "bodyText": "Don't need this class?", "author": "JingsongLi", "createdAt": "2020-05-14T08:58:31Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.io.InputSplit;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.data.util.DataFormatConverters.DataFormatConverter;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FlinkRuntimeException;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for filesystem connector tables.\n+ */\n+public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final InputFormat<RowData, T> inputFormat;\n+\tprivate final LookupContext context;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\t// use Row as key because we'll get external data in eval\n+\tprivate transient Map<Row, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long nextLoadTime;\n+\t// serializer to copy RowData\n+\tprivate transient TypeSerializer<RowData> serializer;\n+\t// converters to convert data from internal to external in order to generate keys for the cache\n+\tprivate final DataFormatConverter[] converters;\n+\n+\tpublic FileSystemLookupFunction(InputFormat<RowData, T> inputFormat, String[] lookupKeys, LookupContext context) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tconverters = new DataFormatConverter[lookupKeys.length];\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, context.selectedNames.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> context.selectedNames[i], i -> i));\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tconverters[i] = DataFormatConverters.getConverterForDataType(context.selectedTypes[index]);\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.context = context;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(context.selectedTypes).map(DataType::getLogicalType).toArray(LogicalType[]::new),\n+\t\t\t\tcontext.selectedNames);\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tnextLoadTime = -1;\n+\t\t// TODO: get ExecutionConfig from context?\n+\t\tserializer = getResultType().createSerializer(new ExecutionConfig());\n+\t}\n+\n+\tpublic void eval(Object... values) {\n+\t\tPreconditions.checkArgument(values.length == lookupCols.length, \"Number of values and lookup keys mismatch\");\n+\t\tcheckCacheReload();\n+\t\tRow probeKey = Row.of(values);\n+\t\tList<RowData> matchedRows = cache.get(probeKey);\n+\t\tif (matchedRows != null) {\n+\t\t\tfor (RowData matchedRow : matchedRows) {\n+\t\t\t\tcollect(matchedRow);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@VisibleForTesting\n+\tpublic Duration getCacheTTL() {\n+\t\treturn context.cacheTTL;\n+\t}\n+\n+\tprivate void checkCacheReload() {\n+\t\tif (nextLoadTime > System.currentTimeMillis()) {\n+\t\t\treturn;\n+\t\t}\n+\t\tcache.clear();\n+\t\ttry {\n+\t\t\tT[] inputSplits = inputFormat.createInputSplits(1);\n+\t\t\tGenericRowData reuse = new GenericRowData(context.selectedNames.length);\n+\t\t\tfor (T split : inputSplits) {\n+\t\t\t\tinputFormat.open(split);\n+\t\t\t\twhile (!inputFormat.reachedEnd()) {\n+\t\t\t\t\tRowData row = inputFormat.nextRecord(reuse);\n+\t\t\t\t\tRow key = extractKey(row);\n+\t\t\t\t\tList<RowData> rows = cache.computeIfAbsent(key, k -> new ArrayList<>());\n+\t\t\t\t\trows.add(serializer.copy(row));\n+\t\t\t\t}\n+\t\t\t\tinputFormat.close();\n+\t\t\t}\n+\t\t\tnextLoadTime = System.currentTimeMillis() + getCacheTTL().toMillis();\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new FlinkRuntimeException(\"Failed to load table into cache\", e);\n+\t\t}\n+\t}\n+\n+\tprivate Row extractKey(RowData row) {\n+\t\tRow key = new Row(lookupCols.length);\n+\t\tfor (int i = 0; i < lookupCols.length; i++) {\n+\t\t\tkey.setField(i, converters[i].toExternal(row, lookupCols[i]));\n+\t\t}\n+\t\treturn key;\n+\t}\n+\n+\t/**\n+\t * A class to store context information for the lookup function.\n+\t */\n+\tpublic static class LookupContext implements Serializable {", "originalCommit": "3591a7e419816e0dc6a13110e180df331726fa44", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk4MjQ2OA==", "url": "https://github.com/apache/flink/pull/11990#discussion_r424982468", "bodyText": "You can just use TimeUtils.parseDuration", "author": "JingsongLi", "createdAt": "2020-05-14T09:03:09Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -366,6 +375,12 @@ public boolean isLimitPushedDown() {\n \t\t\tList<String> partitionColNames = catalogTable.getPartitionKeys();\n \t\t\tTable hiveTable = client.getTable(dbName, tableName);\n \t\t\tProperties tableProps = HiveReflectionUtils.getTableMetadata(hiveShim, hiveTable);\n+\t\t\tString ttlStr = tableProps.getProperty(FileSystemOptions.LOOKUP_JOIN_CACHE_TTL.key());\n+\t\t\tConfiguration configuration = new Configuration();", "originalCommit": "3591a7e419816e0dc6a13110e180df331726fa44", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk4NTI0NQ==", "url": "https://github.com/apache/flink/pull/11990#discussion_r424985245", "bodyText": "Using hive DDL in flink.", "author": "JingsongLi", "createdAt": "2020-05-14T09:07:34Z", "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveLookupJoinTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.internal.TableImpl;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.hive.HiveCatalog;\n+import org.apache.flink.table.catalog.hive.HiveTestUtils;\n+import org.apache.flink.table.factories.TableSourceFactoryContextImpl;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.FileSystemOptions;\n+import org.apache.flink.table.planner.factories.utils.TestCollectionTableFactory;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;\n+\n+import com.klarna.hiverunner.HiveShell;\n+import com.klarna.hiverunner.annotations.HiveSQL;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import java.time.Duration;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test lookup join of hive tables.\n+ */\n+@RunWith(FlinkStandaloneHiveRunner.class)\n+public class HiveLookupJoinTest {\n+\n+\t@HiveSQL(files = {})\n+\tprivate static HiveShell hiveShell;\n+\n+\t@Test\n+\tpublic void test() throws Exception {\n+\t\tEnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();\n+\t\tTableEnvironment tableEnv = TableEnvironment.create(settings);\n+\t\tHiveCatalog hiveCatalog = HiveTestUtils.createHiveCatalog(hiveShell.getHiveConf());\n+\t\ttableEnv.registerCatalog(hiveCatalog.getName(), hiveCatalog);\n+\t\ttableEnv.useCatalog(hiveCatalog.getName());\n+\n+\t\thiveShell.execute(String.format(\"create table build (x int,y string,z int) tblproperties ('%s'='5min')\",", "originalCommit": "3591a7e419816e0dc6a13110e180df331726fa44", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk4NTQ4Mw==", "url": "https://github.com/apache/flink/pull/11990#discussion_r424985483", "bodyText": "Put codes to before.", "author": "JingsongLi", "createdAt": "2020-05-14T09:07:59Z", "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveLookupJoinTest.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.internal.TableImpl;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.hive.HiveCatalog;\n+import org.apache.flink.table.catalog.hive.HiveTestUtils;\n+import org.apache.flink.table.factories.TableSourceFactoryContextImpl;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.FileSystemOptions;\n+import org.apache.flink.table.planner.factories.utils.TestCollectionTableFactory;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.flink.shaded.guava18.com.google.common.collect.Lists;\n+\n+import com.klarna.hiverunner.HiveShell;\n+import com.klarna.hiverunner.annotations.HiveSQL;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import java.time.Duration;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test lookup join of hive tables.\n+ */\n+@RunWith(FlinkStandaloneHiveRunner.class)\n+public class HiveLookupJoinTest {\n+\n+\t@HiveSQL(files = {})\n+\tprivate static HiveShell hiveShell;\n+\n+\t@Test\n+\tpublic void test() throws Exception {\n+\t\tEnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();\n+\t\tTableEnvironment tableEnv = TableEnvironment.create(settings);\n+\t\tHiveCatalog hiveCatalog = HiveTestUtils.createHiveCatalog(hiveShell.getHiveConf());\n+\t\ttableEnv.registerCatalog(hiveCatalog.getName(), hiveCatalog);\n+\t\ttableEnv.useCatalog(hiveCatalog.getName());", "originalCommit": "3591a7e419816e0dc6a13110e180df331726fa44", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk5MDAwMQ==", "url": "https://github.com/apache/flink/pull/11990#discussion_r424990001", "bodyText": "values are internal structures, should be converted to external.", "author": "JingsongLi", "createdAt": "2020-05-14T09:15:08Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.io.InputSplit;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.data.util.DataFormatConverters.DataFormatConverter;\n+import org.apache.flink.table.functions.FunctionContext;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FlinkRuntimeException;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Lookup table function for filesystem connector tables.\n+ */\n+public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final InputFormat<RowData, T> inputFormat;\n+\tprivate final LookupContext context;\n+\t// indices of lookup columns in the record returned by input format\n+\tprivate final int[] lookupCols;\n+\t// use Row as key because we'll get external data in eval\n+\tprivate transient Map<Row, List<RowData>> cache;\n+\t// timestamp when cache expires\n+\tprivate transient long nextLoadTime;\n+\t// serializer to copy RowData\n+\tprivate transient TypeSerializer<RowData> serializer;\n+\t// converters to convert data from internal to external in order to generate keys for the cache\n+\tprivate final DataFormatConverter[] converters;\n+\n+\tpublic FileSystemLookupFunction(InputFormat<RowData, T> inputFormat, String[] lookupKeys, LookupContext context) {\n+\t\tlookupCols = new int[lookupKeys.length];\n+\t\tconverters = new DataFormatConverter[lookupKeys.length];\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, context.selectedNames.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> context.selectedNames[i], i -> i));\n+\t\tfor (int i = 0; i < lookupKeys.length; i++) {\n+\t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n+\t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n+\t\t\tconverters[i] = DataFormatConverters.getConverterForDataType(context.selectedTypes[index]);\n+\t\t\tlookupCols[i] = index;\n+\t\t}\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.context = context;\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getResultType() {\n+\t\treturn new RowDataTypeInfo(\n+\t\t\t\tArrays.stream(context.selectedTypes).map(DataType::getLogicalType).toArray(LogicalType[]::new),\n+\t\t\t\tcontext.selectedNames);\n+\t}\n+\n+\t@Override\n+\tpublic void open(FunctionContext context) throws Exception {\n+\t\tsuper.open(context);\n+\t\tcache = new HashMap<>();\n+\t\tnextLoadTime = -1;\n+\t\t// TODO: get ExecutionConfig from context?\n+\t\tserializer = getResultType().createSerializer(new ExecutionConfig());\n+\t}\n+\n+\tpublic void eval(Object... values) {\n+\t\tPreconditions.checkArgument(values.length == lookupCols.length, \"Number of values and lookup keys mismatch\");\n+\t\tcheckCacheReload();\n+\t\tRow probeKey = Row.of(values);", "originalCommit": "3591a7e419816e0dc6a13110e180df331726fa44", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "013cea649d481b713737fd32f94dd8b46d0f3426", "url": "https://github.com/apache/flink/commit/013cea649d481b713737fd32f94dd8b46d0f3426", "message": "address comments", "committedDate": "2020-05-14T09:56:20Z", "type": "commit"}, {"oid": "013cea649d481b713737fd32f94dd8b46d0f3426", "url": "https://github.com/apache/flink/commit/013cea649d481b713737fd32f94dd8b46d0f3426", "message": "address comments", "committedDate": "2020-05-14T09:56:20Z", "type": "forcePushed"}, {"oid": "99f3bca89ea39ba4b354fee78dcc233394b6477e", "url": "https://github.com/apache/flink/commit/99f3bca89ea39ba4b354fee78dcc233394b6477e", "message": "update doc", "committedDate": "2020-05-14T10:00:55Z", "type": "commit"}]}