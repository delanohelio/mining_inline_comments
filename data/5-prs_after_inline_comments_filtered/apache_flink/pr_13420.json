{"pr_number": 13420, "pr_title": "[FLINK-19229][python] Introduce the PythonStreamGroupAggregateOperator for Python UDAF.", "pr_createdAt": "2020-09-18T09:46:02Z", "pr_url": "https://github.com/apache/flink/pull/13420", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2NjcxOA==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491266718", "bodyText": "It seems that the abstraction of BeamPythonStatefulFunctionRunner and BeamPythonStatelessFunctionRunner is not necessary. What about merge the functionality of BeamPythonStatefulFunctionRunner into BeamPythonFunctionRunner and remove BeamPythonStatelessFunctionRunner?", "author": "dianfu", "createdAt": "2020-09-19T04:16:52Z", "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonStatefulFunctionRunner.java", "diffHunk": "@@ -0,0 +1,368 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.runners.python.beam;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.python.env.PythonEnvironmentManager;\n+import org.apache.flink.python.metric.FlinkMetricContainer;\n+import org.apache.flink.runtime.state.KeyedStateBackend;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+\n+import org.apache.beam.model.fnexecution.v1.BeamFnApi;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.ModelCoders;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.ImmutableExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.SideInputReference;\n+import org.apache.beam.runners.core.construction.graph.TimerReference;\n+import org.apache.beam.runners.core.construction.graph.UserStateReference;\n+import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n+/**\n+ * A {@link BeamPythonStatefulFunctionRunner} used to execute Python stateful functions.\n+ */\n+public abstract class BeamPythonStatefulFunctionRunner extends BeamPythonFunctionRunner {", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2Njg1Nw==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491266857", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\tbyte[] keybytes = bagUserState.getKey().toByteArray();\n          \n          \n            \n            \t\t\t\tbyte[] keyBytes = bagUserState.getKey().toByteArray();", "author": "dianfu", "createdAt": "2020-09-19T04:19:10Z", "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonStatefulFunctionRunner.java", "diffHunk": "@@ -0,0 +1,368 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.runners.python.beam;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.python.env.PythonEnvironmentManager;\n+import org.apache.flink.python.metric.FlinkMetricContainer;\n+import org.apache.flink.runtime.state.KeyedStateBackend;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+\n+import org.apache.beam.model.fnexecution.v1.BeamFnApi;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.ModelCoders;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.ImmutableExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.SideInputReference;\n+import org.apache.beam.runners.core.construction.graph.TimerReference;\n+import org.apache.beam.runners.core.construction.graph.UserStateReference;\n+import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n+/**\n+ * A {@link BeamPythonStatefulFunctionRunner} used to execute Python stateful functions.\n+ */\n+public abstract class BeamPythonStatefulFunctionRunner extends BeamPythonFunctionRunner {\n+\n+\tprivate static final String USER_STATE_PREFIX = \"user-state-\";\n+\n+\tprivate static final String INPUT_ID = \"input\";\n+\tprivate static final String OUTPUT_ID = \"output\";\n+\tprivate static final String TRANSFORM_ID = \"transform\";\n+\n+\tprivate static final String MAIN_INPUT_NAME = \"input\";\n+\tprivate static final String MAIN_OUTPUT_NAME = \"output\";\n+\n+\tprivate static final String INPUT_CODER_ID = \"input_coder\";\n+\tprivate static final String OUTPUT_CODER_ID = \"output_coder\";\n+\tprivate static final String WINDOW_CODER_ID = \"window_coder\";\n+\n+\tprivate static final String WINDOW_STRATEGY = \"windowing_strategy\";\n+\n+\tprivate final String functionUrn;\n+\n+\tpublic BeamPythonStatefulFunctionRunner(\n+\t\tString taskName,\n+\t\tPythonEnvironmentManager environmentManager,\n+\t\tString functionUrn,\n+\t\tMap<String, String> jobOptions,\n+\t\tFlinkMetricContainer flinkMetricContainer,\n+\t\t@Nullable KeyedStateBackend keyedStateBackend,\n+\t\t@Nullable TypeSerializer keySerializer) {\n+\t\tsuper(\n+\t\t\ttaskName,\n+\t\t\tenvironmentManager,\n+\t\t\tgetStateRequestHandler(keyedStateBackend, keySerializer),\n+\t\t\tjobOptions,\n+\t\t\tflinkMetricContainer);\n+\t\tthis.functionUrn = functionUrn;\n+\t}\n+\n+\t@Override\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprotected transient ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprotected transient ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\t\t/**\n+\t\t * InputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t\tpublic SimpleStateRequestHandler(\n+\t\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\t\tTypeSerializer keySerializer) {\n+\t\t\tthis.keySerializer = keySerializer;\n+\t\t\tthis.valueSerializer = PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO\n+\t\t\t\t.createSerializer(new ExecutionConfig());\n+\t\t\tthis.keyedStateBackend = keyedStateBackend;\n+\t\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompletionStage<BeamFnApi.StateResponse.Builder> handle(BeamFnApi.StateRequest request) throws Exception {\n+\t\t\tBeamFnApi.StateKey.TypeCase typeCase = request.getStateKey().getTypeCase();\n+\t\t\tsynchronized (keyedStateBackend) {\n+\t\t\t\tif (typeCase.equals(BeamFnApi.StateKey.TypeCase.BAG_USER_STATE)) {\n+\t\t\t\t\treturn handleBagState(request);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new RuntimeException(\"Unsupported state type: \" + typeCase);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleBagState(BeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tif (request.getStateKey().hasBagUserState()) {\n+\t\t\t\tBeamFnApi.StateKey.BagUserState bagUserState = request.getStateKey().getBagUserState();\n+\t\t\t\t// get key\n+\t\t\t\tbyte[] keybytes = bagUserState.getKey().toByteArray();", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2NzA1Mg==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491267052", "bodyText": "Isn't ByteArrayInputStream enough?", "author": "dianfu", "createdAt": "2020-09-19T04:22:06Z", "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonStatefulFunctionRunner.java", "diffHunk": "@@ -0,0 +1,368 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.runners.python.beam;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.python.env.PythonEnvironmentManager;\n+import org.apache.flink.python.metric.FlinkMetricContainer;\n+import org.apache.flink.runtime.state.KeyedStateBackend;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+\n+import org.apache.beam.model.fnexecution.v1.BeamFnApi;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.ModelCoders;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.ImmutableExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.SideInputReference;\n+import org.apache.beam.runners.core.construction.graph.TimerReference;\n+import org.apache.beam.runners.core.construction.graph.UserStateReference;\n+import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n+/**\n+ * A {@link BeamPythonStatefulFunctionRunner} used to execute Python stateful functions.\n+ */\n+public abstract class BeamPythonStatefulFunctionRunner extends BeamPythonFunctionRunner {\n+\n+\tprivate static final String USER_STATE_PREFIX = \"user-state-\";\n+\n+\tprivate static final String INPUT_ID = \"input\";\n+\tprivate static final String OUTPUT_ID = \"output\";\n+\tprivate static final String TRANSFORM_ID = \"transform\";\n+\n+\tprivate static final String MAIN_INPUT_NAME = \"input\";\n+\tprivate static final String MAIN_OUTPUT_NAME = \"output\";\n+\n+\tprivate static final String INPUT_CODER_ID = \"input_coder\";\n+\tprivate static final String OUTPUT_CODER_ID = \"output_coder\";\n+\tprivate static final String WINDOW_CODER_ID = \"window_coder\";\n+\n+\tprivate static final String WINDOW_STRATEGY = \"windowing_strategy\";\n+\n+\tprivate final String functionUrn;\n+\n+\tpublic BeamPythonStatefulFunctionRunner(\n+\t\tString taskName,\n+\t\tPythonEnvironmentManager environmentManager,\n+\t\tString functionUrn,\n+\t\tMap<String, String> jobOptions,\n+\t\tFlinkMetricContainer flinkMetricContainer,\n+\t\t@Nullable KeyedStateBackend keyedStateBackend,\n+\t\t@Nullable TypeSerializer keySerializer) {\n+\t\tsuper(\n+\t\t\ttaskName,\n+\t\t\tenvironmentManager,\n+\t\t\tgetStateRequestHandler(keyedStateBackend, keySerializer),\n+\t\t\tjobOptions,\n+\t\t\tflinkMetricContainer);\n+\t\tthis.functionUrn = functionUrn;\n+\t}\n+\n+\t@Override\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprotected transient ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprotected transient ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\t\t/**\n+\t\t * InputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t\tpublic SimpleStateRequestHandler(\n+\t\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\t\tTypeSerializer keySerializer) {\n+\t\t\tthis.keySerializer = keySerializer;\n+\t\t\tthis.valueSerializer = PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO\n+\t\t\t\t.createSerializer(new ExecutionConfig());\n+\t\t\tthis.keyedStateBackend = keyedStateBackend;\n+\t\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\t\tbais = new ByteArrayInputStreamWithPos();", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2NzMxNA==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491267314", "bodyText": "Could we avoid constructing flinkStateDescriptor for each request?", "author": "dianfu", "createdAt": "2020-09-19T04:25:17Z", "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonStatefulFunctionRunner.java", "diffHunk": "@@ -0,0 +1,368 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.runners.python.beam;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.python.env.PythonEnvironmentManager;\n+import org.apache.flink.python.metric.FlinkMetricContainer;\n+import org.apache.flink.runtime.state.KeyedStateBackend;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+\n+import org.apache.beam.model.fnexecution.v1.BeamFnApi;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.ModelCoders;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.ImmutableExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.SideInputReference;\n+import org.apache.beam.runners.core.construction.graph.TimerReference;\n+import org.apache.beam.runners.core.construction.graph.UserStateReference;\n+import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n+/**\n+ * A {@link BeamPythonStatefulFunctionRunner} used to execute Python stateful functions.\n+ */\n+public abstract class BeamPythonStatefulFunctionRunner extends BeamPythonFunctionRunner {\n+\n+\tprivate static final String USER_STATE_PREFIX = \"user-state-\";\n+\n+\tprivate static final String INPUT_ID = \"input\";\n+\tprivate static final String OUTPUT_ID = \"output\";\n+\tprivate static final String TRANSFORM_ID = \"transform\";\n+\n+\tprivate static final String MAIN_INPUT_NAME = \"input\";\n+\tprivate static final String MAIN_OUTPUT_NAME = \"output\";\n+\n+\tprivate static final String INPUT_CODER_ID = \"input_coder\";\n+\tprivate static final String OUTPUT_CODER_ID = \"output_coder\";\n+\tprivate static final String WINDOW_CODER_ID = \"window_coder\";\n+\n+\tprivate static final String WINDOW_STRATEGY = \"windowing_strategy\";\n+\n+\tprivate final String functionUrn;\n+\n+\tpublic BeamPythonStatefulFunctionRunner(\n+\t\tString taskName,\n+\t\tPythonEnvironmentManager environmentManager,\n+\t\tString functionUrn,\n+\t\tMap<String, String> jobOptions,\n+\t\tFlinkMetricContainer flinkMetricContainer,\n+\t\t@Nullable KeyedStateBackend keyedStateBackend,\n+\t\t@Nullable TypeSerializer keySerializer) {\n+\t\tsuper(\n+\t\t\ttaskName,\n+\t\t\tenvironmentManager,\n+\t\t\tgetStateRequestHandler(keyedStateBackend, keySerializer),\n+\t\t\tjobOptions,\n+\t\t\tflinkMetricContainer);\n+\t\tthis.functionUrn = functionUrn;\n+\t}\n+\n+\t@Override\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprotected transient ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprotected transient ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\t\t/**\n+\t\t * InputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t\tpublic SimpleStateRequestHandler(\n+\t\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\t\tTypeSerializer keySerializer) {\n+\t\t\tthis.keySerializer = keySerializer;\n+\t\t\tthis.valueSerializer = PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO\n+\t\t\t\t.createSerializer(new ExecutionConfig());\n+\t\t\tthis.keyedStateBackend = keyedStateBackend;\n+\t\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompletionStage<BeamFnApi.StateResponse.Builder> handle(BeamFnApi.StateRequest request) throws Exception {\n+\t\t\tBeamFnApi.StateKey.TypeCase typeCase = request.getStateKey().getTypeCase();\n+\t\t\tsynchronized (keyedStateBackend) {\n+\t\t\t\tif (typeCase.equals(BeamFnApi.StateKey.TypeCase.BAG_USER_STATE)) {\n+\t\t\t\t\treturn handleBagState(request);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new RuntimeException(\"Unsupported state type: \" + typeCase);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleBagState(BeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tif (request.getStateKey().hasBagUserState()) {\n+\t\t\t\tBeamFnApi.StateKey.BagUserState bagUserState = request.getStateKey().getBagUserState();\n+\t\t\t\t// get key\n+\t\t\t\tbyte[] keybytes = bagUserState.getKey().toByteArray();\n+\t\t\t\tbais.setBuffer(keybytes, 0, keybytes.length);\n+\t\t\t\tObject key = keySerializer.deserialize(baisWrapper);\n+\t\t\t\tkeyedStateBackend.setCurrentKey(key);\n+\t\t\t} else {\n+\t\t\t\tthrow new RuntimeException(\"Unsupported bag state request: \" + request);\n+\t\t\t}\n+\n+\t\t\tswitch (request.getRequestCase()) {\n+\t\t\t\tcase GET:\n+\t\t\t\t\treturn handleGetRequest(request);\n+\t\t\t\tcase APPEND:\n+\t\t\t\t\treturn handleAppendRequest(request);\n+\t\t\t\tcase CLEAR:\n+\t\t\t\t\treturn handleClearRequest(request);\n+\t\t\t\tdefault:\n+\t\t\t\t\tthrow new RuntimeException(\n+\t\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Unsupported request type %s for user state.\", request.getRequestCase()));\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate List<ByteString> convertToByteString(ListState<byte[]> listState) throws Exception {\n+\t\t\tList<ByteString> ret = new LinkedList<>();\n+\t\t\tif (listState.get() == null) {\n+\t\t\t\treturn ret;\n+\t\t\t}\n+\t\t\tfor (byte[] v: listState.get()) {\n+\t\t\t\tret.add(ByteString.copyFrom(v));\n+\t\t\t}\n+\t\t\treturn ret;\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleGetRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\t\t\tList<ByteString> byteStrings = convertToByteString(partitionedState);\n+\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setGet(\n+\t\t\t\t\t\tBeamFnApi.StateGetResponse.newBuilder()\n+\t\t\t\t\t\t\t.setData(ByteString.copyFrom(byteStrings))));\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleAppendRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\t\t\t// get values\n+\t\t\tbyte[] valuebytes = request.getAppend().getData().toByteArray();\n+\t\t\tpartitionedState.add(valuebytes);\n+\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setAppend(BeamFnApi.StateAppendResponse.getDefaultInstance()));\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleClearRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\n+\t\t\tpartitionedState.clear();\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setClear(BeamFnApi.StateClearResponse.getDefaultInstance()));\n+\t\t}\n+\n+\t\tprivate ListState<byte[]> getListState(BeamFnApi.StateRequest request) throws Exception {\n+\t\t\tBeamFnApi.StateKey.BagUserState bagUserState = request.getStateKey().getBagUserState();\n+\t\t\tListStateDescriptor<byte[]> flinkStateDescriptor =", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2NzM4Nw==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491267387", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\tListStateDescriptor<byte[]> flinkStateDescriptor =\n          \n          \n            \n            \t\t\tListStateDescriptor<byte[]> listStateDescriptor =", "author": "dianfu", "createdAt": "2020-09-19T04:26:25Z", "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonStatefulFunctionRunner.java", "diffHunk": "@@ -0,0 +1,368 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.runners.python.beam;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.python.env.PythonEnvironmentManager;\n+import org.apache.flink.python.metric.FlinkMetricContainer;\n+import org.apache.flink.runtime.state.KeyedStateBackend;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+\n+import org.apache.beam.model.fnexecution.v1.BeamFnApi;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.ModelCoders;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.ImmutableExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.SideInputReference;\n+import org.apache.beam.runners.core.construction.graph.TimerReference;\n+import org.apache.beam.runners.core.construction.graph.UserStateReference;\n+import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n+/**\n+ * A {@link BeamPythonStatefulFunctionRunner} used to execute Python stateful functions.\n+ */\n+public abstract class BeamPythonStatefulFunctionRunner extends BeamPythonFunctionRunner {\n+\n+\tprivate static final String USER_STATE_PREFIX = \"user-state-\";\n+\n+\tprivate static final String INPUT_ID = \"input\";\n+\tprivate static final String OUTPUT_ID = \"output\";\n+\tprivate static final String TRANSFORM_ID = \"transform\";\n+\n+\tprivate static final String MAIN_INPUT_NAME = \"input\";\n+\tprivate static final String MAIN_OUTPUT_NAME = \"output\";\n+\n+\tprivate static final String INPUT_CODER_ID = \"input_coder\";\n+\tprivate static final String OUTPUT_CODER_ID = \"output_coder\";\n+\tprivate static final String WINDOW_CODER_ID = \"window_coder\";\n+\n+\tprivate static final String WINDOW_STRATEGY = \"windowing_strategy\";\n+\n+\tprivate final String functionUrn;\n+\n+\tpublic BeamPythonStatefulFunctionRunner(\n+\t\tString taskName,\n+\t\tPythonEnvironmentManager environmentManager,\n+\t\tString functionUrn,\n+\t\tMap<String, String> jobOptions,\n+\t\tFlinkMetricContainer flinkMetricContainer,\n+\t\t@Nullable KeyedStateBackend keyedStateBackend,\n+\t\t@Nullable TypeSerializer keySerializer) {\n+\t\tsuper(\n+\t\t\ttaskName,\n+\t\t\tenvironmentManager,\n+\t\t\tgetStateRequestHandler(keyedStateBackend, keySerializer),\n+\t\t\tjobOptions,\n+\t\t\tflinkMetricContainer);\n+\t\tthis.functionUrn = functionUrn;\n+\t}\n+\n+\t@Override\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprotected transient ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprotected transient ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\t\t/**\n+\t\t * InputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t\tpublic SimpleStateRequestHandler(\n+\t\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\t\tTypeSerializer keySerializer) {\n+\t\t\tthis.keySerializer = keySerializer;\n+\t\t\tthis.valueSerializer = PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO\n+\t\t\t\t.createSerializer(new ExecutionConfig());\n+\t\t\tthis.keyedStateBackend = keyedStateBackend;\n+\t\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompletionStage<BeamFnApi.StateResponse.Builder> handle(BeamFnApi.StateRequest request) throws Exception {\n+\t\t\tBeamFnApi.StateKey.TypeCase typeCase = request.getStateKey().getTypeCase();\n+\t\t\tsynchronized (keyedStateBackend) {\n+\t\t\t\tif (typeCase.equals(BeamFnApi.StateKey.TypeCase.BAG_USER_STATE)) {\n+\t\t\t\t\treturn handleBagState(request);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new RuntimeException(\"Unsupported state type: \" + typeCase);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleBagState(BeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tif (request.getStateKey().hasBagUserState()) {\n+\t\t\t\tBeamFnApi.StateKey.BagUserState bagUserState = request.getStateKey().getBagUserState();\n+\t\t\t\t// get key\n+\t\t\t\tbyte[] keybytes = bagUserState.getKey().toByteArray();\n+\t\t\t\tbais.setBuffer(keybytes, 0, keybytes.length);\n+\t\t\t\tObject key = keySerializer.deserialize(baisWrapper);\n+\t\t\t\tkeyedStateBackend.setCurrentKey(key);\n+\t\t\t} else {\n+\t\t\t\tthrow new RuntimeException(\"Unsupported bag state request: \" + request);\n+\t\t\t}\n+\n+\t\t\tswitch (request.getRequestCase()) {\n+\t\t\t\tcase GET:\n+\t\t\t\t\treturn handleGetRequest(request);\n+\t\t\t\tcase APPEND:\n+\t\t\t\t\treturn handleAppendRequest(request);\n+\t\t\t\tcase CLEAR:\n+\t\t\t\t\treturn handleClearRequest(request);\n+\t\t\t\tdefault:\n+\t\t\t\t\tthrow new RuntimeException(\n+\t\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Unsupported request type %s for user state.\", request.getRequestCase()));\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate List<ByteString> convertToByteString(ListState<byte[]> listState) throws Exception {\n+\t\t\tList<ByteString> ret = new LinkedList<>();\n+\t\t\tif (listState.get() == null) {\n+\t\t\t\treturn ret;\n+\t\t\t}\n+\t\t\tfor (byte[] v: listState.get()) {\n+\t\t\t\tret.add(ByteString.copyFrom(v));\n+\t\t\t}\n+\t\t\treturn ret;\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleGetRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\t\t\tList<ByteString> byteStrings = convertToByteString(partitionedState);\n+\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setGet(\n+\t\t\t\t\t\tBeamFnApi.StateGetResponse.newBuilder()\n+\t\t\t\t\t\t\t.setData(ByteString.copyFrom(byteStrings))));\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleAppendRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\t\t\t// get values\n+\t\t\tbyte[] valuebytes = request.getAppend().getData().toByteArray();\n+\t\t\tpartitionedState.add(valuebytes);\n+\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setAppend(BeamFnApi.StateAppendResponse.getDefaultInstance()));\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleClearRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\n+\t\t\tpartitionedState.clear();\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setClear(BeamFnApi.StateClearResponse.getDefaultInstance()));\n+\t\t}\n+\n+\t\tprivate ListState<byte[]> getListState(BeamFnApi.StateRequest request) throws Exception {\n+\t\t\tBeamFnApi.StateKey.BagUserState bagUserState = request.getStateKey().getBagUserState();\n+\t\t\tListStateDescriptor<byte[]> flinkStateDescriptor =", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2NzQ5Nw==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491267497", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\tbyte[] valuebytes = request.getAppend().getData().toByteArray();\n          \n          \n            \n            \t\t\tbyte[] valueBytes = request.getAppend().getData().toByteArray();", "author": "dianfu", "createdAt": "2020-09-19T04:27:38Z", "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonStatefulFunctionRunner.java", "diffHunk": "@@ -0,0 +1,368 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.runners.python.beam;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.python.env.PythonEnvironmentManager;\n+import org.apache.flink.python.metric.FlinkMetricContainer;\n+import org.apache.flink.runtime.state.KeyedStateBackend;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+\n+import org.apache.beam.model.fnexecution.v1.BeamFnApi;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.ModelCoders;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.ImmutableExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.SideInputReference;\n+import org.apache.beam.runners.core.construction.graph.TimerReference;\n+import org.apache.beam.runners.core.construction.graph.UserStateReference;\n+import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n+/**\n+ * A {@link BeamPythonStatefulFunctionRunner} used to execute Python stateful functions.\n+ */\n+public abstract class BeamPythonStatefulFunctionRunner extends BeamPythonFunctionRunner {\n+\n+\tprivate static final String USER_STATE_PREFIX = \"user-state-\";\n+\n+\tprivate static final String INPUT_ID = \"input\";\n+\tprivate static final String OUTPUT_ID = \"output\";\n+\tprivate static final String TRANSFORM_ID = \"transform\";\n+\n+\tprivate static final String MAIN_INPUT_NAME = \"input\";\n+\tprivate static final String MAIN_OUTPUT_NAME = \"output\";\n+\n+\tprivate static final String INPUT_CODER_ID = \"input_coder\";\n+\tprivate static final String OUTPUT_CODER_ID = \"output_coder\";\n+\tprivate static final String WINDOW_CODER_ID = \"window_coder\";\n+\n+\tprivate static final String WINDOW_STRATEGY = \"windowing_strategy\";\n+\n+\tprivate final String functionUrn;\n+\n+\tpublic BeamPythonStatefulFunctionRunner(\n+\t\tString taskName,\n+\t\tPythonEnvironmentManager environmentManager,\n+\t\tString functionUrn,\n+\t\tMap<String, String> jobOptions,\n+\t\tFlinkMetricContainer flinkMetricContainer,\n+\t\t@Nullable KeyedStateBackend keyedStateBackend,\n+\t\t@Nullable TypeSerializer keySerializer) {\n+\t\tsuper(\n+\t\t\ttaskName,\n+\t\t\tenvironmentManager,\n+\t\t\tgetStateRequestHandler(keyedStateBackend, keySerializer),\n+\t\t\tjobOptions,\n+\t\t\tflinkMetricContainer);\n+\t\tthis.functionUrn = functionUrn;\n+\t}\n+\n+\t@Override\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprotected transient ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprotected transient ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\t\t/**\n+\t\t * InputStream Wrapper.\n+\t\t */\n+\t\tprotected transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t\tpublic SimpleStateRequestHandler(\n+\t\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\t\tTypeSerializer keySerializer) {\n+\t\t\tthis.keySerializer = keySerializer;\n+\t\t\tthis.valueSerializer = PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO\n+\t\t\t\t.createSerializer(new ExecutionConfig());\n+\t\t\tthis.keyedStateBackend = keyedStateBackend;\n+\t\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompletionStage<BeamFnApi.StateResponse.Builder> handle(BeamFnApi.StateRequest request) throws Exception {\n+\t\t\tBeamFnApi.StateKey.TypeCase typeCase = request.getStateKey().getTypeCase();\n+\t\t\tsynchronized (keyedStateBackend) {\n+\t\t\t\tif (typeCase.equals(BeamFnApi.StateKey.TypeCase.BAG_USER_STATE)) {\n+\t\t\t\t\treturn handleBagState(request);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new RuntimeException(\"Unsupported state type: \" + typeCase);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleBagState(BeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tif (request.getStateKey().hasBagUserState()) {\n+\t\t\t\tBeamFnApi.StateKey.BagUserState bagUserState = request.getStateKey().getBagUserState();\n+\t\t\t\t// get key\n+\t\t\t\tbyte[] keybytes = bagUserState.getKey().toByteArray();\n+\t\t\t\tbais.setBuffer(keybytes, 0, keybytes.length);\n+\t\t\t\tObject key = keySerializer.deserialize(baisWrapper);\n+\t\t\t\tkeyedStateBackend.setCurrentKey(key);\n+\t\t\t} else {\n+\t\t\t\tthrow new RuntimeException(\"Unsupported bag state request: \" + request);\n+\t\t\t}\n+\n+\t\t\tswitch (request.getRequestCase()) {\n+\t\t\t\tcase GET:\n+\t\t\t\t\treturn handleGetRequest(request);\n+\t\t\t\tcase APPEND:\n+\t\t\t\t\treturn handleAppendRequest(request);\n+\t\t\t\tcase CLEAR:\n+\t\t\t\t\treturn handleClearRequest(request);\n+\t\t\t\tdefault:\n+\t\t\t\t\tthrow new RuntimeException(\n+\t\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Unsupported request type %s for user state.\", request.getRequestCase()));\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate List<ByteString> convertToByteString(ListState<byte[]> listState) throws Exception {\n+\t\t\tList<ByteString> ret = new LinkedList<>();\n+\t\t\tif (listState.get() == null) {\n+\t\t\t\treturn ret;\n+\t\t\t}\n+\t\t\tfor (byte[] v: listState.get()) {\n+\t\t\t\tret.add(ByteString.copyFrom(v));\n+\t\t\t}\n+\t\t\treturn ret;\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleGetRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\t\t\tList<ByteString> byteStrings = convertToByteString(partitionedState);\n+\n+\t\t\treturn CompletableFuture.completedFuture(\n+\t\t\t\tBeamFnApi.StateResponse.newBuilder()\n+\t\t\t\t\t.setId(request.getId())\n+\t\t\t\t\t.setGet(\n+\t\t\t\t\t\tBeamFnApi.StateGetResponse.newBuilder()\n+\t\t\t\t\t\t\t.setData(ByteString.copyFrom(byteStrings))));\n+\t\t}\n+\n+\t\tprivate CompletionStage<BeamFnApi.StateResponse.Builder> handleAppendRequest(\n+\t\t\tBeamFnApi.StateRequest request) throws Exception {\n+\n+\t\t\tListState<byte[]> partitionedState = getListState(request);\n+\t\t\t// get values\n+\t\t\tbyte[] valuebytes = request.getAppend().getData().toByteArray();", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2Nzg1NQ==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491267855", "bodyText": "Add Java doc for the following fields", "author": "dianfu", "createdAt": "2020-09-19T04:32:15Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/AbstractPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonStatefulFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ * The base class for Python stream group aggregate operators.\n+ * @param <IN> Type of the input elements.\n+ * @param <OUT> Type of the output elements.\n+ */\n+public abstract class AbstractPythonStreamGroupAggregateOperator<IN, OUT>\n+\textends AbstractOneInputPythonFunctionOperator<IN, OUT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_REF_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\tprivate final int[] grouping;", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2ODAzOA==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491268038", "bodyText": "What about python.state.cache.size?", "author": "dianfu", "createdAt": "2020-09-19T04:34:27Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/AbstractPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonStatefulFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ * The base class for Python stream group aggregate operators.\n+ * @param <IN> Type of the input elements.\n+ * @param <OUT> Type of the output elements.\n+ */\n+public abstract class AbstractPythonStreamGroupAggregateOperator<IN, OUT>\n+\textends AbstractOneInputPythonFunctionOperator<IN, OUT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_REF_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\tprivate final int[] grouping;\n+\n+\tprivate final int indexOfCountStar;\n+\n+\tprivate final boolean generateUpdateBefore;\n+\n+\tprotected final long minRetentionTime;\n+\n+\tprotected final long maxRetentionTime;\n+\n+\tprivate int stateRefCacheSize = DEFAULT_STATE_REF_CACHE_SIZE;\n+\n+\tprotected final boolean stateCleaningEnabled;\n+\n+\tprivate transient Object keyForTimerService;\n+\n+\t/**\n+\t * The user-defined function input logical type.\n+\t */\n+\tprotected transient RowType userDefinedFunctionInputType;\n+\n+\tpublic AbstractPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(config);\n+\t\tthis.inputType = Preconditions.checkNotNull(inputType);\n+\t\tthis.outputType = Preconditions.checkNotNull(outputType);\n+\t\tthis.aggregateFunctions = aggregateFunctions;\n+\t\tthis.jobOptions = buildJobOptions(config);\n+\t\tthis.grouping = grouping;\n+\t\tthis.indexOfCountStar = indexOfCountStar;\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.minRetentionTime = minRetentionTime;\n+\t\tthis.maxRetentionTime = maxRetentionTime;\n+\t\tthis.stateCleaningEnabled = minRetentionTime > 1;\n+\t\tthis.keyForTimerService = null;\n+\t}\n+\n+\tprivate Map<String, String> buildJobOptions(Configuration config) {\n+\t\tMap<String, String> jobOptions = new HashMap<>();\n+\t\tif (config.containsKey(\"table.exec.timezone\")) {\n+\t\t\tjobOptions.put(\"table.exec.timezone\", config.getString(\"table.exec.timezone\", null));\n+\t\t}\n+\t\tif (config.containsKey(\"python.state.ref.size\")) {", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2ODE0NA==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491268144", "bodyText": "can be removed", "author": "dianfu", "createdAt": "2020-09-19T04:35:38Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/AbstractPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonStatefulFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ * The base class for Python stream group aggregate operators.\n+ * @param <IN> Type of the input elements.\n+ * @param <OUT> Type of the output elements.\n+ */\n+public abstract class AbstractPythonStreamGroupAggregateOperator<IN, OUT>\n+\textends AbstractOneInputPythonFunctionOperator<IN, OUT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_REF_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\tprivate final int[] grouping;\n+\n+\tprivate final int indexOfCountStar;\n+\n+\tprivate final boolean generateUpdateBefore;\n+\n+\tprotected final long minRetentionTime;\n+\n+\tprotected final long maxRetentionTime;\n+\n+\tprivate int stateRefCacheSize = DEFAULT_STATE_REF_CACHE_SIZE;\n+\n+\tprotected final boolean stateCleaningEnabled;\n+\n+\tprivate transient Object keyForTimerService;\n+\n+\t/**\n+\t * The user-defined function input logical type.\n+\t */\n+\tprotected transient RowType userDefinedFunctionInputType;\n+\n+\tpublic AbstractPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(config);\n+\t\tthis.inputType = Preconditions.checkNotNull(inputType);\n+\t\tthis.outputType = Preconditions.checkNotNull(outputType);\n+\t\tthis.aggregateFunctions = aggregateFunctions;\n+\t\tthis.jobOptions = buildJobOptions(config);\n+\t\tthis.grouping = grouping;\n+\t\tthis.indexOfCountStar = indexOfCountStar;\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.minRetentionTime = minRetentionTime;\n+\t\tthis.maxRetentionTime = maxRetentionTime;\n+\t\tthis.stateCleaningEnabled = minRetentionTime > 1;\n+\t\tthis.keyForTimerService = null;", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2ODIzMQ==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491268231", "bodyText": "Add an open method and init userDefinedFunctionInputType in the open method?", "author": "dianfu", "createdAt": "2020-09-19T04:37:03Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/AbstractPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonStatefulFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ * The base class for Python stream group aggregate operators.\n+ * @param <IN> Type of the input elements.\n+ * @param <OUT> Type of the output elements.\n+ */\n+public abstract class AbstractPythonStreamGroupAggregateOperator<IN, OUT>\n+\textends AbstractOneInputPythonFunctionOperator<IN, OUT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_REF_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\tprivate final int[] grouping;\n+\n+\tprivate final int indexOfCountStar;\n+\n+\tprivate final boolean generateUpdateBefore;\n+\n+\tprotected final long minRetentionTime;\n+\n+\tprotected final long maxRetentionTime;\n+\n+\tprivate int stateRefCacheSize = DEFAULT_STATE_REF_CACHE_SIZE;\n+\n+\tprotected final boolean stateCleaningEnabled;\n+\n+\tprivate transient Object keyForTimerService;\n+\n+\t/**\n+\t * The user-defined function input logical type.\n+\t */\n+\tprotected transient RowType userDefinedFunctionInputType;\n+\n+\tpublic AbstractPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(config);\n+\t\tthis.inputType = Preconditions.checkNotNull(inputType);\n+\t\tthis.outputType = Preconditions.checkNotNull(outputType);\n+\t\tthis.aggregateFunctions = aggregateFunctions;\n+\t\tthis.jobOptions = buildJobOptions(config);\n+\t\tthis.grouping = grouping;\n+\t\tthis.indexOfCountStar = indexOfCountStar;\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.minRetentionTime = minRetentionTime;\n+\t\tthis.maxRetentionTime = maxRetentionTime;\n+\t\tthis.stateCleaningEnabled = minRetentionTime > 1;\n+\t\tthis.keyForTimerService = null;\n+\t}\n+\n+\tprivate Map<String, String> buildJobOptions(Configuration config) {\n+\t\tMap<String, String> jobOptions = new HashMap<>();\n+\t\tif (config.containsKey(\"table.exec.timezone\")) {\n+\t\t\tjobOptions.put(\"table.exec.timezone\", config.getString(\"table.exec.timezone\", null));\n+\t\t}\n+\t\tif (config.containsKey(\"python.state.ref.size\")) {\n+\t\t\tstateRefCacheSize = Integer.valueOf(config.getString(\"python.state.ref.cache.size\", null));\n+\t\t}\n+\t\treturn jobOptions;\n+\t}\n+\n+\t@Override\n+\tpublic void setCurrentKey(Object key) {\n+\t\tkeyForTimerService = key;\n+\t}\n+\n+\t@Override\n+\tpublic Object getCurrentKey() {\n+\t\treturn keyForTimerService;\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<IN> element) throws Exception {\n+\t\tIN value = element.getValue();\n+\t\tprocessElementInternal(value);\n+\t\telementCount++;\n+\t\tcheckInvokeFinishBundleByCount();\n+\t\temitResults();\n+\t}\n+\n+\t@Override\n+\tpublic PythonFunctionRunner createPythonFunctionRunner() throws Exception {\n+\n+\t\treturn new BeamTablePythonStatefulFunctionRunner(\n+\t\t\tgetRuntimeContext().getTaskName(),\n+\t\t\tcreatePythonEnvironmentManager(),\n+\t\t\tgetUserDefinedFunctionInputType(),\n+\t\t\toutputType,\n+\t\t\tgetFunctionUrn(),\n+\t\t\tgetUserDefinedFunctionsProto(),\n+\t\t\tgetInputCoderUrn(),\n+\t\t\tgetOutputCoderUrn(),\n+\t\t\tjobOptions,\n+\t\t\tgetFlinkMetricContainer(),\n+\t\t\tgetKeyedStateBackend(),\n+\t\t\tgetKeySerializer());\n+\t}\n+\n+\t@Override\n+\tpublic PythonEnv getPythonEnv() {\n+\t\treturn aggregateFunctions[0].getPythonFunction().getPythonEnv();\n+\t}\n+\n+\tprotected RowType getKeyType() {\n+\t\tRowDataKeySelector selector = KeySelectorUtil.getRowDataSelector(\n+\t\t\tgrouping,\n+\t\t\tInternalTypeInfo.of(inputType));\n+\t\treturn selector.getProducedType().toRowType();\n+\t}\n+\n+\tprotected RowType getUserDefinedFunctionInputType() {", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2ODQ1Ng==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491268456", "bodyText": "Add Java doc on why we need to override setCurrentKey", "author": "dianfu", "createdAt": "2020-09-19T04:40:24Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/AbstractPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonStatefulFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ * The base class for Python stream group aggregate operators.\n+ * @param <IN> Type of the input elements.\n+ * @param <OUT> Type of the output elements.\n+ */\n+public abstract class AbstractPythonStreamGroupAggregateOperator<IN, OUT>\n+\textends AbstractOneInputPythonFunctionOperator<IN, OUT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_REF_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\tprivate final int[] grouping;\n+\n+\tprivate final int indexOfCountStar;\n+\n+\tprivate final boolean generateUpdateBefore;\n+\n+\tprotected final long minRetentionTime;\n+\n+\tprotected final long maxRetentionTime;\n+\n+\tprivate int stateRefCacheSize = DEFAULT_STATE_REF_CACHE_SIZE;\n+\n+\tprotected final boolean stateCleaningEnabled;\n+\n+\tprivate transient Object keyForTimerService;\n+\n+\t/**\n+\t * The user-defined function input logical type.\n+\t */\n+\tprotected transient RowType userDefinedFunctionInputType;\n+\n+\tpublic AbstractPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(config);\n+\t\tthis.inputType = Preconditions.checkNotNull(inputType);\n+\t\tthis.outputType = Preconditions.checkNotNull(outputType);\n+\t\tthis.aggregateFunctions = aggregateFunctions;\n+\t\tthis.jobOptions = buildJobOptions(config);\n+\t\tthis.grouping = grouping;\n+\t\tthis.indexOfCountStar = indexOfCountStar;\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.minRetentionTime = minRetentionTime;\n+\t\tthis.maxRetentionTime = maxRetentionTime;\n+\t\tthis.stateCleaningEnabled = minRetentionTime > 1;\n+\t\tthis.keyForTimerService = null;\n+\t}\n+\n+\tprivate Map<String, String> buildJobOptions(Configuration config) {\n+\t\tMap<String, String> jobOptions = new HashMap<>();\n+\t\tif (config.containsKey(\"table.exec.timezone\")) {\n+\t\t\tjobOptions.put(\"table.exec.timezone\", config.getString(\"table.exec.timezone\", null));\n+\t\t}\n+\t\tif (config.containsKey(\"python.state.ref.size\")) {\n+\t\t\tstateRefCacheSize = Integer.valueOf(config.getString(\"python.state.ref.cache.size\", null));\n+\t\t}\n+\t\treturn jobOptions;\n+\t}\n+\n+\t@Override\n+\tpublic void setCurrentKey(Object key) {", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2ODg0OA==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491268848", "bodyText": "Is there other classes that will extend AbstractPythonStreamGroupAggregateOperator besides RowDataPythonStreamGroupAggregateOperator? If not, we can merge RowDataPythonStreamGroupAggregateOperator and AbstractPythonStreamGroupAggregateOperator together.", "author": "dianfu", "createdAt": "2020-09-19T04:45:14Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/AbstractPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTablePythonStatefulFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ * The base class for Python stream group aggregate operators.\n+ * @param <IN> Type of the input elements.\n+ * @param <OUT> Type of the output elements.\n+ */\n+public abstract class AbstractPythonStreamGroupAggregateOperator<IN, OUT>", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2OTEyNA==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491269124", "bodyText": "the name doesn't apply here?", "author": "dianfu", "createdAt": "2020-09-19T04:49:12Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.operators.python.utils.OperatorUtils;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractPythonStreamGroupAggregateOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\tprivate static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\tprivate static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\tprivate static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\t/**\n+\t * The TypeSerializer for udf execution results.\n+\t */\n+\tprotected transient TypeSerializer<RowData> udfOutputTypeSerializer;\n+\n+\t/**\n+\t * The TypeSerializer for udf input elements.\n+\t */\n+\tprotected transient TypeSerializer<RowData> udfInputTypeSerializer;\n+\n+\t/**\n+\t * Reusable InputStream used to holding the execution results to be deserialized.\n+\t */\n+\tprivate transient ByteArrayInputStreamWithPos bais;\n+\n+\t/**\n+\t * InputStream Wrapper.\n+\t */\n+\tprivate transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t/**\n+\t * Reusable OutputStream used to holding the serialized input elements.\n+\t */\n+\tprivate transient ByteArrayOutputStreamWithPos baos;\n+\n+\t/**\n+\t * OutputStream Wrapper.\n+\t */\n+\tprivate transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\tprivate transient TimerService timerService;\n+\n+\t// holds the latest registered cleanup timer\n+\tprivate transient ValueState<Long> cleanupTimeState;\n+\n+\tprivate transient UpdatableRowData reuseRowData;\n+\tprivate transient UpdatableRowData reuseTimerRowData;\n+\n+\t/**\n+\t * The collector used to collect records.\n+\t */\n+\tprivate transient OperatorUtils.StreamRecordRowDataWrappingCollector rowDataWrapper;\n+\n+\tpublic RowDataPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(\n+\t\t\tconfig,\n+\t\t\tinputType,\n+\t\t\toutputType,\n+\t\t\taggregateFunctions,\n+\t\t\tgrouping,\n+\t\t\tindexOfCountStar,\n+\t\t\tgenerateUpdateBefore,\n+\t\t\tminRetentionTime,\n+\t\t\tmaxRetentionTime);\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tudfInputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(getUserDefinedFunctionInputType());\n+\t\tudfOutputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(outputType);\n+\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\ttimerService = new SimpleTimerService(\n+\t\t\tgetInternalTimerService(\"state-clean-timer\", VoidNamespaceSerializer.INSTANCE, this));\n+\t\treuseRowData = new UpdatableRowData(GenericRowData.of(NORMAL_RECORD, null, null, null), 4);\n+\t\treuseTimerRowData = new UpdatableRowData(GenericRowData.of(TRIGGER_TIMER, null, null, null), 4);\n+\t\trowDataWrapper = new OperatorUtils.StreamRecordRowDataWrappingCollector(output);\n+\t\tinitCleanupTimeState();\n+\t\tsuper.open();\n+\t}\n+\n+\t@Override\n+\tTypeSerializer getKeySerializer() {\n+\t\treturn PythonTypeUtils.toBlinkTypeSerializer(getKeyType());\n+\t}\n+\n+\t@Override\n+\tpublic void processElementInternal(RowData value) throws Exception {\n+\t\tlong currentTime = timerService.currentProcessingTime();\n+\t\tregisterProcessingCleanupTimer(currentTime);\n+\t\treuseRowData.setField(1, value);\n+\t\tudfInputTypeSerializer.serialize(reuseRowData, baosWrapper);\n+\t\tpythonFunctionRunner.process(baos.toByteArray());\n+\t\tbaos.reset();\n+\t}\n+\n+\t@Override\n+\tString getInputCoderUrn() {\n+\t\treturn FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN;\n+\t}\n+\n+\t@Override\n+\tString getOutputCoderUrn() {\n+\t\treturn FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN;\n+\t}\n+\n+\t@Override\n+\tpublic String getFunctionUrn() {\n+\t\treturn STREAM_GROUP_AGGREGATE_URN;\n+\t}\n+\n+\t@Override\n+\tpublic void emitResult(Tuple2<byte[], Integer> resultTuple) throws Exception {\n+\t\tbyte[] rawUdfResult = resultTuple.f0;\n+\t\tint length = resultTuple.f1;\n+\t\tbais.setBuffer(rawUdfResult, 0, length);\n+\t\tRowData udfResult = udfOutputTypeSerializer.deserialize(baisWrapper);\n+\t\trowDataWrapper.collect(udfResult);\n+\t}\n+\n+\t/**\n+\t * Invoked when an event-time timer fires.\n+\t */\n+\t@Override\n+\tpublic void onEventTime(InternalTimer<RowData, VoidNamespace> timer) {\n+\t\t// ignore\n+\t}\n+\n+\t/**\n+\t * Invoked when a processing-time timer fires.\n+\t */\n+\t@Override\n+\tpublic void onProcessingTime(InternalTimer<RowData, VoidNamespace> timer) throws Exception {\n+\t\tif (stateCleaningEnabled) {\n+\t\t\tRowData key = timer.getKey();\n+\t\t\tlong timestamp = timer.getTimestamp();\n+\t\t\treuseTimerRowData.setLong(2, timestamp);\n+\t\t\treuseTimerRowData.setField(3, key);\n+\t\t\tudfInputTypeSerializer.serialize(reuseTimerRowData, baosWrapper);\n+\t\t\tpythonFunctionRunner.process(baos.toByteArray());\n+\t\t\tbaos.reset();\n+\t\t\telementCount++;\n+\t\t}\n+\t}\n+\n+\tprivate void registerProcessingCleanupTimer(long currentTime) throws Exception {\n+\t\tif (stateCleaningEnabled) {\n+\t\t\tsynchronized (getKeyedStateBackend()) {\n+\t\t\t\tgetKeyedStateBackend().setCurrentKey(getCurrentKey());\n+\t\t\t\tregisterProcessingCleanupTimer(\n+\t\t\t\t\tcleanupTimeState,\n+\t\t\t\t\tcurrentTime,\n+\t\t\t\t\tminRetentionTime,\n+\t\t\t\t\tmaxRetentionTime,\n+\t\t\t\t\ttimerService\n+\t\t\t\t);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void initCleanupTimeState() {\n+\t\tif (stateCleaningEnabled) {\n+\t\t\tValueStateDescriptor<Long> inputCntDescriptor =", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2OTY5Mw==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491269693", "bodyText": "Make this class as a standalone class?", "author": "dianfu", "createdAt": "2020-09-19T04:56:45Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/utils/OperatorUtils.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.utils;\n+\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.types.CRow;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.Collector;\n+\n+import com.google.protobuf.ByteString;\n+\n+/**\n+ * The collectors used to collect Row values.\n+ */\n+public enum OperatorUtils {\n+\t;\n+\n+\tpublic static FlinkFnApi.UserDefinedFunction getUserDefinedFunctionProto(PythonFunctionInfo pythonFunctionInfo) {\n+\t\tFlinkFnApi.UserDefinedFunction.Builder builder = FlinkFnApi.UserDefinedFunction.newBuilder();\n+\t\tbuilder.setPayload(ByteString.copyFrom(pythonFunctionInfo.getPythonFunction().getSerializedPythonFunction()));\n+\t\tfor (Object input : pythonFunctionInfo.getInputs()) {\n+\t\t\tFlinkFnApi.UserDefinedFunction.Input.Builder inputProto =\n+\t\t\t\tFlinkFnApi.UserDefinedFunction.Input.newBuilder();\n+\t\t\tif (input instanceof PythonFunctionInfo) {\n+\t\t\t\tinputProto.setUdf(getUserDefinedFunctionProto((PythonFunctionInfo) input));\n+\t\t\t} else if (input instanceof Integer) {\n+\t\t\t\tinputProto.setInputOffset((Integer) input);\n+\t\t\t} else {\n+\t\t\t\tinputProto.setInputConstant(ByteString.copyFrom((byte[]) input));\n+\t\t\t}\n+\t\t\tbuilder.addInputs(inputProto);\n+\t\t}\n+\t\treturn builder.build();\n+\t}\n+\n+\t/**\n+\t * The collector is used to convert a {@link Row} to a {@link CRow}.\n+\t */\n+\tpublic static class StreamRecordCRowWrappingCollector implements Collector<Row> {", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI2OTcwMg==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491269702", "bodyText": "Make this class as a standalone class?", "author": "dianfu", "createdAt": "2020-09-19T04:56:50Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/utils/OperatorUtils.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.utils;\n+\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.types.CRow;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.Collector;\n+\n+import com.google.protobuf.ByteString;\n+\n+/**\n+ * The collectors used to collect Row values.\n+ */\n+public enum OperatorUtils {\n+\t;\n+\n+\tpublic static FlinkFnApi.UserDefinedFunction getUserDefinedFunctionProto(PythonFunctionInfo pythonFunctionInfo) {\n+\t\tFlinkFnApi.UserDefinedFunction.Builder builder = FlinkFnApi.UserDefinedFunction.newBuilder();\n+\t\tbuilder.setPayload(ByteString.copyFrom(pythonFunctionInfo.getPythonFunction().getSerializedPythonFunction()));\n+\t\tfor (Object input : pythonFunctionInfo.getInputs()) {\n+\t\t\tFlinkFnApi.UserDefinedFunction.Input.Builder inputProto =\n+\t\t\t\tFlinkFnApi.UserDefinedFunction.Input.newBuilder();\n+\t\t\tif (input instanceof PythonFunctionInfo) {\n+\t\t\t\tinputProto.setUdf(getUserDefinedFunctionProto((PythonFunctionInfo) input));\n+\t\t\t} else if (input instanceof Integer) {\n+\t\t\t\tinputProto.setInputOffset((Integer) input);\n+\t\t\t} else {\n+\t\t\t\tinputProto.setInputConstant(ByteString.copyFrom((byte[]) input));\n+\t\t\t}\n+\t\t\tbuilder.addInputs(inputProto);\n+\t\t}\n+\t\treturn builder.build();\n+\t}\n+\n+\t/**\n+\t * The collector is used to convert a {@link Row} to a {@link CRow}.\n+\t */\n+\tpublic static class StreamRecordCRowWrappingCollector implements Collector<Row> {\n+\n+\t\tprivate final Collector<StreamRecord<CRow>> out;\n+\t\tprivate final CRow reuseCRow = new CRow();\n+\n+\t\t/**\n+\t\t * For Table API & SQL jobs, the timestamp field is not used.\n+\t\t */\n+\t\tprivate final StreamRecord<CRow> reuseStreamRecord = new StreamRecord<>(reuseCRow);\n+\n+\t\tpublic StreamRecordCRowWrappingCollector(Collector<StreamRecord<CRow>> out) {\n+\t\t\tthis.out = out;\n+\t\t}\n+\n+\t\tpublic void setChange(boolean change) {\n+\t\t\tthis.reuseCRow.change_$eq(change);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void collect(Row record) {\n+\t\t\treuseCRow.row_$eq(record);\n+\t\t\tout.collect(reuseStreamRecord);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() {\n+\t\t\tout.close();\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * The collector is used to convert a {@link RowData} to a {@link StreamRecord}.\n+\t */\n+\tpublic static class StreamRecordRowDataWrappingCollector implements Collector<RowData> {", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI3MDAyNw==", "url": "https://github.com/apache/flink/pull/13420#discussion_r491270027", "bodyText": "Rename to PythonOperatorUtils?", "author": "dianfu", "createdAt": "2020-09-19T05:00:44Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/utils/OperatorUtils.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.utils;\n+\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.runtime.types.CRow;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.Collector;\n+\n+import com.google.protobuf.ByteString;\n+\n+/**\n+ * The collectors used to collect Row values.\n+ */\n+public enum OperatorUtils {", "originalCommit": "c95e880c06847e6ee07c4e6cdf107ce1fa3fcb11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "url": "https://github.com/apache/flink/commit/87967d2198d73dea7a84808d6c1a5076a1c7c317", "message": "[FLINK-19229][python] Introduce the PythonStreamGroupAggregateOperator for Python UDAF.", "committedDate": "2020-09-21T12:01:24Z", "type": "commit"}, {"oid": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "url": "https://github.com/apache/flink/commit/87967d2198d73dea7a84808d6c1a5076a1c7c317", "message": "[FLINK-19229][python] Introduce the PythonStreamGroupAggregateOperator for Python UDAF.", "committedDate": "2020-09-21T12:01:24Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAwMDg2Nw==", "url": "https://github.com/apache/flink/pull/13420#discussion_r492000867", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * A {@link BeamPythonFunctionRunner} used to execute Python stateful functions.\n          \n          \n            \n             * A {@link BeamPythonFunctionRunner} used to execute Python functions.", "author": "dianfu", "createdAt": "2020-09-21T12:23:29Z", "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonFunctionRunner.java", "diffHunk": "@@ -41,30 +60,60 @@\n import org.apache.beam.runners.fnexecution.control.StageBundleFactory;\n import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.fn.data.FnDataReceiver;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.options.PortablePipelineOptions;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.Struct;\n import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import javax.annotation.Nullable;\n \n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n import java.util.concurrent.LinkedBlockingQueue;\n \n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n /**\n- * An base class for {@link PythonFunctionRunner} based on beam.\n+ * A {@link BeamPythonFunctionRunner} used to execute Python stateful functions.", "originalCommit": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAwMTQ4Mw==", "url": "https://github.com/apache/flink/pull/13420#discussion_r492001483", "bodyText": "What about python-state-?", "author": "dianfu", "createdAt": "2020-09-21T12:24:35Z", "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonFunctionRunner.java", "diffHunk": "@@ -41,30 +60,60 @@\n import org.apache.beam.runners.fnexecution.control.StageBundleFactory;\n import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n import org.apache.beam.runners.fnexecution.state.StateRequestHandler;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.fn.data.FnDataReceiver;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.options.PortablePipelineOptions;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;\n import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.Struct;\n import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import javax.annotation.Nullable;\n \n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n import java.util.concurrent.LinkedBlockingQueue;\n \n+import static org.apache.beam.runners.core.construction.BeamUrns.getUrn;\n+\n /**\n- * An base class for {@link PythonFunctionRunner} based on beam.\n+ * A {@link BeamPythonFunctionRunner} used to execute Python stateful functions.\n  */\n @Internal\n public abstract class BeamPythonFunctionRunner implements PythonFunctionRunner {\n \tprotected static final Logger LOG = LoggerFactory.getLogger(BeamPythonFunctionRunner.class);\n \n-\tprivate transient boolean bundleStarted;\n+\tprivate static final String BEAM_STATE_PREFIX = \"beam-state-\";", "originalCommit": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAwNDU1NQ==", "url": "https://github.com/apache/flink/pull/13420#discussion_r492004555", "bodyText": "Can be converted to a local variable.", "author": "dianfu", "createdAt": "2020-09-21T12:29:26Z", "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonFunctionRunner.java", "diffHunk": "@@ -309,4 +356,287 @@ private void checkInvokeStartBundle() throws Exception {\n \t\t}\n \t}\n \n+\t/**\n+\t * Creates a {@link ExecutableStage} which contains the Python user-defined functions to be executed\n+\t * and all the other information needed to execute them, such as the execution environment, the input\n+\t * and output coder, etc.\n+\t */\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprivate final ByteArrayOutputStreamWithPos baos;", "originalCommit": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAwNTAxOQ==", "url": "https://github.com/apache/flink/pull/13420#discussion_r492005019", "bodyText": "It's never used?", "author": "dianfu", "createdAt": "2020-09-21T12:29:49Z", "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonFunctionRunner.java", "diffHunk": "@@ -309,4 +356,287 @@ private void checkInvokeStartBundle() throws Exception {\n \t\t}\n \t}\n \n+\t/**\n+\t * Creates a {@link ExecutableStage} which contains the Python user-defined functions to be executed\n+\t * and all the other information needed to execute them, such as the execution environment, the input\n+\t * and output coder, etc.\n+\t */\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprivate final ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprivate final ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprivate final DataOutputViewStreamWrapper baosWrapper;", "originalCommit": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAwNzEwMA==", "url": "https://github.com/apache/flink/pull/13420#discussion_r492007100", "bodyText": "Move this line to the front of the constructor?", "author": "dianfu", "createdAt": "2020-09-21T12:33:18Z", "path": "flink-python/src/main/java/org/apache/flink/streaming/api/runners/python/beam/BeamPythonFunctionRunner.java", "diffHunk": "@@ -309,4 +356,287 @@ private void checkInvokeStartBundle() throws Exception {\n \t\t}\n \t}\n \n+\t/**\n+\t * Creates a {@link ExecutableStage} which contains the Python user-defined functions to be executed\n+\t * and all the other information needed to execute them, such as the execution environment, the input\n+\t * and output coder, etc.\n+\t */\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate ExecutableStage createExecutableStage() throws Exception {\n+\t\tRunnerApi.Components components =\n+\t\t\tRunnerApi.Components.newBuilder()\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tINPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(INPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putPcollections(\n+\t\t\t\t\tOUTPUT_ID,\n+\t\t\t\t\tRunnerApi.PCollection.newBuilder()\n+\t\t\t\t\t\t.setWindowingStrategyId(WINDOW_STRATEGY)\n+\t\t\t\t\t\t.setCoderId(OUTPUT_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putTransforms(\n+\t\t\t\t\tTRANSFORM_ID,\n+\t\t\t\t\tRunnerApi.PTransform.newBuilder()\n+\t\t\t\t\t\t.setUniqueName(TRANSFORM_ID)\n+\t\t\t\t\t\t.setSpec(RunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t\t\t.setUrn(functionUrn)\n+\t\t\t\t\t\t\t.setPayload(\n+\t\t\t\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(\n+\t\t\t\t\t\t\t\t\tgetUserDefinedFunctionsProtoBytes()))\n+\t\t\t\t\t\t\t.build())\n+\t\t\t\t\t\t.putInputs(MAIN_INPUT_NAME, INPUT_ID)\n+\t\t\t\t\t\t.putOutputs(MAIN_OUTPUT_NAME, OUTPUT_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putWindowingStrategies(\n+\t\t\t\t\tWINDOW_STRATEGY,\n+\t\t\t\t\tRunnerApi.WindowingStrategy.newBuilder()\n+\t\t\t\t\t\t.setWindowCoderId(WINDOW_CODER_ID)\n+\t\t\t\t\t\t.build())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tINPUT_CODER_ID,\n+\t\t\t\t\tgetInputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tOUTPUT_CODER_ID,\n+\t\t\t\t\tgetOutputCoderProto())\n+\t\t\t\t.putCoders(\n+\t\t\t\t\tWINDOW_CODER_ID,\n+\t\t\t\t\tgetWindowCoderProto())\n+\t\t\t\t.build();\n+\n+\t\tPipelineNode.PCollectionNode input =\n+\t\t\tPipelineNode.pCollection(INPUT_ID, components.getPcollectionsOrThrow(INPUT_ID));\n+\t\tList<SideInputReference> sideInputs = Collections.EMPTY_LIST;\n+\t\tList<UserStateReference> userStates = Collections.EMPTY_LIST;\n+\t\tList<TimerReference> timers = Collections.EMPTY_LIST;\n+\t\tList<PipelineNode.PTransformNode> transforms =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pTransform(TRANSFORM_ID, components.getTransformsOrThrow(TRANSFORM_ID)));\n+\t\tList<PipelineNode.PCollectionNode> outputs =\n+\t\t\tCollections.singletonList(\n+\t\t\t\tPipelineNode.pCollection(OUTPUT_ID, components.getPcollectionsOrThrow(OUTPUT_ID)));\n+\t\treturn ImmutableExecutableStage.of(\n+\t\t\tcomponents, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());\n+\t}\n+\n+\tprivate Collection<RunnerApi.ExecutableStagePayload.WireCoderSetting> createValueOnlyWireCoderSetting() throws\n+\t\tIOException {\n+\t\tWindowedValue<byte[]> value = WindowedValue.valueInGlobalWindow(new byte[0]);\n+\t\tCoder<? extends BoundedWindow> windowCoder = GlobalWindow.Coder.INSTANCE;\n+\t\tWindowedValue.FullWindowedValueCoder<byte[]> windowedValueCoder =\n+\t\t\tWindowedValue.FullWindowedValueCoder.of(ByteArrayCoder.of(), windowCoder);\n+\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n+\t\twindowedValueCoder.encode(value, baos);\n+\n+\t\treturn Arrays.asList(\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(INPUT_ID)\n+\t\t\t\t.build(),\n+\t\t\tRunnerApi.ExecutableStagePayload.WireCoderSetting.newBuilder()\n+\t\t\t\t.setUrn(getUrn(RunnerApi.StandardCoders.Enum.PARAM_WINDOWED_VALUE))\n+\t\t\t\t.setPayload(\n+\t\t\t\t\torg.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString.copyFrom(baos.toByteArray()))\n+\t\t\t\t.setInputOrOutputId(OUTPUT_ID)\n+\t\t\t\t.build()\n+\t\t);\n+\t}\n+\n+\tprotected abstract byte[] getUserDefinedFunctionsProtoBytes();\n+\n+\tprotected abstract RunnerApi.Coder getInputCoderProto();\n+\n+\tprotected abstract RunnerApi.Coder getOutputCoderProto();\n+\n+\t/**\n+\t * Gets the proto representation of the window coder.\n+\t */\n+\tprivate RunnerApi.Coder getWindowCoderProto() {\n+\t\treturn RunnerApi.Coder.newBuilder()\n+\t\t\t.setSpec(\n+\t\t\t\tRunnerApi.FunctionSpec.newBuilder()\n+\t\t\t\t\t.setUrn(ModelCoders.GLOBAL_WINDOW_CODER_URN)\n+\t\t\t\t\t.build())\n+\t\t\t.build();\n+\t}\n+\n+\tprivate static StateRequestHandler getStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\tif (keyedStateBackend == null) {\n+\t\t\treturn StateRequestHandler.unsupported();\n+\t\t} else {\n+\t\t\tassert keySerializer != null;\n+\t\t\treturn new SimpleStateRequestHandler(keyedStateBackend, keySerializer);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A state request handler which handles the state request from Python side.\n+\t */\n+\tprivate static class SimpleStateRequestHandler implements StateRequestHandler {\n+\n+\t\tprivate final TypeSerializer keySerializer;\n+\t\tprivate final TypeSerializer<byte[]> valueSerializer;\n+\t\tprivate final KeyedStateBackend keyedStateBackend;\n+\n+\t\t/**\n+\t\t * Reusable OutputStream used to holding the serialized input elements.\n+\t\t */\n+\t\tprivate final ByteArrayOutputStreamWithPos baos;\n+\n+\t\t/**\n+\t\t * Reusable InputStream used to holding the elements to be deserialized.\n+\t\t */\n+\t\tprivate final ByteArrayInputStreamWithPos bais;\n+\n+\t\t/**\n+\t\t * OutputStream Wrapper.\n+\t\t */\n+\t\tprivate final DataOutputViewStreamWrapper baosWrapper;\n+\n+\t\t/**\n+\t\t * InputStream Wrapper.\n+\t\t */\n+\t\tprivate final DataInputViewStreamWrapper baisWrapper;\n+\n+\t\t/**\n+\t\t * The cache of the stateDescriptors.\n+\t\t */\n+\t\tfinal Map<String, StateDescriptor> stateDescriptorCache;\n+\n+\t\tSimpleStateRequestHandler(\n+\t\t\tKeyedStateBackend keyedStateBackend,\n+\t\t\tTypeSerializer keySerializer) {\n+\t\t\tthis.keySerializer = keySerializer;\n+\t\t\tthis.valueSerializer = PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO\n+\t\t\t\t.createSerializer(new ExecutionConfig());\n+\t\t\tthis.keyedStateBackend = keyedStateBackend;", "originalCommit": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAxMjY5Ng==", "url": "https://github.com/apache/flink/pull/13420#discussion_r492012696", "bodyText": "Remove the RowData prefix as we will not support the old planner any more and so the prefix is not necessary.", "author": "dianfu", "createdAt": "2020-09-21T12:42:19Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator", "originalCommit": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAxMzYxNw==", "url": "https://github.com/apache/flink/pull/13420#discussion_r492013617", "bodyText": "Move this line to the front of the class?", "author": "dianfu", "createdAt": "2020-09-21T12:43:18Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractOneInputPythonFunctionOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\tprivate static final long serialVersionUID = 1L;", "originalCommit": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAxNDAyNA==", "url": "https://github.com/apache/flink/pull/13420#discussion_r492014024", "bodyText": "private?", "author": "dianfu", "createdAt": "2020-09-21T12:43:45Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractOneInputPythonFunctionOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\t/**\n+\t * The array of the key indexes.\n+\t */\n+\tprivate final int[] grouping;\n+\n+\t/**\n+\t * The index of a count aggregate used to calculate the number of accumulated rows.\n+\t */\n+\tprivate final int indexOfCountStar;\n+\n+\t/**\n+\t * Generate retract messages if true.\n+\t */\n+\tprivate final boolean generateUpdateBefore;\n+\n+\t/**\n+\t * The minimum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long minRetentionTime;", "originalCommit": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAxNDEyMg==", "url": "https://github.com/apache/flink/pull/13420#discussion_r492014122", "bodyText": "ditto", "author": "dianfu", "createdAt": "2020-09-21T12:43:52Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractOneInputPythonFunctionOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\t/**\n+\t * The array of the key indexes.\n+\t */\n+\tprivate final int[] grouping;\n+\n+\t/**\n+\t * The index of a count aggregate used to calculate the number of accumulated rows.\n+\t */\n+\tprivate final int indexOfCountStar;\n+\n+\t/**\n+\t * Generate retract messages if true.\n+\t */\n+\tprivate final boolean generateUpdateBefore;\n+\n+\t/**\n+\t * The minimum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long minRetentionTime;\n+\n+\t/**\n+\t * The maximum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long maxRetentionTime;", "originalCommit": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAxNDQ1OQ==", "url": "https://github.com/apache/flink/pull/13420#discussion_r492014459", "bodyText": "can be private", "author": "dianfu", "createdAt": "2020-09-21T12:44:23Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractOneInputPythonFunctionOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\t/**\n+\t * The array of the key indexes.\n+\t */\n+\tprivate final int[] grouping;\n+\n+\t/**\n+\t * The index of a count aggregate used to calculate the number of accumulated rows.\n+\t */\n+\tprivate final int indexOfCountStar;\n+\n+\t/**\n+\t * Generate retract messages if true.\n+\t */\n+\tprivate final boolean generateUpdateBefore;\n+\n+\t/**\n+\t * The minimum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long minRetentionTime;\n+\n+\t/**\n+\t * The maximum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long maxRetentionTime;\n+\n+\t/**\n+\t * The maximum NUMBER of the states cached in Python side.\n+\t */\n+\tprivate int stateCacheSize = DEFAULT_STATE_CACHE_SIZE;\n+\n+\t/**\n+\t * Indicates whether state cleaning is enabled. Can be calculated from the `minRetentionTime`.\n+\t */\n+\tprotected final boolean stateCleaningEnabled;", "originalCommit": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAyMTg0MA==", "url": "https://github.com/apache/flink/pull/13420#discussion_r492021840", "bodyText": "Add some comments about the meaning of each column?", "author": "dianfu", "createdAt": "2020-09-21T12:55:59Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractOneInputPythonFunctionOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\t/**\n+\t * The array of the key indexes.\n+\t */\n+\tprivate final int[] grouping;\n+\n+\t/**\n+\t * The index of a count aggregate used to calculate the number of accumulated rows.\n+\t */\n+\tprivate final int indexOfCountStar;\n+\n+\t/**\n+\t * Generate retract messages if true.\n+\t */\n+\tprivate final boolean generateUpdateBefore;\n+\n+\t/**\n+\t * The minimum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long minRetentionTime;\n+\n+\t/**\n+\t * The maximum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long maxRetentionTime;\n+\n+\t/**\n+\t * The maximum NUMBER of the states cached in Python side.\n+\t */\n+\tprivate int stateCacheSize = DEFAULT_STATE_CACHE_SIZE;\n+\n+\t/**\n+\t * Indicates whether state cleaning is enabled. Can be calculated from the `minRetentionTime`.\n+\t */\n+\tprotected final boolean stateCleaningEnabled;\n+\n+\tprivate transient Object keyForTimerService;\n+\n+\t/**\n+\t * The user-defined function input logical type.\n+\t */\n+\tprotected transient RowType userDefinedFunctionInputType;\n+\n+\t/**\n+\t * The TypeSerializer for udf execution results.\n+\t */\n+\tprotected transient TypeSerializer<RowData> udfOutputTypeSerializer;\n+\n+\t/**\n+\t * The TypeSerializer for udf input elements.\n+\t */\n+\tprotected transient TypeSerializer<RowData> udfInputTypeSerializer;\n+\n+\t/**\n+\t * Reusable InputStream used to holding the execution results to be deserialized.\n+\t */\n+\tprivate transient ByteArrayInputStreamWithPos bais;\n+\n+\t/**\n+\t * InputStream Wrapper.\n+\t */\n+\tprivate transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t/**\n+\t * Reusable OutputStream used to holding the serialized input elements.\n+\t */\n+\tprivate transient ByteArrayOutputStreamWithPos baos;\n+\n+\t/**\n+\t * OutputStream Wrapper.\n+\t */\n+\tprivate transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\tprivate transient TimerService timerService;\n+\n+\t// holds the latest registered cleanup timer\n+\tprivate transient ValueState<Long> cleanupTimeState;\n+\n+\tprivate transient UpdatableRowData reuseRowData;\n+\tprivate transient UpdatableRowData reuseTimerRowData;\n+\n+\t/**\n+\t * The collector used to collect records.\n+\t */\n+\tprivate transient StreamRecordRowDataWrappingCollector rowDataWrapper;\n+\n+\tpublic RowDataPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(config);\n+\t\tthis.inputType = Preconditions.checkNotNull(inputType);\n+\t\tthis.outputType = Preconditions.checkNotNull(outputType);\n+\t\tthis.aggregateFunctions = aggregateFunctions;\n+\t\tthis.jobOptions = buildJobOptions(config);\n+\t\tthis.grouping = grouping;\n+\t\tthis.indexOfCountStar = indexOfCountStar;\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.minRetentionTime = minRetentionTime;\n+\t\tthis.maxRetentionTime = maxRetentionTime;\n+\t\tthis.stateCleaningEnabled = minRetentionTime > 1;\n+\t}\n+\n+\t/**\n+\t * As the beam state gRPC service will access the KeyedStateBackend in parallel with this operator, we must\n+\t * override this method to prevent changing the current key of the KeyedStateBackend while the beam service\n+\t * is handling requests.\n+\t */\n+\t@Override\n+\tpublic void setCurrentKey(Object key) {\n+\t\tkeyForTimerService = key;\n+\t}\n+\n+\t@Override\n+\tpublic Object getCurrentKey() {\n+\t\treturn keyForTimerService;\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tList<RowType.RowField> fields = new ArrayList<>();\n+\t\tfields.add(new RowType.RowField(\"record_type\", new TinyIntType()));\n+\t\tfields.add(new RowType.RowField(\"row\", inputType));\n+\t\tfields.add(new RowType.RowField(\"timestamp\", new BigIntType()));\n+\t\tfields.add(new RowType.RowField(\"key\", getKeyType()));\n+\t\tuserDefinedFunctionInputType = new RowType(fields);\n+\t\tudfInputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(userDefinedFunctionInputType);\n+\t\tudfOutputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(outputType);\n+\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\ttimerService = new SimpleTimerService(\n+\t\t\tgetInternalTimerService(\"state-clean-timer\", VoidNamespaceSerializer.INSTANCE, this));\n+\t\treuseRowData = new UpdatableRowData(GenericRowData.of(NORMAL_RECORD, null, null, null), 4);", "originalCommit": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAyNDg5Nw==", "url": "https://github.com/apache/flink/pull/13420#discussion_r492024897", "bodyText": "How users could know about this config? Should we add it in PythonConfig?", "author": "dianfu", "createdAt": "2020-09-21T12:58:57Z", "path": "flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/aggregate/RowDataPythonStreamGroupAggregateOperator.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.operators.python.aggregate;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.common.state.ValueState;\n+import org.apache.flink.api.common.state.ValueStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;\n+import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;\n+import org.apache.flink.core.memory.DataInputViewStreamWrapper;\n+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;\n+import org.apache.flink.fnexecution.v1.FlinkFnApi;\n+import org.apache.flink.python.PythonFunctionRunner;\n+import org.apache.flink.runtime.state.VoidNamespace;\n+import org.apache.flink.runtime.state.VoidNamespaceSerializer;\n+import org.apache.flink.streaming.api.SimpleTimerService;\n+import org.apache.flink.streaming.api.TimerService;\n+import org.apache.flink.streaming.api.operators.InternalTimer;\n+import org.apache.flink.streaming.api.operators.Triggerable;\n+import org.apache.flink.streaming.api.operators.python.AbstractOneInputPythonFunctionOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.UpdatableRowData;\n+import org.apache.flink.table.functions.python.PythonEnv;\n+import org.apache.flink.table.functions.python.PythonFunctionInfo;\n+import org.apache.flink.table.planner.plan.utils.KeySelectorUtil;\n+import org.apache.flink.table.runtime.functions.CleanupState;\n+import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;\n+import org.apache.flink.table.runtime.operators.python.utils.PythonOperatorUtils;\n+import org.apache.flink.table.runtime.operators.python.utils.StreamRecordRowDataWrappingCollector;\n+import org.apache.flink.table.runtime.runners.python.beam.BeamTableStatefulPythonFunctionRunner;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;\n+import org.apache.flink.table.types.logical.BigIntType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TinyIntType;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.runtime.typeutils.PythonTypeUtils.toProtoType;\n+\n+/**\n+ *  The Python AggregateFunction operator for the blink planner.\n+ */\n+public class RowDataPythonStreamGroupAggregateOperator\n+\textends AbstractOneInputPythonFunctionOperator<RowData, RowData>\n+\timplements Triggerable<RowData, VoidNamespace>, CleanupState {\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_input:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String FLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN =\n+\t\t\"flink:coder:schema:aggregate_function_output:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final String STREAM_GROUP_AGGREGATE_URN = \"flink:transform:stream_group_aggregate:v1\";\n+\n+\t@VisibleForTesting\n+\tprotected static final byte NORMAL_RECORD = 0;\n+\n+\tprivate static final byte TRIGGER_TIMER = 1;\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate static final int DEFAULT_STATE_CACHE_SIZE = 1000;\n+\n+\t/**\n+\t * The input logical type.\n+\t */\n+\tprotected final RowType inputType;\n+\n+\t/**\n+\t * The output logical type.\n+\t */\n+\tprotected final RowType outputType;\n+\n+\t/**\n+\t * The options used to configure the Python worker process.\n+\t */\n+\tprivate final Map<String, String> jobOptions;\n+\n+\tprivate final PythonFunctionInfo[] aggregateFunctions;\n+\n+\t/**\n+\t * The array of the key indexes.\n+\t */\n+\tprivate final int[] grouping;\n+\n+\t/**\n+\t * The index of a count aggregate used to calculate the number of accumulated rows.\n+\t */\n+\tprivate final int indexOfCountStar;\n+\n+\t/**\n+\t * Generate retract messages if true.\n+\t */\n+\tprivate final boolean generateUpdateBefore;\n+\n+\t/**\n+\t * The minimum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long minRetentionTime;\n+\n+\t/**\n+\t * The maximum time in milliseconds until state which was not updated will be retained.\n+\t */\n+\tfinal long maxRetentionTime;\n+\n+\t/**\n+\t * The maximum NUMBER of the states cached in Python side.\n+\t */\n+\tprivate int stateCacheSize = DEFAULT_STATE_CACHE_SIZE;\n+\n+\t/**\n+\t * Indicates whether state cleaning is enabled. Can be calculated from the `minRetentionTime`.\n+\t */\n+\tprotected final boolean stateCleaningEnabled;\n+\n+\tprivate transient Object keyForTimerService;\n+\n+\t/**\n+\t * The user-defined function input logical type.\n+\t */\n+\tprotected transient RowType userDefinedFunctionInputType;\n+\n+\t/**\n+\t * The TypeSerializer for udf execution results.\n+\t */\n+\tprotected transient TypeSerializer<RowData> udfOutputTypeSerializer;\n+\n+\t/**\n+\t * The TypeSerializer for udf input elements.\n+\t */\n+\tprotected transient TypeSerializer<RowData> udfInputTypeSerializer;\n+\n+\t/**\n+\t * Reusable InputStream used to holding the execution results to be deserialized.\n+\t */\n+\tprivate transient ByteArrayInputStreamWithPos bais;\n+\n+\t/**\n+\t * InputStream Wrapper.\n+\t */\n+\tprivate transient DataInputViewStreamWrapper baisWrapper;\n+\n+\t/**\n+\t * Reusable OutputStream used to holding the serialized input elements.\n+\t */\n+\tprivate transient ByteArrayOutputStreamWithPos baos;\n+\n+\t/**\n+\t * OutputStream Wrapper.\n+\t */\n+\tprivate transient DataOutputViewStreamWrapper baosWrapper;\n+\n+\tprivate transient TimerService timerService;\n+\n+\t// holds the latest registered cleanup timer\n+\tprivate transient ValueState<Long> cleanupTimeState;\n+\n+\tprivate transient UpdatableRowData reuseRowData;\n+\tprivate transient UpdatableRowData reuseTimerRowData;\n+\n+\t/**\n+\t * The collector used to collect records.\n+\t */\n+\tprivate transient StreamRecordRowDataWrappingCollector rowDataWrapper;\n+\n+\tpublic RowDataPythonStreamGroupAggregateOperator(\n+\t\t\tConfiguration config,\n+\t\t\tRowType inputType,\n+\t\t\tRowType outputType,\n+\t\t\tPythonFunctionInfo[] aggregateFunctions,\n+\t\t\tint[] grouping,\n+\t\t\tint indexOfCountStar,\n+\t\t\tboolean generateUpdateBefore,\n+\t\t\tlong minRetentionTime,\n+\t\t\tlong maxRetentionTime) {\n+\t\tsuper(config);\n+\t\tthis.inputType = Preconditions.checkNotNull(inputType);\n+\t\tthis.outputType = Preconditions.checkNotNull(outputType);\n+\t\tthis.aggregateFunctions = aggregateFunctions;\n+\t\tthis.jobOptions = buildJobOptions(config);\n+\t\tthis.grouping = grouping;\n+\t\tthis.indexOfCountStar = indexOfCountStar;\n+\t\tthis.generateUpdateBefore = generateUpdateBefore;\n+\t\tthis.minRetentionTime = minRetentionTime;\n+\t\tthis.maxRetentionTime = maxRetentionTime;\n+\t\tthis.stateCleaningEnabled = minRetentionTime > 1;\n+\t}\n+\n+\t/**\n+\t * As the beam state gRPC service will access the KeyedStateBackend in parallel with this operator, we must\n+\t * override this method to prevent changing the current key of the KeyedStateBackend while the beam service\n+\t * is handling requests.\n+\t */\n+\t@Override\n+\tpublic void setCurrentKey(Object key) {\n+\t\tkeyForTimerService = key;\n+\t}\n+\n+\t@Override\n+\tpublic Object getCurrentKey() {\n+\t\treturn keyForTimerService;\n+\t}\n+\n+\t@Override\n+\tpublic void open() throws Exception {\n+\t\tList<RowType.RowField> fields = new ArrayList<>();\n+\t\tfields.add(new RowType.RowField(\"record_type\", new TinyIntType()));\n+\t\tfields.add(new RowType.RowField(\"row\", inputType));\n+\t\tfields.add(new RowType.RowField(\"timestamp\", new BigIntType()));\n+\t\tfields.add(new RowType.RowField(\"key\", getKeyType()));\n+\t\tuserDefinedFunctionInputType = new RowType(fields);\n+\t\tudfInputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(userDefinedFunctionInputType);\n+\t\tudfOutputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(outputType);\n+\t\tbais = new ByteArrayInputStreamWithPos();\n+\t\tbaisWrapper = new DataInputViewStreamWrapper(bais);\n+\t\tbaos = new ByteArrayOutputStreamWithPos();\n+\t\tbaosWrapper = new DataOutputViewStreamWrapper(baos);\n+\t\ttimerService = new SimpleTimerService(\n+\t\t\tgetInternalTimerService(\"state-clean-timer\", VoidNamespaceSerializer.INSTANCE, this));\n+\t\treuseRowData = new UpdatableRowData(GenericRowData.of(NORMAL_RECORD, null, null, null), 4);\n+\t\treuseTimerRowData = new UpdatableRowData(GenericRowData.of(TRIGGER_TIMER, null, null, null), 4);\n+\t\trowDataWrapper = new StreamRecordRowDataWrappingCollector(output);\n+\t\tinitCleanupTimeState();\n+\t\tsuper.open();\n+\t}\n+\n+\t@Override\n+\tpublic PythonFunctionRunner createPythonFunctionRunner() throws Exception {\n+\t\treturn new BeamTableStatefulPythonFunctionRunner(\n+\t\t\tgetRuntimeContext().getTaskName(),\n+\t\t\tcreatePythonEnvironmentManager(),\n+\t\t\tuserDefinedFunctionInputType,\n+\t\t\toutputType,\n+\t\t\tSTREAM_GROUP_AGGREGATE_URN,\n+\t\t\tgetUserDefinedFunctionsProto(),\n+\t\t\tFLINK_AGGREGATE_FUNCTION_INPUT_SCHEMA_CODER_URN,\n+\t\t\tFLINK_AGGREGATE_FUNCTION_OUTPUT_SCHEMA_CODER_URN,\n+\t\t\tjobOptions,\n+\t\t\tgetFlinkMetricContainer(),\n+\t\t\tgetKeyedStateBackend(),\n+\t\t\tgetKeySerializer());\n+\t}\n+\n+\t@Override\n+\tpublic void processElement(StreamRecord<RowData> element) throws Exception {\n+\t\tRowData value = element.getValue();\n+\t\tprocessElementInternal(value);\n+\t\telementCount++;\n+\t\tcheckInvokeFinishBundleByCount();\n+\t\temitResults();\n+\t}\n+\n+\t@Override\n+\tpublic void emitResult(Tuple2<byte[], Integer> resultTuple) throws Exception {\n+\t\tbyte[] rawUdfResult = resultTuple.f0;\n+\t\tint length = resultTuple.f1;\n+\t\tbais.setBuffer(rawUdfResult, 0, length);\n+\t\tRowData udfResult = udfOutputTypeSerializer.deserialize(baisWrapper);\n+\t\trowDataWrapper.collect(udfResult);\n+\t}\n+\n+\t/**\n+\t * Invoked when an event-time timer fires.\n+\t */\n+\t@Override\n+\tpublic void onEventTime(InternalTimer<RowData, VoidNamespace> timer) {\n+\t\t// ignore\n+\t}\n+\n+\t/**\n+\t * Invoked when a processing-time timer fires.\n+\t */\n+\t@Override\n+\tpublic void onProcessingTime(InternalTimer<RowData, VoidNamespace> timer) throws Exception {\n+\t\tif (stateCleaningEnabled) {\n+\t\t\tRowData key = timer.getKey();\n+\t\t\tlong timestamp = timer.getTimestamp();\n+\t\t\treuseTimerRowData.setLong(2, timestamp);\n+\t\t\treuseTimerRowData.setField(3, key);\n+\t\t\tudfInputTypeSerializer.serialize(reuseTimerRowData, baosWrapper);\n+\t\t\tpythonFunctionRunner.process(baos.toByteArray());\n+\t\t\tbaos.reset();\n+\t\t\telementCount++;\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic PythonEnv getPythonEnv() {\n+\t\treturn aggregateFunctions[0].getPythonFunction().getPythonEnv();\n+\t}\n+\n+\tprivate void registerProcessingCleanupTimer(long currentTime) throws Exception {\n+\t\tif (stateCleaningEnabled) {\n+\t\t\tsynchronized (getKeyedStateBackend()) {\n+\t\t\t\tgetKeyedStateBackend().setCurrentKey(getCurrentKey());\n+\t\t\t\tregisterProcessingCleanupTimer(\n+\t\t\t\t\tcleanupTimeState,\n+\t\t\t\t\tcurrentTime,\n+\t\t\t\t\tminRetentionTime,\n+\t\t\t\t\tmaxRetentionTime,\n+\t\t\t\t\ttimerService\n+\t\t\t\t);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate void initCleanupTimeState() {\n+\t\tif (stateCleaningEnabled) {\n+\t\t\tValueStateDescriptor<Long> inputCntDescriptor =\n+\t\t\t\tnew ValueStateDescriptor<>(\"PythonGroupAggregateCleanupTime\", Types.LONG);\n+\t\t\tcleanupTimeState = getRuntimeContext().getState(inputCntDescriptor);\n+\t\t}\n+\t}\n+\n+\tprivate Map<String, String> buildJobOptions(Configuration config) {\n+\t\tMap<String, String> jobOptions = new HashMap<>();\n+\t\tif (config.containsKey(\"table.exec.timezone\")) {\n+\t\t\tjobOptions.put(\"table.exec.timezone\", config.getString(\"table.exec.timezone\", null));\n+\t\t}\n+\t\tif (config.containsKey(\"python.state.cache.size\")) {", "originalCommit": "87967d2198d73dea7a84808d6c1a5076a1c7c317", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8a990fb143b847582116fb3178d6350c3e0d38ed", "url": "https://github.com/apache/flink/commit/8a990fb143b847582116fb3178d6350c3e0d38ed", "message": "address comments", "committedDate": "2020-09-22T07:17:57Z", "type": "commit"}, {"oid": "7571b70b754ee4e740b3a3777c506fb8f5e67c40", "url": "https://github.com/apache/flink/commit/7571b70b754ee4e740b3a3777c506fb8f5e67c40", "message": "address comments", "committedDate": "2020-09-22T09:41:07Z", "type": "commit"}]}