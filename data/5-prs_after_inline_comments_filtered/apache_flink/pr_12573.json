{"pr_number": 12573, "pr_title": "[FLINK-18232][hive] Fix Hive streaming source bugs", "pr_createdAt": "2020-06-10T07:32:13Z", "pr_url": "https://github.com/apache/flink/pull/12573", "timeline": [{"oid": "afc18c9ef5b1c7a762ba55b020412578adac402f", "url": "https://github.com/apache/flink/commit/afc18c9ef5b1c7a762ba55b020412578adac402f", "message": "[FLINK-18232][hive] Fix Hive streaming source bugs", "committedDate": "2020-06-10T07:28:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDkxMA==", "url": "https://github.com/apache/flink/pull/12573#discussion_r437930910", "bodyText": "Can we update the doc of FileInputSplit::getLength() to indicate length == -1 means to read all data from the file? I'll feel more comfortable about this change if it's guaranteed by the API contract.", "author": "lirui-apache", "createdAt": "2020-06-10T07:54:53Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java", "diffHunk": "@@ -52,16 +53,26 @@ public HiveTableFileInputFormat(\n \n \t@Override\n \tpublic void open(FileInputSplit fileSplit) throws IOException {\n-\t\tURI uri = fileSplit.getPath().toUri();\n \t\tHiveTableInputSplit split = new HiveTableInputSplit(\n \t\t\t\tfileSplit.getSplitNumber(),\n-\t\t\t\tnew FileSplit(new Path(uri), fileSplit.getStart(), fileSplit.getLength(), (String[]) null),\n+\t\t\t\ttoHadoopFileSplit(fileSplit),\n \t\t\t\tinputFormat.getJobConf(),\n-\t\t\t\thiveTablePartition\n-\t\t);\n+\t\t\t\thiveTablePartition);\n \t\tinputFormat.open(split);\n \t}\n \n+\t@VisibleForTesting\n+\tstatic FileSplit toHadoopFileSplit(FileInputSplit fileSplit) throws IOException {\n+\t\tURI uri = fileSplit.getPath().toUri();\n+\t\tlong length = fileSplit.getLength();\n+\t\t// Hadoop FileSplit should not have -1 length.\n+\t\tif (length == -1) {", "originalCommit": "afc18c9ef5b1c7a762ba55b020412578adac402f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzk1Mzg0OA==", "url": "https://github.com/apache/flink/pull/12573#discussion_r437953848", "bodyText": "Actually, there are comments in FileInputSplit: the number of bytes in the file to process (-1 is flag for \"read whole file\")", "author": "JingsongLi", "createdAt": "2020-06-10T08:32:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzkzMDkxMA=="}], "type": "inlineReview"}, {"oid": "a6ef0628e2b21e484bfefc7fb31bf7c1649ffe34", "url": "https://github.com/apache/flink/commit/a6ef0628e2b21e484bfefc7fb31bf7c1649ffe34", "message": "Fix", "committedDate": "2020-06-10T08:48:40Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODEwMDI1NQ==", "url": "https://github.com/apache/flink/pull/12573#discussion_r438100255", "bodyText": "Add some comments for this method? What is a suitable partition?", "author": "lirui-apache", "createdAt": "2020-06-10T12:57:35Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/DirectoryMonitorDiscovery.java", "diffHunk": "@@ -41,18 +44,36 @@\n \t\t\tContext context, long previousTimestamp) throws Exception {\n \t\tFileStatus[] statuses = getFileStatusRecurse(\n \t\t\t\tcontext.tableLocation(), context.partitionKeys().size(), context.fileSystem());\n+\t\tList<Tuple2<List<String>, Long>> partValueList = suitablePartitions(context, previousTimestamp, statuses);\n+\n \t\tList<Tuple2<Partition, Long>> partitions = new ArrayList<>();\n+\t\tfor (Tuple2<List<String>, Long> tuple2 : partValueList) {\n+\t\t\tcontext.getPartition(tuple2.f0).ifPresent(\n+\t\t\t\t\tpartition -> partitions.add(new Tuple2<>(partition, tuple2.f1)));\n+\t\t}\n+\t\treturn partitions;\n+\t}\n+\n+\t@VisibleForTesting\n+\tstatic List<Tuple2<List<String>, Long>> suitablePartitions(", "originalCommit": "a6ef0628e2b21e484bfefc7fb31bf7c1649ffe34", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f65686a5a865c2165659f14c72a6ba8b19153d35", "url": "https://github.com/apache/flink/commit/f65686a5a865c2165659f14c72a6ba8b19153d35", "message": "Fix comments", "committedDate": "2020-06-11T02:29:07Z", "type": "commit"}]}