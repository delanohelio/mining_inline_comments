{"pr_number": 12687, "pr_title": "[FLINK-17678][hbase] Support fink-sql-connector-hbase", "pr_createdAt": "2020-06-16T16:36:55Z", "pr_url": "https://github.com/apache/flink/pull/12687", "timeline": [{"oid": "472ebc575789c2d6e5f5d6ac50bfd54c93c600ea", "url": "https://github.com/apache/flink/commit/472ebc575789c2d6e5f5d6ac50bfd54c93c600ea", "message": "update e2e test with new connector format", "committedDate": "2020-06-17T06:49:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM2NzY0Mg==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441367642", "bodyText": "Simplify to\nAutoClosableProcess\n\t.create(hbaseDir.resolve(Paths.get(\"bin\", \"hbase\")).toString(), \"shell\")\n\t.setStdoutProcessor(stdoutProcessor)\n\t.setStdInputs(cmd)\n\t.runBlocking();", "author": "wuchong", "createdAt": "2020-06-17T08:19:41Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-hbase/src/main/java/org/apache/flink/tests/util/hbase/LocalStandaloneHBaseResource.java", "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.tests.util.hbase;\n+\n+import org.apache.flink.tests.util.AutoClosableProcess;\n+import org.apache.flink.tests.util.CommandLineWrapper;\n+import org.apache.flink.tests.util.activation.OperatingSystemRestriction;\n+import org.apache.flink.tests.util.cache.DownloadCache;\n+import org.apache.flink.util.OperatingSystem;\n+\n+import org.junit.rules.TemporaryFolder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.PrintStream;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * {@link HBaseResource} that downloads hbase and set up a local hbase cluster.\n+ */\n+public class LocalStandaloneHBaseResource implements HBaseResource {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(LocalStandaloneHBaseResource.class);\n+\n+\tprivate final TemporaryFolder tmp = new TemporaryFolder();\n+\n+\tprivate final DownloadCache downloadCache = DownloadCache.get();\n+\tprivate Path hbaseDir;\n+\n+\tLocalStandaloneHBaseResource() {\n+\t\tOperatingSystemRestriction.forbid(\n+\t\t\tString.format(\"The %s relies on UNIX utils and shell scripts.\", getClass().getSimpleName()),\n+\t\t\tOperatingSystem.WINDOWS);\n+\t}\n+\n+\tprivate static String getHBaseDownloadUrl() {\n+\t\treturn \"https://archive.apache.org/dist/hbase/1.4.3/hbase-1.4.3-bin.tar.gz\";\n+\t}\n+\n+\t@Override\n+\tpublic void before() throws Exception {\n+\t\ttmp.create();\n+\t\tdownloadCache.before();\n+\n+\t\tthis.hbaseDir = tmp.newFolder(\"hbase\").toPath().toAbsolutePath();\n+\t\tsetupHBaseDist();\n+\t\tsetupHBaseCluster();\n+\t}\n+\n+\tprivate void setupHBaseDist() throws IOException {\n+\t\tfinal Path downloadDirectory = tmp.newFolder(\"getOrDownload\").toPath();\n+\t\tfinal Path hbaseArchive = downloadCache.getOrDownload(getHBaseDownloadUrl(), downloadDirectory);\n+\n+\t\tLOG.info(\"HBase localtion: {}\", hbaseDir.toAbsolutePath());\n+\t\tAutoClosableProcess.runBlocking(CommandLineWrapper\n+\t\t\t.tar(hbaseArchive)\n+\t\t\t.extract()\n+\t\t\t.zipped()\n+\t\t\t.strip(1)\n+\t\t\t.targetDir(hbaseDir)\n+\t\t\t.build());\n+\n+\t\tLOG.info(\"Configure {} as hbase.tmp.dir\", hbaseDir.toAbsolutePath());\n+\t\tfinal String tmpDirConfig = \"<configuration><property><name>hbase.tmp.dir</name><value>\" + hbaseDir + \"</value></property></configuration>\";\n+\t\tFiles.write(hbaseDir.resolve(Paths.get(\"conf\", \"hbase-site.xml\")), tmpDirConfig.getBytes());\n+\t}\n+\n+\tprivate void setupHBaseCluster() throws IOException {\n+\t\tLOG.info(\"Starting HBase cluster\");\n+\t\tAutoClosableProcess.runBlocking(\n+\t\t\thbaseDir.resolve(Paths.get(\"bin\", \"start-hbase.sh\")).toString());\n+\n+\t\twhile (!isHBaseRunning()) {\n+\t\t\ttry {\n+\t\t\t\tLOG.info(\"Waiting for HBase to start\");\n+\t\t\t\tThread.sleep(500L);\n+\t\t\t} catch (InterruptedException e) {\n+\t\t\t\tThread.currentThread().interrupt();\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void afterTestSuccess() {\n+\t\ttry {\n+\t\t\tLOG.info(\"Stopping HBase Cluster\");\n+\t\t\tAutoClosableProcess.runBlocking(\n+\t\t\t\thbaseDir.resolve(Paths.get(\"bin\", \"hbase-daemon.sh\")).toString(),\n+\t\t\t\t\"stop\",\n+\t\t\t\t\"master\");\n+\n+\t\t\twhile (isHBaseRunning()) {\n+\t\t\t\ttry {\n+\t\t\t\t\tLOG.info(\"Waiting for HBase to stop\");\n+\t\t\t\t\tThread.sleep(500L);\n+\t\t\t\t} catch (InterruptedException e) {\n+\t\t\t\t\tThread.currentThread().interrupt();\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t} catch (IOException ioe) {\n+\t\t\tLOG.warn(\"Error while shutting down hbase.\", ioe);\n+\t\t}\n+\t\tdownloadCache.afterTestSuccess();\n+\t\ttmp.delete();\n+\t}\n+\n+\tprivate static boolean isHBaseRunning() {\n+\t\ttry {\n+\t\t\tfinal AtomicBoolean atomicHMasterStarted = new AtomicBoolean(false);\n+\t\t\tqueryHMasterStatus(line -> atomicHMasterStarted.compareAndSet(false, line.contains(\"HMaster\")));\n+\t\t\treturn atomicHMasterStarted.get();\n+\t\t} catch (IOException ioe) {\n+\t\t\treturn false;\n+\t\t}\n+\t}\n+\n+\tprivate static void queryHMasterStatus(final Consumer<String> stdoutProcessor) throws IOException {\n+\t\tAutoClosableProcess\n+\t\t\t.create(\"jps\")\n+\t\t\t.setStdoutProcessor(stdoutProcessor)\n+\t\t\t.runBlocking();\n+\t}\n+\n+\t@Override\n+\tpublic void createTable(String tableName, String... columnFamilies) throws IOException {\n+\t\tfinal String createTable = String.format(\"create '%s',\", tableName) +\n+\t\t\tArrays.stream(columnFamilies)\n+\t\t\t\t.map(cf -> String.format(\"{NAME=>'%s'}\", cf))\n+\t\t\t\t.collect(Collectors.joining(\",\"));\n+\n+\t\texecuteHBaseShell(createTable);\n+\t}\n+\n+\t@Override\n+\tpublic List<String> scanTable(String tableName) throws IOException {\n+\t\tfinal List<String> result = new ArrayList<>();\n+\t\texecuteHBaseShell(String.format(\"scan '%s'\", tableName), line -> {\n+\t\t\tif (line.contains(\"value=\")) {\n+\t\t\t\tresult.add(line);\n+\t\t\t}\n+\t\t});\n+\t\treturn result;\n+\t}\n+\n+\t@Override\n+\tpublic void putData(String tableName, String rowKey, String columnFamily, String columnQualifier, String value) throws IOException {\n+\t\texecuteHBaseShell(\n+\t\t\tString.format(\"put '%s','%s','%s:%s','%s'\", tableName, rowKey, columnFamily, columnQualifier, value));\n+\t}\n+\n+\tprivate void executeHBaseShell(String cmd) throws IOException {\n+\t\texecuteHBaseShell(cmd, line -> {\n+\t\t});\n+\t}\n+\n+\tprivate void executeHBaseShell(String cmd, Consumer<String> stdoutProcessor) throws IOException {\n+\t\ttry (AutoClosableProcess autoClosableProcess = AutoClosableProcess\n+\t\t\t.create(hbaseDir.resolve(Paths.get(\"bin\", \"hbase\")).toString(), \"shell\")\n+\t\t\t.setStdoutProcessor(stdoutProcessor)\n+\t\t\t.runNonBlocking()) {\n+\n+\t\t\ttry (PrintStream printStream = new PrintStream(autoClosableProcess.getProcess().getOutputStream(), true, StandardCharsets.UTF_8.name())) {\n+\t\t\t\tLOG.info(\"Executing hbase shell: {}\", cmd);\n+\t\t\t\tprintStream.println(cmd);\n+\t\t\t}\n+\n+\t\t\ttry {\n+\t\t\t\tautoClosableProcess.getProcess().waitFor();\n+\t\t\t} catch (InterruptedException e) {\n+\t\t\t\tThread.currentThread().interrupt();\n+\t\t\t}\n+\t\t}\n+\t}", "originalCommit": "8a8a8090a66006f8386f0737b08d5428fdc53394", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM3MDk0NA==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441370944", "bodyText": "Not found yarn.classpath?", "author": "wuchong", "createdAt": "2020-06-17T08:25:11Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-hbase/src/test/java/org/apache/flink/tests/util/hbase/SQLClientHBaseITCase.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.tests.util.hbase;\n+\n+import org.apache.flink.api.common.time.Deadline;\n+import org.apache.flink.tests.util.TestUtils;\n+import org.apache.flink.tests.util.cache.DownloadCache;\n+import org.apache.flink.tests.util.categories.PreCommit;\n+import org.apache.flink.tests.util.categories.TravisGroup1;\n+import org.apache.flink.tests.util.flink.ClusterController;\n+import org.apache.flink.tests.util.flink.FlinkResource;\n+import org.apache.flink.tests.util.flink.FlinkResourceSetup;\n+import org.apache.flink.tests.util.flink.LocalStandaloneFlinkResourceFactory;\n+import org.apache.flink.tests.util.flink.SQLJobSubmission;\n+import org.apache.flink.util.FileUtils;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.hamcrest.CoreMatchers;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TemporaryFolder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.time.Duration;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.hamcrest.Matchers.arrayContainingInAnyOrder;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * End-to-end test for the HBase connectors.\n+ */\n+@Category(value = {TravisGroup1.class, PreCommit.class})\n+public class SQLClientHBaseITCase extends TestLogger {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(SQLClientHBaseITCase.class);\n+\n+\tprivate static final String HBASE_E2E_SQL = \"hbase_e2e.sql\";\n+\n+\t@Rule\n+\tpublic final HBaseResource hbase;\n+\n+\t@Rule\n+\tpublic final FlinkResource flink = new LocalStandaloneFlinkResourceFactory()\n+\t\t.create(FlinkResourceSetup.builder().build());\n+\n+\t@Rule\n+\tpublic final TemporaryFolder tmp = new TemporaryFolder();\n+\n+\t@ClassRule\n+\tpublic static final DownloadCache DOWNLOAD_CACHE = DownloadCache.get();\n+\n+\tprivate static final Path sqlToolBoxJar = TestUtils.getResourceJar(\".*SqlToolbox.jar\");\n+\tprivate static final Path sqlConnectorHBaseJar = TestUtils.getResourceJar(\".*hbase.jar\");\n+\tprivate List<Path> hadoopClasspathJars;\n+\n+\tpublic SQLClientHBaseITCase() {\n+\t\tthis.hbase = HBaseResource.get();\n+\t}\n+\n+\t@Before\n+\tpublic void before() throws Exception {\n+\t\tDOWNLOAD_CACHE.before();\n+\t\tPath tmpPath = tmp.getRoot().toPath();\n+\t\tLOG.info(\"The current temporary path: {}\", tmpPath);\n+\n+\t\t// Prepare all hadoop jars under HADOOP_CLASSPATH, use yarn.classpath which contains all hadoop jars\n+\t\tPath currentModulePath = Paths.get(\"\").toAbsolutePath();\n+\t\tPath flinkHomePath = currentModulePath.getParent().getParent().toAbsolutePath();\n+\t\tPath hadoopClasspath = Paths.get(flinkHomePath.toString(), \"/flink-yarn-tests/target/yarn.classpath\");\n+\t\tFile hadoopClasspathFile = new File(hadoopClasspath.toString());\n+\n+\t\tif (!hadoopClasspathFile.exists()) {\n+\t\t\tthrow new FileNotFoundException(HBASE_E2E_SQL);", "originalCommit": "8a8a8090a66006f8386f0737b08d5428fdc53394", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk1NDczNA==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441954734", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\t\t\t\" automatically. Please point the 'rootDir' property to the  flink project root directory;\" +\n          \n          \n            \n            \t\t\t\t\t\t\" automatically. Please point the 'rootDir' property to the flink project root directory;\" +", "author": "wuchong", "createdAt": "2020-06-18T03:58:20Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/flink/LocalStandaloneFlinkResourceFactory.java", "diffHunk": "@@ -80,6 +80,33 @@ public FlinkResource create(FlinkResourceSetup setup) {\n \t\treturn new LocalStandaloneFlinkResource(distributionDirectory.get(), logBackupDirectory.orElse(null), setup);\n \t}\n \n+\t/**\n+\t * Utils to find the flink project root directory.\n+\t * @param currentDirectory\n+\t * @return The flink project root directory.\n+\t */\n+\tpublic static Path getProjectRootDirectory(Path currentDirectory) {\n+\t\tPath projectRootPath;\n+\t\tOptional<Path> projectRoot = PROJECT_ROOT_DIRECTORY.get();\n+\t\tif (projectRoot.isPresent()) {\n+\t\t\t// running with maven\n+\t\t\tprojectRootPath = projectRoot.get();\n+\t\t} else {\n+\t\t\t// running in the IDE; working directory is test module\n+\t\t\tOptional<Path> projectRootDirectory = findProjectRootDirectory(currentDirectory);\n+\t\t\t// this distinction is required in case this class is used outside of Flink\n+\t\t\tif (projectRootDirectory.isPresent()) {\n+\t\t\t\tprojectRootPath = projectRootDirectory.get();\n+\t\t\t} else {\n+\t\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\t\"The 'rootDir' property was not set and the flink project root directory could not be found\" +\n+\t\t\t\t\t\t\" automatically. Please point the 'rootDir' property to the  flink project root directory;\" +", "originalCommit": "7aa36ae36a4ab5db1dc6fd03b0ed8b3d8ac9b044", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk1NDk5Ng==", "url": "https://github.com/apache/flink/pull/12687#discussion_r441954996", "bodyText": "We can use this method to replace the code block in LocalStandaloneFlinkResourceFactory#create().", "author": "wuchong", "createdAt": "2020-06-18T03:59:26Z", "path": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/flink/LocalStandaloneFlinkResourceFactory.java", "diffHunk": "@@ -80,6 +80,33 @@ public FlinkResource create(FlinkResourceSetup setup) {\n \t\treturn new LocalStandaloneFlinkResource(distributionDirectory.get(), logBackupDirectory.orElse(null), setup);\n \t}\n \n+\t/**\n+\t * Utils to find the flink project root directory.\n+\t * @param currentDirectory\n+\t * @return The flink project root directory.\n+\t */\n+\tpublic static Path getProjectRootDirectory(Path currentDirectory) {", "originalCommit": "7aa36ae36a4ab5db1dc6fd03b0ed8b3d8ac9b044", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "769512268be9cb063af00d107cbd1382576fb6fb", "url": "https://github.com/apache/flink/commit/769512268be9cb063af00d107cbd1382576fb6fb", "message": "address Chesnay's comments", "committedDate": "2020-06-18T09:30:37Z", "type": "forcePushed"}, {"oid": "15681586a0c9fab7bfbefe943e7b6b1cd6700864", "url": "https://github.com/apache/flink/commit/15681586a0c9fab7bfbefe943e7b6b1cd6700864", "message": "[FLINK-17678][hbase] Support fink-sql-connector-hbase", "committedDate": "2020-06-22T12:16:20Z", "type": "commit"}, {"oid": "15681586a0c9fab7bfbefe943e7b6b1cd6700864", "url": "https://github.com/apache/flink/commit/15681586a0c9fab7bfbefe943e7b6b1cd6700864", "message": "[FLINK-17678][hbase] Support fink-sql-connector-hbase", "committedDate": "2020-06-22T12:16:20Z", "type": "forcePushed"}]}