{"pr_number": 13724, "pr_title": "[FLINK-19581][orc] Introduce Orc ColumnarRow File source bulk Format", "pr_createdAt": "2020-10-21T11:51:47Z", "pr_url": "https://github.com/apache/flink/pull/13724", "timeline": [{"oid": "e998711a72202b142ba69096b8479f961c6c950a", "url": "https://github.com/apache/flink/commit/e998711a72202b142ba69096b8479f961c6c950a", "message": "[FLINK-19581][orc] Introduce Orc ColumnarRow File source bulk Format", "committedDate": "2020-10-21T11:55:45Z", "type": "forcePushed"}, {"oid": "f619476bce3df1f3cff0b0579a0888a98d5312cb", "url": "https://github.com/apache/flink/commit/f619476bce3df1f3cff0b0579a0888a98d5312cb", "message": "[FLINK-19581][orc] Introduce Orc ColumnarRow File source bulk Format", "committedDate": "2020-10-21T11:56:15Z", "type": "forcePushed"}, {"oid": "7df5517fea18dc5665a10303271a2ccd959d71e7", "url": "https://github.com/apache/flink/commit/7df5517fea18dc5665a10303271a2ccd959d71e7", "message": "[FLINK-19581][orc] Introduce Orc ColumnarRow File source bulk Format", "committedDate": "2020-11-04T06:36:37Z", "type": "forcePushed"}, {"oid": "012beed0dd7796ec1fc67a6e717431540461a7e0", "url": "https://github.com/apache/flink/commit/012beed0dd7796ec1fc67a6e717431540461a7e0", "message": "[FLINK-19581][orc] Introduce Orc ColumnarRow File source bulk Format", "committedDate": "2020-11-04T06:38:42Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEyNTcxOQ==", "url": "https://github.com/apache/flink/pull/13724#discussion_r517125719", "bodyText": "This will be introduced in #13919", "author": "JingsongLi", "createdAt": "2020-11-04T06:44:08Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionFieldExtractor.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import java.io.Serializable;\n+import java.util.LinkedHashMap;\n+\n+/**\n+ * Interface to extract partition field from split.\n+ */\n+@FunctionalInterface\n+public interface PartitionFieldExtractor<T extends FileSourceSplit> extends Serializable {", "originalCommit": "012beed0dd7796ec1fc67a6e717431540461a7e0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b90a17be9b69b6db7ff00a4db2b5096da0777673", "url": "https://github.com/apache/flink/commit/b90a17be9b69b6db7ff00a4db2b5096da0777673", "message": "[FLINK-19581][orc] Introduce Orc ColumnarRow File source bulk Format", "committedDate": "2020-11-04T10:07:14Z", "type": "commit"}, {"oid": "3d25c91253ce75af6d2d87b0bae62bc7dbd575bb", "url": "https://github.com/apache/flink/commit/3d25c91253ce75af6d2d87b0bae62bc7dbd575bb", "message": "[FLINK-19581][orc] Integrate orc bulk format to filesystem connector", "committedDate": "2020-11-04T10:07:14Z", "type": "commit"}, {"oid": "3d25c91253ce75af6d2d87b0bae62bc7dbd575bb", "url": "https://github.com/apache/flink/commit/3d25c91253ce75af6d2d87b0bae62bc7dbd575bb", "message": "[FLINK-19581][orc] Integrate orc bulk format to filesystem connector", "committedDate": "2020-11-04T10:07:14Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc3ODQ1MA==", "url": "https://github.com/apache/flink/pull/13724#discussion_r517778450", "bodyText": "Can we make this consistent with ParquetColumnarRowInputFormat? E.g. take produced RowType and no need for selectedFields?", "author": "lirui-apache", "createdAt": "2020-11-05T03:56:20Z", "path": "flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcColumnarRowFileInputFormat.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.orc;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.util.Pool;\n+import org.apache.flink.orc.shim.OrcShim;\n+import org.apache.flink.orc.vector.ColumnBatchFactory;\n+import org.apache.flink.orc.vector.OrcVectorizedBatchWrapper;\n+import org.apache.flink.table.data.ColumnarRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.vector.ColumnVector;\n+import org.apache.flink.table.data.vector.VectorizedColumnBatch;\n+import org.apache.flink.table.filesystem.ColumnarRowIterator;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.TypeDescription;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.orc.OrcSplitReaderUtil.convertToOrcTypeWithPart;\n+import static org.apache.flink.orc.OrcSplitReaderUtil.getNonPartNames;\n+import static org.apache.flink.orc.OrcSplitReaderUtil.getSelectedOrcFields;\n+import static org.apache.flink.orc.vector.AbstractOrcColumnVector.createFlinkVector;\n+import static org.apache.flink.orc.vector.AbstractOrcColumnVector.createFlinkVectorFromConstant;\n+\n+/**\n+ * An ORC reader that produces a stream of {@link ColumnarRowData} records.\n+ *\n+ * <p>This class can add extra fields through {@link ColumnBatchFactory}, for example,\n+ * add partition fields, which can be extracted from path. Therefore, the {@link #getProducedType()}\n+ * may be different and types of extra fields need to be added.\n+ */\n+public class OrcColumnarRowFileInputFormat<BatchT, SplitT extends FileSourceSplit> extends\n+\t\tAbstractOrcFileInputFormat<RowData, BatchT, SplitT> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final ColumnBatchFactory<BatchT, SplitT> batchFactory;\n+\tprivate final RowType projectedOutputType;\n+\n+\tpublic OrcColumnarRowFileInputFormat(\n+\t\t\tfinal OrcShim<BatchT> shim,\n+\t\t\tfinal Configuration hadoopConfig,\n+\t\t\tfinal TypeDescription schema,\n+\t\t\tfinal int[] selectedFields,\n+\t\t\tfinal List<OrcFilters.Predicate> conjunctPredicates,\n+\t\t\tfinal int batchSize,\n+\t\t\tfinal ColumnBatchFactory<BatchT, SplitT> batchFactory,\n+\t\t\tfinal RowType projectedOutputType) {\n+\t\tsuper(shim, hadoopConfig, schema, selectedFields, conjunctPredicates, batchSize);\n+\t\tthis.batchFactory = batchFactory;\n+\t\tthis.projectedOutputType = projectedOutputType;\n+\t}\n+\n+\t@Override\n+\tpublic OrcReaderBatch<RowData, BatchT> createReaderBatch(\n+\t\t\tfinal SplitT split,\n+\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\tfinal Pool.Recycler<OrcReaderBatch<RowData, BatchT>> recycler,\n+\t\t\tfinal int batchSize) {\n+\n+\t\tfinal VectorizedColumnBatch flinkColumnBatch = batchFactory.create(split, orcBatch.getBatch());\n+\t\treturn new VectorizedColumnReaderBatch<>(orcBatch, flinkColumnBatch, recycler);\n+\t}\n+\n+\t@Override\n+\tpublic TypeInformation<RowData> getProducedType() {\n+\t\treturn InternalTypeInfo.of(projectedOutputType);\n+\t}\n+\n+\t// ------------------------------------------------------------------------\n+\n+\t/**\n+\t * One batch of ORC columnar vectors and Flink column vectors.\n+\t */\n+\tprivate static final class VectorizedColumnReaderBatch<BatchT> extends OrcReaderBatch<RowData, BatchT> {\n+\n+\t\tprivate final VectorizedColumnBatch flinkColumnBatch;\n+\t\tprivate final ColumnarRowIterator result;\n+\n+\t\tVectorizedColumnReaderBatch(\n+\t\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\t\tfinal VectorizedColumnBatch flinkColumnBatch,\n+\t\t\t\tfinal Pool.Recycler<OrcReaderBatch<RowData, BatchT>> recycler) {\n+\t\t\tsuper(orcBatch, recycler);\n+\t\t\tthis.flinkColumnBatch = flinkColumnBatch;\n+\t\t\tthis.result = new ColumnarRowIterator(new ColumnarRowData(flinkColumnBatch), this::recycle);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RecordIterator<RowData> convertAndGetIterator(\n+\t\t\t\tfinal OrcVectorizedBatchWrapper<BatchT> orcBatch,\n+\t\t\t\tfinal long startingOffset) {\n+\t\t\t// no copying from the ORC column vectors to the Flink columns vectors necessary,\n+\t\t\t// because they point to the same data arrays internally design\n+\t\t\tint batchSize = orcBatch.size();\n+\t\t\tflinkColumnBatch.setNumRows(batchSize);\n+\t\t\tresult.set(batchSize, startingOffset, 0);\n+\t\t\treturn result;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Create a partitioned {@link OrcColumnarRowFileInputFormat}, the partition columns can be\n+\t * generated by split.\n+\t */\n+\tpublic static <SplitT extends FileSourceSplit> OrcColumnarRowFileInputFormat<VectorizedRowBatch, SplitT> createPartitionedFormat(\n+\t\t\tOrcShim<VectorizedRowBatch> shim,\n+\t\t\tConfiguration hadoopConfig,\n+\t\t\tRowType tableType,", "originalCommit": "3d25c91253ce75af6d2d87b0bae62bc7dbd575bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzgyMTgyNQ==", "url": "https://github.com/apache/flink/pull/13724#discussion_r517821825", "bodyText": "We can not, because orc needs full schema to reader, but parquet does not need.", "author": "JingsongLi", "createdAt": "2020-11-05T06:40:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc3ODQ1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzgyMjA3OQ==", "url": "https://github.com/apache/flink/pull/13724#discussion_r517822079", "bodyText": "And AbstractOrcFileInputFormat is a table-free class.", "author": "JingsongLi", "createdAt": "2020-11-05T06:41:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc3ODQ1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzgyNzU1NQ==", "url": "https://github.com/apache/flink/pull/13724#discussion_r517827555", "bodyText": "OK \ud83e\udd23", "author": "lirui-apache", "createdAt": "2020-11-05T06:57:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc3ODQ1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc4MDEyMQ==", "url": "https://github.com/apache/flink/pull/13724#discussion_r517780121", "bodyText": "Rename this class since it's no longer a FileSystemFormatFactory?", "author": "lirui-apache", "createdAt": "2020-11-05T04:03:50Z", "path": "flink-formats/flink-orc/src/main/java/org/apache/flink/orc/OrcFileSystemFormatFactory.java", "diffHunk": "@@ -18,45 +18,45 @@\n \n package org.apache.flink.orc;\n \n-import org.apache.flink.api.common.io.FileInputFormat;\n-import org.apache.flink.api.common.io.InputFormat;\n import org.apache.flink.api.common.serialization.BulkWriter;\n-import org.apache.flink.api.common.serialization.Encoder;\n import org.apache.flink.configuration.ConfigOption;\n import org.apache.flink.configuration.ReadableConfig;\n-import org.apache.flink.core.fs.FileInputSplit;\n-import org.apache.flink.core.fs.Path;\n+import org.apache.flink.connector.file.src.FileSourceSplit;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.orc.shim.OrcShim;\n import org.apache.flink.orc.vector.RowDataVectorizer;\n import org.apache.flink.orc.writer.OrcBulkWriterFactory;\n+import org.apache.flink.table.connector.ChangelogMode;\n+import org.apache.flink.table.connector.format.BulkDecodingFormat;\n+import org.apache.flink.table.connector.format.EncodingFormat;\n+import org.apache.flink.table.connector.sink.DynamicTableSink;\n+import org.apache.flink.table.connector.source.DynamicTableSource;\n import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.vector.VectorizedColumnBatch;\n import org.apache.flink.table.expressions.Expression;\n-import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.factories.BulkReaderFormatFactory;\n+import org.apache.flink.table.factories.BulkWriterFormatFactory;\n+import org.apache.flink.table.factories.DynamicTableFactory;\n+import org.apache.flink.table.filesystem.FileSystemOptions;\n+import org.apache.flink.table.filesystem.PartitionFieldExtractor;\n import org.apache.flink.table.types.DataType;\n import org.apache.flink.table.types.logical.LogicalType;\n import org.apache.flink.table.types.logical.RowType;\n-import org.apache.flink.table.utils.PartitionPathUtils;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n import org.apache.orc.TypeDescription;\n \n-import java.io.IOException;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.HashSet;\n-import java.util.LinkedHashMap;\n import java.util.List;\n-import java.util.Optional;\n import java.util.Properties;\n import java.util.Set;\n \n-import static org.apache.flink.table.data.vector.VectorizedColumnBatch.DEFAULT_SIZE;\n-import static org.apache.flink.table.filesystem.RowPartitionComputer.restorePartValueFromType;\n-\n /**\n- * Orc {@link FileSystemFormatFactory} for file system.\n+ * Orc format factory for file system.\n  */\n-public class OrcFileSystemFormatFactory implements FileSystemFormatFactory {\n+public class OrcFileSystemFormatFactory implements BulkReaderFormatFactory, BulkWriterFormatFactory {", "originalCommit": "3d25c91253ce75af6d2d87b0bae62bc7dbd575bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzgyMjI4Mw==", "url": "https://github.com/apache/flink/pull/13724#discussion_r517822283", "bodyText": "I think OrcFileFormatFactory is better", "author": "JingsongLi", "createdAt": "2020-11-05T06:42:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc4MDEyMQ=="}], "type": "inlineReview"}, {"oid": "eff771f1557f8fe7dc60a92c22dd1db6a347e047", "url": "https://github.com/apache/flink/commit/eff771f1557f8fe7dc60a92c22dd1db6a347e047", "message": "Address comments", "committedDate": "2020-11-05T06:59:55Z", "type": "commit"}, {"oid": "eff771f1557f8fe7dc60a92c22dd1db6a347e047", "url": "https://github.com/apache/flink/commit/eff771f1557f8fe7dc60a92c22dd1db6a347e047", "message": "Address comments", "committedDate": "2020-11-05T06:59:55Z", "type": "forcePushed"}]}