{"pr_number": 13975, "pr_title": "[FLINK-20041][connector kafka] Support watermark push down for kafka \u2026", "pr_createdAt": "2020-11-07T08:47:18Z", "pr_url": "https://github.com/apache/flink/pull/13975", "timeline": [{"oid": "e1b717404292acb2722298bf58e81622042d9bf4", "url": "https://github.com/apache/flink/commit/e1b717404292acb2722298bf58e81622042d9bf4", "message": "[FLINK-20041][connector kafka] Support watermark push down for kafka in table API", "committedDate": "2020-11-07T08:51:18Z", "type": "commit"}, {"oid": "e1b717404292acb2722298bf58e81622042d9bf4", "url": "https://github.com/apache/flink/commit/e1b717404292acb2722298bf58e81622042d9bf4", "message": "[FLINK-20041][connector kafka] Support watermark push down for kafka in table API", "committedDate": "2020-11-07T08:51:18Z", "type": "forcePushed"}, {"oid": "5947caaa8eca977df7bade88b4c8b576ebc06d2b", "url": "https://github.com/apache/flink/commit/5947caaa8eca977df7bade88b4c8b576ebc06d2b", "message": "refactor the test class", "committedDate": "2020-11-07T09:34:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTE4MzY0NQ==", "url": "https://github.com/apache/flink/pull/13975#discussion_r519183645", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \tprotected WatermarkStrategy<RowData> watermarkStrategy;\n          \n          \n            \n            \tprotected @Nullable WatermarkStrategy<RowData> watermarkStrategy;\n          \n      \n    \n    \n  \n\nMark it nullable.", "author": "wuchong", "createdAt": "2020-11-07T14:37:05Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSource.java", "diffHunk": "@@ -73,6 +75,9 @@\n \t/** Metadata that is appended at the end of a physical source row. */\n \tprotected List<String> metadataKeys;\n \n+\t/** Watermark strategy that is used to generate per-partition watermark. */\n+\tprotected WatermarkStrategy<RowData> watermarkStrategy;", "originalCommit": "5947caaa8eca977df7bade88b4c8b576ebc06d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTE4Mzc4OQ==", "url": "https://github.com/apache/flink/pull/13975#discussion_r519183789", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\treturn record.getInt(PARTITION_ID_FIELD_IN_SCHEMA) % partitions.length;\n          \n          \n            \n            \t\t\treturn partitions[record.getInt(PARTITION_ID_FIELD_IN_SCHEMA) % partitions.length];", "author": "wuchong", "createdAt": "2020-11-07T14:38:56Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaTableITCase.java", "diffHunk": "@@ -516,10 +518,103 @@ public void testKafkaSourceSinkWithKeyAndFullValue() throws Exception {\n \t\tdeleteTestTopic(topic);\n \t}\n \n+\t@Test\n+\tpublic void testPerPartitionWatermarkKafka() throws Exception {\n+\t\tif (isLegacyConnector) {\n+\t\t\treturn;\n+\t\t}\n+\t\t// we always use a different topic name for each parameterized topic,\n+\t\t// in order to make sure the topic can be created.\n+\t\tfinal String topic = \"per_partition_watermark_topic_\" + format;\n+\t\tcreateTestTopic(topic, 4, 1);\n+\n+\t\t// ---------- Produce an event time stream into Kafka -------------------\n+\t\tString groupId = standardProps.getProperty(\"group.id\");\n+\t\tString bootstraps = standardProps.getProperty(\"bootstrap.servers\");\n+\n+\t\tfinal String createTable = String.format(\n+\t\t\t\t\"CREATE TABLE kafka (\\n\"\n+\t\t\t\t\t\t+ \"  `partition_id` INT,\\n\"\n+\t\t\t\t\t\t+ \"  `name` STRING,\\n\"\n+\t\t\t\t\t\t+ \"  `timestamp` TIMESTAMP(3),\\n\"\n+\t\t\t\t\t\t+ \"  WATERMARK FOR `timestamp` AS `timestamp`\\n\"\n+\t\t\t\t\t\t+ \") WITH (\\n\"\n+\t\t\t\t\t\t+ \"  'connector' = 'kafka',\\n\"\n+\t\t\t\t\t\t+ \"  'topic' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'properties.bootstrap.servers' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'properties.group.id' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'scan.startup.mode' = 'earliest-offset',\\n\"\n+\t\t\t\t\t\t+ \"  'sink.partitioner' = 'org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase$TestPartitioner',\\n\"\n+\t\t\t\t\t\t+ \"  'format' = '%s'\\n\"\n+\t\t\t\t\t\t+ \")\",\n+\t\t\t\ttopic,\n+\t\t\t\tbootstraps,\n+\t\t\t\tgroupId,\n+\t\t\t\tformat);\n+\n+\t\ttEnv.executeSql(createTable);\n+\n+\t\tString initialValues = \"INSERT INTO kafka\\n\"\n+\t\t\t\t+ \"VALUES\\n\"\n+\t\t\t\t+ \" (0, 'partition-0-name-0', TIMESTAMP '2020-03-08 13:12:11.123'),\\n\"\n+\t\t\t\t+ \" (0, 'partition-0-name-1', TIMESTAMP '2020-03-08 14:12:12.223'),\\n\"\n+\t\t\t\t+ \" (0, 'partition-0-name-2', TIMESTAMP '2020-03-08 15:12:13.323'),\\n\"\n+\t\t\t\t+ \" (1, 'partition-1-name-0', TIMESTAMP '2020-03-09 13:13:11.123'),\\n\"\n+\t\t\t\t+ \" (1, 'partition-1-name-1', TIMESTAMP '2020-03-09 15:13:11.133'),\\n\"\n+\t\t\t\t+ \" (1, 'partition-1-name-2', TIMESTAMP '2020-03-09 16:13:11.143'),\\n\"\n+\t\t\t\t+ \" (2, 'partition-2-name-0', TIMESTAMP '2020-03-10 13:12:14.123'),\\n\"\n+\t\t\t\t+ \" (3, 'partition-3-name-0', TIMESTAMP '2020-03-11 17:12:11.123')\\n\";\n+\t\ttEnv.executeSql(initialValues).await();\n+\n+\t\t// ---------- Consume stream from Kafka -------------------\n+\t\tString createSink =\n+\t\t\t\t\"CREATE TABLE MySink(\"\n+\t\t\t\t\t\t+ \"  id INT,\"\n+\t\t\t\t\t\t+ \"  name STRING,\"\n+\t\t\t\t\t\t+ \"  ts TIMESTAMP(3)\"\n+\t\t\t\t\t\t+ \") WITH (\"\n+\t\t\t\t\t\t+ \"  'connector' = 'values',\"\n+\t\t\t\t\t\t+ \"  'sink-index-of-rowtime' = '2'\"\n+\t\t\t\t\t\t+ \")\";\n+\t\ttEnv.executeSql(createSink);\n+\t\ttEnv.executeSql(\"INSERT INTO MySink SELECT * FROM kafka\");\n+\t\tfinal List<String> expected = Arrays.asList(\n+\t\t\t\"0,partition-0-name-0,2020-03-08T13:12:11.123\",\n+\t\t\t\"0,partition-0-name-1,2020-03-08T14:12:12.223\",\n+\t\t\t\"0,partition-0-name-2,2020-03-08T15:12:13.323\",\n+\t\t\t\"1,partition-1-name-0,2020-03-09T13:13:11.123\",\n+\t\t\t\"1,partition-1-name-1,2020-03-09T15:13:11.133\",\n+\t\t\t\"1,partition-1-name-2,2020-03-09T16:13:11.143\",\n+\t\t\t\"2,partition-2-name-0,2020-03-10T13:12:14.123\",\n+\t\t\t\"3,partition-3-name-0,2020-03-11T17:12:11.123\"\n+\t\t);\n+\t\tKafkaTableTestUtils.waitingExpectedResults(\"MySink\", expected, Duration.ofSeconds(5));\n+\n+\t\t// ------------- cleanup -------------------\n+\n+\t\tdeleteTestTopic(topic);\n+\t}\n+\n+\n+\n \t// --------------------------------------------------------------------------------------------\n \t// Utilities\n \t// --------------------------------------------------------------------------------------------\n \n+\t/**\n+\t * Extract the partition id from the row and set it on the record.\n+\t */\n+\tpublic static class TestPartitioner extends FlinkKafkaPartitioner<RowData> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate static final int PARTITION_ID_FIELD_IN_SCHEMA = 0;\n+\n+\t\t@Override\n+\t\tpublic int partition(RowData record, byte[] key, byte[] value, String targetTopic, int[] partitions) {\n+\t\t\treturn record.getInt(PARTITION_ID_FIELD_IN_SCHEMA) % partitions.length;", "originalCommit": "5947caaa8eca977df7bade88b4c8b576ebc06d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTE4NDAwMg==", "url": "https://github.com/apache/flink/pull/13975#discussion_r519184002", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\t\t\t+ \"  'sink.partitioner' = 'org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase$TestPartitioner',\\n\"\n          \n          \n            \n            \t\t\t\t\t\t+ \"  'format' = '%s'\\n\"\n          \n          \n            \n            \t\t\t\t\t\t+ \")\",\n          \n          \n            \n            \t\t\t\ttopic,\n          \n          \n            \n            \t\t\t\tbootstraps,\n          \n          \n            \n            \t\t\t\tgroupId,\n          \n          \n            \n            \t\t\t\tformat);\n          \n          \n            \n            \t\t\t\t\t\t+ \"  'sink.partitioner' = '%s',\\n\"\n          \n          \n            \n            \t\t\t\t\t\t+ \"  'format' = '%s'\\n\"\n          \n          \n            \n            \t\t\t\t\t\t+ \")\",\n          \n          \n            \n            \t\t\t\ttopic,\n          \n          \n            \n            \t\t\t\tbootstraps,\n          \n          \n            \n            \t\t\t\tgroupId,\n          \n          \n            \n            \t\t\t\tformat,\n          \n          \n            \n            \t\t\t\tTestPartitioner.class.getCanonicalName());", "author": "wuchong", "createdAt": "2020-11-07T14:40:46Z", "path": "flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaTableITCase.java", "diffHunk": "@@ -516,10 +518,103 @@ public void testKafkaSourceSinkWithKeyAndFullValue() throws Exception {\n \t\tdeleteTestTopic(topic);\n \t}\n \n+\t@Test\n+\tpublic void testPerPartitionWatermarkKafka() throws Exception {\n+\t\tif (isLegacyConnector) {\n+\t\t\treturn;\n+\t\t}\n+\t\t// we always use a different topic name for each parameterized topic,\n+\t\t// in order to make sure the topic can be created.\n+\t\tfinal String topic = \"per_partition_watermark_topic_\" + format;\n+\t\tcreateTestTopic(topic, 4, 1);\n+\n+\t\t// ---------- Produce an event time stream into Kafka -------------------\n+\t\tString groupId = standardProps.getProperty(\"group.id\");\n+\t\tString bootstraps = standardProps.getProperty(\"bootstrap.servers\");\n+\n+\t\tfinal String createTable = String.format(\n+\t\t\t\t\"CREATE TABLE kafka (\\n\"\n+\t\t\t\t\t\t+ \"  `partition_id` INT,\\n\"\n+\t\t\t\t\t\t+ \"  `name` STRING,\\n\"\n+\t\t\t\t\t\t+ \"  `timestamp` TIMESTAMP(3),\\n\"\n+\t\t\t\t\t\t+ \"  WATERMARK FOR `timestamp` AS `timestamp`\\n\"\n+\t\t\t\t\t\t+ \") WITH (\\n\"\n+\t\t\t\t\t\t+ \"  'connector' = 'kafka',\\n\"\n+\t\t\t\t\t\t+ \"  'topic' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'properties.bootstrap.servers' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'properties.group.id' = '%s',\\n\"\n+\t\t\t\t\t\t+ \"  'scan.startup.mode' = 'earliest-offset',\\n\"\n+\t\t\t\t\t\t+ \"  'sink.partitioner' = 'org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase$TestPartitioner',\\n\"\n+\t\t\t\t\t\t+ \"  'format' = '%s'\\n\"\n+\t\t\t\t\t\t+ \")\",\n+\t\t\t\ttopic,\n+\t\t\t\tbootstraps,\n+\t\t\t\tgroupId,\n+\t\t\t\tformat);", "originalCommit": "5947caaa8eca977df7bade88b4c8b576ebc06d2b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTE4NDAzOQ==", "url": "https://github.com/apache/flink/pull/13975#discussion_r519184039", "bodyText": "Do not hard code the class path.", "author": "wuchong", "createdAt": "2020-11-07T14:41:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTE4NDAwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTE4NDE5NA==", "url": "https://github.com/apache/flink/pull/13975#discussion_r519184194", "bodyText": "If this option controls whether to drop late data, then the option name should be something like sink.drop-late-event=true.\nThe rowtime field can be derived from WATERMARK statement (can also be declared on sink), instead of a redundant option.\nBesides, I find you only support this option on append only sink, then you should throw exceptions if the sink is not append only during createDynamicTableSink.", "author": "wuchong", "createdAt": "2020-11-07T14:43:16Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/factories/TestValuesTableFactory.java", "diffHunk": "@@ -291,6 +291,14 @@ private static RowKind parseRowKind(String rowKindShortString) {\n \t\t\t\"Optional map of 'metadata_key:data_type'. The order will be alphabetically. \" +\n \t\t\t\"The metadata is part of the data when enabled.\");\n \n+\tprivate static final ConfigOption<Integer> SINK_INDEX_OF_ROWTIME = ConfigOptions\n+\t\t.key(\"sink-index-of-rowtime\")\n+\t\t.intType()\n+\t\t.defaultValue(-1)\n+\t\t.withDeprecatedKeys(\n+\t\t\t\"Option index of the rowtime field. The default value -1 indicate that don't drop \" +\n+\t\t\t\"the late data.\");", "originalCommit": "5947caaa8eca977df7bade88b4c8b576ebc06d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e9d52fa97e3da9a9b4c24b2ab17037820f26b75e", "url": "https://github.com/apache/flink/commit/e9d52fa97e3da9a9b4c24b2ab17037820f26b75e", "message": "Apply suggestions from code review\r\n\r\napply suggestions\n\nCo-authored-by: Jark Wu <imjark@gmail.com>", "committedDate": "2020-11-07T15:13:51Z", "type": "commit"}, {"oid": "57e3cb930c4a6b2424dc11a7bb8c01134c6912e9", "url": "https://github.com/apache/flink/commit/57e3cb930c4a6b2424dc11a7bb8c01134c6912e9", "message": "address the feedback", "committedDate": "2020-11-07T16:20:08Z", "type": "commit"}]}