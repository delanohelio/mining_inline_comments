{"pr_number": 12908, "pr_title": "[FLINK-18449][table sql/api]Kafka topic discovery & partition discove\u2026", "pr_createdAt": "2020-07-15T13:03:00Z", "pr_url": "https://github.com/apache/flink/pull/12908", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyMzY1NQ==", "url": "https://github.com/apache/flink/pull/12908#discussion_r460723655", "bodyText": "Indent.", "author": "wuchong", "createdAt": "2020-07-27T08:21:27Z", "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/table/Kafka010DynamicSource.java", "diffHunk": "@@ -73,17 +80,26 @@ public Kafka010DynamicSource(\n \n \t@Override\n \tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n-\t\t\tString topic,\n+\t\t\tList<String> topics,\n \t\t\tProperties properties,\n \t\t\tDeserializationSchema<RowData> deserializationSchema) {\n-\t\treturn new FlinkKafkaConsumer010<>(topic, deserializationSchema, properties);\n+\t\treturn new FlinkKafkaConsumer010<>(topics, deserializationSchema, properties);\n+\t}\n+\n+\t@Override\n+\tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n+\t\tPattern pattern,\n+\t\tProperties properties,\n+\t\tDeserializationSchema<RowData> deserializationSchema) {", "originalCommit": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyNTgwOQ==", "url": "https://github.com/apache/flink/pull/12908#discussion_r460725809", "bodyText": "topicPattern", "author": "wuchong", "createdAt": "2020-07-27T08:25:07Z", "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/table/Kafka010DynamicSource.java", "diffHunk": "@@ -73,17 +80,26 @@ public Kafka010DynamicSource(\n \n \t@Override\n \tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n-\t\t\tString topic,\n+\t\t\tList<String> topics,\n \t\t\tProperties properties,\n \t\t\tDeserializationSchema<RowData> deserializationSchema) {\n-\t\treturn new FlinkKafkaConsumer010<>(topic, deserializationSchema, properties);\n+\t\treturn new FlinkKafkaConsumer010<>(topics, deserializationSchema, properties);\n+\t}\n+\n+\t@Override\n+\tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n+\t\tPattern pattern,", "originalCommit": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyNTkyOA==", "url": "https://github.com/apache/flink/pull/12908#discussion_r460725928", "bodyText": "Indent.", "author": "wuchong", "createdAt": "2020-07-27T08:25:19Z", "path": "flink-connectors/flink-connector-kafka-0.11/src/main/java/org/apache/flink/streaming/connectors/kafka/table/Kafka011DynamicSource.java", "diffHunk": "@@ -73,17 +80,26 @@ public Kafka011DynamicSource(\n \n \t@Override\n \tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n-\t\t\tString topic,\n+\t\t\tList<String> topics,\n \t\t\tProperties properties,\n \t\t\tDeserializationSchema<RowData> deserializationSchema) {\n-\t\treturn new FlinkKafkaConsumer011<>(topic, deserializationSchema, properties);\n+\t\treturn new FlinkKafkaConsumer011<>(topics, deserializationSchema, properties);\n+\t}\n+\n+\t@Override\n+\tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n+\t\tPattern topicPattern,\n+\t\tProperties properties,\n+\t\tDeserializationSchema<RowData> deserializationSchema) {", "originalCommit": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyNTk5Mg==", "url": "https://github.com/apache/flink/pull/12908#discussion_r460725992", "bodyText": "Indent.", "author": "wuchong", "createdAt": "2020-07-27T08:25:28Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSourceBase.java", "diffHunk": "@@ -93,16 +101,21 @@\n \t *                               mode is {@link StartupMode#TIMESTAMP}.\n \t */\n \tprotected KafkaDynamicSourceBase(\n-\t\t\tDataType outputDataType,\n-\t\t\tString topic,\n-\t\t\tProperties properties,\n-\t\t\tDecodingFormat<DeserializationSchema<RowData>> decodingFormat,\n-\t\t\tStartupMode startupMode,\n-\t\t\tMap<KafkaTopicPartition, Long> specificStartupOffsets,\n-\t\t\tlong startupTimestampMillis) {\n+\t\tDataType outputDataType,\n+\t\t@Nullable List<String> topics,\n+\t\t@Nullable Pattern topicPattern,\n+\t\tProperties properties,\n+\t\tDecodingFormat<DeserializationSchema<RowData>> decodingFormat,\n+\t\tStartupMode startupMode,\n+\t\tMap<KafkaTopicPartition, Long> specificStartupOffsets,\n+\t\tlong startupTimestampMillis) {", "originalCommit": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyNjA2Nw==", "url": "https://github.com/apache/flink/pull/12908#discussion_r460726067", "bodyText": "Indent.", "author": "wuchong", "createdAt": "2020-07-27T08:25:36Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSourceBase.java", "diffHunk": "@@ -164,34 +178,44 @@ public int hashCode() {\n \t/**\n \t * Creates a version-specific Kafka consumer.\n \t *\n-\t * @param topic                 Kafka topic to consume.\n+\t * @param topics                Kafka topic to consume.\n \t * @param properties            Properties for the Kafka consumer.\n \t * @param deserializationSchema Deserialization schema to use for Kafka records.\n \t * @return The version-specific Kafka consumer\n \t */\n \tprotected abstract FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n-\t\t\tString topic,\n+\t\t\tList<String> topics,\n \t\t\tProperties properties,\n \t\t\tDeserializationSchema<RowData> deserializationSchema);\n \n+\t/**\n+\t * Creates a version-specific Kafka consumer.\n+\t *\n+\t * @param topicPattern          afka topic to consume.\n+\t * @param properties            Properties for the Kafka consumer.\n+\t * @param deserializationSchema Deserialization schema to use for Kafka records.\n+\t * @return The version-specific Kafka consumer\n+\t */\n+\tprotected abstract FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n+\t\tPattern topicPattern,\n+\t\tProperties properties,\n+\t\tDeserializationSchema<RowData> deserializationSchema);", "originalCommit": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyNjE3Ng==", "url": "https://github.com/apache/flink/pull/12908#discussion_r460726176", "bodyText": "Indent.", "author": "wuchong", "createdAt": "2020-07-27T08:25:48Z", "path": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicSource.java", "diffHunk": "@@ -70,17 +77,26 @@ public KafkaDynamicSource(\n \n \t@Override\n \tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n-\t\t\tString topic,\n+\t\t\tList<String> topics,\n \t\t\tProperties properties,\n \t\t\tDeserializationSchema<RowData> deserializationSchema) {\n-\t\treturn new FlinkKafkaConsumer<>(topic, deserializationSchema, properties);\n+\t\treturn new FlinkKafkaConsumer<>(topics, deserializationSchema, properties);\n+\t}\n+\n+\t@Override\n+\tprotected FlinkKafkaConsumerBase<RowData> createKafkaConsumer(\n+\t\tPattern topicPattern,\n+\t\tProperties properties,\n+\t\tDeserializationSchema<RowData> deserializationSchema) {", "originalCommit": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcyNzkyMQ==", "url": "https://github.com/apache/flink/pull/12908#discussion_r460727921", "bodyText": "Split this into two exceptions.", "author": "wuchong", "createdAt": "2020-07-27T08:28:43Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaOptions.java", "diffHunk": "@@ -161,11 +176,22 @@ private KafkaOptions() {}\n \t// --------------------------------------------------------------------------------------------\n \n \tpublic static void validateTableOptions(ReadableConfig tableOptions) {\n+\t\tvalidateTopic(tableOptions);\n \t\tvalidateScanStartupMode(tableOptions);\n \t\tvalidateSinkPartitioner(tableOptions);\n \t\tvalidateSinkSemantic(tableOptions);\n \t}\n \n+\tpublic static void validateTopic(ReadableConfig tableOptions) {\n+\t\tOptional<String> topic = tableOptions.getOptional(TOPIC);\n+\t\tOptional<String> pattern = tableOptions.getOptional(TOPIC_PATTERN);\n+\n+\t\tif ((topic.isPresent() && pattern.isPresent()) || !(topic.isPresent() || pattern.isPresent())) {", "originalCommit": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczMDg2MA==", "url": "https://github.com/apache/flink/pull/12908#discussion_r460730860", "bodyText": "I would suggest to add static util methods List<String> getTopics(ReadableConfig) and Pattern getTopicPattern(ReadableConfig) in KafkaOptions.", "author": "wuchong", "createdAt": "2020-07-27T08:33:33Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryBase.java", "diffHunk": "@@ -72,26 +82,41 @@ public DynamicTableSource createDynamicTableSource(Context context) {\n \t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n \n \t\tReadableConfig tableOptions = helper.getOptions();\n-\n-\t\tString topic = tableOptions.get(TOPIC);\n \t\tDecodingFormat<DeserializationSchema<RowData>> decodingFormat = helper.discoverDecodingFormat(\n \t\t\t\tDeserializationFormatFactory.class,\n \t\t\t\tFactoryUtil.FORMAT);\n+\t\tOptional<String> topic = tableOptions.getOptional(TOPIC);\n+\t\tOptional<String> pattern = tableOptions.getOptional(TOPIC_PATTERN);\n \t\t// Validate the option data type.\n \t\thelper.validateExcept(PROPERTIES_PREFIX);\n \t\t// Validate the option values.\n \t\tvalidateTableOptions(tableOptions);\n \n \t\tDataType producedDataType = context.getCatalogTable().getSchema().toPhysicalRowDataType();\n-\t\tfinal StartupOptions startupOptions = getStartupOptions(tableOptions, topic);\n+\n+\t\tfinal StartupOptions startupOptions = getStartupOptions(tableOptions);\n+\t\tfinal Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n+\t\t// add topic-partition discovery\n+\t\tproperties.setProperty(FlinkKafkaConsumerBase.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS,\n+\t\t\tString.valueOf(tableOptions\n+\t\t\t\t.getOptional(SCAN_TOPIC_PARTITION_DISCOVERY)\n+\t\t\t\t.map(val -> val.toMillis())\n+\t\t\t\t.orElse(FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED)));\n+\n \t\treturn createKafkaTableSource(\n-\t\t\t\tproducedDataType,\n-\t\t\t\ttopic,\n-\t\t\t\tgetKafkaProperties(context.getCatalogTable().getOptions()),\n-\t\t\t\tdecodingFormat,\n-\t\t\t\tstartupOptions.startupMode,\n-\t\t\t\tstartupOptions.specificOffsets,\n-\t\t\t\tstartupOptions.startupTimestampMillis);\n+\t\t\tproducedDataType,\n+\t\t\ttopic.map(value ->\n+\t\t\t\tArrays\n+\t\t\t\t\t.stream(value.split(\",\"))\n+\t\t\t\t\t.map(String::trim)\n+\t\t\t\t\t.collect(Collectors.toList()))\n+\t\t\t\t.orElse(null),\n+\t\t\tpattern.map(value -> Pattern.compile(value)).orElse(null),", "originalCommit": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczMTYzNQ==", "url": "https://github.com/apache/flink/pull/12908#discussion_r460731635", "bodyText": "What about to have a validateTableSinkOptions and validateTableSourceOptions ? We can then move this validation to validateSinkTopic().", "author": "wuchong", "createdAt": "2020-07-27T08:34:55Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryBase.java", "diffHunk": "@@ -109,10 +133,18 @@ public DynamicTableSink createDynamicTableSink(Context context) {\n \t\t// Validate the option values.\n \t\tvalidateTableOptions(tableOptions);\n \n+\t\tif (tableOptions.getOptional(TOPIC_PATTERN).isPresent()){\n+\t\t\tthrow new ValidationException(\"Flink Kafka sink currently doesn't support 'topic-pattern'.\");\n+\t\t}\n+\t\tString[] topics = tableOptions.get(TOPIC).split(\",\");\n+\t\tif (topics.length > 1) {\n+\t\t\tthrow new ValidationException(\"Flink Kafka sink currently doesn't support topic list.\");\n+\t\t}", "originalCommit": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczMzA1Mg==", "url": "https://github.com/apache/flink/pull/12908#discussion_r460733052", "bodyText": "Add a util method boolean isSingleTopic(ReadableConfig). It can also be used in sink side.", "author": "wuchong", "createdAt": "2020-07-27T08:37:23Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaOptions.java", "diffHunk": "@@ -193,6 +219,9 @@ private static void validateScanStartupMode(ReadableConfig tableOptions) {\n \t\t\t\t\t\t\t\t\tSCAN_STARTUP_SPECIFIC_OFFSETS.key(),\n \t\t\t\t\t\t\t\t\tSCAN_STARTUP_MODE_VALUE_SPECIFIC_OFFSETS));\n \t\t\t\t\t\t}\n+\t\t\t\t\t\tif (!tableOptions.getOptional(TOPIC).isPresent() || tableOptions.get(TOPIC).split(\",\").length > 1){", "originalCommit": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczNDY4MQ==", "url": "https://github.com/apache/flink/pull/12908#discussion_r460734681", "bodyText": "Better to use new HashMap<>().", "author": "wuchong", "createdAt": "2020-07-27T08:40:08Z", "path": "flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTestBase.java", "diffHunk": "@@ -187,6 +197,62 @@ public void testTableSourceCommitOnCheckpointsDisabled() {\n \t\tassertFalse(((FlinkKafkaConsumerBase) function).getEnableCommitOnCheckpoints());\n \t}\n \n+\t@Test\n+\tpublic void testTableSourceWithPattern() {\n+\t\t// prepare parameters for Kafka table source\n+\t\tfinal DataType producedDataType = SOURCE_SCHEMA.toPhysicalRowDataType();\n+\n+\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n+\n+\t\tDecodingFormat<DeserializationSchema<RowData>> decodingFormat =\n+\t\t\tnew TestFormatFactory.DecodingFormatMock(\",\", true);\n+\n+\t\t// Construct table source using options and table source factory\n+\t\tObjectIdentifier objectIdentifier = ObjectIdentifier.of(\n+\t\t\t\"default\",\n+\t\t\t\"default\",\n+\t\t\t\"scanTable\");\n+\n+\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n+\t\t\tgetFullSourceOptions(),\n+\t\t\toptions -> {\n+\t\t\t\toptions.remove(\"topic\");\n+\t\t\t\toptions.put(\"topic-pattern\", TOPIC_REGEX);\n+\t\t\t\toptions.put(\"scan.startup.mode\", KafkaOptions.SCAN_STARTUP_MODE_VALUE_EARLIEST);\n+\t\t\t\toptions.remove(\"scan.startup.specific-offsets\");\n+\t\t\t});\n+\t\tCatalogTable catalogTable = createKafkaSourceCatalogTable(modifiedOptions);\n+\n+\t\tfinal DynamicTableSource actualSource = FactoryUtil.createTableSource(null,\n+\t\t\tobjectIdentifier,\n+\t\t\tcatalogTable,\n+\t\t\tnew Configuration(),\n+\t\t\tThread.currentThread().getContextClassLoader());\n+\n+\t\t// Test scan source equals\n+\t\tfinal KafkaDynamicSourceBase expectedKafkaSource = getExpectedScanSource(\n+\t\t\tproducedDataType,\n+\t\t\tnull,\n+\t\t\tPattern.compile(TOPIC_REGEX),\n+\t\t\tKAFKA_PROPERTIES,\n+\t\t\tdecodingFormat,\n+\t\t\tStartupMode.EARLIEST,\n+\t\t\tspecificOffsets,", "originalCommit": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTAyMQ==", "url": "https://github.com/apache/flink/pull/12908#discussion_r462161021", "bodyText": "emm. I think it's more trivial to use a new hashmap to store the option value. Because we need to add more options such as format.", "author": "fsk119", "createdAt": "2020-07-29T09:21:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczNDY4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczNTUwMg==", "url": "https://github.com/apache/flink/pull/12908#discussion_r460735502", "bodyText": "This has been verified in the other test. We don't need to test it again here.", "author": "wuchong", "createdAt": "2020-07-27T08:41:28Z", "path": "flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTestBase.java", "diffHunk": "@@ -187,6 +197,62 @@ public void testTableSourceCommitOnCheckpointsDisabled() {\n \t\tassertFalse(((FlinkKafkaConsumerBase) function).getEnableCommitOnCheckpoints());\n \t}\n \n+\t@Test\n+\tpublic void testTableSourceWithPattern() {\n+\t\t// prepare parameters for Kafka table source\n+\t\tfinal DataType producedDataType = SOURCE_SCHEMA.toPhysicalRowDataType();\n+\n+\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n+\n+\t\tDecodingFormat<DeserializationSchema<RowData>> decodingFormat =\n+\t\t\tnew TestFormatFactory.DecodingFormatMock(\",\", true);\n+\n+\t\t// Construct table source using options and table source factory\n+\t\tObjectIdentifier objectIdentifier = ObjectIdentifier.of(\n+\t\t\t\"default\",\n+\t\t\t\"default\",\n+\t\t\t\"scanTable\");\n+\n+\t\tfinal Map<String, String> modifiedOptions = getModifiedOptions(\n+\t\t\tgetFullSourceOptions(),\n+\t\t\toptions -> {\n+\t\t\t\toptions.remove(\"topic\");\n+\t\t\t\toptions.put(\"topic-pattern\", TOPIC_REGEX);\n+\t\t\t\toptions.put(\"scan.startup.mode\", KafkaOptions.SCAN_STARTUP_MODE_VALUE_EARLIEST);\n+\t\t\t\toptions.remove(\"scan.startup.specific-offsets\");\n+\t\t\t});\n+\t\tCatalogTable catalogTable = createKafkaSourceCatalogTable(modifiedOptions);\n+\n+\t\tfinal DynamicTableSource actualSource = FactoryUtil.createTableSource(null,\n+\t\t\tobjectIdentifier,\n+\t\t\tcatalogTable,\n+\t\t\tnew Configuration(),\n+\t\t\tThread.currentThread().getContextClassLoader());\n+\n+\t\t// Test scan source equals\n+\t\tfinal KafkaDynamicSourceBase expectedKafkaSource = getExpectedScanSource(\n+\t\t\tproducedDataType,\n+\t\t\tnull,\n+\t\t\tPattern.compile(TOPIC_REGEX),\n+\t\t\tKAFKA_PROPERTIES,\n+\t\t\tdecodingFormat,\n+\t\t\tStartupMode.EARLIEST,\n+\t\t\tspecificOffsets,\n+\t\t\t0);\n+\t\tfinal KafkaDynamicSourceBase actualKafkaSource = (KafkaDynamicSourceBase) actualSource;\n+\t\tassertEquals(actualKafkaSource, expectedKafkaSource);\n+\n+\t\t// Test Kafka consumer\n+\t\tScanTableSource.ScanRuntimeProvider provider =\n+\t\t\tactualKafkaSource.getScanRuntimeProvider(ScanRuntimeProviderContext.INSTANCE);\n+\t\tassertThat(provider, instanceOf(SourceFunctionProvider.class));\n+\t\tfinal SourceFunctionProvider sourceFunctionProvider = (SourceFunctionProvider) provider;\n+\t\tfinal SourceFunction<RowData> sourceFunction = sourceFunctionProvider.createSourceFunction();\n+\t\tassertThat(sourceFunction, instanceOf(getExpectedConsumerClass()));\n+\t\t//  Test commitOnCheckpoints flag should be true when set consumer group\n+\t\tassertTrue(((FlinkKafkaConsumerBase) sourceFunction).getEnableCommitOnCheckpoints());", "originalCommit": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDczNjQxNQ==", "url": "https://github.com/apache/flink/pull/12908#discussion_r460736415", "bodyText": "We shouldn't use thrown in one test multiple times, because only the first one will be triggered.", "author": "wuchong", "createdAt": "2020-07-27T08:43:01Z", "path": "flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTestBase.java", "diffHunk": "@@ -352,6 +442,49 @@ public void testInvalidSinkSemantic(){\n \t\t\tnew Configuration(),\n \t\t\tThread.currentThread().getContextClassLoader());\n \t}\n+\n+\t@Test\n+\tpublic void testSinkWithTopicListOrTopicPattern(){\n+\t\tObjectIdentifier objectIdentifier = ObjectIdentifier.of(\n+\t\t\t\"default\",\n+\t\t\t\"default\",\n+\t\t\t\"sinkTable\");\n+\n+\t\tMap<String, String> modifiedOptions = getModifiedOptions(\n+\t\t\tgetFullSourceOptions(),\n+\t\t\toptions -> {\n+\t\t\t\toptions.put(\"topic\", TOPICS);\n+\t\t\t\toptions.put(\"scan.startup.mode\", \"earliest-offset\");\n+\t\t\t\toptions.remove(\"specific-offsets\");\n+\t\t\t});\n+\t\tCatalogTable sinkTable = createKafkaSinkCatalogTable(modifiedOptions);\n+\n+\t\tthrown.expect(ValidationException.class);\n+\t\tthrown.expect(containsCause(new ValidationException(\"Flink Kafka sink currently doesn't support topic list.\")));\n+\t\tFactoryUtil.createTableSink(\n+\t\t\tnull,\n+\t\t\tobjectIdentifier,\n+\t\t\tsinkTable,\n+\t\t\tnew Configuration(),\n+\t\t\tThread.currentThread().getContextClassLoader());\n+\n+\t\tmodifiedOptions = getModifiedOptions(\n+\t\t\tgetFullSourceOptions(),\n+\t\t\toptions -> {\n+\t\t\t\toptions.put(\"topic-pattern\", TOPIC_REGEX);\n+\t\t\t});\n+\t\tsinkTable = createKafkaSinkCatalogTable(modifiedOptions);\n+\n+\t\tthrown.expect(ValidationException.class);\n+\t\tthrown.expect(containsCause(new ValidationException(\"Flink Kafka sink currently doesn't support 'topic-pattern'.\")));", "originalCommit": "1a4e5ee2a2f11ba650a98c1a211cea75736fe79d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwOTQzNA==", "url": "https://github.com/apache/flink/pull/12908#discussion_r464809434", "bodyText": "duplicate", "author": "wuchong", "createdAt": "2020-08-04T05:34:00Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryBase.java", "diffHunk": "@@ -168,10 +185,14 @@ protected abstract KafkaDynamicSinkBase createKafkaTableSink(\n \t@Override\n \tpublic Set<ConfigOption<?>> optionalOptions() {\n \t\tfinal Set<ConfigOption<?>> options = new HashSet<>();\n+\t\toptions.add(TOPIC);\n+\t\toptions.add(TOPIC_PATTERN);\n \t\toptions.add(PROPS_GROUP_ID);\n \t\toptions.add(SCAN_STARTUP_MODE);\n \t\toptions.add(SCAN_STARTUP_SPECIFIC_OFFSETS);\n+\t\toptions.add(SCAN_TOPIC_PARTITION_DISCOVERY);\n \t\toptions.add(SCAN_STARTUP_TIMESTAMP_MILLIS);\n+\t\toptions.add(SCAN_TOPIC_PARTITION_DISCOVERY);", "originalCommit": "8d2e2d4bdf6cc90a3fa7b6be4d5ad4bc469feec6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwOTk4MA==", "url": "https://github.com/apache/flink/pull/12908#discussion_r464809980", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\t.map(val -> val.toMillis())\n          \n          \n            \n            \t\t\t\t.map(Duration::toMillis)", "author": "wuchong", "createdAt": "2020-08-04T05:35:53Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryBase.java", "diffHunk": "@@ -72,26 +80,34 @@ public DynamicTableSource createDynamicTableSource(Context context) {\n \t\tFactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);\n \n \t\tReadableConfig tableOptions = helper.getOptions();\n-\n-\t\tString topic = tableOptions.get(TOPIC);\n \t\tDecodingFormat<DeserializationSchema<RowData>> decodingFormat = helper.discoverDecodingFormat(\n \t\t\t\tDeserializationFormatFactory.class,\n \t\t\t\tFactoryUtil.FORMAT);\n \t\t// Validate the option data type.\n \t\thelper.validateExcept(PROPERTIES_PREFIX);\n \t\t// Validate the option values.\n-\t\tvalidateTableOptions(tableOptions);\n+\t\tvalidateTableSourceOptions(tableOptions);\n \n \t\tDataType producedDataType = context.getCatalogTable().getSchema().toPhysicalRowDataType();\n-\t\tfinal StartupOptions startupOptions = getStartupOptions(tableOptions, topic);\n+\n+\t\tfinal StartupOptions startupOptions = getStartupOptions(tableOptions);\n+\t\tfinal Properties properties = getKafkaProperties(context.getCatalogTable().getOptions());\n+\t\t// add topic-partition discovery\n+\t\tproperties.setProperty(FlinkKafkaConsumerBase.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS,\n+\t\t\tString.valueOf(tableOptions\n+\t\t\t\t.getOptional(SCAN_TOPIC_PARTITION_DISCOVERY)\n+\t\t\t\t.map(val -> val.toMillis())", "originalCommit": "8d2e2d4bdf6cc90a3fa7b6be4d5ad4bc469feec6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgxMDQ0Ng==", "url": "https://github.com/apache/flink/pull/12908#discussion_r464810446", "bodyText": "\"topic-list\" -> \"topic\"? We don't have \"topic-list\" option.", "author": "wuchong", "createdAt": "2020-08-04T05:37:27Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaOptions.java", "diffHunk": "@@ -160,12 +178,45 @@ private KafkaOptions() {}\n \t// Validation\n \t// --------------------------------------------------------------------------------------------\n \n-\tpublic static void validateTableOptions(ReadableConfig tableOptions) {\n+\tpublic static void validateTableSourceOptions(ReadableConfig tableOptions) {\n+\t\tvalidateSourceTopic(tableOptions);\n \t\tvalidateScanStartupMode(tableOptions);\n+\t}\n+\n+\tpublic static void validateTableSinkOptions(ReadableConfig tableOptions) {\n+\t\tvalidateSinkTopic(tableOptions);\n \t\tvalidateSinkPartitioner(tableOptions);\n \t\tvalidateSinkSemantic(tableOptions);\n \t}\n \n+\tpublic static void validateSourceTopic(ReadableConfig tableOptions) {\n+\t\tOptional<String> topic = tableOptions.getOptional(TOPIC);\n+\t\tOptional<String> pattern = tableOptions.getOptional(TOPIC_PATTERN);\n+\n+\t\tif (topic.isPresent() && pattern.isPresent()) {\n+\t\t\tthrow new ValidationException(\"Option 'topic' and 'topic-pattern' shouldn't be set together.\");\n+\t\t}\n+\n+\t\tif (!topic.isPresent() && !pattern.isPresent()) {\n+\t\t\tthrow new ValidationException(\"Either 'topic' or 'topic-pattern' must be set.\");\n+\t\t}\n+\t}\n+\n+\tpublic static void validateSinkTopic(ReadableConfig tableOptions) {\n+\t\tString errorMessageTemp = \"Flink Kafka sink currently only supports single topic, but got %s: %s.\";\n+\t\tif (!isSingleTopic(tableOptions)) {\n+\t\t\tif (tableOptions.getOptional(TOPIC_PATTERN).isPresent()) {\n+\t\t\t\tthrow new ValidationException(String.format(\n+\t\t\t\t\terrorMessageTemp, \"'topic-pattern'\", tableOptions.get(TOPIC_PATTERN)\n+\t\t\t\t));\n+\t\t\t} else {\n+\t\t\t\tthrow new ValidationException(String.format(\n+\t\t\t\t\terrorMessageTemp, \"topic-list\", tableOptions.get(TOPIC)", "originalCommit": "8d2e2d4bdf6cc90a3fa7b6be4d5ad4bc469feec6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgyODI4NQ==", "url": "https://github.com/apache/flink/pull/12908#discussion_r464828285", "bodyText": "Is it still needed? Because we have set it in static block.", "author": "wuchong", "createdAt": "2020-08-04T06:31:07Z", "path": "flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/table/KafkaDynamicTableFactoryTestBase.java", "diffHunk": "@@ -139,10 +152,12 @@ public void testTableSource() {\n \t\t\t\tThread.currentThread().getContextClassLoader());\n \n \t\t// Test scan source equals\n+\t\tKAFKA_SOURCE_PROPERTIES.setProperty(\"flink.partition-discovery.interval-millis\", \"1000\");", "originalCommit": "8d2e2d4bdf6cc90a3fa7b6be4d5ad4bc469feec6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMjI1Mg==", "url": "https://github.com/apache/flink/pull/12908#discussion_r464832252", "bodyText": "The community recommend to use List ConfigOption for list values, framework will handle the parsing. This will also change to use ; as the separator, but this is more align with other list options. You can declare a List ConfigOption by :\n\tpublic static final ConfigOption<List<String>> TOPIC = ConfigOptions\n\t\t\t.key(\"topic\")\n\t\t\t.stringType()\n\t\t\t.asList()\n\t\t\t.noDefaultValue()\n\t\t\t.withDescription(\"...\");\nThen you can call return tableOptions.getOptional(TOPIC).map(t -> t.size() == 1).orElse(false); here.\nSorry for the late reminder.", "author": "wuchong", "createdAt": "2020-08-04T06:41:19Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/table/KafkaOptions.java", "diffHunk": "@@ -239,9 +293,25 @@ public static KafkaSinkSemantic getSinkSemantic(ReadableConfig tableOptions){\n \t\t}\n \t}\n \n-\tpublic static StartupOptions getStartupOptions(\n-\t\t\tReadableConfig tableOptions,\n-\t\t\tString topic) {\n+\tpublic static List<String> getSourceTopics(ReadableConfig tableOptions) {\n+\t\treturn tableOptions.getOptional(TOPIC).map(value ->\n+\t\t\tArrays\n+\t\t\t\t.stream(value.split(\",\"))\n+\t\t\t\t.map(String::trim)\n+\t\t\t\t.collect(Collectors.toList()))\n+\t\t\t.orElse(null);\n+\t}\n+\n+\tpublic static Pattern getSourceTopicPattern(ReadableConfig tableOptions) {\n+\t\treturn tableOptions.getOptional(TOPIC_PATTERN).map(value -> Pattern.compile(value)).orElse(null);\n+\t}\n+\n+\tprivate static boolean isSingleTopic(ReadableConfig tableOptions) {\n+\t\t// Option 'topic-pattern' is regarded as multi-topics.\n+\t\treturn tableOptions.getOptional(TOPIC).isPresent() && tableOptions.get(TOPIC).split(\",\").length == 1;", "originalCommit": "8d2e2d4bdf6cc90a3fa7b6be4d5ad4bc469feec6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2769a8edbd4a4e28ca6b75f39e69d95007b5c4b2", "url": "https://github.com/apache/flink/commit/2769a8edbd4a4e28ca6b75f39e69d95007b5c4b2", "message": "[FLINK-18449][kafka][table] Support topic list and topic pattern and partition discovery for Kafka source in Table API\n\nThis closes #12908", "committedDate": "2020-08-19T04:36:39Z", "type": "commit"}, {"oid": "2769a8edbd4a4e28ca6b75f39e69d95007b5c4b2", "url": "https://github.com/apache/flink/commit/2769a8edbd4a4e28ca6b75f39e69d95007b5c4b2", "message": "[FLINK-18449][kafka][table] Support topic list and topic pattern and partition discovery for Kafka source in Table API\n\nThis closes #12908", "committedDate": "2020-08-19T04:36:39Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjY4MDQxMA==", "url": "https://github.com/apache/flink/pull/12908#discussion_r472680410", "bodyText": "Currently, it is very verbose to pass through these to parameters together here and there. An improvement is that we can use KafkaTopicsDescriptor, but this can be another issue in the future.", "author": "wuchong", "createdAt": "2020-08-19T04:33:43Z", "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/table/Kafka010DynamicSource.java", "diffHunk": "@@ -54,7 +59,8 @@\n \t */\n \tpublic Kafka010DynamicSource(\n \t\t\tDataType outputDataType,\n-\t\t\tString topic,\n+\t\t\t@Nullable List<String> topics,\n+\t\t\t@Nullable Pattern topicPattern,", "originalCommit": "4b06551a39ce8d9d6c5f19948e36d493e3d592e3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}