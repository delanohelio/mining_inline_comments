{"pr_number": 9096, "pr_title": "PIP 76: Streaming Offload(Part I)", "pr_createdAt": "2020-12-30T14:24:56Z", "pr_url": "https://github.com/apache/pulsar/pull/9096", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyODE3NA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554528174", "bodyText": "Do we need to expose the SegmentInfo here?", "author": "codelipenghui", "createdAt": "2021-01-10T07:58:31Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -35,6 +39,78 @@\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    @ToString\n+    class SegmentInfo {", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDY5NDgxOQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554694819", "bodyText": "This should be an interface not a class.", "author": "sijie", "createdAt": "2021-01-11T04:16:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyODE3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyODUyMg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554528522", "bodyText": "Is it possible to check the consecutive in the offloader? I think it's more easy to handle in the managedledger.", "author": "codelipenghui", "createdAt": "2021-01-10T08:02:10Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -35,6 +39,78 @@\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    @ToString\n+    class SegmentInfo {\n+        public SegmentInfo(UUID uuid, long beginLedger, long beginEntry, String driverName,\n+                           Map<String, String> driverMetadata) {\n+            this.uuid = uuid;\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.driverName = driverName;\n+            this.driverMetadata = driverMetadata;\n+        }\n+\n+\n+        public final UUID uuid;\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final String driverName;\n+        volatile private long endLedger;\n+        volatile private long endEntry;\n+        volatile boolean closed = false;\n+        public final Map<String, String> driverMetadata;\n+\n+        public boolean isClosed() {\n+            return closed;\n+        }\n+\n+        public void closeSegment(long endLedger, long endEntry) {\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+            this.closed = true;\n+        }\n+\n+        public OffloadResult result() {\n+            return new OffloadResult(beginLedger, beginEntry, endLedger, endEntry);\n+        }\n+    }\n+\n+\n+    class OffloadResult {\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final long endLedger;\n+        public final long endEntry;\n+\n+        public OffloadResult(long beginLedger, long beginEntry, long endLedger, long endEntry) {\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+        }\n+    }\n+\n+    /**\n+     * Used to store driver info, buffer entries, mark progress, etc.\n+     * Create one per second.\n+     */\n+    interface OffloaderHandle {\n+\n+        /**\n+         * return true when both buffer have enough size and ledger/entry id is next to the current one.\n+         * @param size\n+         * @return\n+         */\n+        boolean canOffer(long size);\n+\n+        PositionImpl lastOffered();\n+\n+        boolean offerEntry(EntryImpl entry) throws OffloadSegmentClosedException,\n+                ManagedLedgerException.OffloadNotConsecutiveException;", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDkwMTU0Mg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554901542", "bodyText": "@codelipenghui managedledger can do a full in-depth check use for ledger information, but managed ledger will also use a naive check(if ledger id equals and entry id is consecutive, then it is consecutive, that's most common case), that can be used to spot bugs when managedledger not work properly.", "author": "Renkai", "createdAt": "2021-01-11T09:03:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyODUyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyODk3OQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554528979", "bodyText": "It's better to also add async method for these 3 methods. Usually, we should implement the sync method based on the async method.", "author": "codelipenghui", "createdAt": "2021-01-10T08:06:35Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -35,6 +39,78 @@\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    @ToString\n+    class SegmentInfo {\n+        public SegmentInfo(UUID uuid, long beginLedger, long beginEntry, String driverName,\n+                           Map<String, String> driverMetadata) {\n+            this.uuid = uuid;\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.driverName = driverName;\n+            this.driverMetadata = driverMetadata;\n+        }\n+\n+\n+        public final UUID uuid;\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final String driverName;\n+        volatile private long endLedger;\n+        volatile private long endEntry;\n+        volatile boolean closed = false;\n+        public final Map<String, String> driverMetadata;\n+\n+        public boolean isClosed() {\n+            return closed;\n+        }\n+\n+        public void closeSegment(long endLedger, long endEntry) {\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+            this.closed = true;\n+        }\n+\n+        public OffloadResult result() {\n+            return new OffloadResult(beginLedger, beginEntry, endLedger, endEntry);\n+        }\n+    }\n+\n+\n+    class OffloadResult {\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final long endLedger;\n+        public final long endEntry;\n+\n+        public OffloadResult(long beginLedger, long beginEntry, long endLedger, long endEntry) {\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+        }\n+    }\n+\n+    /**\n+     * Used to store driver info, buffer entries, mark progress, etc.\n+     * Create one per second.\n+     */\n+    interface OffloaderHandle {\n+\n+        /**\n+         * return true when both buffer have enough size and ledger/entry id is next to the current one.\n+         * @param size\n+         * @return\n+         */\n+        boolean canOffer(long size);\n+\n+        PositionImpl lastOffered();\n+\n+        boolean offerEntry(EntryImpl entry) throws OffloadSegmentClosedException,", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDY5NTY2NQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554695665", "bodyText": "+1", "author": "sijie", "createdAt": "2021-01-11T04:17:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyODk3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyOTIzMw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554529233", "bodyText": "Please check the comment of this method, I think here is not correct, The result just return a future for the OffloadHandle, this does not mean that the data has been persisted", "author": "codelipenghui", "createdAt": "2021-01-10T08:09:02Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -85,6 +161,32 @@\n                                     UUID uid,\n                                     Map<String, String> extraMetadata);\n \n+    /**\n+     * Offload the passed in ledger to longterm storage.\n+     * Metadata passed in is for inspection purposes only and should be stored\n+     * alongside the segment data.\n+     *\n+     * When the returned future completes, the ledger has been persisted to the\n+     * loadterm storage, so it is safe to delete the original copy in bookkeeper.", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTAzMjExNA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555032114", "bodyText": "Comment updated", "author": "Renkai", "createdAt": "2021-01-11T13:04:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyOTIzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyOTI4NA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554529284", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                interface OffloaderHandle {\n          \n          \n            \n                interface OffloadHandle {", "author": "codelipenghui", "createdAt": "2021-01-10T08:09:40Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -35,6 +39,78 @@\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    @ToString\n+    class SegmentInfo {\n+        public SegmentInfo(UUID uuid, long beginLedger, long beginEntry, String driverName,\n+                           Map<String, String> driverMetadata) {\n+            this.uuid = uuid;\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.driverName = driverName;\n+            this.driverMetadata = driverMetadata;\n+        }\n+\n+\n+        public final UUID uuid;\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final String driverName;\n+        volatile private long endLedger;\n+        volatile private long endEntry;\n+        volatile boolean closed = false;\n+        public final Map<String, String> driverMetadata;\n+\n+        public boolean isClosed() {\n+            return closed;\n+        }\n+\n+        public void closeSegment(long endLedger, long endEntry) {\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+            this.closed = true;\n+        }\n+\n+        public OffloadResult result() {\n+            return new OffloadResult(beginLedger, beginEntry, endLedger, endEntry);\n+        }\n+    }\n+\n+\n+    class OffloadResult {\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final long endLedger;\n+        public final long endEntry;\n+\n+        public OffloadResult(long beginLedger, long beginEntry, long endLedger, long endEntry) {\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+        }\n+    }\n+\n+    /**\n+     * Used to store driver info, buffer entries, mark progress, etc.\n+     * Create one per second.\n+     */\n+    interface OffloaderHandle {", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyOTM1Nw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554529357", "bodyText": "UnsupportedOperationException?", "author": "codelipenghui", "createdAt": "2021-01-10T08:10:19Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -115,6 +217,16 @@\n     CompletableFuture<Void> deleteOffloaded(long ledgerId, UUID uid,\n                                             Map<String, String> offloadDriverMetadata);\n \n+    default CompletableFuture<ReadHandle> readOffloaded(long ledgerId, MLDataFormats.OffloadContext ledgerContext,\n+                                                        Map<String, String> offloadDriverMetadata) {\n+        throw new UnsupportedClassVersionError();", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTAzNDkyOQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555034929", "bodyText": "It's the default behavior of the interface, useful when a new implementation only implements a part of the interface but still useful.", "author": "Renkai", "createdAt": "2021-01-11T13:10:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyOTM1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwMjE2MA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555102160", "bodyText": "you should return a CompletableFuture that reports the UnsupportedOperationException\nthe caller of a method that returns a Future does not expect the method to throw exceptions.", "author": "eolivelli", "createdAt": "2021-01-11T14:54:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyOTM1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTE5NzI3Nw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555197277", "bodyText": "Maybe the exception should be UnsupportedOperationException?", "author": "gaoran10", "createdAt": "2021-01-11T16:58:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyOTM1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyOTU5Nw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554529597", "bodyText": "maxOffloadSegmentRolloverTimeInSeconds?", "author": "codelipenghui", "createdAt": "2021-01-10T08:13:22Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TieredStorageConfiguration.java", "diffHunk": "@@ -63,6 +63,10 @@\n     public static final String METADATA_FIELD_MAX_BLOCK_SIZE = \"maxBlockSizeInBytes\";\n     public static final String METADATA_FIELD_READ_BUFFER_SIZE = \"readBufferSizeInBytes\";\n     public static final String OFFLOADER_PROPERTY_PREFIX = \"managedLedgerOffload\";\n+    public static final String MAX_SEGMENT_TIME_IN_SECOND = \"maxSegmentTimeInSecond\";", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyOTkyNQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554529925", "bodyText": "We also need to add the minOffloadSegmentRolloverTimeInSeconds to avoid the segments rollover too often.", "author": "codelipenghui", "createdAt": "2021-01-10T08:16:28Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TieredStorageConfiguration.java", "diffHunk": "@@ -63,6 +63,10 @@\n     public static final String METADATA_FIELD_MAX_BLOCK_SIZE = \"maxBlockSizeInBytes\";\n     public static final String METADATA_FIELD_READ_BUFFER_SIZE = \"readBufferSizeInBytes\";\n     public static final String OFFLOADER_PROPERTY_PREFIX = \"managedLedgerOffload\";\n+    public static final String MAX_SEGMENT_TIME_IN_SECOND = \"maxSegmentTimeInSecond\";\n+    public static final long DEFAULT_MAX_SEGMENT_TIME_IN_SECOND = 600;\n+    public static final String MAX_SEGMENT_SIZE_IN_BYTES = \"maxSegmentSizeInBytes\";\n+    public static final long DEFAULT_MAX_SEGMENT_SIZE_IN_BYTES = 1024 * 1024 * 1024;", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTA1MzA4NQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555053085", "bodyText": "Do you mean segment may become larger than MAX_SEGMENT_SIZE_IN_BYTES if the time it reaches MAX_SEGMENT_SIZE_IN_BYTES is shorter than minOffloadSegmentRolloverTimeInSeconds? If so, maybe we need a REAL_MAX_SEGMENT_SIZE_IN_BYTES to avoid the segment become too big.", "author": "Renkai", "createdAt": "2021-01-11T13:40:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyOTkyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTcwNjQzNg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555706436", "bodyText": "minOffloadSegmentRolloverTimeInSeconds related implementations added", "author": "Renkai", "createdAt": "2021-01-12T11:36:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUyOTkyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUzMDY1OA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554530658", "bodyText": "Can we reuse the OffloadIndexBlock? In my opinion, we just add some fields at the index header. Shall we need to introduce a new interface here? I think this will introduce more duplicate code", "author": "codelipenghui", "createdAt": "2021-01-10T08:23:43Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/StreamingOffloadIndexBlock.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud;\n+\n+import java.io.Closeable;\n+import java.io.FilterInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Map;\n+import org.apache.bookkeeper.client.api.LedgerMetadata;\n+import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;\n+\n+/**\n+ * The Index block abstraction used for offload a ledger to long term storage.\n+ */\n+@Unstable\n+public interface StreamingOffloadIndexBlock extends Closeable {", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDcwMDI5OA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554700298", "bodyText": "+1", "author": "sijie", "createdAt": "2021-01-11T04:24:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUzMDY1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTA2MDExNQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555060115", "bodyText": "We not only add some fields at the index header but also make the content repeatable, it actually makes every method in the OffloadIndexBlock a bit different from before. If we reuse OffloadIndexBlock, I think the methods of OffloadIndexBlock will be doubled, and we need to use extra checks to make sure users use the right version of a method, I don't think it will make less duplicate code.", "author": "Renkai", "createdAt": "2021-01-11T13:52:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUzMDY1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU4MzI5NQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557583295", "bodyText": "okay works for me.", "author": "sijie", "createdAt": "2021-01-14T17:54:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUzMDY1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAzMzQxMQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560033411", "bodyText": "@Renkai, we are talking about the interface, not the implementation. From the OffloadIndexBlock interface, most of the methods are the same as the StreamingOffloadIndexBlock  right?\nAnother unreasonable place is the index block is format concept, Not only streaming offload can use this format right? If we want a new offloaded that offload based on the time window or data size but not streaming offloaded, this new version IndexBlock also can be used?", "author": "codelipenghui", "createdAt": "2021-01-19T09:30:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUzMDY1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUzMDcxMw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554530713", "bodyText": "Please consider reuse the OffloadIndexBlockBuilder", "author": "codelipenghui", "createdAt": "2021-01-10T08:24:11Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/StreamingOffloadIndexBlockBuilder.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import org.apache.bookkeeper.client.api.LedgerMetadata;\n+import org.apache.bookkeeper.common.annotation.InterfaceAudience.LimitedPrivate;\n+import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;\n+import org.apache.bookkeeper.mledger.offload.jcloud.impl.StreamingOffloadIndexBlockBuilderImpl;\n+\n+/**\n+ * Interface for builder of index block used for offload a ledger to long term storage.\n+ */\n+@Unstable\n+@LimitedPrivate\n+public interface StreamingOffloadIndexBlockBuilder {", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTA2MTE4OQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555061189", "bodyText": "Reuse builder is OK to me, I will reuse the builder after we decide on the design of OffloadIndexBlock and StreamingOffloadIndexBlock", "author": "Renkai", "createdAt": "2021-01-11T13:53:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUzMDcxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTU2NDA4NQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555564085", "bodyText": "OffloadIndexBlockBuilder and StreamingOffloadIndexBlockBuilder now are using the same implementation\npublic class OffloadIndexBlockBuilderImpl implements OffloadIndexBlockBuilder, StreamingOffloadIndexBlockBuilder", "author": "Renkai", "createdAt": "2021-01-12T07:34:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUzMDcxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUzMDg1Nw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554530857", "bodyText": "Why change it to getFirstEntryId? The index always point to one entryId right?", "author": "codelipenghui", "createdAt": "2021-01-10T08:25:23Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexEntry.java", "diffHunk": "@@ -33,7 +33,7 @@\n     /**\n      * Get the entryId that this entry contains.\n      */\n-    long getEntryId();\n+    long getFirstEntryId();", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTA2MjgxMw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555062813", "bodyText": "No, the index point to a block, I think the former name has some misleading.", "author": "Renkai", "createdAt": "2021-01-11T13:56:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDUzMDg1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDY5NTIwNw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554695207", "bodyText": "Same question as above. Can we make it an interface?", "author": "sijie", "createdAt": "2021-01-11T04:17:02Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -35,6 +39,78 @@\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    @ToString\n+    class SegmentInfo {\n+        public SegmentInfo(UUID uuid, long beginLedger, long beginEntry, String driverName,\n+                           Map<String, String> driverMetadata) {\n+            this.uuid = uuid;\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.driverName = driverName;\n+            this.driverMetadata = driverMetadata;\n+        }\n+\n+\n+        public final UUID uuid;\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final String driverName;\n+        volatile private long endLedger;\n+        volatile private long endEntry;\n+        volatile boolean closed = false;\n+        public final Map<String, String> driverMetadata;\n+\n+        public boolean isClosed() {\n+            return closed;\n+        }\n+\n+        public void closeSegment(long endLedger, long endEntry) {\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+            this.closed = true;\n+        }\n+\n+        public OffloadResult result() {\n+            return new OffloadResult(beginLedger, beginEntry, endLedger, endEntry);\n+        }\n+    }\n+\n+\n+    class OffloadResult {", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTA2NDkwNA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555064904", "bodyText": "It's a data object like a POJO or Java Bean, I don't see the benefit to make it an interface.", "author": "Renkai", "createdAt": "2021-01-11T13:59:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDY5NTIwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDY5NzU3NQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554697575", "bodyText": "Can you explain why do we need this method? Also, add a comment to this method?", "author": "sijie", "createdAt": "2021-01-11T04:20:35Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/ManagedLedger.java", "diffHunk": "@@ -594,4 +593,6 @@ void asyncSetProperties(Map<String, String> properties, final AsyncCallbacks.Upd\n      * Get the ManagedLedgerInterceptor for ManagedLedger.\n      * */\n     ManagedLedgerInterceptor getManagedLedgerInterceptor();\n+\n+    CompletableFuture<LedgerMetadata> getRawLedgerMetadata(long ledgerId);", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDY5OTA5Mw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554699093", "bodyText": "I would like to see if we can avoid adding LedgerMetadata here. Because LedgerMetadata is a specific interface for bookkeeper. We should avoid binding the metadata format to bookkeeper's ledger metadata. This would simplify tiered storage integration.", "author": "sijie", "createdAt": "2021-01-11T04:22:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDY5NzU3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDgxMzE2MA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554813160", "bodyText": "We use LedgerMetadata because we need to use tiered storage to implement a org.apache.bookkeeper.client.api.ReadHandle, which is also a specific interface for bookkeeper, I think it's not avoidable.", "author": "Renkai", "createdAt": "2021-01-11T06:59:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDY5NzU3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTA3MzE4Mg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555073182", "bodyText": "Can you explain why do we need this method? Also, add a comment to this method?\n\nFormer offloader get a ReadHandle from ManagedLedger, and get LedgerMetadata from ReadHandle, our new offloader does not hold any ReadHandle but it still needs LedgerMetadata to make the block index", "author": "Renkai", "createdAt": "2021-01-11T14:12:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDY5NzU3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTA3NTk4Mw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555075983", "bodyText": "Actually, I want to name this method to getLedgerMetadata, but a method with this name already exists \n  \n    \n      pulsar/managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/ManagedLedgerImpl.java\n    \n    \n         Line 1685\n      in\n      138390c\n    \n    \n    \n    \n\n        \n          \n           public CompletableFuture<String> getLedgerMetadata(long ledgerId) { \n        \n    \n  \n\n\nMaybe it's better to name the existing method getLedgerMetadataStr", "author": "Renkai", "createdAt": "2021-01-11T14:16:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDY5NzU3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDcwMjAzOA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r554702038", "bodyText": "Can you explain why do you add a \"Mock\" method in the actual implementation?", "author": "sijie", "createdAt": "2021-01-11T04:26:56Z", "path": "tiered-storage/jcloud/src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderBase.java", "diffHunk": "@@ -82,7 +71,11 @@ protected static MockZooKeeper createMockZooKeeper() throws Exception {\n                 CreateMode.PERSISTENT);\n         return zk;\n     }\n-    \n+\n+    protected static MockManagedLedger createMockManagedLedger() {", "originalCommit": "527b319ebe57b41bd087e757637a2bf787d22584", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTA3OTM3Mg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555079372", "bodyText": "BlobStoreManagedLedgerOffloaderBase is a class for test, not implementation. Its name doesn't like a test class, but you can see its directory src/test/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloaderBase.java is a test directory.", "author": "Renkai", "createdAt": "2021-01-11T14:21:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDcwMjAzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU4ODU5Mg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557588592", "bodyText": "Make sense now.", "author": "sijie", "createdAt": "2021-01-14T18:02:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDcwMjAzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTIxOTAzNg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555219036", "bodyText": "Does one StreamingOffloadIndexBlock will contain multiple ledgers indexes?", "author": "gaoran10", "createdAt": "2021-01-11T17:30:37Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/StreamingOffloadIndexBlock.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud;\n+\n+import java.io.Closeable;\n+import java.io.FilterInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Map;\n+import org.apache.bookkeeper.client.api.LedgerMetadata;\n+import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;\n+\n+/**\n+ * The Index block abstraction used for offload a ledger to long term storage.\n+ */\n+@Unstable\n+public interface StreamingOffloadIndexBlock extends Closeable {\n+\n+    /**\n+     * Get the content of the index block as InputStream.\n+     * Read out in format:\n+     *   | index_magic_header | index_block_len | index_entry_count |\n+     *   | data_object_size | segment_metadata_length | segment metadata | index entries ... |\n+     */\n+    IndexInputStream toStream() throws IOException;\n+\n+    /**\n+     * Get the related OffloadIndexEntry that contains the given messageEntryId.\n+     *\n+     * @param messageEntryId\n+     *                      the entry id of message\n+     * @return the offload index entry\n+     */\n+    OffloadIndexEntry getIndexEntryForEntry(long ledgerId, long messageEntryId) throws IOException;\n+\n+    public long getStartEntryId(long ledgerId);\n+\n+    /**\n+     * Get the entry count that contained in this index Block.\n+     */\n+    int getEntryCount();\n+\n+    /**\n+     * Get LedgerMetadata.\n+     */\n+    Map<Long, LedgerMetadata> getLedgerMetadata();", "originalCommit": "8c48c61b4d825783a9743fd56f4e0bab65805e88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQzMzE5Mw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555433193", "bodyText": "Yes", "author": "Renkai", "createdAt": "2021-01-12T00:42:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTIxOTAzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwMDY4OA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555100688", "bodyText": "typo: uid", "author": "eolivelli", "createdAt": "2021-01-11T14:51:58Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -85,6 +181,34 @@\n                                     UUID uid,\n                                     Map<String, String> extraMetadata);\n \n+    /**\n+     * Begin offload the passed in ledgers to longterm storage, it will finish\n+     * when a segment reached it's size or time.\n+     * Metadata passed in is for inspection purposes only and should be stored\n+     * alongside the segment data.\n+     *\n+     * When the returned OffloaderHandle.getOffloadResultAsync completes, the corresponding\n+     * ledgers has been persisted to the\n+     * loadterm storage, so it is safe to delete the original copy in bookkeeper.\n+     *\n+     * The uid is used to identify an attempt to offload. The implementation should", "originalCommit": "8c48c61b4d825783a9743fd56f4e0bab65805e88", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwMTMxMQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555101311", "bodyText": "what happens if I call this method for the same ManagedLedger more times, with overlapping intervals ?", "author": "eolivelli", "createdAt": "2021-01-11T14:52:54Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -85,6 +181,34 @@\n                                     UUID uid,\n                                     Map<String, String> extraMetadata);\n \n+    /**\n+     * Begin offload the passed in ledgers to longterm storage, it will finish\n+     * when a segment reached it's size or time.\n+     * Metadata passed in is for inspection purposes only and should be stored\n+     * alongside the segment data.\n+     *\n+     * When the returned OffloaderHandle.getOffloadResultAsync completes, the corresponding\n+     * ledgers has been persisted to the\n+     * loadterm storage, so it is safe to delete the original copy in bookkeeper.\n+     *\n+     * The uid is used to identify an attempt to offload. The implementation should\n+     * use this to deterministically generate a unique name for the offloaded object.\n+     * This uid will be stored in the managed ledger metadata before attempting the\n+     * call to offload(). If a subsequent or concurrent call to streamingOffload() finds\n+     * a uid in the metadata, it will attempt to cleanup this attempt with a call\n+     * to #deleteOffloaded(ReadHandle,UUID). Once the offload attempt completes,\n+     * the managed ledger will update its metadata again, to record the completion,\n+     * ensuring that subsequent calls will not attempt to offload the same ledger\n+     * again.\n+     *\n+     * @return an OffloaderHandle, which when `completeFuture()` completed, denotes that the offload has been successful.\n+     */\n+    default CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                              long beginEntry,", "originalCommit": "8c48c61b4d825783a9743fd56f4e0bab65805e88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjIxNTA3NA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r556215074", "bodyText": "what happens if I call this method for the same ManagedLedger more times, with overlapping intervals?\n\nYes, the caller should prevent this happens.", "author": "Renkai", "createdAt": "2021-01-13T02:02:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwMTMxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjMzNzEzNg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r556337136", "bodyText": "if the caller invokes this method with overlapping intervals, will the system be in a corrupted state ?\nis there any way to detect the problem and fail ?", "author": "eolivelli", "createdAt": "2021-01-13T08:20:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwMTMxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjM2ODk4NQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r556368985", "bodyText": "ManagedLedger will check the begin entry and endEntryId in https://github.com/apache/pulsar/wiki/PIP-76:-Streaming-Offload#metadata-changes to avoid corruption, it's not included in this PR yet because it will make the PR too big and include too many details. This PR focused on the interface change of Offloader, I will submit another PR with corruption detection you mentioned.", "author": "Renkai", "createdAt": "2021-01-13T09:14:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwMTMxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjM4NzM4Ng==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r556387386", "bodyText": "works for me, thanks", "author": "eolivelli", "createdAt": "2021-01-13T09:42:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwMTMxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwMjIzNg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555102236", "bodyText": "the same here", "author": "eolivelli", "createdAt": "2021-01-11T14:54:09Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -115,6 +239,16 @@\n     CompletableFuture<Void> deleteOffloaded(long ledgerId, UUID uid,\n                                             Map<String, String> offloadDriverMetadata);\n \n+    default CompletableFuture<ReadHandle> readOffloaded(long ledgerId, MLDataFormats.OffloadContext ledgerContext,\n+                                                        Map<String, String> offloadDriverMetadata) {\n+        throw new UnsupportedClassVersionError();\n+    }\n+\n+    default CompletableFuture<Void> deleteOffloaded(UUID uid,\n+                                                    Map<String, String> offloadDriverMetadata) {\n+        throw new UnsupportedOperationException();", "originalCommit": "8c48c61b4d825783a9743fd56f4e0bab65805e88", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwNDUwOA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555104508", "bodyText": "should not we perform this operation only after the end of the loop above ?\nwhat happens if the streamingOffloadLoop does not complete in time ?", "author": "eolivelli", "createdAt": "2021-01-11T14:57:11Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java", "diffHunk": "@@ -230,6 +260,183 @@ public String getOffloadDriverName() {\n         return promise;\n     }\n \n+    BlobStore blobStore;\n+    String streamingDataBlockKey;\n+    String streamingDataIndexKey;\n+    MultipartUpload streamingMpu = null;\n+    List<MultipartPart> streamingParts = Lists.newArrayList();\n+\n+    @Override\n+    public CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                             long beginEntry,\n+                                                             Map<String, String> driverMetadata) {\n+        this.ml = ml;\n+        this.segmentInfo = new SegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(), driverMetadata);\n+        log.debug(\"begin offload with {}:{}\", beginLedger, beginEntry);\n+        this.offloadResult = new CompletableFuture<>();\n+        blobStore = blobStores.get(config.getBlobStoreLocation());\n+        streamingIndexBuilder = new StreamingOffloadIndexBlockBuilderImpl();\n+        streamingDataBlockKey = segmentInfo.uuid.toString();\n+        streamingDataIndexKey = String.format(\"%s-index\", segmentInfo.uuid);\n+        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);\n+        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);\n+        Blob blob = blobBuilder.build();\n+        streamingMpu = blobStore\n+                .initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());\n+\n+        scheduler.chooseThread(segmentInfo).execute(() -> {\n+            log.info(\"start offloading segment: {}\", segmentInfo);\n+            streamingOffloadLoop(1, 0);\n+        });\n+        scheduler.schedule(this::closeSegment, segmentCloseTime.toMillis(), TimeUnit.MILLISECONDS);", "originalCommit": "8c48c61b4d825783a9743fd56f4e0bab65805e88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjIxNzE2OA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r556217168", "bodyText": "should not we perform this operation only after the end of the loop above ?\nwhat happens if the streamingOffloadLoop does not complete in time ?\n\nWhat's 'this operation' do you mean here?", "author": "Renkai", "createdAt": "2021-01-13T02:09:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwNDUwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjMzNjEyNw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r556336127", "bodyText": "closeSegment", "author": "eolivelli", "createdAt": "2021-01-13T08:18:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwNDUwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjM2NTMwNA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r556365304", "bodyText": "The segment will be closed when the time limit reached, and the streamingOffloadLoop will aware of it and stop looping.", "author": "Renkai", "createdAt": "2021-01-13T09:08:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwNDUwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjM4NzMzMw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r556387333", "bodyText": "ok, thanks for your clarification", "author": "eolivelli", "createdAt": "2021-01-13T09:42:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwNDUwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwNDY4Mw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555104683", "bodyText": "we can provide more information here", "author": "eolivelli", "createdAt": "2021-01-11T14:57:26Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java", "diffHunk": "@@ -230,6 +260,183 @@ public String getOffloadDriverName() {\n         return promise;\n     }\n \n+    BlobStore blobStore;\n+    String streamingDataBlockKey;\n+    String streamingDataIndexKey;\n+    MultipartUpload streamingMpu = null;\n+    List<MultipartPart> streamingParts = Lists.newArrayList();\n+\n+    @Override\n+    public CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                             long beginEntry,\n+                                                             Map<String, String> driverMetadata) {\n+        this.ml = ml;\n+        this.segmentInfo = new SegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(), driverMetadata);\n+        log.debug(\"begin offload with {}:{}\", beginLedger, beginEntry);\n+        this.offloadResult = new CompletableFuture<>();\n+        blobStore = blobStores.get(config.getBlobStoreLocation());\n+        streamingIndexBuilder = new StreamingOffloadIndexBlockBuilderImpl();\n+        streamingDataBlockKey = segmentInfo.uuid.toString();\n+        streamingDataIndexKey = String.format(\"%s-index\", segmentInfo.uuid);\n+        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);\n+        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);\n+        Blob blob = blobBuilder.build();\n+        streamingMpu = blobStore\n+                .initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());\n+\n+        scheduler.chooseThread(segmentInfo).execute(() -> {\n+            log.info(\"start offloading segment: {}\", segmentInfo);\n+            streamingOffloadLoop(1, 0);\n+        });\n+        scheduler.schedule(this::closeSegment, segmentCloseTime.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        return CompletableFuture.completedFuture(new OffloadHandle() {\n+            @Override\n+            public boolean canOffer(long size) {\n+                return BlobStoreManagedLedgerOffloader.this.canOffer(size);\n+            }\n+\n+            @Override\n+            public PositionImpl lastOffered() {\n+                return BlobStoreManagedLedgerOffloader.this.lastOffered();\n+            }\n+\n+            @Override\n+            public boolean offerEntry(EntryImpl entry) throws OffloadSegmentClosedException,\n+                    OffloadNotConsecutiveException {\n+                return BlobStoreManagedLedgerOffloader.this.offerEntry(entry);\n+            }\n+\n+            @Override\n+            public CompletableFuture<OffloadResult> getOffloadResultAsync() {\n+                return BlobStoreManagedLedgerOffloader.this.getOffloadResultAsync();\n+            }\n+        });\n+    }\n+\n+    private void streamingOffloadLoop(int partId, int dataObjectLength) {\n+        log.debug(\"streaming offload loop {} {}\", partId, dataObjectLength);\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            offloadResult.complete(segmentInfo.result());\n+            return;\n+        }\n+        final BufferedOffloadStream payloadStream;\n+\n+        while (offloadBuffer.isEmpty()) {\n+            if (segmentInfo.isClosed()) {\n+                offloadResult.complete(segmentInfo.result());\n+            } else {\n+                try {\n+                    Thread.sleep(100);\n+                } catch (InterruptedException e) {\n+                    e.printStackTrace();\n+                }\n+            }\n+        }\n+        final Entry peek = offloadBuffer.peek();\n+        //initialize payload when there is at least one entry\n+        final long blockLedgerId = peek.getLedgerId();\n+        final long blockEntryId = peek.getEntryId();\n+        payloadStream = new BufferedOffloadStream(streamingBlockSize, offloadBuffer, segmentInfo,\n+                blockLedgerId, blockEntryId, bufferLength);\n+        try {\n+            streamingIndexBuilder.addLedgerMeta(blockLedgerId, ml.getRawLedgerMetadata(blockLedgerId).get());\n+            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());\n+        } catch (InterruptedException | ExecutionException e) {\n+            offloadResult.completeExceptionally(e);\n+            return;\n+        }\n+        log.debug(\"begin upload payload\");", "originalCommit": "8c48c61b4d825783a9743fd56f4e0bab65805e88", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA0MzU3Nw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560043577", "bodyText": "+1", "author": "codelipenghui", "createdAt": "2021-01-19T09:45:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwNDY4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEwNTQzOQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r555105439", "bodyText": "nit: static ?", "author": "eolivelli", "createdAt": "2021-01-11T14:58:28Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java", "diffHunk": "@@ -230,6 +260,183 @@ public String getOffloadDriverName() {\n         return promise;\n     }\n \n+    BlobStore blobStore;\n+    String streamingDataBlockKey;\n+    String streamingDataIndexKey;\n+    MultipartUpload streamingMpu = null;\n+    List<MultipartPart> streamingParts = Lists.newArrayList();\n+\n+    @Override\n+    public CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                             long beginEntry,\n+                                                             Map<String, String> driverMetadata) {\n+        this.ml = ml;\n+        this.segmentInfo = new SegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(), driverMetadata);\n+        log.debug(\"begin offload with {}:{}\", beginLedger, beginEntry);\n+        this.offloadResult = new CompletableFuture<>();\n+        blobStore = blobStores.get(config.getBlobStoreLocation());\n+        streamingIndexBuilder = new StreamingOffloadIndexBlockBuilderImpl();\n+        streamingDataBlockKey = segmentInfo.uuid.toString();\n+        streamingDataIndexKey = String.format(\"%s-index\", segmentInfo.uuid);\n+        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);\n+        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);\n+        Blob blob = blobBuilder.build();\n+        streamingMpu = blobStore\n+                .initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());\n+\n+        scheduler.chooseThread(segmentInfo).execute(() -> {\n+            log.info(\"start offloading segment: {}\", segmentInfo);\n+            streamingOffloadLoop(1, 0);\n+        });\n+        scheduler.schedule(this::closeSegment, segmentCloseTime.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        return CompletableFuture.completedFuture(new OffloadHandle() {\n+            @Override\n+            public boolean canOffer(long size) {\n+                return BlobStoreManagedLedgerOffloader.this.canOffer(size);\n+            }\n+\n+            @Override\n+            public PositionImpl lastOffered() {\n+                return BlobStoreManagedLedgerOffloader.this.lastOffered();\n+            }\n+\n+            @Override\n+            public boolean offerEntry(EntryImpl entry) throws OffloadSegmentClosedException,\n+                    OffloadNotConsecutiveException {\n+                return BlobStoreManagedLedgerOffloader.this.offerEntry(entry);\n+            }\n+\n+            @Override\n+            public CompletableFuture<OffloadResult> getOffloadResultAsync() {\n+                return BlobStoreManagedLedgerOffloader.this.getOffloadResultAsync();\n+            }\n+        });\n+    }\n+\n+    private void streamingOffloadLoop(int partId, int dataObjectLength) {\n+        log.debug(\"streaming offload loop {} {}\", partId, dataObjectLength);\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            offloadResult.complete(segmentInfo.result());\n+            return;\n+        }\n+        final BufferedOffloadStream payloadStream;\n+\n+        while (offloadBuffer.isEmpty()) {\n+            if (segmentInfo.isClosed()) {\n+                offloadResult.complete(segmentInfo.result());\n+            } else {\n+                try {\n+                    Thread.sleep(100);\n+                } catch (InterruptedException e) {\n+                    e.printStackTrace();\n+                }\n+            }\n+        }\n+        final Entry peek = offloadBuffer.peek();\n+        //initialize payload when there is at least one entry\n+        final long blockLedgerId = peek.getLedgerId();\n+        final long blockEntryId = peek.getEntryId();\n+        payloadStream = new BufferedOffloadStream(streamingBlockSize, offloadBuffer, segmentInfo,\n+                blockLedgerId, blockEntryId, bufferLength);\n+        try {\n+            streamingIndexBuilder.addLedgerMeta(blockLedgerId, ml.getRawLedgerMetadata(blockLedgerId).get());\n+            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());\n+        } catch (InterruptedException | ExecutionException e) {\n+            offloadResult.completeExceptionally(e);\n+            return;\n+        }\n+        log.debug(\"begin upload payload\");\n+        Payload partPayload = Payloads.newInputStreamPayload(payloadStream);\n+        partPayload.getContentMetadata().setContentType(\"application/octet-stream\");\n+        streamingParts.add(blobStore.uploadMultipartPart(streamingMpu, partId, partPayload));\n+        streamingIndexBuilder.addBlock(blockLedgerId, blockEntryId, partId, streamingBlockSize);\n+\n+        log.debug(\"UploadMultipartPart. container: {}, blobName: {}, partId: {}, mpu: {}\",\n+                config.getBucket(), streamingDataBlockKey, partId, streamingMpu.id());\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            try {\n+                blobStore.completeMultipartUpload(streamingMpu, streamingParts);\n+                streamingIndexBuilder.withDataObjectLength(dataObjectLength + streamingBlockSize);\n+                final StreamingOffloadIndexBlock index = streamingIndexBuilder.build();\n+                final StreamingOffloadIndexBlock.IndexInputStream indexStream = index.toStream();\n+                final BlobBuilder indexBlobBuilder = blobStore.blobBuilder(streamingDataIndexKey);\n+                DataBlockUtils.addVersionInfo(indexBlobBuilder, userMetadata);\n+                final InputStreamPayload indexPayLoad = Payloads.newInputStreamPayload(indexStream);\n+                indexPayLoad.getContentMetadata().setContentLength(indexStream.getStreamSize());\n+                indexPayLoad.getContentMetadata().setContentType(\"application/octet-stream\");\n+                final Blob indexBlob = indexBlobBuilder.payload(indexPayLoad)\n+                        .contentLength(indexStream.getStreamSize())\n+                        .build();\n+                blobStore.putBlob(config.getBucket(), indexBlob);\n+\n+                offloadResult.complete(segmentInfo.result());\n+            } catch (Exception e) {\n+                log.error(\"streaming offload failed\", e);\n+                offloadResult.completeExceptionally(e);\n+            }\n+        } else {\n+            scheduler.chooseThread(segmentInfo)\n+                    .execute(() -> streamingOffloadLoop(partId + 1, dataObjectLength + streamingBlockSize));\n+        }\n+    }\n+\n+    private CompletableFuture<OffloadResult> getOffloadResultAsync() {\n+        return this.offloadResult;\n+    }\n+\n+    private synchronized boolean offerEntry(EntryImpl entry) throws OffloadSegmentClosedException,\n+            OffloadNotConsecutiveException {\n+        if (segmentInfo.isClosed()) {\n+            throw new OffloadSegmentClosedException(\"Segment already closed \" + segmentInfo);\n+        } else {\n+            if (!naiveCheckConsecutive(lastOfferedPosition, entry.getPosition())) {\n+                throw new OffloadNotConsecutiveException(\n+                        Strings.lenientFormat(\"position %s and %s are not consecutive\", lastOfferedPosition,\n+                                entry.getPosition()));\n+            }\n+            entry.retain();\n+            offloadBuffer.add(entry);\n+            bufferLength.getAndAdd(entry.getLength());\n+            segmentLength.getAndAdd(entry.getLength());\n+            lastOfferedPosition = entry.getPosition();\n+            if (segmentLength.get() >= maxSegmentLength) {\n+                closeSegment();\n+            }\n+            return true;\n+        }\n+    }\n+\n+    private synchronized void closeSegment() {\n+        log.debug(\"close segment {} {}\", lastOfferedPosition.getLedgerId(), lastOfferedPosition.getEntryId());\n+        this.segmentInfo.closeSegment(lastOfferedPosition.getLedgerId(), lastOfferedPosition.getEntryId());\n+    }\n+\n+    private boolean naiveCheckConsecutive(PositionImpl lastOfferedPosition, PositionImpl offeringPosition) {", "originalCommit": "8c48c61b4d825783a9743fd56f4e0bab65805e88", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "url": "https://github.com/apache/pulsar/commit/d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "message": "implement streaming offloader\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-13T09:01:22Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU3NzYyNw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557577627", "bodyText": "Can you move the implementation outside of an interface?", "author": "sijie", "createdAt": "2021-01-14T17:44:51Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -22,19 +22,115 @@\n import java.util.Map;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n-\n+import lombok.ToString;\n import org.apache.bookkeeper.client.api.ReadHandle;\n import org.apache.bookkeeper.common.annotation.InterfaceAudience;\n import org.apache.bookkeeper.common.annotation.InterfaceStability;\n+import org.apache.bookkeeper.mledger.ManagedLedgerException.OffloadNotConsecutiveException;\n+import org.apache.bookkeeper.mledger.ManagedLedgerException.OffloadSegmentClosedException;\n+import org.apache.bookkeeper.mledger.impl.EntryImpl;\n+import org.apache.bookkeeper.mledger.impl.PositionImpl;\n+import org.apache.bookkeeper.mledger.proto.MLDataFormats;\n import org.apache.pulsar.common.policies.data.OffloadPolicies;\n \n /**\n- * Interface for offloading ledgers to long-term storage\n+ * Interface for offloading ledgers to long-term storage.\n  */\n @InterfaceAudience.LimitedPrivate\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    interface SegmentInfo {\n+        boolean isClosed();\n+\n+        OffloadResult result();\n+    }\n+\n+    @ToString\n+    class SegmentInfoImpl implements SegmentInfo {", "originalCommit": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU3ODA3OQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557578079", "bodyText": "What does \"create one per second\" mean?", "author": "sijie", "createdAt": "2021-01-14T17:45:34Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -22,19 +22,115 @@\n import java.util.Map;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n-\n+import lombok.ToString;\n import org.apache.bookkeeper.client.api.ReadHandle;\n import org.apache.bookkeeper.common.annotation.InterfaceAudience;\n import org.apache.bookkeeper.common.annotation.InterfaceStability;\n+import org.apache.bookkeeper.mledger.ManagedLedgerException.OffloadNotConsecutiveException;\n+import org.apache.bookkeeper.mledger.ManagedLedgerException.OffloadSegmentClosedException;\n+import org.apache.bookkeeper.mledger.impl.EntryImpl;\n+import org.apache.bookkeeper.mledger.impl.PositionImpl;\n+import org.apache.bookkeeper.mledger.proto.MLDataFormats;\n import org.apache.pulsar.common.policies.data.OffloadPolicies;\n \n /**\n- * Interface for offloading ledgers to long-term storage\n+ * Interface for offloading ledgers to long-term storage.\n  */\n @InterfaceAudience.LimitedPrivate\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    interface SegmentInfo {\n+        boolean isClosed();\n+\n+        OffloadResult result();\n+    }\n+\n+    @ToString\n+    class SegmentInfoImpl implements SegmentInfo {\n+        public SegmentInfoImpl(UUID uuid, long beginLedger, long beginEntry, String driverName,\n+                               Map<String, String> driverMetadata) {\n+            this.uuid = uuid;\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.driverName = driverName;\n+            this.driverMetadata = driverMetadata;\n+        }\n+\n+\n+        public final UUID uuid;\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final String driverName;\n+        volatile private long endLedger;\n+        volatile private long endEntry;\n+        volatile boolean closed = false;\n+        public final Map<String, String> driverMetadata;\n+\n+        public boolean isClosed() {\n+            return closed;\n+        }\n+\n+        public void closeSegment(long endLedger, long endEntry) {\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+            this.closed = true;\n+        }\n+\n+        public OffloadResult result() {\n+            return new OffloadResult(beginLedger, beginEntry, endLedger, endEntry);\n+        }\n+    }\n+\n+\n+    class OffloadResult {\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final long endLedger;\n+        public final long endEntry;\n+\n+        public OffloadResult(long beginLedger, long beginEntry, long endLedger, long endEntry) {\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+        }\n+    }\n+\n+    /**\n+     * Used to store driver info, buffer entries, mark progress, etc.\n+     * Create one per second.", "originalCommit": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzkzNjM5Mg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557936392", "bodyText": "'per segment' typo fixed", "author": "Renkai", "createdAt": "2021-01-15T07:18:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU3ODA3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU3ODcxNw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557578717", "bodyText": "It is a bit strange to implement an async method over a sync method. We usually implement the other way around.", "author": "sijie", "createdAt": "2021-01-14T17:46:32Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -22,19 +22,115 @@\n import java.util.Map;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n-\n+import lombok.ToString;\n import org.apache.bookkeeper.client.api.ReadHandle;\n import org.apache.bookkeeper.common.annotation.InterfaceAudience;\n import org.apache.bookkeeper.common.annotation.InterfaceStability;\n+import org.apache.bookkeeper.mledger.ManagedLedgerException.OffloadNotConsecutiveException;\n+import org.apache.bookkeeper.mledger.ManagedLedgerException.OffloadSegmentClosedException;\n+import org.apache.bookkeeper.mledger.impl.EntryImpl;\n+import org.apache.bookkeeper.mledger.impl.PositionImpl;\n+import org.apache.bookkeeper.mledger.proto.MLDataFormats;\n import org.apache.pulsar.common.policies.data.OffloadPolicies;\n \n /**\n- * Interface for offloading ledgers to long-term storage\n+ * Interface for offloading ledgers to long-term storage.\n  */\n @InterfaceAudience.LimitedPrivate\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    interface SegmentInfo {\n+        boolean isClosed();\n+\n+        OffloadResult result();\n+    }\n+\n+    @ToString\n+    class SegmentInfoImpl implements SegmentInfo {\n+        public SegmentInfoImpl(UUID uuid, long beginLedger, long beginEntry, String driverName,\n+                               Map<String, String> driverMetadata) {\n+            this.uuid = uuid;\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.driverName = driverName;\n+            this.driverMetadata = driverMetadata;\n+        }\n+\n+\n+        public final UUID uuid;\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final String driverName;\n+        volatile private long endLedger;\n+        volatile private long endEntry;\n+        volatile boolean closed = false;\n+        public final Map<String, String> driverMetadata;\n+\n+        public boolean isClosed() {\n+            return closed;\n+        }\n+\n+        public void closeSegment(long endLedger, long endEntry) {\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+            this.closed = true;\n+        }\n+\n+        public OffloadResult result() {\n+            return new OffloadResult(beginLedger, beginEntry, endLedger, endEntry);\n+        }\n+    }\n+\n+\n+    class OffloadResult {\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final long endLedger;\n+        public final long endEntry;\n+\n+        public OffloadResult(long beginLedger, long beginEntry, long endLedger, long endEntry) {\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+        }\n+    }\n+\n+    /**\n+     * Used to store driver info, buffer entries, mark progress, etc.\n+     * Create one per second.\n+     */\n+    interface OffloadHandle {\n+\n+        /**\n+         * return true when both buffer have enough size and ledger/entry id is next to the current one.\n+         * @param size\n+         * @return\n+         */\n+        boolean canOffer(long size);\n+\n+        default CompletableFuture<Boolean> asyncCanOffer(long size) {", "originalCommit": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzkzOTYxNw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557939617", "bodyText": "Our current sync method implementation is quite efficient (just compare two existing number and return), though a bit strange, I still don't think we should sacrifice the performance to make the implementation conform common practice.", "author": "Renkai", "createdAt": "2021-01-15T07:22:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU3ODcxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTk5MDE3MA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r559990170", "bodyText": "You can't assume the canOffer always \"efficient\", it depends on the implementations. From the interface perspective, this is not the right way.\nuse default for the interface method, this means it's not required for the implementation to implement this method, so an implementation can only implement the sync method. If the sync method need some time-consuming operations, This destroys the original intention of the interface", "author": "codelipenghui", "createdAt": "2021-01-19T08:24:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU3ODcxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU3OTIzOA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557579238", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * loadterm storage, so it is safe to delete the original copy in bookkeeper.\n          \n          \n            \n                 * longterm storage, so it is safe to delete the original copy in bookkeeper.", "author": "sijie", "createdAt": "2021-01-14T17:47:24Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -85,6 +181,34 @@\n                                     UUID uid,\n                                     Map<String, String> extraMetadata);\n \n+    /**\n+     * Begin offload the passed in ledgers to longterm storage, it will finish\n+     * when a segment reached it's size or time.\n+     * Metadata passed in is for inspection purposes only and should be stored\n+     * alongside the segment data.\n+     *\n+     * When the returned OffloaderHandle.getOffloadResultAsync completes, the corresponding\n+     * ledgers has been persisted to the\n+     * loadterm storage, so it is safe to delete the original copy in bookkeeper.", "originalCommit": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU4MTU2MA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557581560", "bodyText": "If this interface is designed to be an async method, the exception should be returned from the CompletableFuture. The interface seems to be mix async semantic with sync semantic together.", "author": "sijie", "createdAt": "2021-01-14T17:51:07Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/ManagedLedger.java", "diffHunk": "@@ -595,4 +594,10 @@ void asyncSetProperties(Map<String, String> properties, final AsyncCallbacks.Upd\n      * Get the ManagedLedgerInterceptor for ManagedLedger.\n      * */\n     ManagedLedgerInterceptor getManagedLedgerInterceptor();\n+\n+    /**\n+     * Get basic ledger summary after the ledger is closed.\n+     * will got exception if corresponding ledger was not closed when the method called.\n+     */\n+    CompletableFuture<LedgerInfo> getClosedLedgerInfo(long ledgerId) throws ManagedLedgerException;", "originalCommit": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU4MjgwMQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557582801", "bodyText": "What happens if the ledger is closed with zero entries?", "author": "sijie", "createdAt": "2021-01-14T17:53:20Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/ManagedLedgerImpl.java", "diffHunk": "@@ -1685,6 +1686,20 @@ void asyncReadEntries(OpReadEntry opReadEntry) {\n         return getLedgerHandle(ledgerId).thenApply(rh -> rh.getLedgerMetadata().toSafeString());\n     }\n \n+    @Override\n+    public CompletableFuture<LedgerInfo> getClosedLedgerInfo(long ledgerId) throws ManagedLedgerException {\n+        final LedgerInfo ledgerInfo = ledgers.get(ledgerId);\n+        if (ledgerInfo == null) {\n+            throw new ManagedLedgerException(\n+                    Strings.lenientFormat(\"ledger with id %s not found\", ledgerId));\n+        } else if (ledgerInfo.getSize() == 0 || ledgerInfo.getEntries() == 0) {", "originalCommit": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzkxNTU2Nw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557915567", "bodyText": "What happens if the ledger is closed with zero entries?\n\nIt will be deleted from metadata, see \n  \n    \n      pulsar/managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/ManagedLedgerImpl.java\n    \n    \n        Lines 1535 to 1547\n      in\n      d3a6a8c\n    \n    \n    \n    \n\n        \n          \n           if (entriesInLedger > 0) { \n        \n\n        \n          \n               LedgerInfo info = LedgerInfo.newBuilder().setLedgerId(lh.getId()).setEntries(entriesInLedger) \n        \n\n        \n          \n                       .setSize(lh.getLength()).setTimestamp(clock.millis()).build(); \n        \n\n        \n          \n               ledgers.put(lh.getId(), info); \n        \n\n        \n          \n           } else { \n        \n\n        \n          \n               // The last ledger was empty, so we can discard it \n        \n\n        \n          \n               ledgers.remove(lh.getId()); \n        \n\n        \n          \n               mbean.startDataLedgerDeleteOp(); \n        \n\n        \n          \n               bookKeeper.asyncDeleteLedger(lh.getId(), (rc, ctx) -> { \n        \n\n        \n          \n                   mbean.endDataLedgerDeleteOp(); \n        \n\n        \n          \n                   log.info(\"[{}] Delete complete for empty ledger {}. rc={}\", name, lh.getId(), rc); \n        \n\n        \n          \n               }, null); \n        \n\n        \n          \n           }", "author": "Renkai", "createdAt": "2021-01-15T06:53:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU4MjgwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU4Mzk4Ng==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557583986", "bodyText": "Can we move implementation out of this interface?", "author": "sijie", "createdAt": "2021-01-14T17:55:26Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/StreamingOffloadIndexBlock.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud;\n+\n+import java.io.Closeable;\n+import java.io.FilterInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import org.apache.bookkeeper.client.api.LedgerMetadata;\n+import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;\n+\n+/**\n+ * The Index block abstraction used for offload a ledger to long term storage.\n+ */\n+@Unstable\n+public interface StreamingOffloadIndexBlock extends Closeable {\n+\n+    /**\n+     * Get the content of the index block as InputStream.\n+     * Read out in format:\n+     *   | index_magic_header | index_block_len | index_entry_count |\n+     *   | data_object_size | segment_metadata_length | segment metadata | index entries ... |\n+     */\n+    IndexInputStream toStream() throws IOException;\n+\n+    /**\n+     * Get the related OffloadIndexEntry that contains the given messageEntryId.\n+     *\n+     * @param messageEntryId\n+     *                      the entry id of message\n+     * @return the offload index entry\n+     */\n+    OffloadIndexEntry getIndexEntryForEntry(long ledgerId, long messageEntryId) throws IOException;\n+\n+    public long getStartEntryId(long ledgerId);\n+\n+    /**\n+     * Get the entry count that contained in this index Block.\n+     */\n+    int getEntryCount();\n+\n+    /**\n+     * Get LedgerMetadata.\n+     * @return\n+     */\n+    LedgerMetadata getLedgerMetadata(long ledgerId);\n+\n+    /**\n+     * Get the total size of the data object.\n+     */\n+    long getDataObjectLength();\n+\n+    /**\n+     * Get the length of the header in the blocks in the data object.\n+     */\n+    long getDataBlockHeaderLength();\n+\n+    /**\n+     * An input stream which knows the size of the stream upfront.\n+     */\n+    class IndexInputStream extends FilterInputStream {", "originalCommit": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU4NTU1NA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557585554", "bodyText": "Put log.debug into if (log.isDebugEnabled()) { ... }", "author": "sijie", "createdAt": "2021-01-14T17:57:59Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/StreamingBlobStoreBackedReadHandleImpl.java", "diffHunk": "@@ -0,0 +1,284 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud.impl;\n+\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.ScheduledExecutorService;\n+import lombok.val;\n+import org.apache.bookkeeper.client.BKException;\n+import org.apache.bookkeeper.client.api.LastConfirmedAndEntry;\n+import org.apache.bookkeeper.client.api.LedgerEntries;\n+import org.apache.bookkeeper.client.api.LedgerEntry;\n+import org.apache.bookkeeper.client.api.LedgerMetadata;\n+import org.apache.bookkeeper.client.api.ReadHandle;\n+import org.apache.bookkeeper.client.impl.LedgerEntriesImpl;\n+import org.apache.bookkeeper.client.impl.LedgerEntryImpl;\n+import org.apache.bookkeeper.mledger.offload.jcloud.BackedInputStream;\n+import org.apache.bookkeeper.mledger.offload.jcloud.StreamingOffloadIndexBlock;\n+import org.apache.bookkeeper.mledger.offload.jcloud.StreamingOffloadIndexBlockBuilder;\n+import org.apache.bookkeeper.mledger.offload.jcloud.impl.DataBlockUtils.VersionCheck;\n+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;\n+import org.jclouds.blobstore.BlobStore;\n+import org.jclouds.blobstore.domain.Blob;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class StreamingBlobStoreBackedReadHandleImpl implements ReadHandle {\n+    private static final Logger log = LoggerFactory.getLogger(StreamingBlobStoreBackedReadHandleImpl.class);\n+\n+    private final long ledgerId;\n+    private final List<StreamingOffloadIndexBlock> indices;\n+    private final List<BackedInputStream> inputStreams;\n+    private final List<DataInputStream> dataStreams;\n+    private final ExecutorService executor;\n+\n+    static class GroupedReader {\n+        long ledgerId;\n+        long firstEntry;\n+        long lastEntry;\n+        StreamingOffloadIndexBlock index;\n+        BackedInputStream inputStream;\n+        DataInputStream dataStream;\n+\n+        public GroupedReader(long ledgerId, long firstEntry, long lastEntry,\n+                             StreamingOffloadIndexBlock index,\n+                             BackedInputStream inputStream, DataInputStream dataStream) {\n+            this.ledgerId = ledgerId;\n+            this.firstEntry = firstEntry;\n+            this.lastEntry = lastEntry;\n+            this.index = index;\n+            this.inputStream = inputStream;\n+            this.dataStream = dataStream;\n+        }\n+    }\n+\n+    private StreamingBlobStoreBackedReadHandleImpl(long ledgerId, List<StreamingOffloadIndexBlock> indices,\n+                                                   List<BackedInputStream> inputStreams,\n+                                                   ExecutorService executor) {\n+        this.ledgerId = ledgerId;\n+        this.indices = indices;\n+        this.inputStreams = inputStreams;\n+        this.dataStreams = new LinkedList<>();\n+        for (BackedInputStream inputStream : inputStreams) {\n+            dataStreams.add(new DataInputStream(inputStream));\n+        }\n+        this.executor = executor;\n+    }\n+\n+    @Override\n+    public long getId() {\n+        return ledgerId;\n+    }\n+\n+    @Override\n+    public LedgerMetadata getLedgerMetadata() {\n+        return indices.get(0).getLedgerMetadata(ledgerId);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> closeAsync() {\n+        CompletableFuture<Void> promise = new CompletableFuture<>();\n+        executor.submit(() -> {\n+            try {\n+                for (StreamingOffloadIndexBlock indexBlock : indices) {\n+                    indexBlock.close();\n+                }\n+                for (BackedInputStream inputStream : inputStreams) {\n+                    inputStream.close();\n+                }\n+                promise.complete(null);\n+            } catch (IOException t) {\n+                promise.completeExceptionally(t);\n+            }\n+        });\n+        return promise;\n+    }\n+\n+    @Override\n+    public CompletableFuture<LedgerEntries> readAsync(long firstEntry, long lastEntry) {\n+        log.debug(\"Ledger {}: reading {} - {}\", getId(), firstEntry, lastEntry);", "originalCommit": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU4NjQyOA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557586428", "bodyText": "Please address all the occurrences.", "author": "sijie", "createdAt": "2021-01-14T17:59:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU4NTU1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODAxMDM0NQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r558010345", "bodyText": "@sijie I don't think put log.debug into if (log.isDebugEnabled()) { ... } is a good practice today. It was originally designed to reduce string construction cost in log4j1, but today every log framework implemented slf4j provided a lazy formatter to prevent string construction when debug is not enabled. It can not only reduce boilerplate code and avoid check the log level twice.\nif (log.isDebugEnabled()) { ... } should only happened when we have extra computation for debug.\nSee\nhttps://logging.apache.org/log4j/2.x/manual/api.html (Section: Substituting Parameters)\nhttps://logging.apache.org/log4j/log4j-2.3/performance.html", "author": "Renkai", "createdAt": "2021-01-15T08:15:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU4NTU1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODAzODQ4OA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r558038488", "bodyText": "@Renkai that would be a very long discussion, thanks for bringing it up!\nin my opinion it is better to have a consistent way of using the logger thru all of the codebase.\nif we want to change this pattern please start a new discussion, and so we can start a new set of patches.", "author": "eolivelli", "createdAt": "2021-01-15T08:34:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU4NTU1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzU4ODI0OA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557588248", "bodyText": "Can you rename it to maxOffloadSegmentSizeInBytes?", "author": "sijie", "createdAt": "2021-01-14T18:02:07Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/provider/TieredStorageConfiguration.java", "diffHunk": "@@ -63,6 +63,12 @@\n     public static final String METADATA_FIELD_MAX_BLOCK_SIZE = \"maxBlockSizeInBytes\";\n     public static final String METADATA_FIELD_READ_BUFFER_SIZE = \"readBufferSizeInBytes\";\n     public static final String OFFLOADER_PROPERTY_PREFIX = \"managedLedgerOffload\";\n+    public static final String MAX_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC = \"maxOffloadSegmentRolloverTimeInSeconds\";\n+    public static final String MIN_OFFLOAD_SEGMENT_ROLLOVER_TIME_SEC = \"minOffloadSegmentRolloverTimeInSeconds\";\n+    public static final long DEFAULT_MAX_SEGMENT_TIME_IN_SECOND = 600;\n+    public static final long DEFAULT_MIN_SEGMENT_TIME_IN_SECOND = 0;\n+    public static final String MAX_SEGMENT_SIZE_IN_BYTES = \"maxSegmentSizeInBytes\";", "originalCommit": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzcxMzY2NA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557713664", "bodyText": "Should this be info or debug?", "author": "sijie", "createdAt": "2021-01-14T21:35:45Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/StreamingBlobStoreBackedReadHandleImpl.java", "diffHunk": "@@ -0,0 +1,284 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud.impl;\n+\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.ScheduledExecutorService;\n+import lombok.val;\n+import org.apache.bookkeeper.client.BKException;\n+import org.apache.bookkeeper.client.api.LastConfirmedAndEntry;\n+import org.apache.bookkeeper.client.api.LedgerEntries;\n+import org.apache.bookkeeper.client.api.LedgerEntry;\n+import org.apache.bookkeeper.client.api.LedgerMetadata;\n+import org.apache.bookkeeper.client.api.ReadHandle;\n+import org.apache.bookkeeper.client.impl.LedgerEntriesImpl;\n+import org.apache.bookkeeper.client.impl.LedgerEntryImpl;\n+import org.apache.bookkeeper.mledger.offload.jcloud.BackedInputStream;\n+import org.apache.bookkeeper.mledger.offload.jcloud.StreamingOffloadIndexBlock;\n+import org.apache.bookkeeper.mledger.offload.jcloud.StreamingOffloadIndexBlockBuilder;\n+import org.apache.bookkeeper.mledger.offload.jcloud.impl.DataBlockUtils.VersionCheck;\n+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;\n+import org.jclouds.blobstore.BlobStore;\n+import org.jclouds.blobstore.domain.Blob;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class StreamingBlobStoreBackedReadHandleImpl implements ReadHandle {\n+    private static final Logger log = LoggerFactory.getLogger(StreamingBlobStoreBackedReadHandleImpl.class);\n+\n+    private final long ledgerId;\n+    private final List<StreamingOffloadIndexBlock> indices;\n+    private final List<BackedInputStream> inputStreams;\n+    private final List<DataInputStream> dataStreams;\n+    private final ExecutorService executor;\n+\n+    static class GroupedReader {\n+        long ledgerId;\n+        long firstEntry;\n+        long lastEntry;\n+        StreamingOffloadIndexBlock index;\n+        BackedInputStream inputStream;\n+        DataInputStream dataStream;\n+\n+        public GroupedReader(long ledgerId, long firstEntry, long lastEntry,\n+                             StreamingOffloadIndexBlock index,\n+                             BackedInputStream inputStream, DataInputStream dataStream) {\n+            this.ledgerId = ledgerId;\n+            this.firstEntry = firstEntry;\n+            this.lastEntry = lastEntry;\n+            this.index = index;\n+            this.inputStream = inputStream;\n+            this.dataStream = dataStream;\n+        }\n+    }\n+\n+    private StreamingBlobStoreBackedReadHandleImpl(long ledgerId, List<StreamingOffloadIndexBlock> indices,\n+                                                   List<BackedInputStream> inputStreams,\n+                                                   ExecutorService executor) {\n+        this.ledgerId = ledgerId;\n+        this.indices = indices;\n+        this.inputStreams = inputStreams;\n+        this.dataStreams = new LinkedList<>();\n+        for (BackedInputStream inputStream : inputStreams) {\n+            dataStreams.add(new DataInputStream(inputStream));\n+        }\n+        this.executor = executor;\n+    }\n+\n+    @Override\n+    public long getId() {\n+        return ledgerId;\n+    }\n+\n+    @Override\n+    public LedgerMetadata getLedgerMetadata() {\n+        return indices.get(0).getLedgerMetadata(ledgerId);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> closeAsync() {\n+        CompletableFuture<Void> promise = new CompletableFuture<>();\n+        executor.submit(() -> {\n+            try {\n+                for (StreamingOffloadIndexBlock indexBlock : indices) {\n+                    indexBlock.close();\n+                }\n+                for (BackedInputStream inputStream : inputStreams) {\n+                    inputStream.close();\n+                }\n+                promise.complete(null);\n+            } catch (IOException t) {\n+                promise.completeExceptionally(t);\n+            }\n+        });\n+        return promise;\n+    }\n+\n+    @Override\n+    public CompletableFuture<LedgerEntries> readAsync(long firstEntry, long lastEntry) {\n+        log.debug(\"Ledger {}: reading {} - {}\", getId(), firstEntry, lastEntry);\n+        CompletableFuture<LedgerEntries> promise = new CompletableFuture<>();\n+        if (firstEntry > lastEntry\n+                || firstEntry < 0\n+                || lastEntry > getLastAddConfirmed()) {\n+            promise.completeExceptionally(new BKException.BKIncorrectParameterException());\n+            return promise;\n+        }\n+        executor.submit(() -> {\n+            List<LedgerEntry> entries = new ArrayList<LedgerEntry>();\n+            List<GroupedReader> groupedReaders = null;\n+            try {\n+                groupedReaders = getGroupedReader(firstEntry, lastEntry);\n+            } catch (Exception e) {\n+                promise.completeExceptionally(e);\n+                return;\n+            }\n+\n+            for (GroupedReader groupedReader : groupedReaders) {\n+                long entriesToRead = (groupedReader.lastEntry - groupedReader.firstEntry) + 1;\n+                long nextExpectedId = groupedReader.firstEntry;\n+                try {\n+                    while (entriesToRead > 0) {\n+                        int length = groupedReader.dataStream.readInt();\n+                        if (length < 0) { // hit padding or new block\n+                            groupedReader.inputStream\n+                                    .seek(groupedReader.index\n+                                            .getIndexEntryForEntry(groupedReader.ledgerId, nextExpectedId)\n+                                            .getDataOffset());\n+                            continue;\n+                        }\n+                        long entryId = groupedReader.dataStream.readLong();\n+\n+                        if (entryId == nextExpectedId) {\n+                            ByteBuf buf = PulsarByteBufAllocator.DEFAULT.buffer(length, length);\n+                            entries.add(LedgerEntryImpl.create(ledgerId, entryId, length, buf));\n+                            int toWrite = length;\n+                            while (toWrite > 0) {\n+                                toWrite -= buf.writeBytes(groupedReader.dataStream, toWrite);\n+                            }\n+                            entriesToRead--;\n+                            nextExpectedId++;\n+                        } else if (entryId > nextExpectedId) {\n+                            groupedReader.inputStream\n+                                    .seek(groupedReader.index\n+                                            .getIndexEntryForEntry(groupedReader.ledgerId, nextExpectedId)\n+                                            .getDataOffset());\n+                            continue;\n+                        } else if (entryId < nextExpectedId\n+                                && !groupedReader.index.getIndexEntryForEntry(groupedReader.ledgerId, nextExpectedId)\n+                                .equals(\n+                                        groupedReader.index.getIndexEntryForEntry(groupedReader.ledgerId, entryId))) {\n+                            groupedReader.inputStream\n+                                    .seek(groupedReader.index\n+                                            .getIndexEntryForEntry(groupedReader.ledgerId, nextExpectedId)\n+                                            .getDataOffset());\n+                            continue;\n+                        } else if (entryId > groupedReader.lastEntry) {\n+                            log.info(\"Expected to read {}, but read {}, which is greater than last entry {}\",\n+                                    nextExpectedId, entryId, groupedReader.lastEntry);\n+                            throw new BKException.BKUnexpectedConditionException();\n+                        } else {\n+                            val skipped = groupedReader.inputStream.skip(length);\n+                            log.info(\"Skipped {} bytes.\", skipped);", "originalCommit": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzcxNTMxMg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557715312", "bodyText": "PositionImpl is an implementation. Should this be Position?", "author": "sijie", "createdAt": "2021-01-14T21:39:00Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -22,19 +22,115 @@\n import java.util.Map;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n-\n+import lombok.ToString;\n import org.apache.bookkeeper.client.api.ReadHandle;\n import org.apache.bookkeeper.common.annotation.InterfaceAudience;\n import org.apache.bookkeeper.common.annotation.InterfaceStability;\n+import org.apache.bookkeeper.mledger.ManagedLedgerException.OffloadNotConsecutiveException;\n+import org.apache.bookkeeper.mledger.ManagedLedgerException.OffloadSegmentClosedException;\n+import org.apache.bookkeeper.mledger.impl.EntryImpl;\n+import org.apache.bookkeeper.mledger.impl.PositionImpl;\n+import org.apache.bookkeeper.mledger.proto.MLDataFormats;\n import org.apache.pulsar.common.policies.data.OffloadPolicies;\n \n /**\n- * Interface for offloading ledgers to long-term storage\n+ * Interface for offloading ledgers to long-term storage.\n  */\n @InterfaceAudience.LimitedPrivate\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    interface SegmentInfo {\n+        boolean isClosed();\n+\n+        OffloadResult result();\n+    }\n+\n+    @ToString\n+    class SegmentInfoImpl implements SegmentInfo {\n+        public SegmentInfoImpl(UUID uuid, long beginLedger, long beginEntry, String driverName,\n+                               Map<String, String> driverMetadata) {\n+            this.uuid = uuid;\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.driverName = driverName;\n+            this.driverMetadata = driverMetadata;\n+        }\n+\n+\n+        public final UUID uuid;\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final String driverName;\n+        volatile private long endLedger;\n+        volatile private long endEntry;\n+        volatile boolean closed = false;\n+        public final Map<String, String> driverMetadata;\n+\n+        public boolean isClosed() {\n+            return closed;\n+        }\n+\n+        public void closeSegment(long endLedger, long endEntry) {\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+            this.closed = true;\n+        }\n+\n+        public OffloadResult result() {\n+            return new OffloadResult(beginLedger, beginEntry, endLedger, endEntry);\n+        }\n+    }\n+\n+\n+    class OffloadResult {\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final long endLedger;\n+        public final long endEntry;\n+\n+        public OffloadResult(long beginLedger, long beginEntry, long endLedger, long endEntry) {\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+        }\n+    }\n+\n+    /**\n+     * Used to store driver info, buffer entries, mark progress, etc.\n+     * Create one per second.\n+     */\n+    interface OffloadHandle {\n+\n+        /**\n+         * return true when both buffer have enough size and ledger/entry id is next to the current one.\n+         * @param size\n+         * @return\n+         */\n+        boolean canOffer(long size);\n+\n+        default CompletableFuture<Boolean> asyncCanOffer(long size) {\n+            return CompletableFuture.completedFuture(canOffer(size));\n+        }\n+\n+        PositionImpl lastOffered();", "originalCommit": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzkyMTQ2Ng==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557921466", "bodyText": "Position exposes too little methods, I'm not sure if I should add some methods to it.\n\n  \n    \n      pulsar/managed-ledger/src/main/java/org/apache/bookkeeper/mledger/Position.java\n    \n    \n        Lines 29 to 37\n      in\n      3b2c852\n    \n    \n    \n    \n\n        \n          \n           public interface Position { \n        \n\n        \n          \n               /** \n        \n\n        \n          \n                * Get the position of the entry next to this one. The returned position might point to a non-existing, or not-yet \n        \n\n        \n          \n                * existing entry \n        \n\n        \n          \n                * \n        \n\n        \n          \n                * @return the position of the next logical entry \n        \n\n        \n          \n                */ \n        \n\n        \n          \n               Position getNext(); \n        \n\n        \n          \n           }", "author": "Renkai", "createdAt": "2021-01-15T07:00:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzcxNTMxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTk5MTA3MQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r559991071", "bodyText": "From the interface perspective, we should use Position", "author": "codelipenghui", "createdAt": "2021-01-19T08:26:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzcxNTMxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzcxNTkzNg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557715936", "bodyText": "Should this be EntryImpl or Entry?", "author": "sijie", "createdAt": "2021-01-14T21:40:16Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -22,19 +22,115 @@\n import java.util.Map;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n-\n+import lombok.ToString;\n import org.apache.bookkeeper.client.api.ReadHandle;\n import org.apache.bookkeeper.common.annotation.InterfaceAudience;\n import org.apache.bookkeeper.common.annotation.InterfaceStability;\n+import org.apache.bookkeeper.mledger.ManagedLedgerException.OffloadNotConsecutiveException;\n+import org.apache.bookkeeper.mledger.ManagedLedgerException.OffloadSegmentClosedException;\n+import org.apache.bookkeeper.mledger.impl.EntryImpl;\n+import org.apache.bookkeeper.mledger.impl.PositionImpl;\n+import org.apache.bookkeeper.mledger.proto.MLDataFormats;\n import org.apache.pulsar.common.policies.data.OffloadPolicies;\n \n /**\n- * Interface for offloading ledgers to long-term storage\n+ * Interface for offloading ledgers to long-term storage.\n  */\n @InterfaceAudience.LimitedPrivate\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    interface SegmentInfo {\n+        boolean isClosed();\n+\n+        OffloadResult result();\n+    }\n+\n+    @ToString\n+    class SegmentInfoImpl implements SegmentInfo {\n+        public SegmentInfoImpl(UUID uuid, long beginLedger, long beginEntry, String driverName,\n+                               Map<String, String> driverMetadata) {\n+            this.uuid = uuid;\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.driverName = driverName;\n+            this.driverMetadata = driverMetadata;\n+        }\n+\n+\n+        public final UUID uuid;\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final String driverName;\n+        volatile private long endLedger;\n+        volatile private long endEntry;\n+        volatile boolean closed = false;\n+        public final Map<String, String> driverMetadata;\n+\n+        public boolean isClosed() {\n+            return closed;\n+        }\n+\n+        public void closeSegment(long endLedger, long endEntry) {\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+            this.closed = true;\n+        }\n+\n+        public OffloadResult result() {\n+            return new OffloadResult(beginLedger, beginEntry, endLedger, endEntry);\n+        }\n+    }\n+\n+\n+    class OffloadResult {\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final long endLedger;\n+        public final long endEntry;\n+\n+        public OffloadResult(long beginLedger, long beginEntry, long endLedger, long endEntry) {\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+        }\n+    }\n+\n+    /**\n+     * Used to store driver info, buffer entries, mark progress, etc.\n+     * Create one per second.\n+     */\n+    interface OffloadHandle {\n+\n+        /**\n+         * return true when both buffer have enough size and ledger/entry id is next to the current one.\n+         * @param size\n+         * @return\n+         */\n+        boolean canOffer(long size);\n+\n+        default CompletableFuture<Boolean> asyncCanOffer(long size) {\n+            return CompletableFuture.completedFuture(canOffer(size));\n+        }\n+\n+        PositionImpl lastOffered();\n+\n+        default CompletableFuture<PositionImpl> asyncLastOffered() {\n+            return CompletableFuture.completedFuture(lastOffered());\n+        }\n+\n+        boolean offerEntry(EntryImpl entry) throws OffloadSegmentClosedException,", "originalCommit": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzcxNzA2MA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r557717060", "bodyText": "Don't we need a close method to seal the offloaded segment?", "author": "sijie", "createdAt": "2021-01-14T21:42:34Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -22,19 +22,115 @@\n import java.util.Map;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n-\n+import lombok.ToString;\n import org.apache.bookkeeper.client.api.ReadHandle;\n import org.apache.bookkeeper.common.annotation.InterfaceAudience;\n import org.apache.bookkeeper.common.annotation.InterfaceStability;\n+import org.apache.bookkeeper.mledger.ManagedLedgerException.OffloadNotConsecutiveException;\n+import org.apache.bookkeeper.mledger.ManagedLedgerException.OffloadSegmentClosedException;\n+import org.apache.bookkeeper.mledger.impl.EntryImpl;\n+import org.apache.bookkeeper.mledger.impl.PositionImpl;\n+import org.apache.bookkeeper.mledger.proto.MLDataFormats;\n import org.apache.pulsar.common.policies.data.OffloadPolicies;\n \n /**\n- * Interface for offloading ledgers to long-term storage\n+ * Interface for offloading ledgers to long-term storage.\n  */\n @InterfaceAudience.LimitedPrivate\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    interface SegmentInfo {\n+        boolean isClosed();\n+\n+        OffloadResult result();\n+    }\n+\n+    @ToString\n+    class SegmentInfoImpl implements SegmentInfo {\n+        public SegmentInfoImpl(UUID uuid, long beginLedger, long beginEntry, String driverName,\n+                               Map<String, String> driverMetadata) {\n+            this.uuid = uuid;\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.driverName = driverName;\n+            this.driverMetadata = driverMetadata;\n+        }\n+\n+\n+        public final UUID uuid;\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final String driverName;\n+        volatile private long endLedger;\n+        volatile private long endEntry;\n+        volatile boolean closed = false;\n+        public final Map<String, String> driverMetadata;\n+\n+        public boolean isClosed() {\n+            return closed;\n+        }\n+\n+        public void closeSegment(long endLedger, long endEntry) {\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+            this.closed = true;\n+        }\n+\n+        public OffloadResult result() {\n+            return new OffloadResult(beginLedger, beginEntry, endLedger, endEntry);\n+        }\n+    }\n+\n+\n+    class OffloadResult {\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final long endLedger;\n+        public final long endEntry;\n+\n+        public OffloadResult(long beginLedger, long beginEntry, long endLedger, long endEntry) {\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+        }\n+    }\n+\n+    /**\n+     * Used to store driver info, buffer entries, mark progress, etc.\n+     * Create one per second.\n+     */\n+    interface OffloadHandle {", "originalCommit": "d3a6a8c2744733d00e6dfa30c0cac2f06eb1386a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODEwNDM0MQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r558104341", "bodyText": "close method added", "author": "Renkai", "createdAt": "2021-01-15T09:17:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzcxNzA2MA=="}], "type": "inlineReview"}, {"oid": "bb0c55b637e9622ed22306b9236d8a874c03a187", "url": "https://github.com/apache/pulsar/commit/bb0c55b637e9622ed22306b9236d8a874c03a187", "message": "polish offerEntry method\nadd async close\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-15T09:16:53Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTk4NTIwNQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r559985205", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    default CompletableFuture<Boolean> asyncCanOffer(long size) {\n          \n          \n            \n                    default CompletableFuture<Boolean> canOfferAsync(long size) {", "author": "codelipenghui", "createdAt": "2021-01-19T08:16:11Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -22,19 +22,92 @@\n import java.util.Map;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n-\n import org.apache.bookkeeper.client.api.ReadHandle;\n import org.apache.bookkeeper.common.annotation.InterfaceAudience;\n import org.apache.bookkeeper.common.annotation.InterfaceStability;\n+import org.apache.bookkeeper.mledger.impl.EntryImpl;\n+import org.apache.bookkeeper.mledger.impl.PositionImpl;\n+import org.apache.bookkeeper.mledger.proto.MLDataFormats;\n import org.apache.pulsar.common.policies.data.OffloadPolicies;\n \n /**\n- * Interface for offloading ledgers to long-term storage\n+ * Interface for offloading ledgers to long-term storage.\n  */\n @InterfaceAudience.LimitedPrivate\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    interface SegmentInfo {\n+        boolean isClosed();\n+\n+        OffloadResult result();\n+    }\n+\n+    class OffloadResult {\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final long endLedger;\n+        public final long endEntry;\n+\n+        public OffloadResult(long beginLedger, long beginEntry, long endLedger, long endEntry) {\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+        }\n+    }\n+\n+    /**\n+     * Used to store driver info, buffer entries, mark progress, etc.\n+     * Create one per segment.\n+     */\n+    interface OffloadHandle {\n+        enum OfferEntryResult {\n+            SUCCESS,\n+            FAIL_BUFFER_FULL,\n+            FAIL_SEGMENT_CLOSED,\n+            FAIL_NOT_CONSECUTIVE\n+        }\n+\n+        /**\n+         * return true when both buffer have enough size and ledger/entry id is next to the current one.\n+         * @param size\n+         * @return\n+         */\n+        boolean canOffer(long size);\n+\n+        default CompletableFuture<Boolean> asyncCanOffer(long size) {", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTk5MzM3OA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r559993378", "bodyText": "same as the above comment, you can remove the default implementation from the interface.", "author": "codelipenghui", "createdAt": "2021-01-19T08:30:08Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -22,19 +22,92 @@\n import java.util.Map;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n-\n import org.apache.bookkeeper.client.api.ReadHandle;\n import org.apache.bookkeeper.common.annotation.InterfaceAudience;\n import org.apache.bookkeeper.common.annotation.InterfaceStability;\n+import org.apache.bookkeeper.mledger.impl.EntryImpl;\n+import org.apache.bookkeeper.mledger.impl.PositionImpl;\n+import org.apache.bookkeeper.mledger.proto.MLDataFormats;\n import org.apache.pulsar.common.policies.data.OffloadPolicies;\n \n /**\n- * Interface for offloading ledgers to long-term storage\n+ * Interface for offloading ledgers to long-term storage.\n  */\n @InterfaceAudience.LimitedPrivate\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    interface SegmentInfo {\n+        boolean isClosed();\n+\n+        OffloadResult result();\n+    }\n+\n+    class OffloadResult {\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final long endLedger;\n+        public final long endEntry;\n+\n+        public OffloadResult(long beginLedger, long beginEntry, long endLedger, long endEntry) {\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+        }\n+    }\n+\n+    /**\n+     * Used to store driver info, buffer entries, mark progress, etc.\n+     * Create one per segment.\n+     */\n+    interface OffloadHandle {\n+        enum OfferEntryResult {\n+            SUCCESS,\n+            FAIL_BUFFER_FULL,\n+            FAIL_SEGMENT_CLOSED,\n+            FAIL_NOT_CONSECUTIVE\n+        }\n+\n+        /**\n+         * return true when both buffer have enough size and ledger/entry id is next to the current one.\n+         * @param size\n+         * @return\n+         */\n+        boolean canOffer(long size);\n+\n+        default CompletableFuture<Boolean> asyncCanOffer(long size) {\n+            return CompletableFuture.completedFuture(canOffer(size));\n+        }\n+\n+        PositionImpl lastOffered();\n+\n+        default CompletableFuture<PositionImpl> asyncLastOffered() {\n+            return CompletableFuture.completedFuture(lastOffered());", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTk5MzUxNQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r559993515", "bodyText": "Same as above comment.", "author": "codelipenghui", "createdAt": "2021-01-19T08:30:22Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -22,19 +22,92 @@\n import java.util.Map;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n-\n import org.apache.bookkeeper.client.api.ReadHandle;\n import org.apache.bookkeeper.common.annotation.InterfaceAudience;\n import org.apache.bookkeeper.common.annotation.InterfaceStability;\n+import org.apache.bookkeeper.mledger.impl.EntryImpl;\n+import org.apache.bookkeeper.mledger.impl.PositionImpl;\n+import org.apache.bookkeeper.mledger.proto.MLDataFormats;\n import org.apache.pulsar.common.policies.data.OffloadPolicies;\n \n /**\n- * Interface for offloading ledgers to long-term storage\n+ * Interface for offloading ledgers to long-term storage.\n  */\n @InterfaceAudience.LimitedPrivate\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    interface SegmentInfo {\n+        boolean isClosed();\n+\n+        OffloadResult result();\n+    }\n+\n+    class OffloadResult {\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final long endLedger;\n+        public final long endEntry;\n+\n+        public OffloadResult(long beginLedger, long beginEntry, long endLedger, long endEntry) {\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+        }\n+    }\n+\n+    /**\n+     * Used to store driver info, buffer entries, mark progress, etc.\n+     * Create one per segment.\n+     */\n+    interface OffloadHandle {\n+        enum OfferEntryResult {\n+            SUCCESS,\n+            FAIL_BUFFER_FULL,\n+            FAIL_SEGMENT_CLOSED,\n+            FAIL_NOT_CONSECUTIVE\n+        }\n+\n+        /**\n+         * return true when both buffer have enough size and ledger/entry id is next to the current one.\n+         * @param size\n+         * @return\n+         */\n+        boolean canOffer(long size);\n+\n+        default CompletableFuture<Boolean> asyncCanOffer(long size) {\n+            return CompletableFuture.completedFuture(canOffer(size));\n+        }\n+\n+        PositionImpl lastOffered();\n+\n+        default CompletableFuture<PositionImpl> asyncLastOffered() {\n+            return CompletableFuture.completedFuture(lastOffered());\n+        }\n+\n+        /**\n+         * The caller should manually release entry no matter what the offer result is.\n+         */\n+        OfferEntryResult offerEntry(Entry entry);\n+\n+        default CompletableFuture<OfferEntryResult> asyncOfferEntry(EntryImpl entry) {\n+            return CompletableFuture.completedFuture(offerEntry(entry));", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTk5MzY5Mw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r559993693", "bodyText": "Same as the above comment", "author": "codelipenghui", "createdAt": "2021-01-19T08:30:43Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -22,19 +22,92 @@\n import java.util.Map;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n-\n import org.apache.bookkeeper.client.api.ReadHandle;\n import org.apache.bookkeeper.common.annotation.InterfaceAudience;\n import org.apache.bookkeeper.common.annotation.InterfaceStability;\n+import org.apache.bookkeeper.mledger.impl.EntryImpl;\n+import org.apache.bookkeeper.mledger.impl.PositionImpl;\n+import org.apache.bookkeeper.mledger.proto.MLDataFormats;\n import org.apache.pulsar.common.policies.data.OffloadPolicies;\n \n /**\n- * Interface for offloading ledgers to long-term storage\n+ * Interface for offloading ledgers to long-term storage.\n  */\n @InterfaceAudience.LimitedPrivate\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    interface SegmentInfo {\n+        boolean isClosed();\n+\n+        OffloadResult result();\n+    }\n+\n+    class OffloadResult {\n+        public final long beginLedger;\n+        public final long beginEntry;\n+        public final long endLedger;\n+        public final long endEntry;\n+\n+        public OffloadResult(long beginLedger, long beginEntry, long endLedger, long endEntry) {\n+            this.beginLedger = beginLedger;\n+            this.beginEntry = beginEntry;\n+            this.endLedger = endLedger;\n+            this.endEntry = endEntry;\n+        }\n+    }\n+\n+    /**\n+     * Used to store driver info, buffer entries, mark progress, etc.\n+     * Create one per segment.\n+     */\n+    interface OffloadHandle {\n+        enum OfferEntryResult {\n+            SUCCESS,\n+            FAIL_BUFFER_FULL,\n+            FAIL_SEGMENT_CLOSED,\n+            FAIL_NOT_CONSECUTIVE\n+        }\n+\n+        /**\n+         * return true when both buffer have enough size and ledger/entry id is next to the current one.\n+         * @param size\n+         * @return\n+         */\n+        boolean canOffer(long size);\n+\n+        default CompletableFuture<Boolean> asyncCanOffer(long size) {\n+            return CompletableFuture.completedFuture(canOffer(size));\n+        }\n+\n+        PositionImpl lastOffered();\n+\n+        default CompletableFuture<PositionImpl> asyncLastOffered() {\n+            return CompletableFuture.completedFuture(lastOffered());\n+        }\n+\n+        /**\n+         * The caller should manually release entry no matter what the offer result is.\n+         */\n+        OfferEntryResult offerEntry(Entry entry);\n+\n+        default CompletableFuture<OfferEntryResult> asyncOfferEntry(EntryImpl entry) {\n+            return CompletableFuture.completedFuture(offerEntry(entry));\n+        }\n+\n+        CompletableFuture<OffloadResult> getOffloadResultAsync();\n+\n+        /**\n+         * Manually close current offloading segment\n+         * @return true if the segment is not already closed\n+         */\n+        boolean close();\n+\n+        default CompletableFuture<Boolean> AsyncClose() {\n+            return CompletableFuture.completedFuture(close());", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAwMzU5OQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560003599", "bodyText": "Do we need this exception? As I mentioned before, there are some markers in the managed ledger, this will affect the continuity of entry id when offload data to the tiered storage", "author": "codelipenghui", "createdAt": "2021-01-19T08:46:57Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/ManagedLedgerException.java", "diffHunk": "@@ -151,6 +151,18 @@ public OffloadInProgressException(String msg) {\n         }\n     }\n \n+    public static class OffloadSegmentClosedException extends ManagedLedgerException {\n+        public OffloadSegmentClosedException(String msg) {\n+            super(msg);\n+        }\n+    }\n+\n+    public static class OffloadNotConsecutiveException extends ManagedLedgerException {", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAwNTU0NA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560005544", "bodyText": "Is it better to named getLedgerInfo? and does not distinguish whether it is closed. The closed state should contains in the LedgerInfo(Now I think we use the entries=0 to determine the ledger close or not)", "author": "codelipenghui", "createdAt": "2021-01-19T08:49:52Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/ManagedLedger.java", "diffHunk": "@@ -595,4 +594,10 @@ void asyncSetProperties(Map<String, String> properties, final AsyncCallbacks.Upd\n      * Get the ManagedLedgerInterceptor for ManagedLedger.\n      * */\n     ManagedLedgerInterceptor getManagedLedgerInterceptor();\n+\n+    /**\n+     * Get basic ledger summary after the ledger is closed.\n+     * will got exception if corresponding ledger was not closed when the method called.\n+     */\n+    CompletableFuture<LedgerInfo> getClosedLedgerInfo(long ledgerId);", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDE3MDE3MQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560170171", "bodyText": "Is it better to named getLedgerInfo? and does not distinguish whether it is closed. The closed state should contains in the LedgerInfo(Now I think we use the entries=0 to determine the ledger close or not)\n\nMy design is refuse to return an unclosed result to prevent caller to get an incomplete result in accident.", "author": "Renkai", "createdAt": "2021-01-19T13:20:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAwNTU0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAwOTg5NQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560009895", "bodyText": "It's better to define the result as CompletableFuture<Optional>? If use exception, it's better to define a specific exception because we need a way to determine if the exception is ledger not found exception? Use ManagedLedgerException to determine if the ledger is existed or not seem a bit confuse.", "author": "codelipenghui", "createdAt": "2021-01-19T08:56:20Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/ManagedLedgerImpl.java", "diffHunk": "@@ -1685,6 +1686,23 @@ void asyncReadEntries(OpReadEntry opReadEntry) {\n         return getLedgerHandle(ledgerId).thenApply(rh -> rh.getLedgerMetadata().toSafeString());\n     }\n \n+    @Override\n+    public CompletableFuture<LedgerInfo> getClosedLedgerInfo(long ledgerId) {\n+        CompletableFuture<LedgerInfo> result = new CompletableFuture<>();\n+        final LedgerInfo ledgerInfo = ledgers.get(ledgerId);\n+        if (ledgerInfo == null) {\n+            final ManagedLedgerException exception = new ManagedLedgerException(\n+                    Strings.lenientFormat(\"ledger with id %s not found\", ledgerId));\n+            result.completeExceptionally(exception);", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAxMzQ2OA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560013468", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public final long beginLedger;\n          \n          \n            \n                public final long beginLedgerId;\n          \n      \n    \n    \n  \n\nPlease check all.", "author": "codelipenghui", "createdAt": "2021-01-19T09:01:49Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/SegmentInfoImpl.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.impl;\n+\n+\n+import java.util.Map;\n+import java.util.UUID;\n+import lombok.ToString;\n+import org.apache.bookkeeper.mledger.LedgerOffloader;\n+\n+@ToString\n+public class SegmentInfoImpl implements LedgerOffloader.SegmentInfo {\n+    public SegmentInfoImpl(UUID uuid, long beginLedger, long beginEntry, String driverName,\n+                           Map<String, String> driverMetadata) {\n+        this.uuid = uuid;\n+        this.beginLedger = beginLedger;\n+        this.beginEntry = beginEntry;\n+        this.driverName = driverName;\n+        this.driverMetadata = driverMetadata;\n+    }\n+\n+\n+    public final UUID uuid;\n+    public final long beginLedger;", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAxNDAxMw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560014013", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class SegmentInfoImpl implements LedgerOffloader.SegmentInfo {\n          \n          \n            \n            public class OffloadSegmentInfoImpl implements LedgerOffloader.SegmentInfo {", "author": "codelipenghui", "createdAt": "2021-01-19T09:02:43Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/SegmentInfoImpl.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.impl;\n+\n+\n+import java.util.Map;\n+import java.util.UUID;\n+import lombok.ToString;\n+import org.apache.bookkeeper.mledger.LedgerOffloader;\n+\n+@ToString\n+public class SegmentInfoImpl implements LedgerOffloader.SegmentInfo {", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAyNTIzNA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560025234", "bodyText": "Is it used for managed ledger or any other component? If it only for offload internal, we don't need to expose it.", "author": "codelipenghui", "createdAt": "2021-01-19T09:19:32Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -22,19 +22,92 @@\n import java.util.Map;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n-\n import org.apache.bookkeeper.client.api.ReadHandle;\n import org.apache.bookkeeper.common.annotation.InterfaceAudience;\n import org.apache.bookkeeper.common.annotation.InterfaceStability;\n+import org.apache.bookkeeper.mledger.impl.EntryImpl;\n+import org.apache.bookkeeper.mledger.impl.PositionImpl;\n+import org.apache.bookkeeper.mledger.proto.MLDataFormats;\n import org.apache.pulsar.common.policies.data.OffloadPolicies;\n \n /**\n- * Interface for offloading ledgers to long-term storage\n+ * Interface for offloading ledgers to long-term storage.\n  */\n @InterfaceAudience.LimitedPrivate\n @InterfaceStability.Evolving\n public interface LedgerOffloader {\n \n+    interface SegmentInfo {", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDE4OTQxNA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560189414", "bodyText": "Is it used for managed ledger or any other component? If it only for offload internal, we don't need to expose it.\n\nWill be used in managed ledger", "author": "Renkai", "createdAt": "2021-01-19T13:51:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAyNTIzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDE4OTc3NQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560189775", "bodyText": "Is it used for managed ledger or any other component? If it only for offload internal, we don't need to expose it.\n\nWill be used in managed ledger", "author": "Renkai", "createdAt": "2021-01-19T13:52:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAyNTIzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDI3OTI2Nw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560279267", "bodyText": "Could you provide more information about how the managed ledger uses this interface? in my opinion, the ledger offloader only create a ledgerHandle then offer entry by the ledgerHandle and get the offload result. And I think the PIP https://github.com/apache/pulsar/wiki/PIP-76%3A-Streaming-Offload also does not mention this interface. Does the offload result can't work here?", "author": "codelipenghui", "createdAt": "2021-01-19T15:49:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAyNTIzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDU5NDA4OQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560594089", "bodyText": "Could you provide more information about how the managed ledger uses this interface? in my opinion, the ledger offloader only create a ledgerHandle then offer entry by the ledgerHandle and get the offload result. And I think the PIP https://github.com/apache/pulsar/wiki/PIP-76%3A-Streaming-Offload also does not mention this interface. Does the offload result can't work here?\n\nIn my current implementation, the managed ledger will keep a reference of the SegmentInfo, and once the offload got result, it will check if the result is the same with the kept one.", "author": "Renkai", "createdAt": "2021-01-20T00:36:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAyNTIzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAyNjQyOQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560026429", "bodyText": "Any reason to modify this method?", "author": "codelipenghui", "createdAt": "2021-01-19T09:21:19Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/OffloadIndexBlockBuilder.java", "diffHunk": "@@ -71,7 +71,7 @@\n     /**\n      * Construct OffloadIndex from an InputStream.\n      */\n-    OffloadIndexBlock fromStream(InputStream is) throws IOException;\n+    OffloadIndexBlock indexFromStream(InputStream is) throws IOException;", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDE5MjM5OA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560192398", "bodyText": "Because now we have two different method with similar behavior, it's better to have a distinguish\n   @Override\n    public OffloadIndexBlock indexFromStream(InputStream is) throws IOException {\n        return OffloadIndexBlockImpl.get(is);\n    }\n\n    @Override\n    public StreamingOffloadIndexBlock streamingIndexFromStream(InputStream is) throws IOException {\n        return StreamingOffloadIndexBlockImpl.get(is);\n    }", "author": "Renkai", "createdAt": "2021-01-19T13:56:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAyNjQyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDI4MjAyMA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560282020", "bodyText": "Sorry, I can't get the point here. Do you mean the method name will duplicated?", "author": "codelipenghui", "createdAt": "2021-01-19T15:53:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAyNjQyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDU4NTI0OA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560585248", "bodyText": "Sorry, I can't get the point here. Do you mean the method name will duplicated?\n\nYes", "author": "Renkai", "createdAt": "2021-01-20T00:09:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAyNjQyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAzOTE5Mw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560039193", "bodyText": "It's not correct here, because might skip all messages of a ledger. As I mentioned before, it's hard to determine the Consecutive at the offloader. And the marker also will affect this behavior in the future.", "author": "codelipenghui", "createdAt": "2021-01-19T09:39:10Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java", "diffHunk": "@@ -230,6 +265,202 @@ public String getOffloadDriverName() {\n         return promise;\n     }\n \n+    BlobStore blobStore;\n+    String streamingDataBlockKey;\n+    String streamingDataIndexKey;\n+    MultipartUpload streamingMpu = null;\n+    List<MultipartPart> streamingParts = Lists.newArrayList();\n+\n+    @Override\n+    public CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                             long beginEntry,\n+                                                             Map<String, String> driverMetadata) {\n+        this.ml = ml;\n+        this.segmentInfo = new SegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(), driverMetadata);\n+        log.debug(\"begin offload with {}:{}\", beginLedger, beginEntry);\n+        this.offloadResult = new CompletableFuture<>();\n+        blobStore = blobStores.get(config.getBlobStoreLocation());\n+        streamingIndexBuilder = StreamingOffloadIndexBlockBuilder.create();\n+        streamingDataBlockKey = segmentInfo.uuid.toString();\n+        streamingDataIndexKey = String.format(\"%s-index\", segmentInfo.uuid);\n+        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);\n+        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);\n+        Blob blob = blobBuilder.build();\n+        streamingMpu = blobStore\n+                .initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());\n+\n+        scheduler.chooseThread(segmentInfo).execute(() -> {\n+            log.info(\"start offloading segment: {}\", segmentInfo);\n+            streamingOffloadLoop(1, 0);\n+        });\n+        scheduler.schedule(this::closeSegment, maxSegmentCloseTime.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        return CompletableFuture.completedFuture(new OffloadHandle() {\n+            @Override\n+            public boolean canOffer(long size) {\n+                return BlobStoreManagedLedgerOffloader.this.canOffer(size);\n+            }\n+\n+            @Override\n+            public PositionImpl lastOffered() {\n+                return BlobStoreManagedLedgerOffloader.this.lastOffered();\n+            }\n+\n+            @Override\n+            public OfferEntryResult offerEntry(Entry entry) {\n+                return BlobStoreManagedLedgerOffloader.this.offerEntry(entry);\n+            }\n+\n+            @Override\n+            public CompletableFuture<OffloadResult> getOffloadResultAsync() {\n+                return BlobStoreManagedLedgerOffloader.this.getOffloadResultAsync();\n+            }\n+\n+            @Override\n+            public boolean close() {\n+                return BlobStoreManagedLedgerOffloader.this.closeSegment();\n+            }\n+        });\n+    }\n+\n+    private void streamingOffloadLoop(int partId, int dataObjectLength) {\n+        log.debug(\"streaming offload loop {} {}\", partId, dataObjectLength);\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            offloadResult.complete(segmentInfo.result());\n+            return;\n+        }\n+        final BufferedOffloadStream payloadStream;\n+\n+        while (offloadBuffer.isEmpty()) {\n+            if (segmentInfo.isClosed()) {\n+                offloadResult.complete(segmentInfo.result());\n+            } else {\n+                try {\n+                    Thread.sleep(100);\n+                } catch (InterruptedException e) {\n+                    e.printStackTrace();\n+                }\n+            }\n+        }\n+        final Entry peek = offloadBuffer.peek();\n+        //initialize payload when there is at least one entry\n+        final long blockLedgerId = peek.getLedgerId();\n+        final long blockEntryId = peek.getEntryId();\n+        payloadStream = new BufferedOffloadStream(streamingBlockSize, offloadBuffer, segmentInfo,\n+                blockLedgerId, blockEntryId, bufferLength);\n+        try {\n+            streamingIndexBuilder.addLedgerMeta(blockLedgerId, ml.getClosedLedgerInfo(blockLedgerId).get());\n+            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());\n+        } catch (InterruptedException | ExecutionException e) {\n+            offloadResult.completeExceptionally(e);\n+            return;\n+        }\n+        log.debug(\"begin upload payload\");\n+        Payload partPayload = Payloads.newInputStreamPayload(payloadStream);\n+        partPayload.getContentMetadata().setContentType(\"application/octet-stream\");\n+        streamingParts.add(blobStore.uploadMultipartPart(streamingMpu, partId, partPayload));\n+        streamingIndexBuilder.addBlock(blockLedgerId, blockEntryId, partId, streamingBlockSize);\n+\n+        log.debug(\"UploadMultipartPart. container: {}, blobName: {}, partId: {}, mpu: {}\",\n+                config.getBucket(), streamingDataBlockKey, partId, streamingMpu.id());\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            log.debug(\"segment closed, buffer is empty\");\n+            try {\n+                blobStore.completeMultipartUpload(streamingMpu, streamingParts);\n+                streamingIndexBuilder.withDataObjectLength(dataObjectLength + streamingBlockSize);\n+                final StreamingOffloadIndexBlock index = streamingIndexBuilder.buildStreaming();\n+                final IndexInputStream indexStream = index.toStream();\n+                final BlobBuilder indexBlobBuilder = blobStore.blobBuilder(streamingDataIndexKey);\n+                DataBlockUtils.addVersionInfo(indexBlobBuilder, userMetadata);\n+                final InputStreamPayload indexPayLoad = Payloads.newInputStreamPayload(indexStream);\n+                indexPayLoad.getContentMetadata().setContentLength(indexStream.getStreamSize());\n+                indexPayLoad.getContentMetadata().setContentType(\"application/octet-stream\");\n+                final Blob indexBlob = indexBlobBuilder.payload(indexPayLoad)\n+                        .contentLength(indexStream.getStreamSize())\n+                        .build();\n+                blobStore.putBlob(config.getBucket(), indexBlob);\n+\n+                offloadResult.complete(segmentInfo.result());\n+            } catch (Exception e) {\n+                log.error(\"streaming offload failed\", e);\n+                offloadResult.completeExceptionally(e);\n+            }\n+            log.debug(\"offload done\");\n+        } else {\n+            log.debug(\"continue offload loop\");\n+            scheduler.chooseThread(segmentInfo)\n+                    .execute(() -> streamingOffloadLoop(partId + 1, dataObjectLength + streamingBlockSize));\n+        }\n+    }\n+\n+    private CompletableFuture<OffloadResult> getOffloadResultAsync() {\n+        return this.offloadResult;\n+    }\n+\n+    private synchronized OfferEntryResult offerEntry(Entry entry) {\n+\n+        if (segmentInfo.isClosed()) {\n+            log.debug(\"Segment already closed {}\", segmentInfo);\n+            return OfferEntryResult.FAIL_SEGMENT_CLOSED;\n+        } else if (maxBufferLength >= bufferLength.get() + entry.getLength()\n+                //if single message size larger than full buffer size, then ok to offer when buffer is empty\n+                && !(entry.getLength() > maxBufferLength && offloadBuffer.isEmpty())) {\n+            return OfferEntryResult.FAIL_BUFFER_FULL;\n+        } else {\n+            if (!naiveCheckConsecutive(lastOfferedPosition,\n+                    PositionImpl.get(entry.getLedgerId(), entry.getEntryId()))) {\n+                log.error(\"position {} and {} are not consecutive\", lastOfferedPosition,\n+                        entry.getPosition());\n+                return OfferEntryResult.FAIL_NOT_CONSECUTIVE;\n+            }\n+            final EntryImpl entryImpl = EntryImpl\n+                    .create(entry.getLedgerId(), entry.getEntryId(), entry.getDataBuffer());\n+            offloadBuffer.add(entryImpl);\n+            bufferLength.getAndAdd(entryImpl.getLength());\n+            segmentLength.getAndAdd(entryImpl.getLength());\n+            lastOfferedPosition = entryImpl.getPosition();\n+            if (segmentLength.get() >= maxSegmentLength\n+                    && System.currentTimeMillis() - segmentBeginTimeMillis >= minSegmentCloseTimeMillis) {\n+                closeSegment();\n+            }\n+            return OfferEntryResult.SUCCESS;\n+        }\n+    }\n+\n+    private synchronized boolean closeSegment() {\n+        final boolean result = !segmentInfo.isClosed();\n+        log.debug(\"close segment {} {}\", lastOfferedPosition.getLedgerId(), lastOfferedPosition.getEntryId());\n+        this.segmentInfo.closeSegment(lastOfferedPosition.getLedgerId(), lastOfferedPosition.getEntryId());\n+        return result;\n+    }\n+\n+    private static boolean naiveCheckConsecutive(PositionImpl lastOfferedPosition, PositionImpl offeringPosition) {\n+        if (offeringPosition.getLedgerId() == lastOfferedPosition.getLedgerId()\n+                && offeringPosition.getEntryId() == lastOfferedPosition.getEntryId() + 1) {\n+            return true;\n+        } else if (offeringPosition.getEntryId() == 0) {", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDI0ODgwOA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560248808", "bodyText": "consecutive check removed", "author": "Renkai", "createdAt": "2021-01-19T15:10:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDAzOTE5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA0MDM3Ng==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560040376", "bodyText": "If the segment closed return false, the buffer fills up return false, how to determine at the managed ledger?", "author": "codelipenghui", "createdAt": "2021-01-19T09:40:49Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java", "diffHunk": "@@ -230,6 +265,202 @@ public String getOffloadDriverName() {\n         return promise;\n     }\n \n+    BlobStore blobStore;\n+    String streamingDataBlockKey;\n+    String streamingDataIndexKey;\n+    MultipartUpload streamingMpu = null;\n+    List<MultipartPart> streamingParts = Lists.newArrayList();\n+\n+    @Override\n+    public CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                             long beginEntry,\n+                                                             Map<String, String> driverMetadata) {\n+        this.ml = ml;\n+        this.segmentInfo = new SegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(), driverMetadata);\n+        log.debug(\"begin offload with {}:{}\", beginLedger, beginEntry);\n+        this.offloadResult = new CompletableFuture<>();\n+        blobStore = blobStores.get(config.getBlobStoreLocation());\n+        streamingIndexBuilder = StreamingOffloadIndexBlockBuilder.create();\n+        streamingDataBlockKey = segmentInfo.uuid.toString();\n+        streamingDataIndexKey = String.format(\"%s-index\", segmentInfo.uuid);\n+        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);\n+        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);\n+        Blob blob = blobBuilder.build();\n+        streamingMpu = blobStore\n+                .initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());\n+\n+        scheduler.chooseThread(segmentInfo).execute(() -> {\n+            log.info(\"start offloading segment: {}\", segmentInfo);\n+            streamingOffloadLoop(1, 0);\n+        });\n+        scheduler.schedule(this::closeSegment, maxSegmentCloseTime.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        return CompletableFuture.completedFuture(new OffloadHandle() {\n+            @Override\n+            public boolean canOffer(long size) {\n+                return BlobStoreManagedLedgerOffloader.this.canOffer(size);\n+            }\n+\n+            @Override\n+            public PositionImpl lastOffered() {\n+                return BlobStoreManagedLedgerOffloader.this.lastOffered();\n+            }\n+\n+            @Override\n+            public OfferEntryResult offerEntry(Entry entry) {\n+                return BlobStoreManagedLedgerOffloader.this.offerEntry(entry);\n+            }\n+\n+            @Override\n+            public CompletableFuture<OffloadResult> getOffloadResultAsync() {\n+                return BlobStoreManagedLedgerOffloader.this.getOffloadResultAsync();\n+            }\n+\n+            @Override\n+            public boolean close() {\n+                return BlobStoreManagedLedgerOffloader.this.closeSegment();\n+            }\n+        });\n+    }\n+\n+    private void streamingOffloadLoop(int partId, int dataObjectLength) {\n+        log.debug(\"streaming offload loop {} {}\", partId, dataObjectLength);\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            offloadResult.complete(segmentInfo.result());\n+            return;\n+        }\n+        final BufferedOffloadStream payloadStream;\n+\n+        while (offloadBuffer.isEmpty()) {\n+            if (segmentInfo.isClosed()) {\n+                offloadResult.complete(segmentInfo.result());\n+            } else {\n+                try {\n+                    Thread.sleep(100);\n+                } catch (InterruptedException e) {\n+                    e.printStackTrace();\n+                }\n+            }\n+        }\n+        final Entry peek = offloadBuffer.peek();\n+        //initialize payload when there is at least one entry\n+        final long blockLedgerId = peek.getLedgerId();\n+        final long blockEntryId = peek.getEntryId();\n+        payloadStream = new BufferedOffloadStream(streamingBlockSize, offloadBuffer, segmentInfo,\n+                blockLedgerId, blockEntryId, bufferLength);\n+        try {\n+            streamingIndexBuilder.addLedgerMeta(blockLedgerId, ml.getClosedLedgerInfo(blockLedgerId).get());\n+            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());\n+        } catch (InterruptedException | ExecutionException e) {\n+            offloadResult.completeExceptionally(e);\n+            return;\n+        }\n+        log.debug(\"begin upload payload\");\n+        Payload partPayload = Payloads.newInputStreamPayload(payloadStream);\n+        partPayload.getContentMetadata().setContentType(\"application/octet-stream\");\n+        streamingParts.add(blobStore.uploadMultipartPart(streamingMpu, partId, partPayload));\n+        streamingIndexBuilder.addBlock(blockLedgerId, blockEntryId, partId, streamingBlockSize);\n+\n+        log.debug(\"UploadMultipartPart. container: {}, blobName: {}, partId: {}, mpu: {}\",\n+                config.getBucket(), streamingDataBlockKey, partId, streamingMpu.id());\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            log.debug(\"segment closed, buffer is empty\");\n+            try {\n+                blobStore.completeMultipartUpload(streamingMpu, streamingParts);\n+                streamingIndexBuilder.withDataObjectLength(dataObjectLength + streamingBlockSize);\n+                final StreamingOffloadIndexBlock index = streamingIndexBuilder.buildStreaming();\n+                final IndexInputStream indexStream = index.toStream();\n+                final BlobBuilder indexBlobBuilder = blobStore.blobBuilder(streamingDataIndexKey);\n+                DataBlockUtils.addVersionInfo(indexBlobBuilder, userMetadata);\n+                final InputStreamPayload indexPayLoad = Payloads.newInputStreamPayload(indexStream);\n+                indexPayLoad.getContentMetadata().setContentLength(indexStream.getStreamSize());\n+                indexPayLoad.getContentMetadata().setContentType(\"application/octet-stream\");\n+                final Blob indexBlob = indexBlobBuilder.payload(indexPayLoad)\n+                        .contentLength(indexStream.getStreamSize())\n+                        .build();\n+                blobStore.putBlob(config.getBucket(), indexBlob);\n+\n+                offloadResult.complete(segmentInfo.result());\n+            } catch (Exception e) {\n+                log.error(\"streaming offload failed\", e);\n+                offloadResult.completeExceptionally(e);\n+            }\n+            log.debug(\"offload done\");\n+        } else {\n+            log.debug(\"continue offload loop\");\n+            scheduler.chooseThread(segmentInfo)\n+                    .execute(() -> streamingOffloadLoop(partId + 1, dataObjectLength + streamingBlockSize));\n+        }\n+    }\n+\n+    private CompletableFuture<OffloadResult> getOffloadResultAsync() {\n+        return this.offloadResult;\n+    }\n+\n+    private synchronized OfferEntryResult offerEntry(Entry entry) {\n+\n+        if (segmentInfo.isClosed()) {\n+            log.debug(\"Segment already closed {}\", segmentInfo);\n+            return OfferEntryResult.FAIL_SEGMENT_CLOSED;\n+        } else if (maxBufferLength >= bufferLength.get() + entry.getLength()\n+                //if single message size larger than full buffer size, then ok to offer when buffer is empty\n+                && !(entry.getLength() > maxBufferLength && offloadBuffer.isEmpty())) {\n+            return OfferEntryResult.FAIL_BUFFER_FULL;\n+        } else {\n+            if (!naiveCheckConsecutive(lastOfferedPosition,\n+                    PositionImpl.get(entry.getLedgerId(), entry.getEntryId()))) {\n+                log.error(\"position {} and {} are not consecutive\", lastOfferedPosition,\n+                        entry.getPosition());\n+                return OfferEntryResult.FAIL_NOT_CONSECUTIVE;\n+            }\n+            final EntryImpl entryImpl = EntryImpl\n+                    .create(entry.getLedgerId(), entry.getEntryId(), entry.getDataBuffer());\n+            offloadBuffer.add(entryImpl);\n+            bufferLength.getAndAdd(entryImpl.getLength());\n+            segmentLength.getAndAdd(entryImpl.getLength());\n+            lastOfferedPosition = entryImpl.getPosition();\n+            if (segmentLength.get() >= maxSegmentLength\n+                    && System.currentTimeMillis() - segmentBeginTimeMillis >= minSegmentCloseTimeMillis) {\n+                closeSegment();\n+            }\n+            return OfferEntryResult.SUCCESS;\n+        }\n+    }\n+\n+    private synchronized boolean closeSegment() {\n+        final boolean result = !segmentInfo.isClosed();\n+        log.debug(\"close segment {} {}\", lastOfferedPosition.getLedgerId(), lastOfferedPosition.getEntryId());\n+        this.segmentInfo.closeSegment(lastOfferedPosition.getLedgerId(), lastOfferedPosition.getEntryId());\n+        return result;\n+    }\n+\n+    private static boolean naiveCheckConsecutive(PositionImpl lastOfferedPosition, PositionImpl offeringPosition) {\n+        if (offeringPosition.getLedgerId() == lastOfferedPosition.getLedgerId()\n+                && offeringPosition.getEntryId() == lastOfferedPosition.getEntryId() + 1) {\n+            return true;\n+        } else if (offeringPosition.getEntryId() == 0) {\n+            return true;\n+        } else {\n+            // lastOfferedPosition not initialized\n+            return lastOfferedPosition.equals(PositionImpl.latest);\n+        }\n+    }\n+\n+    private PositionImpl lastOffered() {\n+        return lastOfferedPosition;\n+    }\n+\n+    private boolean canOffer(long size) {\n+        if (segmentInfo.isClosed()) {\n+            return false;", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDI1MDc5Mg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560250792", "bodyText": "My new offerEntry implementation returns an enum with more rich info, so we may no need to call canOffer anymore\n        enum OfferEntryResult {\n            SUCCESS,\n            FAIL_BUFFER_FULL,\n            FAIL_SEGMENT_CLOSED,\n            FAIL_NOT_CONSECUTIVE\n        }", "author": "Renkai", "createdAt": "2021-01-19T15:13:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA0MDM3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDI4NDUyOA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560284528", "bodyText": "But canOffer return boolean right? If broker call canOffer it return false here, then the broker need to try to add one more entry to check if the offload segment closed? Is my understanding correct?", "author": "codelipenghui", "createdAt": "2021-01-19T15:56:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA0MDM3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA0NjMzOA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560046338", "bodyText": "we can provide more information here", "author": "codelipenghui", "createdAt": "2021-01-19T09:49:19Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java", "diffHunk": "@@ -230,6 +265,202 @@ public String getOffloadDriverName() {\n         return promise;\n     }\n \n+    BlobStore blobStore;\n+    String streamingDataBlockKey;\n+    String streamingDataIndexKey;\n+    MultipartUpload streamingMpu = null;\n+    List<MultipartPart> streamingParts = Lists.newArrayList();\n+\n+    @Override\n+    public CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                             long beginEntry,\n+                                                             Map<String, String> driverMetadata) {\n+        this.ml = ml;\n+        this.segmentInfo = new SegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(), driverMetadata);\n+        log.debug(\"begin offload with {}:{}\", beginLedger, beginEntry);\n+        this.offloadResult = new CompletableFuture<>();\n+        blobStore = blobStores.get(config.getBlobStoreLocation());\n+        streamingIndexBuilder = StreamingOffloadIndexBlockBuilder.create();\n+        streamingDataBlockKey = segmentInfo.uuid.toString();\n+        streamingDataIndexKey = String.format(\"%s-index\", segmentInfo.uuid);\n+        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);\n+        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);\n+        Blob blob = blobBuilder.build();\n+        streamingMpu = blobStore\n+                .initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());\n+\n+        scheduler.chooseThread(segmentInfo).execute(() -> {\n+            log.info(\"start offloading segment: {}\", segmentInfo);\n+            streamingOffloadLoop(1, 0);\n+        });\n+        scheduler.schedule(this::closeSegment, maxSegmentCloseTime.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        return CompletableFuture.completedFuture(new OffloadHandle() {\n+            @Override\n+            public boolean canOffer(long size) {\n+                return BlobStoreManagedLedgerOffloader.this.canOffer(size);\n+            }\n+\n+            @Override\n+            public PositionImpl lastOffered() {\n+                return BlobStoreManagedLedgerOffloader.this.lastOffered();\n+            }\n+\n+            @Override\n+            public OfferEntryResult offerEntry(Entry entry) {\n+                return BlobStoreManagedLedgerOffloader.this.offerEntry(entry);\n+            }\n+\n+            @Override\n+            public CompletableFuture<OffloadResult> getOffloadResultAsync() {\n+                return BlobStoreManagedLedgerOffloader.this.getOffloadResultAsync();\n+            }\n+\n+            @Override\n+            public boolean close() {\n+                return BlobStoreManagedLedgerOffloader.this.closeSegment();\n+            }\n+        });\n+    }\n+\n+    private void streamingOffloadLoop(int partId, int dataObjectLength) {\n+        log.debug(\"streaming offload loop {} {}\", partId, dataObjectLength);\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            offloadResult.complete(segmentInfo.result());\n+            return;\n+        }\n+        final BufferedOffloadStream payloadStream;\n+\n+        while (offloadBuffer.isEmpty()) {\n+            if (segmentInfo.isClosed()) {\n+                offloadResult.complete(segmentInfo.result());\n+            } else {\n+                try {\n+                    Thread.sleep(100);\n+                } catch (InterruptedException e) {\n+                    e.printStackTrace();\n+                }\n+            }\n+        }\n+        final Entry peek = offloadBuffer.peek();\n+        //initialize payload when there is at least one entry\n+        final long blockLedgerId = peek.getLedgerId();\n+        final long blockEntryId = peek.getEntryId();\n+        payloadStream = new BufferedOffloadStream(streamingBlockSize, offloadBuffer, segmentInfo,\n+                blockLedgerId, blockEntryId, bufferLength);\n+        try {\n+            streamingIndexBuilder.addLedgerMeta(blockLedgerId, ml.getClosedLedgerInfo(blockLedgerId).get());\n+            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());\n+        } catch (InterruptedException | ExecutionException e) {\n+            offloadResult.completeExceptionally(e);\n+            return;\n+        }\n+        log.debug(\"begin upload payload\");\n+        Payload partPayload = Payloads.newInputStreamPayload(payloadStream);\n+        partPayload.getContentMetadata().setContentType(\"application/octet-stream\");\n+        streamingParts.add(blobStore.uploadMultipartPart(streamingMpu, partId, partPayload));\n+        streamingIndexBuilder.addBlock(blockLedgerId, blockEntryId, partId, streamingBlockSize);\n+\n+        log.debug(\"UploadMultipartPart. container: {}, blobName: {}, partId: {}, mpu: {}\",\n+                config.getBucket(), streamingDataBlockKey, partId, streamingMpu.id());\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            log.debug(\"segment closed, buffer is empty\");", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA2MTU3NQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560061575", "bodyText": "StreamingDataBlockHeaderImpl -> DataBlockHeaderImplV2? @Renkai @sijie The header format can be used by different offload implementations, I think coupling with streaming offload is not a good choice.", "author": "codelipenghui", "createdAt": "2021-01-19T10:11:54Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/StreamingDataBlockHeaderImpl.java", "diffHunk": "@@ -0,0 +1,139 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud.impl;\n+\n+import com.google.common.io.CountingInputStream;\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.ByteBufInputStream;\n+import java.io.DataInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import org.apache.bookkeeper.mledger.offload.jcloud.DataBlockHeader;\n+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;\n+\n+/**\n+ * The data block header in code storage for each data block.\n+ */\n+public class StreamingDataBlockHeaderImpl implements DataBlockHeader {", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MzE1MzMwMw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r563153303", "bodyText": "Have you checked this comment @Renkai?", "author": "codelipenghui", "createdAt": "2021-01-23T13:33:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA2MTU3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MzMwMTAxNw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r563301017", "bodyText": "@codelipenghui It's copied from other source code, I think the original author want to use 'cold storage', which is more commonly called 'tiered storage' now, I have changed the comment to tiered storage", "author": "Renkai", "createdAt": "2021-01-24T14:31:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA2MTU3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA2ODU0OQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560068549", "bodyText": "Is the BufferedOffloadStream need to be close after offload complete? I notice during the offload loop, the BufferedOffloadStream is created for each loop, but can't find where to close it.", "author": "codelipenghui", "createdAt": "2021-01-19T10:22:31Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BufferedOffloadStream.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud.impl;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.CompositeByteBuf;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicLong;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.bookkeeper.mledger.Entry;\n+import org.apache.bookkeeper.mledger.impl.SegmentInfoImpl;\n+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;\n+\n+@Slf4j\n+public class BufferedOffloadStream extends InputStream {", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDE5OTM3Nw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560199377", "bodyText": "The JCloud API should closed it, original ledger based offloader created a stream without call close, too.", "author": "Renkai", "createdAt": "2021-01-19T14:06:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA2ODU0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA3NTk3Mw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560075973", "bodyText": "final?", "author": "codelipenghui", "createdAt": "2021-01-19T10:34:30Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BufferedOffloadStream.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud.impl;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.CompositeByteBuf;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicLong;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.bookkeeper.mledger.Entry;\n+import org.apache.bookkeeper.mledger.impl.SegmentInfoImpl;\n+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;\n+\n+@Slf4j\n+public class BufferedOffloadStream extends InputStream {\n+    static final int[] BLOCK_END_PADDING = BlockAwareSegmentInputStreamImpl.BLOCK_END_PADDING;\n+    private final SegmentInfoImpl segmentInfo;\n+\n+    private final long ledgerId;\n+    private final long beginEntryId;\n+    private AtomicLong bufferLength;\n+    static final int ENTRY_HEADER_SIZE = 4 /* entry size */ + 8 /* entry id */;\n+    private final long blockSize;\n+    private final ConcurrentLinkedQueue<Entry> entryBuffer;\n+    private final InputStream blockHead;\n+    int offset = 0;\n+    static int NOT_INITIALIZED = -1;", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA4MjAyNg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560082026", "bodyText": "Should peek first? If the next entry exceeds the max block size, what the behavior?", "author": "codelipenghui", "createdAt": "2021-01-19T10:44:02Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BufferedOffloadStream.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud.impl;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.CompositeByteBuf;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicLong;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.bookkeeper.mledger.Entry;\n+import org.apache.bookkeeper.mledger.impl.SegmentInfoImpl;\n+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;\n+\n+@Slf4j\n+public class BufferedOffloadStream extends InputStream {\n+    static final int[] BLOCK_END_PADDING = BlockAwareSegmentInputStreamImpl.BLOCK_END_PADDING;\n+    private final SegmentInfoImpl segmentInfo;\n+\n+    private final long ledgerId;\n+    private final long beginEntryId;\n+    private AtomicLong bufferLength;\n+    static final int ENTRY_HEADER_SIZE = 4 /* entry size */ + 8 /* entry id */;\n+    private final long blockSize;\n+    private final ConcurrentLinkedQueue<Entry> entryBuffer;\n+    private final InputStream blockHead;\n+    int offset = 0;\n+    static int NOT_INITIALIZED = -1;\n+    int validDataOffset = NOT_INITIALIZED;\n+    CompositeByteBuf currentEntry;\n+\n+    public long getLedgerId() {\n+        return ledgerId;\n+    }\n+\n+    public long getBeginEntryId() {\n+        return beginEntryId;\n+    }\n+\n+    public long getBlockSize() {\n+        return blockSize;\n+    }\n+\n+\n+    public BufferedOffloadStream(int blockSize,\n+                                 ConcurrentLinkedQueue<Entry> entryBuffer,\n+                                 SegmentInfoImpl segmentInfo,\n+                                 long ledgerId,\n+                                 long beginEntryId,\n+                                 AtomicLong bufferLength) {\n+        this.ledgerId = ledgerId;\n+        this.beginEntryId = beginEntryId;\n+        this.blockSize = blockSize;\n+        this.segmentInfo = segmentInfo;\n+        this.entryBuffer = entryBuffer;\n+        this.bufferLength = bufferLength;\n+        this.blockHead = StreamingDataBlockHeaderImpl.of(blockSize, ledgerId, beginEntryId)\n+                .toStream();\n+    }\n+\n+\n+    @Override\n+    public int read() throws IOException {\n+        if (blockHead.available() > 0) {\n+            offset++;\n+            return blockHead.read();\n+        }\n+        //if current exists, use current first\n+        if (currentEntry != null) {\n+            if (currentEntry.readableBytes() > 0) {\n+                offset += 1;\n+                return currentEntry.readUnsignedByte();\n+            } else {\n+                currentEntry.release();\n+                currentEntry = null;\n+            }\n+        }\n+\n+        if (blockSize <= offset) {\n+            return -1;\n+        } else if (validDataOffset != NOT_INITIALIZED) {\n+            return BLOCK_END_PADDING[(offset++ - validDataOffset) % BLOCK_END_PADDING.length];\n+        }\n+\n+        Entry headEntry;\n+\n+        while ((headEntry = entryBuffer.peek()) == null) {\n+            if (segmentInfo.isClosed()) {\n+                if (validDataOffset == NOT_INITIALIZED) {\n+                    validDataOffset = offset;\n+                }\n+                return read();\n+            } else {\n+                try {\n+                    Thread.sleep(100);\n+                } catch (InterruptedException e) {\n+                    log.error(\"sleep failed\", e);\n+                }\n+            }\n+        }\n+\n+        //create new block when a ledger end\n+        if (headEntry.getLedgerId() != this.ledgerId) {\n+            if (validDataOffset == NOT_INITIALIZED) {\n+                validDataOffset = offset;\n+            }\n+            return read();\n+        }\n+\n+        if (blockSize >= offset\n+                + ENTRY_HEADER_SIZE\n+                + headEntry.getLength()) {\n+            entryBuffer.poll();", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDIxMjMzOA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560212338", "bodyText": "Should peek first? If the next entry exceeds the max block size, what the behavior?\n\nSince block size is decided when block is created, I'm afraid we have to make the next block larger than an entry, I will supplement the code.", "author": "Renkai", "createdAt": "2021-01-19T14:25:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA4MjAyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA4NzI5MQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560087291", "bodyText": "is the streamingParts clear automatically after complete the upload? I can't find any places to clear the streamingParts. BTW, the streamingParts seems can be a local variable", "author": "codelipenghui", "createdAt": "2021-01-19T10:52:12Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java", "diffHunk": "@@ -230,6 +265,202 @@ public String getOffloadDriverName() {\n         return promise;\n     }\n \n+    BlobStore blobStore;\n+    String streamingDataBlockKey;\n+    String streamingDataIndexKey;\n+    MultipartUpload streamingMpu = null;\n+    List<MultipartPart> streamingParts = Lists.newArrayList();\n+\n+    @Override\n+    public CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                             long beginEntry,\n+                                                             Map<String, String> driverMetadata) {\n+        this.ml = ml;\n+        this.segmentInfo = new SegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(), driverMetadata);\n+        log.debug(\"begin offload with {}:{}\", beginLedger, beginEntry);\n+        this.offloadResult = new CompletableFuture<>();\n+        blobStore = blobStores.get(config.getBlobStoreLocation());\n+        streamingIndexBuilder = StreamingOffloadIndexBlockBuilder.create();\n+        streamingDataBlockKey = segmentInfo.uuid.toString();\n+        streamingDataIndexKey = String.format(\"%s-index\", segmentInfo.uuid);\n+        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);\n+        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);\n+        Blob blob = blobBuilder.build();\n+        streamingMpu = blobStore\n+                .initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());\n+\n+        scheduler.chooseThread(segmentInfo).execute(() -> {\n+            log.info(\"start offloading segment: {}\", segmentInfo);\n+            streamingOffloadLoop(1, 0);\n+        });\n+        scheduler.schedule(this::closeSegment, maxSegmentCloseTime.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        return CompletableFuture.completedFuture(new OffloadHandle() {\n+            @Override\n+            public boolean canOffer(long size) {\n+                return BlobStoreManagedLedgerOffloader.this.canOffer(size);\n+            }\n+\n+            @Override\n+            public PositionImpl lastOffered() {\n+                return BlobStoreManagedLedgerOffloader.this.lastOffered();\n+            }\n+\n+            @Override\n+            public OfferEntryResult offerEntry(Entry entry) {\n+                return BlobStoreManagedLedgerOffloader.this.offerEntry(entry);\n+            }\n+\n+            @Override\n+            public CompletableFuture<OffloadResult> getOffloadResultAsync() {\n+                return BlobStoreManagedLedgerOffloader.this.getOffloadResultAsync();\n+            }\n+\n+            @Override\n+            public boolean close() {\n+                return BlobStoreManagedLedgerOffloader.this.closeSegment();\n+            }\n+        });\n+    }\n+\n+    private void streamingOffloadLoop(int partId, int dataObjectLength) {\n+        log.debug(\"streaming offload loop {} {}\", partId, dataObjectLength);\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            offloadResult.complete(segmentInfo.result());\n+            return;\n+        }\n+        final BufferedOffloadStream payloadStream;\n+\n+        while (offloadBuffer.isEmpty()) {\n+            if (segmentInfo.isClosed()) {\n+                offloadResult.complete(segmentInfo.result());\n+            } else {\n+                try {\n+                    Thread.sleep(100);\n+                } catch (InterruptedException e) {\n+                    e.printStackTrace();\n+                }\n+            }\n+        }\n+        final Entry peek = offloadBuffer.peek();\n+        //initialize payload when there is at least one entry\n+        final long blockLedgerId = peek.getLedgerId();\n+        final long blockEntryId = peek.getEntryId();\n+        payloadStream = new BufferedOffloadStream(streamingBlockSize, offloadBuffer, segmentInfo,\n+                blockLedgerId, blockEntryId, bufferLength);\n+        try {\n+            streamingIndexBuilder.addLedgerMeta(blockLedgerId, ml.getClosedLedgerInfo(blockLedgerId).get());\n+            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());\n+        } catch (InterruptedException | ExecutionException e) {\n+            offloadResult.completeExceptionally(e);\n+            return;\n+        }\n+        log.debug(\"begin upload payload\");\n+        Payload partPayload = Payloads.newInputStreamPayload(payloadStream);\n+        partPayload.getContentMetadata().setContentType(\"application/octet-stream\");\n+        streamingParts.add(blobStore.uploadMultipartPart(streamingMpu, partId, partPayload));\n+        streamingIndexBuilder.addBlock(blockLedgerId, blockEntryId, partId, streamingBlockSize);\n+\n+        log.debug(\"UploadMultipartPart. container: {}, blobName: {}, partId: {}, mpu: {}\",\n+                config.getBucket(), streamingDataBlockKey, partId, streamingMpu.id());\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            log.debug(\"segment closed, buffer is empty\");\n+            try {\n+                blobStore.completeMultipartUpload(streamingMpu, streamingParts);", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDIxMzU4MQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560213581", "bodyText": "is the streamingParts clear automatically after complete the upload? I can't find any places to clear the streamingParts. BTW, the streamingParts seems can be a local variable\n\nAs we discussed formerly, a new offloader will be created once the segment offloaded, so streamingParts will be garbage collected with the offloader.", "author": "Renkai", "createdAt": "2021-01-19T14:26:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA4NzI5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDI4NjIyOA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560286228", "bodyText": "If the streamingParts does not clear, for the next offload loop, how to avoid offload the duplicate data?", "author": "codelipenghui", "createdAt": "2021-01-19T15:58:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA4NzI5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA4ODI1Ng==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560088256", "bodyText": "Is it need to close?", "author": "codelipenghui", "createdAt": "2021-01-19T10:53:50Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java", "diffHunk": "@@ -230,6 +265,202 @@ public String getOffloadDriverName() {\n         return promise;\n     }\n \n+    BlobStore blobStore;\n+    String streamingDataBlockKey;\n+    String streamingDataIndexKey;\n+    MultipartUpload streamingMpu = null;\n+    List<MultipartPart> streamingParts = Lists.newArrayList();\n+\n+    @Override\n+    public CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                             long beginEntry,\n+                                                             Map<String, String> driverMetadata) {\n+        this.ml = ml;\n+        this.segmentInfo = new SegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(), driverMetadata);\n+        log.debug(\"begin offload with {}:{}\", beginLedger, beginEntry);\n+        this.offloadResult = new CompletableFuture<>();\n+        blobStore = blobStores.get(config.getBlobStoreLocation());\n+        streamingIndexBuilder = StreamingOffloadIndexBlockBuilder.create();\n+        streamingDataBlockKey = segmentInfo.uuid.toString();\n+        streamingDataIndexKey = String.format(\"%s-index\", segmentInfo.uuid);\n+        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);\n+        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);\n+        Blob blob = blobBuilder.build();\n+        streamingMpu = blobStore\n+                .initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());\n+\n+        scheduler.chooseThread(segmentInfo).execute(() -> {\n+            log.info(\"start offloading segment: {}\", segmentInfo);\n+            streamingOffloadLoop(1, 0);\n+        });\n+        scheduler.schedule(this::closeSegment, maxSegmentCloseTime.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        return CompletableFuture.completedFuture(new OffloadHandle() {\n+            @Override\n+            public boolean canOffer(long size) {\n+                return BlobStoreManagedLedgerOffloader.this.canOffer(size);\n+            }\n+\n+            @Override\n+            public PositionImpl lastOffered() {\n+                return BlobStoreManagedLedgerOffloader.this.lastOffered();\n+            }\n+\n+            @Override\n+            public OfferEntryResult offerEntry(Entry entry) {\n+                return BlobStoreManagedLedgerOffloader.this.offerEntry(entry);\n+            }\n+\n+            @Override\n+            public CompletableFuture<OffloadResult> getOffloadResultAsync() {\n+                return BlobStoreManagedLedgerOffloader.this.getOffloadResultAsync();\n+            }\n+\n+            @Override\n+            public boolean close() {\n+                return BlobStoreManagedLedgerOffloader.this.closeSegment();\n+            }\n+        });\n+    }\n+\n+    private void streamingOffloadLoop(int partId, int dataObjectLength) {\n+        log.debug(\"streaming offload loop {} {}\", partId, dataObjectLength);\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            offloadResult.complete(segmentInfo.result());\n+            return;\n+        }\n+        final BufferedOffloadStream payloadStream;\n+\n+        while (offloadBuffer.isEmpty()) {\n+            if (segmentInfo.isClosed()) {\n+                offloadResult.complete(segmentInfo.result());\n+            } else {\n+                try {\n+                    Thread.sleep(100);\n+                } catch (InterruptedException e) {\n+                    e.printStackTrace();\n+                }\n+            }\n+        }\n+        final Entry peek = offloadBuffer.peek();\n+        //initialize payload when there is at least one entry\n+        final long blockLedgerId = peek.getLedgerId();\n+        final long blockEntryId = peek.getEntryId();\n+        payloadStream = new BufferedOffloadStream(streamingBlockSize, offloadBuffer, segmentInfo,\n+                blockLedgerId, blockEntryId, bufferLength);\n+        try {\n+            streamingIndexBuilder.addLedgerMeta(blockLedgerId, ml.getClosedLedgerInfo(blockLedgerId).get());\n+            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());\n+        } catch (InterruptedException | ExecutionException e) {\n+            offloadResult.completeExceptionally(e);\n+            return;\n+        }\n+        log.debug(\"begin upload payload\");\n+        Payload partPayload = Payloads.newInputStreamPayload(payloadStream);\n+        partPayload.getContentMetadata().setContentType(\"application/octet-stream\");\n+        streamingParts.add(blobStore.uploadMultipartPart(streamingMpu, partId, partPayload));\n+        streamingIndexBuilder.addBlock(blockLedgerId, blockEntryId, partId, streamingBlockSize);\n+\n+        log.debug(\"UploadMultipartPart. container: {}, blobName: {}, partId: {}, mpu: {}\",\n+                config.getBucket(), streamingDataBlockKey, partId, streamingMpu.id());\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            log.debug(\"segment closed, buffer is empty\");\n+            try {\n+                blobStore.completeMultipartUpload(streamingMpu, streamingParts);\n+                streamingIndexBuilder.withDataObjectLength(dataObjectLength + streamingBlockSize);\n+                final StreamingOffloadIndexBlock index = streamingIndexBuilder.buildStreaming();\n+                final IndexInputStream indexStream = index.toStream();\n+                final BlobBuilder indexBlobBuilder = blobStore.blobBuilder(streamingDataIndexKey);\n+                DataBlockUtils.addVersionInfo(indexBlobBuilder, userMetadata);\n+                final InputStreamPayload indexPayLoad = Payloads.newInputStreamPayload(indexStream);", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA4ODk2NA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560088964", "bodyText": "Please provide more information, if you have many topics, the debug log can't help find any thing here.", "author": "codelipenghui", "createdAt": "2021-01-19T10:55:05Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java", "diffHunk": "@@ -230,6 +265,202 @@ public String getOffloadDriverName() {\n         return promise;\n     }\n \n+    BlobStore blobStore;\n+    String streamingDataBlockKey;\n+    String streamingDataIndexKey;\n+    MultipartUpload streamingMpu = null;\n+    List<MultipartPart> streamingParts = Lists.newArrayList();\n+\n+    @Override\n+    public CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                             long beginEntry,\n+                                                             Map<String, String> driverMetadata) {\n+        this.ml = ml;\n+        this.segmentInfo = new SegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(), driverMetadata);\n+        log.debug(\"begin offload with {}:{}\", beginLedger, beginEntry);\n+        this.offloadResult = new CompletableFuture<>();\n+        blobStore = blobStores.get(config.getBlobStoreLocation());\n+        streamingIndexBuilder = StreamingOffloadIndexBlockBuilder.create();\n+        streamingDataBlockKey = segmentInfo.uuid.toString();\n+        streamingDataIndexKey = String.format(\"%s-index\", segmentInfo.uuid);\n+        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);\n+        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);\n+        Blob blob = blobBuilder.build();\n+        streamingMpu = blobStore\n+                .initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());\n+\n+        scheduler.chooseThread(segmentInfo).execute(() -> {\n+            log.info(\"start offloading segment: {}\", segmentInfo);\n+            streamingOffloadLoop(1, 0);\n+        });\n+        scheduler.schedule(this::closeSegment, maxSegmentCloseTime.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        return CompletableFuture.completedFuture(new OffloadHandle() {\n+            @Override\n+            public boolean canOffer(long size) {\n+                return BlobStoreManagedLedgerOffloader.this.canOffer(size);\n+            }\n+\n+            @Override\n+            public PositionImpl lastOffered() {\n+                return BlobStoreManagedLedgerOffloader.this.lastOffered();\n+            }\n+\n+            @Override\n+            public OfferEntryResult offerEntry(Entry entry) {\n+                return BlobStoreManagedLedgerOffloader.this.offerEntry(entry);\n+            }\n+\n+            @Override\n+            public CompletableFuture<OffloadResult> getOffloadResultAsync() {\n+                return BlobStoreManagedLedgerOffloader.this.getOffloadResultAsync();\n+            }\n+\n+            @Override\n+            public boolean close() {\n+                return BlobStoreManagedLedgerOffloader.this.closeSegment();\n+            }\n+        });\n+    }\n+\n+    private void streamingOffloadLoop(int partId, int dataObjectLength) {\n+        log.debug(\"streaming offload loop {} {}\", partId, dataObjectLength);\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            offloadResult.complete(segmentInfo.result());\n+            return;\n+        }\n+        final BufferedOffloadStream payloadStream;\n+\n+        while (offloadBuffer.isEmpty()) {\n+            if (segmentInfo.isClosed()) {\n+                offloadResult.complete(segmentInfo.result());\n+            } else {\n+                try {\n+                    Thread.sleep(100);\n+                } catch (InterruptedException e) {\n+                    e.printStackTrace();\n+                }\n+            }\n+        }\n+        final Entry peek = offloadBuffer.peek();\n+        //initialize payload when there is at least one entry\n+        final long blockLedgerId = peek.getLedgerId();\n+        final long blockEntryId = peek.getEntryId();\n+        payloadStream = new BufferedOffloadStream(streamingBlockSize, offloadBuffer, segmentInfo,\n+                blockLedgerId, blockEntryId, bufferLength);\n+        try {\n+            streamingIndexBuilder.addLedgerMeta(blockLedgerId, ml.getClosedLedgerInfo(blockLedgerId).get());\n+            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());\n+        } catch (InterruptedException | ExecutionException e) {\n+            offloadResult.completeExceptionally(e);\n+            return;\n+        }\n+        log.debug(\"begin upload payload\");\n+        Payload partPayload = Payloads.newInputStreamPayload(payloadStream);\n+        partPayload.getContentMetadata().setContentType(\"application/octet-stream\");\n+        streamingParts.add(blobStore.uploadMultipartPart(streamingMpu, partId, partPayload));\n+        streamingIndexBuilder.addBlock(blockLedgerId, blockEntryId, partId, streamingBlockSize);\n+\n+        log.debug(\"UploadMultipartPart. container: {}, blobName: {}, partId: {}, mpu: {}\",\n+                config.getBucket(), streamingDataBlockKey, partId, streamingMpu.id());\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            log.debug(\"segment closed, buffer is empty\");\n+            try {\n+                blobStore.completeMultipartUpload(streamingMpu, streamingParts);\n+                streamingIndexBuilder.withDataObjectLength(dataObjectLength + streamingBlockSize);\n+                final StreamingOffloadIndexBlock index = streamingIndexBuilder.buildStreaming();\n+                final IndexInputStream indexStream = index.toStream();\n+                final BlobBuilder indexBlobBuilder = blobStore.blobBuilder(streamingDataIndexKey);\n+                DataBlockUtils.addVersionInfo(indexBlobBuilder, userMetadata);\n+                final InputStreamPayload indexPayLoad = Payloads.newInputStreamPayload(indexStream);\n+                indexPayLoad.getContentMetadata().setContentLength(indexStream.getStreamSize());\n+                indexPayLoad.getContentMetadata().setContentType(\"application/octet-stream\");\n+                final Blob indexBlob = indexBlobBuilder.payload(indexPayLoad)\n+                        .contentLength(indexStream.getStreamSize())\n+                        .build();\n+                blobStore.putBlob(config.getBucket(), indexBlob);\n+\n+                offloadResult.complete(segmentInfo.result());\n+            } catch (Exception e) {\n+                log.error(\"streaming offload failed\", e);\n+                offloadResult.completeExceptionally(e);\n+            }\n+            log.debug(\"offload done\");", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA4OTA2Mw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560089063", "bodyText": "Same as above comment", "author": "codelipenghui", "createdAt": "2021-01-19T10:55:15Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java", "diffHunk": "@@ -230,6 +265,202 @@ public String getOffloadDriverName() {\n         return promise;\n     }\n \n+    BlobStore blobStore;\n+    String streamingDataBlockKey;\n+    String streamingDataIndexKey;\n+    MultipartUpload streamingMpu = null;\n+    List<MultipartPart> streamingParts = Lists.newArrayList();\n+\n+    @Override\n+    public CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                             long beginEntry,\n+                                                             Map<String, String> driverMetadata) {\n+        this.ml = ml;\n+        this.segmentInfo = new SegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(), driverMetadata);\n+        log.debug(\"begin offload with {}:{}\", beginLedger, beginEntry);\n+        this.offloadResult = new CompletableFuture<>();\n+        blobStore = blobStores.get(config.getBlobStoreLocation());\n+        streamingIndexBuilder = StreamingOffloadIndexBlockBuilder.create();\n+        streamingDataBlockKey = segmentInfo.uuid.toString();\n+        streamingDataIndexKey = String.format(\"%s-index\", segmentInfo.uuid);\n+        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);\n+        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);\n+        Blob blob = blobBuilder.build();\n+        streamingMpu = blobStore\n+                .initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());\n+\n+        scheduler.chooseThread(segmentInfo).execute(() -> {\n+            log.info(\"start offloading segment: {}\", segmentInfo);\n+            streamingOffloadLoop(1, 0);\n+        });\n+        scheduler.schedule(this::closeSegment, maxSegmentCloseTime.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        return CompletableFuture.completedFuture(new OffloadHandle() {\n+            @Override\n+            public boolean canOffer(long size) {\n+                return BlobStoreManagedLedgerOffloader.this.canOffer(size);\n+            }\n+\n+            @Override\n+            public PositionImpl lastOffered() {\n+                return BlobStoreManagedLedgerOffloader.this.lastOffered();\n+            }\n+\n+            @Override\n+            public OfferEntryResult offerEntry(Entry entry) {\n+                return BlobStoreManagedLedgerOffloader.this.offerEntry(entry);\n+            }\n+\n+            @Override\n+            public CompletableFuture<OffloadResult> getOffloadResultAsync() {\n+                return BlobStoreManagedLedgerOffloader.this.getOffloadResultAsync();\n+            }\n+\n+            @Override\n+            public boolean close() {\n+                return BlobStoreManagedLedgerOffloader.this.closeSegment();\n+            }\n+        });\n+    }\n+\n+    private void streamingOffloadLoop(int partId, int dataObjectLength) {\n+        log.debug(\"streaming offload loop {} {}\", partId, dataObjectLength);\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            offloadResult.complete(segmentInfo.result());\n+            return;\n+        }\n+        final BufferedOffloadStream payloadStream;\n+\n+        while (offloadBuffer.isEmpty()) {\n+            if (segmentInfo.isClosed()) {\n+                offloadResult.complete(segmentInfo.result());\n+            } else {\n+                try {\n+                    Thread.sleep(100);\n+                } catch (InterruptedException e) {\n+                    e.printStackTrace();\n+                }\n+            }\n+        }\n+        final Entry peek = offloadBuffer.peek();\n+        //initialize payload when there is at least one entry\n+        final long blockLedgerId = peek.getLedgerId();\n+        final long blockEntryId = peek.getEntryId();\n+        payloadStream = new BufferedOffloadStream(streamingBlockSize, offloadBuffer, segmentInfo,\n+                blockLedgerId, blockEntryId, bufferLength);\n+        try {\n+            streamingIndexBuilder.addLedgerMeta(blockLedgerId, ml.getClosedLedgerInfo(blockLedgerId).get());\n+            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());\n+        } catch (InterruptedException | ExecutionException e) {\n+            offloadResult.completeExceptionally(e);\n+            return;\n+        }\n+        log.debug(\"begin upload payload\");\n+        Payload partPayload = Payloads.newInputStreamPayload(payloadStream);\n+        partPayload.getContentMetadata().setContentType(\"application/octet-stream\");\n+        streamingParts.add(blobStore.uploadMultipartPart(streamingMpu, partId, partPayload));\n+        streamingIndexBuilder.addBlock(blockLedgerId, blockEntryId, partId, streamingBlockSize);\n+\n+        log.debug(\"UploadMultipartPart. container: {}, blobName: {}, partId: {}, mpu: {}\",\n+                config.getBucket(), streamingDataBlockKey, partId, streamingMpu.id());\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            log.debug(\"segment closed, buffer is empty\");\n+            try {\n+                blobStore.completeMultipartUpload(streamingMpu, streamingParts);\n+                streamingIndexBuilder.withDataObjectLength(dataObjectLength + streamingBlockSize);\n+                final StreamingOffloadIndexBlock index = streamingIndexBuilder.buildStreaming();\n+                final IndexInputStream indexStream = index.toStream();\n+                final BlobBuilder indexBlobBuilder = blobStore.blobBuilder(streamingDataIndexKey);\n+                DataBlockUtils.addVersionInfo(indexBlobBuilder, userMetadata);\n+                final InputStreamPayload indexPayLoad = Payloads.newInputStreamPayload(indexStream);\n+                indexPayLoad.getContentMetadata().setContentLength(indexStream.getStreamSize());\n+                indexPayLoad.getContentMetadata().setContentType(\"application/octet-stream\");\n+                final Blob indexBlob = indexBlobBuilder.payload(indexPayLoad)\n+                        .contentLength(indexStream.getStreamSize())\n+                        .build();\n+                blobStore.putBlob(config.getBucket(), indexBlob);\n+\n+                offloadResult.complete(segmentInfo.result());\n+            } catch (Exception e) {\n+                log.error(\"streaming offload failed\", e);\n+                offloadResult.completeExceptionally(e);\n+            }\n+            log.debug(\"offload done\");\n+        } else {\n+            log.debug(\"continue offload loop\");", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA5NDcyNg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560094726", "bodyText": "here will be a serious problem because the streaming offload will retry to get entry from the queue for every 100ms, if a topic does not write any messages, the thread will available for other topic offloading until the offload segment timeout(closed) right?", "author": "codelipenghui", "createdAt": "2021-01-19T11:04:34Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java", "diffHunk": "@@ -230,6 +265,202 @@ public String getOffloadDriverName() {\n         return promise;\n     }\n \n+    BlobStore blobStore;\n+    String streamingDataBlockKey;\n+    String streamingDataIndexKey;\n+    MultipartUpload streamingMpu = null;\n+    List<MultipartPart> streamingParts = Lists.newArrayList();\n+\n+    @Override\n+    public CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                             long beginEntry,\n+                                                             Map<String, String> driverMetadata) {\n+        this.ml = ml;\n+        this.segmentInfo = new SegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(), driverMetadata);\n+        log.debug(\"begin offload with {}:{}\", beginLedger, beginEntry);\n+        this.offloadResult = new CompletableFuture<>();\n+        blobStore = blobStores.get(config.getBlobStoreLocation());\n+        streamingIndexBuilder = StreamingOffloadIndexBlockBuilder.create();\n+        streamingDataBlockKey = segmentInfo.uuid.toString();\n+        streamingDataIndexKey = String.format(\"%s-index\", segmentInfo.uuid);\n+        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);\n+        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);\n+        Blob blob = blobBuilder.build();\n+        streamingMpu = blobStore\n+                .initiateMultipartUpload(config.getBucket(), blob.getMetadata(), new PutOptions());\n+\n+        scheduler.chooseThread(segmentInfo).execute(() -> {\n+            log.info(\"start offloading segment: {}\", segmentInfo);\n+            streamingOffloadLoop(1, 0);\n+        });\n+        scheduler.schedule(this::closeSegment, maxSegmentCloseTime.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        return CompletableFuture.completedFuture(new OffloadHandle() {\n+            @Override\n+            public boolean canOffer(long size) {\n+                return BlobStoreManagedLedgerOffloader.this.canOffer(size);\n+            }\n+\n+            @Override\n+            public PositionImpl lastOffered() {\n+                return BlobStoreManagedLedgerOffloader.this.lastOffered();\n+            }\n+\n+            @Override\n+            public OfferEntryResult offerEntry(Entry entry) {\n+                return BlobStoreManagedLedgerOffloader.this.offerEntry(entry);\n+            }\n+\n+            @Override\n+            public CompletableFuture<OffloadResult> getOffloadResultAsync() {\n+                return BlobStoreManagedLedgerOffloader.this.getOffloadResultAsync();\n+            }\n+\n+            @Override\n+            public boolean close() {\n+                return BlobStoreManagedLedgerOffloader.this.closeSegment();\n+            }\n+        });\n+    }\n+\n+    private void streamingOffloadLoop(int partId, int dataObjectLength) {\n+        log.debug(\"streaming offload loop {} {}\", partId, dataObjectLength);\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            offloadResult.complete(segmentInfo.result());\n+            return;\n+        }\n+        final BufferedOffloadStream payloadStream;\n+\n+        while (offloadBuffer.isEmpty()) {\n+            if (segmentInfo.isClosed()) {\n+                offloadResult.complete(segmentInfo.result());\n+            } else {\n+                try {\n+                    Thread.sleep(100);\n+                } catch (InterruptedException e) {\n+                    e.printStackTrace();\n+                }\n+            }\n+        }\n+        final Entry peek = offloadBuffer.peek();\n+        //initialize payload when there is at least one entry\n+        final long blockLedgerId = peek.getLedgerId();\n+        final long blockEntryId = peek.getEntryId();\n+        payloadStream = new BufferedOffloadStream(streamingBlockSize, offloadBuffer, segmentInfo,\n+                blockLedgerId, blockEntryId, bufferLength);\n+        try {\n+            streamingIndexBuilder.addLedgerMeta(blockLedgerId, ml.getClosedLedgerInfo(blockLedgerId).get());\n+            streamingIndexBuilder.withDataBlockHeaderLength(StreamingDataBlockHeaderImpl.getDataStartOffset());\n+        } catch (InterruptedException | ExecutionException e) {\n+            offloadResult.completeExceptionally(e);\n+            return;\n+        }\n+        log.debug(\"begin upload payload\");\n+        Payload partPayload = Payloads.newInputStreamPayload(payloadStream);\n+        partPayload.getContentMetadata().setContentType(\"application/octet-stream\");\n+        streamingParts.add(blobStore.uploadMultipartPart(streamingMpu, partId, partPayload));\n+        streamingIndexBuilder.addBlock(blockLedgerId, blockEntryId, partId, streamingBlockSize);\n+\n+        log.debug(\"UploadMultipartPart. container: {}, blobName: {}, partId: {}, mpu: {}\",\n+                config.getBucket(), streamingDataBlockKey, partId, streamingMpu.id());\n+        if (segmentInfo.isClosed() && offloadBuffer.isEmpty()) {\n+            log.debug(\"segment closed, buffer is empty\");\n+            try {\n+                blobStore.completeMultipartUpload(streamingMpu, streamingParts);\n+                streamingIndexBuilder.withDataObjectLength(dataObjectLength + streamingBlockSize);\n+                final StreamingOffloadIndexBlock index = streamingIndexBuilder.buildStreaming();\n+                final IndexInputStream indexStream = index.toStream();\n+                final BlobBuilder indexBlobBuilder = blobStore.blobBuilder(streamingDataIndexKey);\n+                DataBlockUtils.addVersionInfo(indexBlobBuilder, userMetadata);\n+                final InputStreamPayload indexPayLoad = Payloads.newInputStreamPayload(indexStream);\n+                indexPayLoad.getContentMetadata().setContentLength(indexStream.getStreamSize());\n+                indexPayLoad.getContentMetadata().setContentType(\"application/octet-stream\");\n+                final Blob indexBlob = indexBlobBuilder.payload(indexPayLoad)\n+                        .contentLength(indexStream.getStreamSize())\n+                        .build();\n+                blobStore.putBlob(config.getBucket(), indexBlob);\n+\n+                offloadResult.complete(segmentInfo.result());\n+            } catch (Exception e) {\n+                log.error(\"streaming offload failed\", e);\n+                offloadResult.completeExceptionally(e);\n+            }\n+            log.debug(\"offload done\");\n+        } else {\n+            log.debug(\"continue offload loop\");\n+            scheduler.chooseThread(segmentInfo)\n+                    .execute(() -> streamingOffloadLoop(partId + 1, dataObjectLength + streamingBlockSize));", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDI0NjY0OA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560246648", "bodyText": "I used schedule instead of sleep", "author": "Renkai", "createdAt": "2021-01-19T15:08:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA5NDcyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA5NjA3OQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560096079", "bodyText": "We should to recycle the entries after read complete?", "author": "codelipenghui", "createdAt": "2021-01-19T11:06:54Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BufferedOffloadStream.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud.impl;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.CompositeByteBuf;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicLong;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.bookkeeper.mledger.Entry;\n+import org.apache.bookkeeper.mledger.impl.SegmentInfoImpl;\n+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;\n+\n+@Slf4j\n+public class BufferedOffloadStream extends InputStream {\n+    static final int[] BLOCK_END_PADDING = BlockAwareSegmentInputStreamImpl.BLOCK_END_PADDING;\n+    private final SegmentInfoImpl segmentInfo;\n+\n+    private final long ledgerId;\n+    private final long beginEntryId;\n+    private AtomicLong bufferLength;\n+    static final int ENTRY_HEADER_SIZE = 4 /* entry size */ + 8 /* entry id */;\n+    private final long blockSize;\n+    private final ConcurrentLinkedQueue<Entry> entryBuffer;\n+    private final InputStream blockHead;\n+    int offset = 0;\n+    static int NOT_INITIALIZED = -1;\n+    int validDataOffset = NOT_INITIALIZED;\n+    CompositeByteBuf currentEntry;\n+\n+    public long getLedgerId() {\n+        return ledgerId;\n+    }\n+\n+    public long getBeginEntryId() {\n+        return beginEntryId;\n+    }\n+\n+    public long getBlockSize() {\n+        return blockSize;\n+    }\n+\n+\n+    public BufferedOffloadStream(int blockSize,\n+                                 ConcurrentLinkedQueue<Entry> entryBuffer,\n+                                 SegmentInfoImpl segmentInfo,\n+                                 long ledgerId,\n+                                 long beginEntryId,\n+                                 AtomicLong bufferLength) {\n+        this.ledgerId = ledgerId;\n+        this.beginEntryId = beginEntryId;\n+        this.blockSize = blockSize;\n+        this.segmentInfo = segmentInfo;\n+        this.entryBuffer = entryBuffer;\n+        this.bufferLength = bufferLength;\n+        this.blockHead = StreamingDataBlockHeaderImpl.of(blockSize, ledgerId, beginEntryId)\n+                .toStream();\n+    }\n+\n+\n+    @Override\n+    public int read() throws IOException {\n+        if (blockHead.available() > 0) {\n+            offset++;\n+            return blockHead.read();\n+        }\n+        //if current exists, use current first\n+        if (currentEntry != null) {\n+            if (currentEntry.readableBytes() > 0) {\n+                offset += 1;\n+                return currentEntry.readUnsignedByte();\n+            } else {\n+                currentEntry.release();\n+                currentEntry = null;\n+            }\n+        }\n+\n+        if (blockSize <= offset) {\n+            return -1;\n+        } else if (validDataOffset != NOT_INITIALIZED) {\n+            return BLOCK_END_PADDING[(offset++ - validDataOffset) % BLOCK_END_PADDING.length];\n+        }\n+\n+        Entry headEntry;\n+\n+        while ((headEntry = entryBuffer.peek()) == null) {\n+            if (segmentInfo.isClosed()) {\n+                if (validDataOffset == NOT_INITIALIZED) {\n+                    validDataOffset = offset;\n+                }\n+                return read();\n+            } else {\n+                try {\n+                    Thread.sleep(100);\n+                } catch (InterruptedException e) {\n+                    log.error(\"sleep failed\", e);\n+                }\n+            }\n+        }\n+\n+        //create new block when a ledger end\n+        if (headEntry.getLedgerId() != this.ledgerId) {\n+            if (validDataOffset == NOT_INITIALIZED) {\n+                validDataOffset = offset;\n+            }\n+            return read();\n+        }\n+\n+        if (blockSize >= offset\n+                + ENTRY_HEADER_SIZE\n+                + headEntry.getLength()) {\n+            entryBuffer.poll();", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDI0NzA1Mw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560247053", "bodyText": "headEntry.release added", "author": "Renkai", "createdAt": "2021-01-19T15:08:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA5NjA3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDEwMDc4NA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560100784", "bodyText": "also close the dataStreams?", "author": "codelipenghui", "createdAt": "2021-01-19T11:14:28Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/StreamingBlobStoreBackedReadHandleImpl.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud.impl;\n+\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.ScheduledExecutorService;\n+import lombok.val;\n+import org.apache.bookkeeper.client.BKException;\n+import org.apache.bookkeeper.client.api.LastConfirmedAndEntry;\n+import org.apache.bookkeeper.client.api.LedgerEntries;\n+import org.apache.bookkeeper.client.api.LedgerEntry;\n+import org.apache.bookkeeper.client.api.LedgerMetadata;\n+import org.apache.bookkeeper.client.api.ReadHandle;\n+import org.apache.bookkeeper.client.impl.LedgerEntriesImpl;\n+import org.apache.bookkeeper.client.impl.LedgerEntryImpl;\n+import org.apache.bookkeeper.mledger.offload.jcloud.BackedInputStream;\n+import org.apache.bookkeeper.mledger.offload.jcloud.StreamingOffloadIndexBlock;\n+import org.apache.bookkeeper.mledger.offload.jcloud.StreamingOffloadIndexBlockBuilder;\n+import org.apache.bookkeeper.mledger.offload.jcloud.impl.DataBlockUtils.VersionCheck;\n+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;\n+import org.jclouds.blobstore.BlobStore;\n+import org.jclouds.blobstore.domain.Blob;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class StreamingBlobStoreBackedReadHandleImpl implements ReadHandle {\n+    private static final Logger log = LoggerFactory.getLogger(StreamingBlobStoreBackedReadHandleImpl.class);\n+\n+    private final long ledgerId;\n+    private final List<StreamingOffloadIndexBlock> indices;\n+    private final List<BackedInputStream> inputStreams;\n+    private final List<DataInputStream> dataStreams;\n+    private final ExecutorService executor;\n+\n+    static class GroupedReader {\n+        long ledgerId;\n+        long firstEntry;\n+        long lastEntry;\n+        StreamingOffloadIndexBlock index;\n+        BackedInputStream inputStream;\n+        DataInputStream dataStream;\n+\n+        public GroupedReader(long ledgerId, long firstEntry, long lastEntry,\n+                             StreamingOffloadIndexBlock index,\n+                             BackedInputStream inputStream, DataInputStream dataStream) {\n+            this.ledgerId = ledgerId;\n+            this.firstEntry = firstEntry;\n+            this.lastEntry = lastEntry;\n+            this.index = index;\n+            this.inputStream = inputStream;\n+            this.dataStream = dataStream;\n+        }\n+    }\n+\n+    private StreamingBlobStoreBackedReadHandleImpl(long ledgerId, List<StreamingOffloadIndexBlock> indices,\n+                                                   List<BackedInputStream> inputStreams,\n+                                                   ExecutorService executor) {\n+        this.ledgerId = ledgerId;\n+        this.indices = indices;\n+        this.inputStreams = inputStreams;\n+        this.dataStreams = new LinkedList<>();\n+        for (BackedInputStream inputStream : inputStreams) {\n+            dataStreams.add(new DataInputStream(inputStream));\n+        }\n+        this.executor = executor;\n+    }\n+\n+    @Override\n+    public long getId() {\n+        return ledgerId;\n+    }\n+\n+    @Override\n+    public LedgerMetadata getLedgerMetadata() {\n+        return indices.get(0).getLedgerMetadata(ledgerId);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> closeAsync() {\n+        CompletableFuture<Void> promise = new CompletableFuture<>();\n+        executor.submit(() -> {\n+            try {\n+                for (StreamingOffloadIndexBlock indexBlock : indices) {\n+                    indexBlock.close();\n+                }\n+                for (BackedInputStream inputStream : inputStreams) {\n+                    inputStream.close();\n+                }\n+                promise.complete(null);", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDIxODAxNA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560218014", "bodyText": "Seems close dataStreams will close inputStreams,too.\nSo I replaced close inputStreams to input close dataStreams", "author": "Renkai", "createdAt": "2021-01-19T14:32:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDEwMDc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDEwMTc5NA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560101794", "bodyText": "Why throw bk exception here? does IllegalArgumentException works?", "author": "codelipenghui", "createdAt": "2021-01-19T11:16:15Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/StreamingBlobStoreBackedReadHandleImpl.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud.impl;\n+\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.ScheduledExecutorService;\n+import lombok.val;\n+import org.apache.bookkeeper.client.BKException;\n+import org.apache.bookkeeper.client.api.LastConfirmedAndEntry;\n+import org.apache.bookkeeper.client.api.LedgerEntries;\n+import org.apache.bookkeeper.client.api.LedgerEntry;\n+import org.apache.bookkeeper.client.api.LedgerMetadata;\n+import org.apache.bookkeeper.client.api.ReadHandle;\n+import org.apache.bookkeeper.client.impl.LedgerEntriesImpl;\n+import org.apache.bookkeeper.client.impl.LedgerEntryImpl;\n+import org.apache.bookkeeper.mledger.offload.jcloud.BackedInputStream;\n+import org.apache.bookkeeper.mledger.offload.jcloud.StreamingOffloadIndexBlock;\n+import org.apache.bookkeeper.mledger.offload.jcloud.StreamingOffloadIndexBlockBuilder;\n+import org.apache.bookkeeper.mledger.offload.jcloud.impl.DataBlockUtils.VersionCheck;\n+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;\n+import org.jclouds.blobstore.BlobStore;\n+import org.jclouds.blobstore.domain.Blob;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class StreamingBlobStoreBackedReadHandleImpl implements ReadHandle {\n+    private static final Logger log = LoggerFactory.getLogger(StreamingBlobStoreBackedReadHandleImpl.class);\n+\n+    private final long ledgerId;\n+    private final List<StreamingOffloadIndexBlock> indices;\n+    private final List<BackedInputStream> inputStreams;\n+    private final List<DataInputStream> dataStreams;\n+    private final ExecutorService executor;\n+\n+    static class GroupedReader {\n+        long ledgerId;\n+        long firstEntry;\n+        long lastEntry;\n+        StreamingOffloadIndexBlock index;\n+        BackedInputStream inputStream;\n+        DataInputStream dataStream;\n+\n+        public GroupedReader(long ledgerId, long firstEntry, long lastEntry,\n+                             StreamingOffloadIndexBlock index,\n+                             BackedInputStream inputStream, DataInputStream dataStream) {\n+            this.ledgerId = ledgerId;\n+            this.firstEntry = firstEntry;\n+            this.lastEntry = lastEntry;\n+            this.index = index;\n+            this.inputStream = inputStream;\n+            this.dataStream = dataStream;\n+        }\n+    }\n+\n+    private StreamingBlobStoreBackedReadHandleImpl(long ledgerId, List<StreamingOffloadIndexBlock> indices,\n+                                                   List<BackedInputStream> inputStreams,\n+                                                   ExecutorService executor) {\n+        this.ledgerId = ledgerId;\n+        this.indices = indices;\n+        this.inputStreams = inputStreams;\n+        this.dataStreams = new LinkedList<>();\n+        for (BackedInputStream inputStream : inputStreams) {\n+            dataStreams.add(new DataInputStream(inputStream));\n+        }\n+        this.executor = executor;\n+    }\n+\n+    @Override\n+    public long getId() {\n+        return ledgerId;\n+    }\n+\n+    @Override\n+    public LedgerMetadata getLedgerMetadata() {\n+        return indices.get(0).getLedgerMetadata(ledgerId);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> closeAsync() {\n+        CompletableFuture<Void> promise = new CompletableFuture<>();\n+        executor.submit(() -> {\n+            try {\n+                for (StreamingOffloadIndexBlock indexBlock : indices) {\n+                    indexBlock.close();\n+                }\n+                for (BackedInputStream inputStream : inputStreams) {\n+                    inputStream.close();\n+                }\n+                promise.complete(null);\n+            } catch (IOException t) {\n+                promise.completeExceptionally(t);\n+            }\n+        });\n+        return promise;\n+    }\n+\n+    @Override\n+    public CompletableFuture<LedgerEntries> readAsync(long firstEntry, long lastEntry) {\n+        log.debug(\"Ledger {}: reading {} - {}\", getId(), firstEntry, lastEntry);\n+        CompletableFuture<LedgerEntries> promise = new CompletableFuture<>();\n+        if (firstEntry > lastEntry\n+                || firstEntry < 0\n+                || lastEntry > getLastAddConfirmed()) {\n+            promise.completeExceptionally(new BKException.BKIncorrectParameterException());", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDEwNTc1Ng==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560105756", "bodyText": "Please double confirm the groupedReaders are sorted by ledger Id, entry id, Otherwise, this will bread the entry read in order guarantee", "author": "codelipenghui", "createdAt": "2021-01-19T11:23:08Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/StreamingBlobStoreBackedReadHandleImpl.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud.impl;\n+\n+import com.google.common.base.Preconditions;\n+import io.netty.buffer.ByteBuf;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.ScheduledExecutorService;\n+import lombok.val;\n+import org.apache.bookkeeper.client.BKException;\n+import org.apache.bookkeeper.client.api.LastConfirmedAndEntry;\n+import org.apache.bookkeeper.client.api.LedgerEntries;\n+import org.apache.bookkeeper.client.api.LedgerEntry;\n+import org.apache.bookkeeper.client.api.LedgerMetadata;\n+import org.apache.bookkeeper.client.api.ReadHandle;\n+import org.apache.bookkeeper.client.impl.LedgerEntriesImpl;\n+import org.apache.bookkeeper.client.impl.LedgerEntryImpl;\n+import org.apache.bookkeeper.mledger.offload.jcloud.BackedInputStream;\n+import org.apache.bookkeeper.mledger.offload.jcloud.StreamingOffloadIndexBlock;\n+import org.apache.bookkeeper.mledger.offload.jcloud.StreamingOffloadIndexBlockBuilder;\n+import org.apache.bookkeeper.mledger.offload.jcloud.impl.DataBlockUtils.VersionCheck;\n+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;\n+import org.jclouds.blobstore.BlobStore;\n+import org.jclouds.blobstore.domain.Blob;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class StreamingBlobStoreBackedReadHandleImpl implements ReadHandle {\n+    private static final Logger log = LoggerFactory.getLogger(StreamingBlobStoreBackedReadHandleImpl.class);\n+\n+    private final long ledgerId;\n+    private final List<StreamingOffloadIndexBlock> indices;\n+    private final List<BackedInputStream> inputStreams;\n+    private final List<DataInputStream> dataStreams;\n+    private final ExecutorService executor;\n+\n+    static class GroupedReader {\n+        long ledgerId;\n+        long firstEntry;\n+        long lastEntry;\n+        StreamingOffloadIndexBlock index;\n+        BackedInputStream inputStream;\n+        DataInputStream dataStream;\n+\n+        public GroupedReader(long ledgerId, long firstEntry, long lastEntry,\n+                             StreamingOffloadIndexBlock index,\n+                             BackedInputStream inputStream, DataInputStream dataStream) {\n+            this.ledgerId = ledgerId;\n+            this.firstEntry = firstEntry;\n+            this.lastEntry = lastEntry;\n+            this.index = index;\n+            this.inputStream = inputStream;\n+            this.dataStream = dataStream;\n+        }\n+    }\n+\n+    private StreamingBlobStoreBackedReadHandleImpl(long ledgerId, List<StreamingOffloadIndexBlock> indices,\n+                                                   List<BackedInputStream> inputStreams,\n+                                                   ExecutorService executor) {\n+        this.ledgerId = ledgerId;\n+        this.indices = indices;\n+        this.inputStreams = inputStreams;\n+        this.dataStreams = new LinkedList<>();\n+        for (BackedInputStream inputStream : inputStreams) {\n+            dataStreams.add(new DataInputStream(inputStream));\n+        }\n+        this.executor = executor;\n+    }\n+\n+    @Override\n+    public long getId() {\n+        return ledgerId;\n+    }\n+\n+    @Override\n+    public LedgerMetadata getLedgerMetadata() {\n+        return indices.get(0).getLedgerMetadata(ledgerId);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> closeAsync() {\n+        CompletableFuture<Void> promise = new CompletableFuture<>();\n+        executor.submit(() -> {\n+            try {\n+                for (StreamingOffloadIndexBlock indexBlock : indices) {\n+                    indexBlock.close();\n+                }\n+                for (BackedInputStream inputStream : inputStreams) {\n+                    inputStream.close();\n+                }\n+                promise.complete(null);\n+            } catch (IOException t) {\n+                promise.completeExceptionally(t);\n+            }\n+        });\n+        return promise;\n+    }\n+\n+    @Override\n+    public CompletableFuture<LedgerEntries> readAsync(long firstEntry, long lastEntry) {\n+        log.debug(\"Ledger {}: reading {} - {}\", getId(), firstEntry, lastEntry);\n+        CompletableFuture<LedgerEntries> promise = new CompletableFuture<>();\n+        if (firstEntry > lastEntry\n+                || firstEntry < 0\n+                || lastEntry > getLastAddConfirmed()) {\n+            promise.completeExceptionally(new BKException.BKIncorrectParameterException());\n+            return promise;\n+        }\n+        executor.submit(() -> {\n+            List<LedgerEntry> entries = new ArrayList<LedgerEntry>();\n+            List<GroupedReader> groupedReaders = null;\n+            try {\n+                groupedReaders = getGroupedReader(firstEntry, lastEntry);\n+            } catch (Exception e) {\n+                promise.completeExceptionally(e);\n+                return;\n+            }\n+\n+            for (GroupedReader groupedReader : groupedReaders) {", "originalCommit": "acc18e061a2437b69f05b30c021a9fd911711bcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDI0NzI5NQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560247295", "bodyText": "double confirm code added", "author": "Renkai", "createdAt": "2021-01-19T15:09:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDEwNTc1Ng=="}], "type": "inlineReview"}, {"oid": "45d435c5bd42e5206f714a2f349cebb81ba1b575", "url": "https://github.com/apache/pulsar/commit/45d435c5bd42e5206f714a2f349cebb81ba1b575", "message": "snapshot 20210120\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-20T02:52:49Z", "type": "commit"}, {"oid": "45d435c5bd42e5206f714a2f349cebb81ba1b575", "url": "https://github.com/apache/pulsar/commit/45d435c5bd42e5206f714a2f349cebb81ba1b575", "message": "snapshot 20210120\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-20T02:52:49Z", "type": "forcePushed"}, {"oid": "3f59f16a32888538a15ee9ee6189372f556f795f", "url": "https://github.com/apache/pulsar/commit/3f59f16a32888538a15ee9ee6189372f556f795f", "message": "remove interface segmentInfo\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-20T03:07:09Z", "type": "commit"}, {"oid": "7692f8aa778daa63dc83be84ebe5547e8a6c2fd6", "url": "https://github.com/apache/pulsar/commit/7692f8aa778daa63dc83be84ebe5547e8a6c2fd6", "message": "use auto-close to streams\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-20T04:09:32Z", "type": "commit"}, {"oid": "9faedd44431de79bfc761fb20ee565f728dbd09d", "url": "https://github.com/apache/pulsar/commit/9faedd44431de79bfc761fb20ee565f728dbd09d", "message": "use throwable instead of Exception\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-20T04:23:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY1Nzk1OQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560657959", "bodyText": "streamingoffload()?", "author": "zymap", "createdAt": "2021-01-20T03:55:20Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/LedgerOffloader.java", "diffHunk": "@@ -85,6 +150,34 @@\n                                     UUID uid,\n                                     Map<String, String> extraMetadata);\n \n+    /**\n+     * Begin offload the passed in ledgers to longterm storage, it will finish\n+     * when a segment reached it's size or time.\n+     * Metadata passed in is for inspection purposes only and should be stored\n+     * alongside the segment data.\n+     *\n+     * When the returned OffloaderHandle.getOffloadResultAsync completes, the corresponding\n+     * ledgers has been persisted to the\n+     * longterm storage, so it is safe to delete the original copy in bookkeeper.\n+     *\n+     * The uid is used to identify an attempt to offload. The implementation should\n+     * use this to deterministically generate a unique name for the offloaded object.\n+     * This uid will be stored in the managed ledger metadata before attempting the\n+     * call to offload(). If a subsequent or concurrent call to streamingOffload() finds", "originalCommit": "45d435c5bd42e5206f714a2f349cebb81ba1b575", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY1OTMyOQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560659329", "bodyText": "Do we need to check the returned value? It's might be null.", "author": "zymap", "createdAt": "2021-01-20T03:59:59Z", "path": "managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/ManagedLedgerImpl.java", "diffHunk": "@@ -1685,6 +1685,14 @@ void asyncReadEntries(OpReadEntry opReadEntry) {\n         return getLedgerHandle(ledgerId).thenApply(rh -> rh.getLedgerMetadata().toSafeString());\n     }\n \n+    @Override\n+    public CompletableFuture<LedgerInfo> getLedgerInfo(long ledgerId) {\n+        CompletableFuture<LedgerInfo> result = new CompletableFuture<>();\n+        final LedgerInfo ledgerInfo = ledgers.get(ledgerId);", "originalCommit": "45d435c5bd42e5206f714a2f349cebb81ba1b575", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc0MjAxMA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560742010", "bodyText": "I added comment for the interface, yes it may return a future with null, the caller should deal with it.", "author": "Renkai", "createdAt": "2021-01-20T07:53:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY1OTMyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc1NjU1Mg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560756552", "bodyText": "We might can throw an exception for that?", "author": "zymap", "createdAt": "2021-01-20T08:19:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY1OTMyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MzEwMDYyOA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r563100628", "bodyText": "And please consider returns a copy of LedgerInfo to void be modified from external?", "author": "codelipenghui", "createdAt": "2021-01-23T11:03:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY1OTMyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MzMwMDI0NQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r563300245", "bodyText": "@codelipenghui The LedgerInfo class is generated by protobuf, which is already an immutable class, we don't need to do extra things to make it immutable.", "author": "Renkai", "createdAt": "2021-01-24T14:25:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY1OTMyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY2MDc4MA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560660780", "bodyText": "any() should be any type, why we need to cast it to the UUID?", "author": "zymap", "createdAt": "2021-01-20T04:05:24Z", "path": "managed-ledger/src/test/java/org/apache/bookkeeper/mledger/impl/OffloadPrefixReadTest.java", "diffHunk": "@@ -97,21 +97,21 @@ public void testOffloadRead() throws Exception {\n             assertEquals(new String(e.getData()), \"entry-\" + i++);\n         }\n         verify(offloader, times(1))\n-            .readOffloaded(anyLong(), any(), anyMap());\n+                .readOffloaded(anyLong(), (UUID) any(), anyMap());", "originalCommit": "45d435c5bd42e5206f714a2f349cebb81ba1b575", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc0MjU5MQ==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560742591", "bodyText": "Because readOffloaded have different version of overload, I used cast to avoid ambiguous method call.", "author": "Renkai", "createdAt": "2021-01-20T07:54:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY2MDc4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc1NzE1OA==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560757158", "bodyText": "okay, you can use any(UUID.class) to do that.", "author": "zymap", "createdAt": "2021-01-20T08:20:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY2MDc4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY3MzE0Mg==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560673142", "bodyText": "If we have another call for the streamingOffloading, do those variables will impact the previous offload process? Because the following offload loop still uses the same variables.", "author": "zymap", "createdAt": "2021-01-20T04:35:58Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java", "diffHunk": "@@ -230,6 +266,213 @@ public String getOffloadDriverName() {\n         return promise;\n     }\n \n+    BlobStore blobStore;\n+    String streamingDataBlockKey;\n+    String streamingDataIndexKey;\n+    MultipartUpload streamingMpu = null;\n+    List<MultipartPart> streamingParts = Lists.newArrayList();\n+\n+    @Override\n+    public CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                             long beginEntry,\n+                                                             Map<String, String> driverMetadata) {\n+        this.ml = ml;\n+        this.segmentInfo = new OffloadSegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(),\n+                driverMetadata);\n+        log.debug(\"begin offload with {}:{}\", beginLedger, beginEntry);\n+        this.offloadResult = new CompletableFuture<>();\n+        blobStore = blobStores.get(config.getBlobStoreLocation());\n+        streamingIndexBuilder = StreamingOffloadIndexBlockBuilder.create();\n+        streamingDataBlockKey = segmentInfo.uuid.toString();\n+        streamingDataIndexKey = String.format(\"%s-index\", segmentInfo.uuid);", "originalCommit": "45d435c5bd42e5206f714a2f349cebb81ba1b575", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc0NjYzNw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560746637", "bodyText": "If we have another call for the streamingOffloading, do those variables will impact the previous offload process? Because the following offload loop still uses the same variables.\n\nIn our design, an offloader instance can only call streamingOffload once, I will add some document and check on it.", "author": "Renkai", "createdAt": "2021-01-20T08:02:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY3MzE0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY3MzI0Nw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r560673247", "bodyText": "same above.", "author": "zymap", "createdAt": "2021-01-20T04:36:17Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/BlobStoreManagedLedgerOffloader.java", "diffHunk": "@@ -230,6 +266,213 @@ public String getOffloadDriverName() {\n         return promise;\n     }\n \n+    BlobStore blobStore;\n+    String streamingDataBlockKey;\n+    String streamingDataIndexKey;\n+    MultipartUpload streamingMpu = null;\n+    List<MultipartPart> streamingParts = Lists.newArrayList();\n+\n+    @Override\n+    public CompletableFuture<OffloadHandle> streamingOffload(ManagedLedger ml, UUID uuid, long beginLedger,\n+                                                             long beginEntry,\n+                                                             Map<String, String> driverMetadata) {\n+        this.ml = ml;\n+        this.segmentInfo = new OffloadSegmentInfoImpl(uuid, beginLedger, beginEntry, config.getDriver(),\n+                driverMetadata);\n+        log.debug(\"begin offload with {}:{}\", beginLedger, beginEntry);\n+        this.offloadResult = new CompletableFuture<>();\n+        blobStore = blobStores.get(config.getBlobStoreLocation());\n+        streamingIndexBuilder = StreamingOffloadIndexBlockBuilder.create();\n+        streamingDataBlockKey = segmentInfo.uuid.toString();\n+        streamingDataIndexKey = String.format(\"%s-index\", segmentInfo.uuid);\n+        BlobBuilder blobBuilder = blobStore.blobBuilder(streamingDataBlockKey);\n+        DataBlockUtils.addVersionInfo(blobBuilder, userMetadata);\n+        Blob blob = blobBuilder.build();\n+        streamingMpu = blobStore", "originalCommit": "45d435c5bd42e5206f714a2f349cebb81ba1b575", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8f4510a2c868dc905d41081a8119be14adb686bb", "url": "https://github.com/apache/pulsar/commit/8f4510a2c868dc905d41081a8119be14adb686bb", "message": "fix bug to pass tests\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-20T06:20:13Z", "type": "commit"}, {"oid": "b8b53dcc1f0787176f66f7533dcb13df1a05b586", "url": "https://github.com/apache/pulsar/commit/b8b53dcc1f0787176f66f7533dcb13df1a05b586", "message": "merge the from stream method of two version index\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-20T07:51:16Z", "type": "commit"}, {"oid": "b8b53dcc1f0787176f66f7533dcb13df1a05b586", "url": "https://github.com/apache/pulsar/commit/b8b53dcc1f0787176f66f7533dcb13df1a05b586", "message": "merge the from stream method of two version index\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-20T07:51:16Z", "type": "forcePushed"}, {"oid": "26cdd20e099dd6a3b56d9e38947c962ae32feec7", "url": "https://github.com/apache/pulsar/commit/26cdd20e099dd6a3b56d9e38947c962ae32feec7", "message": "add info to prevent offload multi times\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-20T08:12:52Z", "type": "commit"}, {"oid": "ce767aef96654be789c265600ad1f3a1065a08fe", "url": "https://github.com/apache/pulsar/commit/ce767aef96654be789c265600ad1f3a1065a08fe", "message": "reduce sleep time\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-20T13:26:34Z", "type": "commit"}, {"oid": "6ecde448b62b5e4336f98c72bdd194fe06d41401", "url": "https://github.com/apache/pulsar/commit/6ecde448b62b5e4336f98c72bdd194fe06d41401", "message": "put blob after buffer reaches threshold\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-21T03:47:11Z", "type": "commit"}, {"oid": "7794e184a56fd7ac007a1180e2263563d060d364", "url": "https://github.com/apache/pulsar/commit/7794e184a56fd7ac007a1180e2263563d060d364", "message": "Merge branch 'master' into new-offloader", "committedDate": "2021-01-22T09:05:40Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MzE1MzA3Mw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r563153073", "bodyText": "This should be OffloadIndexBlockV2Builder?", "author": "codelipenghui", "createdAt": "2021-01-23T13:30:22Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/StreamingOffloadIndexBlockBuilder.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import org.apache.bookkeeper.common.annotation.InterfaceAudience.LimitedPrivate;\n+import org.apache.bookkeeper.common.annotation.InterfaceStability.Unstable;\n+import org.apache.bookkeeper.mledger.offload.jcloud.impl.OffloadIndexBlockBuilderImpl;\n+import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;\n+\n+/**\n+ * Interface for builder of index block used for offload a ledger to long term storage.\n+ */\n+@Unstable\n+@LimitedPrivate\n+public interface StreamingOffloadIndexBlockBuilder {", "originalCommit": "7794e184a56fd7ac007a1180e2263563d060d364", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MzE1MzE5Nw==", "url": "https://github.com/apache/pulsar/pull/9096#discussion_r563153197", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class StreamingOffloadIndexBlockImpl implements OffloadIndexBlockV2 {\n          \n          \n            \n            public class OffloadIndexBlockV2Impl implements OffloadIndexBlockV2 {", "author": "codelipenghui", "createdAt": "2021-01-23T13:32:11Z", "path": "tiered-storage/jcloud/src/main/java/org/apache/bookkeeper/mledger/offload/jcloud/impl/StreamingOffloadIndexBlockImpl.java", "diffHunk": "@@ -0,0 +1,379 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.bookkeeper.mledger.offload.jcloud.impl;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.Maps;\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.ByteBufInputStream;\n+import io.netty.util.Recycler;\n+import io.netty.util.Recycler.Handle;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.Objects;\n+import java.util.TreeMap;\n+import org.apache.bookkeeper.client.api.DigestType;\n+import org.apache.bookkeeper.client.api.LedgerMetadata;\n+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlock.IndexInputStream;\n+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexBlockV2;\n+import org.apache.bookkeeper.mledger.offload.jcloud.OffloadIndexEntry;\n+import org.apache.bookkeeper.mledger.proto.MLDataFormats.ManagedLedgerInfo.LedgerInfo;\n+import org.apache.bookkeeper.net.BookieId;\n+import org.apache.pulsar.common.allocator.PulsarByteBufAllocator;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class StreamingOffloadIndexBlockImpl implements OffloadIndexBlockV2 {", "originalCommit": "7794e184a56fd7ac007a1180e2263563d060d364", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "82c6cb1b9fce171dee3907d7fbb6fbf207b3640d", "url": "https://github.com/apache/pulsar/commit/82c6cb1b9fce171dee3907d7fbb6fbf207b3640d", "message": "Merge branch 'master' into new-offloader", "committedDate": "2021-01-24T14:23:58Z", "type": "commit"}, {"oid": "c622056775a8a3fd0b165341618b0ee644ccc922", "url": "https://github.com/apache/pulsar/commit/c622056775a8a3fd0b165341618b0ee644ccc922", "message": "polish\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-24T14:31:53Z", "type": "commit"}, {"oid": "9e9b1c6fff551d949e5aba17894efc60ed7e05a2", "url": "https://github.com/apache/pulsar/commit/9e9b1c6fff551d949e5aba17894efc60ed7e05a2", "message": "change streaming to v2\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-25T01:01:59Z", "type": "commit"}, {"oid": "ea69a1df3cb22f90bd93a1db601db012908be42e", "url": "https://github.com/apache/pulsar/commit/ea69a1df3cb22f90bd93a1db601db012908be42e", "message": "rename streaming to v2\n\nSigned-off-by: Renkai <gaelookair@gmail.com>", "committedDate": "2021-01-25T01:18:59Z", "type": "commit"}, {"oid": "e4835d43f1abba4183f2e93814690307de72d787", "url": "https://github.com/apache/pulsar/commit/e4835d43f1abba4183f2e93814690307de72d787", "message": "Merge branch 'master' into new-offloader", "committedDate": "2021-01-25T14:04:52Z", "type": "commit"}]}