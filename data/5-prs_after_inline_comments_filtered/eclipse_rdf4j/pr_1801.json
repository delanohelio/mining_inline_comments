{"pr_number": 1801, "pr_title": "GH-1784 eval stats", "pr_createdAt": "2020-01-01T22:33:16Z", "pr_url": "https://github.com/eclipse/rdf4j/pull/1801", "timeline": [{"oid": "6363ba9afa5b499d6d50434358cc03e0179d9a15", "url": "https://github.com/eclipse/rdf4j/commit/6363ba9afa5b499d6d50434358cc03e0179d9a15", "message": "started developing eval stats\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2019-12-26T23:44:51Z", "type": "commit"}, {"oid": "c594ec4ab8f4a6e1ccfa5b63cd7cc2c7caac980c", "url": "https://github.com/eclipse/rdf4j/commit/c594ec4ab8f4a6e1ccfa5b63cd7cc2c7caac980c", "message": "Merge branch 'develop' into issues/1784_eval_stats", "committedDate": "2019-12-28T08:02:39Z", "type": "commit"}, {"oid": "f2046f8622c19f47659fbe50146f2cb35052cd2c", "url": "https://github.com/eclipse/rdf4j/commit/f2046f8622c19f47659fbe50146f2cb35052cd2c", "message": "WIP\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2019-12-29T16:59:20Z", "type": "commit"}, {"oid": "61332a4cb193c1046fa6f95f8e95233704dc8c90", "url": "https://github.com/eclipse/rdf4j/commit/61332a4cb193c1046fa6f95f8e95233704dc8c90", "message": "Merge branch 'develop' into issues/1784_eval_stats\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-01-01T13:42:38Z", "type": "commit"}, {"oid": "a118c7fe2c1c260460e82c3fb9bc1c5ff872c3d7", "url": "https://github.com/eclipse/rdf4j/commit/a118c7fe2c1c260460e82c3fb9bc1c5ff872c3d7", "message": "better dynamic stats\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-01-01T18:06:08Z", "type": "commit"}, {"oid": "448c2342e74426826db1192586442c74ccce952a", "url": "https://github.com/eclipse/rdf4j/commit/448c2342e74426826db1192586442c74ccce952a", "message": "wip\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-01-01T18:29:18Z", "type": "commit"}, {"oid": "f813372a930b59f284439843e7da5e9d79b6c01a", "url": "https://github.com/eclipse/rdf4j/commit/f813372a930b59f284439843e7da5e9d79b6c01a", "message": "wip\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-01-01T22:24:20Z", "type": "commit"}, {"oid": "db2c7917a5752b946dfdedd8c08e673cbd302b52", "url": "https://github.com/eclipse/rdf4j/commit/db2c7917a5752b946dfdedd8c08e673cbd302b52", "message": "wip", "committedDate": "2020-01-01T23:06:24Z", "type": "commit"}, {"oid": "41a9153b2bd7e0754e11494d61bd1963a1dfba2f", "url": "https://github.com/eclipse/rdf4j/commit/41a9153b2bd7e0754e11494d61bd1963a1dfba2f", "message": "Merge branch 'develop' into issues/1784_eval_stats", "committedDate": "2020-01-01T23:07:48Z", "type": "commit"}, {"oid": "a7b9a03ce4e5748eaa203c1e1eeb2e2447a28a3e", "url": "https://github.com/eclipse/rdf4j/commit/a7b9a03ce4e5748eaa203c1e1eeb2e2447a28a3e", "message": "wip", "committedDate": "2020-01-01T23:21:22Z", "type": "commit"}, {"oid": "8d338217bc0466cea25d36188d73df85f29974cd", "url": "https://github.com/eclipse/rdf4j/commit/8d338217bc0466cea25d36188d73df85f29974cd", "message": "wip\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-01-01T23:24:53Z", "type": "commit"}, {"oid": "9ffda07dd5643b3355fb6a646e01ef9f72544f08", "url": "https://github.com/eclipse/rdf4j/commit/9ffda07dd5643b3355fb6a646e01ef9f72544f08", "message": "sparse indexes\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-01-02T21:25:38Z", "type": "commit"}, {"oid": "2094628fbfd9981253dba506e9c62a0dbfe22e0e", "url": "https://github.com/eclipse/rdf4j/commit/2094628fbfd9981253dba506e9c62a0dbfe22e0e", "message": "Merge branch 'develop' into issues/1784_eval_stats", "committedDate": "2020-01-02T21:26:08Z", "type": "commit"}, {"oid": "990c4496ef9414ab5728db87620e730042f167a4", "url": "https://github.com/eclipse/rdf4j/commit/990c4496ef9414ab5728db87620e730042f167a4", "message": "low mem benchmark\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-01-02T21:49:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQyNw==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015427", "bodyText": "What is this? It looks awfully platform-dependent. I see it in several places.", "author": "jeenbroekstra", "createdAt": "2020-01-04T04:28:17Z", "path": "core/sail/elasticsearch-store/src/test/java/org/eclipse/rdf4j/sail/elasticsearchstore/benchmark/DeleteBenchmark.java", "diffHunk": "@@ -60,7 +61,7 @@\n \tpublic void beforeClass() throws IOException, InterruptedException {\n \n \t\tembeddedElastic = TestHelpers.startElasticsearch(installLocation,\n-\t\t\t\t\"/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home\");\n+\t\t\t\t\"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\");", "originalCommit": "990c4496ef9414ab5728db87620e730042f167a4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDg4MjU3NA==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r364882574", "bodyText": "Yeah. Because of how benchmark forks the jvm, the JAVA_HOME env isn't being passed through as expected. This was a dirty hack to get it to function.", "author": "hmottestad", "createdAt": "2020-01-09T18:01:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkyODM0NQ==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r390928345", "bodyText": "I'm not going to try to fix this.", "author": "hmottestad", "createdAt": "2020-03-11T12:13:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI0ODA2NA==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r391248064", "bodyText": "Can you add a comment to explain why this is there?", "author": "jeenbroekstra", "createdAt": "2020-03-11T20:25:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMwNjU3OA==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r391306578", "bodyText": "Will do.", "author": "hmottestad", "createdAt": "2020-03-11T22:28:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQ4Mw==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015483", "bodyText": "Typo, should be: EvaluationStatisticsWrapper", "author": "jeenbroekstra", "createdAt": "2020-01-04T04:30:04Z", "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/EvaluationStisticsWrapper.java", "diffHunk": "@@ -0,0 +1,68 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import org.eclipse.rdf4j.common.iteration.CloseableIteration;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.sail.SailException;\n+import org.eclipse.rdf4j.sail.extensiblestore.DataStructureInterface;\n+\n+public class EvaluationStisticsWrapper implements DataStructureInterface {", "originalCommit": "990c4496ef9414ab5728db87620e730042f167a4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTgyMg==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015822", "bodyText": "Typo: StatementQueueItem.", "author": "jeenbroekstra", "createdAt": "2020-01-04T04:40:16Z", "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {\n+\tprivate static final Logger logger = LoggerFactory.getLogger(ExtensibleDynamicEvaluationStatistics.class);\n+\tprivate static final int QUEUE_LIMIT = 128;\n+\tprivate static final int ONE_DIMENSION_INDEX_SIZE = 1024;\n+\n+\tConcurrentLinkedQueue<StatemetQueueItem> queue = new ConcurrentLinkedQueue<StatemetQueueItem>();\n+\n+\tAtomicInteger queueSize = new AtomicInteger();\n+\n+\tprivate final HashFunction hashFunction = Hashing.murmur3_128();\n+\n+\tprivate final HyperLogLogCollector EMPTY_HLL = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector size = HyperLogLogCollector.makeLatestCollector();\n+\tprivate final HyperLogLogCollector size_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex = new HyperLogLogCollector[64][64];\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex_removed = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex_removed = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex_removed = new HyperLogLogCollector[64][64];\n+\tvolatile private Thread queueThread;\n+\n+\tpublic ExtensibleDynamicEvaluationStatistics(ExtensibleSailStore extensibleSailStore) {\n+\t\tsuper(extensibleSailStore);\n+\n+//\t\tStream.of(subjectIndex, predicateIndex, objectIndex, contextIndex,\n+//\t\t\tsubjectIndex_removed, predicateIndex_removed, objectIndex_removed, contextIndex_removed)\n+//\t\t\t.forEach(index -> {\n+//\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+//\t\t\t\t\tindex[i] = HyperLogLogCollector.makeLatestCollector();\n+//\t\t\t\t}\n+//\t\t\t});\n+\n+\t\tStream.of(subjectPredicateIndex, predicateObjectIndex, subjectPredicateIndex_removed,\n+\t\t\t\tpredicateObjectIndex_removed).forEach(index -> {\n+\t\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+\t\t\t\t\t\tfor (int j = 0; j < index[i].length; j++) {\n+\t\t\t\t\t\t\tindex[i][j] = HyperLogLogCollector.makeLatestCollector();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\n+\t}\n+\n+\t@Override\n+\tprotected CardinalityCalculator createCardinalityCalculator() {\n+\t\treturn cardinalityCalculator;\n+\t}\n+\n+\tCardinalityCalculator cardinalityCalculator = new CardinalityCalculator() {\n+\n+\t\t@Override\n+\t\tprotected double getCardinality(StatementPattern sp) {\n+\t\t\tdouble min = size.estimateCardinality() - size_removed.estimateCardinality();\n+\n+\t\t\tmin = Math.min(min, getSubjectCardinality(sp.getSubjectVar()));\n+\t\t\tmin = Math.min(min, getPredicateCardinality(sp.getPredicateVar()));\n+\t\t\tmin = Math.min(min, getObjectCardinality(sp.getObjectVar()));\n+\n+\t\t\tif (sp.getSubjectVar().getValue() != null && sp.getPredicateVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(subjectPredicateIndex, subjectPredicateIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getSubjectVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue()));\n+\t\t\t}\n+\n+\t\t\tif (sp.getPredicateVar().getValue() != null && sp.getObjectVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(predicateObjectIndex, predicateObjectIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getObjectVar().getValue()));\n+\t\t\t}\n+\n+\t\t\treturn min;\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getSubjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(subjectIndex, subjectIndex_removed, var.getValue());\n+\t\t\t}\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getPredicateCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(predicateIndex, predicateIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getObjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(objectIndex, objectIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getContextCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn defaultContext.estimateCardinality() - defaultContext_removed.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(contextIndex, contextIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\t};\n+\n+\tprivate double getHllCardinality(HyperLogLogCollector[][] index, HyperLogLogCollector[][] index_removed,\n+\t\t\tValue value1, Value value2) {\n+\t\treturn index[Math.abs(value1.hashCode() % index.length)][Math.abs(value2.hashCode() % index.length)]\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed[Math.abs(value1.hashCode() % index_removed.length)][Math\n+\t\t\t\t\t\t.abs(value2.hashCode() % index_removed.length)]\n+\t\t\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\tprivate double getHllCardinality(Map<Integer, HyperLogLogCollector> index,\n+\t\t\tMap<Integer, HyperLogLogCollector> index_removed, Value value) {\n+\n+\t\treturn index.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\t@Override\n+\tpublic void add(Statement statement, boolean inferred) {\n+\n+\t\tqueue.add(new StatemetQueueItem(statement, inferred, true));\n+\n+\t\tint size = queueSize.incrementAndGet();\n+\t\tif (size > QUEUE_LIMIT && queueThread == null) {\n+\t\t\tstartQueueThread();\n+\t\t}\n+\n+\t\t/*\n+\t\t * byte[] statementHash = hashFunction.hashString(statement.toString(), StandardCharsets.UTF_8).asBytes();\n+\t\t *\n+\t\t * size.add(statementHash); int subjectHash = statement.getSubject().hashCode(); int predicateHash =\n+\t\t * statement.getPredicate().hashCode(); int objectHash = statement.getObject().hashCode();\n+\t\t *\n+\t\t * indexOneValue(statementHash, subjectIndex, subjectHash); indexOneValue(statementHash, predicateIndex,\n+\t\t * predicateHash); indexOneValue(statementHash, objectIndex, objectHash);\n+\t\t *\n+\t\t * indexTwoValues(statementHash, subjectPredicateIndex, subjectHash, predicateHash);\n+\t\t * indexTwoValues(statementHash, predicateObjectIndex, predicateHash, objectHash);\n+\t\t *\n+\t\t * if (statement.getContext() == null) { defaultContext.add(statementHash); } else {\n+\t\t * indexOneValue(statementHash, contextIndex, statement.getContext().hashCode()); }\n+\t\t *\n+\t\t */\n+\n+\t\t// logger.info(\"added: {} : {} \", statement, inferred ? \"INFERRED\" : \"REAL\");\n+\t}\n+\n+\tsynchronized private void startQueueThread() {\n+\t\tif (queueThread == null) {\n+\t\t\tqueueThread = new Thread(() -> {\n+\t\t\t\ttry {\n+\t\t\t\t\twhile (!queue.isEmpty()) {\n+\t\t\t\t\t\tStatemetQueueItem poll = queue.poll();\n+\t\t\t\t\t\tqueueSize.decrementAndGet();\n+\t\t\t\t\t\tStatement statement = poll.statement;\n+\t\t\t\t\t\tbyte[] statementHash = hashFunction.hashString(statement.toString(), StandardCharsets.UTF_8)\n+\t\t\t\t\t\t\t\t.asBytes();\n+\n+\t\t\t\t\t\tif (poll.added) {\n+\n+\t\t\t\t\t\t\tsize.add(statementHash);\n+\t\t\t\t\t\t\tint subjectHash = statement.getSubject().hashCode();\n+\t\t\t\t\t\t\tint predicateHash = statement.getPredicate().hashCode();\n+\t\t\t\t\t\t\tint objectHash = statement.getObject().hashCode();\n+\n+\t\t\t\t\t\t\tindexOneValue(statementHash, subjectIndex, subjectHash);\n+\t\t\t\t\t\t\tindexOneValue(statementHash, predicateIndex, predicateHash);\n+\t\t\t\t\t\t\tindexOneValue(statementHash, objectIndex, objectHash);\n+\n+\t\t\t\t\t\t\tindexTwoValues(statementHash, subjectPredicateIndex, subjectHash, predicateHash);\n+\t\t\t\t\t\t\tindexTwoValues(statementHash, predicateObjectIndex, predicateHash, objectHash);\n+\n+\t\t\t\t\t\t\tif (statement.getContext() == null) {\n+\t\t\t\t\t\t\t\tdefaultContext.add(statementHash);\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\tindexOneValue(statementHash, contextIndex, statement.getContext().hashCode());\n+\t\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t} finally {\n+\t\t\t\t\tqueueThread = null;\n+\t\t\t\t}\n+\n+\t\t\t});\n+\n+\t\t\tqueueThread.start();\n+\n+\t\t}\n+\t}\n+\n+\tclass StatemetQueueItem {", "originalCommit": "990c4496ef9414ab5728db87620e730042f167a4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTk3MA==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015970", "bodyText": "I realize this is all WIP, but can you please make sure to use logger.debug instead of System.out.println?\nMore generally: this test - I assume you're looking for it to produce a certain optimized query plan. Can we put the expectation in in the form of an assertion?", "author": "jeenbroekstra", "createdAt": "2020-01-04T04:44:34Z", "path": "core/sail/extensible-store/src/test/java/org/eclipse/rdf4j/sail/extensiblestoreimpl/EvaluationStatisticsTest.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*******************************************************************************\n+ * Copyright (c) 2019 Eclipse RDF4J contributors.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Distribution License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/org/documents/edl-v10.php.\n+ *******************************************************************************/\n+package org.eclipse.rdf4j.sail.extensiblestoreimpl;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.IsolationLevels;\n+import org.eclipse.rdf4j.common.iteration.Iterations;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.impl.SimpleValueFactory;\n+import org.eclipse.rdf4j.model.vocabulary.RDF;\n+import org.eclipse.rdf4j.query.TupleQuery;\n+import org.eclipse.rdf4j.query.TupleQueryResult;\n+import org.eclipse.rdf4j.query.impl.IteratingTupleQueryResult;\n+import org.eclipse.rdf4j.repository.sail.SailRepository;\n+import org.eclipse.rdf4j.repository.sail.SailRepositoryConnection;\n+import org.eclipse.rdf4j.rio.RDFFormat;\n+import org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics.ExtensibleDynamicEvaluationStatistics;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+\n+import static junit.framework.TestCase.assertEquals;\n+\n+public class EvaluationStatisticsTest {\n+\n+\tprivate static final Logger logger = LoggerFactory.getLogger(EvaluationStatisticsTest.class);\n+\n+\t@Test\n+\tpublic void temp() {\n+\n+\t\tExtensibleStoreImplForTests extensibleStoreImplForTests = new ExtensibleStoreImplForTests();\n+\n+\t\tSailRepository sailRepository = new SailRepository(extensibleStoreImplForTests);\n+\n+\t\ttry (SailRepositoryConnection connection = sailRepository.getConnection()) {\n+\n+\t\t\tconnection.begin();\n+\t\t\tconnection.add(RDF.TYPE, RDF.TYPE, RDF.PROPERTY);\n+\t\t\tconnection.commit();\n+\t\t}\n+\n+\t\tsailRepository.shutDown();\n+\t}\n+\n+\t@Test\n+\tpublic void hllTest() {\n+\n+\t\tStatement statement = SimpleValueFactory.getInstance().createStatement(RDF.TYPE, RDF.TYPE, RDF.PROPERTY);\n+\t\tHyperLogLogCollector collector = HyperLogLogCollector.makeLatestCollector();\n+\n+\t\tHashFunction hashFunction = Hashing.murmur3_128();\n+\t\tcollector.add(hashFunction.hashString(statement.toString(), StandardCharsets.UTF_8).asBytes());\n+\n+\t\tdouble cardinality = collector.estimateCardinality();\n+\n+\t\tcollector.add(hashFunction.hashString(statement.toString(), StandardCharsets.UTF_8).asBytes());\n+\n+\t\tassertEquals(cardinality, collector.estimateCardinality());\n+\n+\t}\n+\n+\t@Test\n+\tpublic void queryPlanTest() throws IOException {\n+\n+\t\tExtensibleStoreImplForTests extensibleStoreImplForTests = new ExtensibleStoreImplForTests();\n+\n+\t\tSailRepository sailRepository = new SailRepository(extensibleStoreImplForTests);\n+\n+\t\ttry (SailRepositoryConnection connection = sailRepository.getConnection()) {\n+\n+\t\t\tconnection.begin(IsolationLevels.NONE);\n+\t\t\tconnection.add(EvaluationStatisticsTest.class.getClassLoader().getResourceAsStream(\"bsbm-100.ttl\"), \"\",\n+\t\t\t\t\tRDFFormat.TURTLE);\n+\t\t\tconnection.commit();\n+\n+\t\t\tTupleQuery tupleQuery = connection.prepareTupleQuery(getQuery(\"evaluation-statistics/query1.rq\"));\n+\t\t\tSystem.out.println(tupleQuery.toString());\n+\t\t\ttry (IteratingTupleQueryResult evaluate = (IteratingTupleQueryResult) tupleQuery.evaluate()) {\n+\t\t\t\tSystem.out.println(evaluate.toString());", "originalCommit": "990c4496ef9414ab5728db87620e730042f167a4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTk5Mw==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015993", "bodyText": "Heh - time to update our code templates to 2020 :) I haven't done mine either yet.", "author": "jeenbroekstra", "createdAt": "2020-01-04T04:45:10Z", "path": "core/sail/extensible-store/src/test/java/org/eclipse/rdf4j/sail/extensiblestoreimpl/benchmark/ExtensibleDynamicEvaluationStatisticsBenchmark.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*******************************************************************************\n+ * Copyright (c) 2019 Eclipse RDF4J contributors.", "originalCommit": "990c4496ef9414ab5728db87620e730042f167a4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjU1MA==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363016550", "bodyText": "Copyright header is missing here, and a couple of other places as well.", "author": "jeenbroekstra", "createdAt": "2020-01-04T05:04:08Z", "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;", "originalCommit": "990c4496ef9414ab5728db87620e730042f167a4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjYyMQ==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363016621", "bodyText": "I believe this class, and some of the other implementation classes as well, deserve some extensive javadoc that goes into the how and why of the algorithm used.", "author": "jeenbroekstra", "createdAt": "2020-01-04T05:06:43Z", "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {", "originalCommit": "990c4496ef9414ab5728db87620e730042f167a4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjY4Nw==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363016687", "bodyText": "I'll have to believe you if you say that it does what it should do. Seriously though: I wonder if you could maybe split this out into more than one line of code to make it easier to read.", "author": "jeenbroekstra", "createdAt": "2020-01-04T05:08:36Z", "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {\n+\tprivate static final Logger logger = LoggerFactory.getLogger(ExtensibleDynamicEvaluationStatistics.class);\n+\tprivate static final int QUEUE_LIMIT = 128;\n+\tprivate static final int ONE_DIMENSION_INDEX_SIZE = 1024;\n+\n+\tConcurrentLinkedQueue<StatemetQueueItem> queue = new ConcurrentLinkedQueue<StatemetQueueItem>();\n+\n+\tAtomicInteger queueSize = new AtomicInteger();\n+\n+\tprivate final HashFunction hashFunction = Hashing.murmur3_128();\n+\n+\tprivate final HyperLogLogCollector EMPTY_HLL = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector size = HyperLogLogCollector.makeLatestCollector();\n+\tprivate final HyperLogLogCollector size_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex = new HyperLogLogCollector[64][64];\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex_removed = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex_removed = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex_removed = new HyperLogLogCollector[64][64];\n+\tvolatile private Thread queueThread;\n+\n+\tpublic ExtensibleDynamicEvaluationStatistics(ExtensibleSailStore extensibleSailStore) {\n+\t\tsuper(extensibleSailStore);\n+\n+//\t\tStream.of(subjectIndex, predicateIndex, objectIndex, contextIndex,\n+//\t\t\tsubjectIndex_removed, predicateIndex_removed, objectIndex_removed, contextIndex_removed)\n+//\t\t\t.forEach(index -> {\n+//\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+//\t\t\t\t\tindex[i] = HyperLogLogCollector.makeLatestCollector();\n+//\t\t\t\t}\n+//\t\t\t});\n+\n+\t\tStream.of(subjectPredicateIndex, predicateObjectIndex, subjectPredicateIndex_removed,\n+\t\t\t\tpredicateObjectIndex_removed).forEach(index -> {\n+\t\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+\t\t\t\t\t\tfor (int j = 0; j < index[i].length; j++) {\n+\t\t\t\t\t\t\tindex[i][j] = HyperLogLogCollector.makeLatestCollector();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\n+\t}\n+\n+\t@Override\n+\tprotected CardinalityCalculator createCardinalityCalculator() {\n+\t\treturn cardinalityCalculator;\n+\t}\n+\n+\tCardinalityCalculator cardinalityCalculator = new CardinalityCalculator() {\n+\n+\t\t@Override\n+\t\tprotected double getCardinality(StatementPattern sp) {\n+\t\t\tdouble min = size.estimateCardinality() - size_removed.estimateCardinality();\n+\n+\t\t\tmin = Math.min(min, getSubjectCardinality(sp.getSubjectVar()));\n+\t\t\tmin = Math.min(min, getPredicateCardinality(sp.getPredicateVar()));\n+\t\t\tmin = Math.min(min, getObjectCardinality(sp.getObjectVar()));\n+\n+\t\t\tif (sp.getSubjectVar().getValue() != null && sp.getPredicateVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(subjectPredicateIndex, subjectPredicateIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getSubjectVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue()));\n+\t\t\t}\n+\n+\t\t\tif (sp.getPredicateVar().getValue() != null && sp.getObjectVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(predicateObjectIndex, predicateObjectIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getObjectVar().getValue()));\n+\t\t\t}\n+\n+\t\t\treturn min;\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getSubjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(subjectIndex, subjectIndex_removed, var.getValue());\n+\t\t\t}\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getPredicateCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(predicateIndex, predicateIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getObjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(objectIndex, objectIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getContextCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn defaultContext.estimateCardinality() - defaultContext_removed.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(contextIndex, contextIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\t};\n+\n+\tprivate double getHllCardinality(HyperLogLogCollector[][] index, HyperLogLogCollector[][] index_removed,\n+\t\t\tValue value1, Value value2) {\n+\t\treturn index[Math.abs(value1.hashCode() % index.length)][Math.abs(value2.hashCode() % index.length)]\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed[Math.abs(value1.hashCode() % index_removed.length)][Math\n+\t\t\t\t\t\t.abs(value2.hashCode() % index_removed.length)]\n+\t\t\t\t\t\t\t\t.estimateCardinality();", "originalCommit": "990c4496ef9414ab5728db87620e730042f167a4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDg4MTM5OA==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r364881398", "bodyText": "Will do :D - it got a lot worse when I added in the index_removed support.", "author": "hmottestad", "createdAt": "2020-01-09T17:59:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjY4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjc0Nw==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363016747", "bodyText": "What is the role of this thread? It updates the various indexes I gather, is that such an expensive operation that it should be handled asynchronously? I also note that further down you wait for the thread to re-join before continuing - does the separate thread really help, then?", "author": "jeenbroekstra", "createdAt": "2020-01-04T05:10:55Z", "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {\n+\tprivate static final Logger logger = LoggerFactory.getLogger(ExtensibleDynamicEvaluationStatistics.class);\n+\tprivate static final int QUEUE_LIMIT = 128;\n+\tprivate static final int ONE_DIMENSION_INDEX_SIZE = 1024;\n+\n+\tConcurrentLinkedQueue<StatemetQueueItem> queue = new ConcurrentLinkedQueue<StatemetQueueItem>();\n+\n+\tAtomicInteger queueSize = new AtomicInteger();\n+\n+\tprivate final HashFunction hashFunction = Hashing.murmur3_128();\n+\n+\tprivate final HyperLogLogCollector EMPTY_HLL = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector size = HyperLogLogCollector.makeLatestCollector();\n+\tprivate final HyperLogLogCollector size_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex = new HyperLogLogCollector[64][64];\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex_removed = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex_removed = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex_removed = new HyperLogLogCollector[64][64];\n+\tvolatile private Thread queueThread;\n+\n+\tpublic ExtensibleDynamicEvaluationStatistics(ExtensibleSailStore extensibleSailStore) {\n+\t\tsuper(extensibleSailStore);\n+\n+//\t\tStream.of(subjectIndex, predicateIndex, objectIndex, contextIndex,\n+//\t\t\tsubjectIndex_removed, predicateIndex_removed, objectIndex_removed, contextIndex_removed)\n+//\t\t\t.forEach(index -> {\n+//\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+//\t\t\t\t\tindex[i] = HyperLogLogCollector.makeLatestCollector();\n+//\t\t\t\t}\n+//\t\t\t});\n+\n+\t\tStream.of(subjectPredicateIndex, predicateObjectIndex, subjectPredicateIndex_removed,\n+\t\t\t\tpredicateObjectIndex_removed).forEach(index -> {\n+\t\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+\t\t\t\t\t\tfor (int j = 0; j < index[i].length; j++) {\n+\t\t\t\t\t\t\tindex[i][j] = HyperLogLogCollector.makeLatestCollector();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\n+\t}\n+\n+\t@Override\n+\tprotected CardinalityCalculator createCardinalityCalculator() {\n+\t\treturn cardinalityCalculator;\n+\t}\n+\n+\tCardinalityCalculator cardinalityCalculator = new CardinalityCalculator() {\n+\n+\t\t@Override\n+\t\tprotected double getCardinality(StatementPattern sp) {\n+\t\t\tdouble min = size.estimateCardinality() - size_removed.estimateCardinality();\n+\n+\t\t\tmin = Math.min(min, getSubjectCardinality(sp.getSubjectVar()));\n+\t\t\tmin = Math.min(min, getPredicateCardinality(sp.getPredicateVar()));\n+\t\t\tmin = Math.min(min, getObjectCardinality(sp.getObjectVar()));\n+\n+\t\t\tif (sp.getSubjectVar().getValue() != null && sp.getPredicateVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(subjectPredicateIndex, subjectPredicateIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getSubjectVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue()));\n+\t\t\t}\n+\n+\t\t\tif (sp.getPredicateVar().getValue() != null && sp.getObjectVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(predicateObjectIndex, predicateObjectIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getObjectVar().getValue()));\n+\t\t\t}\n+\n+\t\t\treturn min;\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getSubjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(subjectIndex, subjectIndex_removed, var.getValue());\n+\t\t\t}\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getPredicateCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(predicateIndex, predicateIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getObjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(objectIndex, objectIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getContextCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn defaultContext.estimateCardinality() - defaultContext_removed.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(contextIndex, contextIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\t};\n+\n+\tprivate double getHllCardinality(HyperLogLogCollector[][] index, HyperLogLogCollector[][] index_removed,\n+\t\t\tValue value1, Value value2) {\n+\t\treturn index[Math.abs(value1.hashCode() % index.length)][Math.abs(value2.hashCode() % index.length)]\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed[Math.abs(value1.hashCode() % index_removed.length)][Math\n+\t\t\t\t\t\t.abs(value2.hashCode() % index_removed.length)]\n+\t\t\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\tprivate double getHllCardinality(Map<Integer, HyperLogLogCollector> index,\n+\t\t\tMap<Integer, HyperLogLogCollector> index_removed, Value value) {\n+\n+\t\treturn index.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\t@Override\n+\tpublic void add(Statement statement, boolean inferred) {\n+\n+\t\tqueue.add(new StatemetQueueItem(statement, inferred, true));\n+\n+\t\tint size = queueSize.incrementAndGet();\n+\t\tif (size > QUEUE_LIMIT && queueThread == null) {\n+\t\t\tstartQueueThread();", "originalCommit": "990c4496ef9414ab5728db87620e730042f167a4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDg5MTIzNQ==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r364891235", "bodyText": "It's to do async stats updates. The thread is started if the size of the queue is likely to be above the limit, then runs until the queue is empty. Since the stats are only estimates there is no guarantee for timeliness. Those guarantees are needed when running tests and benchmarking, so there is a poor mans latch that waits for the thread to finish.", "author": "hmottestad", "createdAt": "2020-01-09T18:22:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjc0Nw=="}], "type": "inlineReview"}, {"oid": "f8421600ff8d5a7ae5475b7d912c8164a49b99a9", "url": "https://github.com/eclipse/rdf4j/commit/f8421600ff8d5a7ae5475b7d912c8164a49b99a9", "message": "more tests\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-01-04T13:37:13Z", "type": "commit"}, {"oid": "2fda585e56529e306a44264902f3f33002d1720d", "url": "https://github.com/eclipse/rdf4j/commit/2fda585e56529e306a44264902f3f33002d1720d", "message": "Merge branch 'develop' into issues/1784_eval_stats", "committedDate": "2020-01-04T13:37:34Z", "type": "commit"}, {"oid": "4cb6b96b020f78b27d45d0d9d3d91788af0bde07", "url": "https://github.com/eclipse/rdf4j/commit/4cb6b96b020f78b27d45d0d9d3d91788af0bde07", "message": "wip", "committedDate": "2020-01-05T12:22:06Z", "type": "commit"}, {"oid": "561a53aad05b2a4ac1b2fe3e545991d2583e8a1c", "url": "https://github.com/eclipse/rdf4j/commit/561a53aad05b2a4ac1b2fe3e545991d2583e8a1c", "message": "Merge branch 'develop' into issues/1784_eval_stats\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-01-12T11:49:58Z", "type": "commit"}, {"oid": "bc345bc5ecae4f5580e741564b6d77fcb5b10445", "url": "https://github.com/eclipse/rdf4j/commit/bc345bc5ecae4f5580e741564b6d77fcb5b10445", "message": "Merge branch 'develop' into issues/1784_eval_stats", "committedDate": "2020-01-14T11:24:18Z", "type": "commit"}, {"oid": "db6ee47581e3ceed57c4940c229cc223fb88f667", "url": "https://github.com/eclipse/rdf4j/commit/db6ee47581e3ceed57c4940c229cc223fb88f667", "message": "testing out ways to detect stalenss\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-01-18T10:24:03Z", "type": "commit"}, {"oid": "2c54c196e287fda13796d88065474c24b73f6001", "url": "https://github.com/eclipse/rdf4j/commit/2c54c196e287fda13796d88065474c24b73f6001", "message": "#1784 better staleness\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-02-10T06:46:15Z", "type": "commit"}, {"oid": "a953326e5b68935c4f536954123e5dc877a18abb", "url": "https://github.com/eclipse/rdf4j/commit/a953326e5b68935c4f536954123e5dc877a18abb", "message": "#1784 more tests\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-02-10T07:32:38Z", "type": "commit"}, {"oid": "912584274d0794af206309bc443b33eecd629dd0", "url": "https://github.com/eclipse/rdf4j/commit/912584274d0794af206309bc443b33eecd629dd0", "message": "Merge branch 'develop' into issues/1784_eval_stats\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-02-10T07:33:55Z", "type": "commit"}, {"oid": "5f1b0ba83aef3eca19778d1e144d61e262ec3bb7", "url": "https://github.com/eclipse/rdf4j/commit/5f1b0ba83aef3eca19778d1e144d61e262ec3bb7", "message": "#1784 support inferred deeper into the datastructure because we need it for for the eval stats engine\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-02-13T12:33:35Z", "type": "commit"}, {"oid": "36b2031ff518b91083c1785acf22b18ba13a2fee", "url": "https://github.com/eclipse/rdf4j/commit/36b2031ff518b91083c1785acf22b18ba13a2fee", "message": "WIP inferred triples\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-02-14T14:28:57Z", "type": "commit"}, {"oid": "688b4f968abd57c8deb0785a3512477ce5028289", "url": "https://github.com/eclipse/rdf4j/commit/688b4f968abd57c8deb0785a3512477ce5028289", "message": "#1784 fixed inferred statements in elasticsearch\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-02-14T16:50:40Z", "type": "commit"}, {"oid": "ba51e3a26a205bfce12bbef0b52779701415e4c9", "url": "https://github.com/eclipse/rdf4j/commit/ba51e3a26a205bfce12bbef0b52779701415e4c9", "message": "#1784 introduced concept of inferredOnly to make clearing inferred statements faster\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-02-14T17:14:22Z", "type": "commit"}, {"oid": "f6bba7cd6e5dd6ff98eb7ad90aa1effbec0679a6", "url": "https://github.com/eclipse/rdf4j/commit/f6bba7cd6e5dd6ff98eb7ad90aa1effbec0679a6", "message": "Update IteratingTupleQueryResult.java", "committedDate": "2020-02-14T23:36:19Z", "type": "commit"}, {"oid": "a7be9746988ca4b59ef92225c8db4cbaeb32937b", "url": "https://github.com/eclipse/rdf4j/commit/a7be9746988ca4b59ef92225c8db4cbaeb32937b", "message": "code cleanup\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-02-15T08:55:48Z", "type": "commit"}, {"oid": "4751135ed4653dec9b7c94254c4ab568b1a7064d", "url": "https://github.com/eclipse/rdf4j/commit/4751135ed4653dec9b7c94254c4ab568b1a7064d", "message": "fix read committed wrapper\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-02-15T09:29:13Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTgyNjY5OQ==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r379826699", "bodyText": "@jeenbroekstra\nI found out that the SailSourceConnection will get all statements and remove them from the inferred sink, instead of getting just the inferred statements. So I fixed that by introducing a tri-state enum, essentially you can now specify: all, inferredOnly or explicitOnly. This is only for this class....and branch(...) is a private method, so no breaking changes.\nI don't have any decent benchmarks for the rdfs sail that add and remove statements, but I do have some for the SPIN sail and that's seeing a 20% speed bump for some of the benchmarks.", "author": "hmottestad", "createdAt": "2020-02-15T11:42:37Z", "path": "core/sail/base/src/main/java/org/eclipse/rdf4j/sail/base/SailSourceConnection.java", "diffHunk": "@@ -657,17 +656,17 @@ public void clearInferred(Resource... contexts) throws SailException {\n \t\tverifyIsOpen();\n \t\tverifyIsActive();\n \t\tsynchronized (datasets) {\n-\t\t\tif (inferredSink == null) {\n+\t\t\tif (inferredOnlySink == null) {\n \t\t\t\tIsolationLevel level = getIsolationLevel();\n-\t\t\t\tSailSource branch = branch(true);\n-\t\t\t\tinferredDataset = branch.dataset(level);\n-\t\t\t\tinferredSink = branch.sink(level);\n-\t\t\t\texplicitOnlyDataset = branch(false).dataset(level);\n+\t\t\t\tSailSource branch = branch(IncludeInferred.inferredOnly);\n+\t\t\t\tinferredOnlyDataset = branch.dataset(level);\n+\t\t\t\tinferredOnlySink = branch.sink(level);\n+\t\t\t\texplicitOnlyDataset = branch(IncludeInferred.explicitOnly).dataset(level);\n \t\t\t}\n \t\t\tif (this.hasConnectionListeners()) {\n-\t\t\t\tremove(null, null, null, inferredDataset, inferredSink, contexts);\n+\t\t\t\tremove(null, null, null, inferredOnlyDataset, inferredOnlySink, contexts);\n \t\t\t}\n-\t\t\tinferredSink.clear(contexts);\n+\t\t\tinferredOnlySink.clear(contexts);", "originalCommit": "4751135ed4653dec9b7c94254c4ab568b1a7064d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk2MzEwMA==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r379963100", "bodyText": "Is this related to #1795 ?", "author": "jeenbroekstra", "createdAt": "2020-02-17T02:29:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTgyNjY5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDAwNDU0MA==", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r380004540", "bodyText": "Not related no. This just makes clearing inferred statements faster.", "author": "hmottestad", "createdAt": "2020-02-17T06:34:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTgyNjY5OQ=="}], "type": "inlineReview"}, {"oid": "a05c21b6267f08d09fa84b93f788dad4f9ba5e70", "url": "https://github.com/eclipse/rdf4j/commit/a05c21b6267f08d09fa84b93f788dad4f9ba5e70", "message": "Merge branch 'develop' into issues/1784_eval_stats\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-03-10T13:55:17Z", "type": "commit"}, {"oid": "274476446d4e74d48d746b21f41763186b9ea2af", "url": "https://github.com/eclipse/rdf4j/commit/274476446d4e74d48d746b21f41763186b9ea2af", "message": "trigger dynamic stats refresh\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-03-10T15:49:07Z", "type": "commit"}, {"oid": "3ddc91f0e7cc81bc749a31bc0807f5b50fb567f5", "url": "https://github.com/eclipse/rdf4j/commit/3ddc91f0e7cc81bc749a31bc0807f5b50fb567f5", "message": "refresh stats every 60 seconds\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-03-11T10:12:35Z", "type": "commit"}, {"oid": "8f3d82c58e03926523472c0444663e2fa4d0d338", "url": "https://github.com/eclipse/rdf4j/commit/8f3d82c58e03926523472c0444663e2fa4d0d338", "message": "code review cleanup\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-03-11T10:18:35Z", "type": "commit"}, {"oid": "27f64bd17849987fa3e1838e39849ace05ed8e10", "url": "https://github.com/eclipse/rdf4j/commit/27f64bd17849987fa3e1838e39849ace05ed8e10", "message": "cleaned up some code and added some documentation\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-03-11T10:48:22Z", "type": "commit"}, {"oid": "21b4d488d1a7e1d37d34dd5a0974a63d9cac7117", "url": "https://github.com/eclipse/rdf4j/commit/21b4d488d1a7e1d37d34dd5a0974a63d9cac7117", "message": "more documentation\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-03-11T11:16:43Z", "type": "commit"}, {"oid": "9784b6a16259bbe8ab4ea1259c83f350b0a11873", "url": "https://github.com/eclipse/rdf4j/commit/9784b6a16259bbe8ab4ea1259c83f350b0a11873", "message": "cleanup unused code and code used for testing during development\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-03-11T11:26:42Z", "type": "commit"}, {"oid": "749c8a2f9da74d5786dcde19c6893e1ed644c324", "url": "https://github.com/eclipse/rdf4j/commit/749c8a2f9da74d5786dcde19c6893e1ed644c324", "message": "updated some tests\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-03-11T12:09:57Z", "type": "commit"}, {"oid": "53675cd833bc5bb193818c557bd13bce1c9a5a6a", "url": "https://github.com/eclipse/rdf4j/commit/53675cd833bc5bb193818c557bd13bce1c9a5a6a", "message": "add recommended sorting on ES scroll\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-03-11T13:33:03Z", "type": "commit"}, {"oid": "502333a0bea663bbb12de03e14f51bb34870d325", "url": "https://github.com/eclipse/rdf4j/commit/502333a0bea663bbb12de03e14f51bb34870d325", "message": "code cleanup\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-03-11T13:54:43Z", "type": "commit"}, {"oid": "eaccbf16cdfc09410ba47cfe76043a92cafb0f6a", "url": "https://github.com/eclipse/rdf4j/commit/eaccbf16cdfc09410ba47cfe76043a92cafb0f6a", "message": "comments about JMH not passing JAVA_HOME correctly\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>", "committedDate": "2020-03-12T07:27:20Z", "type": "commit"}]}