{"pr_number": 10605, "pr_title": "Add DiskDisruptionIT to integration tests", "pr_createdAt": "2020-09-28T13:06:04Z", "pr_url": "https://github.com/crate/crate/pull/10605", "timeline": [{"oid": "6d6a38a84b89fcf6048a038554adb12b1aa677c1", "url": "https://github.com/crate/crate/commit/6d6a38a84b89fcf6048a038554adb12b1aa677c1", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-09-29T16:53:22Z", "type": "forcePushed"}, {"oid": "3b20ad83c8ff0bbe1b63ac2acc3cfb2d810780bf", "url": "https://github.com/crate/crate/commit/3b20ad83c8ff0bbe1b63ac2acc3cfb2d810780bf", "message": "cleanup", "committedDate": "2020-09-30T11:50:12Z", "type": "forcePushed"}, {"oid": "44072398444cabca810730fcb4d0bc8be21c2b34", "url": "https://github.com/crate/crate/commit/44072398444cabca810730fcb4d0bc8be21c2b34", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-02T09:07:59Z", "type": "forcePushed"}, {"oid": "5fcfd9cb1b859469228fb05c2f081b08e5d93cd1", "url": "https://github.com/crate/crate/commit/5fcfd9cb1b859469228fb05c2f081b08e5d93cd1", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-02T13:25:32Z", "type": "forcePushed"}, {"oid": "83965e3e129b6ad89036b0b115fa6919c8375390", "url": "https://github.com/crate/crate/commit/83965e3e129b6ad89036b0b115fa6919c8375390", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-02T13:28:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgyNDE0Nw==", "url": "https://github.com/crate/crate/pull/10605#discussion_r498824147", "bodyText": "I removed useAutoGeneratedIDs because afaik primary keys can not be autogenerated using sql.", "author": "mkleen", "createdAt": "2020-10-02T13:33:42Z", "path": "server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.test;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import com.carrotsearch.randomizedtesting.generators.RandomNumbers;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.testing.SQLTransportExecutor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.logging.log4j.util.Supplier;\n+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;\n+import org.junit.Assert;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Locale;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class BackgroundIndexer implements AutoCloseable {\n+\n+    private final Logger logger = LogManager.getLogger(getClass());\n+\n+    final Thread[] writers;\n+    final SQLTransportExecutor sqlExecutor;\n+    final CountDownLatch stopLatch;\n+    final Collection<Exception> failures = new ArrayList<>();\n+    final AtomicBoolean stop = new AtomicBoolean(false);\n+    final AtomicLong idGenerator = new AtomicLong();\n+    final CountDownLatch startLatch = new CountDownLatch(1);\n+    final AtomicBoolean hasBudget = new AtomicBoolean(false); // when set to true, writers will acquire writes from a semaphore\n+    final Semaphore availableBudget = new Semaphore(0);\n+    private final Set<String> ids = ConcurrentCollections.newConcurrentSet();\n+    private volatile Consumer<Exception> failureAssertion = null;\n+\n+    volatile int minFieldSize = 10;\n+    volatile int maxFieldSize = 140;\n+\n+    /**\n+     * Start indexing in the background using a random number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table     table name to write data\n+     * @param column    column name to write data\n+     * @param client    client to use\n+     * @param numOfDocs number of document to index before pausing. Set to -1 to have no limit.\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs) {\n+        this(table, column, client, numOfDocs, RandomizedTest.scaledRandomIntBetween(2, 5));\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param client      client to use\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs, final int writerCount) {\n+        this(table, column, client, numOfDocs, writerCount, true, null);\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     * @param autoStart   set to true to start indexing as soon as all threads have been created.\n+     * @param random      random instance to use\n+     */\n+    public BackgroundIndexer(final String table, final String column, final SQLTransportExecutor sqlExecutor, final int numOfDocs, final int writerCount,\n+                             boolean autoStart, Random random) {\n+        if (random == null) {", "originalCommit": "83965e3e129b6ad89036b0b115fa6919c8375390", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQzODA3MQ==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499438071", "bodyText": "If no primary key is defined on the table, the internal id _id will be generated, otherwise the values of the PK cols are used to compute a _id value.\nAs the _id column can be selected, I see no issue on keeping the useAutogeneratedIDs feature. Do I miss anything?", "author": "seut", "createdAt": "2020-10-05T08:48:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgyNDE0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ2MDg5OA==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499460898", "bodyText": "This requires to initialise the table with or without primary key depending if the id is autogenerated or not. If the primary key is defined it cannot be null, so there is a dependency between the table definition and the indexing routine. However, i can do that.", "author": "mkleen", "createdAt": "2020-10-05T09:25:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgyNDE0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ3MjA0OA==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499472048", "bodyText": "Ah right, so I missed something indeed ;-)\nI've looked into this again and it seems that using autogenerated id's is not required for any current scenario.\nI'd then prefer to not use it like you did.", "author": "seut", "createdAt": "2020-10-05T09:43:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgyNDE0Nw=="}], "type": "inlineReview"}, {"oid": "803846af4c06be3f5b09c4f5c7a3b54877999888", "url": "https://github.com/crate/crate/commit/803846af4c06be3f5b09c4f5c7a3b54877999888", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-02T13:41:40Z", "type": "forcePushed"}, {"oid": "92b78b3fe2a2bc96c2b18d4fbd8c42a4b9fa9d76", "url": "https://github.com/crate/crate/commit/92b78b3fe2a2bc96c2b18d4fbd8c42a4b9fa9d76", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-02T13:48:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM1NDQ3OA==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499354478", "bodyText": "This makes sure file disruption ends after all nodes are stopped.", "author": "mkleen", "createdAt": "2020-10-05T05:38:20Z", "path": "server/src/test/java/org/elasticsearch/test/InternalTestCluster.java", "diffHunk": "@@ -1693,6 +1693,7 @@ public synchronized void fullRestart(RestartCallback callback) throws Exception\n         }\n \n         assert nodesByRoles.values().stream().mapToInt(List::size).sum() == nodes.size();\n+        callback.onAllNodesStopped();", "originalCommit": "92b78b3fe2a2bc96c2b18d4fbd8c42a4b9fa9d76", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c5b6b7597901d9f0b4b07b3a2faa0c6c5b8f3614", "url": "https://github.com/crate/crate/commit/c5b6b7597901d9f0b4b07b3a2faa0c6c5b8f3614", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-05T05:40:08Z", "type": "forcePushed"}, {"oid": "00b983c496ae824e23e9a4a9b9c67733289123c5", "url": "https://github.com/crate/crate/commit/00b983c496ae824e23e9a4a9b9c67733289123c5", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-05T05:47:41Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQyODg5NA==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499428894", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n          \n          \n            \n                                                                   \"from sys.shards where table_name='test'\", null).actionGet();\n          \n          \n            \n                                var response = execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n          \n          \n            \n                                                                   \"from sys.shards where table_name='test'\");", "author": "seut", "createdAt": "2020-10-05T08:33:03Z", "path": "server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.crate.integrationtests.disruption.discovery;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+import io.crate.integrationtests.Setup;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.test.BackgroundIndexer;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.OpenOption;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.FileAttribute;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+@SQLTransportIntegrationTest.Slow\n+public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n+\n+    private static DisruptTranslogFileSystemProvider disruptTranslogFileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installDisruptTranslogFS() {\n+        FileSystem current = FileSystems.getDefault();\n+        disruptTranslogFileSystemProvider = new DisruptTranslogFileSystemProvider(current);\n+        PathUtilsForTesting.installMock(disruptTranslogFileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeDisruptTranslogFS() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    void injectTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(true);\n+    }\n+\n+    @After\n+    void stopTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(false);\n+    }\n+\n+    static class DisruptTranslogFileSystemProvider extends FilterFileSystemProvider {\n+\n+        AtomicBoolean injectFailures = new AtomicBoolean();\n+\n+        DisruptTranslogFileSystemProvider(FileSystem inner) {\n+            super(\"disrupttranslog://\", inner);\n+        }\n+\n+        @Override\n+        public FileChannel newFileChannel(Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n+            if (injectFailures.get() && path.toString().endsWith(\".ckp\")) {\n+                // prevents checkpoint file to be updated\n+                throw new IOException(\"fake IOException\");\n+            }\n+            return super.newFileChannel(path, options, attrs);\n+        }\n+    }\n+\n+    /**\n+     * This test checks that all operations below the global checkpoint are properly persisted.\n+     * It simulates a full power outage by preventing translog checkpoint files to be written and restart the cluster. This means that\n+     * all un-fsynced data will be lost.\n+     */\n+    public void testGlobalCheckpointIsSafe() throws Exception {\n+        startCluster(rarely() ? 5 : 3);\n+\n+        var numberOfShards = 1 + randomInt(2);\n+        var numberOfReplicas = randomInt(2);\n+\n+        execute(\"create table test (id int primary key, data string) clustered into \" + numberOfShards + \" shards \" +\n+                            \"with (number_of_replicas = ?)\", new Object[] {numberOfReplicas});\n+\n+        ensureGreen();\n+\n+        AtomicBoolean stopGlobalCheckpointFetcher = new AtomicBoolean();\n+\n+        Map<Integer, Long> shardToGcp = new ConcurrentHashMap<>();\n+        for (int i = 0; i < numberOfShards; i++) {\n+            shardToGcp.put(i, SequenceNumbers.NO_OPS_PERFORMED);\n+        }\n+        final Thread globalCheckpointSampler = new Thread(() -> {\n+            while (stopGlobalCheckpointFetcher.get() == false) {\n+                try {\n+                    var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n+                                                       \"from sys.shards where table_name='test'\", null).actionGet();", "originalCommit": "00b983c496ae824e23e9a4a9b9c67733289123c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQzMDI5Nw==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499430297", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                   execute(\"select distinct id, seq_no_stats['max_seq_no'] from sys.shards where table_name='test' and \" +\n          \n          \n            \n                    execute(\"select distinct id, seq_no_stats['max_seq_no'] from sys.shards where table_name='test' and \" +", "author": "seut", "createdAt": "2020-10-05T08:35:31Z", "path": "server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.crate.integrationtests.disruption.discovery;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+import io.crate.integrationtests.Setup;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.test.BackgroundIndexer;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.OpenOption;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.FileAttribute;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+@SQLTransportIntegrationTest.Slow\n+public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n+\n+    private static DisruptTranslogFileSystemProvider disruptTranslogFileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installDisruptTranslogFS() {\n+        FileSystem current = FileSystems.getDefault();\n+        disruptTranslogFileSystemProvider = new DisruptTranslogFileSystemProvider(current);\n+        PathUtilsForTesting.installMock(disruptTranslogFileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeDisruptTranslogFS() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    void injectTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(true);\n+    }\n+\n+    @After\n+    void stopTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(false);\n+    }\n+\n+    static class DisruptTranslogFileSystemProvider extends FilterFileSystemProvider {\n+\n+        AtomicBoolean injectFailures = new AtomicBoolean();\n+\n+        DisruptTranslogFileSystemProvider(FileSystem inner) {\n+            super(\"disrupttranslog://\", inner);\n+        }\n+\n+        @Override\n+        public FileChannel newFileChannel(Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n+            if (injectFailures.get() && path.toString().endsWith(\".ckp\")) {\n+                // prevents checkpoint file to be updated\n+                throw new IOException(\"fake IOException\");\n+            }\n+            return super.newFileChannel(path, options, attrs);\n+        }\n+    }\n+\n+    /**\n+     * This test checks that all operations below the global checkpoint are properly persisted.\n+     * It simulates a full power outage by preventing translog checkpoint files to be written and restart the cluster. This means that\n+     * all un-fsynced data will be lost.\n+     */\n+    public void testGlobalCheckpointIsSafe() throws Exception {\n+        startCluster(rarely() ? 5 : 3);\n+\n+        var numberOfShards = 1 + randomInt(2);\n+        var numberOfReplicas = randomInt(2);\n+\n+        execute(\"create table test (id int primary key, data string) clustered into \" + numberOfShards + \" shards \" +\n+                            \"with (number_of_replicas = ?)\", new Object[] {numberOfReplicas});\n+\n+        ensureGreen();\n+\n+        AtomicBoolean stopGlobalCheckpointFetcher = new AtomicBoolean();\n+\n+        Map<Integer, Long> shardToGcp = new ConcurrentHashMap<>();\n+        for (int i = 0; i < numberOfShards; i++) {\n+            shardToGcp.put(i, SequenceNumbers.NO_OPS_PERFORMED);\n+        }\n+        final Thread globalCheckpointSampler = new Thread(() -> {\n+            while (stopGlobalCheckpointFetcher.get() == false) {\n+                try {\n+                    var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n+                                                       \"from sys.shards where table_name='test'\", null).actionGet();\n+                    for (var row : response.rows()) {\n+                        final int shardId = (int) row[0];\n+                        final long globalCheckpoint = (long) row[1];\n+                        shardToGcp.compute(shardId, (i, v) -> Math.max(v, globalCheckpoint));\n+                    }\n+                } catch (Exception e) {\n+                    // ignore\n+                    logger.debug(\"failed to fetch shard stats\", e);\n+                }\n+            }\n+        });\n+\n+        globalCheckpointSampler.start();\n+\n+        try (BackgroundIndexer indexer = new BackgroundIndexer(\"test\", \"data\", sqlExecutor, -1, RandomizedTest.scaledRandomIntBetween(2, 5),\n+                                                               false, random())) {\n+            indexer.setRequestTimeout(TimeValue.ZERO);\n+            indexer.setIgnoreIndexingFailures(true);\n+            indexer.setFailureAssertion(e -> {});\n+            indexer.start(-1);\n+\n+            waitForDocs(randomIntBetween(1, 100), indexer, \"test\");\n+\n+            logger.info(\"injecting failures\");\n+            injectTranslogFailures();\n+            logger.info(\"stopping indexing\");\n+        }\n+\n+        logger.info(\"full cluster restart\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public void onAllNodesStopped() {\n+                logger.info(\"stopping failures\");\n+                stopTranslogFailures();\n+            }\n+\n+        });\n+\n+        stopGlobalCheckpointFetcher.set(true);\n+\n+        logger.info(\"waiting for global checkpoint sampler\");\n+        globalCheckpointSampler.join();\n+\n+        logger.info(\"waiting for green\");\n+        ensureGreen();\n+\n+       execute(\"select distinct id, seq_no_stats['max_seq_no'] from sys.shards where table_name='test' and \" +", "originalCommit": "00b983c496ae824e23e9a4a9b9c67733289123c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQzMjg5NQ==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499432895", "bodyText": "Any reason why this method is not added to the ESIntegTestCase like in the ES upstream?", "author": "seut", "createdAt": "2020-10-05T08:40:20Z", "path": "server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.crate.integrationtests.disruption.discovery;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+import io.crate.integrationtests.Setup;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.test.BackgroundIndexer;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.OpenOption;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.FileAttribute;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+@SQLTransportIntegrationTest.Slow\n+public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n+\n+    private static DisruptTranslogFileSystemProvider disruptTranslogFileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installDisruptTranslogFS() {\n+        FileSystem current = FileSystems.getDefault();\n+        disruptTranslogFileSystemProvider = new DisruptTranslogFileSystemProvider(current);\n+        PathUtilsForTesting.installMock(disruptTranslogFileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeDisruptTranslogFS() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    void injectTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(true);\n+    }\n+\n+    @After\n+    void stopTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(false);\n+    }\n+\n+    static class DisruptTranslogFileSystemProvider extends FilterFileSystemProvider {\n+\n+        AtomicBoolean injectFailures = new AtomicBoolean();\n+\n+        DisruptTranslogFileSystemProvider(FileSystem inner) {\n+            super(\"disrupttranslog://\", inner);\n+        }\n+\n+        @Override\n+        public FileChannel newFileChannel(Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n+            if (injectFailures.get() && path.toString().endsWith(\".ckp\")) {\n+                // prevents checkpoint file to be updated\n+                throw new IOException(\"fake IOException\");\n+            }\n+            return super.newFileChannel(path, options, attrs);\n+        }\n+    }\n+\n+    /**\n+     * This test checks that all operations below the global checkpoint are properly persisted.\n+     * It simulates a full power outage by preventing translog checkpoint files to be written and restart the cluster. This means that\n+     * all un-fsynced data will be lost.\n+     */\n+    public void testGlobalCheckpointIsSafe() throws Exception {\n+        startCluster(rarely() ? 5 : 3);\n+\n+        var numberOfShards = 1 + randomInt(2);\n+        var numberOfReplicas = randomInt(2);\n+\n+        execute(\"create table test (id int primary key, data string) clustered into \" + numberOfShards + \" shards \" +\n+                            \"with (number_of_replicas = ?)\", new Object[] {numberOfReplicas});\n+\n+        ensureGreen();\n+\n+        AtomicBoolean stopGlobalCheckpointFetcher = new AtomicBoolean();\n+\n+        Map<Integer, Long> shardToGcp = new ConcurrentHashMap<>();\n+        for (int i = 0; i < numberOfShards; i++) {\n+            shardToGcp.put(i, SequenceNumbers.NO_OPS_PERFORMED);\n+        }\n+        final Thread globalCheckpointSampler = new Thread(() -> {\n+            while (stopGlobalCheckpointFetcher.get() == false) {\n+                try {\n+                    var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n+                                                       \"from sys.shards where table_name='test'\", null).actionGet();\n+                    for (var row : response.rows()) {\n+                        final int shardId = (int) row[0];\n+                        final long globalCheckpoint = (long) row[1];\n+                        shardToGcp.compute(shardId, (i, v) -> Math.max(v, globalCheckpoint));\n+                    }\n+                } catch (Exception e) {\n+                    // ignore\n+                    logger.debug(\"failed to fetch shard stats\", e);\n+                }\n+            }\n+        });\n+\n+        globalCheckpointSampler.start();\n+\n+        try (BackgroundIndexer indexer = new BackgroundIndexer(\"test\", \"data\", sqlExecutor, -1, RandomizedTest.scaledRandomIntBetween(2, 5),\n+                                                               false, random())) {\n+            indexer.setRequestTimeout(TimeValue.ZERO);\n+            indexer.setIgnoreIndexingFailures(true);\n+            indexer.setFailureAssertion(e -> {});\n+            indexer.start(-1);\n+\n+            waitForDocs(randomIntBetween(1, 100), indexer, \"test\");\n+\n+            logger.info(\"injecting failures\");\n+            injectTranslogFailures();\n+            logger.info(\"stopping indexing\");\n+        }\n+\n+        logger.info(\"full cluster restart\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public void onAllNodesStopped() {\n+                logger.info(\"stopping failures\");\n+                stopTranslogFailures();\n+            }\n+\n+        });\n+\n+        stopGlobalCheckpointFetcher.set(true);\n+\n+        logger.info(\"waiting for global checkpoint sampler\");\n+        globalCheckpointSampler.join();\n+\n+        logger.info(\"waiting for green\");\n+        ensureGreen();\n+\n+       execute(\"select distinct id, seq_no_stats['max_seq_no'] from sys.shards where table_name='test' and \" +\n+               \"routing_state in ('STARTED', 'RELOCATING')\");\n+\n+        assertThat(response.rowCount(), is((long) numberOfShards));\n+\n+        for (var row : response.rows()) {\n+            final int shardId = (int) row[0];\n+            final long maxSeqNo = (long) row[1];\n+            assertThat(maxSeqNo, greaterThanOrEqualTo(shardToGcp.get(shardId)));\n+        }\n+    }\n+\n+    /**\n+     * Waits until at least a give number of document is visible for searchers\n+     *\n+     * @param numDocs number of documents to wait for\n+     * @param indexer a {@link org.elasticsearch.test.BackgroundIndexer}. It will be first checked for documents indexed.\n+     *                This saves on unneeded searches.\n+     */\n+    public void waitForDocs(final long numDocs, final BackgroundIndexer indexer, String table) throws Exception {", "originalCommit": "00b983c496ae824e23e9a4a9b9c67733289123c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MDAzOA==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499440038", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                                var response = sqlExecutor.execute(insert, insertData).actionGet();\n          \n          \n            \n                                                var response = sqlExecutor.exec(insert, insertData);\n          \n      \n    \n    \n  \n\nOtherwise, please also keep in mind to always add proper timeouts to any blocking calls in general to let if fail early.", "author": "seut", "createdAt": "2020-10-05T08:51:57Z", "path": "server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.test;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import com.carrotsearch.randomizedtesting.generators.RandomNumbers;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.testing.SQLTransportExecutor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.logging.log4j.util.Supplier;\n+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;\n+import org.junit.Assert;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Locale;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class BackgroundIndexer implements AutoCloseable {\n+\n+    private final Logger logger = LogManager.getLogger(getClass());\n+\n+    final Thread[] writers;\n+    final SQLTransportExecutor sqlExecutor;\n+    final CountDownLatch stopLatch;\n+    final Collection<Exception> failures = new ArrayList<>();\n+    final AtomicBoolean stop = new AtomicBoolean(false);\n+    final AtomicLong idGenerator = new AtomicLong();\n+    final CountDownLatch startLatch = new CountDownLatch(1);\n+    final AtomicBoolean hasBudget = new AtomicBoolean(false); // when set to true, writers will acquire writes from a semaphore\n+    final Semaphore availableBudget = new Semaphore(0);\n+    private final Set<String> ids = ConcurrentCollections.newConcurrentSet();\n+    private volatile Consumer<Exception> failureAssertion = null;\n+\n+    volatile int minFieldSize = 10;\n+    volatile int maxFieldSize = 140;\n+\n+    /**\n+     * Start indexing in the background using a random number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table     table name to write data\n+     * @param column    column name to write data\n+     * @param client    client to use\n+     * @param numOfDocs number of document to index before pausing. Set to -1 to have no limit.\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs) {\n+        this(table, column, client, numOfDocs, RandomizedTest.scaledRandomIntBetween(2, 5));\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param client      client to use\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs, final int writerCount) {\n+        this(table, column, client, numOfDocs, writerCount, true, null);\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     * @param autoStart   set to true to start indexing as soon as all threads have been created.\n+     * @param random      random instance to use\n+     */\n+    public BackgroundIndexer(final String table, final String column, final SQLTransportExecutor sqlExecutor, final int numOfDocs, final int writerCount,\n+                             boolean autoStart, Random random) {\n+        if (random == null) {\n+            random = RandomizedTest.getRandom();\n+        }\n+        this.sqlExecutor = sqlExecutor;\n+        writers = new Thread[writerCount];\n+        stopLatch = new CountDownLatch(writers.length);\n+        logger.info(\"--> creating {} indexing threads (auto start: [{}], numOfDocs: [{}])\", writerCount, autoStart, numOfDocs);\n+        for (int i = 0; i < writers.length; i++) {\n+            final int indexerId = i;\n+            final boolean batch = random.nextBoolean();\n+            final Random threadRandom = new Random(random.nextLong());\n+            writers[i] = new Thread() {\n+                @Override\n+                public void run() {\n+                    long id = -1;\n+                    try {\n+                        startLatch.await();\n+                        logger.info(\"**** starting indexing thread {}\", indexerId);\n+                        while (!stop.get()) {\n+                            if (batch) {\n+                                int batchSize = threadRandom.nextInt(20) + 1;\n+                                if (hasBudget.get()) {\n+                                    // always try to get at least one\n+                                    batchSize = Math.max(Math.min(batchSize, availableBudget.availablePermits()), 1);\n+                                    if (!availableBudget.tryAcquire(batchSize, 250, TimeUnit.MILLISECONDS)) {\n+                                        // time out -> check if we have to stop.\n+                                        continue;\n+                                    }\n+                                }\n+                                var insertData = new Object[batchSize];\n+                                for (int i = 0; i < batchSize; i++) {\n+                                    insertData[i] = generateValues(idGenerator.incrementAndGet(), threadRandom);\n+                                }\n+                                try {\n+                                    var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n+                                    var response = sqlExecutor.execute(insert, insertData).actionGet();", "originalCommit": "00b983c496ae824e23e9a4a9b9c67733289123c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MDI1OQ==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499440259", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                                var response = sqlExecutor.execute(insert, generateValues(idGenerator.incrementAndGet(), threadRandom)).actionGet();\n          \n          \n            \n                                                var response = sqlExecutor.exec(insert, generateValues(idGenerator.incrementAndGet(), threadRandom));", "author": "seut", "createdAt": "2020-10-05T08:52:18Z", "path": "server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.test;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import com.carrotsearch.randomizedtesting.generators.RandomNumbers;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.testing.SQLTransportExecutor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.logging.log4j.util.Supplier;\n+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;\n+import org.junit.Assert;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Locale;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class BackgroundIndexer implements AutoCloseable {\n+\n+    private final Logger logger = LogManager.getLogger(getClass());\n+\n+    final Thread[] writers;\n+    final SQLTransportExecutor sqlExecutor;\n+    final CountDownLatch stopLatch;\n+    final Collection<Exception> failures = new ArrayList<>();\n+    final AtomicBoolean stop = new AtomicBoolean(false);\n+    final AtomicLong idGenerator = new AtomicLong();\n+    final CountDownLatch startLatch = new CountDownLatch(1);\n+    final AtomicBoolean hasBudget = new AtomicBoolean(false); // when set to true, writers will acquire writes from a semaphore\n+    final Semaphore availableBudget = new Semaphore(0);\n+    private final Set<String> ids = ConcurrentCollections.newConcurrentSet();\n+    private volatile Consumer<Exception> failureAssertion = null;\n+\n+    volatile int minFieldSize = 10;\n+    volatile int maxFieldSize = 140;\n+\n+    /**\n+     * Start indexing in the background using a random number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table     table name to write data\n+     * @param column    column name to write data\n+     * @param client    client to use\n+     * @param numOfDocs number of document to index before pausing. Set to -1 to have no limit.\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs) {\n+        this(table, column, client, numOfDocs, RandomizedTest.scaledRandomIntBetween(2, 5));\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param client      client to use\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs, final int writerCount) {\n+        this(table, column, client, numOfDocs, writerCount, true, null);\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     * @param autoStart   set to true to start indexing as soon as all threads have been created.\n+     * @param random      random instance to use\n+     */\n+    public BackgroundIndexer(final String table, final String column, final SQLTransportExecutor sqlExecutor, final int numOfDocs, final int writerCount,\n+                             boolean autoStart, Random random) {\n+        if (random == null) {\n+            random = RandomizedTest.getRandom();\n+        }\n+        this.sqlExecutor = sqlExecutor;\n+        writers = new Thread[writerCount];\n+        stopLatch = new CountDownLatch(writers.length);\n+        logger.info(\"--> creating {} indexing threads (auto start: [{}], numOfDocs: [{}])\", writerCount, autoStart, numOfDocs);\n+        for (int i = 0; i < writers.length; i++) {\n+            final int indexerId = i;\n+            final boolean batch = random.nextBoolean();\n+            final Random threadRandom = new Random(random.nextLong());\n+            writers[i] = new Thread() {\n+                @Override\n+                public void run() {\n+                    long id = -1;\n+                    try {\n+                        startLatch.await();\n+                        logger.info(\"**** starting indexing thread {}\", indexerId);\n+                        while (!stop.get()) {\n+                            if (batch) {\n+                                int batchSize = threadRandom.nextInt(20) + 1;\n+                                if (hasBudget.get()) {\n+                                    // always try to get at least one\n+                                    batchSize = Math.max(Math.min(batchSize, availableBudget.availablePermits()), 1);\n+                                    if (!availableBudget.tryAcquire(batchSize, 250, TimeUnit.MILLISECONDS)) {\n+                                        // time out -> check if we have to stop.\n+                                        continue;\n+                                    }\n+                                }\n+                                var insertData = new Object[batchSize];\n+                                for (int i = 0; i < batchSize; i++) {\n+                                    insertData[i] = generateValues(idGenerator.incrementAndGet(), threadRandom);\n+                                }\n+                                try {\n+                                    var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n+                                    var response = sqlExecutor.execute(insert, insertData).actionGet();\n+                                    for (var generatedId : response.rows()[0]) {\n+                                        ids.add((String) generatedId);\n+                                    }\n+                                } catch (Exception e) {\n+                                    if (ignoreIndexingFailures == false) {\n+                                        throw e;\n+                                    }\n+                                }\n+                            } else {\n+                                if (hasBudget.get() && !availableBudget.tryAcquire(250, TimeUnit.MILLISECONDS)) {\n+                                    // time out -> check if we have to stop.\n+                                    continue;\n+                                }\n+                                try {\n+                                    var insert = String.format(Locale.ENGLISH, \"insert into %s (id, %s) values(?, ?) returning _id\", table, column);\n+                                    var response = sqlExecutor.execute(insert, generateValues(idGenerator.incrementAndGet(), threadRandom)).actionGet();", "originalCommit": "00b983c496ae824e23e9a4a9b9c67733289123c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MTY2Ng==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499441666", "bodyText": "In the upstream code, this line is above the assertion line. Any reason for this change?", "author": "seut", "createdAt": "2020-10-05T08:54:38Z", "path": "server/src/test/java/org/elasticsearch/test/InternalTestCluster.java", "diffHunk": "@@ -1693,6 +1693,7 @@ public synchronized void fullRestart(RestartCallback callback) throws Exception\n         }\n \n         assert nodesByRoles.values().stream().mapToInt(List::size).sum() == nodes.size();\n+        callback.onAllNodesStopped();", "originalCommit": "00b983c496ae824e23e9a4a9b9c67733289123c5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ2Nzc0Mg==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499467742", "bodyText": "\ud83d\udc4d  My fault, seems like i was on the wrong state of elasticsearch, there was no assert at all when i looked at it.", "author": "mkleen", "createdAt": "2020-10-05T09:35:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MTY2Ng=="}], "type": "inlineReview"}, {"oid": "bd4045719c3e8fc23f35a833ace4263e92fc212c", "url": "https://github.com/crate/crate/commit/bd4045719c3e8fc23f35a833ace4263e92fc212c", "message": "Fix SqlExecutor execution", "committedDate": "2020-10-05T11:14:00Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU1MTc5MQ==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499551791", "bodyText": "could use execute(..) instead. sorry my fault, wrong suggestion by me ;)", "author": "seut", "createdAt": "2020-10-05T12:12:33Z", "path": "server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.crate.integrationtests.disruption.discovery;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.test.BackgroundIndexer;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.OpenOption;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.FileAttribute;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+@SQLTransportIntegrationTest.Slow\n+public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n+\n+    private static DisruptTranslogFileSystemProvider disruptTranslogFileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installDisruptTranslogFS() {\n+        FileSystem current = FileSystems.getDefault();\n+        disruptTranslogFileSystemProvider = new DisruptTranslogFileSystemProvider(current);\n+        PathUtilsForTesting.installMock(disruptTranslogFileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeDisruptTranslogFS() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    void injectTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(true);\n+    }\n+\n+    @After\n+    void stopTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(false);\n+    }\n+\n+    static class DisruptTranslogFileSystemProvider extends FilterFileSystemProvider {\n+\n+        AtomicBoolean injectFailures = new AtomicBoolean();\n+\n+        DisruptTranslogFileSystemProvider(FileSystem inner) {\n+            super(\"disrupttranslog://\", inner);\n+        }\n+\n+        @Override\n+        public FileChannel newFileChannel(Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n+            if (injectFailures.get() && path.toString().endsWith(\".ckp\")) {\n+                // prevents checkpoint file to be updated\n+                throw new IOException(\"fake IOException\");\n+            }\n+            return super.newFileChannel(path, options, attrs);\n+        }\n+    }\n+\n+    /**\n+     * This test checks that all operations below the global checkpoint are properly persisted.\n+     * It simulates a full power outage by preventing translog checkpoint files to be written and restart the cluster. This means that\n+     * all un-fsynced data will be lost.\n+     */\n+    public void testGlobalCheckpointIsSafe() throws Exception {\n+        startCluster(rarely() ? 5 : 3);\n+\n+        var numberOfShards = 1 + randomInt(2);\n+        var numberOfReplicas = randomInt(2);\n+\n+        execute(\"create table test (id int primary key, data string) clustered into \" + numberOfShards + \" shards \" +\n+                            \"with (number_of_replicas = ?)\", new Object[] {numberOfReplicas});\n+\n+        ensureGreen();\n+\n+        AtomicBoolean stopGlobalCheckpointFetcher = new AtomicBoolean();\n+\n+        Map<Integer, Long> shardToGcp = new ConcurrentHashMap<>();\n+        for (int i = 0; i < numberOfShards; i++) {\n+            shardToGcp.put(i, SequenceNumbers.NO_OPS_PERFORMED);\n+        }\n+        final Thread globalCheckpointSampler = new Thread(() -> {\n+            while (stopGlobalCheckpointFetcher.get() == false) {\n+                try {\n+                    var response = sqlExecutor.exec(\"select id, seq_no_stats['global_checkpoint'] \" +", "originalCommit": "8febaf2933a2b9f4b8cfe7a86d24a324b62680cd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU1MzAwNQ==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499553005", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                                var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n          \n          \n            \n                                                var insert = String.format(Locale.ENGLISH, \"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);", "author": "seut", "createdAt": "2020-10-05T12:14:40Z", "path": "server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java", "diffHunk": "@@ -0,0 +1,323 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.test;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import com.carrotsearch.randomizedtesting.generators.RandomNumbers;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.testing.SQLTransportExecutor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.logging.log4j.util.Supplier;\n+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;\n+import org.junit.Assert;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Locale;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class BackgroundIndexer implements AutoCloseable {\n+\n+    private final Logger logger = LogManager.getLogger(getClass());\n+\n+    final Thread[] writers;\n+    final SQLTransportExecutor sqlExecutor;\n+    final CountDownLatch stopLatch;\n+    final Collection<Exception> failures = new ArrayList<>();\n+    final AtomicBoolean stop = new AtomicBoolean(false);\n+    final AtomicLong idGenerator = new AtomicLong();\n+    final CountDownLatch startLatch = new CountDownLatch(1);\n+    final AtomicBoolean hasBudget = new AtomicBoolean(false); // when set to true, writers will acquire writes from a semaphore\n+    final Semaphore availableBudget = new Semaphore(0);\n+    private final Set<String> ids = ConcurrentCollections.newConcurrentSet();\n+    private volatile Consumer<Exception> failureAssertion = null;\n+    final String table;\n+\n+    volatile int minFieldSize = 10;\n+    volatile int maxFieldSize = 140;\n+\n+    /**\n+     * Start indexing in the background using a random number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table     table name to write data\n+     * @param column    column name to write data\n+     * @param client    client to use\n+     * @param numOfDocs number of document to index before pausing. Set to -1 to have no limit.\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs) {\n+        this(table, column, client, numOfDocs, RandomizedTest.scaledRandomIntBetween(2, 5));\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param client      client to use\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs, final int writerCount) {\n+        this(table, column, client, numOfDocs, writerCount, true, null);\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     * @param autoStart   set to true to start indexing as soon as all threads have been created.\n+     * @param random      random instance to use\n+     */\n+    public BackgroundIndexer(final String table, final String column, final SQLTransportExecutor sqlExecutor, final int numOfDocs, final int writerCount,\n+                             boolean autoStart, Random random) {\n+        if (random == null) {\n+            random = RandomizedTest.getRandom();\n+        }\n+        this.table = table;\n+        this.sqlExecutor = sqlExecutor;\n+        writers = new Thread[writerCount];\n+        stopLatch = new CountDownLatch(writers.length);\n+        logger.info(\"--> creating {} indexing threads (auto start: [{}], numOfDocs: [{}])\", writerCount, autoStart, numOfDocs);\n+        for (int i = 0; i < writers.length; i++) {\n+            final int indexerId = i;\n+            final boolean batch = random.nextBoolean();\n+            final Random threadRandom = new Random(random.nextLong());\n+            writers[i] = new Thread() {\n+                @Override\n+                public void run() {\n+                    long id = -1;\n+                    try {\n+                        startLatch.await();\n+                        logger.info(\"**** starting indexing thread {}\", indexerId);\n+                        while (!stop.get()) {\n+                            if (batch) {\n+                                int batchSize = threadRandom.nextInt(20) + 1;\n+                                if (hasBudget.get()) {\n+                                    // always try to get at least one\n+                                    batchSize = Math.max(Math.min(batchSize, availableBudget.availablePermits()), 1);\n+                                    if (!availableBudget.tryAcquire(batchSize, 250, TimeUnit.MILLISECONDS)) {\n+                                        // time out -> check if we have to stop.\n+                                        continue;\n+                                    }\n+                                }\n+                                var insertData = new Object[batchSize];\n+                                for (int i = 0; i < batchSize; i++) {\n+                                    insertData[i] = generateValues(idGenerator.incrementAndGet(), threadRandom);\n+                                }\n+                                try {\n+                                    var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);", "originalCommit": "8febaf2933a2b9f4b8cfe7a86d24a324b62680cd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8cebc7d642107e5dd70383b127a3b276785e4e8e", "url": "https://github.com/crate/crate/commit/8cebc7d642107e5dd70383b127a3b276785e4e8e", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-05T12:26:46Z", "type": "commit"}, {"oid": "8cebc7d642107e5dd70383b127a3b276785e4e8e", "url": "https://github.com/crate/crate/commit/8cebc7d642107e5dd70383b127a3b276785e4e8e", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-05T12:26:46Z", "type": "forcePushed"}]}