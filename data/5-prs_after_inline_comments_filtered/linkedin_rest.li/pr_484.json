{"pr_number": 484, "pr_title": "Overload Failure Retry Implementation", "pr_createdAt": "2020-11-21T00:32:50Z", "pr_url": "https://github.com/linkedin/rest.li/pull/484", "timeline": [{"oid": "953e75ce34050540b34cbe9ca271e2c00da8a383", "url": "https://github.com/linkedin/rest.li/commit/953e75ce34050540b34cbe9ca271e2c00da8a383", "message": "Overload Failure Retry Implementation", "committedDate": "2020-11-20T23:22:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEwOTYxNA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529109614", "bodyText": "Should we break the loop once it is true?", "author": "rachelhanhan", "createdAt": "2020-11-24T01:30:17Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +330,33 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+      boolean hasRetriableRequestException = false;\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          hasRetriableRequestException = true;\n+\n+          if (((RetriableRequestException) throwable).getDoNotRetryOverride())\n+          {\n+            return false;\n+          }", "originalCommit": "953e75ce34050540b34cbe9ca271e2c00da8a383", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc3MjM5Nw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529772397", "bodyText": "Or just return true if the override isn't present", "author": "bbarkley", "createdAt": "2020-11-24T17:59:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEwOTYxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyMjE2NQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r532822165", "bodyText": "Fixed", "author": "rickzx", "createdAt": "2020-11-30T18:49:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEwOTYxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTExMjY1NA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529112654", "bodyText": "I kind of forgot what's the granularity of the D2Client, if a host is making calls to 50 different downstream d2Services, does it use one instance of D2Client or 50 instances of D2Clients?\nThe further questions is that what's  the granularity of the client side retry ratio? Is it one ratio for each d2Service, or a downstream service can be penalized by another downstream?", "author": "rachelhanhan", "createdAt": "2020-11-24T01:39:50Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/D2ClientBuilder.java", "diffHunk": "@@ -146,6 +146,8 @@ public D2Client build()\n                   _config._executorService,\n                   _config.retry,\n                   _config.retryLimit,\n+                  _config.maxClientRequestRetryRatio,", "originalCommit": "953e75ce34050540b34cbe9ca271e2c00da8a383", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTExMzg4Nw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529113887", "bodyText": "A follow up question is that should this be a client-defined ratio or server-defined ratio in lps d2, pros and cons?", "author": "rachelhanhan", "createdAt": "2020-11-24T01:43:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTExMjY1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyNDg2OA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r532824868", "bodyText": "There's only one D2Client per war making calls to different downstream service endpoints. I think it makes more sense to track retry ratio individually for each downstream endpoint. The latest commit uses a map to track retry info for each extracted service name.\nThe ratio should be defined by the server in the TransportClientProperties, because service owners have the best knowledge of how much retry traffic is allowed. This is also reflected in the latest commit.", "author": "rickzx", "createdAt": "2020-11-30T18:54:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTExMjY1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTExNzcwNw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529117707", "bodyText": "Let's add java doc for this, so that we know what this override means", "author": "rachelhanhan", "createdAt": "2020-11-24T01:49:33Z", "path": "r2-core/src/main/java/com/linkedin/r2/RetriableRequestException.java", "diffHunk": "@@ -66,4 +68,14 @@ public RetriableRequestException(Throwable cause)\n   {\n     super(cause);\n   }\n+\n+  public void setDoNotRetryOverride(boolean doNotRetryOverride)", "originalCommit": "953e75ce34050540b34cbe9ca271e2c00da8a383", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyNDk4Nw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r532824987", "bodyText": "Added", "author": "rickzx", "createdAt": "2020-11-30T18:54:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTExNzcwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEyNzI3Nw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529127277", "bodyText": "Should we only obtain the lock if we hit the update time? Otherwise we obtain the lock on each individual request", "author": "rachelhanhan", "createdAt": "2020-11-24T01:59:36Z", "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -74,14 +129,132 @@ public void onStreamError(Throwable ex,\n     {\n       if (cause instanceof RetriableRequestException)\n       {\n+        updateRetryDecision();\n+\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_doNotRetry)\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n+        else\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        }\n         break;\n       }\n       cause = cause.getCause();\n     }\n \n     nextFilter.onError(ex, requestContext, wireAttrs);\n   }\n+\n+  private int getRetryAttempts(Request req)\n+  {\n+    String retryAttemptsHeader = req.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    return retryAttemptsHeader == null ? 0 : Integer.parseInt(retryAttemptsHeader);\n+  }\n+\n+  private void updateRetryDecision()\n+  {\n+    long currentTime = _clock.currentTimeMillis();\n+\n+    synchronized (_lock)\n+    {\n+      // Check if the current interval is stale\n+      if (currentTime >= _lastRollOverTime + _updateIntervalMs)", "originalCommit": "953e75ce34050540b34cbe9ca271e2c00da8a383", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyNjQ2OA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r532826468", "bodyText": "The lock is to protect two threads from entering the critical section simultaneously. If we put the lock inside the \"if\" block, then two threads might both pass the staleness check and begin to update the tracker. Anyways, I rewrote the synchronization logic by centralizing it in the tracker. Please take a look.", "author": "rickzx", "createdAt": "2020-11-30T18:57:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEyNzI3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTcyOTg5Nw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529729897", "bodyText": "More readable IMO:\npublic static final long DEFAULT_UPDATE_INTERVAL_MS = TimeUnit.SECONDS(5).toMillis();", "author": "bbarkley", "createdAt": "2020-11-24T16:55:02Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -59,15 +62,39 @@\n  */\n public class RetryClient extends D2ClientDelegator\n {\n+  public static final long DEFAULT_UPDATE_INTERVAL_MS = 5000L;", "originalCommit": "953e75ce34050540b34cbe9ca271e2c00da8a383", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyNjYzNg==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r532826636", "bodyText": "Fixed", "author": "rickzx", "createdAt": "2020-11-30T18:57:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTcyOTg5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc2Mzk4Ng==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529763986", "bodyText": "If the number of retry attempts is zero can we omit this header? Same for below.", "author": "bbarkley", "createdAt": "2020-11-24T17:45:57Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -154,11 +216,14 @@ public void onSuccess(ByteString result)\n     }\n \n     @Override\n-    public boolean doRetryRequest(StreamRequest request, RequestContext context)\n+    public boolean doRetryRequest(StreamRequest request, RequestContext context, int numberOfRetryAttempts)\n     {\n       if (_recorded == true && _content != null)\n       {\n-        final StreamRequest newRequest = request.builder().build(EntityStreams.newEntityStream(new ByteStringWriter(_content)));\n+        final StreamRequest newRequest = request.builder()\n+            .addHeaderValue(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS, Integer.toString(numberOfRetryAttempts))", "originalCommit": "953e75ce34050540b34cbe9ca271e2c00da8a383", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgzMTQwMA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r532831400", "bodyText": "Actually, this function will only be called when the request is being retried, so the attempt here is at least 1. But I made another change in restRequest and streamRequest to still send the header when the attempt is zero. See below", "author": "rickzx", "createdAt": "2020-11-30T19:05:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc2Mzk4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc2NTgxNw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529765817", "bodyText": "Would suggest changing \"happens\" to happened or occurred", "author": "bbarkley", "createdAt": "2020-11-24T17:48:51Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -234,9 +303,16 @@ public void onError(Throwable e)\n             int attempts = exclusionSet.size();\n             if (attempts <= _limit)\n             {\n-              LOG.warn(\"A retriable exception happens. Going to retry. This is attempt {}. Current exclusion set: \",\n-                  attempts, \". Current exclusion set: \" + exclusionSet);\n-              retry = doRetryRequest(_request, _context);\n+              if (isBelowClientRetryRatio())\n+              {\n+                LOG.warn(\"A retriable exception happens. Going to retry. This is attempt {}. Current exclusion set: {}\",", "originalCommit": "953e75ce34050540b34cbe9ca271e2c00da8a383", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgzMTUyOA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r532831528", "bodyText": "Fixed", "author": "rickzx", "createdAt": "2020-11-30T19:05:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc2NTgxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc5MTAzNA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529791034", "bodyText": "You're modifying the exception here, but not doing anything with it. Is this supposed to call nextFilter.onError?", "author": "bbarkley", "createdAt": "2020-11-24T18:28:24Z", "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ClientRetryFilter.java", "diffHunk": "@@ -73,9 +74,13 @@ public void onStreamError(Throwable ex,\n     {\n       nextFilter.onError(new RetriableRequestException(retryAttr), requestContext, wireAttrs);\n     }\n-    else\n-    {\n-      nextFilter.onError(ex, requestContext, wireAttrs);\n+    else {\n+      Throwable[] throwables = ExceptionUtils.getThrowables(ex);\n+      for (Throwable throwable : throwables) {\n+        if (throwable instanceof RetriableRequestException) {\n+          ((RetriableRequestException) throwable).setDoNotRetryOverride(true);", "originalCommit": "953e75ce34050540b34cbe9ca271e2c00da8a383", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgzMTYxMA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r532831610", "bodyText": "Fixed", "author": "rickzx", "createdAt": "2020-11-30T19:05:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc5MTAzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc5MTk2Ng==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529791966", "bodyText": "Consider TimeUnit.SECONDS.toMillis(5)", "author": "bbarkley", "createdAt": "2020-11-24T18:29:56Z", "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -45,6 +49,57 @@\n public class ServerRetryFilter implements RestFilter, StreamFilter\n {\n   private static final Logger LOG = LoggerFactory.getLogger(ServerRetryFilter.class);\n+  private static final int DEFAULT_RETRY_LIMIT = 3;\n+  private static final long DEFAULT_UPDATE_INTERVAL_MS = 5000L;", "originalCommit": "953e75ce34050540b34cbe9ca271e2c00da8a383", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgzMTY2Ng==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r532831666", "bodyText": "Fixed", "author": "rickzx", "createdAt": "2020-11-30T19:05:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc5MTk2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgyNjA3OQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529826079", "bodyText": "We're going to need to think about how to roll this out safely when not all clients have been updated. For example if we have an overload filter and change it to use Retriable exceptions the clients that are on this new version will behave correctly, but any that are on an older version will blindly retry without having a concept of the client retry ratio (and not providing data to the server about often it has had to retry).\nOne option is to use the presence of the number of retry attempts header as an indicator that the client is participating in the retry tracking (and so my earlier suggestion of omitting the header if the value is zero won't work). In that case an overload filter would not want to use the retriable exception unless it knew the client was up to date.\nIt would be nice to centralize that logic here. We could add a flag to RetriableRequestException, or use a different exception to indicate that the client should only retry if it's up to date.", "author": "bbarkley", "createdAt": "2020-11-24T19:28:58Z", "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -74,14 +129,132 @@ public void onStreamError(Throwable ex,\n     {\n       if (cause instanceof RetriableRequestException)\n       {\n+        updateRetryDecision();\n+\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_doNotRetry)\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n+        else\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);", "originalCommit": "953e75ce34050540b34cbe9ca271e2c00da8a383", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgzNTY4OA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r532835688", "bodyText": "Good catch. I agree that using the retry attempts header would be the most efficient way to do this. But adding a flag to RetriableRequestException will not work because the older clients will just ignore the flag and still blindly retry all the RetriableRequestExceptions. One option is to have the overload filter watch for the retry-attempts header, and return RetriableRequestException only when the header is present.", "author": "rickzx", "createdAt": "2020-11-30T19:12:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgyNjA3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTg2MzI3NA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529863274", "bodyText": "If the header is absent should it be counted as zero? This depends on whether we're omitting the header if it's zero or not. If we're not omitting it (and it's looking like we might need to include it to know if the client is participating) then I think we should ignore requests that don't have the header. Otherwise we can get a very skewed version of the situation where clients that aren't tracking things are counted as reporting zero, when they really don't have the data at all and therefore shouldn't be considered in calculating the ratio.", "author": "bbarkley", "createdAt": "2020-11-24T20:38:30Z", "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -74,14 +129,132 @@ public void onStreamError(Throwable ex,\n     {\n       if (cause instanceof RetriableRequestException)\n       {\n+        updateRetryDecision();\n+\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_doNotRetry)\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n+        else\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        }\n         break;\n       }\n       cause = cause.getCause();\n     }\n \n     nextFilter.onError(ex, requestContext, wireAttrs);\n   }\n+\n+  private int getRetryAttempts(Request req)\n+  {\n+    String retryAttemptsHeader = req.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    return retryAttemptsHeader == null ? 0 : Integer.parseInt(retryAttemptsHeader);", "originalCommit": "953e75ce34050540b34cbe9ca271e2c00da8a383", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgzNzM2NA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r532837364", "bodyText": "Agreed. In the latest commit, RetryClient still sends the retry attempts header when the attempt is zero. And the ServerRetryTracker only counts the attempts if the header is present. This way, the tracker more accurately represents the retry ratio of the participating clients.", "author": "rickzx", "createdAt": "2020-11-30T19:15:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTg2MzI3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk4ODQ0Ng==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529988446", "bodyText": "The concurrency safety of this class is a little hard to follow currently. It looks like things should be safe, because you are using this and rollOverStats while holding a lock, and add and rollOverStats are synchronized and should be safe to use together. It would be nice to have the safety be much clearer though.\nOne option would be to make this synchronized for clarity. This will introduce contention with calls to add, but the method is small and should execute quickly, and not very frequently. Another option would be to consolidate this into a rolloverStatsAsNeededAndGetRetryRatio where this class can manage the thread safety and use separate locks to guard the list and the array. Finally you could just document the current behavior with comments.\nWhichever way you choose I'd suggest annotating the class with @ThreadSafe or @NotThreadSafe, and annotate the fields accessed concurrently with @GuardedBy.", "author": "bbarkley", "createdAt": "2020-11-24T23:13:30Z", "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -74,14 +129,132 @@ public void onStreamError(Throwable ex,\n     {\n       if (cause instanceof RetriableRequestException)\n       {\n+        updateRetryDecision();\n+\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_doNotRetry)\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n+        else\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        }\n         break;\n       }\n       cause = cause.getCause();\n     }\n \n     nextFilter.onError(ex, requestContext, wireAttrs);\n   }\n+\n+  private int getRetryAttempts(Request req)\n+  {\n+    String retryAttemptsHeader = req.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    return retryAttemptsHeader == null ? 0 : Integer.parseInt(retryAttemptsHeader);\n+  }\n+\n+  private void updateRetryDecision()\n+  {\n+    long currentTime = _clock.currentTimeMillis();\n+\n+    synchronized (_lock)\n+    {\n+      // Check if the current interval is stale\n+      if (currentTime >= _lastRollOverTime + _updateIntervalMs)\n+      {\n+          // Rollover stale intervals until the current interval is reached\n+          for (long time = currentTime; time >= _lastRollOverTime + _updateIntervalMs; time -= _updateIntervalMs)\n+          {\n+            _serverRetryTracker.rollOverStats();\n+          }\n+\n+          _doNotRetry = _serverRetryTracker.getRetryRatio() > _maxRequestRetryRatio;\n+          _lastRollOverTime = currentTime;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+   * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+   * {@link ServerRetryFilter#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+   * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+   * intervals by aggregating the recorded requests.\n+   */\n+  private static class ServerRetryTracker\n+  {\n+    private final int _retryLimit;\n+    private final int _aggregatedIntervalNum;\n+\n+    private final LinkedList<int[]> _retryAttemptsCounter;\n+    private final int[] _aggregatedRetryAttemptsCounter;\n+\n+    private ServerRetryTracker(int retryLimit, int aggregatedIntervalNum)\n+    {\n+      _retryLimit = retryLimit;\n+      _aggregatedIntervalNum = aggregatedIntervalNum;\n+\n+      _aggregatedRetryAttemptsCounter = new int[_retryLimit + 1];\n+      _retryAttemptsCounter = new LinkedList<>();\n+      _retryAttemptsCounter.add(new int[_retryLimit + 1]);\n+    }\n+\n+    public synchronized void add(int numberOfRetryAttempts)\n+    {\n+      if (numberOfRetryAttempts <= _retryLimit)\n+      {\n+        _retryAttemptsCounter.getLast()[numberOfRetryAttempts] += 1;\n+      } else\n+      {\n+        LOG.warn(\"Unexpected number of retry attempts: \" + numberOfRetryAttempts + \", current retry limit: \" + _retryLimit);\n+      }\n+    }\n+\n+    public synchronized void rollOverStats()\n+    {\n+      // rollover the current interval to the aggregated counter\n+      int[] intervalToAggregate = _retryAttemptsCounter.getLast();\n+      for (int i = 0; i < _retryLimit; i++)\n+      {\n+        _aggregatedRetryAttemptsCounter[i] += intervalToAggregate[i];\n+      }\n+\n+      if (_retryAttemptsCounter.size() > _aggregatedIntervalNum)\n+      {\n+        // discard the oldest interval\n+        int[] intervalToDiscard = _retryAttemptsCounter.removeFirst();\n+        for (int i = 0; i < _retryLimit; i++)\n+        {\n+          _aggregatedRetryAttemptsCounter[i] -= intervalToDiscard[i];\n+        }\n+      }\n+\n+      // append a new interval\n+      _retryAttemptsCounter.addLast(new int[_retryLimit + 1]);\n+    }\n+\n+    public double getRetryRatio()", "originalCommit": "953e75ce34050540b34cbe9ca271e2c00da8a383", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgzODU2Nw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r532838567", "bodyText": "I rewrote the thread-safety logic in both RetryClient and ServerRetryFilter, and annotated them with @ThreadSafe and @GuardedBy. Hopefully it's cleaner this time.", "author": "rickzx", "createdAt": "2020-11-30T19:17:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk4ODQ0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk4ODkxMw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r529988913", "bodyText": "Suggest changing \"Retries\" to \"Retry\"", "author": "bbarkley", "createdAt": "2020-11-24T23:14:01Z", "path": "r2-core/src/main/java/com/linkedin/r2/transport/http/common/HttpConstants.java", "diffHunk": "@@ -11,6 +11,11 @@\n    */\n   public static final String HEADER_RESPONSE_COMPRESSION_THRESHOLD = \"X-Response-Compression-Threshold\";\n \n+  /**\n+   * Custom header for the number of retries.\n+   */\n+  public static final String HEADER_NUMBER_OF_RETRY_ATTEMPTS = \"X-Number-Of-Retries-Attempts\";", "originalCommit": "953e75ce34050540b34cbe9ca271e2c00da8a383", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgzODY2OQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r532838669", "bodyText": "Fixed", "author": "rickzx", "createdAt": "2020-11-30T19:17:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk4ODkxMw=="}], "type": "inlineReview"}, {"oid": "0f4f1177c51323e20c88dfe4226a87f5a55b2ec8", "url": "https://github.com/linkedin/rest.li/commit/0f4f1177c51323e20c88dfe4226a87f5a55b2ec8", "message": "Enable client-side max retry ratio as a transport client property. Track client-side retry ratio for each downstream endpoint. Centralize thread safety logic. Other minor fixes.", "committedDate": "2020-11-30T18:48:08Z", "type": "forcePushed"}, {"oid": "fae5f6003083cb92f2fc6fc480ee3a86f038946b", "url": "https://github.com/linkedin/rest.li/commit/fae5f6003083cb92f2fc6fc480ee3a86f038946b", "message": "Enable client-side max retry ratio as a transport client property. Track client-side retry ratio for each downstream endpoint. Centralize thread safety logic. Other minor fixes.", "committedDate": "2020-11-30T19:04:03Z", "type": "forcePushed"}, {"oid": "05f72fe3afd7c315b64d9a0eb48a56113e78fecc", "url": "https://github.com/linkedin/rest.li/commit/05f72fe3afd7c315b64d9a0eb48a56113e78fecc", "message": "Enable client-side max retry ratio as a transport client property. Track client-side retry ratio for each downstream endpoint. Centralize thread safety logic. Other minor fixes.", "committedDate": "2020-11-30T19:39:46Z", "type": "commit"}, {"oid": "05f72fe3afd7c315b64d9a0eb48a56113e78fecc", "url": "https://github.com/linkedin/rest.li/commit/05f72fe3afd7c315b64d9a0eb48a56113e78fecc", "message": "Enable client-side max retry ratio as a transport client property. Track client-side retry ratio for each downstream endpoint. Centralize thread safety logic. Other minor fixes.", "committedDate": "2020-11-30T19:39:46Z", "type": "forcePushed"}, {"oid": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "url": "https://github.com/linkedin/rest.li/commit/543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "message": "Add retryCount and retryCountTotal to CallTracker. Make RetryClient use the new CallTracker", "committedDate": "2020-12-05T04:38:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0NjY5Ng==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537846696", "bodyText": "I saw that when we invoke CallTracker.startCall, it's doing a lot more than just counting retry count and total call count, it's also counts the latency, outstanding latency and other stuff. Is this a little bit heavy for retry ratio? Especially that the CallTracker is used again in TrackerClient to track each server host's performance, this may defeat the purpose of CallTracker.", "author": "rachelhanhan", "createdAt": "2020-12-07T21:30:05Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -59,15 +75,31 @@\n  */\n public class RetryClient extends D2ClientDelegator\n {\n+  public static final long DEFAULT_UPDATE_INTERVAL_MS = TimeUnit.SECONDS.toMillis(5);\n   private static final Logger LOG = LoggerFactory.getLogger(RetryClient.class);\n \n+  private final Clock _clock;\n+  private final LoadBalancer _balancer;\n   private final int _limit;\n+  private final long _updateIntervalMs;\n \n-  public RetryClient(D2Client d2Client, int limit)\n+  Map<String, CallTracker> _retryTrackerMap;", "originalCommit": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM2MjU5NQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r540362595", "bodyText": "Addressed. Use a separate light-weight CallTracker for RetryClient. Leave it to container to handle monitoring metrics.", "author": "rickzx", "createdAt": "2020-12-10T17:34:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0NjY5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0ODIxOA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537848218", "bodyText": "Break this loop if one throwable is retriable?", "author": "rachelhanhan", "createdAt": "2020-12-07T21:32:42Z", "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ClientRetryFilter.java", "diffHunk": "@@ -73,8 +74,13 @@ public void onStreamError(Throwable ex,\n     {\n       nextFilter.onError(new RetriableRequestException(retryAttr), requestContext, wireAttrs);\n     }\n-    else\n-    {\n+    else {\n+      Throwable[] throwables = ExceptionUtils.getThrowables(ex);\n+      for (Throwable throwable : throwables) {\n+        if (throwable instanceof RetriableRequestException) {\n+          ((RetriableRequestException) throwable).setDoNotRetryOverride(true);", "originalCommit": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM2MTExNA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r540361114", "bodyText": "Fixed", "author": "rickzx", "createdAt": "2020-12-10T17:32:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0ODIxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg1ODMwNQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537858305", "bodyText": "I'm a little bit confused, what's the purpose of tracking # of retries in TrackerClient here? Each trackerClient corresponds to one server host, do we need to track retry count on server host basis?", "author": "rachelhanhan", "createdAt": "2020-12-07T21:49:46Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/TrackerClientImpl.java", "diffHunk": "@@ -307,4 +309,17 @@ else if (throwable instanceof StreamException)\n     }\n     return false;\n   }\n+\n+  private CallCompletion startCall(Request request)\n+  {\n+    String retryHeader = request.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    if (retryHeader != null && Integer.parseInt(retryHeader) > 0)\n+    {\n+      return _callTracker.startCall(true);\n+    }", "originalCommit": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM2MTAxNQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r540361015", "bodyText": "This change is reverted", "author": "rickzx", "createdAt": "2020-12-10T17:32:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg1ODMwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk2MjU5Nw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537962597", "bodyText": "I'm not that familiar with the different abstraction levels of D2 and if this client could be used by more than one thread. If so this should be a ConcurrentHashMap and for clarity the declared type should be ConcurrentMap", "author": "bbarkley", "createdAt": "2020-12-08T01:30:30Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -59,15 +75,31 @@\n  */\n public class RetryClient extends D2ClientDelegator\n {\n+  public static final long DEFAULT_UPDATE_INTERVAL_MS = TimeUnit.SECONDS.toMillis(5);\n   private static final Logger LOG = LoggerFactory.getLogger(RetryClient.class);\n \n+  private final Clock _clock;\n+  private final LoadBalancer _balancer;\n   private final int _limit;\n+  private final long _updateIntervalMs;\n \n-  public RetryClient(D2Client d2Client, int limit)\n+  Map<String, CallTracker> _retryTrackerMap;\n+\n+  public RetryClient(D2Client d2Client, LoadBalancer balancer, int limit)\n+  {\n+    this(d2Client, balancer, limit, DEFAULT_UPDATE_INTERVAL_MS, SystemClock.instance());\n+  }\n+\n+  public RetryClient(D2Client d2Client, LoadBalancer balancer, int limit, long updateIntervalMs, Clock clock)\n   {\n     super(d2Client);\n+    _balancer = balancer;\n     _limit = limit;\n-    LOG.debug(\"Retry client created with limit set to: \", _limit);\n+    _updateIntervalMs = updateIntervalMs;\n+    _clock = clock;\n+    _retryTrackerMap = new HashMap<>();", "originalCommit": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTgxMzYyMA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r539813620", "bodyText": "Fixed", "author": "rickzx", "createdAt": "2020-12-10T03:17:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk2MjU5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk3NzQ5Mw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537977493", "bodyText": "I agree with Ruxin's other comment that CallTracker seems pretty heavyweight here. I also don't think it will handle the data in the way that you want. It's meant for reporting stats at intervals, but AFAIK can't report across intervals which I think is called for here. For example if the interval is 10s and we're near the end of the current interval at 8s that means the stats are from a window that started 18s ago (I believe it only reports on data from an interval that has completed, not the current interval). But we want to know how many retry attempts there were in the last 10s right? I believe the CallTracker will only give you data in the context of a rotating window, not a sliding one. I would think we would want this data to be as up to date as possible.", "author": "bbarkley", "createdAt": "2020-12-08T02:07:48Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -59,15 +75,31 @@\n  */\n public class RetryClient extends D2ClientDelegator\n {\n+  public static final long DEFAULT_UPDATE_INTERVAL_MS = TimeUnit.SECONDS.toMillis(5);\n   private static final Logger LOG = LoggerFactory.getLogger(RetryClient.class);\n \n+  private final Clock _clock;\n+  private final LoadBalancer _balancer;\n   private final int _limit;\n+  private final long _updateIntervalMs;\n \n-  public RetryClient(D2Client d2Client, int limit)\n+  Map<String, CallTracker> _retryTrackerMap;", "originalCommit": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM2MzIyMw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r540363223", "bodyText": "Addressed. Use a light-weight sliding-window CallTracker for RetryClient. Leave it to container to handle monitoring metrics.", "author": "rickzx", "createdAt": "2020-12-10T17:35:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk3NzQ5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk4ODY5Nw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537988697", "bodyText": "It looks like getLoadBalancedServiceProperties without providing a callback is deprecated which may be because it makes a call out to get the data? If so maybe this should be cached and not called on each error.", "author": "bbarkley", "createdAt": "2020-12-08T02:39:09Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -231,12 +302,34 @@ public void onError(Throwable e)\n           }\n           else\n           {\n+            double maxClientRequestRetryRatio;\n+            try\n+            {\n+              Map<String, Object> transportClientProperties =\n+                  _balancer.getLoadBalancedServiceProperties(_serviceName).getTransportClientProperties();", "originalCommit": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTgxNTc3Ng==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r539815776", "bodyText": "I checked the implementation of getLoadBalancedServiceProperties. It will register a listener on ZK properties event bus only if the service has not already been listened on. And after the listener is registered, subsequent call to getLoadBalancedServiceProperties will directly read the properties from LoadBalancerState, which is a local variable. The RequestTimeoutClient is using the same API to get TransportClientProperties: \n  \n    \n      rest.li/d2/src/main/java/com/linkedin/d2/balancer/clients/RequestTimeoutClient.java\n    \n    \n         Line 150\n      in\n      9999958\n    \n    \n    \n    \n\n        \n          \n           _balancer.getLoadBalancedServiceProperties(serviceName).getTransportClientProperties(); \n        \n    \n  \n\n.\nThe method without a callback is deprecated because we want it to be async. I changed the impl here to use the callback version.", "author": "rickzx", "createdAt": "2020-12-10T03:23:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk4ODY5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk5MTIxOA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r537991218", "bodyText": "If retry is false we will be calling endCall here and also endCallWithError below which doesn't seem right.", "author": "bbarkley", "createdAt": "2020-12-08T02:45:53Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -231,12 +302,34 @@ public void onError(Throwable e)\n           }\n           else\n           {\n+            double maxClientRequestRetryRatio;\n+            try\n+            {\n+              Map<String, Object> transportClientProperties =\n+                  _balancer.getLoadBalancedServiceProperties(_serviceName).getTransportClientProperties();\n+              maxClientRequestRetryRatio = MapUtil.getWithDefault(transportClientProperties, PropertyKeys.HTTP_MAX_CLIENT_REQUEST_RETRY_RATIO,\n+                  HttpClientFactory.DEFAULT_MAX_CLIENT_REQUEST_RETRY_RATIO, Double.class);\n+            } catch (ServiceUnavailableException ex)\n+            {\n+              LOG.warn(\"Failed to fetch transportClientProperties \", ex);\n+              maxClientRequestRetryRatio = HttpClientFactory.DEFAULT_MAX_CLIENT_REQUEST_RETRY_RATIO;\n+            }\n+\n             int attempts = exclusionSet.size();\n             if (attempts <= _limit)\n             {\n-              LOG.warn(\"A retriable exception happens. Going to retry. This is attempt {}. Current exclusion set: \",\n-                  attempts, \". Current exclusion set: \" + exclusionSet);\n-              retry = doRetryRequest(_request, _context);\n+              CallTracker callTracker = _retryTrackerMap.get(_serviceName);\n+              if (callTracker.getCallStats().getRetryRate() <= maxClientRequestRetryRatio)\n+              {\n+                LOG.warn(\"A retriable exception occurred. Going to retry. This is attempt {}. Current exclusion set: {}\",\n+                    attempts, exclusionSet);\n+                _callCompletion.endCall();\n+                retry = doRetryRequest(_request, _context, attempts);", "originalCommit": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM2NDc5MA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r540364790", "bodyText": "Both are removed in the latest commit", "author": "rickzx", "createdAt": "2020-12-10T17:37:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk5MTIxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODA5Njk2Ng==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r538096966", "bodyText": "As noted above this doesn't seem like the best fit for a metrics class to use. It also seems like a poor fit based on the changes needed to the interface here and associated class. This is a fairly generic type that also happens to mirror an internal LI class and interface. Adding something specific to the retry use case seems like a design smell, and diverging from the internal implementation also should be avoided if possible IMO.", "author": "bbarkley", "createdAt": "2020-12-08T07:30:03Z", "path": "degrader/src/main/java/com/linkedin/util/degrader/CallTracker.java", "diffHunk": "@@ -115,11 +121,18 @@\n   void trackCallWithError(long duration);\n \n   /**\n-   * Indicates the start of a method invocation\n+   * Indicates the start of a non-retry method invocation\n    * @return an object that can be used to indicate completion of the call\n    */\n   CallCompletion startCall();\n \n+  /**\n+   * Indicates the start of a method invocation\n+   * @param isRetry whether the call is a retry\n+   * @return an object that can be used to indicate completion of the call\n+   */\n+  CallCompletion startCall(boolean isRetry);", "originalCommit": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM2NTIyNg==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r540365226", "bodyText": "Removed retry-related stuff in default CallTracker", "author": "rickzx", "createdAt": "2020-12-10T17:37:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODA5Njk2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ1ODk1MQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r539458951", "bodyText": "This class has enough logic and edge cases that it should have a decent set of unit tests IMO, and potentially be pulled out to a top level class.", "author": "bbarkley", "createdAt": "2020-12-09T16:33:47Z", "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -75,13 +116,160 @@ public void onStreamError(Throwable ex,\n       if (cause instanceof RetriableRequestException)\n       {\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_serverRetryTracker.isBelowRetryRatio())\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        }\n+        else\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n         break;\n       }\n       cause = cause.getCause();\n     }\n \n     nextFilter.onError(ex, requestContext, wireAttrs);\n   }\n+\n+  private void updateRetryTracker(Request req)\n+  {\n+    String retryAttemptsHeader = req.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    if (retryAttemptsHeader != null)\n+    {\n+      _serverRetryTracker.add(Integer.parseInt(retryAttemptsHeader));\n+    }\n+  }\n+\n+  /**\n+   * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+   * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+   * {@link ServerRetryTracker#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+   * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+   * intervals by aggregating the recorded requests.\n+   */\n+  private static class ServerRetryTracker", "originalCommit": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTgxNjA5Mg==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r539816092", "bodyText": "I separate out ServerRetryTracker from ServerRetryFilter into the r2 util package. Added a test suite.", "author": "rickzx", "createdAt": "2020-12-10T03:24:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ1ODk1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ3OTI3OQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r539479279", "bodyText": "Is this necessary to do here? The add call happening on each request with the header will be doing this already, so I would think it would only be needed if the majority of clients haven't updated (so that their interval data will be current if there aren't recent requests coming in from updated clients), or if there is a concern that there aren't many requests coming in, and they are very long running, in which case the initial state calculated by add will be out of date.\nIf the first issue is the concern I'd suggest pulling this up to do conditionally in processError if the header wasn't present.", "author": "bbarkley", "createdAt": "2020-12-09T16:58:14Z", "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -75,13 +116,160 @@ public void onStreamError(Throwable ex,\n       if (cause instanceof RetriableRequestException)\n       {\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_serverRetryTracker.isBelowRetryRatio())\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        }\n+        else\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n         break;\n       }\n       cause = cause.getCause();\n     }\n \n     nextFilter.onError(ex, requestContext, wireAttrs);\n   }\n+\n+  private void updateRetryTracker(Request req)\n+  {\n+    String retryAttemptsHeader = req.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    if (retryAttemptsHeader != null)\n+    {\n+      _serverRetryTracker.add(Integer.parseInt(retryAttemptsHeader));\n+    }\n+  }\n+\n+  /**\n+   * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+   * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+   * {@link ServerRetryTracker#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+   * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+   * intervals by aggregating the recorded requests.\n+   */\n+  private static class ServerRetryTracker\n+  {\n+    private final int _retryLimit;\n+    private final int _aggregatedIntervalNum;\n+    private final double _maxRequestRetryRatio;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private boolean _isBelowRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<int[]> _retryAttemptsCounter;\n+    private final int[] _aggregatedRetryAttemptsCounter;\n+\n+    private ServerRetryTracker(int retryLimit, int aggregatedIntervalNum, double maxRequestRetryRatio, long updateIntervalMs, Clock clock)\n+    {\n+      _retryLimit = retryLimit;\n+      _aggregatedIntervalNum = aggregatedIntervalNum;\n+      _maxRequestRetryRatio = maxRequestRetryRatio;\n+      _updateIntervalMs = updateIntervalMs;\n+      _clock = clock;\n+\n+      _aggregatedRetryAttemptsCounter = new int[_retryLimit + 1];\n+      _retryAttemptsCounter = new LinkedList<>();\n+      _retryAttemptsCounter.add(new int[_retryLimit + 1]);\n+    }\n+\n+    public void add(int numberOfRetryAttempts)\n+    {\n+      if (numberOfRetryAttempts <= _retryLimit)\n+      {\n+        synchronized (_counterLock)\n+        {\n+          _retryAttemptsCounter.getLast()[numberOfRetryAttempts] += 1;\n+        }\n+      } else\n+      {\n+        LOG.warn(\"Unexpected number of retry attempts: \" + numberOfRetryAttempts + \", current retry limit: \" + _retryLimit);\n+      }\n+\n+      updateRetryDecision();\n+    }\n+\n+    public void rollOverStats()\n+    {\n+      // rollover the current interval to the aggregated counter\n+      synchronized (_counterLock)\n+      {\n+        int[] intervalToAggregate = _retryAttemptsCounter.getLast();\n+        for (int i = 0; i < _retryLimit; i++)\n+        {\n+          _aggregatedRetryAttemptsCounter[i] += intervalToAggregate[i];\n+        }\n+\n+        if (_retryAttemptsCounter.size() > _aggregatedIntervalNum)\n+        {\n+          // discard the oldest interval\n+          int[] intervalToDiscard = _retryAttemptsCounter.removeFirst();\n+          for (int i = 0; i < _retryLimit; i++)\n+          {\n+            _aggregatedRetryAttemptsCounter[i] -= intervalToDiscard[i];\n+          }\n+        }\n+\n+        // append a new interval\n+        _retryAttemptsCounter.addLast(new int[_retryLimit + 1]);\n+      }\n+    }\n+\n+    public boolean isBelowRetryRatio()\n+    {\n+      updateRetryDecision();", "originalCommit": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTgxNjc4MA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r539816780", "bodyText": "This is necessary because if there aren't many requests coming in for a period of time, the information in the tracker is stale. The next time we make the retry decision, we have to first rollover the stale windows.", "author": "rickzx", "createdAt": "2020-12-10T03:26:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ3OTI3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTgzMjcyNw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r539832727", "bodyText": "Right, but that staleness would only apply if it was an older client not sending the header right? So the calculation at the end of the request can be skipped on requests when it's performed at the beginning.", "author": "bbarkley", "createdAt": "2020-12-10T04:16:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ3OTI3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM2NTU1NA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r540365554", "bodyText": "Right. Removed updateRetryDecision here", "author": "rickzx", "createdAt": "2020-12-10T17:38:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ3OTI3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ4MjA0Ng==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r539482046", "bodyText": "Is this protecting against malicious clients? Why would we be getting a value that is above the limit? If something is misconfigured and clients are trying more than they should I would think we would want to track this as another entry for the max number of retries that are allowed instead of ignoring it.", "author": "bbarkley", "createdAt": "2020-12-09T17:01:27Z", "path": "r2-core/src/main/java/com/linkedin/r2/filter/transport/ServerRetryFilter.java", "diffHunk": "@@ -75,13 +116,160 @@ public void onStreamError(Throwable ex,\n       if (cause instanceof RetriableRequestException)\n       {\n         String message = cause.getMessage();\n-        LOG.debug(\"RetriableRequestException caught! Error message: {}\", message);\n-        wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        if (_serverRetryTracker.isBelowRetryRatio())\n+        {\n+          LOG.debug(\"RetriableRequestException caught! Do retry. Error message: {}\", message);\n+          wireAttrs.put(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY, message);\n+        }\n+        else\n+        {\n+          LOG.debug(\"Max request retry ratio exceeded! Will not retry. Error message: {}\", message);\n+          wireAttrs.remove(R2Constants.RETRY_MESSAGE_ATTRIBUTE_KEY);\n+        }\n         break;\n       }\n       cause = cause.getCause();\n     }\n \n     nextFilter.onError(ex, requestContext, wireAttrs);\n   }\n+\n+  private void updateRetryTracker(Request req)\n+  {\n+    String retryAttemptsHeader = req.getHeader(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS);\n+    if (retryAttemptsHeader != null)\n+    {\n+      _serverRetryTracker.add(Integer.parseInt(retryAttemptsHeader));\n+    }\n+  }\n+\n+  /**\n+   * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+   * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+   * {@link ServerRetryTracker#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+   * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+   * intervals by aggregating the recorded requests.\n+   */\n+  private static class ServerRetryTracker\n+  {\n+    private final int _retryLimit;\n+    private final int _aggregatedIntervalNum;\n+    private final double _maxRequestRetryRatio;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private boolean _isBelowRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<int[]> _retryAttemptsCounter;\n+    private final int[] _aggregatedRetryAttemptsCounter;\n+\n+    private ServerRetryTracker(int retryLimit, int aggregatedIntervalNum, double maxRequestRetryRatio, long updateIntervalMs, Clock clock)\n+    {\n+      _retryLimit = retryLimit;\n+      _aggregatedIntervalNum = aggregatedIntervalNum;\n+      _maxRequestRetryRatio = maxRequestRetryRatio;\n+      _updateIntervalMs = updateIntervalMs;\n+      _clock = clock;\n+\n+      _aggregatedRetryAttemptsCounter = new int[_retryLimit + 1];\n+      _retryAttemptsCounter = new LinkedList<>();\n+      _retryAttemptsCounter.add(new int[_retryLimit + 1]);\n+    }\n+\n+    public void add(int numberOfRetryAttempts)\n+    {\n+      if (numberOfRetryAttempts <= _retryLimit)\n+      {\n+        synchronized (_counterLock)\n+        {\n+          _retryAttemptsCounter.getLast()[numberOfRetryAttempts] += 1;\n+        }\n+      } else\n+      {\n+        LOG.warn(\"Unexpected number of retry attempts: \" + numberOfRetryAttempts + \", current retry limit: \" + _retryLimit);", "originalCommit": "543f3a0fb1badcec4ad15fe263df3e9b6e954ec8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTg2OTQzNw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r539869437", "bodyText": "This is to account for a config discrepancy between client and server. Client and server can both configure their own retry limit, so potentially, the client-side config can have a higher value than the server-side config. Do we want to fail the request if the number of attempts is higher than the server-side config value? Or should the server still handle this request? @rachelhanhan @bbarkley @ssheng Any thoughts on this?\nI think not adding this request to RetryTracker will help improve efficiency because we can use a fix-sized array instead of a resizable list. And really, requests with high retry attempts should be extremely rare. We can already infer the retry ratio from the requests with lower retry attempts.", "author": "rickzx", "createdAt": "2020-12-10T06:03:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ4MjA0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTg5OTQ0Ng==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r539899446", "bodyText": "I agree we shouldn't support tracking data for any number of retry requests - if the current count is greater than the retry limit we can just treat it as the retry limit max.\nThough potentially we should be rejecting the request if the count is higher than what the service wants to allow. Is the server side limit used for anything other than tracking?", "author": "bbarkley", "createdAt": "2020-12-10T06:40:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ4MjA0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjkxNDA0Nw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r542914047", "bodyText": "Fixed by treating it as the retry limit max. The server side limit is used only for tracking now. One reason for the server to still handle the request above the limit is that it already defines through TransportClientProperties the max ratio of retry request a client could send. I think it's reasonable to let the client decide the per-request retry limit.", "author": "rickzx", "createdAt": "2020-12-14T23:12:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ4MjA0Ng=="}], "type": "inlineReview"}, {"oid": "dc79c0124df82abb853b452fe4807139a6092e12", "url": "https://github.com/linkedin/rest.li/commit/dc79c0124df82abb853b452fe4807139a6092e12", "message": "Update tracking logic in RetryClient and ServerRetryFilter", "committedDate": "2020-12-10T09:27:30Z", "type": "forcePushed"}, {"oid": "180dc7f6631ec3375e02cefc68e39f7f723877be", "url": "https://github.com/linkedin/rest.li/commit/180dc7f6631ec3375e02cefc68e39f7f723877be", "message": "Update tracking logic in RetryClient and ServerRetryFilter", "committedDate": "2020-12-10T09:30:09Z", "type": "commit"}, {"oid": "180dc7f6631ec3375e02cefc68e39f7f723877be", "url": "https://github.com/linkedin/rest.li/commit/180dc7f6631ec3375e02cefc68e39f7f723877be", "message": "Update tracking logic in RetryClient and ServerRetryFilter", "committedDate": "2020-12-10T09:30:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM4NDU3MQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r540384571", "bodyText": "When you get the _transportClientProperties in async manner, you will not immediately obtain the value. After you invoke _balancer.getLoadBalancedServiceProperties asynchronously, I think when you execute to line 312, for sure it will be null, we should notify a callback instead.", "author": "rachelhanhan", "createdAt": "2020-12-10T18:04:29Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -231,12 +296,43 @@ public void onError(Throwable e)\n           }\n           else\n           {\n+            double maxClientRequestRetryRatio;\n+            _balancer.getLoadBalancedServiceProperties(_serviceName, new Callback<ServiceProperties>() {\n+              @Override\n+              public void onError(Throwable e) {\n+                LOG.warn(\"Failed to fetch transportClientProperties \", e);\n+              }\n+\n+              @Override\n+              public void onSuccess(ServiceProperties result) {\n+                _transportClientProperties = result.getTransportClientProperties();\n+              }\n+            });\n+\n+            if (_transportClientProperties == null)", "originalCommit": "180dc7f6631ec3375e02cefc68e39f7f723877be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTQ5NzkzNA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r541497934", "bodyText": "Fixed. Make isBelowRetryRatio an async call as well.", "author": "rickzx", "createdAt": "2020-12-12T04:27:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM4NDU3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM5MzAxOA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r540393018", "bodyText": "I feel using int[2] to track total count and retry count is a bit implicit without java docs, can we use a new data structure to represent the count?", "author": "rachelhanhan", "createdAt": "2020-12-10T18:17:12Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +350,134 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          return !((RetriableRequestException) throwable).getDoNotRetryOverride();\n+        }\n+      }\n+\n+      return false;\n     }\n \n     /**\n      * Retries a specific request.\n      *\n      * @param request Request to retry.\n      * @param context Context of the retry request.\n+     * @param numberOfRetryAttempts Number of retry attempts.\n      * @return {@code true} if a request can be retried; {@code false} otherwise;\n      */\n-    public abstract boolean doRetryRequest(REQ request, RequestContext context);\n+    public abstract boolean doRetryRequest(REQ request, RequestContext context, int numberOfRetryAttempts);\n+  }\n+\n+  @ThreadSafe\n+  private static class ClientRetryTracker\n+  {\n+    private static final int COUNTER_TOTAL_COUNT_INDEX = 0;\n+    private static final int COUNTER_RETRY_COUNT_INDEX = 1;\n+\n+    private final int _aggregatedIntervalNum;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private double _currentAggregatedRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<int[]> _retryCounter;\n+    private final int[] _aggregatedRetryCounter;", "originalCommit": "180dc7f6631ec3375e02cefc68e39f7f723877be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTQ5Nzk0Nw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r541497947", "bodyText": "Fixed. Added a RetryCounter impl.", "author": "rickzx", "createdAt": "2020-12-12T04:28:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM5MzAxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQxNjk4Mg==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r540416982", "bodyText": "Wondering why isn't this line under the counterlock?", "author": "rachelhanhan", "createdAt": "2020-12-10T18:54:39Z", "path": "r2-core/src/main/java/com/linkedin/r2/util/ServerRetryTracker.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.r2.util;\n+\n+import com.linkedin.util.clock.Clock;\n+import java.util.LinkedList;\n+import org.checkerframework.checker.lock.qual.GuardedBy;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+ * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+ * {@link ServerRetryTracker#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+ * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+ * intervals by aggregating the recorded requests.\n+ */\n+public class ServerRetryTracker\n+{\n+  private static final Logger LOG = LoggerFactory.getLogger(ServerRetryTracker.class);\n+  private final int _retryLimit;\n+  private final int _aggregatedIntervalNum;\n+  private final double _maxRequestRetryRatio;\n+  private final long _updateIntervalMs;\n+  private final Clock _clock;\n+\n+  private final Object _counterLock = new Object();\n+  private final Object _updateLock = new Object();\n+\n+  @GuardedBy(\"_updateLock\")\n+  private volatile long _lastRollOverTime;\n+  private boolean _isBelowRetryRatio;\n+\n+  @GuardedBy(\"_counterLock\")\n+  private final LinkedList<int[]> _retryAttemptsCounter;\n+  private final int[] _aggregatedRetryAttemptsCounter;\n+\n+  public ServerRetryTracker(int retryLimit, int aggregatedIntervalNum, double maxRequestRetryRatio, long updateIntervalMs, Clock clock)\n+  {\n+    _retryLimit = retryLimit;\n+    _aggregatedIntervalNum = aggregatedIntervalNum;\n+    _maxRequestRetryRatio = maxRequestRetryRatio;\n+    _updateIntervalMs = updateIntervalMs;\n+    _clock = clock;\n+\n+    _lastRollOverTime = clock.currentTimeMillis();\n+    _isBelowRetryRatio = true;\n+\n+    _aggregatedRetryAttemptsCounter = new int[_retryLimit + 1];\n+    _retryAttemptsCounter = new LinkedList<>();\n+    _retryAttemptsCounter.add(new int[_retryLimit + 1]);\n+  }\n+\n+  public void add(int numberOfRetryAttempts)\n+  {\n+    if (numberOfRetryAttempts <= _retryLimit)\n+    {\n+      synchronized (_counterLock)\n+      {\n+        _retryAttemptsCounter.getLast()[numberOfRetryAttempts] += 1;\n+      }\n+    } else\n+    {\n+      LOG.warn(\"Unexpected number of retry attempts: \" + numberOfRetryAttempts + \", current retry limit: \" + _retryLimit);\n+      _retryAttemptsCounter.getLast()[_retryLimit] += 1;", "originalCommit": "180dc7f6631ec3375e02cefc68e39f7f723877be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTQ5Nzk1NA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r541497954", "bodyText": "Good catch. Fixed", "author": "rickzx", "createdAt": "2020-12-12T04:28:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQxNjk4Mg=="}], "type": "inlineReview"}, {"oid": "daeaece4e560b0c27feaa4a62404cdf7700fa0d5", "url": "https://github.com/linkedin/rest.li/commit/daeaece4e560b0c27feaa4a62404cdf7700fa0d5", "message": "Change isBelowRetryRatio to async call. Other minor fixes", "committedDate": "2020-12-12T04:26:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjYxMDMzMA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r542610330", "bodyText": "minor: this line seems redundant", "author": "rachelhanhan", "createdAt": "2020-12-14T18:25:01Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +338,201 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          return !((RetriableRequestException) throwable).getDoNotRetryOverride();\n+        }\n+      }\n+\n+      return false;\n     }\n \n     /**\n      * Retries a specific request.\n      *\n      * @param request Request to retry.\n      * @param context Context of the retry request.\n+     * @param numberOfRetryAttempts Number of retry attempts.\n      * @return {@code true} if a request can be retried; {@code false} otherwise;\n      */\n-    public abstract boolean doRetryRequest(REQ request, RequestContext context);\n+    public abstract boolean doRetryRequest(REQ request, RequestContext context, int numberOfRetryAttempts);\n+  }\n+\n+  @ThreadSafe\n+  private class ClientRetryTracker\n+  {\n+    private final int _aggregatedIntervalNum;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+    private final String _serviceName;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private double _currentAggregatedRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<RetryCounter> _retryCounter;\n+    private final RetryCounter _aggregatedRetryCounter;\n+\n+    private ClientRetryTracker(int aggregatedIntervalNum, long updateIntervalMs, Clock clock, String serviceName)\n+    {\n+      _aggregatedIntervalNum = aggregatedIntervalNum;\n+      _updateIntervalMs = updateIntervalMs;\n+      _clock = clock;\n+      _serviceName = serviceName;\n+\n+      _lastRollOverTime = clock.currentTimeMillis();\n+      _currentAggregatedRetryRatio = 0;\n+\n+      _aggregatedRetryCounter = new RetryCounter();\n+      _retryCounter = new LinkedList<>();\n+      _retryCounter.add(new RetryCounter());\n+    }\n+\n+    public void add(boolean isRetry)\n+    {\n+      synchronized (_counterLock)\n+      {\n+        if (isRetry)\n+        {\n+          _retryCounter.getLast().addToRetryRequestCount(1);\n+        }\n+\n+        _retryCounter.getLast().addToTotalRequestCount(1);\n+      }\n+      updateRetryDecision();\n+    }\n+\n+    public void rollOverStats()\n+    {\n+      // rollover the current interval to the aggregated counter\n+      synchronized (_counterLock)\n+      {\n+        RetryCounter intervalToAggregate = _retryCounter.getLast();\n+        _aggregatedRetryCounter.addToTotalRequestCount(intervalToAggregate.getTotalRequestCount());\n+        _aggregatedRetryCounter.addToRetryRequestCount(intervalToAggregate.getRetryRequestCount());\n+\n+        if (_retryCounter.size() > _aggregatedIntervalNum)\n+        {\n+          // discard the oldest interval\n+          RetryCounter intervalToDiscard = _retryCounter.removeFirst();\n+          _aggregatedRetryCounter.subtractFromTotalRequestCount(intervalToDiscard.getTotalRequestCount());\n+          _aggregatedRetryCounter.subtractFromRetryRequestCount(intervalToDiscard.getRetryRequestCount());;\n+        }\n+\n+        // append a new interval\n+        _retryCounter.addLast(new RetryCounter());\n+      }\n+    }\n+\n+    public void isBelowRetryRatio(SuccessCallback<Boolean> callback)\n+    {\n+      _balancer.getLoadBalancedServiceProperties(_serviceName, new Callback<ServiceProperties>()\n+      {\n+        @Override\n+        public void onError(Throwable e)\n+        {\n+          LOG.warn(\"Failed to fetch transportClientProperties \", e);\n+          double maxClientRequestRetryRatio = HttpClientFactory.DEFAULT_MAX_CLIENT_REQUEST_RETRY_RATIO;", "originalCommit": "daeaece4e560b0c27feaa4a62404cdf7700fa0d5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjkwODA2NQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r542908065", "bodyText": "Fixed", "author": "rickzx", "createdAt": "2020-12-14T23:02:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjYxMDMzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjYxNDA4Nw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r542614087", "bodyText": "Minor: if you always operate on total and retry count together, maybe you can merge these 2 lines into 2 method? Will leave the decision to you", "author": "rachelhanhan", "createdAt": "2020-12-14T18:28:10Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +338,201 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          return !((RetriableRequestException) throwable).getDoNotRetryOverride();\n+        }\n+      }\n+\n+      return false;\n     }\n \n     /**\n      * Retries a specific request.\n      *\n      * @param request Request to retry.\n      * @param context Context of the retry request.\n+     * @param numberOfRetryAttempts Number of retry attempts.\n      * @return {@code true} if a request can be retried; {@code false} otherwise;\n      */\n-    public abstract boolean doRetryRequest(REQ request, RequestContext context);\n+    public abstract boolean doRetryRequest(REQ request, RequestContext context, int numberOfRetryAttempts);\n+  }\n+\n+  @ThreadSafe\n+  private class ClientRetryTracker\n+  {\n+    private final int _aggregatedIntervalNum;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+    private final String _serviceName;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private double _currentAggregatedRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<RetryCounter> _retryCounter;\n+    private final RetryCounter _aggregatedRetryCounter;\n+\n+    private ClientRetryTracker(int aggregatedIntervalNum, long updateIntervalMs, Clock clock, String serviceName)\n+    {\n+      _aggregatedIntervalNum = aggregatedIntervalNum;\n+      _updateIntervalMs = updateIntervalMs;\n+      _clock = clock;\n+      _serviceName = serviceName;\n+\n+      _lastRollOverTime = clock.currentTimeMillis();\n+      _currentAggregatedRetryRatio = 0;\n+\n+      _aggregatedRetryCounter = new RetryCounter();\n+      _retryCounter = new LinkedList<>();\n+      _retryCounter.add(new RetryCounter());\n+    }\n+\n+    public void add(boolean isRetry)\n+    {\n+      synchronized (_counterLock)\n+      {\n+        if (isRetry)\n+        {\n+          _retryCounter.getLast().addToRetryRequestCount(1);\n+        }\n+\n+        _retryCounter.getLast().addToTotalRequestCount(1);\n+      }\n+      updateRetryDecision();\n+    }\n+\n+    public void rollOverStats()\n+    {\n+      // rollover the current interval to the aggregated counter\n+      synchronized (_counterLock)\n+      {\n+        RetryCounter intervalToAggregate = _retryCounter.getLast();\n+        _aggregatedRetryCounter.addToTotalRequestCount(intervalToAggregate.getTotalRequestCount());\n+        _aggregatedRetryCounter.addToRetryRequestCount(intervalToAggregate.getRetryRequestCount());", "originalCommit": "daeaece4e560b0c27feaa4a62404cdf7700fa0d5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjkxMTUyMg==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r542911522", "bodyText": "I feel like the current API of RetryCounter is clearer by separating out total request with retry request. Here I need to call the two methods separately https://github.com/linkedin/rest.li/pull/484/files#diff-fb1d5cedc2fd04937e39f9188139915da2a2ae4aa9c736de7503973cc85c4b6eR419", "author": "rickzx", "createdAt": "2020-12-14T23:06:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjYxNDA4Nw=="}], "type": "inlineReview"}, {"oid": "177cc61b814d5f6b4e1f47b53a352df57a7b78d5", "url": "https://github.com/linkedin/rest.li/commit/177cc61b814d5f6b4e1f47b53a352df57a7b78d5", "message": "Add a listener interface for retry request", "committedDate": "2020-12-14T22:02:18Z", "type": "forcePushed"}, {"oid": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef", "url": "https://github.com/linkedin/rest.li/commit/227c34372ba4d107eb7028dbeaa79e3f8f58a5ef", "message": "Add a listener interface for retry request", "committedDate": "2020-12-14T23:47:07Z", "type": "commit"}, {"oid": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef", "url": "https://github.com/linkedin/rest.li/commit/227c34372ba4d107eb7028dbeaa79e3f8f58a5ef", "message": "Add a listener interface for retry request", "committedDate": "2020-12-14T23:47:07Z", "type": "forcePushed"}, {"oid": "bc9e7dfec28b7ca363a745c769c4e1e5f9044f13", "url": "https://github.com/linkedin/rest.li/commit/bc9e7dfec28b7ca363a745c769c4e1e5f9044f13", "message": "Update D2TransportClientProperties.pdl documentation", "committedDate": "2020-12-15T18:24:38Z", "type": "commit"}, {"oid": "93aafa2122db2f6bc7c56b27c4b239ff6e7bf593", "url": "https://github.com/linkedin/rest.li/commit/93aafa2122db2f6bc7c56b27c4b239ff6e7bf593", "message": "Merge branch 'master' into overload_failure_retry", "committedDate": "2020-12-15T21:10:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzcxNzYzNg==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543717636", "bodyText": "Really minor, but I'd suggest reworking this to avoid duplicating the increment logic and making the intention clearer:\nif (numAttempts > _retryLimit) {\n  LOG.warn(...)\n  numAttempts = _retryLimit;\n}\n... // synchronize/increment logic", "author": "bbarkley", "createdAt": "2020-12-15T21:59:52Z", "path": "r2-core/src/main/java/com/linkedin/r2/util/ServerRetryTracker.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.r2.util;\n+\n+import com.linkedin.util.clock.Clock;\n+import java.util.LinkedList;\n+import org.checkerframework.checker.lock.qual.GuardedBy;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+ * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+ * {@link ServerRetryTracker#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+ * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+ * intervals by aggregating the recorded requests.\n+ */\n+public class ServerRetryTracker\n+{\n+  private static final Logger LOG = LoggerFactory.getLogger(ServerRetryTracker.class);\n+  private final int _retryLimit;\n+  private final int _aggregatedIntervalNum;\n+  private final double _maxRequestRetryRatio;\n+  private final long _updateIntervalMs;\n+  private final Clock _clock;\n+\n+  private final Object _counterLock = new Object();\n+  private final Object _updateLock = new Object();\n+\n+  @GuardedBy(\"_updateLock\")\n+  private volatile long _lastRollOverTime;\n+  private boolean _isBelowRetryRatio;\n+\n+  @GuardedBy(\"_counterLock\")\n+  private final LinkedList<int[]> _retryAttemptsCounter;\n+  private final int[] _aggregatedRetryAttemptsCounter;\n+\n+  public ServerRetryTracker(int retryLimit, int aggregatedIntervalNum, double maxRequestRetryRatio, long updateIntervalMs, Clock clock)\n+  {\n+    _retryLimit = retryLimit;\n+    _aggregatedIntervalNum = aggregatedIntervalNum;\n+    _maxRequestRetryRatio = maxRequestRetryRatio;\n+    _updateIntervalMs = updateIntervalMs;\n+    _clock = clock;\n+\n+    _lastRollOverTime = clock.currentTimeMillis();\n+    _isBelowRetryRatio = true;\n+\n+    _aggregatedRetryAttemptsCounter = new int[_retryLimit + 1];\n+    _retryAttemptsCounter = new LinkedList<>();\n+    _retryAttemptsCounter.add(new int[_retryLimit + 1]);\n+  }\n+\n+  public void add(int numberOfRetryAttempts)\n+  {\n+    if (numberOfRetryAttempts <= _retryLimit)\n+    {\n+      synchronized (_counterLock)\n+      {\n+        _retryAttemptsCounter.getLast()[numberOfRetryAttempts] += 1;\n+      }\n+    } else\n+    {\n+      LOG.warn(\"Unexpected number of retry attempts: \" + numberOfRetryAttempts + \", current retry limit: \" + _retryLimit);\n+      synchronized (_counterLock)\n+      {\n+        _retryAttemptsCounter.getLast()[_retryLimit] += 1;\n+      }", "originalCommit": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg2MTIyNQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543861225", "bodyText": "Fixed", "author": "rickzx", "createdAt": "2020-12-16T02:55:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzcxNzYzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzczMjYwMQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543732601", "bodyText": "Can you also add some simple tests covering retry limits of 0 and 1?", "author": "bbarkley", "createdAt": "2020-12-15T22:26:56Z", "path": "r2-core/src/test/java/com/linkedin/r2/util/TestServerRetryTracker.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.r2.util;\n+\n+import com.linkedin.r2.filter.transport.ServerRetryFilter;\n+import com.linkedin.util.clock.SettableClock;\n+import org.testng.Assert;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Test;\n+\n+\n+public class TestServerRetryTracker\n+{\n+  private ServerRetryTracker _serverRetryTracker;\n+  private SettableClock _clock;\n+\n+  @BeforeMethod\n+  public void setUp()\n+  {\n+    _clock = new SettableClock();\n+    _serverRetryTracker = new ServerRetryTracker(ServerRetryFilter.DEFAULT_RETRY_LIMIT,", "originalCommit": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg2MTM1Nw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543861357", "bodyText": "Added", "author": "rickzx", "createdAt": "2020-12-16T02:56:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzczMjYwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzczNjUzNw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543736537", "bodyText": "This test is doing a LOT. With unit tests it's best to write small focused tests that exercise one condition/scenario/code path, which makes it easier to understand the purpose of the test as well as often making it easier to track down what is breaking (if one large test is failing but there are actually three asserts that are failing it's less obvious than three separate tests failing).\nI'd suggest pulling the multiple window scenarios into separate tests at least.", "author": "bbarkley", "createdAt": "2020-12-15T22:34:32Z", "path": "r2-core/src/test/java/com/linkedin/r2/util/TestServerRetryTracker.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.r2.util;\n+\n+import com.linkedin.r2.filter.transport.ServerRetryFilter;\n+import com.linkedin.util.clock.SettableClock;\n+import org.testng.Assert;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Test;\n+\n+\n+public class TestServerRetryTracker\n+{\n+  private ServerRetryTracker _serverRetryTracker;\n+  private SettableClock _clock;\n+\n+  @BeforeMethod\n+  public void setUp()\n+  {\n+    _clock = new SettableClock();\n+    _serverRetryTracker = new ServerRetryTracker(ServerRetryFilter.DEFAULT_RETRY_LIMIT,\n+        ServerRetryFilter.DEFAULT_AGGREGATED_INTERVAL_NUM, ServerRetryFilter.DEFAULT_MAX_REQUEST_RETRY_RATIO,\n+        ServerRetryFilter.DEFAULT_UPDATE_INTERVAL_MS, _clock);\n+  }\n+\n+  @Test\n+  public void testEmptyServerRetryTracker()\n+  {\n+    for (int i = 0; i < 10; i++)\n+    {\n+      Assert.assertTrue(_serverRetryTracker.isBelowRetryRatio());\n+      Assert.assertEquals(_serverRetryTracker.getRetryRatio(),0.0, 0.0001);\n+      _clock.addDuration(ServerRetryFilter.DEFAULT_UPDATE_INTERVAL_MS);\n+    }\n+  }\n+\n+  @Test\n+  public void testServerRetryTracker()", "originalCommit": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg2Mjg2Mw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543862863", "bodyText": "It's kind of hard to separate out the multiple window scenarios because it builds upon the single window case. After separating it out in the latest commit, the test case is still pretty lengthy.", "author": "rickzx", "createdAt": "2020-12-16T02:57:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzczNjUzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzczNzM4OA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543737388", "bodyText": "I didn't see this covered in the test cases - probably worth adding.", "author": "bbarkley", "createdAt": "2020-12-15T22:36:13Z", "path": "r2-core/src/main/java/com/linkedin/r2/util/ServerRetryTracker.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+*/\n+\n+package com.linkedin.r2.util;\n+\n+import com.linkedin.util.clock.Clock;\n+import java.util.LinkedList;\n+import org.checkerframework.checker.lock.qual.GuardedBy;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Stores the number of requests categorized by number of retry attempts. It uses the information to estimate\n+ * a ratio of how many requests are being retried in the cluster. The ratio is then compared with\n+ * {@link ServerRetryTracker#_maxRequestRetryRatio} to make a decision on whether or not to retry in the\n+ * next interval. When calculating the ratio, it looks at the last {@link ServerRetryTracker#_aggregatedIntervalNum}\n+ * intervals by aggregating the recorded requests.\n+ */\n+public class ServerRetryTracker\n+{\n+  private static final Logger LOG = LoggerFactory.getLogger(ServerRetryTracker.class);\n+  private final int _retryLimit;\n+  private final int _aggregatedIntervalNum;\n+  private final double _maxRequestRetryRatio;\n+  private final long _updateIntervalMs;\n+  private final Clock _clock;\n+\n+  private final Object _counterLock = new Object();\n+  private final Object _updateLock = new Object();\n+\n+  @GuardedBy(\"_updateLock\")\n+  private volatile long _lastRollOverTime;\n+  private boolean _isBelowRetryRatio;\n+\n+  @GuardedBy(\"_counterLock\")\n+  private final LinkedList<int[]> _retryAttemptsCounter;\n+  private final int[] _aggregatedRetryAttemptsCounter;\n+\n+  public ServerRetryTracker(int retryLimit, int aggregatedIntervalNum, double maxRequestRetryRatio, long updateIntervalMs, Clock clock)\n+  {\n+    _retryLimit = retryLimit;\n+    _aggregatedIntervalNum = aggregatedIntervalNum;\n+    _maxRequestRetryRatio = maxRequestRetryRatio;\n+    _updateIntervalMs = updateIntervalMs;\n+    _clock = clock;\n+\n+    _lastRollOverTime = clock.currentTimeMillis();\n+    _isBelowRetryRatio = true;\n+\n+    _aggregatedRetryAttemptsCounter = new int[_retryLimit + 1];\n+    _retryAttemptsCounter = new LinkedList<>();\n+    _retryAttemptsCounter.add(new int[_retryLimit + 1]);\n+  }\n+\n+  public void add(int numberOfRetryAttempts)\n+  {\n+    if (numberOfRetryAttempts <= _retryLimit)\n+    {\n+      synchronized (_counterLock)\n+      {\n+        _retryAttemptsCounter.getLast()[numberOfRetryAttempts] += 1;\n+      }\n+    } else\n+    {\n+      LOG.warn(\"Unexpected number of retry attempts: \" + numberOfRetryAttempts + \", current retry limit: \" + _retryLimit);\n+      synchronized (_counterLock)\n+      {\n+        _retryAttemptsCounter.getLast()[_retryLimit] += 1;", "originalCommit": "227c34372ba4d107eb7028dbeaa79e3f8f58a5ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg2Mjk4MQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r543862981", "bodyText": "Added", "author": "rickzx", "createdAt": "2020-12-16T02:57:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzczNzM4OA=="}], "type": "inlineReview"}, {"oid": "eedf854ded4cff74ed4985694ce54baebde14fb4", "url": "https://github.com/linkedin/rest.li/commit/eedf854ded4cff74ed4985694ce54baebde14fb4", "message": "Update ServerRetryTracker test", "committedDate": "2020-12-16T02:54:52Z", "type": "commit"}, {"oid": "b1f9f26799106bb12de486b173645518b16a9c1b", "url": "https://github.com/linkedin/rest.li/commit/b1f9f26799106bb12de486b173645518b16a9c1b", "message": "Remove listener interface from RetryClient", "committedDate": "2020-12-17T19:18:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM5ODYzNg==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r545398636", "bodyText": "Probably want to use computeIfAbsent here - otherwise the tracker will be created each time even if it's not needed.", "author": "bbarkley", "createdAt": "2020-12-17T20:57:10Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -108,8 +148,21 @@ public void streamRequest(StreamRequest request, Callback<StreamResponse> callba\n   @Override\n   public void streamRequest(StreamRequest request, RequestContext requestContext, Callback<StreamResponse> callback)\n   {\n-    final Callback<StreamResponse> transportCallback = new StreamRetryRequestCallback(request, requestContext, callback);\n-    _d2Client.streamRequest(request, requestContext, transportCallback);\n+    StreamRequest newRequest = request.builder()\n+        .addHeaderValue(HttpConstants.HEADER_NUMBER_OF_RETRY_ATTEMPTS, \"0\")\n+        .build(request.getEntityStream());\n+    ClientRetryTracker retryTracker = updateRetryTracker(newRequest.getURI(), false);\n+    final Callback<StreamResponse> transportCallback = new StreamRetryRequestCallback(newRequest, requestContext, callback, retryTracker);\n+    _d2Client.streamRequest(newRequest, requestContext, transportCallback);\n+  }\n+\n+  private ClientRetryTracker updateRetryTracker(URI uri, boolean isRetry)\n+  {\n+    String serviceName = LoadBalancerUtil.getServiceNameFromUri(uri);\n+    _retryTrackerMap.putIfAbsent(serviceName, new ClientRetryTracker(_aggregatedIntervalNum, _updateIntervalMs, _clock, serviceName));", "originalCommit": "b1f9f26799106bb12de486b173645518b16a9c1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTY1NDI4OQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r545654289", "bodyText": "Thanks for pointing this out. Good to learn that computeIfAbsent takes a mapping function so the Tracker object will not be created unless needed.", "author": "rickzx", "createdAt": "2020-12-18T08:35:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM5ODYzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4MTA0Ng==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r545581046", "bodyText": "Minor: if the _updateLock is guarding it there should be a separate annotation for it", "author": "bbarkley", "createdAt": "2020-12-18T05:27:21Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +334,200 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          return !((RetriableRequestException) throwable).getDoNotRetryOverride();\n+        }\n+      }\n+\n+      return false;\n     }\n \n     /**\n      * Retries a specific request.\n      *\n      * @param request Request to retry.\n      * @param context Context of the retry request.\n+     * @param numberOfRetryAttempts Number of retry attempts.\n      * @return {@code true} if a request can be retried; {@code false} otherwise;\n      */\n-    public abstract boolean doRetryRequest(REQ request, RequestContext context);\n+    public abstract boolean doRetryRequest(REQ request, RequestContext context, int numberOfRetryAttempts);\n+  }\n+\n+  @ThreadSafe\n+  private class ClientRetryTracker\n+  {\n+    private final int _aggregatedIntervalNum;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+    private final String _serviceName;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private double _currentAggregatedRetryRatio;", "originalCommit": "b1f9f26799106bb12de486b173645518b16a9c1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTY1NDQwMQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r545654401", "bodyText": "Fixed", "author": "rickzx", "createdAt": "2020-12-18T08:35:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4MTA0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4MTE4Ng==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r545581186", "bodyText": "Same as above - additional annotation if the lock protects this", "author": "bbarkley", "createdAt": "2020-12-18T05:27:45Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +334,200 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          return !((RetriableRequestException) throwable).getDoNotRetryOverride();\n+        }\n+      }\n+\n+      return false;\n     }\n \n     /**\n      * Retries a specific request.\n      *\n      * @param request Request to retry.\n      * @param context Context of the retry request.\n+     * @param numberOfRetryAttempts Number of retry attempts.\n      * @return {@code true} if a request can be retried; {@code false} otherwise;\n      */\n-    public abstract boolean doRetryRequest(REQ request, RequestContext context);\n+    public abstract boolean doRetryRequest(REQ request, RequestContext context, int numberOfRetryAttempts);\n+  }\n+\n+  @ThreadSafe\n+  private class ClientRetryTracker\n+  {\n+    private final int _aggregatedIntervalNum;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+    private final String _serviceName;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private double _currentAggregatedRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<RetryCounter> _retryCounter;\n+    private final RetryCounter _aggregatedRetryCounter;", "originalCommit": "b1f9f26799106bb12de486b173645518b16a9c1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTY1NDUxMw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r545654513", "bodyText": "Fixed", "author": "rickzx", "createdAt": "2020-12-18T08:35:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4MTE4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4NDQzMA==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r545584430", "bodyText": "Is this making a call out to something like ZK to get this data? If so doing this on each invocation that could add a lot of latency. Ideally the result would be cached with some TTL (or maybe just fetched once the first time - do we expect this to change in ZK?).", "author": "bbarkley", "createdAt": "2020-12-18T05:39:07Z", "path": "d2/src/main/java/com/linkedin/d2/balancer/clients/RetryClient.java", "diffHunk": "@@ -254,16 +334,200 @@ public void onError(Throwable e)\n \n     private boolean isRetryException(Throwable e)\n     {\n-      return ExceptionUtils.indexOfType(e, RetriableRequestException.class) != -1;\n+      Throwable[] throwables = ExceptionUtils.getThrowables(e);\n+\n+      for (Throwable throwable: throwables)\n+      {\n+        if (throwable instanceof RetriableRequestException)\n+        {\n+          return !((RetriableRequestException) throwable).getDoNotRetryOverride();\n+        }\n+      }\n+\n+      return false;\n     }\n \n     /**\n      * Retries a specific request.\n      *\n      * @param request Request to retry.\n      * @param context Context of the retry request.\n+     * @param numberOfRetryAttempts Number of retry attempts.\n      * @return {@code true} if a request can be retried; {@code false} otherwise;\n      */\n-    public abstract boolean doRetryRequest(REQ request, RequestContext context);\n+    public abstract boolean doRetryRequest(REQ request, RequestContext context, int numberOfRetryAttempts);\n+  }\n+\n+  @ThreadSafe\n+  private class ClientRetryTracker\n+  {\n+    private final int _aggregatedIntervalNum;\n+    private final long _updateIntervalMs;\n+    private final Clock _clock;\n+    private final String _serviceName;\n+\n+    private final Object _counterLock = new Object();\n+    private final Object _updateLock = new Object();\n+\n+    @GuardedBy(\"_updateLock\")\n+    private volatile long _lastRollOverTime;\n+    private double _currentAggregatedRetryRatio;\n+\n+    @GuardedBy(\"_counterLock\")\n+    private final LinkedList<RetryCounter> _retryCounter;\n+    private final RetryCounter _aggregatedRetryCounter;\n+\n+    private ClientRetryTracker(int aggregatedIntervalNum, long updateIntervalMs, Clock clock, String serviceName)\n+    {\n+      _aggregatedIntervalNum = aggregatedIntervalNum;\n+      _updateIntervalMs = updateIntervalMs;\n+      _clock = clock;\n+      _serviceName = serviceName;\n+\n+      _lastRollOverTime = clock.currentTimeMillis();\n+      _currentAggregatedRetryRatio = 0;\n+\n+      _aggregatedRetryCounter = new RetryCounter();\n+      _retryCounter = new LinkedList<>();\n+      _retryCounter.add(new RetryCounter());\n+    }\n+\n+    public void add(boolean isRetry)\n+    {\n+      synchronized (_counterLock)\n+      {\n+        if (isRetry)\n+        {\n+          _retryCounter.getLast().addToRetryRequestCount(1);\n+        }\n+\n+        _retryCounter.getLast().addToTotalRequestCount(1);\n+      }\n+      updateRetryDecision();\n+    }\n+\n+    public void rollOverStats()\n+    {\n+      // rollover the current interval to the aggregated counter\n+      synchronized (_counterLock)\n+      {\n+        RetryCounter intervalToAggregate = _retryCounter.getLast();\n+        _aggregatedRetryCounter.addToTotalRequestCount(intervalToAggregate.getTotalRequestCount());\n+        _aggregatedRetryCounter.addToRetryRequestCount(intervalToAggregate.getRetryRequestCount());\n+\n+        if (_retryCounter.size() > _aggregatedIntervalNum)\n+        {\n+          // discard the oldest interval\n+          RetryCounter intervalToDiscard = _retryCounter.removeFirst();\n+          _aggregatedRetryCounter.subtractFromTotalRequestCount(intervalToDiscard.getTotalRequestCount());\n+          _aggregatedRetryCounter.subtractFromRetryRequestCount(intervalToDiscard.getRetryRequestCount());\n+        }\n+\n+        // append a new interval\n+        _retryCounter.addLast(new RetryCounter());\n+      }\n+    }\n+\n+    public void isBelowRetryRatio(SuccessCallback<Boolean> callback)\n+    {\n+      _balancer.getLoadBalancedServiceProperties(_serviceName, new Callback<ServiceProperties>()", "originalCommit": "b1f9f26799106bb12de486b173645518b16a9c1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTY2NTcwOQ==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r545665709", "bodyText": "It will only make a call to ZK the first time getting data from a downstream service. At the same time, it will also register a listener to ZK for property updates. The subsequent calls will just read from the local LoadBalancerState.", "author": "rickzx", "createdAt": "2020-12-18T08:46:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4NDQzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk4NzA4Nw==", "url": "https://github.com/linkedin/rest.li/pull/484#discussion_r545987087", "bodyText": "Thanks for clarifying", "author": "bbarkley", "createdAt": "2020-12-18T17:41:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU4NDQzMA=="}], "type": "inlineReview"}, {"oid": "c68fcf122ca5fb9fae481bac37245c6a0fc5a190", "url": "https://github.com/linkedin/rest.li/commit/c68fcf122ca5fb9fae481bac37245c6a0fc5a190", "message": "Minor fixes", "committedDate": "2020-12-18T08:30:10Z", "type": "forcePushed"}, {"oid": "1f5120926850b937193abf18111be77b49f26e7b", "url": "https://github.com/linkedin/rest.li/commit/1f5120926850b937193abf18111be77b49f26e7b", "message": "Minor fixes", "committedDate": "2020-12-18T09:11:38Z", "type": "forcePushed"}, {"oid": "89cc5f30327fdba04ffb2d4c3aed024dad1fea1e", "url": "https://github.com/linkedin/rest.li/commit/89cc5f30327fdba04ffb2d4c3aed024dad1fea1e", "message": "Minor fixes", "committedDate": "2020-12-23T02:26:18Z", "type": "commit"}, {"oid": "89cc5f30327fdba04ffb2d4c3aed024dad1fea1e", "url": "https://github.com/linkedin/rest.li/commit/89cc5f30327fdba04ffb2d4c3aed024dad1fea1e", "message": "Minor fixes", "committedDate": "2020-12-23T02:26:18Z", "type": "forcePushed"}, {"oid": "be5257674b409d25c7221fdfbcab3b1f804e3535", "url": "https://github.com/linkedin/rest.li/commit/be5257674b409d25c7221fdfbcab3b1f804e3535", "message": "Merge branch 'master' into overload_failure_retry", "committedDate": "2020-12-23T08:30:27Z", "type": "commit"}, {"oid": "be5257674b409d25c7221fdfbcab3b1f804e3535", "url": "https://github.com/linkedin/rest.li/commit/be5257674b409d25c7221fdfbcab3b1f804e3535", "message": "Merge branch 'master' into overload_failure_retry", "committedDate": "2020-12-23T08:30:27Z", "type": "forcePushed"}, {"oid": "ffb66e5a618db4e357183f946cf09c53bde8c459", "url": "https://github.com/linkedin/rest.li/commit/ffb66e5a618db4e357183f946cf09c53bde8c459", "message": "Update CHANGELOG and gradle.properties", "committedDate": "2020-12-23T09:07:31Z", "type": "commit"}]}