{"pr_number": 2040, "pr_title": "fix #1992 Reimplement boundedElasticScheduler with reentrancy but less efficient work stealing", "pr_createdAt": "2020-02-07T17:46:20Z", "pr_url": "https://github.com/reactor/reactor-core/pull/2040", "timeline": [{"oid": "6e0d168d8f6c8a49f7a47e1f2729ff9f3aba6303", "url": "https://github.com/reactor/reactor-core/commit/6e0d168d8f6c8a49f7a47e1f2729ff9f3aba6303", "message": "WIP eviction test", "committedDate": "2020-02-07T17:47:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE4MTU4Mg==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r377181582", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * Copyright (c) 2011-2017 Pivotal Software Inc, All Rights Reserved.\n          \n          \n            \n             * Copyright (c) 2011-Present Pivotal Software Inc, All Rights Reserved.", "author": "bsideup", "createdAt": "2020-02-10T16:44:14Z", "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -1,315 +1,239 @@\n /*\n- * Copyright (c) 2011-Present Pivotal Software Inc, All Rights Reserved.\n+ * Copyright (c) 2011-2017 Pivotal Software Inc, All Rights Reserved.", "originalCommit": "71598282865e36d03c459deabe2e95c8faf9edca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE4MzgzMw==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r377183833", "bodyText": "Educational question: can't we just refer the (volatile) field here? Why do we need to get it via the updater?", "author": "bsideup", "createdAt": "2020-02-10T16:47:45Z", "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -1,315 +1,239 @@\n /*\n- * Copyright (c) 2011-Present Pivotal Software Inc, All Rights Reserved.\n+ * Copyright (c) 2011-2017 Pivotal Software Inc, All Rights Reserved.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n  * You may obtain a copy of the License at\n  *\n- *        https://www.apache.org/licenses/LICENSE-2.0\n+ *       https://www.apache.org/licenses/LICENSE-2.0\n  *\n  * Unless required by applicable law or agreed to in writing, software\n  * distributed under the License is distributed on an \"AS IS\" BASIS,\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-\n package reactor.core.scheduler;\n \n+import java.time.Clock;\n+import java.time.Instant;\n+import java.time.ZoneId;\n import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n import java.util.Deque;\n import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n import java.util.Queue;\n+import java.util.concurrent.Callable;\n import java.util.concurrent.ConcurrentLinkedDeque;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.PriorityBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.RejectedExecutionHandler;\n import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n import java.util.concurrent.ScheduledThreadPoolExecutor;\n import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicInteger;\n import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;\n import java.util.concurrent.atomic.AtomicLong;\n-import java.util.concurrent.atomic.AtomicReference;\n import java.util.concurrent.atomic.AtomicReferenceFieldUpdater;\n-import java.util.function.LongSupplier;\n-import java.util.function.Supplier;\n import java.util.stream.Stream;\n \n import reactor.core.Disposable;\n import reactor.core.Disposables;\n import reactor.core.Exceptions;\n import reactor.core.Scannable;\n-import reactor.util.annotation.Nullable;\n \n /**\n- * Dynamically creates ScheduledExecutorService-based Workers and caches the thread pools, reusing\n- * them once the Workers have been shut down. This scheduler is time-capable (can schedule\n- * with delay / periodically).\n- * <p>\n- * The maximum number of created thread pools is capped. Tasks submitted after the cap has been\n- * reached can be enqueued up to a second limit, possibly lifted by using {@link Integer#MAX_VALUE}.\n- * <p>\n- * The default time-to-live for unused thread pools is 60 seconds, use the\n- * appropriate constructor to set a different value.\n- * <p>\n- * This scheduler is not restartable.\n+ * Scheduler that hosts a pool of 0-N single-threaded {@link BoundedScheduledExecutorService} and exposes workers\n+ * backed by these executors, making it suited for moderate amount of blocking work. Note that requests for workers\n+ * will pick an executor in a round-robin fashion, so tasks from a given worker might arbitrarily be impeded by\n+ * long-running tasks of a sibling worker (and tasks are pinned to a given executor, so they won't be stolen\n+ * by an idle executor).\n+ *\n+ * This scheduler is time-capable (can schedule with delay / periodically).\n  *\n  * @author Simon Basl\u00e9\n  */\n-final class BoundedElasticScheduler\n-\t\timplements Scheduler, Supplier<ScheduledExecutorService>, Scannable {\n+final class BoundedElasticScheduler implements Scheduler, Scannable {\n \n-\tstatic final AtomicLong COUNTER = new AtomicLong();\n+\tstatic final int DEFAULT_TTL_SECONDS = 60;\n+\n+\tstatic final AtomicLong EVICTOR_COUNTER = new AtomicLong();\n \n \tstatic final ThreadFactory EVICTOR_FACTORY = r -> {\n-\t\tThread t = new Thread(r, \"elasticBounded-evictor-\" + COUNTER.incrementAndGet());\n+\t\tThread t = new Thread(r, Schedulers.BOUNDED_ELASTIC + \"-evictor-\" + EVICTOR_COUNTER.incrementAndGet());\n \t\tt.setDaemon(true);\n \t\treturn t;\n \t};\n \n-\tstatic final CachedService SHUTDOWN            = new CachedService(null);\n-\tstatic final int           DEFAULT_TTL_SECONDS = 60;\n-\n-\tfinal ThreadFactory              factory;\n-\tfinal int                        ttlSeconds;\n-\tfinal int                        threadCap;\n-\tfinal int                        deferredTaskCap;\n-\tfinal Deque<CachedServiceExpiry> idleServicesWithExpiry;\n-\tfinal Queue<DeferredFacade>      deferredFacades;\n-\tfinal Queue<CachedService>       allServices;\n-\tfinal ScheduledExecutorService   evictor;\n+\tstatic final BoundedServices SHUTDOWN;\n+\tstatic final BoundedState    CREATING;\n+\n+\tstatic {\n+\t\tSHUTDOWN = new BoundedServices();\n+\t\tSHUTDOWN.dispose();\n+\t\tScheduledExecutorService s = Executors.newSingleThreadScheduledExecutor();\n+\t\ts.shutdownNow();\n+\t\tCREATING = new BoundedState(SHUTDOWN, s) {\n+\t\t\t@Override\n+\t\t\tpublic String toString() {\n+\t\t\t\treturn \"CREATING BoundedState\";\n+\t\t\t}\n+\t\t};\n+\t\tCREATING.markCount = -1; //always -1, ensures tryPick never returns true\n+\t\tCREATING.idleSinceTimestamp = -1; //consider evicted\n+\t}\n \n-\tvolatile boolean shutdown;\n+\tfinal int maxThreads;\n+\tfinal int maxTaskQueuedPerThread;\n \n-\tvolatile int                                                    remainingThreads;\n-\tstatic final AtomicIntegerFieldUpdater<BoundedElasticScheduler> REMAINING_THREADS =\n-\t\t\tAtomicIntegerFieldUpdater.newUpdater(BoundedElasticScheduler.class, \"remainingThreads\");\n+\tfinal Clock         clock;\n+\tfinal ThreadFactory factory;\n+\tfinal long          ttlMillis;\n \n-\tvolatile int                                                   remainingDeferredTasks;\n-\tstatic final AtomicIntegerFieldUpdater<BoundedElasticScheduler>REMAINING_DEFERRED_TASKS =\n-\t\t\tAtomicIntegerFieldUpdater.newUpdater(BoundedElasticScheduler.class, \"remainingDeferredTasks\");\n+\tvolatile BoundedServices boundedServices;\n+\tstatic final AtomicReferenceFieldUpdater<BoundedElasticScheduler, BoundedServices> BOUNDED_SERVICES =\n+\t\t\tAtomicReferenceFieldUpdater.newUpdater(BoundedElasticScheduler.class, BoundedServices.class, \"boundedServices\");\n \n+\tvolatile ScheduledExecutorService evictor;\n+\tstatic final AtomicReferenceFieldUpdater<BoundedElasticScheduler, ScheduledExecutorService> EVICTOR =\n+\t\t\tAtomicReferenceFieldUpdater.newUpdater(BoundedElasticScheduler.class, ScheduledExecutorService.class, \"evictor\");\n \n-\tBoundedElasticScheduler(int threadCap, int deferredTaskCap, ThreadFactory factory, int ttlSeconds) {\n-\t\tif (ttlSeconds < 0) {\n-\t\t\tthrow new IllegalArgumentException(\"ttlSeconds must be positive, was: \" + ttlSeconds);\n+\t/**\n+\t * This constructor lets define millisecond-grained TTLs and a custome {@link Clock},\n+\t * which can be useful for tests.\n+\t */\n+\tBoundedElasticScheduler(int maxThreads, int maxTaskQueuedPerThread,\n+\t\t\tThreadFactory threadFactory, long ttlMillis, Clock clock) {\n+\t\tif (ttlMillis <= 0) {\n+\t\t\tthrow new IllegalArgumentException(\"TTL must be strictly positive, was \" + ttlMillis + \"ms\");\n \t\t}\n-\t\tthis.ttlSeconds = ttlSeconds;\n-\t\tif (threadCap < 1) {\n-\t\t\tthrow new IllegalArgumentException(\"threadCap must be strictly positive, was: \" + threadCap);\n+\t\tif (maxThreads <= 0) {\n+\t\t\tthrow new IllegalArgumentException(\"maxThreads must be strictly positive, was \" + maxThreads);\n \t\t}\n-\t\tif (deferredTaskCap < 1) {\n-\t\t\tthrow new IllegalArgumentException(\"deferredTaskCap must be strictly positive, was: \" + deferredTaskCap);\n+\t\tif (maxTaskQueuedPerThread <= 0) {\n+\t\t\tthrow new IllegalArgumentException(\"maxTaskQueuedPerThread must be strictly positive, was \" + maxTaskQueuedPerThread);\n \t\t}\n-\t\tthis.threadCap = threadCap;\n-\t\tthis.remainingThreads = threadCap;\n-\t\tthis.deferredTaskCap = deferredTaskCap;\n-\t\tthis.remainingDeferredTasks = deferredTaskCap;\n-\t\tthis.factory = factory;\n-\t\tthis.idleServicesWithExpiry = new ConcurrentLinkedDeque<>();\n-\t\tthis.deferredFacades = new ConcurrentLinkedQueue<>();\n-\t\tthis.allServices = new ConcurrentLinkedQueue<>();\n+\t\tthis.maxThreads = maxThreads;\n+\t\tthis.maxTaskQueuedPerThread = maxTaskQueuedPerThread;\n+\t\tthis.factory = threadFactory;\n+\t\tthis.clock = Objects.requireNonNull(clock, \"A Clock must be provided\");\n+\t\tthis.ttlMillis = ttlMillis;\n+\n+\t\tthis.boundedServices = new BoundedServices(this);\n \t\tthis.evictor = Executors.newScheduledThreadPool(1, EVICTOR_FACTORY);\n-\t\tthis.evictor.scheduleAtFixedRate(() -> this.eviction(System::currentTimeMillis),\n-\t\t\t\tttlSeconds,\n-\t\t\t\tttlSeconds,\n-\t\t\t\tTimeUnit.SECONDS);\n+\t\tevictor.scheduleAtFixedRate(boundedServices::eviction,\n+\t\t\t\tttlMillis,\n+\t\t\t\tttlMillis,\n+\t\t\t\tTimeUnit.MILLISECONDS);\n \t}\n \n \t/**\n-\t * Instantiates the default {@link ScheduledExecutorService} for the BoundedElasticScheduler\n-\t * ({@code Executors.newScheduledThreadPoolExecutor} with core and max pool size of 1).\n+\t * Create a {@link BoundedElasticScheduler} with the given configuration. Note that backing threads\n+\t * (or executors) can be shared by each {@link reactor.core.scheduler.Scheduler.Worker}, so each worker\n+\t * can contribute to the task queue size.\n+\t *\n+\t * @param maxThreads the maximum number of backing threads to spawn, must be strictly positive\n+\t * @param maxTaskQueuedPerThread the maximum amount of tasks an executor can queue up\n+\t * @param factory the {@link ThreadFactory} to name the backing threads\n+\t * @param ttlSeconds the time-to-live (TTL) of idle threads, in seconds\n \t */\n-\t@Override\n-\tpublic ScheduledExecutorService get() {\n-\t\tScheduledThreadPoolExecutor poolExecutor = new ScheduledThreadPoolExecutor(1, factory);\n-\t\tpoolExecutor.setMaximumPoolSize(1);\n-\t\tpoolExecutor.setRemoveOnCancelPolicy(true);\n-\t\treturn poolExecutor;\n+\tBoundedElasticScheduler(int maxThreads, int maxTaskQueuedPerThread, ThreadFactory factory, int ttlSeconds) {\n+\t\tthis(maxThreads,\n+\t\t\t\tmaxTaskQueuedPerThread, factory, ttlSeconds * 1000, Clock.tickSeconds(ZoneId.systemDefault()));\n \t}\n \n-\t@Override\n-\tpublic void start() {\n-\t\tthrow new UnsupportedOperationException(\"Restarting not supported yet\");\n+\t/**\n+\t * Instantiates the default {@link ScheduledExecutorService} for the scheduler\n+\t * ({@code Executors.newScheduledThreadPoolExecutor} with core and max pool size of 1).\n+\t */\n+\tBoundedScheduledExecutorService createBoundedExecutorService() {\n+\t\treturn new BoundedScheduledExecutorService(this.maxTaskQueuedPerThread, this.factory);\n \t}\n \n \t@Override\n \tpublic boolean isDisposed() {\n-\t\treturn shutdown;\n+\t\treturn BOUNDED_SERVICES.get(this) == SHUTDOWN;", "originalCommit": "71598282865e36d03c459deabe2e95c8faf9edca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIxMDEyNg==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r377210126", "bodyText": "for reads, one can use the volatile directly yeah. It is important to do all writes through the field updater as they have slightly relaxed guarantees. I ended up using the field updater everywhere for consistency, as AFAIK the difference is just cosmetic for reads", "author": "simonbasle", "createdAt": "2020-02-10T17:33:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE4MzgzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE4NTc3Mw==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r377185773", "bodyText": "nit: please extract the constant (to be used in dispose())", "author": "bsideup", "createdAt": "2020-02-10T16:50:32Z", "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,491 +243,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == -1) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse if (a < parent.maxThreads) {\n+\t\t\t\t\t//try to build a new resource\n+\t\t\t\t\tif (compareAndSet(a, a + 1)) {\n+\t\t\t\t\t\tScheduledExecutorService s = Schedulers.decorateExecutorService(parent, parent.createBoundedExecutorService());\n+\t\t\t\t\t\tBoundedState newState = new BoundedState(this, s);\n+\t\t\t\t\t\tnewState.markPicked(); //no need to check return value as this one is brand new\n+\t\t\t\t\t\tbusyQueue.add(newState);\n+\t\t\t\t\t\treturn newState;\n+\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t//pick the least busy one\n+\t\t\t\t\tBoundedState s;\n+\t\t\t\t\ts = busyQueue.poll();\n+\t\t\t\t\tif (s != null && s.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(s); //put it back in the queue with updated priority\n+\t\t\t\t\t\treturn s;\n \t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic Object scanUnsafe(Attr key) {\n-\t\t\tif (key == Attr.NAME) return parent.scanUnsafe(key);\n-\t\t\tif (key == Attr.PARENT) return parent;\n-\t\t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\t\tif (key == Attr.CAPACITY) {\n-\t\t\t\t//assume 1 if unknown, otherwise use the one from underlying executor\n-\t\t\t\tInteger capacity = (Integer) Schedulers.scanExecutor(exec, key);\n-\t\t\t\tif (capacity == null || capacity == -1) return 1;\n-\t\t\t}\n-\t\t\treturn Schedulers.scanExecutor(exec, key);\n+\t\tvoid setIdle(BoundedState boundedState) {\n+\t\t\tthis.idleQueue.add(boundedState);\n+\t\t\tthis.busyQueue.remove(boundedState);\n \t\t}\n-\t}\n \n-\tstatic final class CachedServiceExpiry {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal long          expireMillis;\n+\t\t@Override\n+\t\tpublic boolean isDisposed() {\n+\t\t\treturn get() == -1;", "originalCommit": "71598282865e36d03c459deabe2e95c8faf9edca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIxMDE3MQ==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r377210171", "bodyText": "done", "author": "simonbasle", "createdAt": "2020-02-10T17:33:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE4NTc3Mw=="}], "type": "inlineReview"}, {"oid": "0d02a07b529ecdb2ff0bf0114fc7156a45673f76", "url": "https://github.com/reactor/reactor-core/commit/0d02a07b529ecdb2ff0bf0114fc7156a45673f76", "message": "added smoke test for gh1973", "committedDate": "2020-02-14T09:46:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTM4NzQ2Mw==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379387463", "bodyText": "nit: this.boundedServices = , this.evictor =  and scheduleAtFixedRate can be replaced with a single start() since they share the implementation.", "author": "bsideup", "createdAt": "2020-02-14T11:41:03Z", "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -5,311 +5,236 @@\n  * you may not use this file except in compliance with the License.\n  * You may obtain a copy of the License at\n  *\n- *        https://www.apache.org/licenses/LICENSE-2.0\n+ *       https://www.apache.org/licenses/LICENSE-2.0\n  *\n  * Unless required by applicable law or agreed to in writing, software\n  * distributed under the License is distributed on an \"AS IS\" BASIS,\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-\n package reactor.core.scheduler;\n \n+import java.time.Clock;\n+import java.time.Instant;\n+import java.time.ZoneId;\n import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n import java.util.Deque;\n import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n import java.util.Queue;\n+import java.util.concurrent.Callable;\n import java.util.concurrent.ConcurrentLinkedDeque;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.PriorityBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.RejectedExecutionHandler;\n import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n import java.util.concurrent.ScheduledThreadPoolExecutor;\n import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicInteger;\n import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;\n import java.util.concurrent.atomic.AtomicLong;\n-import java.util.concurrent.atomic.AtomicReference;\n import java.util.concurrent.atomic.AtomicReferenceFieldUpdater;\n-import java.util.function.LongSupplier;\n-import java.util.function.Supplier;\n import java.util.stream.Stream;\n \n import reactor.core.Disposable;\n import reactor.core.Disposables;\n import reactor.core.Exceptions;\n import reactor.core.Scannable;\n-import reactor.util.annotation.Nullable;\n \n /**\n- * Dynamically creates ScheduledExecutorService-based Workers and caches the thread pools, reusing\n- * them once the Workers have been shut down. This scheduler is time-capable (can schedule\n- * with delay / periodically).\n- * <p>\n- * The maximum number of created thread pools is capped. Tasks submitted after the cap has been\n- * reached can be enqueued up to a second limit, possibly lifted by using {@link Integer#MAX_VALUE}.\n- * <p>\n- * The default time-to-live for unused thread pools is 60 seconds, use the\n- * appropriate constructor to set a different value.\n- * <p>\n- * This scheduler is not restartable.\n+ * Scheduler that hosts a pool of 0-N single-threaded {@link BoundedScheduledExecutorService} and exposes workers\n+ * backed by these executors, making it suited for moderate amount of blocking work. Note that requests for workers\n+ * will pick an executor in a round-robin fashion, so tasks from a given worker might arbitrarily be impeded by\n+ * long-running tasks of a sibling worker (and tasks are pinned to a given executor, so they won't be stolen\n+ * by an idle executor).\n+ *\n+ * This scheduler is time-capable (can schedule with delay / periodically).\n  *\n  * @author Simon Basl\u00e9\n  */\n-final class BoundedElasticScheduler\n-\t\timplements Scheduler, Supplier<ScheduledExecutorService>, Scannable {\n+final class BoundedElasticScheduler implements Scheduler, Scannable {\n \n-\tstatic final AtomicLong COUNTER = new AtomicLong();\n+\tstatic final int DEFAULT_TTL_SECONDS = 60;\n+\n+\tstatic final AtomicLong EVICTOR_COUNTER = new AtomicLong();\n \n \tstatic final ThreadFactory EVICTOR_FACTORY = r -> {\n-\t\tThread t = new Thread(r, \"elasticBounded-evictor-\" + COUNTER.incrementAndGet());\n+\t\tThread t = new Thread(r, Schedulers.BOUNDED_ELASTIC + \"-evictor-\" + EVICTOR_COUNTER.incrementAndGet());\n \t\tt.setDaemon(true);\n \t\treturn t;\n \t};\n \n-\tstatic final CachedService SHUTDOWN            = new CachedService(null);\n-\tstatic final int           DEFAULT_TTL_SECONDS = 60;\n-\n-\tfinal ThreadFactory              factory;\n-\tfinal int                        ttlSeconds;\n-\tfinal int                        threadCap;\n-\tfinal int                        deferredTaskCap;\n-\tfinal Deque<CachedServiceExpiry> idleServicesWithExpiry;\n-\tfinal Queue<DeferredFacade>      deferredFacades;\n-\tfinal Queue<CachedService>       allServices;\n-\tfinal ScheduledExecutorService   evictor;\n+\tstatic final int             DISPOSED = -1;\n+\tstatic final BoundedServices SHUTDOWN;\n+\tstatic final BoundedState    CREATING;\n+\n+\tstatic {\n+\t\tSHUTDOWN = new BoundedServices();\n+\t\tSHUTDOWN.dispose();\n+\t\tScheduledExecutorService s = Executors.newSingleThreadScheduledExecutor();\n+\t\ts.shutdownNow();\n+\t\tCREATING = new BoundedState(SHUTDOWN, s) {\n+\t\t\t@Override\n+\t\t\tpublic String toString() {\n+\t\t\t\treturn \"CREATING BoundedState\";\n+\t\t\t}\n+\t\t};\n+\t\tCREATING.markCount = -1; //always -1, ensures tryPick never returns true\n+\t\tCREATING.idleSinceTimestamp = -1; //consider evicted\n+\t}\n \n-\tvolatile boolean shutdown;\n+\tfinal int maxThreads;\n+\tfinal int maxTaskQueuedPerThread;\n \n-\tvolatile int                                                    remainingThreads;\n-\tstatic final AtomicIntegerFieldUpdater<BoundedElasticScheduler> REMAINING_THREADS =\n-\t\t\tAtomicIntegerFieldUpdater.newUpdater(BoundedElasticScheduler.class, \"remainingThreads\");\n+\tfinal Clock         clock;\n+\tfinal ThreadFactory factory;\n+\tfinal long          ttlMillis;\n \n-\tvolatile int                                                   remainingDeferredTasks;\n-\tstatic final AtomicIntegerFieldUpdater<BoundedElasticScheduler>REMAINING_DEFERRED_TASKS =\n-\t\t\tAtomicIntegerFieldUpdater.newUpdater(BoundedElasticScheduler.class, \"remainingDeferredTasks\");\n+\tvolatile BoundedServices boundedServices;\n+\tstatic final AtomicReferenceFieldUpdater<BoundedElasticScheduler, BoundedServices> BOUNDED_SERVICES =\n+\t\t\tAtomicReferenceFieldUpdater.newUpdater(BoundedElasticScheduler.class, BoundedServices.class, \"boundedServices\");\n \n+\tvolatile ScheduledExecutorService evictor;\n+\tstatic final AtomicReferenceFieldUpdater<BoundedElasticScheduler, ScheduledExecutorService> EVICTOR =\n+\t\t\tAtomicReferenceFieldUpdater.newUpdater(BoundedElasticScheduler.class, ScheduledExecutorService.class, \"evictor\");\n \n-\tBoundedElasticScheduler(int threadCap, int deferredTaskCap, ThreadFactory factory, int ttlSeconds) {\n-\t\tif (ttlSeconds < 0) {\n-\t\t\tthrow new IllegalArgumentException(\"ttlSeconds must be positive, was: \" + ttlSeconds);\n+\t/**\n+\t * This constructor lets define millisecond-grained TTLs and a custome {@link Clock},\n+\t * which can be useful for tests.\n+\t */\n+\tBoundedElasticScheduler(int maxThreads, int maxTaskQueuedPerThread,\n+\t\t\tThreadFactory threadFactory, long ttlMillis, Clock clock) {\n+\t\tif (ttlMillis <= 0) {\n+\t\t\tthrow new IllegalArgumentException(\"TTL must be strictly positive, was \" + ttlMillis + \"ms\");\n \t\t}\n-\t\tthis.ttlSeconds = ttlSeconds;\n-\t\tif (threadCap < 1) {\n-\t\t\tthrow new IllegalArgumentException(\"threadCap must be strictly positive, was: \" + threadCap);\n+\t\tif (maxThreads <= 0) {\n+\t\t\tthrow new IllegalArgumentException(\"maxThreads must be strictly positive, was \" + maxThreads);\n \t\t}\n-\t\tif (deferredTaskCap < 1) {\n-\t\t\tthrow new IllegalArgumentException(\"deferredTaskCap must be strictly positive, was: \" + deferredTaskCap);\n+\t\tif (maxTaskQueuedPerThread <= 0) {\n+\t\t\tthrow new IllegalArgumentException(\"maxTaskQueuedPerThread must be strictly positive, was \" + maxTaskQueuedPerThread);\n \t\t}\n-\t\tthis.threadCap = threadCap;\n-\t\tthis.remainingThreads = threadCap;\n-\t\tthis.deferredTaskCap = deferredTaskCap;\n-\t\tthis.remainingDeferredTasks = deferredTaskCap;\n-\t\tthis.factory = factory;\n-\t\tthis.idleServicesWithExpiry = new ConcurrentLinkedDeque<>();\n-\t\tthis.deferredFacades = new ConcurrentLinkedQueue<>();\n-\t\tthis.allServices = new ConcurrentLinkedQueue<>();\n+\t\tthis.maxThreads = maxThreads;\n+\t\tthis.maxTaskQueuedPerThread = maxTaskQueuedPerThread;\n+\t\tthis.factory = threadFactory;\n+\t\tthis.clock = Objects.requireNonNull(clock, \"A Clock must be provided\");\n+\t\tthis.ttlMillis = ttlMillis;\n+\n+\t\tthis.boundedServices = new BoundedServices(this);\n \t\tthis.evictor = Executors.newScheduledThreadPool(1, EVICTOR_FACTORY);\n-\t\tthis.evictor.scheduleAtFixedRate(() -> this.eviction(System::currentTimeMillis),\n-\t\t\t\tttlSeconds,\n-\t\t\t\tttlSeconds,\n-\t\t\t\tTimeUnit.SECONDS);\n+\t\tevictor.scheduleAtFixedRate(boundedServices::eviction,", "originalCommit": "baf4aa6f21d00dc634fd7a74114e08909e0e3408", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDEzMzkwNw==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r380133907", "bodyText": "they don't really share the implementation since the BOUNDED_SERVICE would need to be initialized to SHUTDOWN first, and in constructor the looping is superfluous.", "author": "simonbasle", "createdAt": "2020-02-17T11:41:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTM4NzQ2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTM5MDQ0MQ==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379390441", "bodyText": "unrelated nit: ExecutorServiceWorker#tasks is very misleading (especially when populated from places like here), ExecutorServiceWorker#disposables or something would make it much easier to understand what it actually does (since it does not execute tasks, only disposes them)", "author": "bsideup", "createdAt": "2020-02-14T11:48:55Z", "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed", "originalCommit": "baf4aa6f21d00dc634fd7a74114e08909e0e3408", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDExMzc5Ng==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r380113796", "bodyText": "used to bear a reference to the Disposable handle that allows cancelling all the tasks, but can contain more than just task-related handlers. will rename \ud83d\udc4d", "author": "simonbasle", "createdAt": "2020-02-17T10:55:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTM5MDQ0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQwMTY5Mg==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379401692", "bodyText": "This constant seems to be reused between BoundedServices and BoundedState.\nI would suggest having two, in each class, especially given that BoundedServices uses it for checking a variable that is a counter (although I haven't checked BoundedState), so that the value is more meaningless in the given domain", "author": "bsideup", "createdAt": "2020-02-14T12:18:37Z", "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -5,311 +5,236 @@\n  * you may not use this file except in compliance with the License.\n  * You may obtain a copy of the License at\n  *\n- *        https://www.apache.org/licenses/LICENSE-2.0\n+ *       https://www.apache.org/licenses/LICENSE-2.0\n  *\n  * Unless required by applicable law or agreed to in writing, software\n  * distributed under the License is distributed on an \"AS IS\" BASIS,\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-\n package reactor.core.scheduler;\n \n+import java.time.Clock;\n+import java.time.Instant;\n+import java.time.ZoneId;\n import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n import java.util.Deque;\n import java.util.List;\n+import java.util.Objects;\n+import java.util.PriorityQueue;\n import java.util.Queue;\n+import java.util.concurrent.Callable;\n import java.util.concurrent.ConcurrentLinkedDeque;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.PriorityBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.RejectedExecutionHandler;\n import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledFuture;\n import java.util.concurrent.ScheduledThreadPoolExecutor;\n import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicInteger;\n import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;\n import java.util.concurrent.atomic.AtomicLong;\n-import java.util.concurrent.atomic.AtomicReference;\n import java.util.concurrent.atomic.AtomicReferenceFieldUpdater;\n-import java.util.function.LongSupplier;\n-import java.util.function.Supplier;\n import java.util.stream.Stream;\n \n import reactor.core.Disposable;\n import reactor.core.Disposables;\n import reactor.core.Exceptions;\n import reactor.core.Scannable;\n-import reactor.util.annotation.Nullable;\n \n /**\n- * Dynamically creates ScheduledExecutorService-based Workers and caches the thread pools, reusing\n- * them once the Workers have been shut down. This scheduler is time-capable (can schedule\n- * with delay / periodically).\n- * <p>\n- * The maximum number of created thread pools is capped. Tasks submitted after the cap has been\n- * reached can be enqueued up to a second limit, possibly lifted by using {@link Integer#MAX_VALUE}.\n- * <p>\n- * The default time-to-live for unused thread pools is 60 seconds, use the\n- * appropriate constructor to set a different value.\n- * <p>\n- * This scheduler is not restartable.\n+ * Scheduler that hosts a pool of 0-N single-threaded {@link BoundedScheduledExecutorService} and exposes workers\n+ * backed by these executors, making it suited for moderate amount of blocking work. Note that requests for workers\n+ * will pick an executor in a round-robin fashion, so tasks from a given worker might arbitrarily be impeded by\n+ * long-running tasks of a sibling worker (and tasks are pinned to a given executor, so they won't be stolen\n+ * by an idle executor).\n+ *\n+ * This scheduler is time-capable (can schedule with delay / periodically).\n  *\n  * @author Simon Basl\u00e9\n  */\n-final class BoundedElasticScheduler\n-\t\timplements Scheduler, Supplier<ScheduledExecutorService>, Scannable {\n+final class BoundedElasticScheduler implements Scheduler, Scannable {\n \n-\tstatic final AtomicLong COUNTER = new AtomicLong();\n+\tstatic final int DEFAULT_TTL_SECONDS = 60;\n+\n+\tstatic final AtomicLong EVICTOR_COUNTER = new AtomicLong();\n \n \tstatic final ThreadFactory EVICTOR_FACTORY = r -> {\n-\t\tThread t = new Thread(r, \"elasticBounded-evictor-\" + COUNTER.incrementAndGet());\n+\t\tThread t = new Thread(r, Schedulers.BOUNDED_ELASTIC + \"-evictor-\" + EVICTOR_COUNTER.incrementAndGet());\n \t\tt.setDaemon(true);\n \t\treturn t;\n \t};\n \n-\tstatic final CachedService SHUTDOWN            = new CachedService(null);\n-\tstatic final int           DEFAULT_TTL_SECONDS = 60;\n-\n-\tfinal ThreadFactory              factory;\n-\tfinal int                        ttlSeconds;\n-\tfinal int                        threadCap;\n-\tfinal int                        deferredTaskCap;\n-\tfinal Deque<CachedServiceExpiry> idleServicesWithExpiry;\n-\tfinal Queue<DeferredFacade>      deferredFacades;\n-\tfinal Queue<CachedService>       allServices;\n-\tfinal ScheduledExecutorService   evictor;\n+\tstatic final int             DISPOSED = -1;", "originalCommit": "baf4aa6f21d00dc634fd7a74114e08909e0e3408", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDExNTc0MQ==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r380115741", "bodyText": "they both are atomic counters: BoundedServices counts the number of states, aka executors it spawned. BoundedState counts the number of Workers that picked that state/executor.", "author": "simonbasle", "createdAt": "2020-02-17T10:59:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQwMTY5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDExNzM0MA==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r380117340", "bodyText": "I'll duplicate the constant and rename the second one EVICTED", "author": "simonbasle", "createdAt": "2020-02-17T11:03:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQwMTY5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQwMjAyMQ==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379402021", "bodyText": "WDYT about using continue here (and also in the next two cases)? Otherwise, if we accidentally add some logic to the end of this loop, it will get executed when inner if resolves to false", "author": "bsideup", "createdAt": "2020-02-14T12:19:39Z", "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == DISPOSED) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry", "originalCommit": "baf4aa6f21d00dc634fd7a74114e08909e0e3408", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDExOTI4Nw==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r380119287", "bodyText": "avoiding that is the goal of the comment, but I'll augment the comment with \"(implicit continue here)\". otherwise triggers inspection \"continue is unnecessary as the last statement in a loop\".", "author": "simonbasle", "createdAt": "2020-02-17T11:07:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQwMjAyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQwMjY3NQ==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379402675", "bodyText": "nit: BoundedState s; and s =  can be merged", "author": "bsideup", "createdAt": "2020-02-14T12:21:38Z", "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == DISPOSED) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse if (a < parent.maxThreads) {\n+\t\t\t\t\t//try to build a new resource\n+\t\t\t\t\tif (compareAndSet(a, a + 1)) {\n+\t\t\t\t\t\tScheduledExecutorService s = Schedulers.decorateExecutorService(parent, parent.createBoundedExecutorService());\n+\t\t\t\t\t\tBoundedState newState = new BoundedState(this, s);\n+\t\t\t\t\t\tnewState.markPicked(); //no need to check return value as this one is brand new\n+\t\t\t\t\t\tbusyQueue.add(newState);\n+\t\t\t\t\t\treturn newState;\n+\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t//pick the least busy one\n+\t\t\t\t\tBoundedState s;\n+\t\t\t\t\ts = busyQueue.poll();", "originalCommit": "baf4aa6f21d00dc634fd7a74114e08909e0e3408", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQwMzg5Mw==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379403893", "bodyText": "We need to be careful with the order here, otherwise:\n\nwe add it to idleQueue\nin another thread, pick() polls it and adds to the busy queue and returns\nnext statement removes it from the busy queue", "author": "bsideup", "createdAt": "2020-02-14T12:24:56Z", "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == DISPOSED) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse if (a < parent.maxThreads) {\n+\t\t\t\t\t//try to build a new resource\n+\t\t\t\t\tif (compareAndSet(a, a + 1)) {\n+\t\t\t\t\t\tScheduledExecutorService s = Schedulers.decorateExecutorService(parent, parent.createBoundedExecutorService());\n+\t\t\t\t\t\tBoundedState newState = new BoundedState(this, s);\n+\t\t\t\t\t\tnewState.markPicked(); //no need to check return value as this one is brand new\n+\t\t\t\t\t\tbusyQueue.add(newState);\n+\t\t\t\t\t\treturn newState;\n+\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t//pick the least busy one\n+\t\t\t\t\tBoundedState s;\n+\t\t\t\t\ts = busyQueue.poll();\n+\t\t\t\t\tif (s != null && s.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(s); //put it back in the queue with updated priority\n+\t\t\t\t\t\treturn s;\n \t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic Object scanUnsafe(Attr key) {\n-\t\t\tif (key == Attr.NAME) return parent.scanUnsafe(key);\n-\t\t\tif (key == Attr.PARENT) return parent;\n-\t\t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\t\tif (key == Attr.CAPACITY) {\n-\t\t\t\t//assume 1 if unknown, otherwise use the one from underlying executor\n-\t\t\t\tInteger capacity = (Integer) Schedulers.scanExecutor(exec, key);\n-\t\t\t\tif (capacity == null || capacity == -1) return 1;\n-\t\t\t}\n-\t\t\treturn Schedulers.scanExecutor(exec, key);\n+\t\tvoid setIdle(BoundedState boundedState) {", "originalCommit": "baf4aa6f21d00dc634fd7a74114e08909e0e3408", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDEyMDcyMg==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r380120722", "bodyText": "indeed. reversing the operations should be safer: remove from busyQueue (it is now nowhere to be seen by a concurrent process) -> add to idleQueue \ud83d\udc4d", "author": "simonbasle", "createdAt": "2020-02-17T11:10:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQwMzg5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDEzMTA2Mg==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r380131062", "bodyText": "(I probably should add a test for that)", "author": "simonbasle", "createdAt": "2020-02-17T11:34:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQwMzg5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQxMDU5Mw==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379410593", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t * give ttlMillis. When eligible for eviction, the executor is shut down and the\n          \n          \n            \n            \t\t * given ttlMillis. When eligible for eviction, the executor is shut down and the", "author": "bsideup", "createdAt": "2020-02-14T12:42:05Z", "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == DISPOSED) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse if (a < parent.maxThreads) {\n+\t\t\t\t\t//try to build a new resource\n+\t\t\t\t\tif (compareAndSet(a, a + 1)) {\n+\t\t\t\t\t\tScheduledExecutorService s = Schedulers.decorateExecutorService(parent, parent.createBoundedExecutorService());\n+\t\t\t\t\t\tBoundedState newState = new BoundedState(this, s);\n+\t\t\t\t\t\tnewState.markPicked(); //no need to check return value as this one is brand new\n+\t\t\t\t\t\tbusyQueue.add(newState);\n+\t\t\t\t\t\treturn newState;\n+\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t//pick the least busy one\n+\t\t\t\t\tBoundedState s;\n+\t\t\t\t\ts = busyQueue.poll();\n+\t\t\t\t\tif (s != null && s.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(s); //put it back in the queue with updated priority\n+\t\t\t\t\t\treturn s;\n \t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic Object scanUnsafe(Attr key) {\n-\t\t\tif (key == Attr.NAME) return parent.scanUnsafe(key);\n-\t\t\tif (key == Attr.PARENT) return parent;\n-\t\t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\t\tif (key == Attr.CAPACITY) {\n-\t\t\t\t//assume 1 if unknown, otherwise use the one from underlying executor\n-\t\t\t\tInteger capacity = (Integer) Schedulers.scanExecutor(exec, key);\n-\t\t\t\tif (capacity == null || capacity == -1) return 1;\n-\t\t\t}\n-\t\t\treturn Schedulers.scanExecutor(exec, key);\n+\t\tvoid setIdle(BoundedState boundedState) {\n+\t\t\tthis.idleQueue.add(boundedState);\n+\t\t\tthis.busyQueue.remove(boundedState);\n \t\t}\n-\t}\n \n-\tstatic final class CachedServiceExpiry {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal long          expireMillis;\n+\t\t@Override\n+\t\tpublic boolean isDisposed() {\n+\t\t\treturn get() == DISPOSED;\n+\t\t}\n \n-\t\tCachedServiceExpiry(CachedService cached, long expireMillis) {\n-\t\t\tthis.cached = cached;\n-\t\t\tthis.expireMillis = expireMillis;\n+\t\t@Override\n+\t\tpublic void dispose() {\n+\t\t\tset(DISPOSED);\n+\t\t\tidleQueue.forEach(BoundedState::shutdown);\n+\t\t\tbusyQueue.forEach(BoundedState::shutdown);\n \t\t}\n \t}\n \n-\tstatic final class ActiveWorker extends AtomicBoolean implements Worker, Scannable {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal Composite tasks;\n+\t/**\n+\t * A class that encapsulate state around the {@link BoundedScheduledExecutorService} and\n+\t * atomically marking them picked/idle.\n+\t */\n+\tstatic class BoundedState implements Disposable, Scannable {\n \n-\t\tActiveWorker(CachedService cached) {\n-\t\t\tthis.cached = cached;\n-\t\t\tthis.tasks = Disposables.composite();\n-\t\t}\n+\t\tfinal BoundedServices          parent;\n+\t\tfinal ScheduledExecutorService executor;\n \n-\t\t@Override\n-\t\tpublic Disposable schedule(Runnable task) {\n-\t\t\treturn Schedulers.workerSchedule(cached.exec,\n-\t\t\t\t\ttasks,\n-\t\t\t\t\ttask,\n-\t\t\t\t\t0L,\n-\t\t\t\t\tTimeUnit.MILLISECONDS);\n-\t\t}\n+\t\tlong idleSinceTimestamp = -1L;\n \n-\t\t@Override\n-\t\tpublic Disposable schedule(Runnable task, long delay, TimeUnit unit) {\n-\t\t\treturn Schedulers.workerSchedule(cached.exec, tasks, task, delay, unit);\n-\t\t}\n+\t\tvolatile int markCount;\n+\t\tstatic final AtomicIntegerFieldUpdater<BoundedState> MARK_COUNT = AtomicIntegerFieldUpdater.newUpdater(BoundedState.class, \"markCount\");\n \n-\t\t@Override\n-\t\tpublic Disposable schedulePeriodically(Runnable task,\n-\t\t\t\tlong initialDelay,\n-\t\t\t\tlong period,\n-\t\t\t\tTimeUnit unit) {\n-\t\t\treturn Schedulers.workerSchedulePeriodically(cached.exec,\n-\t\t\t\t\ttasks,\n-\t\t\t\t\ttask,\n-\t\t\t\t\tinitialDelay,\n-\t\t\t\t\tperiod,\n-\t\t\t\t\tunit);\n+\t\tBoundedState(BoundedServices parent, ScheduledExecutorService executor) {\n+\t\t\tthis.parent = parent;\n+\t\t\tthis.executor = executor;\n+\t\t}\n+\n+\t\t/**\n+\t\t * @return the queue size if the executor is a {@link ScheduledThreadPoolExecutor}, -1 otherwise\n+\t\t */\n+\t\tint estimateQueueSize() {\n+\t\t\tif (executor instanceof ScheduledThreadPoolExecutor) {\n+\t\t\t\treturn ((ScheduledThreadPoolExecutor) executor).getQueue().size();\n+\t\t\t}\n+\t\t\treturn -1;\n+\t\t}\n+\n+\t\t/**\n+\t\t * Try to mark this {@link BoundedState} as picked.\n+\t\t *\n+\t\t * @return true if this state could atomically be marked as picked, false if\n+\t\t * eviction started on it in the meantime\n+\t\t */\n+\t\tboolean markPicked() {\n+\t\t\tfor(;;) {\n+\t\t\t\tint i = MARK_COUNT.get(this);\n+\t\t\t\tif (i == DISPOSED) {\n+\t\t\t\t\treturn false; //being evicted\n+\t\t\t\t}\n+\t\t\t\tif (MARK_COUNT.compareAndSet(this, i, i + 1)) {\n+\t\t\t\t\treturn true;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n \n+\t\t/**\n+\t\t * Check if this {@link BoundedState} should be evicted by comparing its idleSince\n+\t\t * timestamp to the evictionTimestamp and comparing the difference with the\n+\t\t * give ttlMillis. When eligible for eviction, the executor is shut down and the", "originalCommit": "baf4aa6f21d00dc634fd7a74114e08909e0e3408", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQxMTI2Ng==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379411266", "bodyText": "nit: since ttlMillis is always positive elapsed > 0 here is redundant", "author": "bsideup", "createdAt": "2020-02-14T12:43:29Z", "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == DISPOSED) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse if (a < parent.maxThreads) {\n+\t\t\t\t\t//try to build a new resource\n+\t\t\t\t\tif (compareAndSet(a, a + 1)) {\n+\t\t\t\t\t\tScheduledExecutorService s = Schedulers.decorateExecutorService(parent, parent.createBoundedExecutorService());\n+\t\t\t\t\t\tBoundedState newState = new BoundedState(this, s);\n+\t\t\t\t\t\tnewState.markPicked(); //no need to check return value as this one is brand new\n+\t\t\t\t\t\tbusyQueue.add(newState);\n+\t\t\t\t\t\treturn newState;\n+\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t//pick the least busy one\n+\t\t\t\t\tBoundedState s;\n+\t\t\t\t\ts = busyQueue.poll();\n+\t\t\t\t\tif (s != null && s.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(s); //put it back in the queue with updated priority\n+\t\t\t\t\t\treturn s;\n \t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic Object scanUnsafe(Attr key) {\n-\t\t\tif (key == Attr.NAME) return parent.scanUnsafe(key);\n-\t\t\tif (key == Attr.PARENT) return parent;\n-\t\t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\t\tif (key == Attr.CAPACITY) {\n-\t\t\t\t//assume 1 if unknown, otherwise use the one from underlying executor\n-\t\t\t\tInteger capacity = (Integer) Schedulers.scanExecutor(exec, key);\n-\t\t\t\tif (capacity == null || capacity == -1) return 1;\n-\t\t\t}\n-\t\t\treturn Schedulers.scanExecutor(exec, key);\n+\t\tvoid setIdle(BoundedState boundedState) {\n+\t\t\tthis.idleQueue.add(boundedState);\n+\t\t\tthis.busyQueue.remove(boundedState);\n \t\t}\n-\t}\n \n-\tstatic final class CachedServiceExpiry {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal long          expireMillis;\n+\t\t@Override\n+\t\tpublic boolean isDisposed() {\n+\t\t\treturn get() == DISPOSED;\n+\t\t}\n \n-\t\tCachedServiceExpiry(CachedService cached, long expireMillis) {\n-\t\t\tthis.cached = cached;\n-\t\t\tthis.expireMillis = expireMillis;\n+\t\t@Override\n+\t\tpublic void dispose() {\n+\t\t\tset(DISPOSED);\n+\t\t\tidleQueue.forEach(BoundedState::shutdown);\n+\t\t\tbusyQueue.forEach(BoundedState::shutdown);\n \t\t}\n \t}\n \n-\tstatic final class ActiveWorker extends AtomicBoolean implements Worker, Scannable {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal Composite tasks;\n+\t/**\n+\t * A class that encapsulate state around the {@link BoundedScheduledExecutorService} and\n+\t * atomically marking them picked/idle.\n+\t */\n+\tstatic class BoundedState implements Disposable, Scannable {\n \n-\t\tActiveWorker(CachedService cached) {\n-\t\t\tthis.cached = cached;\n-\t\t\tthis.tasks = Disposables.composite();\n-\t\t}\n+\t\tfinal BoundedServices          parent;\n+\t\tfinal ScheduledExecutorService executor;\n \n-\t\t@Override\n-\t\tpublic Disposable schedule(Runnable task) {\n-\t\t\treturn Schedulers.workerSchedule(cached.exec,\n-\t\t\t\t\ttasks,\n-\t\t\t\t\ttask,\n-\t\t\t\t\t0L,\n-\t\t\t\t\tTimeUnit.MILLISECONDS);\n-\t\t}\n+\t\tlong idleSinceTimestamp = -1L;\n \n-\t\t@Override\n-\t\tpublic Disposable schedule(Runnable task, long delay, TimeUnit unit) {\n-\t\t\treturn Schedulers.workerSchedule(cached.exec, tasks, task, delay, unit);\n-\t\t}\n+\t\tvolatile int markCount;\n+\t\tstatic final AtomicIntegerFieldUpdater<BoundedState> MARK_COUNT = AtomicIntegerFieldUpdater.newUpdater(BoundedState.class, \"markCount\");\n \n-\t\t@Override\n-\t\tpublic Disposable schedulePeriodically(Runnable task,\n-\t\t\t\tlong initialDelay,\n-\t\t\t\tlong period,\n-\t\t\t\tTimeUnit unit) {\n-\t\t\treturn Schedulers.workerSchedulePeriodically(cached.exec,\n-\t\t\t\t\ttasks,\n-\t\t\t\t\ttask,\n-\t\t\t\t\tinitialDelay,\n-\t\t\t\t\tperiod,\n-\t\t\t\t\tunit);\n+\t\tBoundedState(BoundedServices parent, ScheduledExecutorService executor) {\n+\t\t\tthis.parent = parent;\n+\t\t\tthis.executor = executor;\n+\t\t}\n+\n+\t\t/**\n+\t\t * @return the queue size if the executor is a {@link ScheduledThreadPoolExecutor}, -1 otherwise\n+\t\t */\n+\t\tint estimateQueueSize() {\n+\t\t\tif (executor instanceof ScheduledThreadPoolExecutor) {\n+\t\t\t\treturn ((ScheduledThreadPoolExecutor) executor).getQueue().size();\n+\t\t\t}\n+\t\t\treturn -1;\n+\t\t}\n+\n+\t\t/**\n+\t\t * Try to mark this {@link BoundedState} as picked.\n+\t\t *\n+\t\t * @return true if this state could atomically be marked as picked, false if\n+\t\t * eviction started on it in the meantime\n+\t\t */\n+\t\tboolean markPicked() {\n+\t\t\tfor(;;) {\n+\t\t\t\tint i = MARK_COUNT.get(this);\n+\t\t\t\tif (i == DISPOSED) {\n+\t\t\t\t\treturn false; //being evicted\n+\t\t\t\t}\n+\t\t\t\tif (MARK_COUNT.compareAndSet(this, i, i + 1)) {\n+\t\t\t\t\treturn true;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n \n+\t\t/**\n+\t\t * Check if this {@link BoundedState} should be evicted by comparing its idleSince\n+\t\t * timestamp to the evictionTimestamp and comparing the difference with the\n+\t\t * give ttlMillis. When eligible for eviction, the executor is shut down and the\n+\t\t * method returns true (to remove the state from the array).\n+\t\t *\n+\t\t * @param evictionTimestamp the timestamp at which the eviction process is running\n+\t\t * @param ttlMillis the maximum idle duration\n+\t\t * @return true if this {@link BoundedState} has shut down itself as part of eviction, false otherwise\n+\t\t */\n+\t\tboolean tryEvict(long evictionTimestamp, long ttlMillis) {\n+\t\t\tlong idleSince = this.idleSinceTimestamp;\n+\t\t\tif (idleSince < 0) return false;\n+\t\t\tlong elapsed = evictionTimestamp - idleSince;\n+\t\t\tif (elapsed > 0 && elapsed >= ttlMillis) {", "originalCommit": "baf4aa6f21d00dc634fd7a74114e08909e0e3408", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQxNDI0MA==", "url": "https://github.com/reactor/reactor-core/pull/2040#discussion_r379414240", "bodyText": "nit: could use an early return instead of a comment ;)", "author": "bsideup", "createdAt": "2020-02-14T12:51:01Z", "path": "reactor-core/src/main/java/reactor/core/scheduler/BoundedElasticScheduler.java", "diffHunk": "@@ -319,492 +244,554 @@ public String toString() {\n \t\tif (factory instanceof ReactorThreadFactory) {\n \t\t\tts.append('\\\"').append(((ReactorThreadFactory) factory).get()).append(\"\\\",\");\n \t\t}\n-\t\tts.append(\"maxThreads=\").append(threadCap)\n-\t\t  .append(\",maxTaskQueued=\").append(deferredTaskCap == Integer.MAX_VALUE ? \"unbounded\" : deferredTaskCap)\n-\t\t  .append(\",ttl=\").append(ttlSeconds).append(\"s)\");\n+\t\tts.append(\"maxThreads=\").append(maxThreads)\n+\t\t  .append(\",maxTaskQueuedPerThread=\").append(maxTaskQueuedPerThread == Integer.MAX_VALUE ? \"unbounded\" : maxTaskQueuedPerThread)\n+\t\t  .append(\",ttl=\");\n+\t\tif (ttlMillis < 1000) {\n+\t\t\tts.append(ttlMillis).append(\"ms)\");\n+\t\t}\n+\t\telse {\n+\t\t\tts.append(ttlMillis / 1000).append(\"s)\");\n+\t\t}\n \t\treturn ts.toString();\n \t}\n \n+\t/**\n+\t * @return a best effort total count of the spinned up executors\n+\t */\n+\tint estimateSize() {\n+\t\treturn BOUNDED_SERVICES.get(this).get();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the busy executors\n+\t */\n+\tint estimateBusy() {\n+\t\treturn BOUNDED_SERVICES.get(this).busyQueue.size();\n+\t}\n+\n+\t/**\n+\t * @return a best effort total count of the idle executors\n+\t */\n+\tint estimateIdle() {\n+\t\treturn BOUNDED_SERVICES.get(this).idleQueue.size();\n+\t}\n+\n+\t/**\n+\t * Best effort snapshot of the remaining queue capacity for pending tasks across all the backing executors.\n+\t *\n+\t * @return the total task capacity, or {@literal -1} if any backing executor's task queue size cannot be instrumented\n+\t */\n+\tint estimateRemainingTaskCapacity() {\n+\t\tQueue<BoundedState> busyQueue = BOUNDED_SERVICES.get(this).busyQueue;\n+\t\tint totalTaskCapacity = maxTaskQueuedPerThread * maxThreads;\n+\t\tfor (BoundedState state : busyQueue) {\n+\t\t\tint stateQueueSize = state.estimateQueueSize();\n+\t\t\tif (stateQueueSize >= 0) {\n+\t\t\t\ttotalTaskCapacity -= stateQueueSize;\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t}\n+\t\treturn totalTaskCapacity;\n+\t}\n+\n \t@Override\n \tpublic Object scanUnsafe(Attr key) {\n \t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\tif (key == Attr.CAPACITY) return threadCap;\n-\t\tif (key == Attr.BUFFERED) return idleServicesWithExpiry.size(); //BUFFERED: number of workers alive and backed by thread\n+\t\tif (key == Attr.BUFFERED) return estimateSize();\n+\t\tif (key == Attr.CAPACITY) return maxThreads;\n \t\tif (key == Attr.NAME) return this.toString();\n \n \t\treturn null;\n \t}\n \n \t@Override\n-\t\t//TODO re-evaluate the inners? should these include deferredWorkers? allServices?\n \tpublic Stream<? extends Scannable> inners() {\n-\t\treturn idleServicesWithExpiry.stream()\n-\t\t                             .map(cached -> cached.cached);\n+\t\tBoundedServices services = BOUNDED_SERVICES.get(this);\n+\t\treturn Stream.concat(services.busyQueue.stream(), services.idleQueue.stream())\n+\t\t             .filter(obj -> obj != null && obj != CREATING);\n \t}\n \n-\tvoid eviction(LongSupplier nowSupplier) {\n-\t\tlong now = nowSupplier.getAsLong();\n-\t\tList<CachedServiceExpiry> list = new ArrayList<>(idleServicesWithExpiry);\n-\t\tfor (CachedServiceExpiry e : list) {\n-\t\t\tif (e.expireMillis < now) {\n-\t\t\t\tif (idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\te.cached.exec.shutdownNow();\n-\t\t\t\t\tallServices.remove(e.cached);\n-\t\t\t\t\tREMAINING_THREADS.incrementAndGet(this);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n+\t@Override\n+\tpublic Worker createWorker() {\n+\t\tBoundedState picked = BOUNDED_SERVICES.get(this)\n+\t\t                                      .pick();\n+\t\tExecutorServiceWorker worker = new ExecutorServiceWorker(picked.executor);\n+\t\tworker.tasks.add(picked); //this ensures the BoundedState will be released when worker is disposed\n+\t\treturn worker;\n \t}\n \n-\tstatic final class CachedService implements Disposable, Scannable {\n \n-\t\tfinal BoundedElasticScheduler  parent;\n-\t\tfinal ScheduledExecutorService exec;\n+\tstatic final class BoundedServices extends AtomicInteger implements Disposable {\n \n-\t\tCachedService(@Nullable BoundedElasticScheduler parent) {\n+\t\tfinal BoundedElasticScheduler             parent;\n+\t\t//duplicated Clock field from parent so that SHUTDOWN can be instantiated and partially used\n+\t\tfinal Clock                               clock;\n+\t\tfinal Deque<BoundedState>                 idleQueue;\n+\t\tfinal PriorityBlockingQueue<BoundedState> busyQueue;\n+\n+\t\t//constructor for SHUTDOWN\n+\t\tprivate BoundedServices() {\n+\t\t\tthis.parent = null;\n+\t\t\tthis.clock = Clock.fixed(Instant.EPOCH, ZoneId.systemDefault());\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>();\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\tBoundedServices(BoundedElasticScheduler parent) {\n \t\t\tthis.parent = parent;\n-\t\t\tif (parent != null) {\n-\t\t\t\tthis.exec = Schedulers.decorateExecutorService(parent, parent.get());\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthis.exec = Executors.newSingleThreadScheduledExecutor();\n-\t\t\t\tthis.exec.shutdownNow();\n+\t\t\tthis.clock = parent.clock;\n+\t\t\tthis.busyQueue = new PriorityBlockingQueue<>(parent.maxThreads,\n+\t\t\t\t\tComparator.comparingInt(bs -> bs.markCount));\n+\t\t\tthis.idleQueue = new ConcurrentLinkedDeque<>();\n+\t\t}\n+\n+\t\t/**\n+\t\t * Trigger the eviction by computing the oldest acceptable timestamp and letting each {@link BoundedState}\n+\t\t * check (and potentially shutdown) itself.\n+\t\t */\n+\t\tvoid eviction() {\n+\t\t\tfinal long evictionTimestamp = parent.clock.millis();\n+\t\t\tList<BoundedState> idleCandidates = new ArrayList<>(idleQueue);\n+\t\t\tfor (BoundedState candidate : idleCandidates) {\n+\t\t\t\tif (candidate.tryEvict(evictionTimestamp, parent.ttlMillis)) {\n+\t\t\t\t\tidleQueue.remove(candidate);\n+\t\t\t\t\tdecrementAndGet();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void dispose() {\n-\t\t\tif (exec != null) {\n-\t\t\t\tif (this != SHUTDOWN && !parent.shutdown) {\n-\t\t\t\t\t//in case of work, re-create an ActiveWorker\n-\t\t\t\t\tDeferredFacade deferredFacade = parent.deferredFacades.poll();\n-\t\t\t\t\tif (deferredFacade != null) {\n-\t\t\t\t\t\tdeferredFacade.setService(this);\n+\t\t/**\n+\t\t * Pick a {@link BoundedState}, prioritizing idle ones then spinning up a new one if enough capacity.\n+\t\t * Otherwise, picks an active one by taking from a {@link PriorityQueue}. The picking is\n+\t\t * optimistically re-attempted if the picked slot cannot be marked as picked.\n+\t\t *\n+\t\t * @return the picked {@link BoundedState}\n+\t\t */\n+\t\tBoundedState pick() {\n+\t\t\tfor (;;) {\n+\t\t\t\tint a = get();\n+\t\t\t\tif (a == DISPOSED) {\n+\t\t\t\t\treturn CREATING; //synonym for shutdown, since the underlying executor is shut down\n+\t\t\t\t}\n+\n+\t\t\t\tif (!idleQueue.isEmpty()) {\n+\t\t\t\t\t//try to find an idle resource\n+\t\t\t\t\tBoundedState bs = idleQueue.pollLast();\n+\t\t\t\t\tif (bs != null && bs.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(bs);\n+\t\t\t\t\t\treturn bs;\n \t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\t//if no more work, the service is put back at end of the cached queue and new expiry is started\n-\t\t\t\t\t\tCachedServiceExpiry e = new CachedServiceExpiry(this,\n-\t\t\t\t\t\t\t\tSystem.currentTimeMillis() + parent.ttlSeconds * 1000L);\n-\t\t\t\t\t\tparent.idleServicesWithExpiry.offerLast(e);\n-\t\t\t\t\t\tif (parent.shutdown) {\n-\t\t\t\t\t\t\tif (parent.idleServicesWithExpiry.remove(e)) {\n-\t\t\t\t\t\t\t\texec.shutdownNow();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse if (a < parent.maxThreads) {\n+\t\t\t\t\t//try to build a new resource\n+\t\t\t\t\tif (compareAndSet(a, a + 1)) {\n+\t\t\t\t\t\tScheduledExecutorService s = Schedulers.decorateExecutorService(parent, parent.createBoundedExecutorService());\n+\t\t\t\t\t\tBoundedState newState = new BoundedState(this, s);\n+\t\t\t\t\t\tnewState.markPicked(); //no need to check return value as this one is brand new\n+\t\t\t\t\t\tbusyQueue.add(newState);\n+\t\t\t\t\t\treturn newState;\n+\t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t//pick the least busy one\n+\t\t\t\t\tBoundedState s;\n+\t\t\t\t\ts = busyQueue.poll();\n+\t\t\t\t\tif (s != null && s.markPicked()) {\n+\t\t\t\t\t\tbusyQueue.add(s); //put it back in the queue with updated priority\n+\t\t\t\t\t\treturn s;\n \t\t\t\t\t}\n+\t\t\t\t\t//else optimistically retry\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n-\t\t@Override\n-\t\tpublic Object scanUnsafe(Attr key) {\n-\t\t\tif (key == Attr.NAME) return parent.scanUnsafe(key);\n-\t\t\tif (key == Attr.PARENT) return parent;\n-\t\t\tif (key == Attr.TERMINATED || key == Attr.CANCELLED) return isDisposed();\n-\t\t\tif (key == Attr.CAPACITY) {\n-\t\t\t\t//assume 1 if unknown, otherwise use the one from underlying executor\n-\t\t\t\tInteger capacity = (Integer) Schedulers.scanExecutor(exec, key);\n-\t\t\t\tif (capacity == null || capacity == -1) return 1;\n-\t\t\t}\n-\t\t\treturn Schedulers.scanExecutor(exec, key);\n+\t\tvoid setIdle(BoundedState boundedState) {\n+\t\t\tthis.idleQueue.add(boundedState);\n+\t\t\tthis.busyQueue.remove(boundedState);\n \t\t}\n-\t}\n \n-\tstatic final class CachedServiceExpiry {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal long          expireMillis;\n+\t\t@Override\n+\t\tpublic boolean isDisposed() {\n+\t\t\treturn get() == DISPOSED;\n+\t\t}\n \n-\t\tCachedServiceExpiry(CachedService cached, long expireMillis) {\n-\t\t\tthis.cached = cached;\n-\t\t\tthis.expireMillis = expireMillis;\n+\t\t@Override\n+\t\tpublic void dispose() {\n+\t\t\tset(DISPOSED);\n+\t\t\tidleQueue.forEach(BoundedState::shutdown);\n+\t\t\tbusyQueue.forEach(BoundedState::shutdown);\n \t\t}\n \t}\n \n-\tstatic final class ActiveWorker extends AtomicBoolean implements Worker, Scannable {\n-\n-\t\tfinal CachedService cached;\n-\t\tfinal Composite tasks;\n+\t/**\n+\t * A class that encapsulate state around the {@link BoundedScheduledExecutorService} and\n+\t * atomically marking them picked/idle.\n+\t */\n+\tstatic class BoundedState implements Disposable, Scannable {\n \n-\t\tActiveWorker(CachedService cached) {\n-\t\t\tthis.cached = cached;\n-\t\t\tthis.tasks = Disposables.composite();\n-\t\t}\n+\t\tfinal BoundedServices          parent;\n+\t\tfinal ScheduledExecutorService executor;\n \n-\t\t@Override\n-\t\tpublic Disposable schedule(Runnable task) {\n-\t\t\treturn Schedulers.workerSchedule(cached.exec,\n-\t\t\t\t\ttasks,\n-\t\t\t\t\ttask,\n-\t\t\t\t\t0L,\n-\t\t\t\t\tTimeUnit.MILLISECONDS);\n-\t\t}\n+\t\tlong idleSinceTimestamp = -1L;\n \n-\t\t@Override\n-\t\tpublic Disposable schedule(Runnable task, long delay, TimeUnit unit) {\n-\t\t\treturn Schedulers.workerSchedule(cached.exec, tasks, task, delay, unit);\n-\t\t}\n+\t\tvolatile int markCount;\n+\t\tstatic final AtomicIntegerFieldUpdater<BoundedState> MARK_COUNT = AtomicIntegerFieldUpdater.newUpdater(BoundedState.class, \"markCount\");\n \n-\t\t@Override\n-\t\tpublic Disposable schedulePeriodically(Runnable task,\n-\t\t\t\tlong initialDelay,\n-\t\t\t\tlong period,\n-\t\t\t\tTimeUnit unit) {\n-\t\t\treturn Schedulers.workerSchedulePeriodically(cached.exec,\n-\t\t\t\t\ttasks,\n-\t\t\t\t\ttask,\n-\t\t\t\t\tinitialDelay,\n-\t\t\t\t\tperiod,\n-\t\t\t\t\tunit);\n+\t\tBoundedState(BoundedServices parent, ScheduledExecutorService executor) {\n+\t\t\tthis.parent = parent;\n+\t\t\tthis.executor = executor;\n+\t\t}\n+\n+\t\t/**\n+\t\t * @return the queue size if the executor is a {@link ScheduledThreadPoolExecutor}, -1 otherwise\n+\t\t */\n+\t\tint estimateQueueSize() {\n+\t\t\tif (executor instanceof ScheduledThreadPoolExecutor) {\n+\t\t\t\treturn ((ScheduledThreadPoolExecutor) executor).getQueue().size();\n+\t\t\t}\n+\t\t\treturn -1;\n+\t\t}\n+\n+\t\t/**\n+\t\t * Try to mark this {@link BoundedState} as picked.\n+\t\t *\n+\t\t * @return true if this state could atomically be marked as picked, false if\n+\t\t * eviction started on it in the meantime\n+\t\t */\n+\t\tboolean markPicked() {\n+\t\t\tfor(;;) {\n+\t\t\t\tint i = MARK_COUNT.get(this);\n+\t\t\t\tif (i == DISPOSED) {\n+\t\t\t\t\treturn false; //being evicted\n+\t\t\t\t}\n+\t\t\t\tif (MARK_COUNT.compareAndSet(this, i, i + 1)) {\n+\t\t\t\t\treturn true;\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n \n+\t\t/**\n+\t\t * Check if this {@link BoundedState} should be evicted by comparing its idleSince\n+\t\t * timestamp to the evictionTimestamp and comparing the difference with the\n+\t\t * give ttlMillis. When eligible for eviction, the executor is shut down and the\n+\t\t * method returns true (to remove the state from the array).\n+\t\t *\n+\t\t * @param evictionTimestamp the timestamp at which the eviction process is running\n+\t\t * @param ttlMillis the maximum idle duration\n+\t\t * @return true if this {@link BoundedState} has shut down itself as part of eviction, false otherwise\n+\t\t */\n+\t\tboolean tryEvict(long evictionTimestamp, long ttlMillis) {\n+\t\t\tlong idleSince = this.idleSinceTimestamp;\n+\t\t\tif (idleSince < 0) return false;\n+\t\t\tlong elapsed = evictionTimestamp - idleSince;\n+\t\t\tif (elapsed > 0 && elapsed >= ttlMillis) {\n+\t\t\t\tif (MARK_COUNT.compareAndSet(this, 0, DISPOSED)) {\n+\t\t\t\t\texecutor.shutdownNow();\n+\t\t\t\t\treturn true;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\t/**\n+\t\t * Release the {@link BoundedState}, ie atomically decrease the counter of times it has been picked\n+\t\t * and mark as idle if that counter reaches 0.\n+\t\t * This is called when a worker is done using the executor. {@link #dispose()} is an alias\n+\t\t * to this method (for APIs that take a {@link Disposable}).\n+\t\t *\n+\t\t * @see #shutdown()\n+\t\t * @see #dispose()\n+\t\t */\n+\t\tvoid release() {\n+\t\t\tint picked = MARK_COUNT.decrementAndGet(this);\n+\t\t\tif (picked == 0) {\n+\t\t\t\tthis.idleSinceTimestamp = parent.clock.millis();\n+\t\t\t\tparent.setIdle(this);\n+\t\t\t}\n+\t\t\telse if (picked > 0) {\n+\t\t\t\tthis.idleSinceTimestamp = -1L;\n+\t\t\t}\n+\t\t\t// -1 means being evicted, do nothing", "originalCommit": "baf4aa6f21d00dc634fd7a74114e08909e0e3408", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "cd2beff8eb25e39cffbdeaba644ebf0fb04574b5", "url": "https://github.com/reactor/reactor-core/commit/cd2beff8eb25e39cffbdeaba644ebf0fb04574b5", "message": "fix #1992 Reimplement BoundedElasticScheduler to allow reentrancy\n\nThe general idea is to abandon the facade Worker and instead always\nsubmit tasks to an executor-backed worker. In order of preference, when\nan operator requests a Worker:\n\n - if thread cap not reached, create and pick a new worker\n - else if idle workers, pick an idle worker\n - else pick a busy worker\n\nThis implies a behavior under contention that is closer to parallel(),\nbut with a pool that is expected to be quite larger than the typical\nparallel pool.\n\nThe drawback is that once we get to pick a busy worker, there's no\ntelling when its tasks (typically blocking tasks for a\nBoundedElasticScheduler) will finish. So even though another executor\nmight become idle in the meantime, the operator's tasks will be pinned\nto the (potentially still busy) executor initially picked.\n\nTo try to counter that effect a bit, we use a priority queue for the\nbusy executors, favoring executors that are tied to less Workers (and\nthus less operators). We don't yet go as far as factoring in the task\nqueue of each executor.\n\nFinally, one noticeable change is that the second int parameter in\nthe API, maxPendingTask, is now influencing EACH executor's queue\ninstead of being a shared counter. It should be safe in the sense that\nthe number set with previous version in mind is bound to be\nover-dimensionned for the new version, but it would be recommended for\nusers to reconsider that number.", "committedDate": "2020-02-17T16:19:27Z", "type": "commit"}, {"oid": "cd2beff8eb25e39cffbdeaba644ebf0fb04574b5", "url": "https://github.com/reactor/reactor-core/commit/cd2beff8eb25e39cffbdeaba644ebf0fb04574b5", "message": "fix #1992 Reimplement BoundedElasticScheduler to allow reentrancy\n\nThe general idea is to abandon the facade Worker and instead always\nsubmit tasks to an executor-backed worker. In order of preference, when\nan operator requests a Worker:\n\n - if thread cap not reached, create and pick a new worker\n - else if idle workers, pick an idle worker\n - else pick a busy worker\n\nThis implies a behavior under contention that is closer to parallel(),\nbut with a pool that is expected to be quite larger than the typical\nparallel pool.\n\nThe drawback is that once we get to pick a busy worker, there's no\ntelling when its tasks (typically blocking tasks for a\nBoundedElasticScheduler) will finish. So even though another executor\nmight become idle in the meantime, the operator's tasks will be pinned\nto the (potentially still busy) executor initially picked.\n\nTo try to counter that effect a bit, we use a priority queue for the\nbusy executors, favoring executors that are tied to less Workers (and\nthus less operators). We don't yet go as far as factoring in the task\nqueue of each executor.\n\nFinally, one noticeable change is that the second int parameter in\nthe API, maxPendingTask, is now influencing EACH executor's queue\ninstead of being a shared counter. It should be safe in the sense that\nthe number set with previous version in mind is bound to be\nover-dimensionned for the new version, but it would be recommended for\nusers to reconsider that number.", "committedDate": "2020-02-17T16:19:27Z", "type": "forcePushed"}]}