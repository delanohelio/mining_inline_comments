{"pr_number": 28, "pr_title": "Optimizer package for Graph mode", "pr_createdAt": "2020-02-08T03:11:17Z", "pr_url": "https://github.com/tensorflow/java/pull/28", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjgyOTg3OA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376829878", "bodyText": "Ok now I understand what you meant last time by \"not placing the custom operators as the same spot as the generated ones...\"\nIn fact, you shouldn't, this file should be moved src/main/java/, not under src/gen/java/, without the \"DO NOT EDIT!\" notice. All sources are scanned for the @Operator annotation.\nCheck for example the other similar operators in here.", "author": "karllessard", "createdAt": "2020-02-10T00:09:20Z", "path": "tensorflow-core/tensorflow-core-api/src/gen/java/org/tensorflow/op/core/VariableWithInit.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+\n+// This class has been generated, DO NOT EDIT!\n+\n+package org.tensorflow.op.core;", "originalCommit": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzA5MTYwNg==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r377091606", "bodyText": "Ok, I'll sort that. I think it would be better as an endpoint anyway.", "author": "Craigacp", "createdAt": "2020-02-10T14:23:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjgyOTg3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjgzMDA3NQ==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376830075", "bodyText": "Google Java Style suggests to add a space after each commas.", "author": "karllessard", "createdAt": "2020-02-10T00:11:23Z", "path": "tensorflow-core/tensorflow-core-api/src/gen/java/org/tensorflow/op/core/VariableWithInit.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+\n+// This class has been generated, DO NOT EDIT!\n+\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.OperationBuilder;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.PrimitiveOp;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Holds state in the form of a tensor that persists across steps.\n+ * <p>\n+ * Outputs a ref to the tensor state so it may be read or modified.\n+ * <p>\n+ * Integrates initialisation of the variable into the create call.\n+ *\n+ * @param <T> data type for {@code ref()} output\n+ */\n+@Operator\n+public abstract class VariableWithInit<T extends TType> extends PrimitiveOp implements Operand<T> {\n+\n+  /**\n+   * Factory method to create a class wrapping a new Variable operation.\n+   *\n+   * @param scope current scope\n+   * @param init The op to use to initialise this variable.\n+   * @param options carries optional attributes values\n+   * @return a new instance of Variable\n+   */\n+  public static <T extends TType> Variable<T> create(Scope scope, Operand<T> init, Variable.Options... options) {\n+    Output<T> initOutput = init.asOutput();\n+    Variable<T> newVar = Variable.create(scope,initOutput.shape(),initOutput.dataType(),options);", "originalCommit": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0NzcwNA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376847704", "bodyText": "This pending PR will allow you to rename that method to whatever sounds better than create, which is a bit awkward in this case since it does not return an instance of VariableWithInit but of Variable.\nIf you agree, I suggest that we merge that PR first and that you rebase your code on it, taking advantage of the new @Endpoint annotation.", "author": "karllessard", "createdAt": "2020-02-10T02:36:11Z", "path": "tensorflow-core/tensorflow-core-api/src/gen/java/org/tensorflow/op/core/VariableWithInit.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+\n+// This class has been generated, DO NOT EDIT!\n+\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.OperationBuilder;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.PrimitiveOp;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Holds state in the form of a tensor that persists across steps.\n+ * <p>\n+ * Outputs a ref to the tensor state so it may be read or modified.\n+ * <p>\n+ * Integrates initialisation of the variable into the create call.\n+ *\n+ * @param <T> data type for {@code ref()} output\n+ */\n+@Operator\n+public abstract class VariableWithInit<T extends TType> extends PrimitiveOp implements Operand<T> {\n+\n+  /**\n+   * Factory method to create a class wrapping a new Variable operation.\n+   *\n+   * @param scope current scope\n+   * @param init The op to use to initialise this variable.\n+   * @param options carries optional attributes values\n+   * @return a new instance of Variable\n+   */\n+  public static <T extends TType> Variable<T> create(Scope scope, Operand<T> init, Variable.Options... options) {", "originalCommit": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0Nzg0MQ==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376847841", "bodyText": "Do we know how Assign reacts in eager mode? Does it really initialize the variable?", "author": "karllessard", "createdAt": "2020-02-10T02:37:05Z", "path": "tensorflow-core/tensorflow-core-api/src/gen/java/org/tensorflow/op/core/VariableWithInit.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+\n+// This class has been generated, DO NOT EDIT!\n+\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.OperationBuilder;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.PrimitiveOp;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Holds state in the form of a tensor that persists across steps.\n+ * <p>\n+ * Outputs a ref to the tensor state so it may be read or modified.\n+ * <p>\n+ * Integrates initialisation of the variable into the create call.\n+ *\n+ * @param <T> data type for {@code ref()} output\n+ */\n+@Operator\n+public abstract class VariableWithInit<T extends TType> extends PrimitiveOp implements Operand<T> {\n+\n+  /**\n+   * Factory method to create a class wrapping a new Variable operation.\n+   *\n+   * @param scope current scope\n+   * @param init The op to use to initialise this variable.\n+   * @param options carries optional attributes values\n+   * @return a new instance of Variable\n+   */\n+  public static <T extends TType> Variable<T> create(Scope scope, Operand<T> init, Variable.Options... options) {\n+    Output<T> initOutput = init.asOutput();\n+    Variable<T> newVar = Variable.create(scope,initOutput.shape(),initOutput.dataType(),options);\n+    Assign<T> assignOp = Assign.create(scope,newVar,init);\n+    ExecutionEnvironment exEnv = scope.env();\n+    if (exEnv instanceof Graph) {\n+      Graph graph = (Graph) exEnv;\n+      graph.addInitializer(assignOp);\n+    }", "originalCommit": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzA5MjEyMw==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r377092123", "bodyText": "I'll check.", "author": "Craigacp", "createdAt": "2020-02-10T14:23:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0Nzg0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0ODk4OQ==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376848989", "bodyText": "I was thinking that we could move all examples to our other repository, tensorflow/java-models.\nI also have written MNIST example in its simplest nature, should we merge them together, just keep one of them or keep both (e.g. SimpleMnist vs DeepMnist?) I'm pretty sure @dhruvrajan also have one.\nAnyway, by moving it to the other repository, we won't have to take that decision before merging this PR.", "author": "karllessard", "createdAt": "2020-02-10T02:45:17Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/examples/MNISTTest.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.examples;", "originalCommit": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzA5MjgxOA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r377092818", "bodyText": "I think it might be nice to have both the Logistic Regression MNIST you have, along with this more complex CNN example. Though we could just put them both in the same file, all the data loading, evaluation and training code will be the same.", "author": "Craigacp", "createdAt": "2020-02-10T14:25:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0ODk4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg4NTAxMg==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r378885012", "bodyText": "Ok, let's remove this one for now from this PR as they will go to the other repo. We'll sync up together to see how we want to merge both examples.", "author": "karllessard", "createdAt": "2020-02-13T14:14:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0ODk4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0OTExMQ==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376849111", "bodyText": "Again, this other pending PR would simply the signature of such operation with tf.nn.maxPool(relu1, tf.vector(1, 2, 2, 1), tf.vector(1, 2, 2, 1), PADDING_TYPE);, which I find more convenient. Should we merge it before as well? (if you agree with the proposed solution, of course)", "author": "karllessard", "createdAt": "2020-02-10T02:46:04Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/examples/MNISTTest.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.examples;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.OneHot;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Reshape;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Add;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.Conv2d;\n+import org.tensorflow.op.nn.MaxPool;\n+import org.tensorflow.op.nn.Relu;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.random.TruncatedNormal;\n+import org.tensorflow.training.optimizers.AdaDelta;\n+import org.tensorflow.training.optimizers.AdaGrad;\n+import org.tensorflow.training.optimizers.AdaGradDA;\n+import org.tensorflow.training.optimizers.Adam;\n+import org.tensorflow.training.optimizers.GradientDescent;\n+import org.tensorflow.training.optimizers.Momentum;\n+import org.tensorflow.training.optimizers.Optimizer;\n+import org.tensorflow.training.optimizers.RMSProp;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TInt32;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.util.Arrays;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+/**\n+ * Builds a LeNet-5 style CNN for MNIST.\n+ */\n+public class MNISTTest {\n+\n+  private static final Logger logger = Logger.getLogger(MNISTTest.class.getName());\n+\n+  private static final int PIXEL_DEPTH = 255;\n+  private static final int NUM_CHANNELS = 1;\n+  private static final int IMAGE_SIZE = 28;\n+  private static final int NUM_LABELS = 10;\n+  private static final long SEED = 123456789L;\n+\n+  private static final String PADDING_TYPE = \"SAME\";\n+\n+  public static final String INPUT_NAME = \"input\";\n+  public static final String OUTPUT_NAME = \"output\";\n+  public static final String TARGET = \"target\";\n+  public static final String TRAIN = \"train\";\n+  public static final String TRAINING_LOSS = \"training_loss\";\n+  public static final String EPOCH = \"epoch\";\n+  public static final String INIT = \"init\";\n+\n+  public static Graph build(String optimizerName) {\n+    Graph graph = new Graph();\n+\n+    Ops tf = Ops.create(graph);\n+\n+    // Inputs\n+    Placeholder<TFloat32> input = tf.withName(INPUT_NAME).placeholder(TFloat32.DTYPE, Placeholder.shape(Shape.make(-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)));\n+    Placeholder<TInt32> labels = tf.withName(TARGET).placeholder(TInt32.DTYPE);\n+\n+    // Scaling the features\n+    Constant<TFloat32> centeringFactor = tf.constant(PIXEL_DEPTH / 2.0f);\n+    Constant<TFloat32> scalingFactor = tf.constant((float) PIXEL_DEPTH);\n+    Operand<TFloat32> scaledInput = tf.math.div(tf.math.sub(input, centeringFactor), scalingFactor);\n+\n+    // First conv layer\n+    Variable<TFloat32> conv1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,NUM_CHANNELS,32}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv1 = tf.nn.conv2d(scaledInput, conv1Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{32}), tf.constant(0.0f)));\n+    Relu<TFloat32> relu1 = tf.nn.relu(tf.nn.biasAdd(conv1, conv1Biases));\n+\n+    // First pooling layer\n+    MaxPool<TFloat32> pool1 = tf.nn.maxPool(relu1, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);", "originalCommit": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzA5MzM0Mg==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r377093342", "bodyText": "Yep, I'm happy to rebase after the other two PRs have merged. I've not had chance to review the latest versions of either of them yet though.", "author": "Craigacp", "createdAt": "2020-02-10T14:25:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0OTExMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2MjY5NA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r384062694", "bodyText": "Done", "author": "Craigacp", "createdAt": "2020-02-25T19:01:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0OTExMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0OTMzNA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376849334", "bodyText": "To show good practices, graph should be enclosed by a try-with-resource block.", "author": "karllessard", "createdAt": "2020-02-10T02:47:23Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/examples/MNISTTest.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.examples;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.OneHot;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Reshape;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Add;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.Conv2d;\n+import org.tensorflow.op.nn.MaxPool;\n+import org.tensorflow.op.nn.Relu;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.random.TruncatedNormal;\n+import org.tensorflow.training.optimizers.AdaDelta;\n+import org.tensorflow.training.optimizers.AdaGrad;\n+import org.tensorflow.training.optimizers.AdaGradDA;\n+import org.tensorflow.training.optimizers.Adam;\n+import org.tensorflow.training.optimizers.GradientDescent;\n+import org.tensorflow.training.optimizers.Momentum;\n+import org.tensorflow.training.optimizers.Optimizer;\n+import org.tensorflow.training.optimizers.RMSProp;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TInt32;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.util.Arrays;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+/**\n+ * Builds a LeNet-5 style CNN for MNIST.\n+ */\n+public class MNISTTest {\n+\n+  private static final Logger logger = Logger.getLogger(MNISTTest.class.getName());\n+\n+  private static final int PIXEL_DEPTH = 255;\n+  private static final int NUM_CHANNELS = 1;\n+  private static final int IMAGE_SIZE = 28;\n+  private static final int NUM_LABELS = 10;\n+  private static final long SEED = 123456789L;\n+\n+  private static final String PADDING_TYPE = \"SAME\";\n+\n+  public static final String INPUT_NAME = \"input\";\n+  public static final String OUTPUT_NAME = \"output\";\n+  public static final String TARGET = \"target\";\n+  public static final String TRAIN = \"train\";\n+  public static final String TRAINING_LOSS = \"training_loss\";\n+  public static final String EPOCH = \"epoch\";\n+  public static final String INIT = \"init\";\n+\n+  public static Graph build(String optimizerName) {\n+    Graph graph = new Graph();", "originalCommit": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzA5NTQwMg==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r377095402", "bodyText": "At L268? Sure. I'll do a pass through the test code and tidy these things up.", "author": "Craigacp", "createdAt": "2020-02-10T14:29:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0OTMzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0OTcxOA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376849718", "bodyText": "what do you do with init here?", "author": "karllessard", "createdAt": "2020-02-10T02:50:00Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/examples/MNISTTest.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.examples;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.OneHot;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Reshape;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Add;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.Conv2d;\n+import org.tensorflow.op.nn.MaxPool;\n+import org.tensorflow.op.nn.Relu;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.random.TruncatedNormal;\n+import org.tensorflow.training.optimizers.AdaDelta;\n+import org.tensorflow.training.optimizers.AdaGrad;\n+import org.tensorflow.training.optimizers.AdaGradDA;\n+import org.tensorflow.training.optimizers.Adam;\n+import org.tensorflow.training.optimizers.GradientDescent;\n+import org.tensorflow.training.optimizers.Momentum;\n+import org.tensorflow.training.optimizers.Optimizer;\n+import org.tensorflow.training.optimizers.RMSProp;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TInt32;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.util.Arrays;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+/**\n+ * Builds a LeNet-5 style CNN for MNIST.\n+ */\n+public class MNISTTest {\n+\n+  private static final Logger logger = Logger.getLogger(MNISTTest.class.getName());\n+\n+  private static final int PIXEL_DEPTH = 255;\n+  private static final int NUM_CHANNELS = 1;\n+  private static final int IMAGE_SIZE = 28;\n+  private static final int NUM_LABELS = 10;\n+  private static final long SEED = 123456789L;\n+\n+  private static final String PADDING_TYPE = \"SAME\";\n+\n+  public static final String INPUT_NAME = \"input\";\n+  public static final String OUTPUT_NAME = \"output\";\n+  public static final String TARGET = \"target\";\n+  public static final String TRAIN = \"train\";\n+  public static final String TRAINING_LOSS = \"training_loss\";\n+  public static final String EPOCH = \"epoch\";\n+  public static final String INIT = \"init\";\n+\n+  public static Graph build(String optimizerName) {\n+    Graph graph = new Graph();\n+\n+    Ops tf = Ops.create(graph);\n+\n+    // Inputs\n+    Placeholder<TFloat32> input = tf.withName(INPUT_NAME).placeholder(TFloat32.DTYPE, Placeholder.shape(Shape.make(-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)));\n+    Placeholder<TInt32> labels = tf.withName(TARGET).placeholder(TInt32.DTYPE);\n+\n+    // Scaling the features\n+    Constant<TFloat32> centeringFactor = tf.constant(PIXEL_DEPTH / 2.0f);\n+    Constant<TFloat32> scalingFactor = tf.constant((float) PIXEL_DEPTH);\n+    Operand<TFloat32> scaledInput = tf.math.div(tf.math.sub(input, centeringFactor), scalingFactor);\n+\n+    // First conv layer\n+    Variable<TFloat32> conv1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,NUM_CHANNELS,32}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv1 = tf.nn.conv2d(scaledInput, conv1Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{32}), tf.constant(0.0f)));\n+    Relu<TFloat32> relu1 = tf.nn.relu(tf.nn.biasAdd(conv1, conv1Biases));\n+\n+    // First pooling layer\n+    MaxPool<TFloat32> pool1 = tf.nn.maxPool(relu1, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);\n+\n+    // Second conv layer\n+    Variable<TFloat32> conv2Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,32,64}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv2 = tf.nn.conv2d(pool1, conv2Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv2Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{64}),tf.constant(0.1f)));\n+    Relu<TFloat32> relu2 = tf.nn.relu(tf.nn.biasAdd(conv2, conv2Biases));\n+\n+    // Second pooling layer\n+    MaxPool<TFloat32> pool2 = tf.nn.maxPool(relu2, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);\n+\n+    // Flatten inputs\n+    Reshape<TFloat32> flatten = tf.reshape(pool2, tf.concat(Arrays.asList(tf.slice(tf.shape(pool2), tf.constant(new int[]{0}), tf.constant(new int[]{1})), tf.constant(new int[]{-1})), tf.constant(0)));\n+\n+    // Fully connected layer\n+    Variable<TFloat32> fc1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{IMAGE_SIZE*IMAGE_SIZE*4,512}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Variable<TFloat32> fc1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{512}),tf.constant(0.1f)));\n+    Relu<TFloat32> relu3 = tf.nn.relu(tf.math.add(tf.linalg.matMul(flatten, fc1Weights), fc1Biases));\n+\n+    // Softmax layer\n+    Variable<TFloat32> fc2Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{512,NUM_LABELS}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Variable<TFloat32> fc2Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{NUM_LABELS}),tf.constant(0.1f)));\n+\n+    Add<TFloat32> logits = tf.math.add(tf.linalg.matMul(relu3, fc2Weights), fc2Biases);\n+\n+    // Predicted outputs\n+    Softmax<TFloat32> prediction = tf.withName(OUTPUT_NAME).nn.softmax(logits);\n+\n+    // Loss function & regularization\n+    OneHot<TFloat32> oneHot = tf.oneHot(labels, tf.constant(10), tf.constant(1.0f), tf.constant(0.0f));\n+    SoftmaxCrossEntropyWithLogits<TFloat32> batchLoss = tf.nn.softmaxCrossEntropyWithLogits(logits, oneHot);\n+    Mean<TFloat32> labelLoss = tf.math.mean(batchLoss.loss(), tf.constant(0));\n+    Add<TFloat32> regularizers = tf.math.add(tf.nn.l2Loss(fc1Weights), tf.math.add(tf.nn.l2Loss(fc1Biases), tf.math.add(tf.nn.l2Loss(fc2Weights), tf.nn.l2Loss(fc2Biases))));\n+    Add<TFloat32> loss = tf.withName(TRAINING_LOSS).math.add(labelLoss, tf.math.mul(regularizers, tf.constant(5e-4f)));\n+\n+    optimizerName = optimizerName.toLowerCase();\n+    // Optimizer\n+    Optimizer optimizer;\n+    switch (optimizerName) {\n+      case \"adadelta\":\n+        optimizer = new AdaDelta(graph, 1f, 0.95f, 1e-8f);\n+        break;\n+      case \"adagradda\":\n+        optimizer = new AdaGradDA(graph, 0.01f);\n+        break;\n+      case \"adagrad\":\n+        optimizer = new AdaGrad(graph, 0.01f);\n+        break;\n+      case \"adam\":\n+        optimizer = new Adam(graph,0.001f,0.9f,0.999f,1e-8f);\n+        break;\n+      case \"sgd\":\n+        optimizer = new GradientDescent(graph,0.01f);\n+        break;\n+      case \"momentum\":\n+        optimizer = new Momentum(graph, 0.01f, 0.9f, false);\n+        break;\n+      case \"rmsprop\":\n+        optimizer = new RMSProp(graph,0.01f, 0.9f, 0.0f, 1e-10f, false);\n+        break;\n+      default:\n+        throw new IllegalArgumentException(\"Unknown optimizer \" + optimizerName);\n+    }\n+    logger.info(\"Optimizer = \" + optimizer.toString());\n+    Op minimize = optimizer.minimize(loss, TRAIN);\n+\n+    Op init = graph.variablesInitializer();", "originalCommit": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzA5NDM1MQ==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r377094351", "bodyText": "Without calling graph.variablesInitializer() it doesn't actually create the init node on the graph, as it doesn't know when you've finished adding nodes. Similar to the minimize call above, these both mutate the graph to add appropriate entry points, but I don't need the ops they produce here (because they get special names).", "author": "Craigacp", "createdAt": "2020-02-10T14:27:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg0OTcxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MDA2OA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376850068", "bodyText": "Tensors and sessions should also be enclosed in try-with-resources blocks", "author": "karllessard", "createdAt": "2020-02-10T02:52:18Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/examples/MNISTTest.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.examples;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.OneHot;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Reshape;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Add;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.Conv2d;\n+import org.tensorflow.op.nn.MaxPool;\n+import org.tensorflow.op.nn.Relu;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.random.TruncatedNormal;\n+import org.tensorflow.training.optimizers.AdaDelta;\n+import org.tensorflow.training.optimizers.AdaGrad;\n+import org.tensorflow.training.optimizers.AdaGradDA;\n+import org.tensorflow.training.optimizers.Adam;\n+import org.tensorflow.training.optimizers.GradientDescent;\n+import org.tensorflow.training.optimizers.Momentum;\n+import org.tensorflow.training.optimizers.Optimizer;\n+import org.tensorflow.training.optimizers.RMSProp;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TInt32;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.util.Arrays;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+/**\n+ * Builds a LeNet-5 style CNN for MNIST.\n+ */\n+public class MNISTTest {\n+\n+  private static final Logger logger = Logger.getLogger(MNISTTest.class.getName());\n+\n+  private static final int PIXEL_DEPTH = 255;\n+  private static final int NUM_CHANNELS = 1;\n+  private static final int IMAGE_SIZE = 28;\n+  private static final int NUM_LABELS = 10;\n+  private static final long SEED = 123456789L;\n+\n+  private static final String PADDING_TYPE = \"SAME\";\n+\n+  public static final String INPUT_NAME = \"input\";\n+  public static final String OUTPUT_NAME = \"output\";\n+  public static final String TARGET = \"target\";\n+  public static final String TRAIN = \"train\";\n+  public static final String TRAINING_LOSS = \"training_loss\";\n+  public static final String EPOCH = \"epoch\";\n+  public static final String INIT = \"init\";\n+\n+  public static Graph build(String optimizerName) {\n+    Graph graph = new Graph();\n+\n+    Ops tf = Ops.create(graph);\n+\n+    // Inputs\n+    Placeholder<TFloat32> input = tf.withName(INPUT_NAME).placeholder(TFloat32.DTYPE, Placeholder.shape(Shape.make(-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)));\n+    Placeholder<TInt32> labels = tf.withName(TARGET).placeholder(TInt32.DTYPE);\n+\n+    // Scaling the features\n+    Constant<TFloat32> centeringFactor = tf.constant(PIXEL_DEPTH / 2.0f);\n+    Constant<TFloat32> scalingFactor = tf.constant((float) PIXEL_DEPTH);\n+    Operand<TFloat32> scaledInput = tf.math.div(tf.math.sub(input, centeringFactor), scalingFactor);\n+\n+    // First conv layer\n+    Variable<TFloat32> conv1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,NUM_CHANNELS,32}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv1 = tf.nn.conv2d(scaledInput, conv1Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{32}), tf.constant(0.0f)));\n+    Relu<TFloat32> relu1 = tf.nn.relu(tf.nn.biasAdd(conv1, conv1Biases));\n+\n+    // First pooling layer\n+    MaxPool<TFloat32> pool1 = tf.nn.maxPool(relu1, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);\n+\n+    // Second conv layer\n+    Variable<TFloat32> conv2Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,32,64}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv2 = tf.nn.conv2d(pool1, conv2Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv2Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{64}),tf.constant(0.1f)));\n+    Relu<TFloat32> relu2 = tf.nn.relu(tf.nn.biasAdd(conv2, conv2Biases));\n+\n+    // Second pooling layer\n+    MaxPool<TFloat32> pool2 = tf.nn.maxPool(relu2, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);\n+\n+    // Flatten inputs\n+    Reshape<TFloat32> flatten = tf.reshape(pool2, tf.concat(Arrays.asList(tf.slice(tf.shape(pool2), tf.constant(new int[]{0}), tf.constant(new int[]{1})), tf.constant(new int[]{-1})), tf.constant(0)));\n+\n+    // Fully connected layer\n+    Variable<TFloat32> fc1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{IMAGE_SIZE*IMAGE_SIZE*4,512}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Variable<TFloat32> fc1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{512}),tf.constant(0.1f)));\n+    Relu<TFloat32> relu3 = tf.nn.relu(tf.math.add(tf.linalg.matMul(flatten, fc1Weights), fc1Biases));\n+\n+    // Softmax layer\n+    Variable<TFloat32> fc2Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{512,NUM_LABELS}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Variable<TFloat32> fc2Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{NUM_LABELS}),tf.constant(0.1f)));\n+\n+    Add<TFloat32> logits = tf.math.add(tf.linalg.matMul(relu3, fc2Weights), fc2Biases);\n+\n+    // Predicted outputs\n+    Softmax<TFloat32> prediction = tf.withName(OUTPUT_NAME).nn.softmax(logits);\n+\n+    // Loss function & regularization\n+    OneHot<TFloat32> oneHot = tf.oneHot(labels, tf.constant(10), tf.constant(1.0f), tf.constant(0.0f));\n+    SoftmaxCrossEntropyWithLogits<TFloat32> batchLoss = tf.nn.softmaxCrossEntropyWithLogits(logits, oneHot);\n+    Mean<TFloat32> labelLoss = tf.math.mean(batchLoss.loss(), tf.constant(0));\n+    Add<TFloat32> regularizers = tf.math.add(tf.nn.l2Loss(fc1Weights), tf.math.add(tf.nn.l2Loss(fc1Biases), tf.math.add(tf.nn.l2Loss(fc2Weights), tf.nn.l2Loss(fc2Biases))));\n+    Add<TFloat32> loss = tf.withName(TRAINING_LOSS).math.add(labelLoss, tf.math.mul(regularizers, tf.constant(5e-4f)));\n+\n+    optimizerName = optimizerName.toLowerCase();\n+    // Optimizer\n+    Optimizer optimizer;\n+    switch (optimizerName) {\n+      case \"adadelta\":\n+        optimizer = new AdaDelta(graph, 1f, 0.95f, 1e-8f);\n+        break;\n+      case \"adagradda\":\n+        optimizer = new AdaGradDA(graph, 0.01f);\n+        break;\n+      case \"adagrad\":\n+        optimizer = new AdaGrad(graph, 0.01f);\n+        break;\n+      case \"adam\":\n+        optimizer = new Adam(graph,0.001f,0.9f,0.999f,1e-8f);\n+        break;\n+      case \"sgd\":\n+        optimizer = new GradientDescent(graph,0.01f);\n+        break;\n+      case \"momentum\":\n+        optimizer = new Momentum(graph, 0.01f, 0.9f, false);\n+        break;\n+      case \"rmsprop\":\n+        optimizer = new RMSProp(graph,0.01f, 0.9f, 0.0f, 1e-10f, false);\n+        break;\n+      default:\n+        throw new IllegalArgumentException(\"Unknown optimizer \" + optimizerName);\n+    }\n+    logger.info(\"Optimizer = \" + optimizer.toString());\n+    Op minimize = optimizer.minimize(loss, TRAIN);\n+\n+    Op init = graph.variablesInitializer();\n+\n+    return graph;\n+  }\n+\n+  public static void train(Session session, int epochs, int minibatchSize, float[][][][] data, int[] labels) {\n+    // Initialises the parameters.\n+    session.runner().addTarget(INIT).run();\n+    logger.info(\"Initialised the model parameters\");\n+\n+    float[][][][] featureBatch = new float[minibatchSize][][][];\n+    int[] labelBatch = new int[minibatchSize];\n+\n+    int interval = 0;\n+    for (int i = 0; i < epochs; i++) {\n+      logger.log(Level.INFO, \"Starting epoch \" + i);\n+      //Tensor<?> epoch = Tensor.create(i);\n+      for (int j = 0; j < data.length; j += minibatchSize) {\n+        for (int k = j, m = 0; k < (j + minibatchSize) && k < data.length; k++, m++) {\n+          featureBatch[m] = data[k];\n+          labelBatch[m] = labels[k];\n+        }\n+        //logger.info(\"Batch = \" + batch.size());\n+        Tensor<?> input = Tensor.create(featureBatch);\n+        Tensor<?> target = Tensor.create(labelBatch);", "originalCommit": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MDMyMw==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376850323", "bodyText": "All those beautiful multidimensional arrays should be replaced TFTools NdArray, I can help you out with this, I really think writing our first MNIST example should be collaborative work to start with.", "author": "karllessard", "createdAt": "2020-02-10T02:54:03Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/examples/MNISTTest.java", "diffHunk": "@@ -0,0 +1,331 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.examples;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.OneHot;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Reshape;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Add;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.Conv2d;\n+import org.tensorflow.op.nn.MaxPool;\n+import org.tensorflow.op.nn.Relu;\n+import org.tensorflow.op.nn.Softmax;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.random.TruncatedNormal;\n+import org.tensorflow.training.optimizers.AdaDelta;\n+import org.tensorflow.training.optimizers.AdaGrad;\n+import org.tensorflow.training.optimizers.AdaGradDA;\n+import org.tensorflow.training.optimizers.Adam;\n+import org.tensorflow.training.optimizers.GradientDescent;\n+import org.tensorflow.training.optimizers.Momentum;\n+import org.tensorflow.training.optimizers.Optimizer;\n+import org.tensorflow.training.optimizers.RMSProp;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TInt32;\n+\n+import java.io.BufferedInputStream;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.util.Arrays;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+/**\n+ * Builds a LeNet-5 style CNN for MNIST.\n+ */\n+public class MNISTTest {\n+\n+  private static final Logger logger = Logger.getLogger(MNISTTest.class.getName());\n+\n+  private static final int PIXEL_DEPTH = 255;\n+  private static final int NUM_CHANNELS = 1;\n+  private static final int IMAGE_SIZE = 28;\n+  private static final int NUM_LABELS = 10;\n+  private static final long SEED = 123456789L;\n+\n+  private static final String PADDING_TYPE = \"SAME\";\n+\n+  public static final String INPUT_NAME = \"input\";\n+  public static final String OUTPUT_NAME = \"output\";\n+  public static final String TARGET = \"target\";\n+  public static final String TRAIN = \"train\";\n+  public static final String TRAINING_LOSS = \"training_loss\";\n+  public static final String EPOCH = \"epoch\";\n+  public static final String INIT = \"init\";\n+\n+  public static Graph build(String optimizerName) {\n+    Graph graph = new Graph();\n+\n+    Ops tf = Ops.create(graph);\n+\n+    // Inputs\n+    Placeholder<TFloat32> input = tf.withName(INPUT_NAME).placeholder(TFloat32.DTYPE, Placeholder.shape(Shape.make(-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)));\n+    Placeholder<TInt32> labels = tf.withName(TARGET).placeholder(TInt32.DTYPE);\n+\n+    // Scaling the features\n+    Constant<TFloat32> centeringFactor = tf.constant(PIXEL_DEPTH / 2.0f);\n+    Constant<TFloat32> scalingFactor = tf.constant((float) PIXEL_DEPTH);\n+    Operand<TFloat32> scaledInput = tf.math.div(tf.math.sub(input, centeringFactor), scalingFactor);\n+\n+    // First conv layer\n+    Variable<TFloat32> conv1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,NUM_CHANNELS,32}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv1 = tf.nn.conv2d(scaledInput, conv1Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{32}), tf.constant(0.0f)));\n+    Relu<TFloat32> relu1 = tf.nn.relu(tf.nn.biasAdd(conv1, conv1Biases));\n+\n+    // First pooling layer\n+    MaxPool<TFloat32> pool1 = tf.nn.maxPool(relu1, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);\n+\n+    // Second conv layer\n+    Variable<TFloat32> conv2Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{5,5,32,64}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Conv2d<TFloat32> conv2 = tf.nn.conv2d(pool1, conv2Weights, Arrays.asList(1L, 1L, 1L, 1L), PADDING_TYPE);\n+    Variable<TFloat32> conv2Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{64}),tf.constant(0.1f)));\n+    Relu<TFloat32> relu2 = tf.nn.relu(tf.nn.biasAdd(conv2, conv2Biases));\n+\n+    // Second pooling layer\n+    MaxPool<TFloat32> pool2 = tf.nn.maxPool(relu2, tf.constant(new int[]{1, 2, 2, 1}), tf.constant(new int[]{1, 2, 2, 1}), PADDING_TYPE);\n+\n+    // Flatten inputs\n+    Reshape<TFloat32> flatten = tf.reshape(pool2, tf.concat(Arrays.asList(tf.slice(tf.shape(pool2), tf.constant(new int[]{0}), tf.constant(new int[]{1})), tf.constant(new int[]{-1})), tf.constant(0)));\n+\n+    // Fully connected layer\n+    Variable<TFloat32> fc1Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{IMAGE_SIZE*IMAGE_SIZE*4,512}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Variable<TFloat32> fc1Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{512}),tf.constant(0.1f)));\n+    Relu<TFloat32> relu3 = tf.nn.relu(tf.math.add(tf.linalg.matMul(flatten, fc1Weights), fc1Biases));\n+\n+    // Softmax layer\n+    Variable<TFloat32> fc2Weights = tf.variableWithInit(tf.math.mul(tf.random.truncatedNormal(tf.constant(new int[]{512,NUM_LABELS}), TFloat32.DTYPE, TruncatedNormal.seed(SEED)), tf.constant(0.1f)));\n+    Variable<TFloat32> fc2Biases = tf.variableWithInit(tf.fill(tf.constant(new int[]{NUM_LABELS}),tf.constant(0.1f)));\n+\n+    Add<TFloat32> logits = tf.math.add(tf.linalg.matMul(relu3, fc2Weights), fc2Biases);\n+\n+    // Predicted outputs\n+    Softmax<TFloat32> prediction = tf.withName(OUTPUT_NAME).nn.softmax(logits);\n+\n+    // Loss function & regularization\n+    OneHot<TFloat32> oneHot = tf.oneHot(labels, tf.constant(10), tf.constant(1.0f), tf.constant(0.0f));\n+    SoftmaxCrossEntropyWithLogits<TFloat32> batchLoss = tf.nn.softmaxCrossEntropyWithLogits(logits, oneHot);\n+    Mean<TFloat32> labelLoss = tf.math.mean(batchLoss.loss(), tf.constant(0));\n+    Add<TFloat32> regularizers = tf.math.add(tf.nn.l2Loss(fc1Weights), tf.math.add(tf.nn.l2Loss(fc1Biases), tf.math.add(tf.nn.l2Loss(fc2Weights), tf.nn.l2Loss(fc2Biases))));\n+    Add<TFloat32> loss = tf.withName(TRAINING_LOSS).math.add(labelLoss, tf.math.mul(regularizers, tf.constant(5e-4f)));\n+\n+    optimizerName = optimizerName.toLowerCase();\n+    // Optimizer\n+    Optimizer optimizer;\n+    switch (optimizerName) {\n+      case \"adadelta\":\n+        optimizer = new AdaDelta(graph, 1f, 0.95f, 1e-8f);\n+        break;\n+      case \"adagradda\":\n+        optimizer = new AdaGradDA(graph, 0.01f);\n+        break;\n+      case \"adagrad\":\n+        optimizer = new AdaGrad(graph, 0.01f);\n+        break;\n+      case \"adam\":\n+        optimizer = new Adam(graph,0.001f,0.9f,0.999f,1e-8f);\n+        break;\n+      case \"sgd\":\n+        optimizer = new GradientDescent(graph,0.01f);\n+        break;\n+      case \"momentum\":\n+        optimizer = new Momentum(graph, 0.01f, 0.9f, false);\n+        break;\n+      case \"rmsprop\":\n+        optimizer = new RMSProp(graph,0.01f, 0.9f, 0.0f, 1e-10f, false);\n+        break;\n+      default:\n+        throw new IllegalArgumentException(\"Unknown optimizer \" + optimizerName);\n+    }\n+    logger.info(\"Optimizer = \" + optimizer.toString());\n+    Op minimize = optimizer.minimize(loss, TRAIN);\n+\n+    Op init = graph.variablesInitializer();\n+\n+    return graph;\n+  }\n+\n+  public static void train(Session session, int epochs, int minibatchSize, float[][][][] data, int[] labels) {\n+    // Initialises the parameters.\n+    session.runner().addTarget(INIT).run();\n+    logger.info(\"Initialised the model parameters\");\n+\n+    float[][][][] featureBatch = new float[minibatchSize][][][];\n+    int[] labelBatch = new int[minibatchSize];\n+\n+    int interval = 0;\n+    for (int i = 0; i < epochs; i++) {\n+      logger.log(Level.INFO, \"Starting epoch \" + i);\n+      //Tensor<?> epoch = Tensor.create(i);\n+      for (int j = 0; j < data.length; j += minibatchSize) {\n+        for (int k = j, m = 0; k < (j + minibatchSize) && k < data.length; k++, m++) {\n+          featureBatch[m] = data[k];\n+          labelBatch[m] = labels[k];\n+        }\n+        //logger.info(\"Batch = \" + batch.size());\n+        Tensor<?> input = Tensor.create(featureBatch);\n+        Tensor<?> target = Tensor.create(labelBatch);\n+        Tensor<?> loss = session.runner()\n+            .feed(INPUT_NAME, input)\n+            .feed(TARGET, target)\n+            .addTarget(TRAIN)\n+            .fetch(TRAINING_LOSS)\n+            .run().get(0);\n+        if (interval % 100 == 0) {\n+          logger.log(Level.INFO, \"Iteration = \" + interval + \", training loss = \" + loss.floatValue());\n+        }\n+        input.close();\n+        target.close();\n+        loss.close();\n+        interval++;\n+      }\n+      //epoch.close();\n+    }\n+  }\n+\n+  /**\n+   * Find the maximum probability and return it's index.\n+   *\n+   * @param probabilities The probabilites.\n+   * @return The index of the max.\n+   */\n+  public static int pred(float[] probabilities) {\n+    float maxVal = Float.NEGATIVE_INFINITY;\n+    int idx = 0;\n+    for (int i = 0; i < probabilities.length; i++) {\n+      if (probabilities[i] > maxVal) {\n+        maxVal = probabilities[i];\n+        idx = i;\n+      }\n+    }\n+    return idx;\n+  }\n+\n+  public static DataTuple loadData(String path) throws IOException, ClassNotFoundException {\n+    try (ObjectInputStream ois = new ObjectInputStream(new BufferedInputStream(new FileInputStream(path)))) {\n+      float[][][][] data = (float[][][][]) ois.readObject();\n+      int[] labels = (int[]) ois.readObject();\n+      return new DataTuple(data, labels);\n+    }\n+  }\n+\n+  private static class DataTuple {\n+    public final float[][][][] features;\n+    public final int[] labels;\n+\n+    public DataTuple(float[][][][] features, int[] labels) {\n+      this.features = features;\n+      this.labels = labels;\n+    }\n+  }\n+\n+  public static void main(String[] args) throws IOException, ClassNotFoundException {\n+    logger.info(\"Usage: MNISTTest <num-epochs> <minibatch-size> <optimizer-name> <train-data-path> <test-data-path>\");\n+\n+    logger.info(\"Loading training data\");\n+    DataTuple train = loadData(args[3]);\n+    logger.info(\"Loading testing data\");\n+    DataTuple test = loadData(args[4]);\n+\n+    logger.info(\"Loaded data.\");\n+\n+    float[][][][] trainData = train.features;\n+    int[] trainLabels = train.labels;\n+\n+    float[][][][] testData = test.features;", "originalCommit": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzExNDQ3OA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r377114478", "bodyText": "That's because I load in multidimensional arrays from the serialised files I have lying around with MNIST in. When we move this over to the java-models repo I can use your MNIST loader which will get rid of this nonsense.", "author": "Craigacp", "createdAt": "2020-02-10T14:59:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MDMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2MjUwOA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r384062508", "bodyText": "Moved the file over to examples.", "author": "Craigacp", "createdAt": "2020-02-25T19:00:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MDMyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MDU4MA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376850580", "bodyText": "2020, also shouldn't it be copyright to The Tensorflow Authors like all other files in the project?", "author": "karllessard", "createdAt": "2020-02-10T02:56:08Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.", "originalCommit": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzExNzUxOA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r377117518", "bodyText": "We don't have an authors file in this repo, plus this is the text that Oracle Legal requires on all outbound source contributions. I'm not sure if putting it in the AUTHORS file will be sufficient for them. It looks like the main TF repo has an AUTHORS, but then it says to look at CONTRIBUTORS which doesn't exist, so maybe someone at Google should figure out what they want it to look like for all the TF related repos.", "author": "Craigacp", "createdAt": "2020-02-10T15:04:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MDU4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MTE0OA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376851148", "bodyText": "I don't think we need to enforce it but just FYI, pretty much all TF classes declare their members in this order, including both fields and methods: public, protected, default, private", "author": "karllessard", "createdAt": "2020-02-10T03:00:04Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.List;\n+\n+/**\n+ * Optimizer that implements the Adadelta algorithm.\n+ *\n+ * See the <a href=\"http://arxiv.org/abs/1212.5701\">paper</a>.\n+ */\n+public class AdaDelta extends Optimizer {\n+\n+  public static final String ACCUMULATOR = \"accum\";\n+  public static final String ACCUMULATOR_UPDATE = \"accum_update\";\n+\n+  private final float learningRate;\n+\n+  private final float rho;\n+\n+  private final float epsilon;", "originalCommit": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzExODEzNA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r377118134", "bodyText": "Do you mean all the public methods & fields, then protected etc, or all the public methods, protected methods, ... then public fields, protected fields etc?", "author": "Craigacp", "createdAt": "2020-02-10T15:05:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MTE0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3MjUyNg==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379172526", "bodyText": "Well in this format, things are grouped per scope, meaning:\n\npublic fields\npublic methods\nprotected fields\nprotected methods\n... and so on\n\nAgain, I don't necessarily want to continue enforcing this in our new repo but maybe we can apply it at least in the core, so that all classes of a single artifact follows the same pattern?", "author": "karllessard", "createdAt": "2020-02-13T23:16:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MTE0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MTc1Mg==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r376851752", "bodyText": "would it be easy to allow the user override the name of the scope for the optimizer ops (instead of getOptimizerName())? Also, maybe a user would like to pass its own instance of Ops (which might already be a subscope of another block or contains control dependencies, etc.)?", "author": "karllessard", "createdAt": "2020-02-10T03:03:50Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/Optimizer.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.NoOp;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *\n+ */\n+public abstract class Optimizer {\n+  public static final String VARIABLE_V2 = \"VariableV2\";\n+\n+  /**\n+   * Top level map key is the variable name, lower level map key is the slot name.\n+   */\n+  private final Map<String, Map<String, Variable<?>>> slots;\n+\n+  /**\n+   * Global state variables\n+   */\n+  //TODO make this be used.\n+  protected final List<Variable<?>> globals;\n+\n+  /**\n+   * The Graph this optimizer is operating on.\n+   */\n+  protected final Graph graph;\n+\n+  /**\n+   * The ops builder for the graph.\n+   */\n+  protected final Ops tf;\n+\n+  protected Optimizer(Graph graph) {\n+    this.graph = graph;\n+    this.tf = Ops.create(graph).withName(getOptimizerName());", "originalCommit": "4aabcc5f79d16ee4f5b1ffe0bd3b7da2360ca4d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzExOTQ4NA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r377119484", "bodyText": "It's locked off to Graph at the moment as I don't think any of this works in eager mode, so the type system enforces that. I can allow a hook for the name, and we could relax the Graph check to an instanceof check throwing IllegalArgumentException to supply an ops. Should I keep the Graph entry point as well?", "author": "Craigacp", "createdAt": "2020-02-10T15:07:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MTc1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ0OTA2Ng==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379449066", "bodyText": "Yes I think it is ok that to have an entry point that accept a Graph but maybe add another that accept an Ops as well, so the user has full control on the name and control dependencies of the optmizers? Or maybe just a String for the name\nFor instance, does it make sense that a user would want to create two different optimizers of the same type (but with different parameters) for handling different variables? If so, then he'll need to give them different names or he will endup with a name conflict.\nI think you understand my questioning here, I'll let you decide what would be the best approach to handle those corner cases.", "author": "karllessard", "createdAt": "2020-02-14T14:10:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MTc1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDAwOTY3NQ==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r384009675", "bodyText": "I added an additional constructor which accepts a Graph and a String for the base name of the operations.", "author": "Craigacp", "createdAt": "2020-02-25T17:12:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1MTc1Mg=="}], "type": "inlineReview"}, {"oid": "7e3459b02ca2ca005de9f7725ea68b20ef19c4b2", "url": "https://github.com/tensorflow/java/commit/7e3459b02ca2ca005de9f7725ea68b20ef19c4b2", "message": "Bumping the copyright year, and switching to try-with-resources in the MNISTTest.", "committedDate": "2020-02-13T02:08:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg3NzcwOQ==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r378877709", "bodyText": "Now that we can rename the endpoint so that they don't necessarily match their operator class, would it make sense to simply name this one variable?\nIt won't conflict with the other generated variable endpoint, that takes a Shape and DataType in input.", "author": "karllessard", "createdAt": "2020-02-13T14:02:05Z", "path": "tensorflow-core/tensorflow-core-api/src/gen/annotations/org/tensorflow/op/Ops.java", "diffHunk": "@@ -7117,6 +7118,19 @@ public VarIsInitializedOp varIsInitializedOp(Operand<?> resource) {\n     return VariableShape.create(scope, input, outType);\n   }\n \n+  /**\n+   * Factory method to create a new Variable with it's initializer.\n+   *\n+   * @param scope current scope\n+   * @param init The op to use to initialise this variable.\n+   * @param options carries optional attributes values\n+   * @return a new instance of Variable\n+   */\n+  public <T extends TType> Variable<T> variableWithInit(Operand<T> init,", "originalCommit": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg4MzQ3Mg==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r378883472", "bodyText": "This class name is a bit misleading, as all other *Ops classes are generated and exposes tf.* endpoints. Here, we are dealing with the endpoint implementations.\nI've never been a huge fan of the *Ops name for the generated classes neither (even if that was my idea if I recall correctly...) and we could rename them instead. Like TensorFlowApi, TensorFlowLinearApi, TensorFlowSparseApi, etc. would be better picks.\nBut if we don't want to do this breaking change then I think we need to come up with something else for this new class.", "author": "karllessard", "createdAt": "2020-02-13T14:11:55Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/CoreOps.java", "diffHunk": "@@ -0,0 +1,45 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Container class for core methods which add or perform several operations\n+ * and return one of them.\n+ */\n+@Operator\n+public abstract class CoreOps {", "originalCommit": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk1ODMxNQ==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r383958315", "bodyText": "I renamed it to Helpers, but that's also not a great name.", "author": "Craigacp", "createdAt": "2020-02-25T15:38:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg4MzQ3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg4NDAxOQ==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r378884019", "bodyText": "Just checking again with you if you tested it out in eager mode. If it fails, then we might prefer to throw explicitly an exception if the environment execution is not a Graph.", "author": "karllessard", "createdAt": "2020-02-13T14:12:54Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/CoreOps.java", "diffHunk": "@@ -0,0 +1,45 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Container class for core methods which add or perform several operations\n+ * and return one of them.\n+ */\n+@Operator\n+public abstract class CoreOps {\n+\n+    /**\n+     * This class contains static factories.\n+     */\n+    private CoreOps() {}\n+\n+    /**\n+     * Factory method to create a new Variable with it's initializer.\n+     *\n+     * @param scope current scope\n+     * @param init The op to use to initialise this variable.\n+     * @param options carries optional attributes values\n+     * @return a new instance of Variable\n+     */\n+    @Endpoint(name=\"variableWithInit\")\n+    public static <T extends TType> Variable<T> createVariableWithInit(Scope scope, Operand<T> init, Variable.Options... options) {\n+        Output<T> initOutput = init.asOutput();\n+        Variable<T> newVar = Variable.create(scope,initOutput.shape(),initOutput.dataType(),options);\n+        Assign<T> assignOp = Assign.create(scope,newVar,init);\n+        ExecutionEnvironment exEnv = scope.env();\n+        if (exEnv instanceof Graph) {\n+            Graph graph = (Graph) exEnv;\n+            graph.addInitializer(assignOp);\n+        }", "originalCommit": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2MTYzNA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r384061634", "bodyText": "It doesn't work in Eager mode, I've changed it to throw an IllegalArgumentException and documented that it doesn't work.", "author": "Craigacp", "createdAt": "2020-02-25T18:59:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg4NDAxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3Mzc3NQ==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379173775", "bodyText": "Would it be simpler to use Zeros for initializing this tensor? (I'm really asking, I'm not too sure if this Zeros operator is useful at all)", "author": "karllessard", "createdAt": "2020-02-13T23:20:24Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.List;\n+\n+/**\n+ * Optimizer that implements the Adadelta algorithm.\n+ * <p>\n+ * See the <a href=\"http://arxiv.org/abs/1212.5701\">paper</a>.\n+ */\n+public class AdaDelta extends Optimizer {\n+\n+  public static final String ACCUMULATOR = \"accum\";\n+  public static final String ACCUMULATOR_UPDATE = \"accum_update\";\n+\n+  private final float learningRate;\n+\n+  private final float rho;\n+\n+  private final float epsilon;\n+\n+  public AdaDelta(Graph graph, float learningRate) {\n+    this(graph, learningRate, 0.95f, 1e-8f);\n+  }\n+\n+  public AdaDelta(Graph graph, float learningRate, float rho, float epsilon) {\n+    super(graph);\n+    this.learningRate = learningRate;\n+    this.rho = rho;\n+    this.epsilon = epsilon;\n+  }\n+\n+  @Override\n+  protected void createSlots(List<Output<? extends TType>> variables) {\n+    for (Output<? extends TType> v : variables) {\n+      createAdaDeltaSlot(v);\n+    }\n+  }\n+\n+  private <T extends TType> void createAdaDeltaSlot(Output<T> v) {\n+    Operand<T> accumulatorInitializer = tf", "originalCommit": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3NjczNg==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379176736", "bodyText": "this tf.constant(Object, DataType) won't convert automatically your float values to the gradient type, so it will probably fail if you test with variable other that TFloat32. Plus, I've removed it in this PR, e.g. only tf.val(rho) is available now.\nIf you really need to convert your constants to the gradient datatype, then you'll need to do an explicit tf.cast(tf.val(rho)).\nThe same comment applies for other optimizers.", "author": "karllessard", "createdAt": "2020-02-13T23:29:52Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.List;\n+\n+/**\n+ * Optimizer that implements the Adadelta algorithm.\n+ * <p>\n+ * See the <a href=\"http://arxiv.org/abs/1212.5701\">paper</a>.\n+ */\n+public class AdaDelta extends Optimizer {\n+\n+  public static final String ACCUMULATOR = \"accum\";\n+  public static final String ACCUMULATOR_UPDATE = \"accum_update\";\n+\n+  private final float learningRate;\n+\n+  private final float rho;\n+\n+  private final float epsilon;\n+\n+  public AdaDelta(Graph graph, float learningRate) {\n+    this(graph, learningRate, 0.95f, 1e-8f);\n+  }\n+\n+  public AdaDelta(Graph graph, float learningRate, float rho, float epsilon) {\n+    super(graph);\n+    this.learningRate = learningRate;\n+    this.rho = rho;\n+    this.epsilon = epsilon;\n+  }\n+\n+  @Override\n+  protected void createSlots(List<Output<? extends TType>> variables) {\n+    for (Output<? extends TType> v : variables) {\n+      createAdaDeltaSlot(v);\n+    }\n+  }\n+\n+  private <T extends TType> void createAdaDeltaSlot(Output<T> v) {\n+    Operand<T> accumulatorInitializer = tf\n+        .fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f, TFloat32.DTYPE), v.dataType()));\n+    createSlot(v.asOutput(), ACCUMULATOR, accumulatorInitializer);\n+    Operand<T> updateInitializer = tf\n+        .fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f, TFloat32.DTYPE), v.dataType()));\n+    createSlot(v.asOutput(), ACCUMULATOR_UPDATE, updateInitializer);\n+  }\n+\n+  @Override\n+  protected <T extends TType> Operand<T> applyDense(Output<T> gradient, Output<T> variable) {\n+    Variable<T> accumSlot = getSlot(variable, ACCUMULATOR).get();\n+    Variable<T> accumUpdateSlot = getSlot(variable, ACCUMULATOR_UPDATE).get();\n+    return tf.train.applyAdadelta(variable, accumSlot, accumUpdateSlot,\n+        tf.constant(learningRate, gradient.dataType()),\n+        tf.constant(rho, gradient.dataType()),\n+        tf.constant(epsilon, gradient.dataType()),", "originalCommit": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3ODcwNQ==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379178705", "bodyText": "Nit: spaces after commas", "author": "karllessard", "createdAt": "2020-02-13T23:35:12Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/Adam.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+/**\n+ * Optimizer that implements the Adam algorithm.\n+ * <p>\n+ * See the <a href=\"http://arxiv.org/abs/1412.6980\">paper</a>.\n+ */\n+@Operator\n+public class Adam extends Optimizer {\n+\n+  public static final String FIRST_MOMENT = \"m\";\n+  public static final String SECOND_MOMENT = \"v\";\n+\n+  private final float learningRate;\n+\n+  private final float betaOne;\n+\n+  private final float betaTwo;\n+\n+  private final float epsilon;\n+\n+  private Constant<TFloat32> learningRateConst;\n+  private Constant<TFloat32> epsilonConst;\n+  private Constant<TFloat32> betaOneConst;\n+  private Constant<TFloat32> betaTwoConst;\n+  private Variable<TFloat32> betaOnePower;\n+  private Variable<TFloat32> betaTwoPower;\n+\n+  public Adam(Graph graph, float learningRate) {\n+    this(graph, learningRate, 0.9f, 0.999f, 1e-8f);\n+  }\n+\n+  public Adam(Graph graph, float learningRate, float betaOne, float betaTwo, float epsilon) {\n+    super(graph);\n+    this.learningRate = learningRate;\n+    this.betaOne = betaOne;\n+    this.betaTwo = betaTwo;\n+    this.epsilon = epsilon;\n+  }\n+\n+  @Endpoint(name=\"adam_minimize\")\n+  public static <T extends TType> Op createAdamMinimize(Scope scope, Operand<T> loss, float learningRate, float betaOne, float betaTwo, float epsilon, Optimizer.Options... options) {\n+    if (!(scope.env() instanceof Graph)) {\n+      throw new IllegalArgumentException(\"Optimizers are only supported on Graphs\");\n+    }\n+    Adam adam = new Adam((Graph)scope.env(),learningRate,betaOne,betaTwo,epsilon);", "originalCommit": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3OTA5MA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379179090", "bodyText": "Again, Zeros maybe?", "author": "karllessard", "createdAt": "2020-02-13T23:36:18Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/Adam.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.tools.Shape;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+/**\n+ * Optimizer that implements the Adam algorithm.\n+ * <p>\n+ * See the <a href=\"http://arxiv.org/abs/1412.6980\">paper</a>.\n+ */\n+@Operator\n+public class Adam extends Optimizer {\n+\n+  public static final String FIRST_MOMENT = \"m\";\n+  public static final String SECOND_MOMENT = \"v\";\n+\n+  private final float learningRate;\n+\n+  private final float betaOne;\n+\n+  private final float betaTwo;\n+\n+  private final float epsilon;\n+\n+  private Constant<TFloat32> learningRateConst;\n+  private Constant<TFloat32> epsilonConst;\n+  private Constant<TFloat32> betaOneConst;\n+  private Constant<TFloat32> betaTwoConst;\n+  private Variable<TFloat32> betaOnePower;\n+  private Variable<TFloat32> betaTwoPower;\n+\n+  public Adam(Graph graph, float learningRate) {\n+    this(graph, learningRate, 0.9f, 0.999f, 1e-8f);\n+  }\n+\n+  public Adam(Graph graph, float learningRate, float betaOne, float betaTwo, float epsilon) {\n+    super(graph);\n+    this.learningRate = learningRate;\n+    this.betaOne = betaOne;\n+    this.betaTwo = betaTwo;\n+    this.epsilon = epsilon;\n+  }\n+\n+  @Endpoint(name=\"adam_minimize\")\n+  public static <T extends TType> Op createAdamMinimize(Scope scope, Operand<T> loss, float learningRate, float betaOne, float betaTwo, float epsilon, Optimizer.Options... options) {\n+    if (!(scope.env() instanceof Graph)) {\n+      throw new IllegalArgumentException(\"Optimizers are only supported on Graphs\");\n+    }\n+    Adam adam = new Adam((Graph)scope.env(),learningRate,betaOne,betaTwo,epsilon);\n+    String name = null;\n+    for (Options o : options) {\n+      if (o.sharedName != null) {\n+        name = o.sharedName;\n+      }\n+    }\n+    if (name == null) {\n+      return adam.minimize(loss);\n+    } else {\n+      return adam.minimize(loss,name);\n+    }\n+  }\n+\n+  @Override\n+  protected void createSlots(List<Output<? extends TType>> variables) {\n+    for (Output<? extends TType> v : variables) {\n+      createAdamSlot(v.asOutput());\n+    }\n+    betaOnePower = tf.withName(\"beta1_power\").variable(Shape.scalar(), TFloat32.DTYPE);\n+    Assign<TFloat32> betaOnePowerInit = tf\n+        .assign(betaOnePower, tf.constant(betaOne, TFloat32.DTYPE));\n+    graph.addInitializer(betaOnePowerInit);\n+    betaTwoPower = tf.withName(\"beta2_power\").variable(Shape.scalar(), TFloat32.DTYPE);\n+    Assign<TFloat32> betaTwoPowerInit = tf\n+        .assign(betaTwoPower, tf.constant(betaTwo, TFloat32.DTYPE));\n+    graph.addInitializer(betaTwoPowerInit);\n+  }\n+\n+  @Override\n+  protected Optional<Operand<?>> prepare(String scopeName) {\n+    betaOneConst = tf.constant(betaOne);\n+    betaTwoConst = tf.constant(betaTwo);\n+    learningRateConst = tf.constant(learningRate);\n+    epsilonConst = tf.constant(epsilon);\n+    return Optional.empty();\n+  }\n+\n+  private <T extends TType> void createAdamSlot(Output<T> v) {\n+    Operand<T> firstMomentInitializer = tf\n+        .fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f, TFloat32.DTYPE), v.dataType()));\n+    createSlot(v.asOutput(), FIRST_MOMENT, firstMomentInitializer);\n+    Operand<T> secondMomentInitializer = tf\n+        .fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f, TFloat32.DTYPE), v.dataType()));", "originalCommit": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3OTU3OQ==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379179579", "bodyText": "Missing doc", "author": "karllessard", "createdAt": "2020-02-13T23:37:54Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/Optimizer.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.NoOp;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *", "originalCommit": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ1MDc5OA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379450798", "bodyText": "graph.operations().forEachRemaining maybe?\nAlso, is it OK that an optimizer is always applied to all variables in the graph? Is this true for all kind of graphs?", "author": "karllessard", "createdAt": "2020-02-14T14:13:55Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/Optimizer.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.NoOp;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *\n+ */\n+public abstract class Optimizer {\n+\n+  public static final String VARIABLE_V2 = \"VariableV2\";\n+\n+  /**\n+   * Optional attributes for {@link org.tensorflow.training.optimizers.Optimizer}\n+   */\n+  public static class Options {\n+\n+    /**\n+     * @param sharedName If non-empty, this variable is named in the given bucket\n+     * with this shared_name. Otherwise, the node name is used instead.\n+     */\n+    public Optimizer.Options sharedName(String sharedName) {\n+      this.sharedName = sharedName;\n+      return this;\n+    }\n+\n+    protected String sharedName;\n+\n+    private Options() {\n+    }\n+  }\n+  /**\n+   * Top level map key is the variable name, lower level map key is the slot name.\n+   */\n+  private final Map<String, Map<String, Variable<?>>> slots;\n+\n+  /**\n+   * Global state variables\n+   */\n+  //TODO make this be used.\n+  protected final List<Variable<?>> globals;\n+\n+  /**\n+   * The Graph this optimizer is operating on.\n+   */\n+  protected final Graph graph;\n+\n+  /**\n+   * The ops builder for the graph.\n+   */\n+  protected final Ops tf;\n+\n+  protected Optimizer(Graph graph) {\n+    this.graph = graph;\n+    this.tf = Ops.create(graph).withName(getOptimizerName());\n+    this.slots = new HashMap<>();\n+    this.globals = new ArrayList<>();\n+  }\n+\n+  public Op minimize(Operand<?> loss) {\n+    return minimize(loss, getOptimizerName() + \"-minimize\");\n+  }\n+\n+  public Op minimize(Operand<?> loss, String name) {\n+    List<GradAndVar<?>> gradsAndVars = computeGradients(loss);\n+\n+    return applyGradients(gradsAndVars, name);\n+  }\n+\n+  public <T extends TType> List<GradAndVar<?>> computeGradients(Operand<?> loss) {\n+    List<Operation> variables = new ArrayList<>();\n+    Iterator<Operation> opItr = graph.operations();\n+    while (opItr.hasNext()) {\n+      Operation op = opItr.next();", "originalCommit": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk1NjgxMQ==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r383956811", "bodyText": "That's how they work in Python. It's really hard to make it work any other way without a lot of ceremony.", "author": "Craigacp", "createdAt": "2020-02-25T15:36:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ1MDc5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ1Nzk0Mg==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379457942", "bodyText": "Nit: either it is GitHub or there is a wrong left margin to this line. Also, no need of else blocks below since you return in previous blocks.", "author": "karllessard", "createdAt": "2020-02-14T14:27:26Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/Optimizer.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Operation;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.NoOp;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *\n+ */\n+public abstract class Optimizer {\n+\n+  public static final String VARIABLE_V2 = \"VariableV2\";\n+\n+  /**\n+   * Optional attributes for {@link org.tensorflow.training.optimizers.Optimizer}\n+   */\n+  public static class Options {\n+\n+    /**\n+     * @param sharedName If non-empty, this variable is named in the given bucket\n+     * with this shared_name. Otherwise, the node name is used instead.\n+     */\n+    public Optimizer.Options sharedName(String sharedName) {\n+      this.sharedName = sharedName;\n+      return this;\n+    }\n+\n+    protected String sharedName;\n+\n+    private Options() {\n+    }\n+  }\n+  /**\n+   * Top level map key is the variable name, lower level map key is the slot name.\n+   */\n+  private final Map<String, Map<String, Variable<?>>> slots;\n+\n+  /**\n+   * Global state variables\n+   */\n+  //TODO make this be used.\n+  protected final List<Variable<?>> globals;\n+\n+  /**\n+   * The Graph this optimizer is operating on.\n+   */\n+  protected final Graph graph;\n+\n+  /**\n+   * The ops builder for the graph.\n+   */\n+  protected final Ops tf;\n+\n+  protected Optimizer(Graph graph) {\n+    this.graph = graph;\n+    this.tf = Ops.create(graph).withName(getOptimizerName());\n+    this.slots = new HashMap<>();\n+    this.globals = new ArrayList<>();\n+  }\n+\n+  public Op minimize(Operand<?> loss) {\n+    return minimize(loss, getOptimizerName() + \"-minimize\");\n+  }\n+\n+  public Op minimize(Operand<?> loss, String name) {\n+    List<GradAndVar<?>> gradsAndVars = computeGradients(loss);\n+\n+    return applyGradients(gradsAndVars, name);\n+  }\n+\n+  public <T extends TType> List<GradAndVar<?>> computeGradients(Operand<?> loss) {\n+    List<Operation> variables = new ArrayList<>();\n+    Iterator<Operation> opItr = graph.operations();\n+    while (opItr.hasNext()) {\n+      Operation op = opItr.next();\n+      if (op.type().equals(VARIABLE_V2)) {\n+        variables.add(op);\n+      }\n+    }\n+\n+    Output<?>[] variableOutputArray = new Output[variables.size()];\n+    for (int i = 0; i < variables.size(); i++) {\n+      // First output of a variable is it's output.\n+      variableOutputArray[i] = variables.get(i).output(0);\n+    }\n+\n+    Output<?>[] gradients = graph.addGradients(loss.asOutput(), variableOutputArray);\n+    List<GradAndVar<? extends TType>> gradVarPairs = new ArrayList<>();\n+\n+    for (int i = 0; i < variableOutputArray.length; i++) {\n+      @SuppressWarnings(\"unchecked\")\n+      Output<T> typedGrad = (Output<T>) gradients[i];\n+      @SuppressWarnings(\"unchecked\")\n+      Output<T> typedVar = (Output<T>) variableOutputArray[i];\n+      gradVarPairs.add(new GradAndVar<>(typedGrad, typedVar));\n+    }\n+\n+    return gradVarPairs;\n+  }\n+\n+  public Op applyGradients(List<GradAndVar<? extends TType>> gradsAndVars, String name) {\n+    List<Output<? extends TType>> variables = gradsAndVars.stream().map(GradAndVar::getVariable)\n+        .collect(Collectors.toList());\n+\n+    createSlots(variables);\n+\n+    Optional<Operand<? extends TType>> prepOp = prepare(name + \"/prepare\");\n+\n+    List<Operand<? extends TType>> updateOps = new ArrayList<>();\n+    prepOp.ifPresent(updateOps::add);\n+    for (GradAndVar<? extends TType> pair : gradsAndVars) {\n+      updateOps.add(applyDense(pair));\n+    }\n+\n+    return finish(updateOps, name);\n+  }\n+\n+  /**\n+   * Gets the slot associated with the specified variable and slot name.\n+   *\n+   * @param var      The variable to lookup.\n+   * @param slotName The slot name.\n+   * @return The slot or {@link Optional#empty}.\n+   */\n+  public <T extends TType> Optional<Variable<T>> getSlot(Output<T> var, String slotName) {\n+    return getSlot(var.op().name(), slotName);\n+  }\n+\n+  /**\n+   * Gets the slot associated with the specified variable and slot name.\n+   *\n+   * @param varName  The variable to lookup.\n+   * @param slotName The slot name.\n+   * @return The slot or {@link Optional#empty}.\n+   */\n+  private <T extends TType> Optional<Variable<T>> getSlot(String varName, String slotName) {\n+    Map<String, Variable<? extends TType>> variables = slots.get(slotName);\n+    if (variables != null) {\n+      Variable<? extends TType> slot = variables.get(varName);\n+      if (slot != null) {\n+        @SuppressWarnings(\"unchecked\") // This method should only be called when the type is known.\n+            Optional<Variable<T>> opt = Optional.of((Variable<T>) slot);", "originalCommit": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk1NTA1Ng==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r383955056", "bodyText": "The Google code formatter in IntelliJ keeps doing this, I'll keep moving it back. It's because it treats the annotation and the variable creation as the same statement, and thinks it's a line continuation.", "author": "Craigacp", "createdAt": "2020-02-25T15:33:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ1Nzk0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ1OTM5OA==", "url": "https://github.com/tensorflow/java/pull/28#discussion_r379459398", "bodyText": "Nit: no need of else block", "author": "karllessard", "createdAt": "2020-02-14T14:30:15Z", "path": "tensorflow-training/src/main/java/org/tensorflow/training/optimizers/RMSProp.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.tensorflow.training.optimizers;\n+\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+import java.util.List;\n+\n+/**\n+ * Optimizer that implements the RMSProp algorithm.\n+ * <p>\n+ * See the <a href=\"http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\">lecture\n+ * notes</a> that is inexplicably the canonical reference.\n+ */\n+public class RMSProp extends Optimizer {\n+\n+  public static final String RMS = \"rms\";\n+  public static final String MG = \"mg\"; // mean gradient?\n+  public static final String MOMENTUM = \"momentum\";\n+\n+  private final float learningRate;\n+  private final float decay;\n+  private final float momentum;\n+  private final float epsilon;\n+  private final boolean centered;\n+\n+  public RMSProp(Graph graph, float learningRate) {\n+    this(graph, learningRate, 0.9f, 0.0f, 1e-10f, false);\n+  }\n+\n+  public RMSProp(Graph graph, float learningRate, float decay, float momentum, float epsilon,\n+      boolean centered) {\n+    super(graph);\n+    this.learningRate = learningRate;\n+    this.decay = decay;\n+    this.momentum = momentum;\n+    this.epsilon = epsilon;\n+    this.centered = centered;\n+  }\n+\n+  @Override\n+  protected void createSlots(List<Output<? extends TType>> variables) {\n+    for (Output<? extends TType> v : variables) {\n+      createRMSPropSlot(v);\n+    }\n+  }\n+\n+  private <T extends TType> void createRMSPropSlot(Output<T> v) {\n+    Operand<T> rmsInitializer = tf\n+        .fill(tf.shape(v), tf.dtypes.cast(tf.constant(1.0f, TFloat32.DTYPE), v.dataType()));\n+    createSlot(v.asOutput(), RMS, rmsInitializer);\n+    Operand<T> momentumInitializer = tf\n+        .fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f, TFloat32.DTYPE), v.dataType()));\n+    createSlot(v.asOutput(), MOMENTUM, momentumInitializer);\n+    if (centered) {\n+      Operand<T> mgInitializer = tf\n+          .fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f, TFloat32.DTYPE), v.dataType()));\n+      createSlot(v.asOutput(), MG, mgInitializer);\n+    }\n+  }\n+\n+  @Override\n+  protected <T extends TType> Operand<T> applyDense(Output<T> gradient, Output<T> variable) {\n+    Variable<T> rmsSlot = getSlot(variable, RMS).get();\n+    Variable<T> momentumSlot = getSlot(variable, MOMENTUM).get();\n+    if (centered) {\n+      Variable<T> mgSlot = getSlot(variable, MG).get();\n+      return tf.train.applyCenteredRmsProp(variable, mgSlot, rmsSlot, momentumSlot,\n+          tf.constant(learningRate, gradient.dataType()),\n+          tf.constant(decay, gradient.dataType()),\n+          tf.constant(momentum, gradient.dataType()),\n+          tf.constant(epsilon, gradient.dataType()),\n+          gradient);\n+    } else {", "originalCommit": "02c25c25a33c6d4e86ea1eb21bcd6760706724d1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9e9268791cddd0360cc646039994f20cb0bac808", "url": "https://github.com/tensorflow/java/commit/9e9268791cddd0360cc646039994f20cb0bac808", "message": "Initial commit of gradient descent optimizers.", "committedDate": "2020-02-25T13:34:18Z", "type": "commit"}, {"oid": "68a353c1fb555ac8a4e39f0ed65ac75de77864f4", "url": "https://github.com/tensorflow/java/commit/68a353c1fb555ac8a4e39f0ed65ac75de77864f4", "message": "Adding Apache 2.0 license header to all optimizer files.", "committedDate": "2020-02-25T13:34:18Z", "type": "commit"}, {"oid": "b3f4be8ee32caeeb503ff625fcd8c95e4d0f33cd", "url": "https://github.com/tensorflow/java/commit/b3f4be8ee32caeeb503ff625fcd8c95e4d0f33cd", "message": "Bug fix for the MNISTTest.", "committedDate": "2020-02-25T13:34:18Z", "type": "commit"}, {"oid": "d1868ea31b7d0ca9b01cc0ad01bafb6f22c5536f", "url": "https://github.com/tensorflow/java/commit/d1868ea31b7d0ca9b01cc0ad01bafb6f22c5536f", "message": "Refactor to uptake latest tensorflow-core changes.", "committedDate": "2020-02-25T13:34:18Z", "type": "commit"}, {"oid": "6d189cc79d58d968bd004c7c0c425c4dccfe802b", "url": "https://github.com/tensorflow/java/commit/6d189cc79d58d968bd004c7c0c425c4dccfe802b", "message": "Added type safety and updates for new api.", "committedDate": "2020-02-25T13:34:18Z", "type": "commit"}, {"oid": "53e438a76ea768341ae8bc4bd5aa8c1db4cca77a", "url": "https://github.com/tensorflow/java/commit/53e438a76ea768341ae8bc4bd5aa8c1db4cca77a", "message": "Small changes, plus a fix for DataTypes to include references to the type.", "committedDate": "2020-02-25T13:35:00Z", "type": "commit"}, {"oid": "83140b46bc1d4a712a81c4ac7c7b13776770eb0c", "url": "https://github.com/tensorflow/java/commit/83140b46bc1d4a712a81c4ac7c7b13776770eb0c", "message": "Repackaging the optimizers into tensorflow-training, org.tensorflow.training.", "committedDate": "2020-02-25T13:35:48Z", "type": "commit"}, {"oid": "b2ac923b5c61b00f04bac75b0735ebfc3177d4cc", "url": "https://github.com/tensorflow/java/commit/b2ac923b5c61b00f04bac75b0735ebfc3177d4cc", "message": "Initial commit of gradient descent optimizers.", "committedDate": "2020-02-25T13:37:32Z", "type": "commit"}, {"oid": "e7eb2e8c4e3597f4e05184af9ae70181f346e744", "url": "https://github.com/tensorflow/java/commit/e7eb2e8c4e3597f4e05184af9ae70181f346e744", "message": "Adding Apache 2.0 license header to all optimizer files.", "committedDate": "2020-02-25T13:37:32Z", "type": "commit"}, {"oid": "3d63564458081e5c5bca6405aed0952ef346d383", "url": "https://github.com/tensorflow/java/commit/3d63564458081e5c5bca6405aed0952ef346d383", "message": "Bug fix for the MNISTTest.", "committedDate": "2020-02-25T13:37:32Z", "type": "commit"}, {"oid": "ed71dc55ca5cad1d365f548166e0e853b62ba8e2", "url": "https://github.com/tensorflow/java/commit/ed71dc55ca5cad1d365f548166e0e853b62ba8e2", "message": "Refactor to uptake latest tensorflow-core changes.", "committedDate": "2020-02-25T13:37:32Z", "type": "commit"}, {"oid": "b0544494775ede7f856f454cfc2f3f8c20303f24", "url": "https://github.com/tensorflow/java/commit/b0544494775ede7f856f454cfc2f3f8c20303f24", "message": "Added type safety and updates for new api.", "committedDate": "2020-02-25T13:37:32Z", "type": "commit"}, {"oid": "b29be50c4a61bcd43afea1a163fd669f6246562a", "url": "https://github.com/tensorflow/java/commit/b29be50c4a61bcd43afea1a163fd669f6246562a", "message": "Repackaging the optimizers into tensorflow-training, org.tensorflow.training.", "committedDate": "2020-02-25T13:37:32Z", "type": "commit"}, {"oid": "6cdb55c16b4437e5396f8959ccb1348212b0ec1f", "url": "https://github.com/tensorflow/java/commit/6cdb55c16b4437e5396f8959ccb1348212b0ec1f", "message": "Delete pom.xml", "committedDate": "2020-02-25T13:39:13Z", "type": "commit"}, {"oid": "b9d64c5885e169d5202f335b70992e9afb2af124", "url": "https://github.com/tensorflow/java/commit/b9d64c5885e169d5202f335b70992e9afb2af124", "message": "Googlify with IntelliJ's Google Java Style Guide formatter.", "committedDate": "2020-02-25T13:40:26Z", "type": "commit"}, {"oid": "6ae5ace082234c9c78e8d42efe550be42032edad", "url": "https://github.com/tensorflow/java/commit/6ae5ace082234c9c78e8d42efe550be42032edad", "message": "Bumping the copyright year, and switching to try-with-resources in the MNISTTest.", "committedDate": "2020-02-25T13:40:26Z", "type": "commit"}, {"oid": "6ae5ace082234c9c78e8d42efe550be42032edad", "url": "https://github.com/tensorflow/java/commit/6ae5ace082234c9c78e8d42efe550be42032edad", "message": "Bumping the copyright year, and switching to try-with-resources in the MNISTTest.", "committedDate": "2020-02-25T13:40:26Z", "type": "forcePushed"}, {"oid": "56429e80522c205b2327a5c838dfe98894bcfe23", "url": "https://github.com/tensorflow/java/commit/56429e80522c205b2327a5c838dfe98894bcfe23", "message": "Updating variableWithInit to use @Endpoint.", "committedDate": "2020-02-25T15:08:23Z", "type": "commit"}, {"oid": "5d8cb690974d9af71331dafa25891cd06fa8d123", "url": "https://github.com/tensorflow/java/commit/5d8cb690974d9af71331dafa25891cd06fa8d123", "message": "Refactorings after code review.", "committedDate": "2020-02-25T15:40:56Z", "type": "commit"}, {"oid": "51f5d47e87de878c16cf252a1e8e2578717ecbab", "url": "https://github.com/tensorflow/java/commit/51f5d47e87de878c16cf252a1e8e2578717ecbab", "message": "Adding a couple of lines to the gitignore.", "committedDate": "2020-02-25T15:41:20Z", "type": "commit"}, {"oid": "66876eddb2403d5ea60a96b42f071545b45802bd", "url": "https://github.com/tensorflow/java/commit/66876eddb2403d5ea60a96b42f071545b45802bd", "message": "Adding a bit of documentation, threading the named operations through the constructors, removing the MNISTTtest.", "committedDate": "2020-02-25T17:11:57Z", "type": "commit"}, {"oid": "7a2fd256ff60786165f6f14772b9af05a0d3c9f2", "url": "https://github.com/tensorflow/java/commit/7a2fd256ff60786165f6f14772b9af05a0d3c9f2", "message": "Adding a guard to prevent variableWithInit being called on an EagerSession.", "committedDate": "2020-02-25T17:47:49Z", "type": "commit"}, {"oid": "1b98f5227169bd09abd1390039ea6c41cf09075c", "url": "https://github.com/tensorflow/java/commit/1b98f5227169bd09abd1390039ea6c41cf09075c", "message": "Update Ops.java", "committedDate": "2020-03-02T03:29:09Z", "type": "commit"}]}