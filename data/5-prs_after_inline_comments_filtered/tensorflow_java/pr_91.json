{"pr_number": 91, "pr_title": "Initial checkin of Keras Optimzers and helper classes.", "pr_createdAt": "2020-07-28T21:03:34Z", "pr_url": "https://github.com/tensorflow/java/pull/91", "timeline": [{"oid": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "url": "https://github.com/tensorflow/java/commit/ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "message": "Initial checkin of Keras Optimzers and helper classes.\nFixed dependencies in pom.xml", "committedDate": "2020-07-28T20:26:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxNjEyNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463616125", "bodyText": "Can we type these Operands generically? Many of them seem to be missing types.", "author": "Craigacp", "createdAt": "2020-07-31T13:38:45Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2NjQwNA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463966404", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-08-01T14:19:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxNjEyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE1NTQyMQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474155421", "bodyText": "The K.java and backend package has been removed from this PR. It will be added  later when needed for future Keras PRs.", "author": "JimClarke5", "createdAt": "2020-08-20T17:29:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxNjEyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxNzQxNA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463617414", "bodyText": "Do we have the String op names anywhere? If not we should figure out how to add them to the generated ops so we don't have random string constants. cc @karllessard", "author": "Craigacp", "createdAt": "2020-07-31T13:40:56Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTAyNjY5NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r465026695", "bodyText": "I found \"Sigmoid\" in tensorflow-core/tensorflow-core-api/src/bazel/api_def/api_def_Sigmoid.pbtxt.\nop {\n  graph_op_name: \"Sigmoid\"\n  endpoint {\n    name: \"math.Sigmoid\"\n  }\n}\n\nI am not sure how these files are parsed, but it seems the String name could be used to create a public static final String in the generated Java file.", "author": "JimClarke5", "createdAt": "2020-08-04T12:52:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxNzQxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAyOTg2MA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r467029860", "bodyText": "The graph_op_name is processed C++ code in the tensorflow-core-api/src/bazel/op_generator directory.\nI modified source_writer.cc, source_writer.h to add a method WriteFieldWithInitializer and modified op_generator.cc to output the following static field for every generated Operator.\n/** The name of this op, as known by TensorFlow core engine */\n  public static final String OP_NAME = \"SoftmaxCrossEntropyWithLogits\";\n\nThis allows access to the graph operation name using, for example, SoftmaxCrossEntropyWithLogits.OP_NAME rather than using a raw string.\nIf everyone agrees with this approach,  I can push these changes", "author": "JimClarke5", "createdAt": "2020-08-07T13:10:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxNzQxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2Mzk3NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469663975", "bodyText": "Congrats @JimClarke5 , that is the right way of doing it \ud83d\udc4d", "author": "karllessard", "createdAt": "2020-08-13T02:43:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxNzQxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE1NjMzNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474156335", "bodyText": "I will be checking in the change for the bazel/op_generator C++ code.", "author": "JimClarke5", "createdAt": "2020-08-20T17:31:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxNzQxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxOTA0OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463619049", "bodyText": "If they are in python TF, then yes. You can add them to the ops system by moving them to a helper class in core and annotating them appropriately.", "author": "Craigacp", "createdAt": "2020-07-31T13:43:57Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2Njc0OA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463966748", "bodyText": "We can do this for method names that are not currently used in Java tf.nn, but we need to resolve the issue pointed out with softmax_cross_entropy_with_logits", "author": "JimClarke5", "createdAt": "2020-08-01T14:24:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxOTA0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA0MTg4OA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r467041888", "bodyText": "I am resolving this conversation as it is directly tied to the conversation on softmax_cross_entropy_with_logits", "author": "JimClarke5", "createdAt": "2020-08-07T13:31:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYxOTA0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463620028", "bodyText": "We've already got this one (https://github.com/tensorflow/java/blob/master/tensorflow-core/tensorflow-core-api/src/gen/java/org/tensorflow/op/nn/SoftmaxCrossEntropyWithLogits.java). Is the Keras one different in some way?", "author": "Craigacp", "createdAt": "2020-07-31T13:45:39Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2MzcyNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463963725", "bodyText": "Python TensorFlow defines tf.nn.softmax_cross_entropy_with_logits in nn_ops.py. which calls softmax_cross_entropy_with_logits_v2_helper in the same file, (reimplemented in Java with this method),\nsoftmax_cross_entropy_with_logits_v2_helper manipulates the shapes before calling gen_nn_ops.softmax_cross_entropy_with_logits (equivalent to the current tf.nn.softmax_cross_entropy_with_logits in Java). It appears that the low level op in python is not exposed to the high-level API but is fronted with python logic in softmax_cross_entropy_with_logits_v2_helper.\nI have found this pattern a couple of times in python where python code fronts the call to the C api and the actual C api is not exposed in the higher level TF api.", "author": "JimClarke5", "createdAt": "2020-08-01T13:46:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4OTY1Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463989653", "bodyText": "Ok, so sounds like we should move the current one we've got into a less obvious place, rename it and replace the one in tf.nn with yours.", "author": "Craigacp", "createdAt": "2020-08-01T18:54:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDAwMjEzMw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r464002133", "bodyText": "I could overload softmaxCrossEntropyWithLogits by adding the axis parameter.\nThat would create 2 versions of softmaxCrossEntropyWithLogits though.\npublic static <U extends TType, T extends TNumber> Operand<T> `softmaxCrossEntropyWithLogits`(\n            Scope scope, Operand<T> labels, Operand<U> logits, int axis)", "author": "JimClarke5", "createdAt": "2020-08-01T21:30:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDAwMzYwNw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r464003607", "bodyText": "But then I cannot do this with sparseSoftmaxCrossEntropyWithLogits. We would need either to come up with a naming convention that distinguishes the two, or hide the low level version from NnOps. They would only be accessible through the higher level api, at least from NnOps.", "author": "JimClarke5", "createdAt": "2020-08-01T21:50:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDAwNzMyMQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r464007321", "bodyText": "We can make tf.nn.internal and put the low level ones there. Or rename them to include internal in the name. @karllessard what do you think?", "author": "Craigacp", "createdAt": "2020-08-01T22:40:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4Njk1NA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r464386954", "bodyText": "I favor tf.nn.internal, so far this applies to  softmaxCrossEntropyWithLogits and sparseSoftmaxCrossEntropyWithLogits. There are probably more.", "author": "JimClarke5", "createdAt": "2020-08-03T12:40:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAzMzY1MQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r467033651", "bodyText": "I have made these changes adding tf.nn.internal.softmaxCrossEntropyWithLogits() and tf.nn.internal. sparseSoftmaxCrossEntropyWithLogits and created  NN.java in package org.tensorflow.op.core that creates\ntf.nn.softmaxCrossEntropyWithLogits()  and tf.nn.sparseSoftmaxCrossEntropyWithLogits() for the higher level functionality. Both of these classes use the org.tensorflow.op.nn.internal.SoftmaxCrossEntropyWithLogits and org.tensorflow.op.nn.internal.SparseSoftmaxCrossEntropyWithLogits classes.\nIf this is OK with everyone, I can push these changes.", "author": "JimClarke5", "createdAt": "2020-08-07T13:17:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2ODkwNg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469668906", "bodyText": "I'm ok with moving raw ops that we front in a .internal subpackage (or maybe tf.nn.raw?) To do this @JimClarke5 , just have just updated the API def for that op, right?\nThe original initial idea though was to leave all ops in Ops as closest as possible to their raw implementation in the native library and to wrap them with high-level manipulations only in tensorflow-framework. I think what prevent us to do this efficiently is that the annotation processor right now only runs on top of tensorflow-core-api... I think we need to discuss more about this.", "author": "karllessard", "createdAt": "2020-08-13T03:02:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE1NDI1NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r470154255", "bodyText": "Yes, I just updated the api_def_SoftmaxCrossEntropyWithLogits.pbtxt and api_def_SparseSoftmaxCrossEntropyWithLogits.pbtxt. Just let me know the agreed upon name of the new subpackage.\nop {\n  graph_op_name: \"SoftmaxCrossEntropyWithLogits\"\n  endpoint {\n    name: \"nn.internal.SoftmaxCrossEntropyWithLogits\"\n  }\n}", "author": "JimClarke5", "createdAt": "2020-08-13T18:16:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDM0ODEwNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r470348105", "bodyText": "I suggest we name it raw instead of internal, which reflects the same package name in Python.\nStill, I think we need to discuss a bit more about it during our next community call.", "author": "karllessard", "createdAt": "2020-08-14T00:57:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzQxODUxMw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473418513", "bodyText": "I just found another instance of a high level Python op on top of the raw op with the same name.\nPython tensorflow, tf.linalg.matmul()  in math_ops.py, has python logic on top of the low level matmul Op in gen_math_ops.py.\nAll I know for now, is if I run python numpy.tensordot() or python tf.tensordot() on one set of inputs, I do not get the same results as when calling my java version of  tensordot() that merely ends up calling the current raw op tf.lingalg.matMul() in Java. I have isolated the difference to the missing higher level logic of python tf.linalg.matmull()in math_ops.py.\nMy purpose here is to bring attention to the pattern of higher level ops on top of the raw ops, so the issue I am seeing with nn is not a one off.", "author": "JimClarke5", "createdAt": "2020-08-19T23:10:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzkzMDY2Mg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473930662", "bodyText": "Ok changed to raw", "author": "JimClarke5", "createdAt": "2020-08-20T12:26:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE1NjkwOA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474156908", "bodyText": "I changed the package name to raw, and will be checking in the new NNops.java.", "author": "JimClarke5", "createdAt": "2020-08-20T17:32:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDAyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDgxMg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463620812", "bodyText": "What's this for? If it's used across threads should it be a ConcurrentHashMap?", "author": "Craigacp", "createdAt": "2020-07-31T13:47:07Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand minusOne = tf.constant(-1);\n+    Operand precise_logits = logits;\n+    Operand one = tf.constant(1L);\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n+    }\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = tf.dtypes.cast(labels, dtype);\n+    Operand inputRank = tf.dtypes.cast(tf.rank(precise_logits), TInt64.DTYPE);\n+    Operand inputRankMinusOne = tf.dtypes.cast(tf.math.sub(inputRank, one), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(tf, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(tf, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(tf, precise_logits);\n+    labels = flattenOuterDims(tf, labels);\n+    SoftmaxCrossEntropyWithLogits smax =\n+        tf.nn.softmaxCrossEntropyWithLogits(precise_logits, labels);\n+    Operand cost = smax.loss();\n+    Operand outputShape =\n+        tf.slice(\n+            tf.constant(inputShape.asArray()),\n+            tf.constant(new long[] {0}),\n+            tf.constant(new long[] {inputShape.numDimensions() - 1}));\n+    cost = tf.reshape(cost, outputShape);\n+    if (tf.scope().env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = tf.reshape(cost, tf.constant(newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = tf.dtypes.cast(cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  public static <T extends TType> Operand<T> map(\n+      Operand<T> input, Function<Operand<T>, Operand<T>> mapFunc) {\n+    return null;\n+  }\n+\n+  public static long[] concatenate(long first, long... remaining) {\n+    long[] dims = new long[remaining.length + 1];\n+    System.arraycopy(remaining, 0, dims, 1, remaining.length);\n+    dims[0] = first;\n+    return dims;\n+  }\n+\n+  private static Map<ExecutionEnvironment, Map<String, Integer>> uidMap = new HashMap<>();", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2NDc4MQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463964781", "bodyText": "Usually, if you pass a \"null\" for a name, it defaults to Class.getSImpleName(), but there may be circumstances where two objects of the same class would be needed (e.g. layers) and the name needs to be unique.\nThis tracks a UID for assigning uniq names to objects.. For example, if I create two objects of the same type with name \"dense\", then the first one would be \"dense\"( or \"dense_0\"), followed by \"dense_1\", \"dense_2\" etc.  I have not used this yet in the Optimizers. Should we?, If there are multiple layers, then an Optimizer would presumably be created for each layer, should these have unique names?.\nIf we are to keep this, then maybe it should be a WeakHashMap.", "author": "JimClarke5", "createdAt": "2020-08-01T13:59:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDgxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4OTY0OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463989649", "bodyText": "I think the optimizers apply to the whole model, and do the name munging themselves. However having something to track the names seems reasonable. I agree we don't want it to keep an execution environment around if there are no other references, but we might need something that actively cleans this uidMap. Can that be controlled from elsewhere in Keras?", "author": "Craigacp", "createdAt": "2020-08-01T18:54:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDgxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4ODcwMw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r464388703", "bodyText": "You are right on the Optimizers, they are at the model level. However, metrics, activation, initializers,regularizers, and constraints, may be used with layers, sometimes more than once per layer.", "author": "JimClarke5", "createdAt": "2020-08-03T12:44:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDgxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4OTU0Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r464389546", "bodyText": "Keras could set Model to AutoCloseable and do any remaining clean up then.", "author": "JimClarke5", "createdAt": "2020-08-03T12:45:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDgxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY3MTA5OA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469671098", "bodyText": "Also agree that we don't want to keep a strong reference to the ExecutionEnvironment here", "author": "karllessard", "createdAt": "2020-08-13T03:11:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDgxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE1OTQ1Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474159453", "bodyText": "K.java has been removed from this PR. I will add it back in to another PR when the next feature (metrics, loss, etc.) needs it.", "author": "JimClarke5", "createdAt": "2020-08-20T17:36:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMDgxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMTEwOQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463621109", "bodyText": "This and the one below should probably live on DataType.", "author": "Craigacp", "createdAt": "2020-07-31T13:47:39Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand minusOne = tf.constant(-1);\n+    Operand precise_logits = logits;\n+    Operand one = tf.constant(1L);\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n+    }\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = tf.dtypes.cast(labels, dtype);\n+    Operand inputRank = tf.dtypes.cast(tf.rank(precise_logits), TInt64.DTYPE);\n+    Operand inputRankMinusOne = tf.dtypes.cast(tf.math.sub(inputRank, one), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(tf, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(tf, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(tf, precise_logits);\n+    labels = flattenOuterDims(tf, labels);\n+    SoftmaxCrossEntropyWithLogits smax =\n+        tf.nn.softmaxCrossEntropyWithLogits(precise_logits, labels);\n+    Operand cost = smax.loss();\n+    Operand outputShape =\n+        tf.slice(\n+            tf.constant(inputShape.asArray()),\n+            tf.constant(new long[] {0}),\n+            tf.constant(new long[] {inputShape.numDimensions() - 1}));\n+    cost = tf.reshape(cost, outputShape);\n+    if (tf.scope().env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = tf.reshape(cost, tf.constant(newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = tf.dtypes.cast(cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  public static <T extends TType> Operand<T> map(\n+      Operand<T> input, Function<Operand<T>, Operand<T>> mapFunc) {\n+    return null;\n+  }\n+\n+  public static long[] concatenate(long first, long... remaining) {\n+    long[] dims = new long[remaining.length + 1];\n+    System.arraycopy(remaining, 0, dims, 1, remaining.length);\n+    dims[0] = first;\n+    return dims;\n+  }\n+\n+  private static Map<ExecutionEnvironment, Map<String, Integer>> uidMap = new HashMap<>();\n+\n+  /**\n+   * Associates a string prefix with an integer counter in a TensorFlow graph.\n+   *\n+   * <p>Example:\n+   *\n+   * <pre>\n+   * get_uid('dense')\n+   * 1\n+   * get_uid('dense')\n+   * 2\n+   * </pre>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param prefix String prefix to index.\n+   * @return Unique integer ID.\n+   */\n+  public static int getUid(Ops tf, String prefix) {\n+    ExecutionEnvironment env = tf.scope().env();\n+    Map<String, Integer> uids = uidMap.get(env);\n+    if (uids == null) {\n+      uids = new HashMap<>();\n+      uidMap.put(env, uids);\n+    }\n+    Integer id = uids.get(prefix);\n+    if (id == null) {\n+      id = 0;\n+    } else {\n+      id++;\n+    }\n+\n+    uids.put(prefix, id);\n+    return id;\n+  }\n+\n+  /**\n+   * returns the larger DataType between the two.\n+   *\n+   * @param a the first DataType to compare\n+   * @param b the second DataType to compare\n+   * @return the wider DataType\n+   */\n+  public DataType wider(DataType a, DataType b) {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMjM0NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463622345", "bodyText": "Given the output washes the type off anyway, can we just have a single long if statement that checks it's a valid type, and then returns the NdArraySequence? Also do we want it to return Optional<NdArraySequence>?", "author": "Craigacp", "createdAt": "2020-07-31T13:49:39Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand minusOne = tf.constant(-1);\n+    Operand precise_logits = logits;\n+    Operand one = tf.constant(1L);\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n+    }\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = tf.dtypes.cast(labels, dtype);\n+    Operand inputRank = tf.dtypes.cast(tf.rank(precise_logits), TInt64.DTYPE);\n+    Operand inputRankMinusOne = tf.dtypes.cast(tf.math.sub(inputRank, one), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(tf, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(tf, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(tf, precise_logits);\n+    labels = flattenOuterDims(tf, labels);\n+    SoftmaxCrossEntropyWithLogits smax =\n+        tf.nn.softmaxCrossEntropyWithLogits(precise_logits, labels);\n+    Operand cost = smax.loss();\n+    Operand outputShape =\n+        tf.slice(\n+            tf.constant(inputShape.asArray()),\n+            tf.constant(new long[] {0}),\n+            tf.constant(new long[] {inputShape.numDimensions() - 1}));\n+    cost = tf.reshape(cost, outputShape);\n+    if (tf.scope().env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = tf.reshape(cost, tf.constant(newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = tf.dtypes.cast(cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  public static <T extends TType> Operand<T> map(\n+      Operand<T> input, Function<Operand<T>, Operand<T>> mapFunc) {\n+    return null;\n+  }\n+\n+  public static long[] concatenate(long first, long... remaining) {\n+    long[] dims = new long[remaining.length + 1];\n+    System.arraycopy(remaining, 0, dims, 1, remaining.length);\n+    dims[0] = first;\n+    return dims;\n+  }\n+\n+  private static Map<ExecutionEnvironment, Map<String, Integer>> uidMap = new HashMap<>();\n+\n+  /**\n+   * Associates a string prefix with an integer counter in a TensorFlow graph.\n+   *\n+   * <p>Example:\n+   *\n+   * <pre>\n+   * get_uid('dense')\n+   * 1\n+   * get_uid('dense')\n+   * 2\n+   * </pre>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param prefix String prefix to index.\n+   * @return Unique integer ID.\n+   */\n+  public static int getUid(Ops tf, String prefix) {\n+    ExecutionEnvironment env = tf.scope().env();\n+    Map<String, Integer> uids = uidMap.get(env);\n+    if (uids == null) {\n+      uids = new HashMap<>();\n+      uidMap.put(env, uids);\n+    }\n+    Integer id = uids.get(prefix);\n+    if (id == null) {\n+      id = 0;\n+    } else {\n+      id++;\n+    }\n+\n+    uids.put(prefix, id);\n+    return id;\n+  }\n+\n+  /**\n+   * returns the larger DataType between the two.\n+   *\n+   * @param a the first DataType to compare\n+   * @param b the second DataType to compare\n+   * @return the wider DataType\n+   */\n+  public DataType wider(DataType a, DataType b) {\n+    return a.byteSize() < b.byteSize() ? b : a;\n+  }\n+\n+  /**\n+   * returns the smaller DataType between the two.\n+   *\n+   * @param a the first DataType to compare\n+   * @param b the second DataType to compare\n+   * @return the smaller DataType\n+   */\n+  public DataType narrower(DataType a, DataType b) {\n+    return a.byteSize() > b.byteSize() ? b : a;\n+  }\n+\n+  public <T extends TNumber> NdArraySequence getTensorValue(Ops tf, Operand<T> operand) {\n+    DataType dtype = operand.asOutput().dataType();\n+    if (tf.scope().env().isGraph()) {\n+      try (Session session = new Session((Graph) tf.scope().env())) {\n+        if (dtype.equals(TInt32.DTYPE)) {\n+          try (Tensor<TInt32> result =", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY3MjY2Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469672663", "bodyText": "To make this usable, NdArraySequence must be typed. If we decide to go ahead with this PR, it could make things easier but I'll need to double-check before confirming this. Otherwise, we will need one method per type.", "author": "karllessard", "createdAt": "2020-08-13T03:18:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyMjM0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNjM3OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463626379", "bodyText": "No star imports please.", "author": "Craigacp", "createdAt": "2020-07-31T13:56:24Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/ConfusionMatrix.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.keras.backend.*;", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDYyNTYyNg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r470625626", "bodyText": "OK on the import star.\nIn reviewing  ConfusionMatrix, these methods really belong in a new MathOps class. tf.math.confusion_matrix in python. I have found a few other methods, like tf.tensordot() that also belong in a MathOps class. We can discuss along with the tf.nn issues.", "author": "JimClarke5", "createdAt": "2020-08-14T13:32:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNjM3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM4NjUzNw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473386537", "bodyText": "If this not belong to Keras in Python, then we should move either to tensorflow-framework or tensorflow-core-api. The way to decide if it is the one or the other is if we consider the operation as being something very close to a what could have been a core operation (e.g. sparseSoftmaxCrossEntropyWithLogits) or if it defines a higher-level concept. If that can help you to decide where this one (and the others you are adding) could fit....", "author": "karllessard", "createdAt": "2020-08-19T22:26:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNjM3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNjgyMA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463626820", "bodyText": "Too many single quotes here.", "author": "Craigacp", "createdAt": "2020-07-31T13:57:07Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/ConfusionMatrix.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.keras.backend.*;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.keras.utils.ShapeUtils;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Stack;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** ConfusionMatrix operations */\n+public class ConfusionMatrix {\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static Tuple removeSqueezableDimensions(Ops tf, Operand labels, Operand predictions) {\n+    return removeSqueezableDimensions(tf, labels, predictions, 0);\n+  }\n+\n+  /**\n+   * Squeeze last dim if ranks differ from expected by exactly 1.\n+   *\n+   * @param tf the TensorFlowOps\n+   * @param labels Label values, a `Tensor` whose dimensions match `predictions`.\n+   * @param predictions Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param expectedRankDiff Expected result of `rank(predictions) - rank(labels)`.\n+   * @return `labels` and `predictions`, possibly with last dim squeezed.\n+   */\n+  public static Tuple removeSqueezableDimensions(\n+      Ops tf, Operand labels, Operand predictions, int expectedRankDiff) {\n+\n+    tf = tf.withSubScope(\"removeSqueezableDimensions\");\n+    Shape predictionsShape = predictions.asOutput().shape();\n+    int predictionsRank = predictionsShape.numDimensions();\n+    Shape labelsShape = labels.asOutput().shape();\n+    int labelsRank = labelsShape.numDimensions();\n+\n+    if (predictionsRank != Shape.UNKNOWN_SIZE && labelsRank != Shape.UNKNOWN_SIZE) {\n+      // Use static rank.\n+      int rankDiff = predictionsRank - labelsRank;\n+      if (rankDiff == expectedRankDiff + 1\n+          && ShapeUtils.isCompatible(predictionsShape.size(-1), 1)) {\n+        predictions = tf.squeeze(predictions);\n+      } else if (rankDiff == expectedRankDiff - 1\n+          && ShapeUtils.isCompatible(labelsShape.size(-1), 1)) {\n+        labels = tf.squeeze(labels);\n+      }\n+      return new Tuple(labels, predictions);\n+    }\n+    // Use dynamic rank.\n+    Operand rankDiff = tf.math.sub(tf.rank(predictions), tf.rank(labels));\n+    if (predictionsRank == Shape.UNKNOWN_SIZE\n+        && ShapeUtils.isCompatible(predictionsShape.size(-1), 1)) {\n+      /**\n+       * TODO, if we ever get a select that does lazy evaluation, but for now do the tf.squeeze\n+       * predictions = tf.select( tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ),\n+       * tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      predictions = tf.squeeze(predictions, Squeeze.axis(Arrays.asList(-1L)));\n+    }\n+    if (labelsRank == Shape.UNKNOWN_SIZE && ShapeUtils.isCompatible(labelsShape.size(-1), 1)) {\n+      /**\n+       * TODO, if we ever get a select that does lazy evaluation labels = tf.select(\n+       * tf.math.equal(tf.constant(expectedRankDiff+1),rankDiff ), tf.squeeze(labels,\n+       * Squeeze.axis(Arrays.asList(-1L))), predictions ); *\n+       */\n+      labels = tf.squeeze(labels, Squeeze.axis(Arrays.asList(-1L)));\n+    }\n+    return new Tuple(labels, predictions);\n+  }\n+\n+  /**\n+   * Computes the confusion matrix from predictions and labels.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param labels 1-D `Tensor` of real labels for the classification task.\n+   * @param predictions 1-D `Tensor` of predictions for a given classification.\n+   * @param numClasses The possible number of labels the classification task can have.\n+   * @param weights optional weights to be applied to the confusion matris\n+   * @param dtype Data type of the confusion matrix.\n+   * @param <T> the type of Operands\n+   * @param <U> the data type.\n+   * @return A `Tensor` of type `dtype` with shape `[n, n]` representing the confusion matrix, where\n+   *     `n` is the number of possible labels in the classification task.\n+   * @throws IllegalArgumentException If both predictions and labels are not 1-D vectors and have\n+   *     mismatched shapes, or if `weights` is not `None` and its shape doesn't match `predictions`.\n+   */\n+  public static <T extends TType, U extends TNumber> Operand confusionMatrix(\n+      Ops tf,\n+      Operand<T> labels,\n+      Operand<T> predictions,\n+      Operand<TInt64> numClasses,\n+      Operand<T> weights,\n+      DataType<U> dtype) {\n+    tf = tf.withSubScope(\"confusion_matrix\");\n+    Tuple ops = K.squeezeOrExpandDimensions(tf, predictions, labels, null);\n+    predictions = tf.dtypes.cast(ops.getPredictions(), TInt64.DTYPE);\n+    labels = tf.dtypes.cast(ops.getLabels(), TInt64.DTYPE);\n+\n+    List<Op> labelControls = new ArrayList<>();\n+    List<Op> predictionControls = new ArrayList<>();\n+\n+    labelControls.add(\n+        tf.assertThat(\n+            tf.reduceAny(\n+                tf.math.greaterEqual((Operand<TInt64>) labels, tf.constant(0L)),\n+                K.allAxis(tf, labels)),\n+            Arrays.asList(tf.constant(\"`labels` contains negative values\"))));\n+\n+    predictionControls.add(\n+        tf.assertThat(\n+            tf.reduceAny(\n+                tf.math.greaterEqual((Operand<TInt64>) predictions, tf.constant(0L)),\n+                K.allAxis(tf, labels)),\n+            Arrays.asList(tf.constant(\"`predictions` contains negative values\"))));\n+    if (numClasses == null) {\n+      numClasses =\n+          tf.math.maximum(\n+              tf.reduceMax(predictions, K.allAxis(tf, predictions)),\n+              tf.reduceMax(labels, K.allAxis(tf, labels)));\n+    } else {\n+      labelControls.add(\n+          tf.assertThat(\n+              tf.reduceAny(\n+                  tf.math.less((Operand<TInt64>) labels, numClasses), K.allAxis(tf, labels)),\n+              Arrays.asList(tf.constant(\"``labels` out of bound\"))));", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNzAxOQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463627019", "bodyText": "No star imports.", "author": "Craigacp", "createdAt": "2020-07-31T13:57:26Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/ControlDependencies.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.keras.backend.*;", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNzc5Mg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463627792", "bodyText": "Where is createOperand from? Is it a class or a static?", "author": "Craigacp", "createdAt": "2020-07-31T13:58:44Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/ControlDependencies.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.keras.backend.*;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.function.Function;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TType;\n+\n+/** Container for ControlDepencies, so that the primary Operand is remembered. */\n+public class ControlDependencies {\n+\n+  /**\n+   * Create a control dependency for the operand;\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param createOperand a function that creates an operand with the control dependency context\n+   * @param name the scope name to use\n+   * @param dependencies a list of control ops.\n+   * @param <T> the type of Operand\n+   * @return the Operand with control dependency scope\n+   */\n+  public static <T extends TType> Operand<T> addControlDependencies(\n+      Ops tf, Function<Ops, Operand<T>> createOperand, String name, Op... dependencies) {\n+    return addControlDependencies(tf, createOperand, name, Arrays.asList(dependencies));\n+  }\n+\n+  /**\n+   * Create a control dependency for an operand.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param createOperand function that creates an operand with the control dependency context\n+   * @param name the scope name to use\n+   * @param dependencies a list of control ops.\n+   * @param <T> the type of Operand\n+   * @return the Operand with control dependency scope\n+   */\n+  public static <T extends TType> Operand<T> addControlDependencies(\n+      Ops tf, Function<Ops, Operand<T>> createOperand, String name, List<Op> dependencies) {\n+    tf = tf.withSubScope(name).withControlDependencies(dependencies);\n+    return createOperand.apply(tf);", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyODM2NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463628365", "bodyText": "Should we override the one without gradients with this one? It should be straightforward to turn the other off, and to add this as an op in the right place. Needs more types first though.", "author": "Craigacp", "createdAt": "2020-07-31T13:59:46Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/NN.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.nn.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+\n+/** NN Operations */\n+public class NN {\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param tf\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  public static Operand sparse_softmax_cross_entropy_with_logits(", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5NzI4OA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473397288", "bodyText": "We should add sparseSoftmaxCrossEntropyWithLogits as an operator accessible via the Ops API, probably under tf.sparse.*", "author": "karllessard", "createdAt": "2020-08-19T22:41:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyODM2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDExMTI5Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474111296", "bodyText": "I don't think it wants to go in tf.sparse as it's not an operation on a sparse tensor. In TF Python its in tf.nn - https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits", "author": "Craigacp", "createdAt": "2020-08-20T16:24:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyODM2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyODk1MA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463628950", "bodyText": "Do we know if upstream plans to change SparseTensor into something the C API knows about? cc @karllessard.", "author": "Craigacp", "createdAt": "2020-07-31T14:00:51Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/SparseTensor.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Represents a sparse tensor.\n+ *\n+ * @param <T> the type of the SparseTensor\n+ */\n+public class SparseTensor<T extends TType> {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5OTk4OA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473399988", "bodyText": "I really think SparseTensor should be a core concept of the library. I still don't know where and how it will fit exactly. Do you need this one to be checked-in immediatly @JimClarke5 ?", "author": "karllessard", "createdAt": "2020-08-19T22:44:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyODk1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYzMTEyNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463631125", "bodyText": "I think I'd prefer this to be called OperandTuple as it's not sufficiently general to use the name Tuple, especially when that might well collide with user code.\nAlso should it have more type bounds? The sample weights are probably floats, and the labels could be floats or ints. Ditto the lossesOrPredictions.\nOr we could make a more general Pair and a Triple. I nearly added a Pair when we added optimizer support, and there are places that could benefit.\nOne further alternative is just making explicit classes for the use cases we have, as those could be converted into records when we eventually bump Java versions.", "author": "Craigacp", "createdAt": "2020-07-31T14:04:54Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/Tuple.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Returns labels, losses or predictions and sample weights as a Tuple\n+ *\n+ * @param <T> the type of Operand\n+ */\n+public class Tuple<T extends TType> {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYzMjUxMA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463632510", "bodyText": "I don't like two methods returning the same field here. These should be combined (or we should completely refactor this class if we want the names to be meaningful).", "author": "Craigacp", "createdAt": "2020-07-31T14:07:17Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/Tuple.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.Operand;\n+import org.tensorflow.types.family.TType;\n+\n+/**\n+ * Returns labels, losses or predictions and sample weights as a Tuple\n+ *\n+ * @param <T> the type of Operand\n+ */\n+public class Tuple<T extends TType> {\n+\n+  private final Operand<T> labels;\n+  private final Operand<T> lossesOrPredictions;\n+  private final Operand<T> sampleWeights;\n+\n+  /**\n+   * Creates a Tuple of Operands for labels, predictions, and sampleWeights\n+   *\n+   * @param labels the labels\n+   * @param lossesOrPredictions the losses or predictions\n+   */\n+  public Tuple(Operand<T> labels, Operand<T> lossesOrPredictions) {\n+    this(labels, lossesOrPredictions, null);\n+  }\n+\n+  /**\n+   * Creates a Tuple of Operands for labels, predictions, and sampleWeights\n+   *\n+   * @param labels the labels\n+   * @param lossesOrPredictions the losses or predictions\n+   * @param sampleWeights the sample weights\n+   */\n+  public Tuple(Operand<T> labels, Operand<T> lossesOrPredictions, Operand<T> sampleWeights) {\n+    this.labels = labels;\n+    this.lossesOrPredictions = lossesOrPredictions;\n+    this.sampleWeights = sampleWeights;\n+  }\n+\n+  /**\n+   * Indicates whether this Tuple contains Labels\n+   *\n+   * @return true is this Tuple contains Labels\n+   */\n+  public boolean containsLabels() {\n+    return this.labels != null;\n+  }\n+\n+  /**\n+   * Indicates whether this Tuple contains Labels\n+   *\n+   * @return true is this Tuple contains Labels\n+   */\n+  public boolean containsPredictions() {\n+    return this.lossesOrPredictions != null;\n+  }\n+\n+  /**\n+   * Indicates whether this Tuple contains Labels\n+   *\n+   * @return true is this Tuple contains Labels\n+   */\n+  public boolean containsLosses() {\n+    return this.lossesOrPredictions != null;\n+  }\n+\n+  /**\n+   * Indicates whether this Tuple contains Labels\n+   *\n+   * @return true is this Tuple contains Labels\n+   */\n+  public boolean containsSampleWeights() {\n+    return this.sampleWeights != null;\n+  }\n+\n+  /** @return the labels */\n+  public Operand<T> getLabels() {\n+    return labels;\n+  }\n+\n+  /** @return the predictions */\n+  public Operand<T> getPredictions() {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY0ODM5OA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463648398", "bodyText": "Typo in commment.", "author": "Craigacp", "createdAt": "2020-07-31T14:35:39Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/AdaGradDA.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/** Optimizer that implements the Adagrad Dual-Averaging algorithm. */\n+public class AdaGradDA extends org.tensorflow.framework.optimizers.AdaGradDA\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String INITIAL_ACCUM_KEY = \"accumulator\";\n+  public static final String L1STRENGTH_KEY = \"l1Strength\";\n+  public static final String L2STRENGTH_KEY = \"l2Strength\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F; // arbitray number", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY0OTc2NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463649765", "bodyText": "Asserts are usually disabled right? Maybe these should be ifs that throw?", "author": "Craigacp", "createdAt": "2020-07-31T14:37:58Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/AdaGradDA.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/** Optimizer that implements the Adagrad Dual-Averaging algorithm. */\n+public class AdaGradDA extends org.tensorflow.framework.optimizers.AdaGradDA\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String INITIAL_ACCUM_KEY = \"accumulator\";\n+  public static final String L1STRENGTH_KEY = \"l1Strength\";\n+  public static final String L2STRENGTH_KEY = \"l2Strength\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F; // arbitray number\n+  public static final float INITIAL_ACCUM__DEFAULT = 0.1f;\n+  public static final float L1STRENGTH_DEFAULT = 0.0F;\n+  public static final float L2STRENGTH_DEFAULT = 0.0F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values name=\"adagrad-da\". learning_rate=.001,\n+   * initial accumulator= 0.1, l1Strength=0.0, l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   */\n+  public AdaGradDA(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values initial accumulator= 0.1, l1Strength=0.0,\n+   * l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   * @param learningRate The learning rate.\n+   */\n+  public AdaGradDA(Ops tf, float learningRate) {\n+    this(tf, learningRate, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values initial accumulator= 0.1, l1Strength=0.0,\n+   * l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   * @param name the name of the Optimizer, defaults to \"adagrad-da\"\n+   * @param learningRate The learning rate.\n+   */\n+  public AdaGradDA(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer\n+   *\n+   * @param tf the tensorflow tf\n+   * @param learningRate the learning rate, default is 0.001\n+   * @param initialAccumulatorValue Starting value for the accumulators, must be >= 0.0.\n+   * @param l1Strength L1 Regularization Strength\n+   * @param l2Strength L2 Regularization Strength\n+   */\n+  public AdaGradDA(\n+      Ops tf,\n+      float learningRate,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength) {\n+    super(assertGraph(tf), learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+    assert initialAccumulatorValue >= 0.0F", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1MDQ5NA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463650494", "bodyText": "s/AdaGrad/AdaGradDA/", "author": "Craigacp", "createdAt": "2020-07-31T14:39:13Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/AdaGradDA.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/** Optimizer that implements the Adagrad Dual-Averaging algorithm. */\n+public class AdaGradDA extends org.tensorflow.framework.optimizers.AdaGradDA\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String INITIAL_ACCUM_KEY = \"accumulator\";\n+  public static final String L1STRENGTH_KEY = \"l1Strength\";\n+  public static final String L2STRENGTH_KEY = \"l2Strength\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F; // arbitray number\n+  public static final float INITIAL_ACCUM__DEFAULT = 0.1f;\n+  public static final float L1STRENGTH_DEFAULT = 0.0F;\n+  public static final float L2STRENGTH_DEFAULT = 0.0F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values name=\"adagrad-da\". learning_rate=.001,\n+   * initial accumulator= 0.1, l1Strength=0.0, l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   */\n+  public AdaGradDA(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values initial accumulator= 0.1, l1Strength=0.0,\n+   * l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   * @param learningRate The learning rate.\n+   */\n+  public AdaGradDA(Ops tf, float learningRate) {\n+    this(tf, learningRate, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values initial accumulator= 0.1, l1Strength=0.0,\n+   * l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   * @param name the name of the Optimizer, defaults to \"adagrad-da\"\n+   * @param learningRate The learning rate.\n+   */\n+  public AdaGradDA(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer\n+   *\n+   * @param tf the tensorflow tf\n+   * @param learningRate the learning rate, default is 0.001\n+   * @param initialAccumulatorValue Starting value for the accumulators, must be >= 0.0.\n+   * @param l1Strength L1 Regularization Strength\n+   * @param l2Strength L2 Regularization Strength\n+   */\n+  public AdaGradDA(\n+      Ops tf,\n+      float learningRate,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength) {\n+    super(assertGraph(tf), learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+    assert initialAccumulatorValue >= 0.0F\n+        : \"initial_accumulator_value must be non-negative: \" + initialAccumulatorValue;\n+    assert l1Strength >= 0.0F : \"l1Strength must be non-negative: \" + l1Strength;\n+    assert l2Strength >= 0.0F : \"l2Strength must be non-negative: \" + l2Strength;\n+    initConfig(learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer\n+   *\n+   * @param tf the tensorflow tf\n+   * @param name the name of the Optimizer, defaults to \"adagrad-da\"\n+   * @param learningRate the learning rate, default is 0.001\n+   * @param initialAccumulatorValue Starting value for the accumulators, must be positive.\n+   * @param l1Strength L1 Regularization Strength\n+   * @param l2Strength L2 Regularization Strength\n+   */\n+  public AdaGradDA(\n+      Ops tf,\n+      String name,\n+      float learningRate,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength) {\n+    super(assertGraph(tf), name, learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+    assert initialAccumulatorValue >= 0.0F\n+        : \"initial_accumulator_value must be non-negative: \" + initialAccumulatorValue;\n+    assert l1Strength >= 0.0F : \"l1Strength must be non-negative: \" + l1Strength;\n+    assert l2Strength >= 0.0F : \"l2Strength must be non-negative: \" + l2Strength;\n+    initConfig(learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+  }\n+\n+  /**\n+   * Create an AdaGrad Optimizer from a config object", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1MTg0OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463651849", "bodyText": "Should we make base optimizers for these that the Keras ones extend? I think some of these optimizers might be new, and so I should incorporate them into the tf optimizers.", "author": "Craigacp", "createdAt": "2020-07-31T14:41:27Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/Adamax.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.train.ApplyAdaMax;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+/** Adamax Optimizer that implements the Adamax algorithm. */\n+public class Adamax extends org.tensorflow.framework.optimizers.Optimizer\n+    implements OptimizerInterface {\n+\n+  public static final String FIRST_MOMENT = \"m\";\n+  public static final String SECOND_MOMENT = \"v\";\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+  public static final String BETA_ONE_KEY = \"beta_1\";\n+  public static final String BETA_TWO_KEY = \"beta_2\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float EPSILON_DEFAULT = 1e-07F;\n+  public static final float BETA_ONE_DEFAULT = 0.9F;\n+  public static final float BETA_TWO_DEFAULT = 0.999F;\n+\n+  private Scope scope;\n+  private Map<String, Object> config = new HashMap<>();\n+\n+  private float learningRate;\n+  private final float betaOne;\n+  private final float betaTwo;\n+  private final float epsilon;\n+\n+  private Constant<TFloat32> learningRateConst;\n+  private Constant<TFloat32> epsilonConst;\n+  private Constant<TFloat32> betaOneConst;\n+  private Constant<TFloat32> betaTwoConst;\n+  private Variable<TFloat32> betaOnePower;\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   */\n+  public Adamax(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param name name for the operations Created when applying gradients. Defaults to \"Adamax\".\n+   */\n+  public Adamax(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param learningRate The learning rate.\n+   */\n+  public Adamax(Ops tf, float learningRate) {\n+    this(tf, learningRate, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param name name for the operations Created when applying gradients. Defaults to \"Adamax\".\n+   * @param learningRate The learning rate.\n+   */\n+  public Adamax(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param learningRate The learning rate.\n+   * @param betaOne The exponential decay rate for the 1st moment estimates.\n+   * @param betaTwo The exponential decay rate for the exponentially weighted infinity norm.\n+   * @param epsilon A small constant for numerical stability.\n+   */\n+  public Adamax(Ops tf, float learningRate, float betaOne, float betaTwo, float epsilon) {\n+    super(assertGraph(tf));\n+    this.learningRate = learningRate;\n+    this.betaOne = betaOne;\n+    this.betaTwo = betaTwo;\n+    this.epsilon = epsilon;\n+    this.scope = tf.scope();\n+    initConfig(learningRate, betaOne, betaTwo, epsilon);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param name name for the operations Created when applying gradients. Defaults to \"Adamax\".\n+   * @param learningRate The learning rate.\n+   * @param betaOne The exponential decay rate for the 1st moment estimates.\n+   * @param betaTwo The exponential decay rate for the exponentially weighted infinity norm.\n+   * @param epsilon A small constant for numerical stability.\n+   */\n+  public Adamax(\n+      Ops tf, String name, float learningRate, float betaOne, float betaTwo, float epsilon) {\n+    super(assertGraph(tf), name);\n+    this.learningRate = learningRate;\n+    this.betaOne = betaOne;\n+    this.betaTwo = betaTwo;\n+    this.epsilon = epsilon;\n+    this.scope = tf.scope();\n+\n+    initConfig(learningRate, betaOne, betaTwo, epsilon);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm from a config object\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param config a config object to initialize, the config object has keys for \"name\",\n+   *     \"learning_rate\", \"epsilon\", \"beta_1\", \"beta_2\". If a key is missing the default value is\n+   *     used.\n+   */\n+  public static Adamax fromConfig(Ops tf, Map<String, Object> config) {\n+    return create(tf, config);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm from a config object\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param config a config object to initialize\n+   */\n+  public static Adamax create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float epsilon = (float) config.getOrDefault(EPSILON_KEY, EPSILON_DEFAULT);\n+    float betaOne = (float) config.getOrDefault(BETA_ONE_KEY, BETA_ONE_DEFAULT);\n+    float betaTwo = (float) config.getOrDefault(BETA_TWO_KEY, BETA_TWO_DEFAULT);\n+    if (name == null) {\n+      return new Adamax(tf, learningRate, betaOne, betaTwo, epsilon);\n+    } else {\n+      return new Adamax(tf, name, learningRate, betaOne, betaTwo, epsilon);\n+    }\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Map<String, Object> getConfig() {\n+    return config;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public float getLearningRate() {\n+    return this.learningRate;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public void setLearningRate(float learningRate) {\n+    this.learningRate = learningRate;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String scopeName) {\n+    betaOneConst = tf.constant(betaOne);\n+    betaTwoConst = tf.constant(betaTwo);\n+    learningRateConst = tf.constant(learningRate);\n+    epsilonConst = tf.constant(epsilon);\n+\n+    return Optional.empty();\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected void createSlots(List<Output<? extends TType>> variables) {\n+    for (Output<? extends TType> v : variables) {\n+      createAdamaxSlot(v.asOutput());\n+    }\n+    betaOnePower = tf.withName(\"beta1_power\").variable(Shape.scalar(), TFloat32.DTYPE);\n+    Assign<TFloat32> betaOnePowerInit = tf.assign(betaOnePower, tf.constant(betaOne));\n+    ((Graph) tf.scope().env()).addInitializer(betaOnePowerInit);\n+  }\n+\n+  /**\n+   * Create the first and second moment slots\n+   *\n+   * @param v the variable\n+   * @param <T> the datatype of the variable\n+   */\n+  private <T extends TType> void createAdamaxSlot(Output<T> v) {\n+    Operand<T> firstMomentInitializer =\n+        tf.fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f), v.dataType()));\n+    createSlot(v.asOutput(), FIRST_MOMENT, firstMomentInitializer);\n+    Operand<T> secondMomentInitializer =\n+        tf.fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f), v.dataType()));\n+    createSlot(v.asOutput(), SECOND_MOMENT, secondMomentInitializer);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected <T extends TType> Op applyDense(Output<T> gradient, Output<T> variable) {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1MjI0Nw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463652247", "bodyText": "Same comment as AdaMax.", "author": "Craigacp", "createdAt": "2020-07-31T14:42:05Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/Ftrl.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.Session;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.train.ApplyFtrl;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+/** Ftrl Optimizer that implements the FTRL algorithm. */\n+public class Ftrl extends org.tensorflow.framework.optimizers.Optimizer", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1MjUyNw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463652527", "bodyText": "Should we check it's not positive infinity?", "author": "Craigacp", "createdAt": "2020-07-31T14:42:35Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/Ftrl.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.Session;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Placeholder;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.train.ApplyFtrl;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+/** Ftrl Optimizer that implements the FTRL algorithm. */\n+public class Ftrl extends org.tensorflow.framework.optimizers.Optimizer\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String LEARNING_RATE_POWER_KEY = \"learning_rate_power\";\n+  public static final String INITIAL_ACCUM_VALUE_KEY = \"initial_accumulator_value\";\n+  public static final String L1STRENGTH_KEY = \"l1_regularization_strength\";\n+  public static final String L2STRENGTH_KEY = \"l2_regularization_strength\";\n+  public static final String L2_SHRINKAGE_REGULARIZATION_STRENGTH_KEY =\n+      \"l2_shrinkage_regularization_strength\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float LEARNING_RATE_POWER_DEFAULT = -0.5F;\n+  public static final float INITIAL_ACCUM_VALUE_DEFAULT = 0.1F;\n+  public static final float L1STRENGTH_DEFAULT = 0.0F;\n+  public static final float L2STRENGTH_DEFAULT = 0.0F;\n+  public static final float L2_SHRINKAGE_REGULARIZATION_STRENGTH_DEFAULT = 0.0F;\n+\n+  public static final String ACCUMULATOR = \"gradient_accumulator\";\n+  public static final String LINEAR_ACCUMULATOR = \"linear_accumulator\";\n+\n+  private final String name;\n+  private float learningRate;\n+  private final float learningRatePower;\n+  private final float initialAccumulatorValue;\n+  private final float l1RegularizationStrength;\n+  private final float l2RegularizationStrength;\n+  private final float l2ShrinkageRegularizationStrength;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+\n+  private boolean useLocking = true;\n+\n+  /**\n+   * Create a Ftrl Optimizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   */\n+  public Ftrl(Ops tf) {\n+    this(\n+        tf,\n+        LEARNING_RATE_DEFAULT,\n+        LEARNING_RATE_POWER_DEFAULT,\n+        INITIAL_ACCUM_VALUE_DEFAULT,\n+        L1STRENGTH_DEFAULT,\n+        L2STRENGTH_DEFAULT,\n+        L2_SHRINKAGE_REGULARIZATION_STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create a Ftrl Optimizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the Optmizer name\n+   */\n+  public Ftrl(Ops tf, String name) {\n+    this(\n+        tf,\n+        name,\n+        LEARNING_RATE_DEFAULT,\n+        LEARNING_RATE_POWER_DEFAULT,\n+        INITIAL_ACCUM_VALUE_DEFAULT,\n+        L1STRENGTH_DEFAULT,\n+        L2STRENGTH_DEFAULT,\n+        L2_SHRINKAGE_REGULARIZATION_STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create a Ftrl Optimizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param learningRate the learning rate\n+   */\n+  public Ftrl(Ops tf, float learningRate) {\n+    this(\n+        tf,\n+        learningRate,\n+        LEARNING_RATE_POWER_DEFAULT,\n+        INITIAL_ACCUM_VALUE_DEFAULT,\n+        L1STRENGTH_DEFAULT,\n+        L2STRENGTH_DEFAULT,\n+        L2_SHRINKAGE_REGULARIZATION_STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create a Ftrl Optimizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the Optmizer name\n+   * @param learningRate the learning rate\n+   */\n+  public Ftrl(Ops tf, String name, float learningRate) {\n+    this(\n+        tf,\n+        name,\n+        learningRate,\n+        LEARNING_RATE_POWER_DEFAULT,\n+        INITIAL_ACCUM_VALUE_DEFAULT,\n+        L1STRENGTH_DEFAULT,\n+        L2STRENGTH_DEFAULT,\n+        L2_SHRINKAGE_REGULARIZATION_STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create a Ftrl Optimizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param learningRate the learning rate\n+   * @param learningRatePower\n+   * @param initialAccumulatorValue\n+   * @param l1Strength\n+   * @param l2Strength\n+   * @param l2ShrinkageRegularizationStrength\n+   */\n+  public Ftrl(\n+      Ops tf,\n+      float learningRate,\n+      float learningRatePower,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength,\n+      float l2ShrinkageRegularizationStrength) {\n+    super(assertGraph(tf));\n+    this.name = getOptimizerName();\n+    this.learningRate = learningRate;\n+    this.learningRatePower = learningRatePower;\n+    this.initialAccumulatorValue = initialAccumulatorValue;\n+    this.l1RegularizationStrength = l1Strength;\n+    this.l2RegularizationStrength = l2Strength;\n+    this.l2ShrinkageRegularizationStrength = l2ShrinkageRegularizationStrength;\n+    validateParams();\n+    initConfig();\n+  }\n+\n+  /**\n+   * Create a Ftrl Optimizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param name the Optmizer name\n+   * @param learningRate the learning rate\n+   * @param learningRatePower\n+   * @param initialAccumulatorValue\n+   * @param l1Strength\n+   * @param l2Strength\n+   * @param l2ShrinkageRegularizationStrength\n+   */\n+  public Ftrl(\n+      Ops tf,\n+      String name,\n+      float learningRate,\n+      float learningRatePower,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength,\n+      float l2ShrinkageRegularizationStrength) {\n+    super(assertGraph(tf), name);\n+    this.name = name;\n+    this.learningRate = learningRate;\n+    this.learningRatePower = learningRatePower;\n+    this.initialAccumulatorValue = initialAccumulatorValue;\n+    this.l1RegularizationStrength = l1Strength;\n+    this.l2RegularizationStrength = l2Strength;\n+    this.l2ShrinkageRegularizationStrength = l2ShrinkageRegularizationStrength;\n+    validateParams();\n+    initConfig();\n+  }\n+\n+\n+\n+  /**\n+   * Create a Ftrl Optmizer\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param config a config object to initialize\n+   * @return a new Frtl Optimizer\n+   */\n+  public static Ftrl create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float learningRatePower =\n+        (float) config.getOrDefault(LEARNING_RATE_POWER_KEY, LEARNING_RATE_POWER_DEFAULT);\n+    float initialAccumulatorValue =\n+        (float) config.getOrDefault(INITIAL_ACCUM_VALUE_KEY, INITIAL_ACCUM_VALUE_DEFAULT);\n+    float l1RegularizationStrength =\n+        (float) config.getOrDefault(L1STRENGTH_KEY, L1STRENGTH_DEFAULT);\n+    float l2RegularizationStrength =\n+        (float) config.getOrDefault(L2STRENGTH_KEY, L2STRENGTH_DEFAULT);\n+    float l2ShrinkageRegularizationStrength =\n+        (float)\n+            config.getOrDefault(\n+                L2_SHRINKAGE_REGULARIZATION_STRENGTH_KEY,\n+                L2_SHRINKAGE_REGULARIZATION_STRENGTH_DEFAULT);\n+\n+    if (name == null) {\n+      return new Ftrl(\n+          tf,\n+          learningRate,\n+          learningRatePower,\n+          initialAccumulatorValue,\n+          l1RegularizationStrength,\n+          l2RegularizationStrength,\n+          l2ShrinkageRegularizationStrength);\n+    } else {\n+      return new Ftrl(\n+          tf,\n+          name,\n+          learningRate,\n+          learningRatePower,\n+          initialAccumulatorValue,\n+          l1RegularizationStrength,\n+          l2RegularizationStrength,\n+          l2ShrinkageRegularizationStrength);\n+    }\n+  }\n+\n+  /** Initialize the Config object from the current settings */\n+  protected void initConfig() {\n+    config.put(NAME_KEY, this.name);\n+    config.put(LEARNING_RATE_KEY, learningRate);\n+    config.put(LEARNING_RATE_POWER_KEY, learningRatePower);\n+    config.put(INITIAL_ACCUM_VALUE_KEY, initialAccumulatorValue);\n+    config.put(L1STRENGTH_KEY, l1RegularizationStrength);\n+    config.put(L2STRENGTH_KEY, l2RegularizationStrength);\n+    config.put(L2_SHRINKAGE_REGULARIZATION_STRENGTH_KEY, l2ShrinkageRegularizationStrength);\n+  }\n+\n+  /** Validate all the settings of the Frtl Optmizer */\n+  private void validateParams() {\n+    if (this.initialAccumulatorValue < 0.0F) {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1NDcxOQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463654719", "bodyText": "Should this be final and immutable after construction? initConfig could return a Collections.unmodifiableMap(). Alternatively should the learning rate be updated in here?", "author": "Craigacp", "createdAt": "2020-07-31T14:46:25Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/Adam.java", "diffHunk": "@@ -0,0 +1,182 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/** Adam Optimizer that implements the Adam algorithm. */\n+public class Adam extends org.tensorflow.framework.optimizers.Adam implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+  public static final String BETA_ONE_KEY = \"beta_1\";\n+  public static final String BETA_TWO_KEY = \"beta_2\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float EPSILON_DEFAULT = 1e-07F;\n+  public static final float BETA_ONE_DEFAULT = 0.9F;\n+  public static final float BETA_TWO_DEFAULT = 0.999F;\n+\n+  private float learningRate;\n+  private Map<String, Object> config = new HashMap<>();", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1NTU1MQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463655551", "bodyText": "Can we type this strongly? It seems weird to mimic method overloading by having a series of instanceofs inside a method. I realise it will blow up this class by making many more methods, but it's far safer. Especially as the generic check for the functions and suppliers will be washed off and so the user might get odd error messages.", "author": "Craigacp", "createdAt": "2020-07-31T14:47:54Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/Optimizers.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import org.tensorflow.framework.optimizers.Optimizer;\n+import org.tensorflow.op.Ops;\n+\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Functions to get an Optimizer based on String name, an Optimizer class, or lambda function.\n+ *\n+ * <p>Example:\n+ *\n+ * <pre>\n+ *     Adam instance = Optimizers.get(tf, \"adam\");\n+ *     Ftrl instance = Optimizers.get(tf, ltf -> new Ftrl(ltf, 0.1f);\n+ * </pre>\n+ */\n+public class Optimizers {\n+\n+  static Map<String, Function<Ops, Optimizer>> map =\n+      new HashMap<String, Function<Ops, Optimizer>>() {\n+        {\n+          put(\"adadelta\", tf -> new AdaDelta(tf));\n+          put(\"adagrad\", tf -> new AdaGrad(tf));\n+          put(\"adagrad-da\", tf -> new AdaGradDA(tf));\n+          put(\"adam\", tf -> new Adam(tf));\n+          put(\"adamax\", tf -> new Adamax(tf));\n+          put(\"ftrl\", tf -> new Ftrl(tf));\n+          put(\"nadam\", tf -> new Nadam(tf));\n+          put(\"rmsprop\", tf -> new RMSProp(tf));\n+          put(\"sgd\", tf -> new SGD(tf));\n+        }\n+      };\n+\n+  /**\n+   * Get an Optimizer\n+   *\n+   * @param optimizerFunction either a String that identifies the Optimizer, an Optimizer class, or\n+   *     an Optimizer object.\n+   * @return the Optimizer object or null if not found.\n+   */\n+  public static Optimizer get(Ops tf, Object optimizerFunction) {\n+    return get(tf, optimizerFunction, null);\n+  }\n+\n+  /**\n+   * Get an Initializer\n+   *\n+   * @param si a lamda function\n+   * @return the Intializer object\n+   */\n+  public static Optimizer get(Ops tf, Function<Ops, Optimizer> func) {\n+    return func.apply(tf);\n+  }\n+\n+  /**\n+   * Get an Initializer\n+   *\n+   * @param optimizerFunction\n+   * @param custom_functions a map of Initializer lambdas that will be queried if the Optimizer is\n+   *     not found in the standard keys\n+   * @return the Optimizer object\n+   */\n+  public static Optimizer get(\n+      Ops tf, Object optimizerFunction, Map<String, Function<Ops, Optimizer>> custom_functions) {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY1NzE2MA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463657160", "bodyText": "If momentum is zero, doesn't this still allocate all the extra RAM necessary to store the momentum parameters?", "author": "Craigacp", "createdAt": "2020-07-31T14:50:31Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizer/SGD.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/** Stochastic Gradient Descent and momentum optimizer. */\n+public class SGD extends org.tensorflow.framework.optimizers.Momentum", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2MDA3Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463660076", "bodyText": "This assert should probably be an if check. Also shouldn't it validate that the shapes are the same, rather than just containing the same number of elements?", "author": "Craigacp", "createdAt": "2020-07-31T14:55:21Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/ND.java", "diffHunk": "@@ -0,0 +1,717 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.util.Arrays;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.NdArray;\n+import org.tensorflow.ndarray.NdArrays;\n+import org.tensorflow.ndarray.Shape;\n+\n+// TODO used in the Callbacks, this should be a part of NDArray?\n+/** NDArray math Utilities */\n+public class ND {\n+\n+  /**\n+   * Returns a string representation of the contents of the specified array.\n+   *\n+   * <p>The string representation consists of a list of the array's elements, enclosed in square\n+   * brackets (\"[]\"). Adjacent elements are separated by the characters \", \" (a comma followed by a\n+   * space). Elements are converted to strings as by String.valueOf(int). Returns \"null\" if a is\n+   * null.\n+   *\n+   * @param array the array to convert.\n+   * @return the String representaion of the contents of the specified array\n+   */\n+  public static String toString(NdArray<?> array) {\n+    if (array == null) return \"null\";\n+    StringBuilder sb = new StringBuilder();\n+    sb.append(\"[\");\n+    AtomicBoolean first = new AtomicBoolean(true);\n+    array\n+        .elements(0)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              if (!first.get()) {\n+                sb.append(\", \");\n+              } else {\n+                first.set(false);\n+              }\n+              Object f = v.getObject();\n+              if (v.rank() == 0) {\n+                sb.append(f);\n+              } else {\n+                sb.append(toString(v));\n+              }\n+            });\n+    sb.append(\"]\");\n+    return sb.toString();\n+  }\n+\n+  /**\n+   * Transforms a flat index into coordinates based on shape.\n+   *\n+   * @param shape the shape\n+   * @param index the index\n+   * @return the coordinates\n+   */\n+  private static long[] getCoordinates(Shape shape, long index) {\n+    long[] coordinates = new long[shape.numDimensions()];\n+\n+    int numDims = shape.numDimensions();\n+    int i = numDims - 1;\n+    for (; i >= 0; i--) {\n+      long size = shape.size(i);\n+      long mod = index % size;\n+      coordinates[i] = mod;\n+      index -= mod;\n+    }\n+    return coordinates;\n+  }\n+\n+  /**\n+   * Gets the square root of an array.\n+   *\n+   * @param a the array\n+   * @return the square root of the array.\n+   */\n+  public static FloatNdArray sqrt(FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.sqrt(v.getFloat()), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Gets the square of an array.\n+   *\n+   * @param a the array\n+   * @return the square of the array.\n+   */\n+  public static FloatNdArray square(FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() * v.getFloat(), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Adds two arrays\n+   *\n+   * @param a the array\n+   * @param b the array\n+   * @return the resulting array from the add operation\n+   */\n+  public static FloatNdArray add(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() + b.getFloat(idx), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Adds an array with a scalar value\n+   *\n+   * @param a the array\n+   * @param scalar the scalar value\n+   * @return the resulting array from the add operation\n+   */\n+  public static FloatNdArray add(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() + scalar, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Adds an array with a scalar value\n+   *\n+   * @param scalar the scalar value\n+   * @param a the array\n+   * @return the resulting array from the add operation\n+   */\n+  public static FloatNdArray add(float scalar, FloatNdArray a) {\n+    return add(a, scalar);\n+  }\n+\n+  /**\n+   * subtracts one array from the other\n+   *\n+   * @param a the minuend array\n+   * @param b the subtrahend array\n+   * @return the resulting array from the subtraction operation\n+   */\n+  public static FloatNdArray sub(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() - b.getFloat(idx), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * subtracte scalar from an array\n+   *\n+   * @param a the minuend array\n+   * @param scalar the subtrahend value\n+   * @return the resulting array from the subtraction operation\n+   */\n+  public static FloatNdArray sub(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() - scalar, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * subtract an array from a scalar\n+   *\n+   * @param scalar the minuend value\n+   * @param a the subtrahend array\n+   * @return the resulting array from the subtraction operation\n+   */\n+  public static FloatNdArray sub(float scalar, FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(scalar - v.getFloat(), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Multiply 2 arrays\n+   *\n+   * @param a the first array\n+   * @param b the second array\n+   * @return the resulting array from the muliply operation\n+   */\n+  public static FloatNdArray mul(FloatNdArray a, FloatNdArray b) {\n+    assert a.shape().equals(b.shape())\n+        : String.format(\n+            \"ValueError: operands do not have same shapes %s %s \", a.shape(), b.shape());\n+    boolean sameSize = a.shape().size() == b.shape().size();\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              if (sameSize) {\n+                result.setFloat(v.getFloat() * b.getFloat(idx), idx);\n+              } else {\n+                float value = v.getFloat() * b.getFloat(idx[0], 0L);\n+                result.setFloat(value, idx);\n+              }\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Multiply an array with a scalar value\n+   *\n+   * @param a the array\n+   * @param scalar the scalar value\n+   * @return the resulting array from the Multiply operation\n+   */\n+  public static FloatNdArray mul(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    if (a.shape().isScalar()) {\n+      a.scalars().forEach(f -> result.setFloat(f.getFloat() * scalar));\n+    } else {\n+      a.scalars().forEachIndexed((idx, f) -> result.setFloat(f.getFloat() * scalar, idx));\n+    }\n+\n+    return result;\n+  }\n+\n+  /**\n+   * Multiply a scalar value with an array\n+   *\n+   * @param scalar the scalar value\n+   * @param a the array\n+   * @return the resulting array from the Multiply operation\n+   */\n+  public static FloatNdArray mul(float scalar, FloatNdArray a) {\n+    return mul(a, scalar);\n+  }\n+\n+  /**\n+   * Divide two arrays\n+   *\n+   * @param a the dividend array\n+   * @param b the divisor array\n+   * @return the resulting array from the Divide operation\n+   */\n+  public static FloatNdArray div(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() / b.getFloat(idx), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Divide an array by a scalar\n+   *\n+   * @param a the dividend array\n+   * @param scalar the scalar divisor\n+   * @return the resulting array from the Divide operation\n+   */\n+  public static FloatNdArray div(FloatNdArray a, float scalar) {\n+    assert (scalar != 0);\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() / scalar, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Divide a scalar by an array\n+   *\n+   * @param scalar the scalar dividend\n+   * @param a the divisor array\n+   * @return the resulting array from the Divide operation\n+   */\n+  public static FloatNdArray div(float scalar, FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              float value = v.getFloat() == 0.0F ? Float.NaN : scalar / v.getFloat();\n+              result.setFloat(value, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Raise the first array by the power of the second array\n+   *\n+   * @param a the first array\n+   * @param b the second array\n+   * @return the array result of the power operation\n+   */\n+  public static FloatNdArray pow(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.pow(v.getFloat(), b.getFloat(idx)), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Raise the first array by the power of the scalar value\n+   *\n+   * @param a the first array\n+   * @param scalar the scalar value\n+   * @return the array result of the power operation\n+   */\n+  public static FloatNdArray pow(FloatNdArray a, float scalar) {\n+    assert (scalar != 0);\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.pow(v.getFloat(), scalar), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Raise the scalar value by the power of the array\n+   *\n+   * @param scalar the scalar value\n+   * @param a the first array\n+   * @return the array result of the power operation\n+   */\n+  public static FloatNdArray pow(float scalar, FloatNdArray a) {\n+    assert (scalar != 0);\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.pow(scalar, v.getFloat()), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Flatten an array to 1D\n+   *\n+   * @param a the array to flatten\n+   * @return the flattened array\n+   */\n+  public static float[] flatten(FloatNdArray a) {\n+    float[] result = new float[(int) a.shape().size()];\n+    int nDims = a.shape().numDimensions();\n+    AtomicInteger counter = new AtomicInteger();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result[counter.getAndAdd(1)] = v.getFloat();\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Get the maximum value of the array\n+   *\n+   * @param a the array\n+   * @return the maximum value of the array\n+   */\n+  public static float max(FloatNdArray a) {\n+    AtomicReference<Float> maximum = new AtomicReference<>(Float.MIN_VALUE);\n+    a.scalars().forEach(f -> maximum.set(Math.max(maximum.get(), f.getFloat())));\n+    return maximum.get();\n+  }\n+\n+  /**\n+   * Get the minimum value of the array\n+   *\n+   * @param a the array\n+   * @return the minimum value of the array\n+   */\n+  public static float min(FloatNdArray a) {\n+    AtomicReference<Float> minimum = new AtomicReference<>(Float.MAX_VALUE);\n+    a.scalars().forEach(f -> minimum.set(Math.min(minimum.get(), f.getFloat())));\n+    return minimum.get();\n+  }\n+\n+  /**\n+   * Get the maximum value of comparing the arrays\n+   *\n+   * @param a the first array\n+   * @param a the second array\n+   * @return the resulting array with the maximum values between each element of the arrays.\n+   * @throws java.lang.AssertionError if the two arrays are not the same size.\n+   */\n+  public static FloatNdArray max(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2MDY5Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463660693", "bodyText": "Do we want a method that accepts a logger of some kind? Or an output stream?", "author": "Craigacp", "createdAt": "2020-07-31T14:56:25Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/ND.java", "diffHunk": "@@ -0,0 +1,717 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.util.Arrays;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.NdArray;\n+import org.tensorflow.ndarray.NdArrays;\n+import org.tensorflow.ndarray.Shape;\n+\n+// TODO used in the Callbacks, this should be a part of NDArray?\n+/** NDArray math Utilities */\n+public class ND {\n+\n+  /**\n+   * Returns a string representation of the contents of the specified array.\n+   *\n+   * <p>The string representation consists of a list of the array's elements, enclosed in square\n+   * brackets (\"[]\"). Adjacent elements are separated by the characters \", \" (a comma followed by a\n+   * space). Elements are converted to strings as by String.valueOf(int). Returns \"null\" if a is\n+   * null.\n+   *\n+   * @param array the array to convert.\n+   * @return the String representaion of the contents of the specified array\n+   */\n+  public static String toString(NdArray<?> array) {\n+    if (array == null) return \"null\";\n+    StringBuilder sb = new StringBuilder();\n+    sb.append(\"[\");\n+    AtomicBoolean first = new AtomicBoolean(true);\n+    array\n+        .elements(0)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              if (!first.get()) {\n+                sb.append(\", \");\n+              } else {\n+                first.set(false);\n+              }\n+              Object f = v.getObject();\n+              if (v.rank() == 0) {\n+                sb.append(f);\n+              } else {\n+                sb.append(toString(v));\n+              }\n+            });\n+    sb.append(\"]\");\n+    return sb.toString();\n+  }\n+\n+  /**\n+   * Transforms a flat index into coordinates based on shape.\n+   *\n+   * @param shape the shape\n+   * @param index the index\n+   * @return the coordinates\n+   */\n+  private static long[] getCoordinates(Shape shape, long index) {\n+    long[] coordinates = new long[shape.numDimensions()];\n+\n+    int numDims = shape.numDimensions();\n+    int i = numDims - 1;\n+    for (; i >= 0; i--) {\n+      long size = shape.size(i);\n+      long mod = index % size;\n+      coordinates[i] = mod;\n+      index -= mod;\n+    }\n+    return coordinates;\n+  }\n+\n+  /**\n+   * Gets the square root of an array.\n+   *\n+   * @param a the array\n+   * @return the square root of the array.\n+   */\n+  public static FloatNdArray sqrt(FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.sqrt(v.getFloat()), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Gets the square of an array.\n+   *\n+   * @param a the array\n+   * @return the square of the array.\n+   */\n+  public static FloatNdArray square(FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() * v.getFloat(), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Adds two arrays\n+   *\n+   * @param a the array\n+   * @param b the array\n+   * @return the resulting array from the add operation\n+   */\n+  public static FloatNdArray add(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() + b.getFloat(idx), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Adds an array with a scalar value\n+   *\n+   * @param a the array\n+   * @param scalar the scalar value\n+   * @return the resulting array from the add operation\n+   */\n+  public static FloatNdArray add(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() + scalar, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Adds an array with a scalar value\n+   *\n+   * @param scalar the scalar value\n+   * @param a the array\n+   * @return the resulting array from the add operation\n+   */\n+  public static FloatNdArray add(float scalar, FloatNdArray a) {\n+    return add(a, scalar);\n+  }\n+\n+  /**\n+   * subtracts one array from the other\n+   *\n+   * @param a the minuend array\n+   * @param b the subtrahend array\n+   * @return the resulting array from the subtraction operation\n+   */\n+  public static FloatNdArray sub(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() - b.getFloat(idx), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * subtracte scalar from an array\n+   *\n+   * @param a the minuend array\n+   * @param scalar the subtrahend value\n+   * @return the resulting array from the subtraction operation\n+   */\n+  public static FloatNdArray sub(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() - scalar, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * subtract an array from a scalar\n+   *\n+   * @param scalar the minuend value\n+   * @param a the subtrahend array\n+   * @return the resulting array from the subtraction operation\n+   */\n+  public static FloatNdArray sub(float scalar, FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(scalar - v.getFloat(), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Multiply 2 arrays\n+   *\n+   * @param a the first array\n+   * @param b the second array\n+   * @return the resulting array from the muliply operation\n+   */\n+  public static FloatNdArray mul(FloatNdArray a, FloatNdArray b) {\n+    assert a.shape().equals(b.shape())\n+        : String.format(\n+            \"ValueError: operands do not have same shapes %s %s \", a.shape(), b.shape());\n+    boolean sameSize = a.shape().size() == b.shape().size();\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              if (sameSize) {\n+                result.setFloat(v.getFloat() * b.getFloat(idx), idx);\n+              } else {\n+                float value = v.getFloat() * b.getFloat(idx[0], 0L);\n+                result.setFloat(value, idx);\n+              }\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Multiply an array with a scalar value\n+   *\n+   * @param a the array\n+   * @param scalar the scalar value\n+   * @return the resulting array from the Multiply operation\n+   */\n+  public static FloatNdArray mul(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    if (a.shape().isScalar()) {\n+      a.scalars().forEach(f -> result.setFloat(f.getFloat() * scalar));\n+    } else {\n+      a.scalars().forEachIndexed((idx, f) -> result.setFloat(f.getFloat() * scalar, idx));\n+    }\n+\n+    return result;\n+  }\n+\n+  /**\n+   * Multiply a scalar value with an array\n+   *\n+   * @param scalar the scalar value\n+   * @param a the array\n+   * @return the resulting array from the Multiply operation\n+   */\n+  public static FloatNdArray mul(float scalar, FloatNdArray a) {\n+    return mul(a, scalar);\n+  }\n+\n+  /**\n+   * Divide two arrays\n+   *\n+   * @param a the dividend array\n+   * @param b the divisor array\n+   * @return the resulting array from the Divide operation\n+   */\n+  public static FloatNdArray div(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() / b.getFloat(idx), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Divide an array by a scalar\n+   *\n+   * @param a the dividend array\n+   * @param scalar the scalar divisor\n+   * @return the resulting array from the Divide operation\n+   */\n+  public static FloatNdArray div(FloatNdArray a, float scalar) {\n+    assert (scalar != 0);\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat(v.getFloat() / scalar, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Divide a scalar by an array\n+   *\n+   * @param scalar the scalar dividend\n+   * @param a the divisor array\n+   * @return the resulting array from the Divide operation\n+   */\n+  public static FloatNdArray div(float scalar, FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              float value = v.getFloat() == 0.0F ? Float.NaN : scalar / v.getFloat();\n+              result.setFloat(value, idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Raise the first array by the power of the second array\n+   *\n+   * @param a the first array\n+   * @param b the second array\n+   * @return the array result of the power operation\n+   */\n+  public static FloatNdArray pow(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.pow(v.getFloat(), b.getFloat(idx)), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Raise the first array by the power of the scalar value\n+   *\n+   * @param a the first array\n+   * @param scalar the scalar value\n+   * @return the array result of the power operation\n+   */\n+  public static FloatNdArray pow(FloatNdArray a, float scalar) {\n+    assert (scalar != 0);\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.pow(v.getFloat(), scalar), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Raise the scalar value by the power of the array\n+   *\n+   * @param scalar the scalar value\n+   * @param a the first array\n+   * @return the array result of the power operation\n+   */\n+  public static FloatNdArray pow(float scalar, FloatNdArray a) {\n+    assert (scalar != 0);\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.pow(scalar, v.getFloat()), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Flatten an array to 1D\n+   *\n+   * @param a the array to flatten\n+   * @return the flattened array\n+   */\n+  public static float[] flatten(FloatNdArray a) {\n+    float[] result = new float[(int) a.shape().size()];\n+    int nDims = a.shape().numDimensions();\n+    AtomicInteger counter = new AtomicInteger();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result[counter.getAndAdd(1)] = v.getFloat();\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Get the maximum value of the array\n+   *\n+   * @param a the array\n+   * @return the maximum value of the array\n+   */\n+  public static float max(FloatNdArray a) {\n+    AtomicReference<Float> maximum = new AtomicReference<>(Float.MIN_VALUE);\n+    a.scalars().forEach(f -> maximum.set(Math.max(maximum.get(), f.getFloat())));\n+    return maximum.get();\n+  }\n+\n+  /**\n+   * Get the minimum value of the array\n+   *\n+   * @param a the array\n+   * @return the minimum value of the array\n+   */\n+  public static float min(FloatNdArray a) {\n+    AtomicReference<Float> minimum = new AtomicReference<>(Float.MAX_VALUE);\n+    a.scalars().forEach(f -> minimum.set(Math.min(minimum.get(), f.getFloat())));\n+    return minimum.get();\n+  }\n+\n+  /**\n+   * Get the maximum value of comparing the arrays\n+   *\n+   * @param a the first array\n+   * @param a the second array\n+   * @return the resulting array with the maximum values between each element of the arrays.\n+   * @throws java.lang.AssertionError if the two arrays are not the same size.\n+   */\n+  public static FloatNdArray max(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.max(v.getFloat(), b.getFloat(idx)), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Get the maximum value of comparing each item of the array to scalar\n+   *\n+   * @param a the array\n+   * @param scalar the scalar value\n+   * @return the resulting array with the maximum values between each element of the array and the\n+   *     scalar value\n+   */\n+  public static FloatNdArray max(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.max(v.getFloat(), scalar), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Get the maximum value of comparing each item of the array to scalar\n+   *\n+   * @param scalar the scalar value\n+   * @param a the array\n+   * @return the resulting array with the maximum values between each element of the array and the\n+   *     scalar value\n+   */\n+  public static FloatNdArray max(float scalar, FloatNdArray a) {\n+    return max(a, scalar);\n+  }\n+\n+  /**\n+   * Get the minimum value of comparing the arrays\n+   *\n+   * @param a the first array\n+   * @param a the second array\n+   * @return the resulting array with the minimum values between each element of the arrays.\n+   * @throws java.lang.AssertionError if the two arrays are not the same size.\n+   */\n+  public static FloatNdArray min(FloatNdArray a, FloatNdArray b) {\n+    assert (a.shape().size() == b.shape().size());\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.min(v.getFloat(), b.getFloat(idx)), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Get the minimum value of comparing each item of the array to scalar\n+   *\n+   * @param a the array\n+   * @param scalar the scalar value\n+   * @return the resulting array with the minimum values between each element of the array and the\n+   *     scalar value\n+   */\n+  public static FloatNdArray min(FloatNdArray a, float scalar) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    int nDims = a.shape().numDimensions();\n+    a.elements(nDims - 1)\n+        .forEachIndexed(\n+            (idx, v) -> {\n+              result.setFloat((float) Math.min(v.getFloat(), scalar), idx);\n+            });\n+    return result;\n+  }\n+\n+  /**\n+   * Get the minimum value of comparing each item of the array to scalar\n+   *\n+   * @param scalar the scalar value\n+   * @param a the array\n+   * @return the resulting array with the minimum values between each element of the array and the\n+   *     scalar value\n+   */\n+  public static FloatNdArray min(float scalar, FloatNdArray a) {\n+    return min(a, scalar);\n+  }\n+\n+  /**\n+   * Get the absolute value of each member of the array\n+   *\n+   * @param a the array\n+   * @return the array with the absolute value of each item.\n+   */\n+  public static FloatNdArray abs(FloatNdArray a) {\n+    FloatNdArray result = NdArrays.ofFloats(a.shape());\n+    a.scalars().forEachIndexed((idx, f) -> result.setFloat((float) Math.abs(f.getFloat()), idx));\n+    return result;\n+  }\n+\n+  /**\n+   * Sum all elements of an array\n+   *\n+   * @param a the array\n+   * @return an a array with one element containing the sum.\n+   */\n+  public static FloatNdArray sum(FloatNdArray a) {\n+    AtomicReference<Float> sum = new AtomicReference<>(0.f);\n+    a.scalars().forEach(f -> sum.set(sum.get() + f.getFloat()));\n+    return NdArrays.scalarOf(sum.get());\n+  }\n+\n+  /**\n+   * Sum all elements of an array based on the specified axis\n+   *\n+   * @param a the array\n+   * @param axis the axis to sum\n+   * @return an a array the sum over the axis less the diemsnion\n+   */\n+  public static FloatNdArray sum(FloatNdArray a, int axis) {\n+    return sum(a, axis, false);\n+  }\n+\n+  /**\n+   * Sum all elements of an array based on the specified axis\n+   *\n+   * @param a the array\n+   * @param axis the axis to sum\n+   * @param keepDims indicates whether the dimensions over the sum should be kept or not.\n+   * @return an a array the sum over the axis\n+   */\n+  public static FloatNdArray sum(FloatNdArray a, int axis, boolean keepDims) {\n+    Shape shape = a.shape();\n+    int nDims = shape.numDimensions();\n+    int xis = nDims - 1 - axis;\n+    long totalSize = shape.size();\n+    long axisSize = shape.size(xis);\n+    final float[] sums = new float[(int) axisSize];\n+\n+    a.scalars()\n+        .forEachIndexed(\n+            (idx, f) -> {\n+              sums[(int) idx[xis]] += f.getFloat();\n+            });\n+\n+    if (keepDims) {\n+      long[] newDims = shape.asArray();\n+      newDims[axis] = 1;\n+      final AtomicInteger counter = new AtomicInteger();\n+      FloatNdArray arrayK = NdArrays.ofFloats(Shape.of(newDims));\n+      arrayK\n+          .elements(newDims.length - 1)\n+          .forEachIndexed(\n+              (idx, v) -> {\n+                v.setFloat(sums[counter.getAndAdd(1)]);\n+              });\n+      return arrayK;\n+    } else {\n+      return NdArrays.vectorOf(sums);\n+    }\n+  }\n+\n+  /**\n+   * Sum all elements of an array based on the specified axis\n+   *\n+   * @param a the array\n+   * @param axes the axis to sum\n+   * @param keepDims indicates whether the dimensions over the sum should be kept or not.\n+   * @return an a array the sum over the axis\n+   */\n+  public static FloatNdArray sum(FloatNdArray a, Integer[] axes, boolean keepDims) {\n+    Shape shape = a.shape();\n+    if (axes == null) {\n+      FloatNdArray result = sum(a);\n+      if (keepDims) {\n+        float scalar = result.getFloat(0);\n+        long[] dims = {1, 1};\n+        Shape bShape = Shape.of(dims);\n+        FloatNdArray resultK = NdArrays.ofFloats(bShape);\n+        resultK.setFloat(scalar, 0, 0);\n+        return resultK;\n+      }\n+      return result;\n+    } else if (axes.length == 1) {\n+      return sum(a, axes[0], keepDims);\n+    } else {\n+      // TODO\n+      throw new UnsupportedOperationException(\"Multi Axis Not implemented Yet\");\n+    }\n+  }\n+\n+  /**\n+   * Calculate the l2 norm of the array\n+   *\n+   * @param x the array\n+   * @return the l2 norm of the array\n+   */\n+  public static FloatNdArray l2_norm(FloatNdArray x) {\n+    return l2_norm(x, -1);\n+  }\n+\n+  /**\n+   * Calculate the l2 norm of the array\n+   *\n+   * @param x the array\n+   * @param axis the axis to calculate over\n+   * @return the l2 norm of the array\n+   */\n+  public static FloatNdArray l2_norm(FloatNdArray x, int axis) {\n+    float epsilon = 1e-12F;\n+    FloatNdArray square_sum = ND.sum(ND.square(x), axis, true);\n+    FloatNdArray x_inv_norm = ND.div(1, ND.sqrt(ND.max(square_sum, epsilon)));\n+    return ND.mul(x, x_inv_norm);\n+  }\n+\n+  /**\n+   * Print the array to System.out.\n+   *\n+   * @param a the array\n+   */\n+  public static void print(FloatNdArray a) {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2MjE4Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463662183", "bodyText": "Typo: \"Tensot\"", "author": "Craigacp", "createdAt": "2020-07-31T14:59:06Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/PrintUtils.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TString;\n+\n+/**\n+ *  Utility to print Tensor values\n+ */\n+public class PrintUtils {\n+\n+    /**\n+     * Print a tensor's values\n+     * @param tensor the tensor to print\n+     */\n+    public static void print(Tensor tensor) {\n+        switch(tensor.dataType().name()) {\n+            case \"BFLOAT16\":\n+                printTBfloat16(tensor);\n+                break;\n+            case \"FLOAT16\":\n+                 printTFloat16(tensor);\n+                break;\n+            case \"FLOAT\":\n+                 printTFloat32(tensor);\n+                break;\n+            case \"DOUBLE\":\n+                 printTFloat64(tensor);\n+                break;\n+            case \"INT32\":\n+                 printTInt32(tensor);\n+                break;\n+            case \"INT64\":\n+                 printTInt64(tensor);\n+                break;\n+            case \"UINT8\":\n+                 printTUint8(tensor);\n+                break;\n+            case \"BOOL\":\n+                 printTBool(tensor);\n+                break;\n+            case \"STRING\":\n+                 printTString(tensor);\n+                break;\n+            default:\n+                break;\n+        }\n+        \n+    }\n+\n+    /**\n+     * Print a boolean Tensot", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2MjMxNw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463662317", "bodyText": "Typo: \"Tensot\"", "author": "Craigacp", "createdAt": "2020-07-31T14:59:22Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/PrintUtils.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TString;\n+\n+/**\n+ *  Utility to print Tensor values\n+ */\n+public class PrintUtils {\n+\n+    /**\n+     * Print a tensor's values\n+     * @param tensor the tensor to print\n+     */\n+    public static void print(Tensor tensor) {\n+        switch(tensor.dataType().name()) {\n+            case \"BFLOAT16\":\n+                printTBfloat16(tensor);\n+                break;\n+            case \"FLOAT16\":\n+                 printTFloat16(tensor);\n+                break;\n+            case \"FLOAT\":\n+                 printTFloat32(tensor);\n+                break;\n+            case \"DOUBLE\":\n+                 printTFloat64(tensor);\n+                break;\n+            case \"INT32\":\n+                 printTInt32(tensor);\n+                break;\n+            case \"INT64\":\n+                 printTInt64(tensor);\n+                break;\n+            case \"UINT8\":\n+                 printTUint8(tensor);\n+                break;\n+            case \"BOOL\":\n+                 printTBool(tensor);\n+                break;\n+            case \"STRING\":\n+                 printTString(tensor);\n+                break;\n+            default:\n+                break;\n+        }\n+        \n+    }\n+\n+    /**\n+     * Print a boolean Tensot\n+     * @param t the tensor to print\n+     */\n+    public static void printTBool(Tensor<TBool> t) {\n+        t.data().scalars().forEach(s -> System.out.print(s.getBoolean() + \", \"));\n+        System.out.println();\n+    }\n+\n+    /**\n+     * Print a String Tensot", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2MzQ3OA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463663478", "bodyText": "Swap asserts for ifs.", "author": "Craigacp", "createdAt": "2020-07-31T15:01:30Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/SmartCond.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.util.function.Supplier;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+\n+/** Implements a select Operation using Lambdas */\n+public class SmartCond {\n+\n+  /**\n+   * Creates a select operation\n+   *\n+   * @param pred the predicate (boolean) used to select\n+   * @param then_fn the function to execute if the predicate is true\n+   * @param else_fn the function to execute if the predicate is false\n+   * @return the result of the select on the condition\n+   */\n+  public static Operand select(Boolean pred, Supplier<Operand> then_fn, Supplier<Operand> else_fn) {\n+    assert pred != null : \"pred must not be null\";", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NDEzMg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463664132", "bodyText": "typo", "author": "Craigacp", "createdAt": "2020-07-31T15:02:42Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/SmartCond.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.util.function.Supplier;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+\n+/** Implements a select Operation using Lambdas */\n+public class SmartCond {\n+\n+  /**\n+   * Creates a select operation\n+   *\n+   * @param pred the predicate (boolean) used to select\n+   * @param then_fn the function to execute if the predicate is true\n+   * @param else_fn the function to execute if the predicate is false\n+   * @return the result of the select on the condition\n+   */\n+  public static Operand select(Boolean pred, Supplier<Operand> then_fn, Supplier<Operand> else_fn) {\n+    assert pred != null : \"pred must not be null\";\n+    assert then_fn != null : \"then_fn must not be null\";\n+    assert else_fn != null : \"else_fn must not be null\";\n+    return pred ? then_fn.get() : else_fn.get();\n+  }\n+\n+  /**\n+   * Creates a select operation\n+   *\n+   * @param pred the predicate ( true == 1) used to select\n+   * @param then_fn the function to execute if the predicate is true\n+   * @param else_fn the function to execute if the predicate is false\n+   * @return the result of the select on the condition\n+   */\n+  public static Operand select(Number pred, Supplier<Operand> then_fn, Supplier<Operand> else_fn) {\n+    assert pred != null : \"pred must not be null\";\n+    assert then_fn != null : \"then_fn must not be null\";\n+    assert else_fn != null : \"else_fn must not be null\";\n+    return pred.intValue() == 1 ? then_fn.get() : else_fn.get();\n+  }\n+\n+  /**\n+   * Creates a select operation\n+   *\n+   * @param pred the predicate ( true if the string argument is not null and is equal, ignoring\n+   *     case, to the string \"true\") used to select\n+   * @param then_fn the function to execute if the predicate is true\n+   * @param else_fn the function to execute if the predicate is false\n+   * @return the result of the select on the condition\n+   */\n+  public static Operand select(String pred, Supplier<Operand> then_fn, Supplier<Operand> else_fn) {\n+    assert pred != null : \"pred must not be null\";\n+    assert then_fn != null : \"then_fn must not be null\";\n+    assert else_fn != null : \"else_fn must not be null\";\n+    return Boolean.valueOf(pred) ? then_fn.get() : else_fn.get();\n+  }\n+\n+  /**\n+   * Create a Select operation\n+   *\n+   * @param tf the tensorFlow Ops\n+   * @param pred the operand that evaluates to true or false\n+   * @param then_fn the function to execute if the predicate is true\n+   * @param else_fn the function to execute if the predicate is false\n+   * @return a Select Operation if in graph mode, else return the result of the select\n+   */\n+  public static Operand select(\n+      Ops tf, Operand<TBool> pred, Supplier<Operand> then_fn, Supplier<Operand> else_fn) {\n+    assert pred != null : \"pred must not be null\";\n+    assert then_fn != null : \"then_fn must not be null\";\n+    assert else_fn != null : \"else_fn must not be null\";\n+    if (tf.scope().env().isEager()) {\n+      return pred.asOutput().data().getBoolean() ? then_fn.get() : else_fn.get();\n+    } else { // TODO, maybe some day handle Supplier in the c interface\n+      return tf.select(pred, then_fn.get(), else_fn.get());\n+    }\n+  }\n+\n+  /**\n+   * Creates a slect operation", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NTUxNA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463665514", "bodyText": "Typos.", "author": "Craigacp", "createdAt": "2020-07-31T15:05:12Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/SymbolicShape.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.tensorflow.Operand;\n+\n+/**\n+ * Utility class that handles sybmolic shapes so that the shapes can be resolved during runtime and\n+ * stay consistent across operands.\n+ *\n+ * <p>The same symbols accros variaous shapes should have the same dimension values.", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2NTQ2Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463965463", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-08-01T14:07:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NTUxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE1ODgzOQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474158839", "bodyText": "I removed SymbolicShapes from this PR as it is needed when I do a PR on metrics.", "author": "JimClarke5", "createdAt": "2020-08-20T17:35:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NTUxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NjA3MA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463666070", "bodyText": "Do we want the ability to overwrite values that already exist? If so, should we also have a remove method?", "author": "Craigacp", "createdAt": "2020-07-31T15:06:13Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/SymbolicShapeDict.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Utility class that contains a dictionary of Symbols. The map is updated with symbols and their\n+ * dimension values. Used in creating matching shapes that depend on other shapes' dimensions.\n+ */\n+public class SymbolicShapeDict {\n+\n+  /** Dictionary containing the sybmols and their associated dimensions sizes. */\n+  private final Map<String, Long> map = new HashMap<>();\n+\n+  /**\n+   * Add a symbol with its associated dimensions size.\n+   *\n+   * @param symbol the symbol name\n+   * @param size the shape dimension to associate with the symbol\n+   */\n+  public void put(String symbol, Long size) {\n+    this.map.put(symbol, size);", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2NTY5Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463965696", "bodyText": "I could change the name to \"add\", then have the method remove the old one first.", "author": "JimClarke5", "createdAt": "2020-08-01T14:11:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NjA3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk4OTkwMA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463989900", "bodyText": "Well it'll overwrite them at the moment, but it's more from a semantics perspective, do we need the ability to overwrite symbol names, and if we have it will it cause trouble.", "author": "Craigacp", "createdAt": "2020-08-01T18:57:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NjA3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ1MTA2NA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r464451064", "bodyText": "SymbolicShapes define a set of rules for shapes that a set of Operands have in common. Once created, they are used to create AssertThat's to make sure the rules are followed among the Operands. This is done within one method, as method variables, so currently, there is no chance for the rules to be replaced later.\nUsually the symbols are created as a group within  a Metric, primarily the ConfusionMatrix type metrics (AUC),\nAfter they are set up, then the AssertThat's are created and the SymbolicShapes are no longer used.", "author": "JimClarke5", "createdAt": "2020-08-03T14:30:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NjA3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA0NDkyMA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r467044920", "bodyText": "Also, the only place that I have currently found where this used is in the AUC (Area Under Curve) Metric. We could implement the Map and Symbolic Shape classes inside of the AUC class.", "author": "JimClarke5", "createdAt": "2020-08-07T13:36:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NjA3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDExMzE0OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474113149", "bodyText": "That sounds good.", "author": "Craigacp", "createdAt": "2020-08-20T16:27:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NjA3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE1ODMzNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474158335", "bodyText": "SymbolicShapes is not need until I check in Metrics, so I removed it from this PR.", "author": "JimClarke5", "createdAt": "2020-08-20T17:34:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NjA3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2Njg0OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463666849", "bodyText": "Do we need this if there is no support for it?", "author": "Craigacp", "createdAt": "2020-07-31T15:07:43Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/TypeUtils.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TString;\n+import org.tensorflow.types.TUint8;\n+\n+\n+/**\n+ *\n+ * @author Jim Clarke\n+ */\n+public class TypeUtils {\n+    \n+    //TODO\n+    public static boolean isComplex(DataType dtype) {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2NTU2NA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463965564", "bodyText": "I can remove it. It was just a placeholder for the future.", "author": "JimClarke5", "createdAt": "2020-08-01T14:09:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2Njg0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE1Nzc5Mg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474157792", "bodyText": "I removed the functionality of TypeUtils.java to DataType.java. I have removed TypeUtils.java from this PR and will be checking in the changes to DataTypes and its instances (e.g. TFloat32). I did not include isComplex().", "author": "JimClarke5", "createdAt": "2020-08-20T17:33:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2Njg0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NzIxMw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463667213", "bodyText": "Should we move all of this onto DataType? It feels like these would be better as instance methods.", "author": "Craigacp", "createdAt": "2020-07-31T15:08:22Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/utils/TypeUtils.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TString;\n+import org.tensorflow.types.TUint8;\n+\n+\n+/**\n+ *\n+ * @author Jim Clarke\n+ */\n+public class TypeUtils {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2NTU5NA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463965594", "bodyText": "I agree", "author": "JimClarke5", "createdAt": "2020-08-01T14:09:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NzIxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA3NDgyMg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r467074822", "bodyText": "In the different data types, the String names are raw strings, e.g.\n DataType<TInt32> DTYPE = DataType.create(\"INT32\", 3, 4, TInt32Impl::mapTensor);\nshould we create a public static field?\npublic static final String NAME = \"INT32\";\nThen the name can be accessed as TInt32.NAME rather than repeating the raw strings in the  DataType source.", "author": "JimClarke5", "createdAt": "2020-08-07T14:26:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NzIxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDExMzM3Mg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474113372", "bodyText": "I think that's a good idea.", "author": "Craigacp", "createdAt": "2020-08-20T16:28:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NzIxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE2MDEwOA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474160108", "bodyText": "I have moved this functionality to DataType and its instances (e.g TFloat32). This change will be added to this PR.", "author": "JimClarke5", "createdAt": "2020-08-20T17:37:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY2NzIxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY3MTM4OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463671389", "bodyText": "Formatting", "author": "Craigacp", "createdAt": "2020-07-31T15:15:29Z", "path": "tensorflow-keras/src/test/java/org/tensorflow/keras/utils/EagerTestSession.java", "diffHunk": "@@ -0,0 +1,747 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.utils;\n+\n+import java.io.PrintWriter;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Predicate;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.junit.jupiter.api.Assertions.fail;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import org.tensorflow.Session;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TString;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Eaager Mode Test Session */\n+public class EagerTestSession extends TestSession {\n+\n+  private final EagerSession session;\n+  private final Ops tf;\n+\n+  /** Create an Eager mode test session. */\n+  public EagerTestSession() {\n+    this.session = EagerSession.create();\n+    this.tf = Ops.create(session).withName(\"test\");\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Ops getTF() {\n+    return tf;\n+  }\n+\n+    /**", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2NjM2NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r463966365", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-08-01T14:19:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY3MTM4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1Njg1Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469656853", "bodyText": "According to Google Java Style guide, which we apply as much as possible, constants should be all uppercase.", "author": "karllessard", "createdAt": "2020-08-13T02:16:31Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzQ1MTU3NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473451575", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-08-19T23:57:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1Njg1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE2MTAxNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474161015", "bodyText": "I had fixed this to be public static final float EPSILON = 1e-7F; However, K.java has been removed from this PR and will be reintroduced as needed in future PRs.", "author": "JimClarke5", "createdAt": "2020-08-20T17:39:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1Njg1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1ODMzNA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469658334", "bodyText": "I think throwing a regular IllegalArgumentException would be more appropriate since that is what we do everywhere else in the code on similar validations.", "author": "karllessard", "createdAt": "2020-08-13T02:22:09Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE1Njc4Nw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r470156787", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-08-13T18:21:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1ODMzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE2MTA5Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474161096", "bodyText": "Done", "author": "JimClarke5", "createdAt": "2020-08-20T17:39:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1ODMzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1ODk4MQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469658981", "bodyText": "Just curious, why casting the booleans only, it was causing an error?", "author": "karllessard", "createdAt": "2020-08-13T02:24:28Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1OTU4NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469659585", "bodyText": "Before casting, should we check that the datatypes are not the same? Frankly, I don't know which one is the faster. If we do this, then we should probably do it everywhere we are casting a tensor to reflect the type of another. Maybe we can then wrap the cast method with somelike like this?\npublic static <T extends TType> Operand<T> castLike(Ops tf, Operand<T> x, Operand<?> y) {\n    if (x.asOutput().dataType() != y.asOutput().dataType()) {\n        return tf.dtypes.cast(y, x.asOutput().dataType());\n    }\n    return (Operand)y;\n}", "author": "karllessard", "createdAt": "2020-08-13T02:26:41Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2MDc5OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469660799", "bodyText": "Again, maybe replace assertions by exceptions?", "author": "karllessard", "createdAt": "2020-08-13T02:31:32Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2MTIxMg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469661212", "bodyText": "Typo: \"lables\" -> \"labels\"", "author": "karllessard", "createdAt": "2020-08-13T02:32:51Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2MTUxMA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469661510", "bodyText": "Just double-checking with you @JimClarke5  that we expose as much as possible only the same methods found in the Python's version of this class. The goal of tensorflow-keras is really only to replicate the Keras API and I think other useful high-level utilities should be made available to users via tensorflow-framework instead.", "author": "karllessard", "createdAt": "2020-08-13T02:34:14Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2MzAwMQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469663001", "bodyText": "The way you retrieve the op type (in comment below) is a proper way to do it. Though I'm not sure to understand clearly what is the ultimate goal of this method.", "author": "karllessard", "createdAt": "2020-08-13T02:39:39Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2MzYzNg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469663636", "bodyText": "maybe checking the output.op.type() instead of the instance is better again because if your operand comes from, let say, a saved model that you have loaded, you won't retrieve the original instance of the op wrappers, it will just be a graph of nodes (outputs).", "author": "karllessard", "createdAt": "2020-08-13T02:41:58Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2NDMyMA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469664320", "bodyText": "For Java, let's stick to camel case method names, as found in the guide as well (also applies to the methods below)", "author": "karllessard", "createdAt": "2020-08-13T02:44:58Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2NTUyMw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469665523", "bodyText": "\"oneMinusEpsilonConst\"", "author": "karllessard", "createdAt": "2020-08-13T02:49:42Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2NzMyNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469667325", "bodyText": "Shape already support equality check via .equals(), I don't think we need something else", "author": "karllessard", "createdAt": "2020-08-13T02:56:35Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2OTQ2Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469669463", "bodyText": "We should be able to support this soon :)", "author": "karllessard", "createdAt": "2020-08-13T03:05:25Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand minusOne = tf.constant(-1);\n+    Operand precise_logits = logits;\n+    Operand one = tf.constant(1L);\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n+    }\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = tf.dtypes.cast(labels, dtype);\n+    Operand inputRank = tf.dtypes.cast(tf.rank(precise_logits), TInt64.DTYPE);\n+    Operand inputRankMinusOne = tf.dtypes.cast(tf.math.sub(inputRank, one), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(tf, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(tf, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(tf, precise_logits);\n+    labels = flattenOuterDims(tf, labels);\n+    SoftmaxCrossEntropyWithLogits smax =\n+        tf.nn.softmaxCrossEntropyWithLogits(precise_logits, labels);\n+    Operand cost = smax.loss();\n+    Operand outputShape =\n+        tf.slice(\n+            tf.constant(inputShape.asArray()),\n+            tf.constant(new long[] {0}),\n+            tf.constant(new long[] {inputShape.numDimensions() - 1}));\n+    cost = tf.reshape(cost, outputShape);\n+    if (tf.scope().env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = tf.reshape(cost, tf.constant(newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = tf.dtypes.cast(cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  public static <T extends TType> Operand<T> map(\n+      Operand<T> input, Function<Operand<T>, Operand<T>> mapFunc) {\n+    return null;", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ4NTU2Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r471485566", "bodyText": "Will this be the equivalent of tf.map_fn() in Python?", "author": "JimClarke5", "createdAt": "2020-08-17T13:40:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2OTQ2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU2OTUxNA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r471569514", "bodyText": "FYI, I did this implementation. It may not be as efficient as tf.map_fn()\npublic static <T extends TType> Operand<T> map(Ops tf, Operand<T> input,\n            Function<Operand<T>, Operand<T>> mapFunc ) {\n        Split split = tf.split(tf.constant(0), input, 1L);\n        List<Operand<T>> result = new ArrayList<>();\n        split.iterator().forEachRemaining(e -> \n                   result.add( mapFunc.apply((Operand<T>)e)));\n        return (Operand<T>)tf.stack(result);\n    }", "author": "JimClarke5", "createdAt": "2020-08-17T15:45:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY2OTQ2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY3Mjg4Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r469672886", "bodyText": "At some point, we will need to document all these methods but I guess it is fine doing it later when the interface is more stable", "author": "karllessard", "createdAt": "2020-08-13T03:19:40Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/K.java", "diffHunk": "@@ -0,0 +1,713 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.tensorflow.DataType;\n+import org.tensorflow.EagerSession;\n+import org.tensorflow.ExecutionEnvironment;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Session;\n+import org.tensorflow.Tensor;\n+import org.tensorflow.keras.backend.tf.ConfusionMatrix;\n+import org.tensorflow.keras.backend.tf.NN;\n+import org.tensorflow.keras.backend.tf.Tuple;\n+import org.tensorflow.ndarray.NdArraySequence;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.ReduceSum;\n+import org.tensorflow.op.core.Squeeze;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.math.Mean;\n+import org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TBool;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.TFloat64;\n+import org.tensorflow.types.TInt32;\n+import org.tensorflow.types.TInt64;\n+import org.tensorflow.types.TUint8;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.types.family.TType;\n+\n+/** Keras backend methods */\n+public class K {\n+\n+  public static final double Epsilon = 1e-7;\n+  public static final float EpsilonF = 1e-7F;\n+\n+  public static final double epsilon() {\n+    return Epsilon;\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf) {\n+    return tf.constant(Epsilon);\n+  }\n+\n+  public static final Operand epsilonConstant(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(Epsilon), dtype);\n+  }\n+\n+  public static final Operand one(Ops tf) {\n+    return tf.constant(1);\n+  }\n+\n+  public static final Operand one(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(1), dtype);\n+  }\n+\n+  public static final Operand minusOne(Ops tf) {\n+    return tf.constant(-1);\n+  }\n+\n+  public static final Operand minusOne(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(-1), dtype);\n+  }\n+\n+  public static final Operand zero(Ops tf) {\n+    return tf.constant(0);\n+  }\n+\n+  public static final Operand zero(Ops tf, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(0), dtype);\n+  }\n+\n+  public static final Operand constant(Ops tf, double number, DataType dtype) {\n+    return tf.dtypes.cast(tf.constant(number), dtype);\n+  }\n+\n+  public static Operand clip(Ops tf, Operand x, double minValue, double maxValue) {\n+    assert x != null : \"Operand x must not be null\";\n+    DataType dtype = x.asOutput().dataType();\n+    if (maxValue < minValue) {\n+      double tmp = maxValue;\n+      maxValue = minValue;\n+      minValue = tmp;\n+    }\n+    Operand minValueConstant = tf.dtypes.cast(tf.constant(minValue), dtype);\n+    Operand maxValueConstant = tf.dtypes.cast(tf.constant(maxValue), dtype);\n+    return tf.clipByValue(x, minValueConstant, maxValueConstant);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x) {\n+    return mean(tf, x, null, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis) {\n+    return mean(tf, x, axis, false);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, boolean keepDims) {\n+    return mean(tf, x, null, keepDims);\n+  }\n+\n+  public static Operand mean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    if (x.asOutput().dataType() == TBool.DTYPE) {\n+      x = tf.dtypes.cast(x, TFloat32.DTYPE);\n+    }\n+    if (axis == null) {\n+      axis = allAxis(tf, x);\n+    }\n+    return tf.math.mean(x, axis, Mean.keepDims(keepDims));\n+  }\n+\n+  // alias for mean\n+  public static Operand reduceMean(Ops tf, Operand x, Operand axis, boolean keepDims) {\n+    return mean(tf, x, axis, keepDims);\n+  }\n+\n+  public static Operand maximum(Ops tf, Operand x, Operand y) {\n+    y = tf.dtypes.cast(y, x.asOutput().dataType());\n+    return tf.math.maximum(x, y);\n+  }\n+\n+  public static <T extends TType> Operand<T> sqrt(Ops tf, Operand<T> x) {\n+    DataType dType = x.asOutput().dataType();\n+    Operand<T> zero = tf.dtypes.cast(tf.constant(0), dType);\n+    Operand<T> inf = tf.dtypes.cast(tf.constant(Float.POSITIVE_INFINITY), dType);\n+    x = tf.clipByValue(x, zero, inf);\n+    return tf.math.sqrt(x);\n+  }\n+\n+  public static Shape merge(Shape a, Shape b) {\n+    assert a.numDimensions() == b.numDimensions()\n+        : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+    long[] array = new long[a.numDimensions()];\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      if (a.size(i) != Shape.UNKNOWN_SIZE) {\n+        if (b.size(i) != Shape.UNKNOWN_SIZE) {\n+          assert a.size(i) == b.size(i) : String.format(\"Shapes %s and %s are incompatible\", a, b);\n+        }\n+        array[i] = a.size(i);\n+      } else {\n+        array[i] = b.size(i);\n+      }\n+    }\n+    return Shape.of(array);\n+  }\n+\n+  // this is from nn in Python, I could not find it in the Java frameworks.\n+  public static Operand sigmoidCrossEntropyWithLogits(Ops tf, Operand labels, Operand logits) {\n+    Shape lablesShape = labels.asOutput().shape();\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape newShape = merge(lablesShape, logitsShape);\n+\n+    Operand zeros = tf.dtypes.cast(tf.zerosLike(logits), logits.asOutput().dataType());\n+    Operand cond = tf.math.greaterEqual(logits, zeros);\n+\n+    Operand relu_logits = tf.select(cond, logits, zeros);\n+    Operand neg_abs_logits = tf.select(cond, tf.math.neg(logits), logits);\n+    return tf.math.add(\n+        tf.math.sub(relu_logits, tf.math.mul(logits, labels)),\n+        tf.math.log1p(tf.math.exp(neg_abs_logits)));\n+  }\n+\n+  // TODO need to walk back identity until it hits something else\n+  // not sure how to get the input nodes for the Operand.\n+  private static Operand backtrackIdentity(Operand output) {\n+    // while(!output.op().type().equals(\"Identity\"))\n+    //    output = output.op().output(0);\n+    return output;\n+  }\n+\n+  public static Operand binary_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    if (fromLogits) {\n+      return sigmoidCrossEntropyWithLogits(tf, target, output);\n+    }\n+\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // output = backtrackIdentity(output); // TODO - this does not work, goes infinite loop\n+      if (output.op().type().equals(\"Sigmoid\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        return sigmoidCrossEntropyWithLogits(tf, target, output);\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+    output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand bce = tf.math.mul(target, tf.math.log(tf.math.add(output, epsilonConst)));\n+    bce =\n+        tf.math.add(\n+            bce,\n+            tf.math.mul(\n+                tf.math.sub(one, target),\n+                tf.math.log(tf.math.add(tf.math.sub(one, output), epsilonConst))));\n+    Operand result = tf.math.neg(bce);\n+    return result;\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits) {\n+    return categorical_crossentropy(tf, target, output, fromLogits, -1);\n+  }\n+\n+  public static Operand categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+\n+    if (fromLogits) {\n+      return softmax_cross_entropy_with_logits(tf, target, output);\n+    }\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        assert output.op().numOutputs() == 1;\n+        output = output.op().output(0);\n+        Operand op = softmax_cross_entropy_with_logits(tf, target, output);\n+        return op;\n+      }\n+    }\n+    DataType dtype = output.asOutput().dataType();\n+    Operand one = one(tf, dtype);\n+    Operand epsilonConst = K.epsilonConstant(tf, dtype);\n+    Operand oneMinusepsilonConst = tf.math.sub(one, epsilonConst);\n+    output =\n+        tf.math.div(\n+            output, tf.reduceSum(output, tf.constant(axis), ReduceSum.keepDims(Boolean.TRUE)));\n+    output = tf.clipByValue(output, epsilonConst, oneMinusepsilonConst);\n+\n+    // Compute cross entropy from probabilities.\n+    Operand cce =\n+        tf.reduceSum(\n+            tf.math.mul(target, tf.math.log(output)),\n+            tf.constant(axis),\n+            ReduceSum.keepDims(Boolean.FALSE));\n+    return tf.math.neg(cce);\n+  }\n+\n+  public static Operand flatten(Ops tf, Operand t) {\n+    Shape shape = Shape.of(1L);\n+    return tf.reshape(t, tf.constant(shape));\n+  }\n+\n+  public static Operand sparse_categorical_crossentropy(\n+      Ops tf, Operand target, Operand output, boolean fromLogits, int axis) {\n+    DataType dType = output.asOutput().dataType();\n+    if (!(output instanceof Variable) && (!tf.scope().env().isEager())) {\n+      // TODO output = backtrackIdentity(output); doesn't seem to work with Java version.\n+      if (output.op().type().equals(\"Softmax\")) {\n+        // assert output.op().numOutputs() == 1;\n+        // When softmax activation function is used for output operation, we\n+        // use logits from the softmax function directly to compute loss in order\n+        // to prevent collapsing zero when training.\n+        // TODO assert len(output.op.inputs) == 1\n+        // TODO output = output.op.inputs[0]\n+        fromLogits = true;\n+      }\n+    }\n+    if (!fromLogits) {\n+      Operand epsilonConst = epsilonConstant(tf, dType);\n+      Operand one = one(tf, dType);\n+      Operand oneMinusEpsilonConst = tf.math.sub(one, epsilonConst);\n+      output = tf.clipByValue(output, epsilonConst, oneMinusEpsilonConst);\n+      output = tf.math.log(output);\n+    }\n+    Shape outputShape = output.asOutput().shape();\n+    int outputRank = outputShape.numDimensions();\n+    axis %= outputRank;\n+    if (axis < 0) {\n+      axis += outputRank;\n+    }\n+    if (axis != outputRank - 1) {\n+      int[] axisNew = moveAxisToEnd(axis, outputRank);\n+      output = tf.linalg.transpose(output, tf.constant(axisNew));\n+    }\n+\n+    target = tf.dtypes.cast(target, TInt64.DTYPE);\n+    // TODO Try to adjust the shape so that rank of labels = rank of logits - 1.\n+    outputShape = output.asOutput().shape();\n+    Shape targetShape = target.asOutput().shape();\n+    int targetRank = targetShape.numDimensions();\n+\n+    boolean updateShape = targetRank != outputRank - 1;\n+    if (updateShape) { // TODO check to see if this is right\n+      target = tf.reshape(target, tf.constant(-1L)); // flatten\n+      output =\n+          tf.reshape(\n+              output,\n+              tf.constant(new long[] {-1L, outputShape.size(outputShape.numDimensions() - 1)}));\n+    }\n+\n+    // call nn.nn.sparse_softmax_cross_entropy_with_logits_v2\n+    Operand loss = NN.sparse_softmax_cross_entropy_with_logits(tf, target, output);\n+    if (updateShape && outputRank >= 3) {\n+      long[] dims = outputShape.asArray();\n+      long[] newDims = new long[dims.length - 1];\n+      System.arraycopy(dims, 0, newDims, 0, newDims.length);\n+      loss = tf.reshape(loss, tf.constant(newDims));\n+    }\n+    return loss;\n+  }\n+\n+  private static int[] allAxis(Operand op) {\n+    int rank = op.asOutput().shape().numDimensions();\n+    int[] ranks = new int[rank];\n+    for (int i = 0; i < rank; i++) {\n+      ranks[i] = i;\n+    }\n+    return ranks;\n+  }\n+\n+  public static Operand allAxis(Ops tf, Operand op) {\n+    int[] ranks = allAxis(op);\n+    return tf.constant(ranks);\n+  }\n+\n+  // TODO shouldn't these be in tensorflow itself under nn?\n+  private static <T extends TType, U extends TNumber> Operand moveDimToEnd(\n+      Ops tf, Operand tensor, int dim_index, Operand rank) {\n+    Operand one = one(tf, TInt32.DTYPE);\n+    List<Operand<T>> concatList =\n+        Arrays.asList(\n+            tf.range(tf.constant(dim_index), one, one),\n+            tf.range(tf.constant(dim_index + 1), rank, one));\n+    return tf.linalg.transpose(\n+        tensor,\n+        (Operand<U>) tf.concat((Iterable<Operand<T>>) concatList, (Operand<U>) tf.constant(0)));\n+  }\n+\n+  private static <T extends TType, U extends TNumber> Operand flattenOuterDims(\n+      Ops tf, Operand logits) {\n+    Operand zero = zero(tf, TInt64.DTYPE);\n+    Operand one = one(tf, TInt64.DTYPE);\n+    Operand minusOne = tf.constant(-1);\n+\n+    // Shape logitsShape = logits.asOutput().shape();\n+    // long lastDimSize = logitsShape.size(logitsShape.numDimensions()-1);\n+    // if(!tf.scope().env().isEager()) {\n+    Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return tf.reshape(logits, tf.constant(outputShape.asArray()));\n+      }\n+    }\n+    // }\n+\n+    Operand rank = tf.dtypes.cast(tf.rank(logits), TInt64.DTYPE);\n+    Operand rankMinusOne = tf.math.sub(rank, one);\n+\n+    Operand last_dim_size = tf.slice(tf.shape(logits), rankMinusOne, tf.constant(1));\n+    Operand concat =\n+        tf.concat(Arrays.asList(tf.constant(new int[] {-1}), last_dim_size), tf.constant(0));\n+    return tf.reshape(zero, concat);\n+  }\n+\n+  private static int[] moveAxisToEnd(int axis, int outputRank) {\n+    int[] axisNew = new int[outputRank];\n+    for (int i = 0; i < axis; i++) {\n+      axisNew[i] = i;\n+    }\n+    for (int i = axis + 1; i < outputRank; i++) {\n+      axisNew[i - 1] = i;\n+    }\n+    axisNew[outputRank - 1] = axis;\n+    return axisNew;\n+  }\n+\n+  // TODO, maybe part of Shape ??\n+  private static boolean shapeIsCompatible(Shape a, Shape b) {\n+    if (a.numDimensions() != b.numDimensions()) {\n+      return false;\n+    }\n+    for (int i = 0; i < a.numDimensions(); i++) {\n+      long aSize = a.size(i);\n+      long bSize = b.size(i);\n+      if (aSize != Shape.UNKNOWN_SIZE && bSize != Shape.UNKNOWN_SIZE && aSize != bSize) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  // TODO these are \"nn\" ops\n+  public static Operand softmax_cross_entropy_with_logits(Ops tf, Operand labels, Operand logits) {\n+    return softmax_cross_entropy_with_logits(tf, labels, logits, -1);\n+  }\n+\n+  public static Operand softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand minusOne = tf.constant(-1);\n+    Operand precise_logits = logits;\n+    Operand one = tf.constant(1L);\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n+    }\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = tf.dtypes.cast(labels, dtype);\n+    Operand inputRank = tf.dtypes.cast(tf.rank(precise_logits), TInt64.DTYPE);\n+    Operand inputRankMinusOne = tf.dtypes.cast(tf.math.sub(inputRank, one), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(tf, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(tf, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(tf, precise_logits);\n+    labels = flattenOuterDims(tf, labels);\n+    SoftmaxCrossEntropyWithLogits smax =\n+        tf.nn.softmaxCrossEntropyWithLogits(precise_logits, labels);\n+    Operand cost = smax.loss();\n+    Operand outputShape =\n+        tf.slice(\n+            tf.constant(inputShape.asArray()),\n+            tf.constant(new long[] {0}),\n+            tf.constant(new long[] {inputShape.numDimensions() - 1}));\n+    cost = tf.reshape(cost, outputShape);\n+    if (tf.scope().env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = tf.reshape(cost, tf.constant(newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = tf.dtypes.cast(cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  public static <T extends TType> Operand<T> map(\n+      Operand<T> input, Function<Operand<T>, Operand<T>> mapFunc) {\n+    return null;\n+  }\n+\n+  public static long[] concatenate(long first, long... remaining) {\n+    long[] dims = new long[remaining.length + 1];\n+    System.arraycopy(remaining, 0, dims, 1, remaining.length);\n+    dims[0] = first;\n+    return dims;\n+  }\n+\n+  private static Map<ExecutionEnvironment, Map<String, Integer>> uidMap = new HashMap<>();\n+\n+  /**\n+   * Associates a string prefix with an integer counter in a TensorFlow graph.\n+   *\n+   * <p>Example:\n+   *\n+   * <pre>\n+   * get_uid('dense')\n+   * 1\n+   * get_uid('dense')\n+   * 2\n+   * </pre>\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @param prefix String prefix to index.\n+   * @return Unique integer ID.\n+   */\n+  public static int getUid(Ops tf, String prefix) {\n+    ExecutionEnvironment env = tf.scope().env();\n+    Map<String, Integer> uids = uidMap.get(env);\n+    if (uids == null) {\n+      uids = new HashMap<>();\n+      uidMap.put(env, uids);\n+    }\n+    Integer id = uids.get(prefix);\n+    if (id == null) {\n+      id = 0;\n+    } else {\n+      id++;\n+    }\n+\n+    uids.put(prefix, id);\n+    return id;\n+  }\n+\n+  /**\n+   * returns the larger DataType between the two.\n+   *\n+   * @param a the first DataType to compare\n+   * @param b the second DataType to compare\n+   * @return the wider DataType\n+   */\n+  public DataType wider(DataType a, DataType b) {\n+    return a.byteSize() < b.byteSize() ? b : a;\n+  }\n+\n+  /**\n+   * returns the smaller DataType between the two.\n+   *\n+   * @param a the first DataType to compare\n+   * @param b the second DataType to compare\n+   * @return the smaller DataType\n+   */\n+  public DataType narrower(DataType a, DataType b) {\n+    return a.byteSize() > b.byteSize() ? b : a;\n+  }\n+\n+  public <T extends TNumber> NdArraySequence getTensorValue(Ops tf, Operand<T> operand) {\n+    DataType dtype = operand.asOutput().dataType();\n+    if (tf.scope().env().isGraph()) {\n+      try (Session session = new Session((Graph) tf.scope().env())) {\n+        if (dtype.equals(TInt32.DTYPE)) {\n+          try (Tensor<TInt32> result =\n+              session.runner().fetch(operand).run().get(0).expect(TInt32.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else if (dtype.equals(TInt64.DTYPE)) {\n+          try (Tensor<TInt64> result =\n+              session.runner().fetch(operand).run().get(0).expect(TInt64.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else if (dtype.equals(TUint8.DTYPE)) {\n+          try (Tensor<TUint8> result =\n+              session.runner().fetch(operand).run().get(0).expect(TUint8.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else if (dtype.equals(TBfloat16.DTYPE)) {\n+          try (Tensor<TBfloat16> result =\n+              session.runner().fetch(operand).run().get(0).expect(TBfloat16.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else if (dtype.equals(TFloat16.DTYPE)) {\n+          try (Tensor<TFloat16> result =\n+              session.runner().fetch(operand).run().get(0).expect(TFloat16.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else if (dtype.equals(TFloat32.DTYPE)) {\n+          try (Tensor<TFloat32> result =\n+              session.runner().fetch(operand).run().get(0).expect(TFloat32.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else if (dtype.equals(TFloat64.DTYPE)) {\n+          try (Tensor<TFloat64> result =\n+              session.runner().fetch(operand).run().get(0).expect(TFloat64.DTYPE)) {\n+            return result.data().scalars();\n+          }\n+        } else {\n+          return null;\n+        }\n+      }\n+    } else {\n+      try (EagerSession session = EagerSession.create()) {\n+        if (dtype.equals(TInt32.DTYPE)) {\n+          return ((Operand<TInt32>) operand).data().scalars();\n+        } else if (dtype.equals(TInt64.DTYPE)) {\n+          return ((Operand<TInt64>) operand).data().scalars();\n+        } else if (dtype.equals(TUint8.DTYPE)) {\n+          return ((Operand<TUint8>) operand).data().scalars();\n+        } else if (dtype.equals(TBfloat16.DTYPE)) {\n+          return ((Operand<TBfloat16>) operand).data().scalars();\n+        } else if (dtype.equals(TFloat16.DTYPE)) {\n+          return ((Operand<TFloat16>) operand).data().scalars();\n+        } else if (dtype.equals(TFloat32.DTYPE)) {\n+          return ((Operand<TFloat32>) operand).data().scalars();\n+        } else if (dtype.equals(TFloat64.DTYPE)) {\n+          return ((Operand<TFloat64>) operand).data().scalars();\n+        } else {\n+          return null;\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed. 1. Squeezes last dim of `y_pred` or `y_true` if\n+   * their rank differs by 1 (using `confusion_matrix.remove_squeezable_dimensions`). 2. Squeezes or\n+   * expands last dim of `sample_weight` if its rank differs by 1 from the new rank of `y_pred`. If\n+   * `sample_weight` is scalar, it is kept scalar.\n+   *\n+   * @param tf the TensorVlow Ops\n+   * @param yPred Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param yTrue Optional label `Tensor` whose dimensions match `y_pred`.\n+   * @return Tuple of `y_pred`, `y_true` and `sample_weight`. Each of them possibly has the last\n+   *     dimension squeezed, `sample_weight` could be extended by one dimension. If `sample_weight`\n+   *     is null, (y_pred, y_true) is returned.\n+   */\n+  public static Tuple squeezeOrExpandDimensions(Ops tf, Operand yTrue, Operand yPred) {\n+    return squeezeOrExpandDimensions(tf, yTrue, yPred, null);\n+  }\n+\n+  /**\n+   * Squeeze or expand last dimension if needed. 1. Squeezes last dim of `y_pred` or `y_true` if\n+   * their rank differs by 1 (using `confusion_matrix.remove_squeezable_dimensions`). 2. Squeezes or\n+   * expands last dim of `sample_weight` if its rank differs by 1 from the new rank of `y_pred`. If\n+   * `sample_weight` is scalar, it is kept scalar.\n+   *\n+   * @param tf the TensorVlow Ops\n+   * @param yPred Predicted values, a `Tensor` of arbitrary dimensions.\n+   * @param yTrue Optional label `Tensor` whose dimensions match `y_pred`.\n+   * @param sampleWeight Optional weight scalar or `Tensor` whose dimensions match `y_pred`.\n+   * @return Tuple of `y_pred`, `y_true` and `sample_weight`. Each of them possibly has the last\n+   *     dimension squeezed, `sample_weight` could be extended by one dimension. If `sample_weight`\n+   *     is null, (y_pred, y_true) is returned.\n+   */\n+  public static Tuple squeezeOrExpandDimensions(\n+      Ops tf, Operand yTrue, Operand yPred, Operand sampleWeight) {\n+    Tuple tuple = new Tuple(yTrue, yPred);\n+    Shape ypredShape = yPred.asOutput().shape();\n+    long ypredRank = ypredShape.numDimensions();\n+\n+    if (yTrue != null) {\n+      Shape ytrueShape = yTrue.asOutput().shape();\n+      long ytrueRank = ytrueShape.numDimensions();\n+      if (ytrueRank != Shape.UNKNOWN_SIZE && ypredRank != Shape.UNKNOWN_SIZE) {\n+        // Use static rank for `y_true` and `y_pred`.\n+        if (ypredRank - ytrueRank != 1 || ypredShape.size(-1) == 1) {\n+          // y_true, y_pred = confusion_matrix.remove_squeezable_dimensions(y_true, y_pred)\n+          tuple = ConfusionMatrix.removeSqueezableDimensions(tf, yTrue, yPred);\n+        }\n+      } else { // use dynamic rank\n+        tuple = ConfusionMatrix.removeSqueezableDimensions(tf, yTrue, yPred);\n+      }\n+    }\n+    if (sampleWeight == null) {\n+      return tuple;\n+    }\n+    Shape weightsShape = sampleWeight.asOutput().shape();\n+    long weightsRank = weightsShape.numDimensions();\n+    if (weightsRank == 0) { // scalar\n+      return new Tuple(yTrue, yPred, sampleWeight);\n+    }\n+\n+    if (ypredRank != Shape.UNKNOWN_SIZE && weightsRank != Shape.UNKNOWN_SIZE) {\n+\n+      if (weightsRank - ypredRank == 1) {\n+        sampleWeight = tf.squeeze(sampleWeight);\n+      } else if (ypredRank - weightsRank == 1) {\n+        sampleWeight = tf.expandDims(sampleWeight, tf.constant(-1L));\n+      }\n+      return new Tuple(yTrue, yPred, sampleWeight);\n+    }\n+    // Use dynamic rank.\n+    Operand weightsRankTensor = tf.rank(sampleWeight);\n+    Operand rankDiff = tf.math.sub(weightsRankTensor, tf.rank(yPred));\n+    sampleWeight =\n+        tf.select(\n+            tf.math.equal(weightsRankTensor, tf.constant(0)),\n+            sampleWeight,\n+            maybeAdjustWeights(tf, sampleWeight, rankDiff));\n+    return new Tuple(yTrue, yPred, sampleWeight);\n+  }\n+\n+  private static Operand maybeAdjustWeights(Ops tf, Operand sampleWeight, Operand rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(1)),\n+        tf.squeeze(sampleWeight, Squeeze.axis(Arrays.asList(-1L))),\n+        maybeExpandWeights(tf, sampleWeight, rankDiff));\n+  }\n+\n+  private static Operand maybeExpandWeights(Ops tf, Operand sampleWeight, Operand rankDiff) {\n+    return tf.select(\n+        tf.math.equal(rankDiff, tf.constant(-1)),\n+        tf.expandDims(sampleWeight, tf.constant(-1)),\n+        sampleWeight);\n+  }\n+}", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE1OTA2Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r470159066", "bodyText": "I have added JavaDoc to my copy, that I haven't pushed yet.\nAs far as NdArraySequence, could we have a common method on each numeric type that returns java.lang.Number?", "author": "JimClarke5", "createdAt": "2020-08-13T18:25:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY3Mjg4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDM1NzYyOQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r470357629", "bodyText": "I think that would improve the experience at the user level (while passing the burden to the author :) ). Most of the NdArray library has been written with that spirit.\nWith type erasure though, you will need to provide a distinct name for each variant, i.e. something like:\npublic NdArraySequence<FloatNdArray> getTensorFloats(Ops tf, Operand<TFloat32> operand)", "author": "karllessard", "createdAt": "2020-08-14T01:11:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY3Mjg4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5NTg4OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473395889", "bodyText": "I don't think we need this class, as the user can very easily add its control dependencies using directly tf.withControlDependencies(...), unless you have another use case in mind?", "author": "karllessard", "createdAt": "2020-08-19T22:39:11Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/ControlDependencies.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import org.tensorflow.keras.backend.*;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.function.Function;\n+import org.tensorflow.Operand;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.types.family.TType;\n+\n+/** Container for ControlDepencies, so that the primary Operand is remembered. */\n+public class ControlDependencies {", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzQ0NTE5NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473445195", "bodyText": "org.tensorflow.keras.backend.K is a part of Keras, however, the sub packages that I used under the  backend package were parking places for classes that probably should be in other tensorflow modules.", "author": "JimClarke5", "createdAt": "2020-08-19T23:48:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5NTg4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzQ0NzE2Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473447163", "bodyText": "ControlDependencies is just a utility class to create control dependencies. It was manly put in to avoid rewriting all that code all the time.", "author": "JimClarke5", "createdAt": "2020-08-19T23:51:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5NTg4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzk2MDM4Mg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473960382", "bodyText": "ConfusionMatrix actually belongs in MathOps. At least that is where it is in Python TF.", "author": "JimClarke5", "createdAt": "2020-08-20T13:16:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5NTg4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDExNTUzOQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474115539", "bodyText": "If ControlDependencies is a utility for the keras package we could make it package private? We can always make it public later if we want to expose the functionality.", "author": "Craigacp", "createdAt": "2020-08-20T16:31:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5NTg4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE2MjAzMg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r474162032", "bodyText": "I removed ControlDependencies from this PR. I may revisit it when new PRs are created for other Keras features.", "author": "JimClarke5", "createdAt": "2020-08-20T17:41:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5NTg4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5OTMzMQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473399331", "bodyText": "This might get out of date if we add more float types in the future. We should add a type family called TFloating that is used to tagged all our floating point tensor types (that is what I did in https://github.com/karllessard/tensorflow-java/tree/tensor-as-ndarrays-3)", "author": "karllessard", "createdAt": "2020-08-19T22:43:57Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/backend/tf/NN.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.backend.tf;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.nn.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.TBfloat16;\n+import org.tensorflow.types.TFloat16;\n+import org.tensorflow.types.TFloat32;\n+\n+/** NN Operations */\n+public class NN {\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param tf\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  public static Operand sparse_softmax_cross_entropy_with_logits(\n+      Ops tf, Operand labels, Operand logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    tf = tf.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;", "originalCommit": "ef0ce67a2e79628380729cf16fb0cd4d99cbe6a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzkzMTc1NA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r473931754", "bodyText": "This specific logic is primarily checking to see if the float type is smaller than TFloat32 and if so, flag it for casting up later. I don't think TFloating would capture this specific logic.  I have modified my version of DataType to add isFloating(), isInteger(), isNumeric(), isBoolean(), and isString(). I did this based on Craig's suggestion earlier.\nalso sparseCrossEntropyWithLogits is defined in nn in Python TF \"nn.softmax_cross_entropy_with_logits\". It has nothing to do with SparseTensor if that is what you thought.", "author": "JimClarke5", "createdAt": "2020-08-20T12:27:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM5OTMzMQ=="}], "type": "inlineReview"}, {"oid": "9c113a7e6dfcc98c0e57bede8dcb46de56125320", "url": "https://github.com/tensorflow/java/commit/9c113a7e6dfcc98c0e57bede8dcb46de56125320", "message": "Added static final NAME to replace hardcoded String in the create method. This allows the NAME to be used elsewhere instead of hardcoding the string.", "committedDate": "2020-08-20T12:12:27Z", "type": "commit"}, {"oid": "824d4872257a5a614a53c7dff579748b819c800e", "url": "https://github.com/tensorflow/java/commit/824d4872257a5a614a53c7dff579748b819c800e", "message": "Changed of method to use the DataType NAME attribute rather than hardcoding the string.\nadded methods isFloating(), isInteger(), isNUmeric(), isBoolean() and isString()", "committedDate": "2020-08-20T12:14:26Z", "type": "commit"}, {"oid": "07a83a5aef5a25f385aa762e3bb929e22c8052e3", "url": "https://github.com/tensorflow/java/commit/07a83a5aef5a25f385aa762e3bb929e22c8052e3", "message": "Added method WriteFieldWithInitializer to output a \"final static String OP_NAME\" to each generated operation.", "committedDate": "2020-08-20T12:17:40Z", "type": "commit"}, {"oid": "3d26831bab64cc4fdd2340403cafa229f0fb7099", "url": "https://github.com/tensorflow/java/commit/3d26831bab64cc4fdd2340403cafa229f0fb7099", "message": "Added tf.nn.softmaxCrossEntropyWitLogits() and tf.nn.raw.softmaxCrossEntropyWitLogits()\nAdded tf.nn.sparesSoftmaxCrossEntropyWithLogits() and\ntf.nn.raw.sparesSoftmaxCrossEntropyWithLogits()\n\nAdded tf.nn.sigmoidCrossEntropyWithLogits()", "committedDate": "2020-08-20T13:52:13Z", "type": "commit"}, {"oid": "11cda5fde99a6bec21fb04ed1465934ec1889485", "url": "https://github.com/tensorflow/java/commit/11cda5fde99a6bec21fb04ed1465934ec1889485", "message": "Moved SoftmaxCrossEntropyWithLogits and  SparseSoftmaxCrossEntropyWithLogits to org.tensorflow.op.nn.raw", "committedDate": "2020-08-20T13:58:05Z", "type": "commit"}, {"oid": "9c7dfaa92815bf85cbf97febcbb61e1aaf707142", "url": "https://github.com/tensorflow/java/commit/9c7dfaa92815bf85cbf97febcbb61e1aaf707142", "message": "Generated classes now have public static final String OP_NAME = \"XXXXXXXX\";", "committedDate": "2020-08-20T14:00:25Z", "type": "commit"}, {"oid": "84f49db3fb6c085befabfcb8356ab77589facf5d", "url": "https://github.com/tensorflow/java/commit/84f49db3fb6c085befabfcb8356ab77589facf5d", "message": "Generated classes now have public static final String OP_NAME = \"XXXXXXXX\";", "committedDate": "2020-08-20T14:03:38Z", "type": "commit"}, {"oid": "208b84a1a09d38ab9821d66572c1328326685c5a", "url": "https://github.com/tensorflow/java/commit/208b84a1a09d38ab9821d66572c1328326685c5a", "message": "fix dependencies for other Tensorflow Java modules", "committedDate": "2020-08-20T14:42:22Z", "type": "commit"}, {"oid": "39131619f3a4c970f5f62dacdbaa93ac3f9bf996", "url": "https://github.com/tensorflow/java/commit/39131619f3a4c970f5f62dacdbaa93ac3f9bf996", "message": "formatting fix", "committedDate": "2020-08-20T14:46:20Z", "type": "commit"}, {"oid": "b5a7c0f9f54acf1fb0f57003027aa0843ae67c82", "url": "https://github.com/tensorflow/java/commit/b5a7c0f9f54acf1fb0f57003027aa0843ae67c82", "message": "Fix ctors with name to properly pass the name to the the super ctor.", "committedDate": "2020-08-20T14:48:23Z", "type": "commit"}, {"oid": "fcba0a525d4712fca5210db71c141bc1bce87307", "url": "https://github.com/tensorflow/java/commit/fcba0a525d4712fca5210db71c141bc1bce87307", "message": "change asserts to IllegalArgumentException\nfix javadoc, fix casts", "committedDate": "2020-08-20T17:04:27Z", "type": "commit"}, {"oid": "960cfc3ca4e010fca3bd549df6d16a17fb0b22bc", "url": "https://github.com/tensorflow/java/commit/960cfc3ca4e010fca3bd549df6d16a17fb0b22bc", "message": "change asserts to IllegalArgumentException", "committedDate": "2020-08-20T17:05:22Z", "type": "commit"}, {"oid": "d37298a6b9dcb206cdff985b96e340f7f93a075c", "url": "https://github.com/tensorflow/java/commit/d37298a6b9dcb206cdff985b96e340f7f93a075c", "message": "Moved back to tests", "committedDate": "2020-08-20T17:06:58Z", "type": "commit"}, {"oid": "c68812cc9c8815b0ea186f273748078c4907fbaa", "url": "https://github.com/tensorflow/java/commit/c68812cc9c8815b0ea186f273748078c4907fbaa", "message": "Moved SoftmaxCrossEntropyWithLogits.java and SparseSoftmaxCrossEntropyWithLogits.java to nn.raw,\nadded new versions of these to NnOps", "committedDate": "2020-08-20T17:08:44Z", "type": "commit"}, {"oid": "6b8eb26edafc54a70d6c5a1bd9e21cacf3ac04e7", "url": "https://github.com/tensorflow/java/commit/6b8eb26edafc54a70d6c5a1bd9e21cacf3ac04e7", "message": "Deleted files that are not necessary yet", "committedDate": "2020-08-20T17:50:01Z", "type": "commit"}, {"oid": "6515c248ddd8f6911f267bee0972d44ca20f0038", "url": "https://github.com/tensorflow/java/commit/6515c248ddd8f6911f267bee0972d44ca20f0038", "message": "Added nn.raw group for softmaxCrossEntropyWithLogits() and sparseSoftmaxCrossEntropyWithLogits()", "committedDate": "2020-08-20T17:51:04Z", "type": "commit"}, {"oid": "76d0fe553559176a33040b4bdb8e07d8e033b08e", "url": "https://github.com/tensorflow/java/commit/76d0fe553559176a33040b4bdb8e07d8e033b08e", "message": "Added nn.raw group for softmaxCrossEntropyWithLogits() and sparseSoftmaxCrossEntropyWithLogits()", "committedDate": "2020-08-20T18:30:16Z", "type": "commit"}, {"oid": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "url": "https://github.com/tensorflow/java/commit/d2201df3a78e82c79b142fc7d7c3a461afa63444", "message": "Merge branch 'master' into master", "committedDate": "2020-08-20T19:43:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDYzODI3MA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480638270", "bodyText": "Can we rename this class to something else?\nActually it should be located under org.tensorflow.op.nn. The packages in src/main and src/gen overlaps each other \"by design\", like it is done with org.tensorflow.op.core. So the idea is to put the wrapper directly into the right directory, org.tensorflow.op.nn in this case.\nThe class can either be split in 3 classes (one per endpoint, i.e. SoftmaxCrossEntropyWithLogits, SparseSoftmaxCrossEntropyWithLogits or SigmoidCrossEntropyWithLogits) or remain in a single class (CrossEntropy maybe? or just Helpers to reflect what the one already present in the core package?)", "author": "karllessard", "createdAt": "2020-09-01T02:42:45Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTA4NzU0NA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r481087544", "bodyText": "I vote for 3 individual classes under org.tensorflow.op.nn.", "author": "JimClarke5", "createdAt": "2020-09-01T12:06:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDYzODI3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODY4Mjg5MQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r488682891", "bodyText": "The 3 files have been checked-in", "author": "JimClarke5", "createdAt": "2020-09-15T13:49:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDYzODI3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY0NTQ2Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480645463", "bodyText": "Since this documentation is not generated, let's transcript all the Python code in it to Java, and replace markdown by Javadoc tags.", "author": "karllessard", "createdAt": "2020-09-01T02:47:58Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI2NTU2Mg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483265562", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T21:32:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY0NTQ2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MDI2Nw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480650267", "bodyText": "Please use camelCase for variable names. The line wrapping is a bit weird here as well.", "author": "karllessard", "createdAt": "2020-09-01T02:51:31Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI2NTYzMg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483265632", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T21:32:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MDI2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MzM5Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480653396", "bodyText": "If generic parameter cannot be carried, then we should carried the wildcard so IDEs or lint checks will not complain about it: DataType<?> dtype = ...", "author": "karllessard", "createdAt": "2020-09-01T02:53:51Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzAxMzc1Mg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483013752", "bodyText": "I have tried all kinds of combinations, <?>, <? extends TType> or <? extends TNumber>, etc.,  but the main hangup is when I call other methods that have generics defined as in <T extends TType>, it won't compile. The only way I can get it to compile is to remove the generic.\nDo you have any other suggestions?", "author": "JimClarke5", "createdAt": "2020-09-03T14:18:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MzM5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzEyNzM0MA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483127340", "bodyText": "Interesting. You are right that <?> is a bad pick, we need at least to be bound to TType, so <? extends TType>. Now you are saying that even with this generic, you are not able to call a method that accepts only <T extends TType>? Do you have an example?", "author": "karllessard", "createdAt": "2020-09-03T17:01:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MzM5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzE3Mjc5OA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483172798", "bodyText": "Here is a code fragment that highlights the issue.\npublic static <T extends TNumber> Operand<T> foo(Ops tf, Operand<T> logits) {\n        Operand<? extends TNumber> preciseLogits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n        return flattenOuterDims(tf.scope(), preciseLogits);\n}\n    \nprivate static <T extends TNumber> Operand<T> flattenOuterDims(Scope scope, Operand<T> logits) {\n    return null;\n}\n\nI get compile error on the line return flattenOuterDims:\nGenericExample.java:[35,32] incompatible types: inferred type does not conform to equality constraint(s)\n inferred: T\n equality constraints(s): T,capture#1 of ? extends org.tensorflow.types.family.TNumber\n\nBTW: I have also run into issues with consistency of <T> and <U>. Usually, <T extends TType> and <U extends TNumber>, but some times I run into methods where <T extends TNumber>. This causes conflicts similar to what I see with the above code. Seems that the compiler is looking for consistency in the generic definitions.\nIf I change the above  to:\nOperand<T> preciseLogits = tf.dtypes.cast(logits, TFloat32.DTYPE);\nI get an error complaining mismatched generics on  and  on the line doing the cast.\nGenericExample.java:[34,50] incompatible types: inferred type does not conform to equality constraint(s)\n    inferred: T\n    equality constraints(s): T,org.tensorflow.types.TFloat32", "author": "JimClarke5", "createdAt": "2020-09-03T18:25:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MzM5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzk5MTEyNA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483991124", "bodyText": "Ok for this example specifically, I can see that it won't work because the type of the returned value in foo is T, while the one returned by flattenOuterDims is <? extends Number> (inferred by preciseLogits). Both T are unrelated.\nIt seems that in your code, you are always expecting to return a TFloat32 tensor as the result of foo with the explicit cast. Therefore, you should enforce this in the signature of the method as well, i.e.\npublic static <T extends TNumber> Operand<TFloat32> foo(Ops tf, Operand<T> logits) {\n        Operand<TFloat32> preciseLogits = tf.dtypes.cast(logits, TFloat32.DTYPE);\n        return flattenOuterDims(tf.scope(), preciseLogits);\n}\n    \nprivate static <T extends TNumber> Operand<T> flattenOuterDims(Scope scope, Operand<T> logits) {\n    return null;\n}\nDo you have other examples to share that we can look at? I can imagine though that we could lose track of the TNumber boundary sometimes for some operands which is then causing problem when calling a method only accepting them. Having a concrete example showing can help us figure out what would be the best approach to take.", "author": "karllessard", "createdAt": "2020-09-05T21:23:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MzM5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQ5MjUzNw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484492537", "bodyText": "This was a quick example, I could make this change with these methods which are under my control, but I will still run into the underlying issue when I call other apis in tensorflow.core-api. The real code in org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits only does the cast to TFloat32 when it is 16  bit Float type (TBFLoat16 or TFloat16). No cast is required, If is is already TFLoat32, TFloat64, TInt32, or TInt64.\nIn org.tensorflow.op.nn.SoftmaxCrossEntropyWithLogits, if I change to Operand<? extends TNumber> preciseLogits, then I get an error when calling:\norg.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits smax =\n        org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits.create(\n            scope, preciseLogits, castLabels);\n\n'create(org.tensorflow.op.Scope, org.tensorflow.Operand<T>, org.tensorflow.Operand<T>)' in 'org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits' cannot be applied to '(org.tensorflow.op.Scope, org.tensorflow.Operand<capture<? extends org.tensorflow.types.family.TNumber>>, org.tensorflow.Operand<capture<? extends org.tensorflow.types.family.TNumber>>)'", "author": "JimClarke5", "createdAt": "2020-09-07T15:26:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MzM5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUwOTMwMg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484509302", "bodyText": "In this case, the problem is that the signature of raw.SoftmaxCrossEntropyWithLogits enforces that both preciseLogits and castLabels should be of the same type. When I look at your current implementation, this information gets lost so the compiler fails (and TF runtime will also fail if this condition is not met).\nI see a few things that could help, if you want to give it a try:\n\nU must be bound to TNumber as well, since you assert it later with an explicit cast\nYou can get rid of the casting complexity for float types with a recursively call the same method when required, so the method could recapture the effective type:\n\n    if (logits.asOutput().dataType() == TFloat16.DTYPE || logits.asOutput().dataType() == TBfloat16.DTYPE) {\n        Operand<TFloat32> cost = softmaxCrossEntropyWithLogits(scope, labels, Cast.create(scope, logits, TFloat32.DTYPE));\n        return Cast.create(scope, cost, logits.asOutput().dataType());\n    }\n\nMake sure all parameterized variables keeps track of their type (DataType, Operand, ...), avoiding wildcards as much as possible", "author": "karllessard", "createdAt": "2020-09-07T16:11:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MzM5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyOTc5MQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484529791", "bodyText": "Based on your suggestion, I did the following, using recursion, which does compile. I still need to validate that I don't get a runtime error.\npublic static <T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n      Scope scope, Operand<T> labels, Operand<T> logits, int axis) {\n\n...\nif (convertToFloat32) {\n      Operand<TFloat32> result =  softmaxCrossEntropyWithLogits(scope,\n              Cast.create(scope, labels, TFloat32.DTYPE),\n              Cast.create(scope, logits, TFloat32.DTYPE),\n              axis);\n      return Cast.create(scope, result, logits.asOutput().dataType());\n} else if(!logits.asOutput().dataType().equals(labels.asOutput().dataType())) {\n      return softmaxCrossEntropyWithLogits(scope,\n              Cast.create(scope, labels, logits.asOutput().dataType()),\n              logits,\n              axis);\n}\n....", "author": "JimClarke5", "createdAt": "2020-09-07T17:27:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MzM5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUzMzY3Mg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484533672", "bodyText": "I did have to cast labels when calling  org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits.create\nAt this point, both args will be the same datatype.\norg.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits<T> smax =\n        org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits.create(\n            scope, logits, (Operand<T>)labels);", "author": "JimClarke5", "createdAt": "2020-09-07T17:47:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MzM5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUzNjA5Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484536096", "bodyText": "I had to change the signature to:\npublic static <T extends TNumber, U extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n      Scope scope, Operand<U> labels, Operand<T> logits, int axis) {\n\nThis is because labels and logits may be different types initially.", "author": "JimClarke5", "createdAt": "2020-09-07T17:59:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1MzM5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1NTc5Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480655793", "bodyText": "\".... if axis is not the last dimension\"?", "author": "karllessard", "createdAt": "2020-09-01T02:55:39Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk5NDgxNA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r482994814", "bodyText": "Actually, it is dim_index, which I changed to dimIndex.\n// Move the dim to the end if dimIndex is not the last dimension.", "author": "JimClarke5", "createdAt": "2020-09-03T13:53:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1NTc5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1ODEyMQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480658121", "bodyText": "Idem: Operand<?>", "author": "karllessard", "createdAt": "2020-09-01T02:57:23Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk5NTE0OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r482995149", "bodyText": "It's the same issue with generics as mentioned earlier in this method.", "author": "JimClarke5", "createdAt": "2020-09-03T13:53:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1ODEyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2NDQ3Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480664473", "bodyText": "Shortcut: you can Constant.tensorOf(scope, inputShape) directly for the first, and Constant.arrayOf(scope, num) for the others", "author": "karllessard", "createdAt": "2020-09-01T03:02:04Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzAzNDY0Mg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483034642", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T14:45:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2NDQ3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2NzY3OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480667679", "bodyText": "no need to convert newArray to a Shape if you pass it back as an array in Constant.vectorOf.", "author": "karllessard", "createdAt": "2020-09-01T03:04:26Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzAzNDc5MQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483034791", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T14:45:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2NzY3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2OTUyNA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480669524", "bodyText": "Maybe you should create a subscope like you do in sparseSoftmaxCrossEntropyWithLogits?", "author": "karllessard", "createdAt": "2020-09-01T03:05:46Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzAzNTA4NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483035085", "bodyText": "OK, also did it in SigmoidCrossEntropyWithLogits", "author": "JimClarke5", "createdAt": "2020-09-03T14:45:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2OTUyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3NzQyMA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480677420", "bodyText": "Typpppo :)", "author": "karllessard", "createdAt": "2020-09-01T03:11:43Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzAzNTkyNg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483035926", "bodyText": "OK, also changed it to logitsShape.size(-1)", "author": "JimClarke5", "createdAt": "2020-09-03T14:46:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3NzQyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3Nzk4OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480677989", "bodyText": "camelCase for variable names", "author": "karllessard", "createdAt": "2020-09-01T03:12:08Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzAzNjA3OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483036079", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T14:46:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3Nzk4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3ODk4MQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480678981", "bodyText": "...or Constant.arrayOf(scope, -1L, numClasses)", "author": "karllessard", "createdAt": "2020-09-01T03:12:52Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzAzNjE3NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483036175", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T14:47:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3ODk4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4MDgxNg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480680816", "bodyText": "I think you are missing TBfloat16 here?", "author": "karllessard", "createdAt": "2020-09-01T03:14:10Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzAzODgwNw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483038807", "bodyText": "Python source only refers to dtypes.float16.  It does not show anything with dtypes.bfloat16.\ncost, _ = gen_nn_ops.sparse_softmax_cross_entropy_with_logits(precise_logits, labels, name=name)\nif logits.dtype == dtypes.float16:\n        return math_ops.cast(cost, dtypes.float16)\nelse:\n        return cost", "author": "JimClarke5", "createdAt": "2020-09-03T14:50:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4MDgxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4Mjk2Nw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480682967", "bodyText": "Missing brackets for if block", "author": "karllessard", "createdAt": "2020-09-01T03:15:46Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzEwMjE1Mg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483102152", "bodyText": "OK, I actually changed it to:\nif (!isCompatible(labels.asOutput().shape(), logits.asOutput().shape())) {\n      throw new IllegalArgumentException(\n          String.format(\n              \"logits and labels must have the same shape (%s vs %s)\",\n              labels.asOutput().shape().toString(), logits.asOutput().shape()));\n    }\n\nand added method isCompatible.\n shapes are compatible if they have the same number of dimensions, and\n if the corresponding dimensions are equal, or at least one of the corresponding dimensions is unknown\n\nShould isCompatible be a part of Shape.java?", "author": "JimClarke5", "createdAt": "2020-09-03T16:20:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4Mjk2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4MzY5OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480683699", "bodyText": "camelCase variable names", "author": "karllessard", "createdAt": "2020-09-01T03:16:19Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"logits and labels must have the same shape (%s vs %s)\",\n+              labels.asOutput().shape().toString(), logits.asOutput().shape()));\n+    Operand<T> zeros =\n+        Cast.create(scope, ZerosLike.create(scope, logits), logits.asOutput().dataType());\n+    Operand<TBool> cond = GreaterEqual.create(scope, logits, zeros);\n+\n+    Operand<T> relu_logits = Select.create(scope, cond, logits, zeros);\n+    Operand<T> neg_abs_logits = Select.create(scope, cond, Neg.create(scope, logits), logits);", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzEwMzEyNA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483103124", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T16:22:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4MzY5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NDM4NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480684385", "bodyText": "you shouldn't need the full canonical name for Shape in this method as you've already imported it", "author": "karllessard", "createdAt": "2020-09-01T03:16:51Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"logits and labels must have the same shape (%s vs %s)\",\n+              labels.asOutput().shape().toString(), logits.asOutput().shape()));\n+    Operand<T> zeros =\n+        Cast.create(scope, ZerosLike.create(scope, logits), logits.asOutput().dataType());\n+    Operand<TBool> cond = GreaterEqual.create(scope, logits, zeros);\n+\n+    Operand<T> relu_logits = Select.create(scope, cond, logits, zeros);\n+    Operand<T> neg_abs_logits = Select.create(scope, cond, Neg.create(scope, logits), logits);\n+    return Add.create(\n+        scope,\n+        Sub.create(scope, relu_logits, Mul.create(scope, logits, labels)),\n+        Log1p.create(scope, Exp.create(scope, neg_abs_logits)));\n+  }\n+\n+  /**\n+   * Flattens logits' outer dimensions and keep its last dimension.\n+   *\n+   * @param scope the TensorFlow scope\n+   * @param logits the logits\n+   * @param <T> the type of logits\n+   * @return the flattened logits\n+   */\n+  private static <T extends TNumber> Operand<T> flattenOuterDims(Scope scope, Operand<T> logits) {\n+    Operand<TInt64> one = Constant.scalarOf(scope, 1L);\n+\n+    org.tensorflow.ndarray.Shape shape = logits.asOutput().shape();", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzEwNDMwOA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483104308", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T16:24:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NDM4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NjQyOA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480686428", "bodyText": "just Constant.tensorOf(scope, shape.size(ndims - 1)) will do the job here", "author": "karllessard", "createdAt": "2020-09-01T03:18:25Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"logits and labels must have the same shape (%s vs %s)\",\n+              labels.asOutput().shape().toString(), logits.asOutput().shape()));\n+    Operand<T> zeros =\n+        Cast.create(scope, ZerosLike.create(scope, logits), logits.asOutput().dataType());\n+    Operand<TBool> cond = GreaterEqual.create(scope, logits, zeros);\n+\n+    Operand<T> relu_logits = Select.create(scope, cond, logits, zeros);\n+    Operand<T> neg_abs_logits = Select.create(scope, cond, Neg.create(scope, logits), logits);\n+    return Add.create(\n+        scope,\n+        Sub.create(scope, relu_logits, Mul.create(scope, logits, labels)),\n+        Log1p.create(scope, Exp.create(scope, neg_abs_logits)));\n+  }\n+\n+  /**\n+   * Flattens logits' outer dimensions and keep its last dimension.\n+   *\n+   * @param scope the TensorFlow scope\n+   * @param logits the logits\n+   * @param <T> the type of logits\n+   * @return the flattened logits\n+   */\n+  private static <T extends TNumber> Operand<T> flattenOuterDims(Scope scope, Operand<T> logits) {\n+    Operand<TInt64> one = Constant.scalarOf(scope, 1L);\n+\n+    org.tensorflow.ndarray.Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == org.tensorflow.ndarray.Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        org.tensorflow.ndarray.Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return Reshape.create(scope, logits, Constant.vectorOf(scope, outputShape.asArray()));", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE2OTgwNg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r482169806", "bodyText": "I don't understand your comment.\nThis method, preserves the last dimension of shape, but does a product on all the dimensions but the last dimension. The resulting dimension is a long[] of product, last size.\nFor example, shape [2,3,4] becomes [6,4].\nThis is a more concise way to do this though, getting rid of the intermediate shape.\nreturn Reshape.create(scope, logits,\n                        Constant.vectorOf(scope, new long[] { product, shape.size(-1)});", "author": "JimClarke5", "createdAt": "2020-09-02T15:38:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NjQyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzEwNTU5Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483105596", "bodyText": "I just changed it to use Constant.arrayOf()\nreturn Reshape.create(\n            scope, logits, Constant.arrayOf(scope, product, shape.size(-1)));", "author": "JimClarke5", "createdAt": "2020-09-03T16:25:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NjQyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzEyOTI2Nw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483129267", "bodyText": "Sorry, my comment was wrong as I've ignored the product value. What you did is right by using Constant.arrayOf is basically what I meant, i.e. there is no need to convert an array to a Shape if we are passing just back this array to Constant", "author": "karllessard", "createdAt": "2020-09-03T17:05:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NjQyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NjgwNw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480686807", "bodyText": "camelCase variable names", "author": "karllessard", "createdAt": "2020-09-01T03:18:44Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"logits and labels must have the same shape (%s vs %s)\",\n+              labels.asOutput().shape().toString(), logits.asOutput().shape()));\n+    Operand<T> zeros =\n+        Cast.create(scope, ZerosLike.create(scope, logits), logits.asOutput().dataType());\n+    Operand<TBool> cond = GreaterEqual.create(scope, logits, zeros);\n+\n+    Operand<T> relu_logits = Select.create(scope, cond, logits, zeros);\n+    Operand<T> neg_abs_logits = Select.create(scope, cond, Neg.create(scope, logits), logits);\n+    return Add.create(\n+        scope,\n+        Sub.create(scope, relu_logits, Mul.create(scope, logits, labels)),\n+        Log1p.create(scope, Exp.create(scope, neg_abs_logits)));\n+  }\n+\n+  /**\n+   * Flattens logits' outer dimensions and keep its last dimension.\n+   *\n+   * @param scope the TensorFlow scope\n+   * @param logits the logits\n+   * @param <T> the type of logits\n+   * @return the flattened logits\n+   */\n+  private static <T extends TNumber> Operand<T> flattenOuterDims(Scope scope, Operand<T> logits) {\n+    Operand<TInt64> one = Constant.scalarOf(scope, 1L);\n+\n+    org.tensorflow.ndarray.Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == org.tensorflow.ndarray.Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        org.tensorflow.ndarray.Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return Reshape.create(scope, logits, Constant.vectorOf(scope, outputShape.asArray()));\n+      }\n+    }\n+\n+    Operand<TInt64> rank = Cast.create(scope, Rank.create(scope, logits), TInt64.DTYPE);\n+    Operand<TInt64> rankMinusOne = Sub.create(scope, rank, one);\n+\n+    Operand<TInt64> last_dim_size =", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk5NDU1NA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r482994554", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T13:52:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NjgwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NzQzNA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480687434", "bodyText": "...Constant.arrayOf(scope, -1L)...", "author": "karllessard", "createdAt": "2020-09-01T03:19:13Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"logits and labels must have the same shape (%s vs %s)\",\n+              labels.asOutput().shape().toString(), logits.asOutput().shape()));\n+    Operand<T> zeros =\n+        Cast.create(scope, ZerosLike.create(scope, logits), logits.asOutput().dataType());\n+    Operand<TBool> cond = GreaterEqual.create(scope, logits, zeros);\n+\n+    Operand<T> relu_logits = Select.create(scope, cond, logits, zeros);\n+    Operand<T> neg_abs_logits = Select.create(scope, cond, Neg.create(scope, logits), logits);\n+    return Add.create(\n+        scope,\n+        Sub.create(scope, relu_logits, Mul.create(scope, logits, labels)),\n+        Log1p.create(scope, Exp.create(scope, neg_abs_logits)));\n+  }\n+\n+  /**\n+   * Flattens logits' outer dimensions and keep its last dimension.\n+   *\n+   * @param scope the TensorFlow scope\n+   * @param logits the logits\n+   * @param <T> the type of logits\n+   * @return the flattened logits\n+   */\n+  private static <T extends TNumber> Operand<T> flattenOuterDims(Scope scope, Operand<T> logits) {\n+    Operand<TInt64> one = Constant.scalarOf(scope, 1L);\n+\n+    org.tensorflow.ndarray.Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == org.tensorflow.ndarray.Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        org.tensorflow.ndarray.Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return Reshape.create(scope, logits, Constant.vectorOf(scope, outputShape.asArray()));\n+      }\n+    }\n+\n+    Operand<TInt64> rank = Cast.create(scope, Rank.create(scope, logits), TInt64.DTYPE);\n+    Operand<TInt64> rankMinusOne = Sub.create(scope, rank, one);\n+\n+    Operand<TInt64> last_dim_size =\n+        Slice.create(\n+            scope,\n+            org.tensorflow.op.core.Shape.create(scope, logits, TInt64.DTYPE),\n+            rankMinusOne,\n+            one);\n+    Operand<TInt64> concat =\n+        Concat.create(\n+            scope,\n+            Arrays.asList(Constant.vectorOf(scope, new long[] {-1}), last_dim_size),", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk5NzQwOA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r482997408", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T13:56:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NzQzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4ODEzNw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480688137", "bodyText": "dimIndex", "author": "karllessard", "createdAt": "2020-09-01T03:19:46Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/op/core/NN.java", "diffHunk": "@@ -0,0 +1,379 @@\n+package org.tensorflow.op.core;\n+\n+import org.tensorflow.DataType;\n+import org.tensorflow.Operand;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.annotation.Endpoint;\n+import org.tensorflow.op.annotation.Operator;\n+import org.tensorflow.op.math.*;\n+import org.tensorflow.op.nn.raw.SoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.op.nn.raw.SparseSoftmaxCrossEntropyWithLogits;\n+import org.tensorflow.types.*;\n+import org.tensorflow.types.family.TNumber;\n+import org.tensorflow.op.dtypes.Cast;\n+import org.tensorflow.types.family.TType;\n+import org.tensorflow.op.linalg.Transpose;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+@Operator(group = \"nn\")\n+public abstract class NN {\n+\n+  /**\n+   * Computes softmax cross entropy between `logits` and `labels`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which the classes are\n+   * mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is\n+   * labeled with one and only one label: an image can be a dog or a truck, but not both.\n+   *\n+   * <p>**NOTE:** While the classes are mutually exclusive, their probabilities need not be. All\n+   * that is required is that each row of `labels` is a valid probability distribution. If they are\n+   * not, the computation of the gradient will be incorrect.\n+   *\n+   * <p>If using exclusive `labels` (wherein one and only one class is true at a time), see\n+   * `sparse_softmax_cross_entropy_with_logits`.\n+   *\n+   * <p>Usage:\n+   *\n+   * <pre>\n+   *   >>> logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]\n+   *   >>> labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]\n+   *   >>> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n+   *   <tf.Tensor: shape=(2,), dtype=float32,\n+   *   numpy=array([0.16984604, 0.82474494], dtype=float32)>\n+   * </pre>\n+   *\n+   * <p>Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation\n+   * into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this\n+   * function.\n+   *\n+   * @param scope current scope\n+   * @param labels Each vector along the class dimension should hold a valid probability\n+   *     distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`,\n+   *     each row of `labels[i]` must be a valid probability distribution.\n+   * @param logits Per-label activations, typically a linear output. These activation energies are\n+   *     interpreted as unnormalized log probabilities.\n+   * @param axis The class dimension. -1 is the last dimension.\n+   * @param <U> the data type of the logits\n+   * @param <T> the number type of the operands\n+   * @return the softmax cross entropy loss. Its type is the same as `logits` and its shape is the\n+   *     same as `labels` except that it does not have the last dimension of `labels`.\n+   */\n+  @Endpoint(name = \"softmaxCrossEntropyWithLogits\")\n+  public static <U extends TType, T extends TNumber> Operand<T> softmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits, int axis) {\n+    axis = axis % logits.asOutput().shape().numDimensions();\n+    if (axis < 0) {\n+      axis += logits.asOutput().shape().numDimensions();\n+    }\n+\n+    Operand precise_logits =\n+        logits; // cannot use generics cause logits of bool gets cast to TFloat32\n+\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    /* cannot use generics on DataType because precis_logits may have been cast. */\n+    DataType dtype = precise_logits.asOutput().dataType();\n+    labels = Cast.create(scope, labels, dtype);\n+    Operand<TInt64> inputRank =\n+        Cast.create(scope, Rank.create(scope, precise_logits), TInt64.DTYPE);\n+    Shape shape = logits.asOutput().shape();\n+\n+    // Move the dim to the end if dim is not the last dimension.\n+    if (axis != -1 && axis != precise_logits.asOutput().shape().numDimensions() - 1) {\n+      precise_logits = moveDimToEnd(scope, precise_logits, axis, inputRank);\n+      labels = moveDimToEnd(scope, labels, axis, inputRank);\n+    }\n+\n+    Shape inputShape = precise_logits.asOutput().shape();\n+    precise_logits = flattenOuterDims(scope, precise_logits);\n+    labels = flattenOuterDims(scope, labels);\n+    SoftmaxCrossEntropyWithLogits<T> smax =\n+        SoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    /* cannot use generic on cost, because cost may be recast later. */\n+    Operand cost = smax.loss();\n+    Operand<TInt64> outputShape =\n+        Slice.create(\n+            scope,\n+            Constant.vectorOf(scope, inputShape.asArray()),\n+            Constant.vectorOf(scope, new long[] {0}),\n+            Constant.vectorOf(scope, new long[] {inputShape.numDimensions() - 1}));\n+    cost = Reshape.create(scope, cost, outputShape);\n+    if (scope.env().isGraph() && !shape.hasUnknownDimension()) {\n+      long[] array = shape.asArray();\n+      long[] newArray = new long[array.length - 1];\n+      if (axis < 0) {\n+        axis = shape.numDimensions() + axis;\n+      }\n+      for (int i = 0; i < axis; i++) {\n+        newArray[i] = shape.size(i);\n+      }\n+      for (int i = axis + 1; i < shape.numDimensions(); i++) {\n+        newArray[i - 1] = shape.size(i);\n+      }\n+      Shape newShape = Shape.of(newArray);\n+      cost = Reshape.create(scope, cost, Constant.vectorOf(scope, newShape.asArray()));\n+    }\n+\n+    if (convertToFloat32) {\n+      cost = Cast.create(scope, cost, logits.asOutput().dataType());\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sparse softmax cross entropy between `logits` and `labels`.\n+   *\n+   * @param scope current scope\n+   * @param labels `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and\n+   *     result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0,\n+   *     num_classes)`. Other values will raise an exception when this op is run on CPU, and return\n+   *     `NaN` for corresponding loss and gradient rows on GPU.\n+   * @param logits Per-label activations (typically a linear output) of shape `[d_0, d_1, ...,\n+   *     d_{r-1}, num_classes]` and dtype `float16`, `float32`, or `float64`. These activation\n+   *     energies are interpreted as unnormalized log probabilities.\n+   * @return A `Tensor` of the same shape as `labels` and of the same type as `logits` with the\n+   *     softmax cross entropy loss.\n+   */\n+  @Endpoint(name = \"sparseSoftmaxCrossEntropyWithLogits\")\n+  public static <T extends TNumber, U extends TNumber> Operand sparseSoftmaxCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<U> logits) {\n+    // assert shapeIsCompatible(labels.asOutput().shape(), logits.asOutput().shape()):\n+    //        String.format(\"Shapes %s and %s are incompatible\",\n+    //                labels.asOutput().shape(), logits.asOutput().shape());\n+    scope = scope.withSubScope(\"SparseSoftmaxCrossEntropyWithLogits\");\n+    /** cannot use generics on precise_logits as it may be recast later */\n+    Operand precise_logits = logits;\n+    boolean convertToFloat32 =\n+        logits.asOutput().dataType() == TFloat16.DTYPE\n+            || logits.asOutput().dataType() == TBfloat16.DTYPE;\n+    if (convertToFloat32) {\n+      precise_logits = Cast.create(scope, logits, TFloat32.DTYPE);\n+    }\n+    Shape labelsStaticShape = labels.asOutput().shape();\n+    org.tensorflow.op.core.Shape<TInt32> labelsShape =\n+        org.tensorflow.op.core.Shape.create(scope, labels);\n+    Shape logitsShape = logits.asOutput().shape();\n+    Shape logitsShortened = logitsShape.take(logitsShape.numDimensions() - 1);\n+\n+    boolean staticShapesFullyDefined =\n+        !labelsStaticShape.hasUnknownDimension() && !logitsShortened.hasUnknownDimension();\n+    if (logitsShape.numDimensions() == 0) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Logits cannot be scalars - received shape %s.\", logitsShape));\n+    }\n+    if (!logitsShape.hasUnknownDimension()\n+        && !labelsStaticShape.hasUnknownDimension()\n+        && labelsStaticShape.numDimensions() != logitsShape.numDimensions() - 1) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Rank mismatch: Rank of labels (received %s) should equal rank of logits minus 1 (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+\n+    if (staticShapesFullyDefined && !labelsStaticShape.equals(logitsShortened)) {\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"Shape mismatch: The shape of labels (received %s) \"\n+                  + \"should equal the shape of logits except for the last \"\n+                  + \"dimension (received %s).\",\n+              labelsStaticShape.toString(), logitsShape.toString()));\n+    }\n+    // Check if no reshapes are required.\n+    if (logitsShape.numDimensions() == 2) {\n+      SparseSoftmaxCrossEntropyWithLogits smax =\n+          SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+      Operand loss = smax.loss();\n+      if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+        loss = Cast.create(scope, loss, TFloat16.DTYPE);\n+      }\n+      return loss;\n+    }\n+\n+    List<Op> shapeChecks = new ArrayList<>();\n+\n+    if (!staticShapesFullyDefined) {\n+      shapeChecks.add(\n+          AssertThat.create(\n+              scope,\n+              Equal.create(\n+                  scope,\n+                  org.tensorflow.op.core.Shape.create(scope, labels),\n+                  Shapes.take(\n+                      scope,\n+                      org.tensorflow.op.core.Shape.create(scope, logits),\n+                      Constant.scalarOf(scope, -1))),\n+              Collections.singletonList(\n+                  Constant.scalarOf(\n+                      scope,\n+                      \"Shape mismatch: The shape of labels  \"\n+                          + \"should equal the shape of logits except for the last \"\n+                          + \"dimension \"))));\n+    }\n+\n+    // Reshape logits to 2 dim, labels to 1 dim.\n+    long numClassses = logitsShape.size(logitsShape.numDimensions() - 1);\n+\n+    precise_logits =\n+        Reshape.create(\n+            scope, precise_logits, Constant.vectorOf(scope, new long[] {-1, numClassses}));\n+    labels = Reshape.create(scope, labels, Constant.scalarOf(scope, -1));\n+    scope.withControlDependencies(shapeChecks);\n+    SparseSoftmaxCrossEntropyWithLogits smax =\n+        SparseSoftmaxCrossEntropyWithLogits.create(scope, precise_logits, labels);\n+    Operand cost = smax.loss();\n+    cost = Reshape.create(scope, cost, labelsShape);\n+    if (logits.asOutput().dataType() == TFloat16.DTYPE) {\n+      cost = Cast.create(scope, cost, TFloat16.DTYPE);\n+    }\n+    return cost;\n+  }\n+\n+  /**\n+   * Computes sigmoid cross entropy given `logits`.\n+   *\n+   * <p>Measures the probability error in discrete classification tasks in which each class is\n+   * independent and not mutually exclusive. For instance, one could perform multilabel\n+   * classification where a picture can contain both an elephant and a dog at the same time.\n+   *\n+   * <p>For brevity, let `x = logits`, `z = labels`. The logistic loss is\n+   *\n+   * <pre>\n+   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n+   *     = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n+   *     = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n+   *     = (1 - z) * x + log(1 + exp(-x))\n+   *     = x - x * z + log(1 + exp(-x))\n+   * </pre>\n+   *\n+   * <p>For x < 0, to avoid overflow in exp(-x), we reformulate the above\n+   *\n+   * <pre>\n+   *      x - x * z + log(1 + exp(-x))\n+   *      = log(exp(x)) - x * z + log(1 + exp(-x))\n+   *      = - x * z + log(1 + exp(x))\n+   * </pre>\n+   *\n+   * <p>Hence, to ensure stability and avoid overflow, the implementation uses this equivalent\n+   * formulation\n+   *\n+   * <pre>\n+   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+   * </pre>\n+   *\n+   * <p>`logits` and `labels` must have the same type and shape.\n+   *\n+   * @param scope The TensorFlow scope\n+   * @param labels the labels\n+   * @param logits the logits of type float32 or float64\n+   * @param <T> the type of labels and logits\n+   * @return the component-wise logistic losses.\n+   */\n+  @Endpoint(name = \"sigmoidCrossEntropyWithLogits\")\n+  public static <T extends TNumber> Operand<T> sigmoidCrossEntropyWithLogits(\n+      Scope scope, Operand<T> labels, Operand<T> logits) {\n+    if (labels.asOutput().shape().numDimensions() != logits.asOutput().shape().numDimensions())\n+      throw new IllegalArgumentException(\n+          String.format(\n+              \"logits and labels must have the same shape (%s vs %s)\",\n+              labels.asOutput().shape().toString(), logits.asOutput().shape()));\n+    Operand<T> zeros =\n+        Cast.create(scope, ZerosLike.create(scope, logits), logits.asOutput().dataType());\n+    Operand<TBool> cond = GreaterEqual.create(scope, logits, zeros);\n+\n+    Operand<T> relu_logits = Select.create(scope, cond, logits, zeros);\n+    Operand<T> neg_abs_logits = Select.create(scope, cond, Neg.create(scope, logits), logits);\n+    return Add.create(\n+        scope,\n+        Sub.create(scope, relu_logits, Mul.create(scope, logits, labels)),\n+        Log1p.create(scope, Exp.create(scope, neg_abs_logits)));\n+  }\n+\n+  /**\n+   * Flattens logits' outer dimensions and keep its last dimension.\n+   *\n+   * @param scope the TensorFlow scope\n+   * @param logits the logits\n+   * @param <T> the type of logits\n+   * @return the flattened logits\n+   */\n+  private static <T extends TNumber> Operand<T> flattenOuterDims(Scope scope, Operand<T> logits) {\n+    Operand<TInt64> one = Constant.scalarOf(scope, 1L);\n+\n+    org.tensorflow.ndarray.Shape shape = logits.asOutput().shape();\n+    int ndims = shape.numDimensions();\n+    if (!shape.hasUnknownDimension()) {\n+      long product = 1L;\n+      boolean productValid = true;\n+      for (int i = ndims - 2; i >= 0; i--) {\n+        long d = shape.size(i);\n+        if (d == org.tensorflow.ndarray.Shape.UNKNOWN_SIZE) {\n+          productValid = false;\n+          break;\n+        }\n+        product *= d;\n+      }\n+      if (productValid) {\n+        org.tensorflow.ndarray.Shape outputShape = Shape.of(product, shape.size(ndims - 1));\n+        return Reshape.create(scope, logits, Constant.vectorOf(scope, outputShape.asArray()));\n+      }\n+    }\n+\n+    Operand<TInt64> rank = Cast.create(scope, Rank.create(scope, logits), TInt64.DTYPE);\n+    Operand<TInt64> rankMinusOne = Sub.create(scope, rank, one);\n+\n+    Operand<TInt64> last_dim_size =\n+        Slice.create(\n+            scope,\n+            org.tensorflow.op.core.Shape.create(scope, logits, TInt64.DTYPE),\n+            rankMinusOne,\n+            one);\n+    Operand<TInt64> concat =\n+        Concat.create(\n+            scope,\n+            Arrays.asList(Constant.vectorOf(scope, new long[] {-1}), last_dim_size),\n+            Constant.scalarOf(scope, 0));\n+    return Reshape.create(scope, logits, concat);\n+  }\n+\n+  /**\n+   * Move the dim to the end if dim is not the last dimension.\n+   *\n+   * @param scope The TensorFlow Scope\n+   * @param input the input to reshape\n+   * @param dim_index the index to move\n+   * @param rank the number of Dimensions in the tensor\n+   * @param <T> the data type of the tensor.\n+   * @param <U> the data type of the rank\n+   * @return the reshaped input\n+   */\n+  private static <T extends TNumber, U extends TNumber> Operand<T> moveDimToEnd(\n+      Scope scope, Operand<T> input, int dim_index, Operand<U> rank) {", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjk5NzcwOA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r482997708", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T13:56:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4ODEzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY5MTY5MQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r480691691", "bodyText": "I get a feeling that auto-formatter was a bit aggressive here, can you please just double-checked that the Google one was used when reformatting all the types classes?", "author": "karllessard", "createdAt": "2020-09-01T03:22:26Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/types/TString.java", "diffHunk": "@@ -235,23 +232,26 @@ public TString using(Charset charset) {\n \n   static <T> Tensor<TString> createTensor(NdArray<T> src, Function<T, byte[]> getBytes) {\n     long size = StringTensorBuffer.computeSize(src, getBytes);\n-    return Tensor.of(TString.DTYPE, src.shape(), size, data ->\n-        ((TStringImpl)data).tensorBuffer.init(src, getBytes)\n-    );\n+    return Tensor.of(\n+        TString.DTYPE,\n+        src.shape(),\n+        size,\n+        data -> ((TStringImpl) data).tensorBuffer.init(src, getBytes));\n   }\n \n   static TString mapTensor(TF_Tensor nativeTensor, Shape shape) {\n     StringTensorBuffer buffer = TensorBuffers.toStrings(nativeTensor, shape.size());\n     return new TStringImpl(buffer, UTF_8_LAYOUT, shape);\n   }\n \n-  private static DataLayout<DataBuffer<byte[]>, String> UTF_8_LAYOUT = DataLayouts.ofStrings(StandardCharsets.UTF_8);\n+  private static DataLayout<DataBuffer<byte[]>, String> UTF_8_LAYOUT =\n+      DataLayouts.ofStrings(StandardCharsets.UTF_8);\n \n   private final StringTensorBuffer tensorBuffer;\n \n-  private TStringImpl(StringTensorBuffer buffer, DataLayout<DataBuffer<byte[]>, String> layout, Shape shape) {\n+  private TStringImpl(\n+      StringTensorBuffer buffer, DataLayout<DataBuffer<byte[]>, String> layout, Shape shape) {\n     super(layout.applyTo(buffer), shape);\n     tensorBuffer = buffer;\n   }\n }\n-", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzAyMDY0Nw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483020647", "bodyText": "I have verified that google-java-format settings are installed and enabled in InteliJ", "author": "JimClarke5", "createdAt": "2020-09-03T14:27:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY5MTY5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzEwNjEyOQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483106129", "bodyText": "There was a format change on TString.java and TBool.java when I double checked all the formats on the  types.", "author": "JimClarke5", "createdAt": "2020-09-03T16:26:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY5MTY5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzEyOTc2Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483129766", "bodyText": "Ok, it's fine I just wanted to make sure that the right formatter was applied", "author": "karllessard", "createdAt": "2020-09-03T17:06:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY5MTY5MQ=="}], "type": "inlineReview"}, {"oid": "ab379d1b49d90bb42c8654f2e0cf0c258e0d3406", "url": "https://github.com/tensorflow/java/commit/ab379d1b49d90bb42c8654f2e0cf0c258e0d3406", "message": "Refactor NN into individual operations under org.tensorflow.op.nn. Fix JavaDoc. Change from snake case to camel case.", "committedDate": "2020-09-03T14:21:27Z", "type": "commit"}, {"oid": "889d67e11ec7154605a0cb235097e30c53a5704a", "url": "https://github.com/tensorflow/java/commit/889d67e11ec7154605a0cb235097e30c53a5704a", "message": "Refactor NN into individual operations under org.tensorflow.op.nn. Fix JavaDoc. Change from snake case to camel case.", "committedDate": "2020-09-03T14:21:33Z", "type": "commit"}, {"oid": "515b799bf793fbb38a47b5aa92d948dad052187b", "url": "https://github.com/tensorflow/java/commit/515b799bf793fbb38a47b5aa92d948dad052187b", "message": "Reformatted code", "committedDate": "2020-09-03T14:34:57Z", "type": "commit"}, {"oid": "5a9fe3747b730cbfb28b544bcac3aeb2077499a2", "url": "https://github.com/tensorflow/java/commit/5a9fe3747b730cbfb28b544bcac3aeb2077499a2", "message": "Added sub scope", "committedDate": "2020-09-03T14:35:19Z", "type": "commit"}, {"oid": "8d21dd7266e48a4df36813ed3fb9dc1adee58915", "url": "https://github.com/tensorflow/java/commit/8d21dd7266e48a4df36813ed3fb9dc1adee58915", "message": "Miscellaneous fixes based on review comments.", "committedDate": "2020-09-03T16:30:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzIzOTUxNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483239515", "bodyText": "extra space before tf", "author": "karllessard", "createdAt": "2020-09-03T20:35:07Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+\n+/**\n+ * AdaDelta Optimizer that implements the AdaDelta algorithm. Keras wrapper around the Tensorflow\n+ * Framework optimizer. Adadelta optimization is a stochastic gradient descent method that is based\n+ * on adaptive learning rate per dimension to address two drawbacks: 1) the continual decay of\n+ * learning rates throughout training 2) the need for a manually selected global learning rate\n+ *\n+ * <p>Two accumulation steps are required: 1) the accumulation of gradients squared, 2) the\n+ * accumulation of updates squared.\n+ *\n+ */\n+public class AdaDelta extends org.tensorflow.framework.optimizers.AdaDelta\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String RHO_RATE_KEY = \"rho\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float RHO_DEFAULT = 0.95F;\n+  public static final float EPSILON_DEFAULT = 1e-7F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  private List<Op> initializers = new ArrayList<>();\n+\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", learning_rate=0.001F, rho=0.95F, and\n+   * epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   */\n+  public AdaDelta(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default learning_rate=0.001F, rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   */\n+  public AdaDelta(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, float learningRate) {\n+    this(tf, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\",\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), name, learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String name) {\n+    switch (initializers.size()) {\n+      case 0:\n+        return Optional.empty();\n+      case 1:\n+        return Optional.of(initializers.get(0));\n+      default:\n+        return Optional.of( tf.withSubScope(name).withControlDependencies(initializers).noOp());", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI5OTc5Mg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483299792", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T23:10:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzIzOTUxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MDc0Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483240743", "bodyText": "Can we create a concrete type for the config instead of having a map of untyped values? e.g. could it be something like an AdaDelta.Config subclass following a building pattern similar to the ops options classes like this one? I probably don't understand the use of this config format though.\nThe same comment applies for all other optimizers", "author": "karllessard", "createdAt": "2020-09-03T20:37:51Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+\n+/**\n+ * AdaDelta Optimizer that implements the AdaDelta algorithm. Keras wrapper around the Tensorflow\n+ * Framework optimizer. Adadelta optimization is a stochastic gradient descent method that is based\n+ * on adaptive learning rate per dimension to address two drawbacks: 1) the continual decay of\n+ * learning rates throughout training 2) the need for a manually selected global learning rate\n+ *\n+ * <p>Two accumulation steps are required: 1) the accumulation of gradients squared, 2) the\n+ * accumulation of updates squared.\n+ *\n+ */\n+public class AdaDelta extends org.tensorflow.framework.optimizers.AdaDelta\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String RHO_RATE_KEY = \"rho\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float RHO_DEFAULT = 0.95F;\n+  public static final float EPSILON_DEFAULT = 1e-7F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  private List<Op> initializers = new ArrayList<>();\n+\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", learning_rate=0.001F, rho=0.95F, and\n+   * epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   */\n+  public AdaDelta(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default learning_rate=0.001F, rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   */\n+  public AdaDelta(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, float learningRate) {\n+    this(tf, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\",\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), name, learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String name) {\n+    switch (initializers.size()) {\n+      case 0:\n+        return Optional.empty();\n+      case 1:\n+        return Optional.of(initializers.get(0));\n+      default:\n+        return Optional.of( tf.withSubScope(name).withControlDependencies(initializers).noOp());\n+    }\n+  }\n+\n+  /**\n+   * Create an Adam Optimizer from a config object\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, he config object has keys for \"name\",\n+   *     \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the default value is used.\n+   */\n+  public static AdaDelta fromConfig(Ops tf, Map<String, Object> config) {\n+    return create(tf, config);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, the config\n+   *     object has keys for \"name\", \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the\n+   *     default value is used.\n+   */\n+  public static AdaDelta create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float rho = (float) config.getOrDefault(RHO_RATE_KEY, RHO_DEFAULT);\n+    float epsilon = (float) config.getOrDefault(EPSILON_KEY, EPSILON_DEFAULT);", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMwMzQyMg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483303422", "bodyText": "This was used in Python for \"serialization\" of the Optimizers. The Config could be saved, then used to restore the state of the Optimizer. The key is always a String, but the value can be a String, a Float or Integer, (but maybe also a Double or Boolean), so Map<String, Object> should suffice. Since I did this, I have not yet seen where this dictionary is actually used, so I assume the python code expected some user software or tool to use it. I am not sure how this  plays into  saving/restoring the model, as I haven't looked at that logic yet.\nWe could pull it out for now, then revisit it when we start tackling saving/restoring the Keras model.\nOr I can change it to the options pattern, as long as there is a way to read/write it, perhaps as a JSON string.", "author": "JimClarke5", "createdAt": "2020-09-03T23:22:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MDc0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzk5MTE3MA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483991170", "bodyText": "I vote for removing the serialization/deserialization support for now and add it later.", "author": "karllessard", "createdAt": "2020-09-05T21:24:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MDc0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA2NDQwOQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484064409", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-06T12:32:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MDc0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDE2NDA4NA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484164084", "bodyText": "When we return to this we could make the config objects implement Map<String,Object> but specialise the types per optimiser and provide specific getters & setters?", "author": "Craigacp", "createdAt": "2020-09-07T03:22:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MDc0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQ5Mzc5OA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484493798", "bodyText": "I have started to write the \"options\" pattern with internal classes on the Optimizers. Do we want to do this? Some of the optimizers' ctor's are trivial, but then others may have a more significant number of options with Keras specified defaults.", "author": "JimClarke5", "createdAt": "2020-09-07T15:29:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MDc0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUwMDI0OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484500249", "bodyText": "I really think we should postpone this feature for now as there is no urgent need to support serialization/deserialization of the configurations, we can then brainstorm on the right approach to take.", "author": "karllessard", "createdAt": "2020-09-07T15:46:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MDc0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUzMDU4NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484530585", "bodyText": "I have removed it for now and we can revisit later when we look at serializing the Keras Model", "author": "JimClarke5", "createdAt": "2020-09-07T17:31:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MDc0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MTA5OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483241099", "bodyText": "unrequired new line before bracket", "author": "karllessard", "createdAt": "2020-09-03T20:38:39Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+\n+/**\n+ * AdaDelta Optimizer that implements the AdaDelta algorithm. Keras wrapper around the Tensorflow\n+ * Framework optimizer. Adadelta optimization is a stochastic gradient descent method that is based\n+ * on adaptive learning rate per dimension to address two drawbacks: 1) the continual decay of\n+ * learning rates throughout training 2) the need for a manually selected global learning rate\n+ *\n+ * <p>Two accumulation steps are required: 1) the accumulation of gradients squared, 2) the\n+ * accumulation of updates squared.\n+ *\n+ */\n+public class AdaDelta extends org.tensorflow.framework.optimizers.AdaDelta\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String RHO_RATE_KEY = \"rho\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float RHO_DEFAULT = 0.95F;\n+  public static final float EPSILON_DEFAULT = 1e-7F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  private List<Op> initializers = new ArrayList<>();\n+\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", learning_rate=0.001F, rho=0.95F, and\n+   * epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   */\n+  public AdaDelta(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default learning_rate=0.001F, rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   */\n+  public AdaDelta(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, float learningRate) {\n+    this(tf, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\",\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), name, learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String name) {\n+    switch (initializers.size()) {\n+      case 0:\n+        return Optional.empty();\n+      case 1:\n+        return Optional.of(initializers.get(0));\n+      default:\n+        return Optional.of( tf.withSubScope(name).withControlDependencies(initializers).noOp());\n+    }\n+  }\n+\n+  /**\n+   * Create an Adam Optimizer from a config object\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, he config object has keys for \"name\",\n+   *     \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the default value is used.\n+   */\n+  public static AdaDelta fromConfig(Ops tf, Map<String, Object> config) {\n+    return create(tf, config);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, the config\n+   *     object has keys for \"name\", \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the\n+   *     default value is used.\n+   */\n+  public static AdaDelta create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float rho = (float) config.getOrDefault(RHO_RATE_KEY, RHO_DEFAULT);\n+    float epsilon = (float) config.getOrDefault(EPSILON_KEY, EPSILON_DEFAULT);\n+    if (name == null) // doe this to get the default name\n+    {", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMwMzk4Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483303986", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T23:24:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MTA5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MjQ3Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483242476", "bodyText": "extra lines", "author": "karllessard", "createdAt": "2020-09-03T20:41:39Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+\n+/**\n+ * AdaDelta Optimizer that implements the AdaDelta algorithm. Keras wrapper around the Tensorflow\n+ * Framework optimizer. Adadelta optimization is a stochastic gradient descent method that is based\n+ * on adaptive learning rate per dimension to address two drawbacks: 1) the continual decay of\n+ * learning rates throughout training 2) the need for a manually selected global learning rate\n+ *\n+ * <p>Two accumulation steps are required: 1) the accumulation of gradients squared, 2) the\n+ * accumulation of updates squared.\n+ *\n+ */\n+public class AdaDelta extends org.tensorflow.framework.optimizers.AdaDelta\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String RHO_RATE_KEY = \"rho\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float RHO_DEFAULT = 0.95F;\n+  public static final float EPSILON_DEFAULT = 1e-7F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  private List<Op> initializers = new ArrayList<>();\n+\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", learning_rate=0.001F, rho=0.95F, and\n+   * epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   */\n+  public AdaDelta(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default learning_rate=0.001F, rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   */\n+  public AdaDelta(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, float learningRate) {\n+    this(tf, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\",\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), name, learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String name) {\n+    switch (initializers.size()) {\n+      case 0:\n+        return Optional.empty();\n+      case 1:\n+        return Optional.of(initializers.get(0));\n+      default:\n+        return Optional.of( tf.withSubScope(name).withControlDependencies(initializers).noOp());\n+    }\n+  }\n+\n+  /**\n+   * Create an Adam Optimizer from a config object\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, he config object has keys for \"name\",\n+   *     \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the default value is used.\n+   */\n+  public static AdaDelta fromConfig(Ops tf, Map<String, Object> config) {\n+    return create(tf, config);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, the config\n+   *     object has keys for \"name\", \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the\n+   *     default value is used.\n+   */\n+  public static AdaDelta create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float rho = (float) config.getOrDefault(RHO_RATE_KEY, RHO_DEFAULT);\n+    float epsilon = (float) config.getOrDefault(EPSILON_KEY, EPSILON_DEFAULT);\n+    if (name == null) // doe this to get the default name\n+    {\n+      return new AdaDelta(tf, learningRate, rho, epsilon);\n+    } else {\n+      return new AdaDelta(tf, name, learningRate, rho, epsilon);\n+    }\n+  }\n+\n+  /**\n+   * Initialize the configuration based on which constructor is called.\n+   *\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  private void initConfig(float learningRate, float rho, float epsilon) {\n+    this.learningRate = learningRate;\n+    config.put(NAME_KEY, this.getOptimizerName());\n+    config.put(LEARNING_RATE_KEY, learningRate);\n+    config.put(RHO_RATE_KEY, rho);\n+    config.put(EPSILON_KEY, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Map<String, Object> getConfig() {\n+    return config;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public float getLearningRate() {\n+    return this.learningRate;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public void setLearningRate(float learningRate) {\n+    this.learningRate = learningRate;\n+  }\n+\n+", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMwOTA0MA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483309040", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T23:42:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0MjQ3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0Mjk0MQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483242941", "bodyText": "double underscores.", "author": "karllessard", "createdAt": "2020-09-03T20:42:41Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaGrad.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/**\n+ * AdaGrad Optimizer that implements the AdaGrad algorithm. Adagrad is an optimizer with\n+ * parameter-specific learning rates, which are adapted relative to how frequently a parameter gets\n+ * updated during training. The more updates a parameter receives, the smaller the updates.\n+ */\n+public class AdaGrad extends org.tensorflow.framework.optimizers.AdaGrad\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String INITIAL_ACCUM_KEY = \"accumulator\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float INITIAL_ACCUM__DEFAULT = 0.1f;", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMwODk5NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483308995", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-03T23:41:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0Mjk0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0Mzg0Mg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483243842", "bodyText": "please reformat these if blocks", "author": "karllessard", "createdAt": "2020-09-03T20:44:32Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaGradDA.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Ops;\n+\n+/** Optimizer that implements the Adagrad Dual-Averaging algorithm. */\n+public class AdaGradDA extends org.tensorflow.framework.optimizers.AdaGradDA\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String INITIAL_ACCUM_KEY = \"accumulator\";\n+  public static final String L1STRENGTH_KEY = \"l1Strength\";\n+  public static final String L2STRENGTH_KEY = \"l2Strength\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F; // arbitray number\n+  public static final float INITIAL_ACCUM__DEFAULT = 0.1f;\n+  public static final float L1STRENGTH_DEFAULT = 0.0F;\n+  public static final float L2STRENGTH_DEFAULT = 0.0F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values name=\"adagrad-da\". learning_rate=.001,\n+   * initial accumulator= 0.1, l1Strength=0.0, l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   */\n+  public AdaGradDA(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values initial accumulator= 0.1, l1Strength=0.0,\n+   * l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   * @param learningRate The learning rate.\n+   */\n+  public AdaGradDA(Ops tf, float learningRate) {\n+    this(tf, learningRate, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer with default values initial accumulator= 0.1, l1Strength=0.0,\n+   * l2Strength=0.0;\n+   *\n+   * @param tf the tensorflow tf\n+   * @param name the name of the Optimizer, defaults to \"adagrad-da\"\n+   * @param learningRate The learning rate.\n+   */\n+  public AdaGradDA(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, INITIAL_ACCUM__DEFAULT, L1STRENGTH_DEFAULT, L2STRENGTH_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer\n+   *\n+   * @param tf the tensorflow tf\n+   * @param learningRate the learning rate, default is 0.001\n+   * @param initialAccumulatorValue Starting value for the accumulators, must be >= 0.0.\n+   * @param l1Strength L1 Regularization Strength\n+   * @param l2Strength L2 Regularization Strength\n+   */\n+  public AdaGradDA(\n+      Ops tf,\n+      float learningRate,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength) {\n+    super(assertGraph(tf), learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+    if( initialAccumulatorValue < 0.0F)\n+        throw new IllegalArgumentException(\"initial_accumulator_value must be non-negative: \" + initialAccumulatorValue);\n+    if(l1Strength < 0)\n+      throw new IllegalArgumentException(\"l1Strength must be non-negative: \" + l1Strength);\n+    if(l2Strength < 0)\n+      throw new IllegalArgumentException(\"l2Strength must be non-negative: \" + l2Strength);\n+    initConfig(learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+  }\n+\n+  /**\n+   * Create an AdagradDA Optimizer\n+   *\n+   * @param tf the tensorflow tf\n+   * @param name the name of the Optimizer, defaults to \"adagrad-da\"\n+   * @param learningRate the learning rate, default is 0.001\n+   * @param initialAccumulatorValue Starting value for the accumulators, must be positive.\n+   * @param l1Strength L1 Regularization Strength\n+   * @param l2Strength L2 Regularization Strength\n+   */\n+  public AdaGradDA(\n+      Ops tf,\n+      String name,\n+      float learningRate,\n+      float initialAccumulatorValue,\n+      float l1Strength,\n+      float l2Strength) {\n+    super(assertGraph(tf), name, learningRate, initialAccumulatorValue, l1Strength, l2Strength);\n+    if( initialAccumulatorValue < 0.0F)\n+      throw new IllegalArgumentException(\"initial_accumulator_value must be non-negative: \" + initialAccumulatorValue);\n+    if(l1Strength < 0)\n+      throw new IllegalArgumentException(\"l1Strength must be non-negative: \" + l1Strength);\n+    if(l2Strength < 0)", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA2NTA2MQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484065061", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-06T12:39:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0Mzg0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NTI4Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483245283", "bodyText": "Can we create a Adamax implementation in the framework first and then having its Keras wrapper extend from it like other optimizers? The same applies for all other Keras optimizers that are not present in the framework.", "author": "karllessard", "createdAt": "2020-09-03T20:47:26Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/Adamax.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.train.ApplyAdaMax;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+/** Adamax Optimizer that implements the Adamax algorithm. */\n+public class Adamax extends org.tensorflow.framework.optimizers.Optimizer", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMwNDUyNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483304525", "bodyText": "Yes.", "author": "JimClarke5", "createdAt": "2020-09-03T23:26:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NTI4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NjQ4Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483246486", "bodyText": "Again are you sure we need these explicit casting to Operand? If so, something is wrong with our generic signatures and we should address that.", "author": "karllessard", "createdAt": "2020-09-03T20:49:59Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/Adamax.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import org.tensorflow.Graph;\n+import org.tensorflow.Operand;\n+import org.tensorflow.Output;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.Scope;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.op.train.ApplyAdaMax;\n+import org.tensorflow.types.TFloat32;\n+import org.tensorflow.types.family.TType;\n+\n+/** Adamax Optimizer that implements the Adamax algorithm. */\n+public class Adamax extends org.tensorflow.framework.optimizers.Optimizer\n+    implements OptimizerInterface {\n+\n+  public static final String FIRST_MOMENT = \"m\";\n+  public static final String SECOND_MOMENT = \"v\";\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+  public static final String BETA_ONE_KEY = \"beta_1\";\n+  public static final String BETA_TWO_KEY = \"beta_2\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float EPSILON_DEFAULT = 1e-07F;\n+  public static final float BETA_ONE_DEFAULT = 0.9F;\n+  public static final float BETA_TWO_DEFAULT = 0.999F;\n+\n+  private Scope scope;\n+  private Map<String, Object> config = new HashMap<>();\n+\n+  private float learningRate;\n+  private final float betaOne;\n+  private final float betaTwo;\n+  private final float epsilon;\n+\n+  private Constant<TFloat32> learningRateConst;\n+  private Constant<TFloat32> epsilonConst;\n+  private Constant<TFloat32> betaOneConst;\n+  private Constant<TFloat32> betaTwoConst;\n+  private Variable<TFloat32> betaOnePower;\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   */\n+  public Adamax(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param name name for the operations Created when applying gradients. Defaults to \"Adamax\".\n+   */\n+  public Adamax(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param learningRate The learning rate.\n+   */\n+  public Adamax(Ops tf, float learningRate) {\n+    this(tf, learningRate, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param name name for the operations Created when applying gradients. Defaults to \"Adamax\".\n+   * @param learningRate The learning rate.\n+   */\n+  public Adamax(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, BETA_ONE_DEFAULT, BETA_TWO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param learningRate The learning rate.\n+   * @param betaOne The exponential decay rate for the 1st moment estimates.\n+   * @param betaTwo The exponential decay rate for the exponentially weighted infinity norm.\n+   * @param epsilon A small constant for numerical stability.\n+   */\n+  public Adamax(Ops tf, float learningRate, float betaOne, float betaTwo, float epsilon) {\n+    super(assertGraph(tf));\n+    this.learningRate = learningRate;\n+    this.betaOne = betaOne;\n+    this.betaTwo = betaTwo;\n+    this.epsilon = epsilon;\n+    this.scope = tf.scope();\n+    initConfig(learningRate, betaOne, betaTwo, epsilon);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm.\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param name name for the operations Created when applying gradients. Defaults to \"Adamax\".\n+   * @param learningRate The learning rate.\n+   * @param betaOne The exponential decay rate for the 1st moment estimates.\n+   * @param betaTwo The exponential decay rate for the exponentially weighted infinity norm.\n+   * @param epsilon A small constant for numerical stability.\n+   */\n+  public Adamax(\n+      Ops tf, String name, float learningRate, float betaOne, float betaTwo, float epsilon) {\n+    super(assertGraph(tf), name);\n+    this.learningRate = learningRate;\n+    this.betaOne = betaOne;\n+    this.betaTwo = betaTwo;\n+    this.epsilon = epsilon;\n+    this.scope = tf.scope();\n+\n+    initConfig(learningRate, betaOne, betaTwo, epsilon);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm from a config object\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param config a config object to initialize, the config object has keys for \"name\",\n+   *     \"learning_rate\", \"epsilon\", \"beta_1\", \"beta_2\". If a key is missing the default value is\n+   *     used.\n+   */\n+  public static Adamax fromConfig(Ops tf, Map<String, Object> config) {\n+    return create(tf, config);\n+  }\n+\n+  /**\n+   * Create an Optimizer that implements the Adamax algorithm from a config object\n+   *\n+   * @param tf the TensoFlow Ops\n+   * @param config a config object to initialize\n+   */\n+  public static Adamax create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float epsilon = (float) config.getOrDefault(EPSILON_KEY, EPSILON_DEFAULT);\n+    float betaOne = (float) config.getOrDefault(BETA_ONE_KEY, BETA_ONE_DEFAULT);\n+    float betaTwo = (float) config.getOrDefault(BETA_TWO_KEY, BETA_TWO_DEFAULT);\n+    if (name == null) {\n+      return new Adamax(tf, learningRate, betaOne, betaTwo, epsilon);\n+    } else {\n+      return new Adamax(tf, name, learningRate, betaOne, betaTwo, epsilon);\n+    }\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Map<String, Object> getConfig() {\n+    return config;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public float getLearningRate() {\n+    return this.learningRate;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public void setLearningRate(float learningRate) {\n+    this.learningRate = learningRate;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String scopeName) {\n+    betaOneConst = tf.constant(betaOne);\n+    betaTwoConst = tf.constant(betaTwo);\n+    learningRateConst = tf.constant(learningRate);\n+    epsilonConst = tf.constant(epsilon);\n+\n+    return Optional.empty();\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected void createSlots(List<Output<? extends TType>> variables) {\n+    for (Output<? extends TType> v : variables) {\n+      createAdamaxSlot(v.asOutput());\n+    }\n+    betaOnePower = tf.withName(\"beta1_power\").variable(Shape.scalar(), TFloat32.DTYPE);\n+    Assign<TFloat32> betaOnePowerInit = tf.assign(betaOnePower, tf.constant(betaOne));\n+    ((Graph) tf.scope().env()).addInitializer(betaOnePowerInit);\n+  }\n+\n+  /**\n+   * Create the first and second moment slots\n+   *\n+   * @param v the variable\n+   * @param <T> the datatype of the variable\n+   */\n+  private <T extends TType> void createAdamaxSlot(Output<T> v) {\n+    Operand<T> firstMomentInitializer =\n+        tf.fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f), v.dataType()));\n+    createSlot(v.asOutput(), FIRST_MOMENT, firstMomentInitializer);\n+    Operand<T> secondMomentInitializer =\n+        tf.fill(tf.shape(v), tf.dtypes.cast(tf.constant(0.0f), v.dataType()));\n+    createSlot(v.asOutput(), SECOND_MOMENT, secondMomentInitializer);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected <T extends TType> Op applyDense(Output<T> gradient, Output<T> variable) {\n+    Variable<T> firstMomentSlot = getSlot(variable, FIRST_MOMENT).get();\n+    Variable<T> secondMomentSlot = getSlot(variable, SECOND_MOMENT).get();\n+    return ApplyAdaMax.create(\n+        scope,\n+        (Operand) variable,\n+        (Operand) firstMomentSlot,\n+        (Operand) secondMomentSlot,\n+        (Operand) tf.dtypes.cast(betaOnePower, gradient.dataType()),\n+        (Operand) tf.dtypes.cast(learningRateConst, gradient.dataType()),\n+        (Operand) tf.dtypes.cast(betaOneConst, gradient.dataType()),\n+        (Operand) tf.dtypes.cast(betaTwoConst, gradient.dataType()),\n+        (Operand) tf.dtypes.cast(epsilonConst, gradient.dataType()),\n+        (Operand) gradient);", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMwNTU3MA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483305570", "bodyText": "I removed the (Operand) casts. It must have been when I was just figuring out the framework.", "author": "JimClarke5", "createdAt": "2020-09-03T23:30:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NjQ4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzY1MQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483247651", "bodyText": "If possible, I would prefer we avoid to suffix this class name with Interface, since none of our other interfaces have this suffix.", "author": "karllessard", "createdAt": "2020-09-03T20:52:24Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/OptimizerInterface.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.Map;\n+import org.tensorflow.Graph;\n+import org.tensorflow.op.Ops;\n+\n+/** The main Interface for Keras Optimizers */\n+public interface OptimizerInterface {", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMwNjYxMw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483306613", "bodyText": "I was trying to avoid a name clash with org.tensorflow.framework.optimizers.Optimizer. Some of OptimizerInterface could be refactored into org.tensorflow.framework.optimizers.Optimizer. At the time, I was trying to keep the two projects, keras and frameworks, totally separate.", "author": "JimClarke5", "createdAt": "2020-09-03T23:33:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDE2Mzg0MQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484163841", "bodyText": "We could call this KerasOptimizer? The fact that python overloads all the names doesn't mean we need to. Or rename org.tensorflow.framework.optimizers.Optimizer to BaseOptimizer or FrameworkOptimizer?", "author": "Craigacp", "createdAt": "2020-09-07T03:20:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQ5ODEzOQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484498139", "bodyText": "Actually, I have done away with the KerasOptimizer interface for now and put assertGraph as a static call in a helper class. However, this helper class will probably be useful in some of the other Keras packages that use Variables, so maybe it should be moved to a general util package that is only visible to the java module, wherever we decide to put it.\nOne question that remains for the Optimizer interface, is where to put the method prototypes for get/setLearningRate, when that feature comes in the next PR. The Keras code will need to treat these methods as a general Optimizer, rather than as individual types of Optimizer. There are Optimizers that I found on the Web that don't use learning rate, but all of the currently defined Optimizers in the framework do use it.", "author": "JimClarke5", "createdAt": "2020-09-07T15:40:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzY1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483247815", "bodyText": "missing space after if", "author": "karllessard", "createdAt": "2020-09-03T20:52:45Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/OptimizerInterface.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.Map;\n+import org.tensorflow.Graph;\n+import org.tensorflow.op.Ops;\n+\n+/** The main Interface for Keras Optimizers */\n+public interface OptimizerInterface {\n+\n+  /** The value for the name key in the Config object */\n+  String NAME_KEY = \"name\";\n+\n+  /**\n+   * Get a TensorFlow Graph from the Ops.\n+   *\n+   * @param tf the TensorFlow Ops\n+   * @return the graph\n+   * @throws java.lang.IllegalArgumentException if the TensorFlow Ops does not represent Graph mode\n+   */\n+  static Graph assertGraph(Ops tf) {\n+    if(!tf.scope().env().isGraph()) {", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMwODU0NA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483308544", "bodyText": "OK, on the formatting,\nIf I refactor OptimizerInterface into org.tensorflow.framework.optimizers.Optimizer, then where should assetGraph() method go?  It is basically a utility that can be called from a super() ctor method, ensuring that Ops tf always represents a Graph, and then passes the Graph to the framework optimizers ctors.", "author": "JimClarke5", "createdAt": "2020-09-03T23:40:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzk5MTY1MA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483991650", "bodyText": "I'm fine with either or.\nI myself have some trouble figuring out how the framework and keras layer should split the job but at a high level, what I'm thinking is that most of the logic should occur in the framework and the core libraries while the Keras library should add a Pythonic Keras-like API as a facade to them. But I'm sure there will be exceptions. So in this case, if you don't think Keras users will need to access directly the graph of an optimizer and no such getter exists in the Python Keras lib, then maybe we should move it to the framework.", "author": "karllessard", "createdAt": "2020-09-05T21:31:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA2NDk4Nw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484064987", "bodyText": "Main issue is  consistency. The rest of the Keras apis are using Ops instead of Graph directly. Using Ops has an advantage, from my point of view, in that it insulates from a tight binding to Graph, when  one wants Eager support.", "author": "JimClarke5", "createdAt": "2020-09-06T12:38:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA2NTYwMQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484065601", "bodyText": "Your comment also  brings up another consideration. If you take the premise that \"Keras library should add a Pythonic Keras-like API as a facade to them\", then why wouldn't activations, loss, metrics, initializers, etc. be primarily in frameworks rather than solely in Keras?", "author": "JimClarke5", "createdAt": "2020-09-06T12:44:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA4MDQ3OA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484080478", "bodyText": "...which is a totally legitimate question. It is not the first time we are questioning the roles of the framework and the Keras libraries, and the reason why we have both.\nSo initially the idea was what I explained, which is:\n\ncore: exposes API of the native library in Java\nframework: high(mid)-level layer enriching this API with common ML utilities\nKeras: replicates the Keras API of Python on top of both\n\nBut now what we see is that we started to do some mid-level wrappers in the core directly (e.g. sparseCrossEntropyWithLogits) as we want to benefit from the Ops annotation processor exclusive to the core. Plus, the actual state of the TF API in Python is a little bit counter-intuitive as you cannot do everything just using the Keras API, as a result of the existence of the legacy API still present to this day. Since in Java we start everything from scratch, why not just doing a enhanced Keras API that can handle everything at the user perspective?\nAs a Keras user myself, I'm always looking to use exclusively this API which I prefer to the old one and find it a bit incoherent and disappointing when I see that a few things are only possible to do with using vanilla TF. Why not keeping it simpler in Java and put everything in Keras then? It is questionable.\nIt sounds to me that too that it would be easier for our collaborators if everything is one place instead of having to ask themselves how it should be split between these two similar-libraries...\nSorry if I'm not answering your question and probably bring more confusion but I think we need to set this up correctly to start with. Let's bring more people to this conversation: @Craigacp , @KartikChugh, @deansher ?", "author": "karllessard", "createdAt": "2020-09-06T15:02:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA4OTA3Nw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484089077", "bodyText": "To me, it seems worth writing down carefully chosen goals for each of the layers. Just to illustrate the sort of confusion I worry about, when @karllessard says \"the Keras library should add a Pythonic Keras-like API as a facade to them\", I ask myself these questions:\n\nWhy didn't we want a Pythonic Keras-like API for the framework and core libraries in the first place?\nGiven that we didn't, what's our goal in providing such an API?\n\nOur existing  top-level README.md takes a shot at defining goals, but I don't think it goes far enough:\n\n\ntensorflow-core\n\nAll artifacts that build up the core language bindings of TensorFlow for Java.\nThose artifacts provide the minimal support required to use the TensorFlow runtime on a JVM.\n\n\n\ntensorflow-framework\n\nHigh-level APIs built on top of the core libraries to simplify neural network training and inference\nusing TensorFlow.\n\n\n\nI'm guessing what we need goes something like this:\n\n\nThe core libraries should provide the most essential capabilities for someone who is strong in all three of Java, Tensorflow, and deep learning. It's hard to foresee why anyone besides this project would code entirely against this layer in the future, but it should be possible in principle, mostly just so this layer can be understood on its own.\n\n\nThe framework should provide a comfortable deep learning framework for someone who is strong in both Java and deep learning, who wants fine control throughout their modeling, and who is happy to learn considerable TensorFlow along the way.\n\n\nThe Keras layer should provide a comfortable deep learning API for someone who is fluent (maybe not super strong) in Java, is beginner to expert in deep learning, and wants to rely mainly on established best practice rather than controlling every detail. It should also be a reasonably comfortable transition for someone who knows the Python Keras API, although not at the expense of being clean, idiomatic Java.", "author": "deansher", "createdAt": "2020-09-06T16:25:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5MjQxNA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484092414", "bodyText": "Just an FYI, the Keras Optimizers end up just being a thin facade over the framework Optimizers. The only difference may end up being the Keras defined parameter default values used in the Keras constructor that is passed to the framework constructor. However, I still think there is value in including classes in familiar locations. like org.tensroflow.keras.optimizers, even though they are a mere facade to the corresponding framework class..", "author": "JimClarke5", "createdAt": "2020-09-06T17:00:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDE1MjE4OA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484152188", "bodyText": "Just an FYI, the Keras Optimizers end up just being a thin facade over the framework Optimizers.\n\nYes, and that is what they should be to fit in the current state of the project.\nI think @deansher did a pretty good summary of who's the audience for each library we distribute. The first customers of the core library I can think of is all the integrators of TensorFlow (DL4J, DJL, Tribuo...) that already have their own set of user libraries and API, so all they want/need is a thin layer to access the TensorFlow runtime from the JVM. I think we are actually doing good with this one.\nNow about the framework and Keras, do we really need both? If we do, then yes probably we want all initializers, metrics, etc. to be first added to the framework and then wrapped in Keras with a simplified interface, like @JimClarke5  is doing here with optimizers. But I'm wondering why a user needs to choose between an advanced API (framework) or a user-friendly one (Keras) instead of just having everything in a single library. And this might simplify/speedup our development as well.\nI'm still just throwing ideas here, for the sake of brainstorming, but maybe there should not be a Keras library at all and that everything should end up in the framework, to which we would now give a \"Keras\" taste without being bound to strictly mirror the Python implementation?", "author": "karllessard", "createdAt": "2020-09-07T02:20:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDE2MjY0Ng==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484162646", "bodyText": "Actually my plan for Tribuo is that we'll allow users to define their graphs and models in TF, and then take over the presentation of training data batches, and wrap the trained model in Tribuo's interface. Depending on how the Keras interface turns out, we might be able to accept that, or we'll allow users to use the things in frameworks to define the model structure & training algorithm.\nMy preference is to have both a low and high level framework, which is how TF python currently is. You don't need to use Keras if you don't want to, but many people do.\nOne reason to advocate for both frameworks is that it might actually take less development effort. Building out Keras to have full coverage requires a lot of consistent effort, but supporting ops that are added to TF's C API in a lower level API is essentially free for us. Adding the necessary support to keep the lower level APIs useful (i.e. by building out the optimisers and adding the odd critical operation like the sparse cross entropy) seems less effort than building out everything into Keras.", "author": "Craigacp", "createdAt": "2020-09-07T03:14:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM5ODUyOA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484398528", "bodyText": "@Craigacp when you say \"my preference is to have both a low and high level framework\", you clearly have in mind different audiences, missions, and/or characteristics of the two. Could you articulate those?", "author": "deansher", "createdAt": "2020-09-07T12:21:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQ5ODkyNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484498925", "bodyText": "And @JimClarke5 , based on your current experience, do you have any preference on this matter?", "author": "karllessard", "createdAt": "2020-09-07T15:42:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUwNzc0Nw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484507747", "bodyText": "IMHO, the beauty of Keras is in the simple, straight forward, Model and Layers. Most of the Layers have defaults for constructs like Metrics, Optimizers, Activations, etc. Also, they allow simple strings in their parameters that instruct the underlying layers  to construct elements, like new Dense(24, \"relu\"), so the way these elements are constructed can be hidden from a Keras user.   The fact that Optimizers, or other classes like Metrics, Loss, etc. are in org.tensorflow.framework versus org.tensorflow.keras does not seem that compelling to me. These elements can easily stand on their own outside of Keras. To a more sophisticated user, using org.tensorflow.framework.metrics.Mean to construct a specialized version of a Metric,  is not that different to including org.tensorflow.keras.metrics.Mean. The advantage of moving these elements to framework, is it would be clearer that they can be used in other contexts outside of Keras. The main disadvantage is that someone who is familiar with current Keras, would expect similar package names. However, this could be handled with the appropriate JavaDoc comments. So, I am leaning to add these elements to framework, similar to the way optimizers are currently postitioned.\nThe next question would then be, where is the appropriate place for Model (and Sequential) and the numerous layers, like Dense, Dropout, LSTM, etc.? These too could be positioned under framework in packages like org.tensorflow.framework.keras.model and org.tensorflow.framework.keras.layers .", "author": "JimClarke5", "createdAt": "2020-09-07T16:06:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxMDQzNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484510435", "bodyText": "By low level framework I mean the core ops, plus the optimizers that annotate the graph with the appropriate gradient optimizer, and potentially the dataset interface. That's what I'm likely to use for research, and also is probably the simplest to consume from Tribuo. Using the core ops directly without the convenience of the graph walking that the optimizers do means that the user will have to write all the graph walking code themselves which is probably too low level for most users. I think we should expose generic graph walking and modification operations at the framework level too to make it simpler for users who do want to do most things themselves.\nThe high level framework is for people who use Keras in TF Python, and want an API that guides them better. I think that we should have stronger typing information than exists in Python, as it's what would be expected from idiomatic Java and it helps IDEs & discoverability. For example, TF Keras in python has \"activations\" but these are bare functions in the activation package and specified by a String as an argument to a layer. In Java we should have a concrete interface that the activations extend from (even if they also expose the underlying function which directly applies the operation or writes it to the graph), and have the layer accept a strongly typed thing (I think this is the direction Jim is currently taking).", "author": "Craigacp", "createdAt": "2020-09-07T16:14:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDk2MjYyMw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484962623", "bodyText": "Here's my shot at capturing this discussion. Perhaps we could reach consensus on this level of description and then I'll PR it into the README?\n\n\ntensorflow-core\n\nAll artifacts that build up the core language bindings of TensorFlow for Java\nIntended audience: projects that provide their own APIs or frameworks on top of TensorFlow and just want a thin layer to access the TensorFlow runtime from the JVM\n\n\n\ntensorflow-framework\n\nComplete but fairly primitive API for building and training neural networks with TensorFlow\nIntended audience: expert neural network developers who prefer to make explicit, detailed decisions about their models and training algorithms\n\n\n\ntensorflow-keras\n\nPartially covers the framework API to allow simpler definition of models and training algorithms\nIntended to be familiar if you know the Python Keras API, but prioritizes clean, idiomatic Java over fidelity to Python\nProvides defaults based on common best practices\nAllows developers to selectively be more explicit by overriding defaults or dipping into the framework API\nIntended audience: neural network developers across the spectrum from beginner to expert who prefer to rely mostly on best-practice defaults and then selectively fine-tune\n\n\n\nndarray\n\nGeneric utility library for n-dimensional data I/O operations\nUsed by TensorFlow but does not depend on TensorFlow\nIntended audience: any developer who needs a Java n-dimensional array implementation, whether or not they use it with TensorFlow", "author": "deansher", "createdAt": "2020-09-08T14:25:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI5MjI2NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r485292265", "bodyText": "I think that we should have stronger typing information than exists in Python, as it's what would be expected from idiomatic Java and it helps IDEs & discoverability\n\nI totally support that as well, we should avoid users to pass arbitrary string labels as parameters in the Java implementation. We could though have some sort of enums per components (optimizers, metrics, ...) which will restrict the possible choices without the need for the user to instantiate explicitly that component, to get closer to what the Python implementation offers. e.g. model.compile(Optimizers.ADAM, Losses.BINARY_CROSSENTROPY)\n\nAllows developers to selectively be more explicit by overriding defaults or dipping into the framework API\n\nThis is where I'm hesitating. If we allow users to access lower level functionalities from the Keras API, why do we need another API then? The current Python API is made up of two layers because it is historically the product of a merge between two different projects: the original TF API and the Keras project. I personally think it brings more confusion to the users that benefits and we don't need to follow this schema if we think we can do better in Java since we start from scratch.\nI'm slowly leaning now to the idea of having a single API that supports both \"beginner\" and \"advanced\" modes, whether we call it Keras or not. At the same time, I don't want to slow @JimClarke5 down any further with this PR. Maybe let's continue that discussion with a broader audience? We can raise an new issue in GitHub, start another discussion on the mailing list or we could do this in the scope of a PR created by @deansher as he suggested (these descriptions sound good), what do you think?", "author": "karllessard", "createdAt": "2020-09-09T01:58:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMwNDI2Mw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r485304263", "bodyText": "I think a PR to add that to the readme sounds good, and after that's merged we can discuss it in an issue. We definitely shouldn't hold up this optimizer PR any longer.\nMy worry about forcing everything through something that's like Keras is that I train models that won't fit into a keras style model.fit call. One of the things I'm currently looking at is USE, which is trained on multiple tasks, with each task using a different combination of dataset and loss function (e.g. train a question answering loss on one dataset, a textual similarity loss on another, and an MLM loss on a third, where apart from the top layer all layers are shared). However this isn't something that is well supported by any framework I've come across (unfortunately Google didn't release any training details or code on USE), and requires a lot of complexity that most users won't need. I can just about do it in bare TF or pytorch (in python, not tried using TF Java yet as we'd need TF text to work), but anything higher level requires fighting the framework at each step because I need control of the training loop.", "author": "Craigacp", "createdAt": "2020-09-09T02:43:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTU4Njc2MA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r485586760", "bodyText": "To move forward, may I suggest the following:\n\nAdd the 3 new Optimizers, Adamax, Ftrl, and Nadam,to framework.\nMove the test cases to framework.\nHold off on the remaining classes in org.tensorflow.keras.optimizers, which are just a facade on framework, until we decide what to do with Keras.\n\nThe next PR that I will work on will allow for a changing LearningRate and only impacts Optimizers in framework.\nAfter that, I propose adding the other Keras elements, Initializers, Activations, etc., to framework.", "author": "JimClarke5", "createdAt": "2020-09-09T12:53:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTU4ODYzMQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r485588631", "bodyText": "One other issue, Keras defines the Optimizer SGD which is actually just Momentum in framework, should we just ignore this for the time being?", "author": "JimClarke5", "createdAt": "2020-09-09T12:56:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgxMjY3NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r485812675", "bodyText": "Add the 3 new Optimizers, Adamax, Ftrl, and Nadam,to framework.\n\n\nMove the test cases to framework.\n\n\nHold off on the remaining classes in org.tensorflow.keras.optimizers, which are just a facade on framework, until we decide what to do with Keras.\n\n\n\nSounds good @JimClarke5 , as in both approaches we are currently looking at, none of the optimizer backends will end up in the Keras module. Let's continue this discussion on #109 as it is becoming soon a priority that we all agree on this.\nSGD can be added later, yes.", "author": "karllessard", "createdAt": "2020-09-09T18:00:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0NzgxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0OTU0NA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483249544", "bodyText": "If we have a concrete type for our config, then either our Keras optimizers classes should be parametized to return the right type of config, or either it should be returned as an Object itself. I can't see in this PR what is the use of getConfig() so I can't give a clear answer.", "author": "karllessard", "createdAt": "2020-09-03T20:56:28Z", "path": "tensorflow-keras/src/main/java/org/tensorflow/keras/optimizers/AdaDelta.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the );\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an  BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.assertGraph;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+\n+/**\n+ * AdaDelta Optimizer that implements the AdaDelta algorithm. Keras wrapper around the Tensorflow\n+ * Framework optimizer. Adadelta optimization is a stochastic gradient descent method that is based\n+ * on adaptive learning rate per dimension to address two drawbacks: 1) the continual decay of\n+ * learning rates throughout training 2) the need for a manually selected global learning rate\n+ *\n+ * <p>Two accumulation steps are required: 1) the accumulation of gradients squared, 2) the\n+ * accumulation of updates squared.\n+ *\n+ */\n+public class AdaDelta extends org.tensorflow.framework.optimizers.AdaDelta\n+    implements OptimizerInterface {\n+\n+  public static final String LEARNING_RATE_KEY = \"learning_rate\";\n+  public static final String RHO_RATE_KEY = \"rho\";\n+  public static final String EPSILON_KEY = \"epsilon\";\n+\n+  public static final float LEARNING_RATE_DEFAULT = 0.001F;\n+  public static final float RHO_DEFAULT = 0.95F;\n+  public static final float EPSILON_DEFAULT = 1e-7F;\n+\n+  private Map<String, Object> config = new HashMap<>();\n+  private float learningRate;\n+\n+  private List<Op> initializers = new ArrayList<>();\n+\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", learning_rate=0.001F, rho=0.95F, and\n+   * epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   */\n+  public AdaDelta(Ops tf) {\n+    this(tf, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default learning_rate=0.001F, rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   */\n+  public AdaDelta(Ops tf, String name) {\n+    this(tf, name, LEARNING_RATE_DEFAULT, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\", rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, float learningRate) {\n+    this(tf, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default rho=0.95F, and epsilon=1e-7F\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate) {\n+    this(tf, name, learningRate, RHO_DEFAULT, EPSILON_DEFAULT);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer with default name=\"Adadelta\",\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param name the name of the Optimizer, defaults to \"Adadelta\"\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  public AdaDelta(Ops tf, String name, float learningRate, float rho, float epsilon) {\n+    super(assertGraph(tf), name, learningRate, rho, epsilon);\n+    initConfig(learningRate, rho, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  protected Optional<Op> prepare(String name) {\n+    switch (initializers.size()) {\n+      case 0:\n+        return Optional.empty();\n+      case 1:\n+        return Optional.of(initializers.get(0));\n+      default:\n+        return Optional.of( tf.withSubScope(name).withControlDependencies(initializers).noOp());\n+    }\n+  }\n+\n+  /**\n+   * Create an Adam Optimizer from a config object\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, he config object has keys for \"name\",\n+   *     \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the default value is used.\n+   */\n+  public static AdaDelta fromConfig(Ops tf, Map<String, Object> config) {\n+    return create(tf, config);\n+  }\n+\n+  /**\n+   * Create an Adadelta optimizer\n+   *\n+   * @param tf the tensorflow Ops\n+   * @param config a config object to initialize, the config\n+   *     object has keys for \"name\", \"learning_rate\", \"rho\" and \"epsilon\". If a key is missing the\n+   *     default value is used.\n+   */\n+  public static AdaDelta create(Ops tf, Map<String, Object> config) {\n+    String name = (String) config.get(NAME_KEY);\n+    float learningRate = (float) config.getOrDefault(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+    float rho = (float) config.getOrDefault(RHO_RATE_KEY, RHO_DEFAULT);\n+    float epsilon = (float) config.getOrDefault(EPSILON_KEY, EPSILON_DEFAULT);\n+    if (name == null) // doe this to get the default name\n+    {\n+      return new AdaDelta(tf, learningRate, rho, epsilon);\n+    } else {\n+      return new AdaDelta(tf, name, learningRate, rho, epsilon);\n+    }\n+  }\n+\n+  /**\n+   * Initialize the configuration based on which constructor is called.\n+   *\n+   * @param learningRate The learning rate\n+   * @param rho The decay rate.\n+   * @param epsilon A constant epsilon used to better conditioning the grad update.\n+   */\n+  private void initConfig(float learningRate, float rho, float epsilon) {\n+    this.learningRate = learningRate;\n+    config.put(NAME_KEY, this.getOptimizerName());\n+    config.put(LEARNING_RATE_KEY, learningRate);\n+    config.put(RHO_RATE_KEY, rho);\n+    config.put(EPSILON_KEY, epsilon);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public Map<String, Object> getConfig() {\n+    return config;", "originalCommit": "d2201df3a78e82c79b142fc7d7c3a461afa63444", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzMwODc1MA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r483308750", "bodyText": "See my previous comment on Config.", "author": "JimClarke5", "createdAt": "2020-09-03T23:41:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0OTU0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUzMTM3OQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484531379", "bodyText": "I have removed config from this based on deferring the question to when we address serializing the Keras model.", "author": "JimClarke5", "createdAt": "2020-09-07T17:35:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI0OTU0NA=="}], "type": "inlineReview"}, {"oid": "4c3cc78a999505240be35040b8865d86b955e1af", "url": "https://github.com/tensorflow/java/commit/4c3cc78a999505240be35040b8865d86b955e1af", "message": "Fixed op_generator.cc to remove a spurious new line in the generated Java files for some Ops. This also  resulted in new generated  source that are also committed.", "committedDate": "2020-09-03T22:52:11Z", "type": "commit"}, {"oid": "44f530f292fdba34164de696fb454b30108064d3", "url": "https://github.com/tensorflow/java/commit/44f530f292fdba34164de696fb454b30108064d3", "message": "Changed back to non-generic Operand until we resolve how to handle generics.", "committedDate": "2020-09-03T22:53:44Z", "type": "commit"}, {"oid": "b8d3ac2d001251c95bd55ed9cd902431108468bd", "url": "https://github.com/tensorflow/java/commit/b8d3ac2d001251c95bd55ed9cd902431108468bd", "message": "Regenerated due to creation of SoftmaxCrossEntropyWithLogits.java,  SigmoidCrossEntropyWithLogits.java, and SparseSoftmaxCrossEntropyWithLogits.java under package org.tensorflow.op.nn in", "committedDate": "2020-09-03T22:55:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDMxNTUyNA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484315524", "bodyText": "AdaDelta -> Adam", "author": "deansher", "createdAt": "2020-09-07T09:38:01Z", "path": "tensorflow-keras/src/test/java/org/tensorflow/keras/optimizers/AdamTest.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n+import static org.junit.jupiter.api.Assertions.*;\n+import org.tensorflow.Tensor;\n+import static org.tensorflow.framework.optimizers.Adam.FIRST_MOMENT;\n+import static org.tensorflow.framework.optimizers.Adam.SECOND_MOMENT;\n+import org.tensorflow.framework.optimizers.Optimizer;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import org.tensorflow.keras.utils.ND;\n+import org.tensorflow.keras.utils.TestSession;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.NdArrays;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+\n+/** Test cases for Adam Optimizer */\n+public class AdamTest {\n+  private TestSession.Mode tf_mode = TestSession.Mode.GRAPH;\n+\n+  int index;\n+\n+  public AdamTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of create method, of class Adam. */\n+  @Test\n+  public void testCreate() {\n+    try (TestSession testSession = TestSession.createTestSession(tf_mode)) {\n+      Ops tf = testSession.getTF();\n+      Map<String, Object> config = new HashMap<>();\n+      config.put(NAME_KEY, \"AdaDelta\");\n+      config.put(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+      config.put(BETA_ONE_KEY, BETA_ONE_DEFAULT);\n+      config.put(BETA_TWO_KEY, BETA_TWO_DEFAULT);\n+      config.put(EPSILON_KEY, EPSILON_DEFAULT);\n+      AdaDelta expResult = new AdaDelta(tf);", "originalCommit": "b8d3ac2d001251c95bd55ed9cd902431108468bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUwODQ2NQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484508465", "bodyText": "This test is being removed as the config has been removed until we figure out the best way to serialize a Keras model.", "author": "JimClarke5", "createdAt": "2020-09-07T16:08:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDMxNTUyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDMyNTIwMw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484325203", "bodyText": "shape1 -> shape0 on this line and the next.", "author": "deansher", "createdAt": "2020-09-07T09:54:22Z", "path": "tensorflow-keras/src/test/java/org/tensorflow/keras/optimizers/AdamTest.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n+import static org.junit.jupiter.api.Assertions.*;\n+import org.tensorflow.Tensor;\n+import static org.tensorflow.framework.optimizers.Adam.FIRST_MOMENT;\n+import static org.tensorflow.framework.optimizers.Adam.SECOND_MOMENT;\n+import org.tensorflow.framework.optimizers.Optimizer;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import org.tensorflow.keras.utils.ND;\n+import org.tensorflow.keras.utils.TestSession;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.NdArrays;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+\n+/** Test cases for Adam Optimizer */\n+public class AdamTest {\n+  private TestSession.Mode tf_mode = TestSession.Mode.GRAPH;\n+\n+  int index;\n+\n+  public AdamTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of create method, of class Adam. */\n+  @Test\n+  public void testCreate() {\n+    try (TestSession testSession = TestSession.createTestSession(tf_mode)) {\n+      Ops tf = testSession.getTF();\n+      Map<String, Object> config = new HashMap<>();\n+      config.put(NAME_KEY, \"AdaDelta\");\n+      config.put(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+      config.put(BETA_ONE_KEY, BETA_ONE_DEFAULT);\n+      config.put(BETA_TWO_KEY, BETA_TWO_DEFAULT);\n+      config.put(EPSILON_KEY, EPSILON_DEFAULT);\n+      AdaDelta expResult = new AdaDelta(tf);\n+      AdaDelta result = AdaDelta.create(tf, config);\n+      assertEquals(expResult.getConfig(), result.getConfig());\n+    }\n+  }\n+\n+  @Test\n+  public void testBasic() {\n+    float m0 = 0.0F;\n+    float v0 = 0.0F;\n+    float m1 = 0.0F;\n+    float v1 = 0.0F;\n+    float[] var0_init = {1.0F, 2.0F};\n+    float[] var1_init = {3.0F, 4.0F};\n+    float[] grads0_init = {0.1F, 0.1F};\n+    float[] grads1_init = {0.01F, 0.01F};\n+    FloatNdArray var0_np = NdArrays.vectorOf(var0_init);\n+    FloatNdArray var1_np = NdArrays.vectorOf(var1_init);\n+    FloatNdArray grads0_np = NdArrays.vectorOf(grads0_init);\n+    FloatNdArray grads1_np = NdArrays.vectorOf(grads1_init);\n+\n+    float epsilon1 = 1e-3F;\n+\n+    try (TestSession session = TestSession.createTestSession(tf_mode)) {\n+      Ops tf = session.getTF();\n+\n+      session.setEpsilon(epsilon1);\n+\n+      Shape shape0 = Shape.of(var0_init.length);\n+      Shape shape1 = Shape.of(var1_init.length);\n+      Variable<TFloat32> var0 = tf.withName(\"var0\").variable(shape0, TFloat32.DTYPE);\n+      Variable<TFloat32> var1 = tf.withName(\"var1\").variable(shape1, TFloat32.DTYPE);\n+\n+      Assign<TFloat32> var0Initializer = tf.assign(var0, tf.constant(var0_init));\n+      Assign<TFloat32> var1Initializer = tf.assign(var1, tf.constant(var1_init));\n+\n+      Constant<TFloat32> grads0 = tf.constant(grads0_init);\n+      Constant<TFloat32> grads1 = tf.constant(grads1_init);\n+\n+      /* initialize the local variables */\n+      session.run(var0Initializer);\n+      session.run(var1Initializer);\n+\n+      float learningRate = 0.001F;\n+      float beta1 = 0.9F;\n+      float beta2 = 0.999F;\n+      float epsilon = 1e-8F;\n+\n+      /* build the GradsAnvVars */\n+      List gradsAndVars = new ArrayList<>();\n+      gradsAndVars.add(new Optimizer.GradAndVar<>(grads0.asOutput(), var0.asOutput()));\n+      gradsAndVars.add(new Optimizer.GradAndVar<>(grads1.asOutput(), var1.asOutput()));\n+\n+      Adam instance = new Adam(tf, learningRate);\n+\n+      Op update = instance.applyGradients(gradsAndVars, \"AdamTest\");\n+\n+      /* Create and validae the shapes of the slota */\n+      Variable<TFloat32>[] firstMomentSlots = new Variable[2];\n+      Variable<TFloat32>[] secondMomentSlots = new Variable[2];\n+\n+      firstMomentSlots[0] = instance.getSlot(var0.asOutput(), FIRST_MOMENT).get();\n+      assertEquals(firstMomentSlots[0].asOutput().shape(), var0.asOutput().shape());\n+\n+      secondMomentSlots[0] = instance.getSlot(var0.asOutput(), SECOND_MOMENT).get();\n+      assertEquals(secondMomentSlots[0].asOutput().shape(), var0.asOutput().shape());\n+\n+      firstMomentSlots[1] = instance.getSlot(var1.asOutput(), FIRST_MOMENT).get();\n+      assertEquals(firstMomentSlots[1].asOutput().shape(), var1.asOutput().shape());\n+\n+      secondMomentSlots[1] = instance.getSlot(var1.asOutput(), SECOND_MOMENT).get();\n+      assertEquals(secondMomentSlots[1].asOutput().shape(), var1.asOutput().shape());\n+\n+      /** initialize the accumulators */\n+      session.run(tf.init());\n+\n+      session.evaluate(var0_init, var0);\n+      session.evaluate(var1_init, var1);\n+\n+      FloatNdArray m0_np = NdArrays.ofFloats(shape1);", "originalCommit": "b8d3ac2d001251c95bd55ed9cd902431108468bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxMTUxNg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484511516", "bodyText": "OK", "author": "JimClarke5", "createdAt": "2020-09-07T16:18:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDMyNTIwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM0OTYyNQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484349625", "bodyText": "1e-7F -> epsilon here and a few lines down for var1_np.\nThe fact that the test doesn't notice this difference suggests using a substantially larger epsilon, but consistency with the Python test may be more important at the moment.", "author": "deansher", "createdAt": "2020-09-07T10:41:23Z", "path": "tensorflow-keras/src/test/java/org/tensorflow/keras/optimizers/AdamTest.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n+import static org.junit.jupiter.api.Assertions.*;\n+import org.tensorflow.Tensor;\n+import static org.tensorflow.framework.optimizers.Adam.FIRST_MOMENT;\n+import static org.tensorflow.framework.optimizers.Adam.SECOND_MOMENT;\n+import org.tensorflow.framework.optimizers.Optimizer;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import org.tensorflow.keras.utils.ND;\n+import org.tensorflow.keras.utils.TestSession;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.NdArrays;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+\n+/** Test cases for Adam Optimizer */\n+public class AdamTest {\n+  private TestSession.Mode tf_mode = TestSession.Mode.GRAPH;\n+\n+  int index;\n+\n+  public AdamTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of create method, of class Adam. */\n+  @Test\n+  public void testCreate() {\n+    try (TestSession testSession = TestSession.createTestSession(tf_mode)) {\n+      Ops tf = testSession.getTF();\n+      Map<String, Object> config = new HashMap<>();\n+      config.put(NAME_KEY, \"AdaDelta\");\n+      config.put(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+      config.put(BETA_ONE_KEY, BETA_ONE_DEFAULT);\n+      config.put(BETA_TWO_KEY, BETA_TWO_DEFAULT);\n+      config.put(EPSILON_KEY, EPSILON_DEFAULT);\n+      AdaDelta expResult = new AdaDelta(tf);\n+      AdaDelta result = AdaDelta.create(tf, config);\n+      assertEquals(expResult.getConfig(), result.getConfig());\n+    }\n+  }\n+\n+  @Test\n+  public void testBasic() {\n+    float m0 = 0.0F;\n+    float v0 = 0.0F;\n+    float m1 = 0.0F;\n+    float v1 = 0.0F;\n+    float[] var0_init = {1.0F, 2.0F};\n+    float[] var1_init = {3.0F, 4.0F};\n+    float[] grads0_init = {0.1F, 0.1F};\n+    float[] grads1_init = {0.01F, 0.01F};\n+    FloatNdArray var0_np = NdArrays.vectorOf(var0_init);\n+    FloatNdArray var1_np = NdArrays.vectorOf(var1_init);\n+    FloatNdArray grads0_np = NdArrays.vectorOf(grads0_init);\n+    FloatNdArray grads1_np = NdArrays.vectorOf(grads1_init);\n+\n+    float epsilon1 = 1e-3F;\n+\n+    try (TestSession session = TestSession.createTestSession(tf_mode)) {\n+      Ops tf = session.getTF();\n+\n+      session.setEpsilon(epsilon1);\n+\n+      Shape shape0 = Shape.of(var0_init.length);\n+      Shape shape1 = Shape.of(var1_init.length);\n+      Variable<TFloat32> var0 = tf.withName(\"var0\").variable(shape0, TFloat32.DTYPE);\n+      Variable<TFloat32> var1 = tf.withName(\"var1\").variable(shape1, TFloat32.DTYPE);\n+\n+      Assign<TFloat32> var0Initializer = tf.assign(var0, tf.constant(var0_init));\n+      Assign<TFloat32> var1Initializer = tf.assign(var1, tf.constant(var1_init));\n+\n+      Constant<TFloat32> grads0 = tf.constant(grads0_init);\n+      Constant<TFloat32> grads1 = tf.constant(grads1_init);\n+\n+      /* initialize the local variables */\n+      session.run(var0Initializer);\n+      session.run(var1Initializer);\n+\n+      float learningRate = 0.001F;\n+      float beta1 = 0.9F;\n+      float beta2 = 0.999F;\n+      float epsilon = 1e-8F;\n+\n+      /* build the GradsAnvVars */\n+      List gradsAndVars = new ArrayList<>();\n+      gradsAndVars.add(new Optimizer.GradAndVar<>(grads0.asOutput(), var0.asOutput()));\n+      gradsAndVars.add(new Optimizer.GradAndVar<>(grads1.asOutput(), var1.asOutput()));\n+\n+      Adam instance = new Adam(tf, learningRate);\n+\n+      Op update = instance.applyGradients(gradsAndVars, \"AdamTest\");\n+\n+      /* Create and validae the shapes of the slota */\n+      Variable<TFloat32>[] firstMomentSlots = new Variable[2];\n+      Variable<TFloat32>[] secondMomentSlots = new Variable[2];\n+\n+      firstMomentSlots[0] = instance.getSlot(var0.asOutput(), FIRST_MOMENT).get();\n+      assertEquals(firstMomentSlots[0].asOutput().shape(), var0.asOutput().shape());\n+\n+      secondMomentSlots[0] = instance.getSlot(var0.asOutput(), SECOND_MOMENT).get();\n+      assertEquals(secondMomentSlots[0].asOutput().shape(), var0.asOutput().shape());\n+\n+      firstMomentSlots[1] = instance.getSlot(var1.asOutput(), FIRST_MOMENT).get();\n+      assertEquals(firstMomentSlots[1].asOutput().shape(), var1.asOutput().shape());\n+\n+      secondMomentSlots[1] = instance.getSlot(var1.asOutput(), SECOND_MOMENT).get();\n+      assertEquals(secondMomentSlots[1].asOutput().shape(), var1.asOutput().shape());\n+\n+      /** initialize the accumulators */\n+      session.run(tf.init());\n+\n+      session.evaluate(var0_init, var0);\n+      session.evaluate(var1_init, var1);\n+\n+      FloatNdArray m0_np = NdArrays.ofFloats(shape1);\n+      FloatNdArray v0_np = NdArrays.ofFloats(shape1);\n+      FloatNdArray m1_np = NdArrays.ofFloats(shape1);\n+      FloatNdArray v1_np = NdArrays.ofFloats(shape1);\n+\n+      for (int step = 0; step < 3; step++) {\n+\n+        // Test powers\n+        final float[] powers = {\n+          (float) Math.pow(beta1, step + 1), (float) Math.pow(beta2, step + 1)\n+        };\n+\n+        try (Tensor<TFloat32> result =\n+            session\n+                .getGraphSession()\n+                .runner()\n+                .fetch(\"beta1_power\")\n+                .run()\n+                .get(0)\n+                .expect(TFloat32.DTYPE)) {\n+          result\n+              .data()\n+              .scalars()\n+              .forEach(\n+                  f -> {\n+                    assertEquals(powers[0], f.getFloat(), epsilon1);\n+                  });\n+        }\n+        try (Tensor<TFloat32> result =\n+            session\n+                .getGraphSession()\n+                .runner()\n+                .fetch(\"beta2_power\")\n+                .run()\n+                .get(0)\n+                .expect(TFloat32.DTYPE)) {\n+          result\n+              .data()\n+              .scalars()\n+              .forEach(\n+                  f -> {\n+                    assertEquals(powers[1], f.getFloat(), epsilon1);\n+                  });\n+        }\n+        session.run(update);\n+\n+        float lr_t =\n+            learningRate\n+                * (float) Math.sqrt(1 - (float) Math.pow(beta2, (step + 1)))\n+                / (1 - (float) Math.pow(beta1, (step + 1)));\n+\n+        m0_np = calculateM(m0_np, grads0_np, beta1);\n+        v0_np = calculateV(v0_np, grads0_np, beta2);\n+        var0_np = calculateParam(var0_np, lr_t, m0_np, v0_np, 1e-7F);", "originalCommit": "b8d3ac2d001251c95bd55ed9cd902431108468bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQ5ODc1Mg==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484498752", "bodyText": "Also @JimClarke5 , don't forget please to camel-case all these variables as well, or once we'll reactivate lint checks there will be a lot of failures.", "author": "karllessard", "createdAt": "2020-09-07T15:42:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM0OTYyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUwOTA3OA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484509078", "bodyText": "This test was borrowed from the Python test, and the calculation of the expected values is not as precise as the TF calculation. Maybe there is a way to make the expected calculation  more precise.", "author": "JimClarke5", "createdAt": "2020-09-07T16:10:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM0OTYyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUzNjMzNw==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484536337", "bodyText": "OK on camel-case", "author": "JimClarke5", "createdAt": "2020-09-07T18:00:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM0OTYyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQxOTUyNA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484419524", "bodyText": "(I wouldn't propose holding up this PR for this issue.)\nThis test raises interesting questions about both our API layers and our testing strategy. It purports to be a test of keras.optimizers.Adam, but in fact it tests the actual underlying Adam logic that is exposed by framework.optimizers.Adam -- which is not, however, yet tested in framework.optimizers.\nIf we stick with the current layering strategy, I'd propose that in a future PR we move this test into framework.optimizers.", "author": "deansher", "createdAt": "2020-09-07T13:02:35Z", "path": "tensorflow-keras/src/test/java/org/tensorflow/keras/optimizers/AdamTest.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+=======================================================================*/\n+package org.tensorflow.keras.optimizers;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n+import static org.junit.jupiter.api.Assertions.*;\n+import org.tensorflow.Tensor;\n+import static org.tensorflow.framework.optimizers.Adam.FIRST_MOMENT;\n+import static org.tensorflow.framework.optimizers.Adam.SECOND_MOMENT;\n+import org.tensorflow.framework.optimizers.Optimizer;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_ONE_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.BETA_TWO_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.EPSILON_KEY;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_DEFAULT;\n+import static org.tensorflow.keras.optimizers.Adam.LEARNING_RATE_KEY;\n+import static org.tensorflow.keras.optimizers.OptimizerInterface.NAME_KEY;\n+import org.tensorflow.keras.utils.ND;\n+import org.tensorflow.keras.utils.TestSession;\n+import org.tensorflow.ndarray.FloatNdArray;\n+import org.tensorflow.ndarray.NdArrays;\n+import org.tensorflow.ndarray.Shape;\n+import org.tensorflow.op.Op;\n+import org.tensorflow.op.Ops;\n+import org.tensorflow.op.core.Assign;\n+import org.tensorflow.op.core.Constant;\n+import org.tensorflow.op.core.Variable;\n+import org.tensorflow.types.TFloat32;\n+\n+/** Test cases for Adam Optimizer */\n+public class AdamTest {\n+  private TestSession.Mode tf_mode = TestSession.Mode.GRAPH;\n+\n+  int index;\n+\n+  public AdamTest() {}\n+\n+  @BeforeAll\n+  public static void setUpClass() {}\n+\n+  @AfterAll\n+  public static void tearDownClass() {}\n+\n+  @BeforeEach\n+  public void setUp() {}\n+\n+  @AfterEach\n+  public void tearDown() {}\n+\n+  /** Test of create method, of class Adam. */\n+  @Test\n+  public void testCreate() {\n+    try (TestSession testSession = TestSession.createTestSession(tf_mode)) {\n+      Ops tf = testSession.getTF();\n+      Map<String, Object> config = new HashMap<>();\n+      config.put(NAME_KEY, \"AdaDelta\");\n+      config.put(LEARNING_RATE_KEY, LEARNING_RATE_DEFAULT);\n+      config.put(BETA_ONE_KEY, BETA_ONE_DEFAULT);\n+      config.put(BETA_TWO_KEY, BETA_TWO_DEFAULT);\n+      config.put(EPSILON_KEY, EPSILON_DEFAULT);\n+      AdaDelta expResult = new AdaDelta(tf);\n+      AdaDelta result = AdaDelta.create(tf, config);\n+      assertEquals(expResult.getConfig(), result.getConfig());\n+    }\n+  }\n+\n+  @Test\n+  public void testBasic() {", "originalCommit": "b8d3ac2d001251c95bd55ed9cd902431108468bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxMDA0NA==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484510044", "bodyText": "As of now, I would vote that these tests be moved to framework, especially in that we are moving an addition 3 optimizers to framework that are defined in Keras.  Also, the JavaDoc on the existing Optimizers need a lot of work.", "author": "JimClarke5", "createdAt": "2020-09-07T16:13:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQxOTUyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDk2NTkwMQ==", "url": "https://github.com/tensorflow/java/pull/91#discussion_r484965901", "bodyText": "Given that I have limited time availability and am a decent technical writer, that's a likely area for me to contribute.", "author": "deansher", "createdAt": "2020-09-08T14:30:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQxOTUyNA=="}], "type": "inlineReview"}, {"oid": "c32fc5be951166ffde8d8763dcc99dd7e5879e86", "url": "https://github.com/tensorflow/java/commit/c32fc5be951166ffde8d8763dcc99dd7e5879e86", "message": "change snake case to camel case. format code", "committedDate": "2020-09-07T18:15:30Z", "type": "commit"}, {"oid": "171cd2f4f9878b00e35f3175b0a8c7954ef21f07", "url": "https://github.com/tensorflow/java/commit/171cd2f4f9878b00e35f3175b0a8c7954ef21f07", "message": "clean upd warning,  format code", "committedDate": "2020-09-07T18:38:12Z", "type": "commit"}, {"oid": "e9c3134742e9155e457be157515d67b5c0bb7ac4", "url": "https://github.com/tensorflow/java/commit/e9c3134742e9155e457be157515d67b5c0bb7ac4", "message": "Added Adamax, Ftrl, and Nadam Optimizers. Added Optimizers enum for easy inclusion of a default optimizer. Cleaned up JavaDoc", "committedDate": "2020-09-09T20:14:20Z", "type": "commit"}, {"oid": "5c30a72fa335f338727358b4299e4796a211403d", "url": "https://github.com/tensorflow/java/commit/5c30a72fa335f338727358b4299e4796a211403d", "message": "Removed optimize classes from tensorflow-keras, moved optimizer test cases to framework. Created Tests for GradientDescent and Momentum", "committedDate": "2020-09-09T20:17:16Z", "type": "commit"}, {"oid": "ebefc2ea54b2f3d53b8b8f92912175eac50330d0", "url": "https://github.com/tensorflow/java/commit/ebefc2ea54b2f3d53b8b8f92912175eac50330d0", "message": "Fixed generics", "committedDate": "2020-09-09T20:17:37Z", "type": "commit"}, {"oid": "7915e6309e9db7a536cda24eac8264578cdbfe31", "url": "https://github.com/tensorflow/java/commit/7915e6309e9db7a536cda24eac8264578cdbfe31", "message": "Fixed from Unit test results", "committedDate": "2020-09-09T23:03:09Z", "type": "commit"}, {"oid": "ec4f6790ff666a4c23f60e1b4764874d6167d392", "url": "https://github.com/tensorflow/java/commit/ec4f6790ff666a4c23f60e1b4764874d6167d392", "message": "added @SuppressWarnings(\"unchecked\") on Variable array", "committedDate": "2020-09-09T23:08:41Z", "type": "commit"}]}