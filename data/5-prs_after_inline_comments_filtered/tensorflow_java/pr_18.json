{"pr_number": 18, "pr_title": "Refactor JNI code in C++ into Java code with JavaCPP", "pr_createdAt": "2020-01-20T14:31:03Z", "pr_url": "https://github.com/tensorflow/java/pull/18", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODcyNzU3NQ==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368727575", "bodyText": "So, is the good practice to always start a new pointer scope as soon as we start to allocate a few native resources? If we don't do it, will the resources be deallocated by the GC listening thread at some point?", "author": "karllessard", "createdAt": "2020-01-20T21:13:49Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/SavedModelBundle.java", "diffHunk": "@@ -163,8 +169,46 @@ private static SavedModelBundle fromHandle(\n     return new SavedModelBundle(graph, session, metaGraphDef);\n   }\n \n-  private static native SavedModelBundle load(\n-      String exportDir, String[] tags, byte[] config, byte[] runOptions);\n+  private static SavedModelBundle load(\n+      String exportDir, String[] tags, byte[] config, byte[] runOptions) {\n+    SavedModelBundle bundle = null;\n+\n+    try (PointerScope scope = new PointerScope()) {", "originalCommit": "f84af9055b3622d0bb71dfb2986c392f4e1ff564", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc3MTA0MA==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368771040", "bodyText": "Yes, the deallocators are added to a ReferenceQueue for that purpose, but as usual there is no guarantees that the GC will run at all, so we shouldn't rely on it other than as safety net.", "author": "saudet", "createdAt": "2020-01-21T00:58:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODcyNzU3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODcyODA5Mw==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368728093", "bodyText": "wildcard imports will probably fail after we introduce back Google lints to the build so let's avoid them. I think for static imports though it's fine.", "author": "karllessard", "createdAt": "2020-01-20T21:15:45Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/SavedModelBundle.java", "diffHunk": "@@ -15,6 +15,12 @@\n \n package org.tensorflow;\n \n+import static org.tensorflow.internal.c_api.global.tensorflow.*;\n+\n+import org.bytedeco.javacpp.BytePointer;\n+import org.bytedeco.javacpp.PointerScope;\n+import org.tensorflow.internal.c_api.*;", "originalCommit": "f84af9055b3622d0bb71dfb2986c392f4e1ff564", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODcyODUzNQ==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368728535", "bodyText": "I see that your current approach it to match the C variable code style in the Java code for instances of the C API classes (snake case).\nI personally would prefer that we stick to the Java standard (camel case) instead of mixing styles. Or may just add a c_ prefix to the variable name, so c_runOptions here?", "author": "karllessard", "createdAt": "2020-01-20T21:17:21Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/SavedModelBundle.java", "diffHunk": "@@ -163,8 +169,46 @@ private static SavedModelBundle fromHandle(\n     return new SavedModelBundle(graph, session, metaGraphDef);\n   }\n \n-  private static native SavedModelBundle load(\n-      String exportDir, String[] tags, byte[] config, byte[] runOptions);\n+  private static SavedModelBundle load(\n+      String exportDir, String[] tags, byte[] config, byte[] runOptions) {\n+    SavedModelBundle bundle = null;\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Status status = TF_Status.newStatus();\n+\n+      // allocate parameters for TF_LoadSessionFromSavedModel\n+      TF_SessionOptions opts = TF_SessionOptions.newSessionOptions();\n+      if (config != null && config.length > 0) {\n+        TF_SetConfig(opts, new BytePointer(config), config.length, status);\n+        status.throwExceptionIfNotOK();\n+      }\n+      TF_Buffer crun_options = null;\n+      if (runOptions != null && runOptions.length > 0) {\n+        crun_options = TF_Buffer.newBufferFromString(runOptions);", "originalCommit": "f84af9055b3622d0bb71dfb2986c392f4e1ff564", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc3MTY0NA==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368771644", "bodyText": "It's not my approach, I just left what was there from the JNI code to make it easier to understand where it comes from, but if that doesn't matter, we can change those names as well.", "author": "saudet", "createdAt": "2020-01-21T01:02:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODcyODUzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODcyOTQ5OA==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368729498", "bodyText": "don't need the else clause since you throw in your if block", "author": "karllessard", "createdAt": "2020-01-20T21:20:51Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/SavedModelBundle.java", "diffHunk": "@@ -163,8 +169,46 @@ private static SavedModelBundle fromHandle(\n     return new SavedModelBundle(graph, session, metaGraphDef);\n   }\n \n-  private static native SavedModelBundle load(\n-      String exportDir, String[] tags, byte[] config, byte[] runOptions);\n+  private static SavedModelBundle load(\n+      String exportDir, String[] tags, byte[] config, byte[] runOptions) {\n+    SavedModelBundle bundle = null;\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Status status = TF_Status.newStatus();\n+\n+      // allocate parameters for TF_LoadSessionFromSavedModel\n+      TF_SessionOptions opts = TF_SessionOptions.newSessionOptions();\n+      if (config != null && config.length > 0) {\n+        TF_SetConfig(opts, new BytePointer(config), config.length, status);\n+        status.throwExceptionIfNotOK();\n+      }\n+      TF_Buffer crun_options = null;\n+      if (runOptions != null && runOptions.length > 0) {\n+        crun_options = TF_Buffer.newBufferFromString(runOptions);\n+      }\n+\n+      // load the session\n+      TF_Graph graph = TF_Graph.newGraph();\n+      TF_Buffer metagraph_def = TF_Buffer.newBuffer();\n+      TF_Session session = TF_Session.loadSessionFromSavedModel(\n+          opts, crun_options, exportDir, tags, graph,\n+          metagraph_def, status);\n+      status.throwExceptionIfNotOK();\n+\n+      // handle the result\n+      if (metagraph_def.length() > Integer.MAX_VALUE) {\n+        throw new IndexOutOfBoundsException(\"MetaGraphDef is too large to serialize into a byte[] array\");\n+      } else {", "originalCommit": "f84af9055b3622d0bb71dfb2986c392f4e1ff564", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODczOTQ5Mw==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368739493", "bodyText": "I suggest we move this array copy stunt in AbstractTF_Buffer as a utility method like toBytes(), since TF_Buffer is meant to be generic.\nAlternatively, we could retain a reference to metaGraphDef and return its data as a ByteBuffer to the user instead of an array, as it is only used for deserializing the proto message... But I'm wondering why we just don't import those proto classes in the core API and return typed objects to the user like MetaGraphDef instead of leaving him the burden to serialize/deserialize the proto messages.\nWhat do you think @sjamesr ? Is the original reason of leaving the protos outside the Java client was only to avoid an extra dependency to grpc?", "author": "karllessard", "createdAt": "2020-01-20T22:02:05Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/SavedModelBundle.java", "diffHunk": "@@ -163,8 +169,46 @@ private static SavedModelBundle fromHandle(\n     return new SavedModelBundle(graph, session, metaGraphDef);\n   }\n \n-  private static native SavedModelBundle load(\n-      String exportDir, String[] tags, byte[] config, byte[] runOptions);\n+  private static SavedModelBundle load(\n+      String exportDir, String[] tags, byte[] config, byte[] runOptions) {\n+    SavedModelBundle bundle = null;\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Status status = TF_Status.newStatus();\n+\n+      // allocate parameters for TF_LoadSessionFromSavedModel\n+      TF_SessionOptions opts = TF_SessionOptions.newSessionOptions();\n+      if (config != null && config.length > 0) {\n+        TF_SetConfig(opts, new BytePointer(config), config.length, status);\n+        status.throwExceptionIfNotOK();\n+      }\n+      TF_Buffer crun_options = null;\n+      if (runOptions != null && runOptions.length > 0) {\n+        crun_options = TF_Buffer.newBufferFromString(runOptions);\n+      }\n+\n+      // load the session\n+      TF_Graph graph = TF_Graph.newGraph();\n+      TF_Buffer metagraph_def = TF_Buffer.newBuffer();\n+      TF_Session session = TF_Session.loadSessionFromSavedModel(\n+          opts, crun_options, exportDir, tags, graph,\n+          metagraph_def, status);\n+      status.throwExceptionIfNotOK();\n+\n+      // handle the result\n+      if (metagraph_def.length() > Integer.MAX_VALUE) {\n+        throw new IndexOutOfBoundsException(\"MetaGraphDef is too large to serialize into a byte[] array\");\n+      } else {\n+        byte[] jmetagraph_def = new byte[(int)metagraph_def.length()];\n+        new BytePointer(metagraph_def.data()).get(jmetagraph_def);", "originalCommit": "f84af9055b3622d0bb71dfb2986c392f4e1ff564", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc2MTkyNA==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368761924", "bodyText": "Just for my understanding, so when Graph and Session will support receiving directly an instance of TF_Graph/TF_Session respectively instead of an address, retainReference() will still be required but deallocate(false) won't be?", "author": "karllessard", "createdAt": "2020-01-20T23:58:10Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/SavedModelBundle.java", "diffHunk": "@@ -163,8 +169,46 @@ private static SavedModelBundle fromHandle(\n     return new SavedModelBundle(graph, session, metaGraphDef);\n   }\n \n-  private static native SavedModelBundle load(\n-      String exportDir, String[] tags, byte[] config, byte[] runOptions);\n+  private static SavedModelBundle load(\n+      String exportDir, String[] tags, byte[] config, byte[] runOptions) {\n+    SavedModelBundle bundle = null;\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Status status = TF_Status.newStatus();\n+\n+      // allocate parameters for TF_LoadSessionFromSavedModel\n+      TF_SessionOptions opts = TF_SessionOptions.newSessionOptions();\n+      if (config != null && config.length > 0) {\n+        TF_SetConfig(opts, new BytePointer(config), config.length, status);\n+        status.throwExceptionIfNotOK();\n+      }\n+      TF_Buffer crun_options = null;\n+      if (runOptions != null && runOptions.length > 0) {\n+        crun_options = TF_Buffer.newBufferFromString(runOptions);\n+      }\n+\n+      // load the session\n+      TF_Graph graph = TF_Graph.newGraph();\n+      TF_Buffer metagraph_def = TF_Buffer.newBuffer();\n+      TF_Session session = TF_Session.loadSessionFromSavedModel(\n+          opts, crun_options, exportDir, tags, graph,\n+          metagraph_def, status);\n+      status.throwExceptionIfNotOK();\n+\n+      // handle the result\n+      if (metagraph_def.length() > Integer.MAX_VALUE) {\n+        throw new IndexOutOfBoundsException(\"MetaGraphDef is too large to serialize into a byte[] array\");\n+      } else {\n+        byte[] jmetagraph_def = new byte[(int)metagraph_def.length()];\n+        new BytePointer(metagraph_def.data()).get(jmetagraph_def);\n+        bundle = fromHandle(graph.address(), session.address(), jmetagraph_def);\n+        graph.retainReference().deallocate(false);\n+        session.retainReference().deallocate(false);", "originalCommit": "f84af9055b3622d0bb71dfb2986c392f4e1ff564", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc3Mjc3Nw==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368772777", "bodyText": "If we keep around the original Pointer objects, yes that's the basic idea. But ideally we shouldn't try to manage references manually. We can add PointerScope fields to Session and Graph and do something like attach(graph) and attach(session) to those, and whatever else is associated with those objects, and call scope.close() in their close() methods. Anyway, like I said, this is all stuff we can look back at in a second phase.", "author": "saudet", "createdAt": "2020-01-21T01:08:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc2MTkyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4MzM2Ng==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368783366", "bodyText": "Yes, I just wanted to know how you were foreseeing it, cool", "author": "karllessard", "createdAt": "2020-01-21T02:10:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc2MTkyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc2MjQ2Nw==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368762467", "bodyText": "I'm not sure to understand why we need to set explicitly all the fields here, aren't they initialized properly by the TF_LoadSessionFromSavedModel call?", "author": "karllessard", "createdAt": "2020-01-21T00:01:33Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/internal/c_api/AbstractTF_Session.java", "diffHunk": "@@ -53,6 +63,25 @@ public static TF_Session newSession(TF_Graph graph, TF_SessionOptions opts, TF_S\n         return s;\n     }\n \n+    /**\n+     * Calls TF_LoadSessionFromSavedModel(), and registers a deallocator.\n+     * @return TF_Session created. Do not call TF_DeleteSession() on it.\n+     */\n+    public static TF_Session loadSessionFromSavedModel(TF_SessionOptions session_options, TF_Buffer run_options,\n+        String export_dir, String[] tags, TF_Graph graph, TF_Buffer meta_graph_def, TF_Status status) {\n+        TF_Session s = TF_LoadSessionFromSavedModel(session_options, run_options,\n+                new BytePointer(export_dir), new PointerPointer(tags), tags.length, graph, meta_graph_def, status);\n+        if (s != null) {\n+            s.graph = graph;\n+            s.opts = session_options;\n+            s.run_options = run_options;\n+            s.meta_graph_def = meta_graph_def;\n+            s.status = status;", "originalCommit": "f84af9055b3622d0bb71dfb2986c392f4e1ff564", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc3MjkwMA==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368772900", "bodyText": "No, TF_LoadSessionFromSavedModel() is just a C function, it can't keep references to Java objects.", "author": "saudet", "createdAt": "2020-01-21T01:09:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc2MjQ2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4NDc5MA==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368784790", "bodyText": "... oh ok, those fields are declared in the Java class, not the original C++ struct, got it.", "author": "karllessard", "createdAt": "2020-01-21T02:18:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc2MjQ2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4NTU3Mw==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r368785573", "bodyText": "But again, it's not really required, it's just to try to \"rein in\" the garbage collector a bit more...", "author": "saudet", "createdAt": "2020-01-21T02:22:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc2MjQ2Nw=="}], "type": "inlineReview"}, {"oid": "534e9d49eb0767d30ad5edf4d54f4512db97aa6c", "url": "https://github.com/tensorflow/java/commit/534e9d49eb0767d30ad5edf4d54f4512db97aa6c", "message": "Refactor saved_model_bundle_jni into SavedModelBundle with JavaCPP", "committedDate": "2020-01-28T00:26:39Z", "type": "commit"}, {"oid": "e69874cf80500b8b44613865077350b32e08da8c", "url": "https://github.com/tensorflow/java/commit/e69874cf80500b8b44613865077350b32e08da8c", "message": "Fix code in response to review comments", "committedDate": "2020-01-28T00:26:39Z", "type": "commit"}, {"oid": "b8ff850a93d921cec7aa4ead4f48050296dcf5f7", "url": "https://github.com/tensorflow/java/commit/b8ff850a93d921cec7aa4ead4f48050296dcf5f7", "message": "Refactor the rest of the JNI code into Java with JavaCPP", "committedDate": "2020-01-28T00:27:27Z", "type": "commit"}, {"oid": "55b7b5be984bf709882ead204cc4e1833b98d104", "url": "https://github.com/tensorflow/java/commit/55b7b5be984bf709882ead204cc4e1833b98d104", "message": "Fix formatting of switch statements and update URL in error message", "committedDate": "2020-01-28T03:33:06Z", "type": "commit"}, {"oid": "55b7b5be984bf709882ead204cc4e1833b98d104", "url": "https://github.com/tensorflow/java/commit/55b7b5be984bf709882ead204cc4e1833b98d104", "message": "Fix formatting of switch statements and update URL in error message", "committedDate": "2020-01-28T03:33:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE1NDA0Nw==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372154047", "bodyText": "All those NativeReference should be replaced at some point by JavaCPP managed pointers but that could probably be done in a second phase, I think you were referring to that as well in a previous post.", "author": "karllessard", "createdAt": "2020-01-29T01:53:12Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/EagerOperation.java", "diffHunk": "@@ -131,43 +145,104 @@ public Shape shape(int outputIndex) {\n   private static class NativeReference extends EagerSession.NativeReference {", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE1NDI2MQ==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372154261", "bodyText": "Nit: both exception message should be the same, looks like \"Eager session\" is the most consistent choice", "author": "karllessard", "createdAt": "2020-01-29T01:54:05Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/EagerOperation.java", "diffHunk": "@@ -131,43 +145,104 @@ public Shape shape(int outputIndex) {\n   private static class NativeReference extends EagerSession.NativeReference {\n \n     NativeReference(\n-        EagerSession session, EagerOperation operation, long opHandle, long[] outputHandles) {\n+        EagerSession session, EagerOperation operation, TFE_Op opHandle, TFE_TensorHandle[] outputHandles) {\n       super(session, operation);\n       this.opHandle = opHandle;\n       this.outputHandles = outputHandles;\n     }\n \n     @Override\n     void delete() {\n-      if (opHandle != 0L) {\n+      if (opHandle != null && !opHandle.isNull()) {\n         for (int i = 0; i < outputHandles.length; ++i) {\n-          if (outputHandles[i] != 0L) {\n+          if (outputHandles[i] != null && !outputHandles[i].isNull()) {\n             EagerOperation.deleteTensorHandle(outputHandles[i]);\n-            outputHandles[i] = 0L;\n+            outputHandles[i] = null;\n           }\n         }\n         EagerOperation.delete(opHandle);\n-        opHandle = 0L;\n+        opHandle = null;\n       }\n     }\n \n-    private long opHandle;\n-    private final long[] outputHandles;\n+    private TFE_Op opHandle;\n+    private final TFE_TensorHandle[] outputHandles;\n   }\n-  \n-  private static native void delete(long handle);\n \n-  private static native void deleteTensorHandle(long handle);\n+  private static void requireOp(TFE_Op handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new IllegalStateException(\"Eager session has been closed\");\n+    }\n+  }\n \n-  private static native long resolveTensorHandle(long handle);\n+  private static void requireTensorHandle(TFE_TensorHandle handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new IllegalStateException(\"EagerSession has been closed\");", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2OTY1Mg==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372169652", "bodyText": "I just copy/pasted...", "author": "saudet", "createdAt": "2020-01-29T03:08:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE1NDI2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE1NTE0OQ==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372155149", "bodyText": "ok for my understanding again, the status is attached to the scope so will be freed when the latter is closed. But what about tensor, isn't it attached as well? Then how does it remains alive if we do not specify explicitly retain()?", "author": "karllessard", "createdAt": "2020-01-29T01:57:57Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/EagerOperation.java", "diffHunk": "@@ -131,43 +145,104 @@ public Shape shape(int outputIndex) {\n   private static class NativeReference extends EagerSession.NativeReference {\n \n     NativeReference(\n-        EagerSession session, EagerOperation operation, long opHandle, long[] outputHandles) {\n+        EagerSession session, EagerOperation operation, TFE_Op opHandle, TFE_TensorHandle[] outputHandles) {\n       super(session, operation);\n       this.opHandle = opHandle;\n       this.outputHandles = outputHandles;\n     }\n \n     @Override\n     void delete() {\n-      if (opHandle != 0L) {\n+      if (opHandle != null && !opHandle.isNull()) {\n         for (int i = 0; i < outputHandles.length; ++i) {\n-          if (outputHandles[i] != 0L) {\n+          if (outputHandles[i] != null && !outputHandles[i].isNull()) {\n             EagerOperation.deleteTensorHandle(outputHandles[i]);\n-            outputHandles[i] = 0L;\n+            outputHandles[i] = null;\n           }\n         }\n         EagerOperation.delete(opHandle);\n-        opHandle = 0L;\n+        opHandle = null;\n       }\n     }\n \n-    private long opHandle;\n-    private final long[] outputHandles;\n+    private TFE_Op opHandle;\n+    private final TFE_TensorHandle[] outputHandles;\n   }\n-  \n-  private static native void delete(long handle);\n \n-  private static native void deleteTensorHandle(long handle);\n+  private static void requireOp(TFE_Op handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new IllegalStateException(\"Eager session has been closed\");\n+    }\n+  }\n \n-  private static native long resolveTensorHandle(long handle);\n+  private static void requireTensorHandle(TFE_TensorHandle handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new IllegalStateException(\"EagerSession has been closed\");\n+    }\n+  }\n \n-  private static native int outputListLength(long handle, String name);\n+  private static void delete(TFE_Op handle) {\n+    if (handle == null || handle.isNull()) return;\n+    TFE_DeleteOp(handle);\n+  }\n \n-  private static native int inputListLength(long handle, String name);\n+  private static void deleteTensorHandle(TFE_TensorHandle handle) {\n+    if (handle == null || handle.isNull()) return;\n+    TFE_DeleteTensorHandle(handle);\n+  }\n \n-  private static native int dataType(long handle);\n+  private static TF_Tensor resolveTensorHandle(TFE_TensorHandle handle) {\n+    requireTensorHandle(handle);\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Status status = TF_Status.newStatus();\n+      TF_Tensor tensor = TFE_TensorHandleResolve(handle, status);\n+      status.throwExceptionIfNotOK();\n+      return tensor;", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE3MDY1Mg==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372170652", "bodyText": "The convention here is that we leave original C/C++ functions behave like they do in C/C++, where they return an integer for pointer addresses and what not. Then we add manually factory methods like TF_Status.newStatus() that wrap things up with deallocators to support garbage collection and reference counting, calling them only when we want those features.", "author": "saudet", "createdAt": "2020-01-29T03:13:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE1NTE0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE1NTczMQ==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372155731", "bodyText": "Instead of starting new scopes in all of these methods, could it be simpler and more efficient to just create the status in a try-with-resource block when there are not other resource allocated?\ntry (TF_Status status = TF_Status.newStatus()) { \n    ... \n}", "author": "karllessard", "createdAt": "2020-01-29T02:00:49Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/EagerOperation.java", "diffHunk": "@@ -131,43 +145,104 @@ public Shape shape(int outputIndex) {\n   private static class NativeReference extends EagerSession.NativeReference {\n \n     NativeReference(\n-        EagerSession session, EagerOperation operation, long opHandle, long[] outputHandles) {\n+        EagerSession session, EagerOperation operation, TFE_Op opHandle, TFE_TensorHandle[] outputHandles) {\n       super(session, operation);\n       this.opHandle = opHandle;\n       this.outputHandles = outputHandles;\n     }\n \n     @Override\n     void delete() {\n-      if (opHandle != 0L) {\n+      if (opHandle != null && !opHandle.isNull()) {\n         for (int i = 0; i < outputHandles.length; ++i) {\n-          if (outputHandles[i] != 0L) {\n+          if (outputHandles[i] != null && !outputHandles[i].isNull()) {\n             EagerOperation.deleteTensorHandle(outputHandles[i]);\n-            outputHandles[i] = 0L;\n+            outputHandles[i] = null;\n           }\n         }\n         EagerOperation.delete(opHandle);\n-        opHandle = 0L;\n+        opHandle = null;\n       }\n     }\n \n-    private long opHandle;\n-    private final long[] outputHandles;\n+    private TFE_Op opHandle;\n+    private final TFE_TensorHandle[] outputHandles;\n   }\n-  \n-  private static native void delete(long handle);\n \n-  private static native void deleteTensorHandle(long handle);\n+  private static void requireOp(TFE_Op handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new IllegalStateException(\"Eager session has been closed\");\n+    }\n+  }\n \n-  private static native long resolveTensorHandle(long handle);\n+  private static void requireTensorHandle(TFE_TensorHandle handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new IllegalStateException(\"EagerSession has been closed\");\n+    }\n+  }\n \n-  private static native int outputListLength(long handle, String name);\n+  private static void delete(TFE_Op handle) {\n+    if (handle == null || handle.isNull()) return;\n+    TFE_DeleteOp(handle);\n+  }\n \n-  private static native int inputListLength(long handle, String name);\n+  private static void deleteTensorHandle(TFE_TensorHandle handle) {\n+    if (handle == null || handle.isNull()) return;\n+    TFE_DeleteTensorHandle(handle);\n+  }\n \n-  private static native int dataType(long handle);\n+  private static TF_Tensor resolveTensorHandle(TFE_TensorHandle handle) {\n+    requireTensorHandle(handle);\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Status status = TF_Status.newStatus();\n+      TF_Tensor tensor = TFE_TensorHandleResolve(handle, status);\n+      status.throwExceptionIfNotOK();\n+      return tensor;\n+    }\n+  }\n \n-  private static native int numDims(long handle);\n+  private static int outputListLength(TFE_Op handle, String name) {\n+    requireOp(handle);\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Status status = TF_Status.newStatus();\n+      int length = TFE_OpGetOutputLength(handle, name, status);\n+      status.throwExceptionIfNotOK();\n+      return length;\n+    }", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE3MTQwNQ==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372171405", "bodyText": "Yes, it would be more efficient, but it would also make it more error-prone when we start creating other objects in there that may start doing temporary allocations.", "author": "saudet", "createdAt": "2020-01-29T03:17:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE1NTczMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjM5MzQ5OQ==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372393499", "bodyText": "Yeah... I would still prefer we go with the most efficient approach but it's up to you if you want to make the changes or not, we can merge it like this too.", "author": "karllessard", "createdAt": "2020-01-29T13:53:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE1NTczMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2MDkyOQ==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372160929", "bodyText": "Nit: camel case", "author": "karllessard", "createdAt": "2020-01-29T02:25:45Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Graph.java", "diffHunk": "@@ -448,38 +470,227 @@ public void remove() {\n     private int position;\n   }\n \n-  private static native long allocate();\n+  private static TF_Graph allocate() {\n+      return TF_NewGraph();\n+  }\n+\n+  private static void delete(TF_Graph handle) {\n+    if (handle == null || handle.isNull()) return;\n+    TF_DeleteGraph(handle);\n+  }\n \n-  private static native void delete(long handle);\n+  private static void requireHandle(Pointer handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new IllegalStateException(\"close() has been called on the Graph\");\n+    }\n+  }\n \n-  private static native long operation(long handle, String name);\n+  private static TF_Operation operation(TF_Graph handle, String name) {\n+    requireHandle(handle);\n+    return TF_GraphOperationByName(handle, name);\n+  }\n \n   // This method returns the Operation native handle at index 0 and the new value for pos at index 1\n   // (see TF_GraphNextOperation)\n-  private static native long[] nextOperation(long handle, int position);\n+  private static Object[] nextOperation(TF_Graph handle, int position) {\n+    requireHandle(handle);\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      SizeTPointer pos = new SizeTPointer(1).put(position);\n+      TF_Operation operation = TF_GraphNextOperation(handle, pos);\n+      if (operation == null || operation.isNull()) return null;\n+\n+      Object[] handleAndPosition = new Object[2];\n+      handleAndPosition[0] = operation;\n+      handleAndPosition[1] = (int)pos.get();\n+      return handleAndPosition;\n+    }\n+  }\n+\n+  private static void importGraphDef(TF_Graph handle, byte[] graphDef, String prefix)\n+      throws IllegalArgumentException {\n+    requireHandle(handle);\n+\n+    // Continue cleaning up resources even if an exception was thrown.\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_ImportGraphDefOptions opts = TF_ImportGraphDefOptions.newImportGraphDefOptions();\n+\n+      TF_ImportGraphDefOptionsSetPrefix(opts, prefix);\n \n-  private static native void importGraphDef(long handle, byte[] graphDef, String prefix)\n-      throws IllegalArgumentException;\n+      TF_Buffer buf = TF_Buffer.newBufferFromString(graphDef);\n+      TF_Status status = TF_Status.newStatus();\n \n-  private static native byte[] toGraphDef(long handle);\n+      TF_GraphImportGraphDef(handle, buf, opts, status);\n+      status.throwExceptionIfNotOK();\n+    }\n+  }\n+\n+  private static byte[] toGraphDef(TF_Graph handle) {\n+    requireHandle(handle);\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Buffer buf = TF_Buffer.newBuffer();\n+      TF_Status status = TF_Status.newStatus();\n+      TF_GraphToGraphDef(handle, buf, status);\n+      status.throwExceptionIfNotOK();\n+      return buf.get();\n+    }\n+  }\n+\n+  static void resolveOutputs(String type, TF_Operation[] srcOps,\n+                             int[] srcIndices, TF_Output dst, int n) {\n+    if (srcOps.length != n) {\n+      throw new IllegalArgumentException(\"expected \" + n + \", got \" + srcOps.length + \" \" + type + \" Operations\");\n+    }\n+    if (srcIndices.length != n) {\n+      throw new IllegalArgumentException(\"expected \" + n + \", got \" + srcIndices.length + \" \" + type + \" Operation output indices\");\n+    }\n+    for (int i = 0; i < n; ++i) {\n+      if (srcOps[i] == null || srcOps[i].isNull()) {\n+        throw new NullPointerException(\"invalid \" + type + \" (#\" + i + \" of \" + n + \")\");\n+      }\n+      dst.position(i).oper(srcOps[i]).index(srcIndices[i]);\n+    }\n+    dst.position(0);\n+  }\n \n-  private static native long[] addGradients(\n-      long handle,\n+  private static Object[] addGradients(\n+      TF_Graph handle,\n       String prefix,\n-      long[] inputHandles,\n+      TF_Operation[] inputHandles,\n       int[] inputIndices,\n-      long[] outputHandles,\n+      TF_Operation[] outputHandles,\n       int[] outputIndices,\n-      long[] gradInputHandles,\n-      int[] gradInputIndices);\n+      TF_Operation[] gradInputHandles,\n+      int[] gradInputIndices) {\n+    requireHandle(handle);\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      int ny = inputHandles.length;\n+      int nx = outputHandles.length;\n+\n+      TF_Output y = new TF_Output(ny);\n+      TF_Output x = new TF_Output(nx);\n+      TF_Output dx = null;\n+      TF_Output dy = new TF_Output(nx);\n+\n+      resolveOutputs(\"y\", inputHandles, inputIndices, y, ny);\n+      resolveOutputs(\"x\", outputHandles, outputIndices, x, nx);\n+      if (gradInputHandles != null) {\n+        if (gradInputHandles.length != ny) {\n+          throw new IllegalArgumentException(\"expected \" + ny + \", got \" + gradInputHandles.length + \" handles\");\n+        }\n+        dx = new TF_Output(ny);\n+        resolveOutputs(\"dx\", gradInputHandles, gradInputIndices, dx, ny);\n+      }\n+\n+      TF_Status status = TF_Status.newStatus();\n+      TF_AddGradientsWithPrefix(handle, prefix, y, ny, x, nx, dx, status, dy);\n+      status.throwExceptionIfNotOK();\n+\n+      // returned array contains both op handles and output indices, in pair\n+      Object[] gradOutputHandlesAndIndices = new Object[nx * 2];\n+      for (int i = 0, j = nx; i < nx; ++i, ++j) {\n+        TF_Output gradOutput = dy.position(i);\n+        gradOutputHandlesAndIndices[i] = gradOutput.oper();\n+        gradOutputHandlesAndIndices[j] = gradOutput.index();\n+      }\n+      return gradOutputHandlesAndIndices;\n+    }\n+  }\n \n-  private static native long[] whileLoop(\n-      long handle,\n-      long[] inputHandles,\n+  private static Object[] whileLoop(\n+      TF_Graph handle,\n+      TF_Operation[] inputHandles,\n       int[] inputIndices,\n       String name,\n       WhileSubgraphBuilder condGraphBuilder,\n-      WhileSubgraphBuilder bodyGraphBuilder);\n+      WhileSubgraphBuilder bodyGraphBuilder) {\n+    requireHandle(handle);\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Status status = TF_Status.newStatus();\n+\n+      int ninputs = inputHandles.length;\n+\n+      TF_Output inputs = new TF_Output(ninputs);\n+      resolveOutputs(\"inputs\", inputHandles, inputIndices, inputs, ninputs);\n+\n+      // initialize while params\n+      TF_WhileParams params = TF_NewWhile(handle, inputs, ninputs, status);\n+      status.throwExceptionIfNotOK();\n+\n+      // build conditional subgraph\n+      TF_Output condInputsOutput = params.cond_inputs();\n+      TF_Output condOutputOutput = params.cond_output();\n+      TF_Operation[] condInputHandles = new TF_Operation[ninputs];\n+      int[] condInputIndices = new int[ninputs];\n+      TF_Operation[] condOutputHandles = new TF_Operation[1];\n+      int[] condOutputIndices = new int[1];\n+      for (int i = 0; i < ninputs; i++) {\n+          condInputHandles[i] = condInputsOutput.position(i).oper();\n+          condInputIndices[i] = condInputsOutput.position(i).index();\n+      }\n+      condOutputHandles[0] = condOutputOutput.oper();\n+      condOutputIndices[0] = condOutputOutput.index();\n+\n+      Object[] cond_output_handles_and_indices =", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2MDk4NQ==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372160985", "bodyText": "Nit: camel case", "author": "karllessard", "createdAt": "2020-01-29T02:25:59Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Graph.java", "diffHunk": "@@ -448,38 +470,227 @@ public void remove() {\n     private int position;\n   }\n \n-  private static native long allocate();\n+  private static TF_Graph allocate() {\n+      return TF_NewGraph();\n+  }\n+\n+  private static void delete(TF_Graph handle) {\n+    if (handle == null || handle.isNull()) return;\n+    TF_DeleteGraph(handle);\n+  }\n \n-  private static native void delete(long handle);\n+  private static void requireHandle(Pointer handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new IllegalStateException(\"close() has been called on the Graph\");\n+    }\n+  }\n \n-  private static native long operation(long handle, String name);\n+  private static TF_Operation operation(TF_Graph handle, String name) {\n+    requireHandle(handle);\n+    return TF_GraphOperationByName(handle, name);\n+  }\n \n   // This method returns the Operation native handle at index 0 and the new value for pos at index 1\n   // (see TF_GraphNextOperation)\n-  private static native long[] nextOperation(long handle, int position);\n+  private static Object[] nextOperation(TF_Graph handle, int position) {\n+    requireHandle(handle);\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      SizeTPointer pos = new SizeTPointer(1).put(position);\n+      TF_Operation operation = TF_GraphNextOperation(handle, pos);\n+      if (operation == null || operation.isNull()) return null;\n+\n+      Object[] handleAndPosition = new Object[2];\n+      handleAndPosition[0] = operation;\n+      handleAndPosition[1] = (int)pos.get();\n+      return handleAndPosition;\n+    }\n+  }\n+\n+  private static void importGraphDef(TF_Graph handle, byte[] graphDef, String prefix)\n+      throws IllegalArgumentException {\n+    requireHandle(handle);\n+\n+    // Continue cleaning up resources even if an exception was thrown.\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_ImportGraphDefOptions opts = TF_ImportGraphDefOptions.newImportGraphDefOptions();\n+\n+      TF_ImportGraphDefOptionsSetPrefix(opts, prefix);\n \n-  private static native void importGraphDef(long handle, byte[] graphDef, String prefix)\n-      throws IllegalArgumentException;\n+      TF_Buffer buf = TF_Buffer.newBufferFromString(graphDef);\n+      TF_Status status = TF_Status.newStatus();\n \n-  private static native byte[] toGraphDef(long handle);\n+      TF_GraphImportGraphDef(handle, buf, opts, status);\n+      status.throwExceptionIfNotOK();\n+    }\n+  }\n+\n+  private static byte[] toGraphDef(TF_Graph handle) {\n+    requireHandle(handle);\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Buffer buf = TF_Buffer.newBuffer();\n+      TF_Status status = TF_Status.newStatus();\n+      TF_GraphToGraphDef(handle, buf, status);\n+      status.throwExceptionIfNotOK();\n+      return buf.get();\n+    }\n+  }\n+\n+  static void resolveOutputs(String type, TF_Operation[] srcOps,\n+                             int[] srcIndices, TF_Output dst, int n) {\n+    if (srcOps.length != n) {\n+      throw new IllegalArgumentException(\"expected \" + n + \", got \" + srcOps.length + \" \" + type + \" Operations\");\n+    }\n+    if (srcIndices.length != n) {\n+      throw new IllegalArgumentException(\"expected \" + n + \", got \" + srcIndices.length + \" \" + type + \" Operation output indices\");\n+    }\n+    for (int i = 0; i < n; ++i) {\n+      if (srcOps[i] == null || srcOps[i].isNull()) {\n+        throw new NullPointerException(\"invalid \" + type + \" (#\" + i + \" of \" + n + \")\");\n+      }\n+      dst.position(i).oper(srcOps[i]).index(srcIndices[i]);\n+    }\n+    dst.position(0);\n+  }\n \n-  private static native long[] addGradients(\n-      long handle,\n+  private static Object[] addGradients(\n+      TF_Graph handle,\n       String prefix,\n-      long[] inputHandles,\n+      TF_Operation[] inputHandles,\n       int[] inputIndices,\n-      long[] outputHandles,\n+      TF_Operation[] outputHandles,\n       int[] outputIndices,\n-      long[] gradInputHandles,\n-      int[] gradInputIndices);\n+      TF_Operation[] gradInputHandles,\n+      int[] gradInputIndices) {\n+    requireHandle(handle);\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      int ny = inputHandles.length;\n+      int nx = outputHandles.length;\n+\n+      TF_Output y = new TF_Output(ny);\n+      TF_Output x = new TF_Output(nx);\n+      TF_Output dx = null;\n+      TF_Output dy = new TF_Output(nx);\n+\n+      resolveOutputs(\"y\", inputHandles, inputIndices, y, ny);\n+      resolveOutputs(\"x\", outputHandles, outputIndices, x, nx);\n+      if (gradInputHandles != null) {\n+        if (gradInputHandles.length != ny) {\n+          throw new IllegalArgumentException(\"expected \" + ny + \", got \" + gradInputHandles.length + \" handles\");\n+        }\n+        dx = new TF_Output(ny);\n+        resolveOutputs(\"dx\", gradInputHandles, gradInputIndices, dx, ny);\n+      }\n+\n+      TF_Status status = TF_Status.newStatus();\n+      TF_AddGradientsWithPrefix(handle, prefix, y, ny, x, nx, dx, status, dy);\n+      status.throwExceptionIfNotOK();\n+\n+      // returned array contains both op handles and output indices, in pair\n+      Object[] gradOutputHandlesAndIndices = new Object[nx * 2];\n+      for (int i = 0, j = nx; i < nx; ++i, ++j) {\n+        TF_Output gradOutput = dy.position(i);\n+        gradOutputHandlesAndIndices[i] = gradOutput.oper();\n+        gradOutputHandlesAndIndices[j] = gradOutput.index();\n+      }\n+      return gradOutputHandlesAndIndices;\n+    }\n+  }\n \n-  private static native long[] whileLoop(\n-      long handle,\n-      long[] inputHandles,\n+  private static Object[] whileLoop(\n+      TF_Graph handle,\n+      TF_Operation[] inputHandles,\n       int[] inputIndices,\n       String name,\n       WhileSubgraphBuilder condGraphBuilder,\n-      WhileSubgraphBuilder bodyGraphBuilder);\n+      WhileSubgraphBuilder bodyGraphBuilder) {\n+    requireHandle(handle);\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Status status = TF_Status.newStatus();\n+\n+      int ninputs = inputHandles.length;\n+\n+      TF_Output inputs = new TF_Output(ninputs);\n+      resolveOutputs(\"inputs\", inputHandles, inputIndices, inputs, ninputs);\n+\n+      // initialize while params\n+      TF_WhileParams params = TF_NewWhile(handle, inputs, ninputs, status);\n+      status.throwExceptionIfNotOK();\n+\n+      // build conditional subgraph\n+      TF_Output condInputsOutput = params.cond_inputs();\n+      TF_Output condOutputOutput = params.cond_output();\n+      TF_Operation[] condInputHandles = new TF_Operation[ninputs];\n+      int[] condInputIndices = new int[ninputs];\n+      TF_Operation[] condOutputHandles = new TF_Operation[1];\n+      int[] condOutputIndices = new int[1];\n+      for (int i = 0; i < ninputs; i++) {\n+          condInputHandles[i] = condInputsOutput.position(i).oper();\n+          condInputIndices[i] = condInputsOutput.position(i).index();\n+      }\n+      condOutputHandles[0] = condOutputOutput.oper();\n+      condOutputIndices[0] = condOutputOutput.index();\n+\n+      Object[] cond_output_handles_and_indices =\n+          buildSubgraph(condGraphBuilder, params.cond_graph(),\n+                        condInputHandles, condInputIndices,\n+                        condOutputHandles, condOutputIndices);\n+\n+      // build body subgraph\n+      TF_Output bodyInputsOutput = params.body_inputs();\n+      TF_Output bodyOutputsOutput = params.body_outputs();\n+      TF_Operation[] bodyInputHandles = new TF_Operation[ninputs];\n+      int[] bodyInputIndices = new int[ninputs];\n+      TF_Operation[] bodyOutputHandles = new TF_Operation[ninputs];\n+      int[] bodyOutputIndices = new int[ninputs];\n+      for (int i = 0; i < ninputs; i++) {\n+          bodyInputHandles[i] = bodyInputsOutput.position(i).oper();\n+          bodyInputIndices[i] = bodyInputsOutput.position(i).index();\n+          bodyOutputHandles[i] = bodyOutputsOutput.position(i).oper();\n+          bodyOutputIndices[i] = bodyOutputsOutput.position(i).index();\n+      }\n+\n+      Object[] body_output_handles_and_indices =", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2MTQ2OA==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372161468", "bodyText": "Nit: camel case", "author": "karllessard", "createdAt": "2020-01-29T02:28:18Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Graph.java", "diffHunk": "@@ -448,38 +470,227 @@ public void remove() {\n     private int position;\n   }\n \n-  private static native long allocate();\n+  private static TF_Graph allocate() {\n+      return TF_NewGraph();\n+  }\n+\n+  private static void delete(TF_Graph handle) {\n+    if (handle == null || handle.isNull()) return;\n+    TF_DeleteGraph(handle);\n+  }\n \n-  private static native void delete(long handle);\n+  private static void requireHandle(Pointer handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new IllegalStateException(\"close() has been called on the Graph\");\n+    }\n+  }\n \n-  private static native long operation(long handle, String name);\n+  private static TF_Operation operation(TF_Graph handle, String name) {\n+    requireHandle(handle);\n+    return TF_GraphOperationByName(handle, name);\n+  }\n \n   // This method returns the Operation native handle at index 0 and the new value for pos at index 1\n   // (see TF_GraphNextOperation)\n-  private static native long[] nextOperation(long handle, int position);\n+  private static Object[] nextOperation(TF_Graph handle, int position) {\n+    requireHandle(handle);\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      SizeTPointer pos = new SizeTPointer(1).put(position);\n+      TF_Operation operation = TF_GraphNextOperation(handle, pos);\n+      if (operation == null || operation.isNull()) return null;\n+\n+      Object[] handleAndPosition = new Object[2];\n+      handleAndPosition[0] = operation;\n+      handleAndPosition[1] = (int)pos.get();\n+      return handleAndPosition;\n+    }\n+  }\n+\n+  private static void importGraphDef(TF_Graph handle, byte[] graphDef, String prefix)\n+      throws IllegalArgumentException {\n+    requireHandle(handle);\n+\n+    // Continue cleaning up resources even if an exception was thrown.\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_ImportGraphDefOptions opts = TF_ImportGraphDefOptions.newImportGraphDefOptions();\n+\n+      TF_ImportGraphDefOptionsSetPrefix(opts, prefix);\n \n-  private static native void importGraphDef(long handle, byte[] graphDef, String prefix)\n-      throws IllegalArgumentException;\n+      TF_Buffer buf = TF_Buffer.newBufferFromString(graphDef);\n+      TF_Status status = TF_Status.newStatus();\n \n-  private static native byte[] toGraphDef(long handle);\n+      TF_GraphImportGraphDef(handle, buf, opts, status);\n+      status.throwExceptionIfNotOK();\n+    }\n+  }\n+\n+  private static byte[] toGraphDef(TF_Graph handle) {\n+    requireHandle(handle);\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Buffer buf = TF_Buffer.newBuffer();\n+      TF_Status status = TF_Status.newStatus();\n+      TF_GraphToGraphDef(handle, buf, status);\n+      status.throwExceptionIfNotOK();\n+      return buf.get();\n+    }\n+  }\n+\n+  static void resolveOutputs(String type, TF_Operation[] srcOps,\n+                             int[] srcIndices, TF_Output dst, int n) {\n+    if (srcOps.length != n) {\n+      throw new IllegalArgumentException(\"expected \" + n + \", got \" + srcOps.length + \" \" + type + \" Operations\");\n+    }\n+    if (srcIndices.length != n) {\n+      throw new IllegalArgumentException(\"expected \" + n + \", got \" + srcIndices.length + \" \" + type + \" Operation output indices\");\n+    }\n+    for (int i = 0; i < n; ++i) {\n+      if (srcOps[i] == null || srcOps[i].isNull()) {\n+        throw new NullPointerException(\"invalid \" + type + \" (#\" + i + \" of \" + n + \")\");\n+      }\n+      dst.position(i).oper(srcOps[i]).index(srcIndices[i]);\n+    }\n+    dst.position(0);\n+  }\n \n-  private static native long[] addGradients(\n-      long handle,\n+  private static Object[] addGradients(\n+      TF_Graph handle,\n       String prefix,\n-      long[] inputHandles,\n+      TF_Operation[] inputHandles,\n       int[] inputIndices,\n-      long[] outputHandles,\n+      TF_Operation[] outputHandles,\n       int[] outputIndices,\n-      long[] gradInputHandles,\n-      int[] gradInputIndices);\n+      TF_Operation[] gradInputHandles,\n+      int[] gradInputIndices) {\n+    requireHandle(handle);\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      int ny = inputHandles.length;\n+      int nx = outputHandles.length;\n+\n+      TF_Output y = new TF_Output(ny);\n+      TF_Output x = new TF_Output(nx);\n+      TF_Output dx = null;\n+      TF_Output dy = new TF_Output(nx);\n+\n+      resolveOutputs(\"y\", inputHandles, inputIndices, y, ny);\n+      resolveOutputs(\"x\", outputHandles, outputIndices, x, nx);\n+      if (gradInputHandles != null) {\n+        if (gradInputHandles.length != ny) {\n+          throw new IllegalArgumentException(\"expected \" + ny + \", got \" + gradInputHandles.length + \" handles\");\n+        }\n+        dx = new TF_Output(ny);\n+        resolveOutputs(\"dx\", gradInputHandles, gradInputIndices, dx, ny);\n+      }\n+\n+      TF_Status status = TF_Status.newStatus();\n+      TF_AddGradientsWithPrefix(handle, prefix, y, ny, x, nx, dx, status, dy);\n+      status.throwExceptionIfNotOK();\n+\n+      // returned array contains both op handles and output indices, in pair\n+      Object[] gradOutputHandlesAndIndices = new Object[nx * 2];\n+      for (int i = 0, j = nx; i < nx; ++i, ++j) {\n+        TF_Output gradOutput = dy.position(i);\n+        gradOutputHandlesAndIndices[i] = gradOutput.oper();\n+        gradOutputHandlesAndIndices[j] = gradOutput.index();\n+      }\n+      return gradOutputHandlesAndIndices;\n+    }\n+  }\n \n-  private static native long[] whileLoop(\n-      long handle,\n-      long[] inputHandles,\n+  private static Object[] whileLoop(\n+      TF_Graph handle,\n+      TF_Operation[] inputHandles,\n       int[] inputIndices,\n       String name,\n       WhileSubgraphBuilder condGraphBuilder,\n-      WhileSubgraphBuilder bodyGraphBuilder);\n+      WhileSubgraphBuilder bodyGraphBuilder) {\n+    requireHandle(handle);\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Status status = TF_Status.newStatus();\n+\n+      int ninputs = inputHandles.length;\n+\n+      TF_Output inputs = new TF_Output(ninputs);\n+      resolveOutputs(\"inputs\", inputHandles, inputIndices, inputs, ninputs);\n+\n+      // initialize while params\n+      TF_WhileParams params = TF_NewWhile(handle, inputs, ninputs, status);\n+      status.throwExceptionIfNotOK();\n+\n+      // build conditional subgraph\n+      TF_Output condInputsOutput = params.cond_inputs();\n+      TF_Output condOutputOutput = params.cond_output();\n+      TF_Operation[] condInputHandles = new TF_Operation[ninputs];\n+      int[] condInputIndices = new int[ninputs];\n+      TF_Operation[] condOutputHandles = new TF_Operation[1];\n+      int[] condOutputIndices = new int[1];\n+      for (int i = 0; i < ninputs; i++) {\n+          condInputHandles[i] = condInputsOutput.position(i).oper();\n+          condInputIndices[i] = condInputsOutput.position(i).index();\n+      }\n+      condOutputHandles[0] = condOutputOutput.oper();\n+      condOutputIndices[0] = condOutputOutput.index();\n+\n+      Object[] cond_output_handles_and_indices =\n+          buildSubgraph(condGraphBuilder, params.cond_graph(),\n+                        condInputHandles, condInputIndices,\n+                        condOutputHandles, condOutputIndices);\n+\n+      // build body subgraph\n+      TF_Output bodyInputsOutput = params.body_inputs();\n+      TF_Output bodyOutputsOutput = params.body_outputs();\n+      TF_Operation[] bodyInputHandles = new TF_Operation[ninputs];\n+      int[] bodyInputIndices = new int[ninputs];\n+      TF_Operation[] bodyOutputHandles = new TF_Operation[ninputs];\n+      int[] bodyOutputIndices = new int[ninputs];\n+      for (int i = 0; i < ninputs; i++) {\n+          bodyInputHandles[i] = bodyInputsOutput.position(i).oper();\n+          bodyInputIndices[i] = bodyInputsOutput.position(i).index();\n+          bodyOutputHandles[i] = bodyOutputsOutput.position(i).oper();\n+          bodyOutputIndices[i] = bodyOutputsOutput.position(i).index();\n+      }\n+\n+      Object[] body_output_handles_and_indices =\n+          buildSubgraph(bodyGraphBuilder, params.body_graph(),\n+                        bodyInputHandles, bodyInputIndices,\n+                        bodyOutputHandles, bodyOutputIndices);\n+\n+      if (cond_output_handles_and_indices == null ||\n+          body_output_handles_and_indices == null)\n+        return null;\n+\n+      // set cond_output param to output of the conditional subgraph\n+      condOutputOutput.oper((TF_Operation)cond_output_handles_and_indices[0])\n+                      .index((Integer)cond_output_handles_and_indices[1]);\n+\n+      // set body_outputs param to outputs of the body subgraph\n+      for (int i = 0, j = ninputs; i < ninputs; ++i, ++j) {\n+        bodyOutputsOutput.position(i).oper((TF_Operation)body_output_handles_and_indices[i])\n+                                     .index((Integer)body_output_handles_and_indices[j]);\n+      }\n+\n+      // set loop name param\n+      params.name(new BytePointer(name));\n+\n+      // build the while loop, storing loop outputs in `outputs`\n+      TF_Output outputs = new TF_Output(ninputs);\n+      TF_FinishWhile(params, status, outputs);\n+\n+      status.throwExceptionIfNotOK();\n+\n+      // returned array contains both op handles and output indices, in pair\n+      Object[] output_handles_and_indices = new Object[ninputs * 2];", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2MTkwNg==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372161906", "bodyText": "BytePointer returned by these methods don't need to be freed/scoped?", "author": "karllessard", "createdAt": "2020-01-29T02:30:46Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/GraphOperation.java", "diffHunk": "@@ -147,25 +162,92 @@ Shape shape(int outputIdx) {\n     throw new IllegalStateException(\"Graph tensors must be fetched by running a session\");\n   }\n \n-  long getUnsafeNativeHandle() {\n+  TF_Operation getUnsafeNativeHandle() {\n     return unsafeNativeHandle;\n   }\n \n   private final Graph graph;\n \n-  private final long unsafeNativeHandle;\n+  private final TF_Operation unsafeNativeHandle;\n \n-  private static native String name(long handle);\n+  private static void requireHandle(Pointer handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new IllegalStateException(\"close() has been called on the Graph this Operation was a part of\");\n+    }\n+  }\n+\n+  private static String name(TF_Operation handle) {\n+    requireHandle(handle);\n+    return TF_OperationName(handle).getString();", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE3MTgxMQ==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372171811", "bodyText": "No, as explained above, by convention, JavaCPP doesn't perform any allocation for directly mapped C/C++ functions.", "author": "saudet", "createdAt": "2020-01-29T03:19:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2MTkwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2MzE0Mg==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372163142", "bodyText": "instead of all those explicit casts everywhere, maybe we can add 2 methods to Output: getGraphOperationUnsafeNativeHandle and getEagerOperationUnsafeNativeHandle? I think the code would end up cleaner.", "author": "karllessard", "createdAt": "2020-01-29T02:36:43Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -305,13 +324,13 @@ private Run runHelper(boolean wantMetadata) {\n       }\n       idx = 0;\n       for (Output<?> o : inputs) {\n-        inputOpHandles[idx] = o.getUnsafeNativeHandle();\n+        inputOpHandles[idx] = (TF_Operation)o.getUnsafeNativeHandle();\n         inputOpIndices[idx] = o.index();\n         idx++;\n       }\n       idx = 0;\n       for (Output<?> o : outputs) {\n-        outputOpHandles[idx] = o.getUnsafeNativeHandle();\n+        outputOpHandles[idx] = (TF_Operation)o.getUnsafeNativeHandle();", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE3MjUzOA==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372172538", "bodyText": "Well, it's not that bad. Though, I agree that all those weird methods relying on Object references should be totally refactored/removed, but let's leave them like they are for now instead of coming up with workarounds?", "author": "saudet", "createdAt": "2020-01-29T03:24:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2MzE0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjM5NDAzNw==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372394037", "bodyText": "Ok, we can do this later if it gets really nasty.", "author": "karllessard", "createdAt": "2020-01-29T13:54:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2MzE0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2MzQ4Ng==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372163486", "bodyText": "I don't know what was thrown by the original JNI code but I think we should throw ourselves NPE, IllegalStateException sounds a better choice. Same thing in allocate2 below.", "author": "karllessard", "createdAt": "2020-01-29T02:38:30Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -440,15 +459,63 @@ public Runner runner() {\n   private final Graph.Reference graphRef;\n \n   private final Object nativeHandleLock = new Object();\n-  private long nativeHandle;\n+  private TF_Session nativeHandle;\n   private int numActiveRuns;\n \n+  private static void requireHandle(Pointer handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new IllegalStateException(\"close() has been called on the Session\");\n+    }\n+  }\n+\n+  private static void resolveHandles(String type, Pointer[] src, PointerPointer dst, int n) {\n+    if (src.length != n) {\n+      throw new IllegalArgumentException(\"expected \" + n + \", got \" + src.length + \" \" + type);\n+    }\n+    for (int i = 0; i < n; ++i) {\n+      if (src[i] == null || src[i].isNull()) {\n+        throw new NullPointerException(\"invalid \" + type + \" (#\" + i + \" of \" + n + \")\");", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2NDM0Mg==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372164342", "bodyText": "Question: did you ever ran a benchmark showing that all these context switches between the JVM and the native code (i.e. at each JavaCPP generated method) ends up to be as performant as when only one call was made (run in this case)?", "author": "karllessard", "createdAt": "2020-01-29T02:43:01Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Session.java", "diffHunk": "@@ -477,15 +544,49 @@ public Runner runner() {\n    * @return if wantRunMetadata is true, serialized representation of the RunMetadata protocol\n    *     buffer, false otherwise.\n    */\n-  private static native byte[] run(\n-      long handle,\n+  private static byte[] run(\n+      TF_Session handle,\n       byte[] runOptions,\n-      long[] inputTensorHandles,\n-      long[] inputOpHandles,\n+      TF_Tensor[] inputTensorHandles,\n+      TF_Operation[] inputOpHandles,\n       int[] inputOpIndices,\n-      long[] outputOpHandles,\n+      TF_Operation[] outputOpHandles,\n       int[] outputOpIndices,\n-      long[] targetOpHandles,\n+      TF_Operation[] targetOpHandles,\n       boolean wantRunMetadata,\n-      long[] outputTensorHandles);\n+      TF_Tensor[] outputTensorHandles) {\n+    requireHandle(handle);\n+\n+    int ninputs = inputTensorHandles.length;\n+    int noutputs = outputTensorHandles.length;\n+    int ntargets = targetOpHandles.length;\n+\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Output inputs = new TF_Output(ninputs);\n+      PointerPointer<TF_Tensor> inputValues = new PointerPointer<TF_Tensor>(ninputs);\n+      TF_Output outputs = new TF_Output(noutputs);\n+      PointerPointer<TF_Tensor> outputValues = new PointerPointer<TF_Tensor>(noutputs);\n+      PointerPointer<TF_Operation> targets = new PointerPointer<TF_Operation>(ntargets);\n+      TF_Buffer runMetadata = wantRunMetadata ? TF_Buffer.newBuffer() : null;\n+\n+      resolveHandles(\"input Tensors\", inputTensorHandles, inputValues, ninputs);\n+      resolveOutputs(\"input\", inputOpHandles, inputOpIndices, inputs, ninputs);\n+      resolveOutputs(\"output\", outputOpHandles, outputOpIndices, outputs, noutputs);\n+      resolveHandles(\"target Operations\", targetOpHandles, targets, ntargets);\n+\n+      TF_Status status = TF_Status.newStatus();\n+      TF_Buffer runOpts = TF_Buffer.newBufferFromString(runOptions);\n+\n+      TF_SessionRun(handle, runOpts, inputs, inputValues, ninputs,\n+                    outputs, outputValues, noutputs, targets, ntargets,\n+                    runMetadata, status);\n+      status.throwExceptionIfNotOK();\n+\n+      for (int i = 0; i < noutputs; ++i) {\n+        outputTensorHandles[i] = outputValues.get(TF_Tensor.class, i);\n+      }\n+\n+      return runMetadata != null ? runMetadata.get() : null;", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE3NDEwNw==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372174107", "bodyText": "Calls from the JVM to native code are very efficient, in the order of 30 ns. On the other hand, calls from native code to the JVM are typically very expensive, in the order of 300 ns, so it's almost certain that the new code here is going to be faster. I'll run a simple benchmark and post the results here just to confirm, but if you're worried about performance, we should think about writing a whole set of benchmarks to make sure there is never any regression in performance anywhere.", "author": "saudet", "createdAt": "2020-01-29T03:33:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2NDM0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjM5NjI1OA==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372396258", "bodyText": "I've started to write some of them as well (like this one), I think with high-performance projects like TF we should have a set of benchmarks, not only it is useful to detect regression but also is a very good indicator of where we should spend time for optimization.\nIf you can add some, that would be great, but we can merge without if you want to create these later.", "author": "karllessard", "createdAt": "2020-01-29T13:58:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2NDM0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2NTAyNQ==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372165025", "bodyText": "Most of the stuff here and below in this class should ultimately be dropped and replaced with the usage of TF Tools to read or write data from/to the tensor.\nI guess we can migrate it to JavaCPP meanwhile and I can take care of the cleanup after.", "author": "karllessard", "createdAt": "2020-01-29T02:46:23Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Tensor.java", "diffHunk": "@@ -821,35 +848,505 @@ private void throwExceptionIfTypeIsIncompatible(Object o) {\n     }\n   }\n \n-  private static native long allocate(int dtype, long[] shape, long byteSize);\n+  private static void requireHandle(TF_Tensor handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new NullPointerException(\"close() was called on the Tensor\");\n+    }\n+  }\n+\n+  private static int elemByteSize(int dtype) {\n+    // The code in this file makes the assumption that the\n+    // TensorFlow TF_DataTypes and the Java primitive types\n+    // have the same byte sizes. Validate that:\n+    switch (dtype) {\n+      case TF_BOOL:\n+      case TF_UINT8:\n+        assert Loader.sizeof(BooleanPointer.class) == 1 :\n+               \"Java boolean not compatible with TF_BOOL\";\n+        assert Loader.sizeof(BytePointer.class) == 1 :\n+               \"Java byte not compatible with TF_UINT8\";\n+        return 1;\n+      case TF_FLOAT:\n+      case TF_INT32:\n+        assert Loader.sizeof(FloatPointer.class) == 4 :\n+               \"Java float not compatible with TF_FLOAT\";\n+        assert Loader.sizeof(IntPointer.class) == 4 :\n+               \"Java int not compatible with TF_INT32\";\n+        return 4;\n+      case TF_DOUBLE:\n+      case TF_INT64:\n+        assert Loader.sizeof(DoublePointer.class) == 8 :\n+               \"Java double not compatible with TF_DOUBLE\";\n+        assert Loader.sizeof(LongPointer.class) == 8 :\n+               \"Java long not compatible with TF_INT64\";\n+        return 8;\n+      default:\n+        return 0;\n+    }\n+  }\n+\n+  /** Write a Java scalar object (java.lang.Integer etc.) to a TF_Tensor. */\n+  private static void writeScalar(Object src, int dtype, BytePointer dst, long dstSize) {", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2NTE3OQ==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372165179", "bodyText": "No need of an else condition", "author": "karllessard", "createdAt": "2020-01-29T02:47:13Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Tensor.java", "diffHunk": "@@ -821,35 +848,505 @@ private void throwExceptionIfTypeIsIncompatible(Object o) {\n     }\n   }\n \n-  private static native long allocate(int dtype, long[] shape, long byteSize);\n+  private static void requireHandle(TF_Tensor handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new NullPointerException(\"close() was called on the Tensor\");\n+    }\n+  }\n+\n+  private static int elemByteSize(int dtype) {\n+    // The code in this file makes the assumption that the\n+    // TensorFlow TF_DataTypes and the Java primitive types\n+    // have the same byte sizes. Validate that:\n+    switch (dtype) {\n+      case TF_BOOL:\n+      case TF_UINT8:\n+        assert Loader.sizeof(BooleanPointer.class) == 1 :\n+               \"Java boolean not compatible with TF_BOOL\";\n+        assert Loader.sizeof(BytePointer.class) == 1 :\n+               \"Java byte not compatible with TF_UINT8\";\n+        return 1;\n+      case TF_FLOAT:\n+      case TF_INT32:\n+        assert Loader.sizeof(FloatPointer.class) == 4 :\n+               \"Java float not compatible with TF_FLOAT\";\n+        assert Loader.sizeof(IntPointer.class) == 4 :\n+               \"Java int not compatible with TF_INT32\";\n+        return 4;\n+      case TF_DOUBLE:\n+      case TF_INT64:\n+        assert Loader.sizeof(DoublePointer.class) == 8 :\n+               \"Java double not compatible with TF_DOUBLE\";\n+        assert Loader.sizeof(LongPointer.class) == 8 :\n+               \"Java long not compatible with TF_INT64\";\n+        return 8;\n+      default:\n+        return 0;\n+    }\n+  }\n+\n+  /** Write a Java scalar object (java.lang.Integer etc.) to a TF_Tensor. */\n+  private static void writeScalar(Object src, int dtype, BytePointer dst, long dstSize) {\n+    int sz = elemByteSize(dtype);\n+    if (sz != dstSize) {\n+      throw new IllegalStateException(\"scalar (\" + sz\n+          + \" bytes) not compatible with allocated tensor (\" + dstSize + \" bytes)\");\n+    }\n+    switch (dtype) {\n+      case TF_FLOAT:\n+        dst.putFloat((Float)src);\n+        break;\n+      case TF_DOUBLE:\n+        dst.putDouble((Double)src);\n+        break;\n+      case TF_INT32:\n+        dst.putInt((Integer)src);\n+        break;\n+      case TF_INT64:\n+        dst.putLong((Long)src);\n+        break;\n+      case TF_UINT8:\n+        dst.put((Byte)src);\n+        break;\n+      case TF_BOOL:\n+        dst.putBool((Boolean)src);\n+        break;\n+      default:\n+        throw new IllegalStateException(\"invalid DataType(\" + dtype + \")\");\n+    }\n+  }\n+\n+  private static int getArrayLength(Object array, int dtype) {\n+    switch (dtype) {\n+      case TF_FLOAT: return ((float[])array).length;\n+      case TF_DOUBLE: return ((double[])array).length;\n+      case TF_INT32: return ((int[])array).length;\n+      case TF_INT64: return ((long[])array).length;\n+      case TF_UINT8: return ((byte[])array).length;\n+      case TF_BOOL: return ((boolean[])array).length;\n+      default: throw new IllegalStateException(\"invalid DataType(\" + dtype + \")\");\n+    }\n+  }\n+\n+  /** Copy a 1-D array of Java primitive types to the tensor buffer dst.\n+   * Returns the number of bytes written to dst. */\n+  private static long write1DArray(Object array, int dtype, BytePointer dst, long dstSize) {\n+    int nelems = getArrayLength(array, dtype);\n+    long toCopy = nelems * elemByteSize(dtype);\n+    if (toCopy > dstSize) {\n+      throw new IllegalStateException(\n+          \"cannot write Java array of \" + toCopy + \" bytes to Tensor of \" + dstSize + \" bytes\");\n+    }\n+    switch (dtype) {\n+      case TF_FLOAT:\n+        dst.put(new FloatPointer((float[])array).capacity(nelems));\n+        break;\n+      case TF_DOUBLE:\n+        dst.put(new DoublePointer((double[])array).capacity(nelems));\n+        break;\n+      case TF_INT32:\n+        dst.put(new IntPointer((int[])array).capacity(nelems));\n+        break;\n+      case TF_INT64:\n+        dst.put(new LongPointer((long[])array).capacity(nelems));\n+        break;\n+      case TF_UINT8:\n+        dst.put(new BytePointer((byte[])array).capacity(nelems));\n+        break;\n+      case TF_BOOL:\n+        dst.put(new BooleanPointer((boolean[])array).capacity(nelems));\n+        break;\n+      default:\n+        throw new IllegalStateException(\"invalid DataType(\" + dtype + \")\");\n+    }\n+    return toCopy;\n+  }\n+\n+  /** Copy the elements of a 1-D array from the tensor buffer src to a 1-D array of\n+   * Java primitive types. Returns the number of bytes read from src. */\n+  private static long read1DArray(int dtype, BytePointer src, long srcSize, Object dst) {\n+    int len = getArrayLength(dst, dtype);\n+    long sz = len * elemByteSize(dtype);\n+    if (sz > srcSize) {\n+      throw new IllegalStateException(\n+          \"cannot fill a Java array of \" + sz + \"bytes with a Tensor of \" + srcSize + \" bytes\");\n+    }\n+    switch (dtype) {\n+      case TF_FLOAT:\n+        new FloatPointer(src).position(src.position() / 4).get((float[])dst);\n+        break;\n+      case TF_DOUBLE:\n+        new DoublePointer(src).position(src.position() / 8).get((double[])dst);\n+        break;\n+      case TF_INT32:\n+        new IntPointer(src).position(src.position() / 4).get((int[])dst);\n+        break;\n+      case TF_INT64:\n+        new LongPointer(src).position(src.position() / 8).get((long[])dst);\n+        break;\n+      case TF_UINT8:\n+        src.get((byte[])dst);\n+        break;\n+      case TF_BOOL:\n+        new BooleanPointer(src).position(src.position()).get((boolean[])dst);\n+        break;\n+      default:\n+        throw new IllegalStateException(\"invalid DataType(\" + dtype + \")\");\n+    }\n+    return sz;\n+  }\n+\n+  private static long writeNDArray(Object src, int dtype, int dimsLeft,\n+                                   BytePointer dst, long dstSize) {\n+    if (dimsLeft == 1) {\n+      return write1DArray(src, dtype, dst, dstSize);\n+    } else {", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2NTYyMw==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372165623", "bodyText": "No need of else conditions (applicable to all methods below)", "author": "karllessard", "createdAt": "2020-01-29T02:49:16Z", "path": "tensorflow-core/tensorflow-core-api/src/main/java/org/tensorflow/Tensor.java", "diffHunk": "@@ -821,35 +848,505 @@ private void throwExceptionIfTypeIsIncompatible(Object o) {\n     }\n   }\n \n-  private static native long allocate(int dtype, long[] shape, long byteSize);\n+  private static void requireHandle(TF_Tensor handle) {\n+    if (handle == null || handle.isNull()) {\n+      throw new NullPointerException(\"close() was called on the Tensor\");\n+    }\n+  }\n+\n+  private static int elemByteSize(int dtype) {\n+    // The code in this file makes the assumption that the\n+    // TensorFlow TF_DataTypes and the Java primitive types\n+    // have the same byte sizes. Validate that:\n+    switch (dtype) {\n+      case TF_BOOL:\n+      case TF_UINT8:\n+        assert Loader.sizeof(BooleanPointer.class) == 1 :\n+               \"Java boolean not compatible with TF_BOOL\";\n+        assert Loader.sizeof(BytePointer.class) == 1 :\n+               \"Java byte not compatible with TF_UINT8\";\n+        return 1;\n+      case TF_FLOAT:\n+      case TF_INT32:\n+        assert Loader.sizeof(FloatPointer.class) == 4 :\n+               \"Java float not compatible with TF_FLOAT\";\n+        assert Loader.sizeof(IntPointer.class) == 4 :\n+               \"Java int not compatible with TF_INT32\";\n+        return 4;\n+      case TF_DOUBLE:\n+      case TF_INT64:\n+        assert Loader.sizeof(DoublePointer.class) == 8 :\n+               \"Java double not compatible with TF_DOUBLE\";\n+        assert Loader.sizeof(LongPointer.class) == 8 :\n+               \"Java long not compatible with TF_INT64\";\n+        return 8;\n+      default:\n+        return 0;\n+    }\n+  }\n+\n+  /** Write a Java scalar object (java.lang.Integer etc.) to a TF_Tensor. */\n+  private static void writeScalar(Object src, int dtype, BytePointer dst, long dstSize) {\n+    int sz = elemByteSize(dtype);\n+    if (sz != dstSize) {\n+      throw new IllegalStateException(\"scalar (\" + sz\n+          + \" bytes) not compatible with allocated tensor (\" + dstSize + \" bytes)\");\n+    }\n+    switch (dtype) {\n+      case TF_FLOAT:\n+        dst.putFloat((Float)src);\n+        break;\n+      case TF_DOUBLE:\n+        dst.putDouble((Double)src);\n+        break;\n+      case TF_INT32:\n+        dst.putInt((Integer)src);\n+        break;\n+      case TF_INT64:\n+        dst.putLong((Long)src);\n+        break;\n+      case TF_UINT8:\n+        dst.put((Byte)src);\n+        break;\n+      case TF_BOOL:\n+        dst.putBool((Boolean)src);\n+        break;\n+      default:\n+        throw new IllegalStateException(\"invalid DataType(\" + dtype + \")\");\n+    }\n+  }\n+\n+  private static int getArrayLength(Object array, int dtype) {\n+    switch (dtype) {\n+      case TF_FLOAT: return ((float[])array).length;\n+      case TF_DOUBLE: return ((double[])array).length;\n+      case TF_INT32: return ((int[])array).length;\n+      case TF_INT64: return ((long[])array).length;\n+      case TF_UINT8: return ((byte[])array).length;\n+      case TF_BOOL: return ((boolean[])array).length;\n+      default: throw new IllegalStateException(\"invalid DataType(\" + dtype + \")\");\n+    }\n+  }\n+\n+  /** Copy a 1-D array of Java primitive types to the tensor buffer dst.\n+   * Returns the number of bytes written to dst. */\n+  private static long write1DArray(Object array, int dtype, BytePointer dst, long dstSize) {\n+    int nelems = getArrayLength(array, dtype);\n+    long toCopy = nelems * elemByteSize(dtype);\n+    if (toCopy > dstSize) {\n+      throw new IllegalStateException(\n+          \"cannot write Java array of \" + toCopy + \" bytes to Tensor of \" + dstSize + \" bytes\");\n+    }\n+    switch (dtype) {\n+      case TF_FLOAT:\n+        dst.put(new FloatPointer((float[])array).capacity(nelems));\n+        break;\n+      case TF_DOUBLE:\n+        dst.put(new DoublePointer((double[])array).capacity(nelems));\n+        break;\n+      case TF_INT32:\n+        dst.put(new IntPointer((int[])array).capacity(nelems));\n+        break;\n+      case TF_INT64:\n+        dst.put(new LongPointer((long[])array).capacity(nelems));\n+        break;\n+      case TF_UINT8:\n+        dst.put(new BytePointer((byte[])array).capacity(nelems));\n+        break;\n+      case TF_BOOL:\n+        dst.put(new BooleanPointer((boolean[])array).capacity(nelems));\n+        break;\n+      default:\n+        throw new IllegalStateException(\"invalid DataType(\" + dtype + \")\");\n+    }\n+    return toCopy;\n+  }\n+\n+  /** Copy the elements of a 1-D array from the tensor buffer src to a 1-D array of\n+   * Java primitive types. Returns the number of bytes read from src. */\n+  private static long read1DArray(int dtype, BytePointer src, long srcSize, Object dst) {\n+    int len = getArrayLength(dst, dtype);\n+    long sz = len * elemByteSize(dtype);\n+    if (sz > srcSize) {\n+      throw new IllegalStateException(\n+          \"cannot fill a Java array of \" + sz + \"bytes with a Tensor of \" + srcSize + \" bytes\");\n+    }\n+    switch (dtype) {\n+      case TF_FLOAT:\n+        new FloatPointer(src).position(src.position() / 4).get((float[])dst);\n+        break;\n+      case TF_DOUBLE:\n+        new DoublePointer(src).position(src.position() / 8).get((double[])dst);\n+        break;\n+      case TF_INT32:\n+        new IntPointer(src).position(src.position() / 4).get((int[])dst);\n+        break;\n+      case TF_INT64:\n+        new LongPointer(src).position(src.position() / 8).get((long[])dst);\n+        break;\n+      case TF_UINT8:\n+        src.get((byte[])dst);\n+        break;\n+      case TF_BOOL:\n+        new BooleanPointer(src).position(src.position()).get((boolean[])dst);\n+        break;\n+      default:\n+        throw new IllegalStateException(\"invalid DataType(\" + dtype + \")\");\n+    }\n+    return sz;\n+  }\n+\n+  private static long writeNDArray(Object src, int dtype, int dimsLeft,\n+                                   BytePointer dst, long dstSize) {\n+    if (dimsLeft == 1) {\n+      return write1DArray(src, dtype, dst, dstSize);\n+    } else {\n+      Object[] ndarray = (Object[])src;\n+      long sz = 0;\n+      for (int i = 0; i < ndarray.length; ++i) {\n+        Object row = ndarray[i];\n+        sz += writeNDArray(row, dtype, dimsLeft - 1,\n+            new BytePointer(dst).position(dst.position() + sz), dstSize - sz);\n+      }\n+      return sz;\n+    }\n+  }\n+\n+  private static long readNDArray(int dtype, BytePointer src, long srcSize,\n+                                  int dimsLeft, Object dst) {\n+    if (dimsLeft == 1) {\n+      return read1DArray(dtype, src, srcSize, dst);\n+    } else {\n+      Object[] ndarray = (Object[])dst;\n+      long sz = 0;\n+      for (int i = 0; i < ndarray.length; ++i) {\n+        Object row = ndarray[i];\n+        sz += readNDArray(dtype, new BytePointer(src).position(src.position() + sz),\n+            srcSize - sz, dimsLeft - 1, row);\n+      }\n+      return sz;\n+    }\n+  }\n+\n+  private static byte[] TF_StringDecodeToArray(BytePointer src, long srcLen, TF_Status status) {\n+    try (PointerScope scope = new PointerScope()) {\n+      BytePointer dst = new BytePointer((Pointer)null);\n+      SizeTPointer dstLen = new SizeTPointer(1);\n+      TF_StringDecode(src, srcLen, dst, dstLen, status);\n+      if (TF_GetCode(status) != TF_OK) {\n+        return null;\n+      }\n+      byte[] ret = new byte[(int)dstLen.get()];\n+      dst.get(ret);\n+      return ret;\n+    }\n+  }\n+\n+  private static class StringTensorWriter {\n+    StringTensorWriter(TF_Tensor t, long numElements) {\n+      offset = 0;\n+      poffsets = new BytePointer(TF_TensorData(t));\n+      pdata = new BytePointer(poffsets).position(8 * numElements);\n+      plimit = new BytePointer(poffsets).position(TF_TensorByteSize(t));\n+    }\n+\n+    void Add(BytePointer src, long len, TF_Status status) {\n+      if (TF_GetCode(status) != TF_OK) return;\n+      if (plimit.position() - poffsets.position() < 8) {\n+        TF_SetStatus(status, TF_OUT_OF_RANGE,\n+                     \"TF_STRING tensor encoding ran out of space for offsets, \"\n+                   + \"this is likely a bug, please file an issue at \"\n+                   + \"https://github.com/tensorflow/java/issues/new\");\n+        return;\n+      }\n+      poffsets.putLong(offset);\n+      long written =\n+          TF_StringEncode(src, len, pdata, plimit.position() - pdata.position(), status);\n+      offset += written;\n+      poffsets.position(poffsets.position() + 8);\n+      pdata.position(pdata.position() + written);\n+    }\n+\n+    long offset;\n+    BytePointer poffsets;\n+    BytePointer pdata;\n+    BytePointer plimit;\n+  }\n+\n+  private static class StringTensorReader {\n+    StringTensorReader(TF_Tensor t, long numElements) {\n+      index = 0;\n+      offsets = new BytePointer(TF_TensorData(t));\n+      data = new BytePointer(offsets).position(8 * numElements);\n+      limit = new BytePointer(offsets).position(TF_TensorByteSize(t));\n+    }\n+\n+    byte[] Next(TF_Status status) {\n+      if (TF_GetCode(status) != TF_OK) return null;\n+      long offset = 0;\n+      BytePointer poffset = new BytePointer(offsets).position(8 * index);\n+      if (poffset.position() >= limit.position()) {\n+        TF_SetStatus(status, TF_INTERNAL,\n+            \"Invalid TF_STRING tensor, offsets table seems to be too small\");\n+        return null;\n+      }\n+      offset = poffset.getLong();\n+      BytePointer pdata = new BytePointer(data).position(data.position() + offset);\n+      if (pdata.position() >= limit.position()) {\n+        TF_SetStatus(status, TF_INTERNAL,\n+            \"Invalid TF_STRING tensor, invalid entry in offset table\");\n+        return null;\n+      }\n+      ++index;\n+      return TF_StringDecodeToArray(pdata, limit.position() - pdata.position(), status);\n+    }\n+\n+    int index;\n+    BytePointer offsets;\n+    BytePointer data;\n+    BytePointer limit;\n+  }\n+\n+  private static void readNDStringArray(StringTensorReader reader, int dimsLeft,\n+                                        Object[] dst, TF_Status status) {\n+    if (dimsLeft == 1) {\n+      for (int i = 0; i < dst.length; ++i) {\n+        byte[] elem = reader.Next(status);\n+        if (TF_GetCode(status) != TF_OK) return;\n+        dst[i] = elem;\n+      }\n+      return;\n+    }\n+    for (int i = 0; i < dst.length; ++i) {\n+      readNDStringArray(reader, dimsLeft - 1, (Object[])dst[i], status);\n+      if (TF_GetCode(status) != TF_OK) return;\n+    }\n+  }\n+\n+  private static TF_Tensor allocate(int dtype, long[] shape, long byteSize) {\n+    TF_Tensor t = TF_AllocateTensor(dtype, shape, shape.length, byteSize);\n+    if (t == null || t.isNull()) {\n+      throw new NullPointerException(\"unable to allocate memory for the Tensor\");\n+    }\n+    return t;\n+  }\n \n-  private static native long allocateScalarBytes(byte[] value);\n+  private static TF_Tensor allocateScalarBytes(byte[] value) {\n+    // TF_STRING tensors are encoded with a table of 8-byte offsets followed by\n+    // TF_StringEncode-encoded bytes.\n+    long dstLen = TF_StringEncodedSize(value.length);\n+    TF_Tensor t = TF_AllocateTensor(TF_STRING, (long[])null, 0, 8 + dstLen);\n+    BytePointer dst = new BytePointer(TF_TensorData(t));\n+    dst.putLong(0);  // The offset table\n+    try (PointerScope scope = new PointerScope()) {\n+      TF_Status status = TF_Status.newStatus();\n+      TF_StringEncode(new BytePointer(value), value.length, dst.position(8), dstLen, status);\n+      status.throwExceptionIfNotOK();\n+      return t;\n+    }\n+  }\n \n-  private static native long allocateNonScalarBytes(long[] shape, Object[] value);\n+  private static long nonScalarStringTensorSize(Object value, int numDims) {\n+    if (numDims == 0) {\n+      // This is the last dimension, i.e., value should correspond to a jbyteArray\n+      // encoding the string.\n+      return TF_StringEncodedSize(((byte[])value).length);\n+    }\n+    Object[] array = (Object[])value;\n+    long ret = 0;\n+    for (int i = 0; i < array.length; ++i) {\n+      Object elem = array[i];\n+      if (elem == null) {\n+        throw new NullPointerException(\"null entries in provided array\");\n+      }\n+      ret += nonScalarStringTensorSize(elem, numDims - 1);\n+    }\n+    return ret;\n+  }\n \n-  private static native void delete(long handle);\n+  private static void fillNonScalarStringTensorData(Object value, int numDims,\n+      StringTensorWriter writer, TF_Status status) {\n+    if (numDims == 0) {\n+      byte[] src = (byte[])value;\n+      writer.Add(new BytePointer(src), src.length, status);\n+      return;\n+    }\n+    Object[] array = (Object[])value;\n+    for (int i = 0; i < array.length; ++i) {\n+      Object elem = array[i];\n+      if (elem == null) {\n+        throw new NullPointerException(\"null entries in provided array\");\n+      }\n+      fillNonScalarStringTensorData(elem, numDims - 1, writer, status);\n+      if (TF_GetCode(status) != TF_OK) return;\n+    }\n+  }\n+\n+  private static TF_Tensor allocateNonScalarBytes(long[] shape, Object[] value) {\n+    // TF_STRING tensors are encoded with a table of 8-byte offsets following by\n+    // TF_StringEncode-encoded bytes.\n+    int numDims = shape.length;\n+    long numElements = 1;\n+    for (int i = 0; i < numDims; ++i) {\n+      numElements *= shape[i];\n+    }\n+    long encodedSize = nonScalarStringTensorSize(value, numDims);\n+    TF_Tensor t = TF_AllocateTensor(TF_STRING, shape, numDims,\n+                                    8 * numElements + encodedSize);\n+    if (t == null || t.isNull()) {\n+      throw new NullPointerException(\"unable to allocate memory for the Tensor\");\n+    }\n+    TF_Status status = TF_Status.newStatus();\n+    try (PointerScope scope = new PointerScope()) {\n+      StringTensorWriter writer = new StringTensorWriter(t, numElements);\n+      fillNonScalarStringTensorData(value, numDims, writer, status);\n+      status.throwExceptionIfNotOK();\n+      return t;\n+    }\n+  }\n \n-  private static native ByteBuffer buffer(long handle);\n+  private static void delete(TF_Tensor handle) {\n+    if (handle == null || handle.isNull()) return;\n+    TF_DeleteTensor(handle);\n+  }\n \n-  private static native int dtype(long handle);\n+  private static ByteBuffer buffer(TF_Tensor handle) {\n+    requireHandle(handle);\n+    return TF_TensorData(handle).capacity(TF_TensorByteSize(handle)).asByteBuffer();\n+  }\n \n-  private static native long[] shape(long handle);\n+  private static int dtype(TF_Tensor handle) {\n+    requireHandle(handle);\n+    return TF_TensorType(handle);\n+  }\n \n-  private static native void setValue(long handle, Object value);\n+  private static long[] shape(TF_Tensor handle) {\n+    requireHandle(handle);\n+    int numDims = TF_NumDims(handle);\n+    long[] dims = new long[numDims];\n+    for (int i = 0; i < numDims; ++i) {\n+      dims[i] = TF_Dim(handle, i);\n+    }\n+    return dims;\n+  }\n+\n+  private static void setValue(TF_Tensor handle, Object value) {\n+    requireHandle(handle);\n+    int numDims = TF_NumDims(handle);\n+    int dtype = TF_TensorType(handle);\n+    BytePointer data = new BytePointer(TF_TensorData(handle));\n+    long sz = TF_TensorByteSize(handle);\n+    if (numDims == 0) {\n+      writeScalar(value, dtype, data, sz);\n+    } else {\n+      writeNDArray(value, dtype, numDims, data, sz);\n+    }\n+  }\n \n-  private static native float scalarFloat(long handle);\n+  private static float scalarFloat(TF_Tensor handle) {\n+    requireHandle(handle);\n+    if (TF_NumDims(handle) != 0) {\n+      throw new IllegalStateException(\"Tensor is not a scalar\");\n+    } else if (TF_TensorType(handle) != TF_FLOAT) {", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2NjUzMw==", "url": "https://github.com/tensorflow/java/pull/18#discussion_r372166533", "bodyText": "assertNotNull?", "author": "karllessard", "createdAt": "2020-01-29T02:53:56Z", "path": "tensorflow-core/tensorflow-core-api/src/test/java/org/tensorflow/TensorTest.java", "diffHunk": "@@ -535,10 +535,10 @@ public void eagerTensorIsReleasedAfterSessionIsClosed() {\n       Output<?> x = TestUtil.constant(session, \"Const1\", 10);\n       Output<?> y = TestUtil.constant(session, \"Const2\", 20);\n       sum = TestUtil.<TInt32>addN(session, x, y).tensor();\n-      assertNotEquals(0L, sum.getNativeHandle());\n+      assertNotEquals(null, sum.getNativeHandle());", "originalCommit": "55b7b5be984bf709882ead204cc4e1833b98d104", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "47125efd05b38fe961cf341d3d061886c8070b83", "url": "https://github.com/tensorflow/java/commit/47125efd05b38fe961cf341d3d061886c8070b83", "message": "Remove calls to inefficient generic Pointer.put()", "committedDate": "2020-01-29T03:41:45Z", "type": "commit"}, {"oid": "1e22569dcb60a5a292bc37e4540767c960c75818", "url": "https://github.com/tensorflow/java/commit/1e22569dcb60a5a292bc37e4540767c960c75818", "message": "Fix nits", "committedDate": "2020-01-29T04:14:34Z", "type": "commit"}]}