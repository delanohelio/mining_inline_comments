{"pr_number": 1670, "pr_title": "[BC-25] Add snappy compression strategy for rpc messages", "pr_createdAt": "2020-04-24T23:35:02Z", "pr_url": "https://github.com/ConsenSys/teku/pull/1670", "timeline": [{"oid": "6634d70a44de3b73266606b89a3bf7a5fcd95868", "url": "https://github.com/ConsenSys/teku/commit/6634d70a44de3b73266606b89a3bf7a5fcd95868", "message": "Experiment with test fix", "committedDate": "2020-04-28T15:04:44Z", "type": "forcePushed"}, {"oid": "ddd13f70c5ec0b9ebcfa1e9dd6e7912d03cf6344", "url": "https://github.com/ConsenSys/teku/commit/ddd13f70c5ec0b9ebcfa1e9dd6e7912d03cf6344", "message": "Experiment with test fix", "committedDate": "2020-04-28T15:47:18Z", "type": "forcePushed"}, {"oid": "162d662bc8b0d14676c5c99134be1d2311dcbe4f", "url": "https://github.com/ConsenSys/teku/commit/162d662bc8b0d14676c5c99134be1d2311dcbe4f", "message": "Update tests to wait for async tasks to run", "committedDate": "2020-04-28T17:38:12Z", "type": "forcePushed"}, {"oid": "440fd76f38dbbd9fd75b84adf1cc49d8b97702a5", "url": "https://github.com/ConsenSys/teku/commit/440fd76f38dbbd9fd75b84adf1cc49d8b97702a5", "message": "Update tests to wait for async tasks to run", "committedDate": "2020-04-28T17:55:42Z", "type": "forcePushed"}, {"oid": "39b5e08229b424ac8a3d1c3f4ac4ed76a6fdd126", "url": "https://github.com/ConsenSys/teku/commit/39b5e08229b424ac8a3d1c3f4ac4ed76a6fdd126", "message": "Add cli option for enabling snappy compression", "committedDate": "2020-04-29T18:13:49Z", "type": "commit"}, {"oid": "857de0c480da22d65cd1ad856bed97a99845a14d", "url": "https://github.com/ConsenSys/teku/commit/857de0c480da22d65cd1ad856bed97a99845a14d", "message": "Set snappy compression according to network", "committedDate": "2020-04-29T18:13:49Z", "type": "commit"}, {"oid": "8612eca94b684dc8e7ff3af602a46fedf898f910", "url": "https://github.com/ConsenSys/teku/commit/8612eca94b684dc8e7ff3af602a46fedf898f910", "message": "Add ByteUtil, fix byte validation in RocksDB utils", "committedDate": "2020-04-29T18:13:49Z", "type": "commit"}, {"oid": "fca5db90c3b02c0dc36dfe3a24e16b1ce36f694a", "url": "https://github.com/ConsenSys/teku/commit/fca5db90c3b02c0dc36dfe3a24e16b1ce36f694a", "message": "Update async and future utils to accept exception-throwing methods", "committedDate": "2020-04-29T18:13:50Z", "type": "commit"}, {"oid": "78325105f244bd37c088a699c443d72788b32203", "url": "https://github.com/ConsenSys/teku/commit/78325105f244bd37c088a699c443d72788b32203", "message": "Rework RPC handler to operate on InputStreams, add Compressor's", "committedDate": "2020-04-29T18:13:50Z", "type": "commit"}, {"oid": "4194ba685cfec4c67f9d5469243d0fb66654f593", "url": "https://github.com/ConsenSys/teku/commit/4194ba685cfec4c67f9d5469243d0fb66654f593", "message": "Improve logging / error handling, small cleanup", "committedDate": "2020-04-29T18:13:50Z", "type": "commit"}, {"oid": "bf52c934a66f86c181a554fc77b3cecca25a1dd5", "url": "https://github.com/ConsenSys/teku/commit/bf52c934a66f86c181a554fc77b3cecca25a1dd5", "message": "Copy relevant bytes to OutputStream", "committedDate": "2020-04-29T18:13:50Z", "type": "commit"}, {"oid": "a2da34090b21e539a3d0eef9885c87a3f8464ba0", "url": "https://github.com/ConsenSys/teku/commit/a2da34090b21e539a3d0eef9885c87a3f8464ba0", "message": "Update tests to wait for async tasks to run", "committedDate": "2020-04-29T18:13:50Z", "type": "commit"}, {"oid": "dfe9d3418a7641eabacad274c39814049f302e4d", "url": "https://github.com/ConsenSys/teku/commit/dfe9d3418a7641eabacad274c39814049f302e4d", "message": "Select rpc encoding approach according to CLI configuration", "committedDate": "2020-04-29T18:15:53Z", "type": "commit"}, {"oid": "6c0ee76e3a05da16723292da51a7419ab2e80edb", "url": "https://github.com/ConsenSys/teku/commit/6c0ee76e3a05da16723292da51a7419ab2e80edb", "message": "Modify integration tests to run across encoding strategies", "committedDate": "2020-04-29T18:26:19Z", "type": "commit"}, {"oid": "48044e17daa1c08ef248e142fed832164843d625", "url": "https://github.com/ConsenSys/teku/commit/48044e17daa1c08ef248e142fed832164843d625", "message": "Don't close underlying inputStream when we uncompress data", "committedDate": "2020-04-29T18:26:23Z", "type": "commit"}, {"oid": "e79d61fa021d6179b8c69d70b5e7e6fc357d013f", "url": "https://github.com/ConsenSys/teku/commit/e79d61fa021d6179b8c69d70b5e7e6fc357d013f", "message": "Fix request handler tests", "committedDate": "2020-04-29T18:26:23Z", "type": "commit"}, {"oid": "ca9b65cdc79583ced1ce98180e0a359bcbe7164b", "url": "https://github.com/ConsenSys/teku/commit/ca9b65cdc79583ced1ce98180e0a359bcbe7164b", "message": "Run RequestHandler tests with different encodings", "committedDate": "2020-04-29T18:26:23Z", "type": "commit"}, {"oid": "ca9b65cdc79583ced1ce98180e0a359bcbe7164b", "url": "https://github.com/ConsenSys/teku/commit/ca9b65cdc79583ced1ce98180e0a359bcbe7164b", "message": "Run RequestHandler tests with different encodings", "committedDate": "2020-04-29T18:26:23Z", "type": "forcePushed"}, {"oid": "7394d5ac8719db5e236dc925816c698649e376c3", "url": "https://github.com/ConsenSys/teku/commit/7394d5ac8719db5e236dc925816c698649e376c3", "message": "Fix bad merge", "committedDate": "2020-04-29T18:59:01Z", "type": "commit"}, {"oid": "bbfbea0cf6a28d565e421d7e9636f02002be304a", "url": "https://github.com/ConsenSys/teku/commit/bbfbea0cf6a28d565e421d7e9636f02002be304a", "message": "Make default null value explicit", "committedDate": "2020-04-29T19:00:09Z", "type": "commit"}, {"oid": "2727a0b183d80178ad6c5487f2307628ebd5a13e", "url": "https://github.com/ConsenSys/teku/commit/2727a0b183d80178ad6c5487f2307628ebd5a13e", "message": "Cleanup - fix a few small issues", "committedDate": "2020-04-29T19:31:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzNTYwOQ==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r417635609", "bodyText": "Internally, the SnappyFramedInputStream blocks waiting for the next full frame to arrive. So if we get a partial frame over the wire, the SnappyCompressor will block waiting for the rest of the frame to arrive.", "author": "mbaxter", "createdAt": "2020-04-29T21:54:36Z", "path": "networking/eth2/src/test/java/tech/pegasys/artemis/networking/eth2/compression/SnappyCompressorTest.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright 2020 ConsenSys AG.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+ * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package tech.pegasys.artemis.networking.eth2.compression;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static tech.pegasys.artemis.networking.eth2.compression.SnappyCompressor.MAX_FRAME_CONTENT_SIZE;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.InputStream;\n+import java.io.PipedInputStream;\n+import java.io.PipedOutputStream;\n+import org.apache.tuweni.bytes.Bytes;\n+import org.junit.jupiter.api.Disabled;\n+import org.junit.jupiter.api.Test;\n+import tech.pegasys.artemis.datastructures.state.BeaconState;\n+import tech.pegasys.artemis.datastructures.util.DataStructureUtil;\n+import tech.pegasys.artemis.datastructures.util.SimpleOffsetSerializer;\n+\n+public class SnappyCompressorTest {\n+  private final DataStructureUtil dataStructureUtil = new DataStructureUtil();\n+  private final Compressor compressor = new SnappyCompressor();\n+\n+  @Test\n+  public void roundTrip() throws Exception {\n+    final BeaconState state = dataStructureUtil.randomBeaconState(0);\n+    final Bytes serializedState =\n+        Bytes.wrap(SimpleOffsetSerializer.serialize(state).toArrayUnsafe());\n+\n+    final Bytes compressed = compressor.compress(serializedState);\n+    assertThat(compressed).isNotEqualTo(serializedState);\n+\n+    final Bytes uncompressed = compressor.uncompress(compressed);\n+    assertThat(uncompressed).isEqualTo(serializedState);\n+  }\n+\n+  @Test\n+  public void uncompress_invalidData() {\n+    final BeaconState state = dataStructureUtil.randomBeaconState(0);\n+    final Bytes serializedState =\n+        Bytes.wrap(SimpleOffsetSerializer.serialize(state).toArrayUnsafe());\n+\n+    assertThatThrownBy(() -> compressor.uncompress(serializedState))\n+        .isInstanceOf(CompressionException.class);\n+  }\n+\n+  @Test\n+  public void uncompress_seriesOfValues() throws Exception {\n+    final BeaconState stateA = dataStructureUtil.randomBeaconState(0);\n+    final BeaconState stateB = dataStructureUtil.randomBeaconState(1);\n+    final Bytes serializedStateA =\n+        Bytes.wrap(SimpleOffsetSerializer.serialize(stateA).toArrayUnsafe());\n+    final Bytes serializedStateB =\n+        Bytes.wrap(SimpleOffsetSerializer.serialize(stateB).toArrayUnsafe());\n+\n+    final Bytes compressedA = compressor.compress(serializedStateA);\n+    final Bytes compressedB = compressor.compress(serializedStateB);\n+    final Bytes compressedSeries = Bytes.concatenate(compressedA, compressedB);\n+    final InputStream input = new ByteArrayInputStream(compressedSeries.toArrayUnsafe());\n+\n+    // Get first value\n+    final Bytes uncompressed = compressor.uncompress(input, serializedStateA.size());\n+    assertThat(uncompressed).isEqualTo(serializedStateA);\n+    // Then next value\n+    final Bytes uncompressed2 = compressor.uncompress(input, serializedStateB.size());\n+    assertThat(uncompressed2).isEqualTo(serializedStateB);\n+    // Input stream should now be closed\n+    assertThat(input.available()).isEqualTo(0);\n+    assertThat(input.read()).isEqualTo(-1);\n+  }\n+\n+  @Test\n+  public void uncompress_partialValue() throws Exception {\n+    final BeaconState state = dataStructureUtil.randomBeaconState(0);\n+    final Bytes serializedState =\n+        Bytes.wrap(SimpleOffsetSerializer.serialize(state).toArrayUnsafe());\n+\n+    final Bytes compressed = compressor.compress(serializedState);\n+    final int maxBytes = MAX_FRAME_CONTENT_SIZE / 2;\n+    // Check assumptions\n+    assertThat(serializedState.size()).isGreaterThan(MAX_FRAME_CONTENT_SIZE);\n+\n+    final InputStream input = new ByteArrayInputStream(compressed.toArrayUnsafe());\n+    final Bytes uncompressed = compressor.uncompress(input, maxBytes);\n+    assertThat(uncompressed.size()).isLessThanOrEqualTo(maxBytes);\n+  }\n+\n+  @Test\n+  @Disabled(\"SnappyCompressor will currently block in this case\")", "originalCommit": "2727a0b183d80178ad6c5487f2307628ebd5a13e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMyNDYzMg==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418324632", "bodyText": "This issue has been fixed and the test has been enabled.", "author": "mbaxter", "createdAt": "2020-04-30T22:27:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzNTYwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzNzA2Nw==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r417637067", "bodyText": "The networking spec says:\n\nA reader SHOULD NOT read more than max_encoded_len(n) bytes after reading the SSZ length prefix n from the header.\n\nBut I don't think we can actually enforce that atm because of how the Snappy library works.  See: https://github.com/PegaSysEng/teku/pull/1670/files#r417635609", "author": "mbaxter", "createdAt": "2020-04-29T21:57:57Z", "path": "networking/eth2/src/main/java/tech/pegasys/artemis/networking/eth2/compression/Compressor.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Copyright 2020 ConsenSys AG.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+ * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package tech.pegasys.artemis.networking.eth2.compression;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import org.apache.tuweni.bytes.Bytes;\n+\n+public interface Compressor {\n+\n+  /**\n+   * Returns the compressed data\n+   *\n+   * @param data The data to compress\n+   * @return The compressed data\n+   */\n+  Bytes compress(final Bytes data);\n+\n+  /**\n+   * Returns the uncompressed data.\n+   *\n+   * @param data The data to uncompress.\n+   * @return The uncompressed data.\n+   */\n+  default Bytes uncompress(final Bytes data) throws CompressionException {\n+    try (final InputStream byteStream = new ByteArrayInputStream(data.toArrayUnsafe())) {\n+      // Read everything\n+      return uncompress(byteStream, Integer.MAX_VALUE);\n+    } catch (IOException e) {\n+      throw new RuntimeException(\n+          \"Unexpected error encountered while preparing to uncompress bytes\", e);\n+    }\n+  }\n+\n+  /**\n+   * Uncompress up to {@code maxBytes} bytes. May return fewer bytes if the end of stream is\n+   * detected or an error occurs.\n+   *\n+   * @param input The underlying {@link InputStream} to read from.\n+   * @param maxBytes The maximum number of uncompressed bytes to produce\n+   * @return The uncompressed bytes read from the underlying inputStream {@code input} stream.\n+   */\n+  Bytes uncompress(final InputStream input, final int maxBytes) throws CompressionException;\n+\n+  /**\n+   * Returns a maximum estimate of the size of a compressed payload given the uncompressed payload\n+   * size.\n+   *\n+   * @param uncompressedLength The size of an uncompressed payload.\n+   * @return The maximum size of a payload of size {@code uncompressedLength} after compression.\n+   */\n+  int getMaxCompressedLength(final int uncompressedLength);", "originalCommit": "2727a0b183d80178ad6c5487f2307628ebd5a13e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczNzY4Mg==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r417737682", "bodyText": "I'm not 100% sure I've understood exactly where the issue enforcing this check is so these comments may be off the mark but...\nI think the main security concern is that we can't be made to read massive amounts of data from the stream.  The total number of uncompressed bytes we could read is limited but there's a risk that we could be fed a series of effectively empty snappy frames so that we never reach that limit. Not sure if that's actually possible with snappy or if it would be rejected as invalid compressed data.\nOne option we could have is to limit the underlying input stream.  ie we have an InputStream that is the source of compressed data - we could wrap it in a custom InputStream that mostly just delegates but keeps track of the number of bytes read and throws an exception if we wind up reading more than the expected limit.", "author": "ajsutton", "createdAt": "2020-04-30T03:48:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzNzA2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE1NjY4Mw==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418156683", "bodyText": "Yeah, you're right.  I was hoping snappy was smarter about what constitutes a valid sequence of frames, but it will happily decode a sequence of identical frames each with a 1-byte payload: 8eb8780\nSo yeah, we'll have to have some kind of wrapped InputStream that tracks the number of bytes actually read.", "author": "mbaxter", "createdAt": "2020-04-30T17:01:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzNzA2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMyNjAzNQ==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418326035", "bodyText": "Turns out guava has a util for created a limited input stream.  It's marked @Beta: https://guava.dev/releases/19.0/api/docs/com/google/common/annotations/Beta.html.  But I think that's fine for us?  If not, I started on an implementation that we could swap in.", "author": "mbaxter", "createdAt": "2020-04-30T22:31:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzNzA2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMzMTQyOA==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418331428", "bodyText": "I'm fine with using the Guava version.", "author": "ajsutton", "createdAt": "2020-04-30T22:46:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzNzA2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzODg0Ng==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r417638846", "bodyText": "(discussion) Here, we're processing the input asynchronously using the standard DelayedExecutorAsyncRunner.  Wondering if we should run these tasks in a dedicated thread pool?", "author": "mbaxter", "createdAt": "2020-04-29T22:01:42Z", "path": "networking/p2p/src/main/java/tech/pegasys/artemis/networking/p2p/libp2p/rpc/RpcHandler.java", "diffHunk": "@@ -169,11 +194,25 @@ public void setRequestHandler(RpcRequestHandler rpcRequestHandler) {\n         throw new IllegalStateException(\"Attempt to set an already set data handler\");\n       }\n       this.rpcRequestHandler = rpcRequestHandler;\n-      activeFuture.thenAccept(__ -> rpcRequestHandler.onActivation(rpcStream)).reportExceptions();\n-      while (!bufferedData.isEmpty()) {\n-        ByteBuf currentBuffer = bufferedData.remove(0);\n-        this.rpcRequestHandler.onData(nodeId, rpcStream, currentBuffer);\n-      }\n+\n+      activeFuture\n+          .thenCompose(\n+              __ ->\n+                  asyncRunner.runAsync(", "originalCommit": "2727a0b183d80178ad6c5487f2307628ebd5a13e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY5NzIyOQ==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r417697229", "bodyText": "Yeah I've been generally trying to ensure we use a single async runner that's injected rather than creating them all over the place so we can start to make decisions about how many threads it can use etc.\nIt probably makes sense to have networking use it's own thread pool.  If we're blocking we may need to make the limit on number of threads quite high, but it would definitely be good to know how many threads are being used by networking and be able to limit them independently.", "author": "ajsutton", "createdAt": "2020-04-30T00:58:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzODg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMyNjQxNQ==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418326415", "bodyText": "I think this might require a bit of reorganization and this PR is already pretty big, so I think I'll log a ticket to follow-up on this if that sounds fine to you.", "author": "mbaxter", "createdAt": "2020-04-30T22:32:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzODg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMzMTM3Ng==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418331376", "bodyText": "Yep agreed.", "author": "ajsutton", "createdAt": "2020-04-30T22:46:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzODg0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzOTgzOQ==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r417639839", "bodyText": "We might want to have some kind of custom InputStream that runs over appended ByteBuf instances, similar to how we were previously managing these ByteBuf's in MessageBuffer.  If we think that's worth doing, I can add a follow-up ticket.", "author": "mbaxter", "createdAt": "2020-04-29T22:03:55Z", "path": "networking/p2p/src/main/java/tech/pegasys/artemis/networking/p2p/libp2p/rpc/RpcHandler.java", "diffHunk": "@@ -157,10 +174,18 @@ public RpcStream getRpcStream() {\n \n     @Override\n     protected void channelRead0(final ChannelHandlerContext ctx, final ByteBuf msg) {\n-      if (rpcRequestHandler != null) {\n-        rpcRequestHandler.onData(nodeId, rpcStream, msg);\n-      } else {\n-        bufferedData.add(msg);\n+      if (outputStreamClosed) {\n+        // Discard any data if output stream has been closed\n+        return;\n+      }\n+      try {\n+        // TODO - we may want to optimize this to pass on ByteBuf's directly and manage their\n+        //  garbage collection rather than immediately copying these bytes\n+        final Bytes bytes = Bytes.wrapByteBuf(msg);\n+        outputStream.write(bytes.toArray());", "originalCommit": "2727a0b183d80178ad6c5487f2307628ebd5a13e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3NjIzNQ==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r417676235", "bodyText": "Slightly crazy idea and probably something for a follow up PR anyway, but what if we had a test fixture class implementing ArgumentsProvider that could gave a stream of Eth2Network.builder().rpcEncoding(..) with each of the encodings.  Then you could just use @ArgumentsSource(RpcEncodingsProvider.class) on methods.", "author": "ajsutton", "createdAt": "2020-04-29T23:46:22Z", "path": "networking/eth2/src/integration-test/java/tech/pegasys/artemis/networking/eth2/PeerStatusIntegrationTest.java", "diffHunk": "@@ -51,17 +55,29 @@ public void tearDown() {\n     networkFactory.stopAll();\n   }\n \n-  @Test\n-  public void shouldExchangeStatusMessagesOnConnection() throws Exception {\n+  public static Stream<Arguments> getEncodings() {\n+    final List<RpcEncoding> encodings = List.of(RpcEncoding.SSZ, RpcEncoding.SSZ_SNAPPY);\n+    return encodings.stream().map(e -> Arguments.of(e.getName(), e));\n+  }", "originalCommit": "2727a0b183d80178ad6c5487f2307628ebd5a13e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMzQ5OQ==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418123499", "bodyText": "Yeah - I'm a bit unhappy with all of the duplication of the arguments boilerplate.  Sounds like a good idea \ud83d\udc4d", "author": "mbaxter", "createdAt": "2020-04-30T16:07:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3NjIzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4NDU1MA==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r417684550", "bodyText": "The potential risk here is that SnappyFramedInputStream maintains any kind of internal state or buffer of read data.  Sounds like it works now but it feels slightly dangerous to be creating and discarding wrapper input streams.\nIdeally when the input stream was first created, the compressor would get a chance to wrap it and then you could just use it as an input stream to read data from.  It might be possible to do that in the RpcRequestHandler but not entirely sure.", "author": "ajsutton", "createdAt": "2020-04-30T00:13:12Z", "path": "networking/eth2/src/main/java/tech/pegasys/artemis/networking/eth2/compression/SnappyCompressor.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Copyright 2020 ConsenSys AG.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+ * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package tech.pegasys.artemis.networking.eth2.compression;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import org.apache.tuweni.bytes.Bytes;\n+import org.xerial.snappy.SnappyFramedInputStream;\n+import org.xerial.snappy.SnappyFramedOutputStream;\n+import tech.pegasys.artemis.util.iostreams.DelegatingInputStream;\n+\n+public class SnappyCompressor implements Compressor {\n+  // The max uncompressed bytes that will be packed into a single frame\n+  // See:\n+  // https://github.com/google/snappy/blob/251d935d5096da77c4fef26ea41b019430da5572/framing_format.txt#L104-L106\n+  static final int MAX_FRAME_CONTENT_SIZE = 65536;\n+\n+  @Override\n+  public Bytes compress(final Bytes data) {\n+\n+    try (final ByteArrayOutputStream out = new ByteArrayOutputStream(data.size() / 2);\n+        final OutputStream compressor = new SnappyFramedOutputStream(out)) {\n+      compressor.write(data.toArrayUnsafe());\n+      compressor.flush();\n+      return Bytes.wrap(out.toByteArray());\n+    } catch (IOException e) {\n+      throw new RuntimeException(\"Failed to compress data\", e);\n+    }\n+  }\n+\n+  @Override\n+  public Bytes uncompress(final InputStream input, final int maxBytes) throws CompressionException {\n+    // This is a bit of a hack - but we don't want to close the underlying stream when\n+    // we close the SnappyFramedInputStream\n+    final UnclosableInputStream wrappedStream = new UnclosableInputStream(input);\n+\n+    try (final InputStream snappyIn = new SnappyFramedInputStream(wrappedStream)) {\n+      return Bytes.wrap(snappyIn.readNBytes(maxBytes));\n+    } catch (IOException e) {", "originalCommit": "2727a0b183d80178ad6c5487f2307628ebd5a13e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE2MzM4Nw==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418163387", "bodyText": "I can rework the javadoc to make it clear that the maxBytes param is really the expected size of the final payload to uncompress.  If the SnappyFramedInputStream ends up pulling more data than expected from the underlying stream, it means that there's extra data appended that we don't expect, so it's an error anyway.\nThe problem with wrapping the initial input stream is that we're not actually processing a series of snappy data.  For responses, we're processing a sequence of statusCode|lengthPrefix|snappyData.  So it's not really straightforward.  I guess we could wrap the entire inputStream, and then read from the underlying inputStream when we process the status and lengthPrefix, but that seems kind of messy too.", "author": "mbaxter", "createdAt": "2020-04-30T17:13:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4NDU1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODIzNTUxOA==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418235518", "bodyText": "I reworked the API, but it may be cleaner to try the other approach - I'll experiment.", "author": "mbaxter", "createdAt": "2020-04-30T19:21:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4NDU1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMzMTA3Nw==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418331077", "bodyText": "Hmm, yeah the fact that it's not all snappy data makes life much more interesting. I don't think it would be right to just always wrap it so maybe better to stick with what we have then.\nThe completely correct answer is probably to read statusCode|lengthPrefix from the input stream, then wrap it with SnappyFramedInputStream and use that to read until the end of that message (even if it's received across multiple Netty reads).  But there's still a risk the snappy stream read past the end of the message which would consume the status code and length prefix of the next message.  So I think we're always dependent on SnappyFramedInputStream not doing any read ahead regardless. If that becomes an issue we may need to contribute to the upstream project or fork it.", "author": "ajsutton", "createdAt": "2020-04-30T22:46:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4NDU1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODczMjAyNw==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418732027", "bodyText": "The completely correct answer is probably to read statusCode|lengthPrefix from the input stream, then wrap it with SnappyFramedInputStream and use that to read until the end of that message (even if it's received across multiple Netty reads).\n\nThat's pretty much what we're doing now.  But yeah, if snappy reads too much that's still a problem.  Though since we're asking it for a specific number of bytes which should correspond to the full uncompressed payload, it doesn't really have a reason to read beyond the current chunk unless the message is malformed.", "author": "mbaxter", "createdAt": "2020-05-01T20:50:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4NDU1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4NDU5Ng==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r417684596", "bodyText": "We don't ever seem to call this method.", "author": "ajsutton", "createdAt": "2020-04-30T00:13:23Z", "path": "networking/eth2/src/main/java/tech/pegasys/artemis/networking/eth2/compression/SnappyCompressor.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Copyright 2020 ConsenSys AG.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+ * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package tech.pegasys.artemis.networking.eth2.compression;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import org.apache.tuweni.bytes.Bytes;\n+import org.xerial.snappy.SnappyFramedInputStream;\n+import org.xerial.snappy.SnappyFramedOutputStream;\n+import tech.pegasys.artemis.util.iostreams.DelegatingInputStream;\n+\n+public class SnappyCompressor implements Compressor {\n+  // The max uncompressed bytes that will be packed into a single frame\n+  // See:\n+  // https://github.com/google/snappy/blob/251d935d5096da77c4fef26ea41b019430da5572/framing_format.txt#L104-L106\n+  static final int MAX_FRAME_CONTENT_SIZE = 65536;\n+\n+  @Override\n+  public Bytes compress(final Bytes data) {\n+\n+    try (final ByteArrayOutputStream out = new ByteArrayOutputStream(data.size() / 2);\n+        final OutputStream compressor = new SnappyFramedOutputStream(out)) {\n+      compressor.write(data.toArrayUnsafe());\n+      compressor.flush();\n+      return Bytes.wrap(out.toByteArray());\n+    } catch (IOException e) {\n+      throw new RuntimeException(\"Failed to compress data\", e);\n+    }\n+  }\n+\n+  @Override\n+  public Bytes uncompress(final InputStream input, final int maxBytes) throws CompressionException {\n+    // This is a bit of a hack - but we don't want to close the underlying stream when\n+    // we close the SnappyFramedInputStream\n+    final UnclosableInputStream wrappedStream = new UnclosableInputStream(input);\n+\n+    try (final InputStream snappyIn = new SnappyFramedInputStream(wrappedStream)) {\n+      return Bytes.wrap(snappyIn.readNBytes(maxBytes));\n+    } catch (IOException e) {\n+      throw new CompressionException(\"Unable to uncompress data\", e);\n+    }\n+  }\n+\n+  @Override\n+  public int getMaxCompressedLength(final int uncompressedLength) {", "originalCommit": "2727a0b183d80178ad6c5487f2307628ebd5a13e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE2NDc0Mg==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418164742", "bodyText": "This is because we're not strictly enforcing read limits atm.  Added a test that uses this and we'll need it to handle the limits properly.", "author": "mbaxter", "createdAt": "2020-04-30T17:15:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4NDU5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMyNjczNQ==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418326735", "bodyText": "Update - we're now using this to enforce read limits.", "author": "mbaxter", "createdAt": "2020-04-30T22:33:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4NDU5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4NzAwNQ==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r417687005", "bodyText": "In terms of being able to re-use the one input stream instead of discarding the wrapper each time - I wonder if responseDecoder rather than being used directly could be changed to something like responseDecoder.forInputStream(input) and that returns an object that provides the decodeNextResponse methods - then it'd be able to hold onto the one snappy wrapper stream.  I think you'd have to do the same trick with RpcEncoding having a decoderFor(InputStream) kind of method too and maybe again at the actual Compressor level.\nUnfortunate amount of boiler plate code there but SnappyFramedInputStream has a frameHeader buffer which worries me that would could potentially lose bytes.", "author": "ajsutton", "createdAt": "2020-04-30T00:22:09Z", "path": "networking/eth2/src/main/java/tech/pegasys/artemis/networking/eth2/rpc/core/Eth2OutgoingRequestHandler.java", "diffHunk": "@@ -57,56 +58,70 @@ public Eth2OutgoingRequestHandler(\n \n     responseProcessor =\n         new AsyncResponseProcessor<>(asyncRunner, responseStream, this::onAsyncProcessorError);\n-    responseHandler = new ResponseRpcDecoder<>(responseProcessor::processResponse, this.method);\n+    responseDecoder = method.createResponseDecoder();\n   }\n \n-  @Override\n-  public void onActivation(final RpcStream rpcStream) {\n-    this.rpcStream = rpcStream;\n+  public void handleInitialPayloadSent(final RpcStream stream) {\n+    this.rpcStream = stream;\n+\n+    if (method.shouldReceiveResponse()) {\n+      // Close the write side of the stream\n+      stream.closeWriteStream().reportExceptions();\n+      // Start timer for first bytes\n+      ensureFirstBytesArriveWithinTimeLimit(stream);\n+    } else {\n+      // If we're not expecting any response, complete the request\n+      completeRequest(stream);\n+    }\n   }\n \n   @Override\n-  public void onData(final NodeId nodeId, final RpcStream rpcStream, final ByteBuf bytes) {\n+  public void processInput(\n+      final NodeId nodeId, final RpcStream rpcStream, final InputStream input) {\n     try {\n-      if (hasReceivedInitialBytes.compareAndSet(false, true)) {\n-        // Setup initial chunk timeout\n-        ensureNextResponseArrivesInTime(rpcStream, currentChunkCount.get(), currentChunkCount);\n-      }\n-      LOG.trace(\"Requester received {} bytes.\", bytes.capacity());\n-      responseHandler.onDataReceived(bytes);\n-\n-      final int previousResponseCount = currentChunkCount.get();\n-      currentChunkCount.set(responseProcessor.getResponseCount());\n-      if (currentChunkCount.get() >= maximumResponseChunks) {\n-        completeRequest(rpcStream);\n-      } else if (currentChunkCount.get() > previousResponseCount) {\n-        ensureNextResponseArrivesInTime(rpcStream, currentChunkCount.get(), currentChunkCount);\n+      this.rpcStream = rpcStream;\n+\n+      Optional<TResponse> maybeResponse =\n+          responseDecoder.decodeNextResponse(input, this::onFirstByteReceived);", "originalCommit": "2727a0b183d80178ad6c5487f2307628ebd5a13e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY5MTQ1Mw==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r417691453", "bodyText": "We probably want to get this test working before we merge.  Will unfortunately have to be multithreaded I guess and likely not all that easy to write but seems quite important.", "author": "ajsutton", "createdAt": "2020-04-30T00:36:35Z", "path": "networking/eth2/src/test/java/tech/pegasys/artemis/networking/eth2/compression/SnappyCompressorTest.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright 2020 ConsenSys AG.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+ * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package tech.pegasys.artemis.networking.eth2.compression;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static tech.pegasys.artemis.networking.eth2.compression.SnappyCompressor.MAX_FRAME_CONTENT_SIZE;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.InputStream;\n+import java.io.PipedInputStream;\n+import java.io.PipedOutputStream;\n+import org.apache.tuweni.bytes.Bytes;\n+import org.junit.jupiter.api.Disabled;\n+import org.junit.jupiter.api.Test;\n+import tech.pegasys.artemis.datastructures.state.BeaconState;\n+import tech.pegasys.artemis.datastructures.util.DataStructureUtil;\n+import tech.pegasys.artemis.datastructures.util.SimpleOffsetSerializer;\n+\n+public class SnappyCompressorTest {\n+  private final DataStructureUtil dataStructureUtil = new DataStructureUtil();\n+  private final Compressor compressor = new SnappyCompressor();\n+\n+  @Test\n+  public void roundTrip() throws Exception {\n+    final BeaconState state = dataStructureUtil.randomBeaconState(0);\n+    final Bytes serializedState =\n+        Bytes.wrap(SimpleOffsetSerializer.serialize(state).toArrayUnsafe());\n+\n+    final Bytes compressed = compressor.compress(serializedState);\n+    assertThat(compressed).isNotEqualTo(serializedState);\n+\n+    final Bytes uncompressed = compressor.uncompress(compressed);\n+    assertThat(uncompressed).isEqualTo(serializedState);\n+  }\n+\n+  @Test\n+  public void uncompress_invalidData() {\n+    final BeaconState state = dataStructureUtil.randomBeaconState(0);\n+    final Bytes serializedState =\n+        Bytes.wrap(SimpleOffsetSerializer.serialize(state).toArrayUnsafe());\n+\n+    assertThatThrownBy(() -> compressor.uncompress(serializedState))\n+        .isInstanceOf(CompressionException.class);\n+  }\n+\n+  @Test\n+  public void uncompress_seriesOfValues() throws Exception {\n+    final BeaconState stateA = dataStructureUtil.randomBeaconState(0);\n+    final BeaconState stateB = dataStructureUtil.randomBeaconState(1);\n+    final Bytes serializedStateA =\n+        Bytes.wrap(SimpleOffsetSerializer.serialize(stateA).toArrayUnsafe());\n+    final Bytes serializedStateB =\n+        Bytes.wrap(SimpleOffsetSerializer.serialize(stateB).toArrayUnsafe());\n+\n+    final Bytes compressedA = compressor.compress(serializedStateA);\n+    final Bytes compressedB = compressor.compress(serializedStateB);\n+    final Bytes compressedSeries = Bytes.concatenate(compressedA, compressedB);\n+    final InputStream input = new ByteArrayInputStream(compressedSeries.toArrayUnsafe());\n+\n+    // Get first value\n+    final Bytes uncompressed = compressor.uncompress(input, serializedStateA.size());\n+    assertThat(uncompressed).isEqualTo(serializedStateA);\n+    // Then next value\n+    final Bytes uncompressed2 = compressor.uncompress(input, serializedStateB.size());\n+    assertThat(uncompressed2).isEqualTo(serializedStateB);\n+    // Input stream should now be closed\n+    assertThat(input.available()).isEqualTo(0);\n+    assertThat(input.read()).isEqualTo(-1);\n+  }\n+\n+  @Test\n+  public void uncompress_partialValue() throws Exception {\n+    final BeaconState state = dataStructureUtil.randomBeaconState(0);\n+    final Bytes serializedState =\n+        Bytes.wrap(SimpleOffsetSerializer.serialize(state).toArrayUnsafe());\n+\n+    final Bytes compressed = compressor.compress(serializedState);\n+    final int maxBytes = MAX_FRAME_CONTENT_SIZE / 2;\n+    // Check assumptions\n+    assertThat(serializedState.size()).isGreaterThan(MAX_FRAME_CONTENT_SIZE);\n+\n+    final InputStream input = new ByteArrayInputStream(compressed.toArrayUnsafe());\n+    final Bytes uncompressed = compressor.uncompress(input, maxBytes);\n+    assertThat(uncompressed.size()).isLessThanOrEqualTo(maxBytes);\n+  }\n+\n+  @Test\n+  @Disabled(\"SnappyCompressor will currently block in this case\")", "originalCommit": "2727a0b183d80178ad6c5487f2307628ebd5a13e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMyNjkzMg==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r418326932", "bodyText": "This test is enabled and works now.", "author": "mbaxter", "createdAt": "2020-04-30T22:34:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY5MTQ1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY5MzgyNg==", "url": "https://github.com/ConsenSys/teku/pull/1670#discussion_r417693826", "bodyText": "On the plus side, copying the data means we don't have to manage retaining and releasing the ByteBuf if we jump threads later.  So unless this shows up as a significant performance issue it would be great to leave as-is.", "author": "ajsutton", "createdAt": "2020-04-30T00:45:26Z", "path": "networking/p2p/src/main/java/tech/pegasys/artemis/networking/p2p/libp2p/rpc/RpcHandler.java", "diffHunk": "@@ -157,10 +174,18 @@ public RpcStream getRpcStream() {\n \n     @Override\n     protected void channelRead0(final ChannelHandlerContext ctx, final ByteBuf msg) {\n-      if (rpcRequestHandler != null) {\n-        rpcRequestHandler.onData(nodeId, rpcStream, msg);\n-      } else {\n-        bufferedData.add(msg);\n+      if (outputStreamClosed) {\n+        // Discard any data if output stream has been closed\n+        return;\n+      }\n+      try {\n+        // TODO - we may want to optimize this to pass on ByteBuf's directly and manage their\n+        //  garbage collection rather than immediately copying these bytes\n+        final Bytes bytes = Bytes.wrapByteBuf(msg);", "originalCommit": "2727a0b183d80178ad6c5487f2307628ebd5a13e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0e83546ae4c43520f6efcce98ee827d4bd9206ef", "url": "https://github.com/ConsenSys/teku/commit/0e83546ae4c43520f6efcce98ee827d4bd9206ef", "message": "Merge branch 'master' into bc-25/compress-rpc-messages", "committedDate": "2020-04-30T15:40:40Z", "type": "commit"}, {"oid": "8eb87800d7f0267a055ee36e8a4687eeafcba857", "url": "https://github.com/ConsenSys/teku/commit/8eb87800d7f0267a055ee36e8a4687eeafcba857", "message": "Add test case for malicious payload, currently disabled\n\nWe need to rework the compression logic to handle this case", "committedDate": "2020-04-30T16:56:41Z", "type": "commit"}, {"oid": "d424fb768a1fd43dfe3768e39aafc8aea420b1b1", "url": "https://github.com/ConsenSys/teku/commit/d424fb768a1fd43dfe3768e39aafc8aea420b1b1", "message": "Rework Compressor API to explicitly require expected payload size", "committedDate": "2020-04-30T17:52:14Z", "type": "commit"}, {"oid": "347d6cdc1d4829755b260a77eafef5d077c78259", "url": "https://github.com/ConsenSys/teku/commit/347d6cdc1d4829755b260a77eafef5d077c78259", "message": "Extract constant to utils", "committedDate": "2020-04-30T20:16:18Z", "type": "commit"}, {"oid": "dc4c5ff6253e2889b24c426fff4c1fb13fdd65db", "url": "https://github.com/ConsenSys/teku/commit/dc4c5ff6253e2889b24c426fff4c1fb13fdd65db", "message": "Limit the number of bytes we're allowed to read while uncompressing", "committedDate": "2020-04-30T20:16:49Z", "type": "commit"}, {"oid": "bb3f874e9004f325c9bdf1a71110ada65f0f577f", "url": "https://github.com/ConsenSys/teku/commit/bb3f874e9004f325c9bdf1a71110ada65f0f577f", "message": "Be more forgiving when handling requests", "committedDate": "2020-04-30T20:44:00Z", "type": "commit"}, {"oid": "66c6104851bb2e2b183852878c52ed91e96900ee", "url": "https://github.com/ConsenSys/teku/commit/66c6104851bb2e2b183852878c52ed91e96900ee", "message": "Merge branch 'master' into bc-25/compress-rpc-messages", "committedDate": "2020-04-30T22:24:46Z", "type": "commit"}, {"oid": "d3e46e1214fcc1727200f4f670474721f8526d33", "url": "https://github.com/ConsenSys/teku/commit/d3e46e1214fcc1727200f4f670474721f8526d33", "message": "Update comment", "committedDate": "2020-04-30T23:12:52Z", "type": "commit"}, {"oid": "0c94eac258ad2f49cd408e820dcb1b7934d36c72", "url": "https://github.com/ConsenSys/teku/commit/0c94eac258ad2f49cd408e820dcb1b7934d36c72", "message": "Fix MockInputStream implementation, fix error encoding in test", "committedDate": "2020-04-30T23:13:15Z", "type": "commit"}]}