{"pr_number": 2288, "pr_title": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer", "pr_createdAt": "2020-10-19T09:36:08Z", "pr_url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288", "timeline": [{"oid": "d79eae8548866858e58088e3ea29b2dd33025563", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/d79eae8548866858e58088e3ea29b2dd33025563", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-20T16:18:03Z", "type": "forcePushed"}, {"oid": "573a3a77f45f7b7ac38d9b9da851be60e02abaeb", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/573a3a77f45f7b7ac38d9b9da851be60e02abaeb", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-21T11:36:02Z", "type": "forcePushed"}, {"oid": "e21df6112b0f9b0c88cb5129cabfa94de9e4a525", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/e21df6112b0f9b0c88cb5129cabfa94de9e4a525", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-21T14:57:34Z", "type": "forcePushed"}, {"oid": "67f19ebca307e4d3c223b704c040d024d16bc600", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/67f19ebca307e4d3c223b704c040d024d16bc600", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-21T15:29:59Z", "type": "forcePushed"}, {"oid": "e29dee4abca91ee94fa1028d6837290b9e0d3f50", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/e29dee4abca91ee94fa1028d6837290b9e0d3f50", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-22T14:20:14Z", "type": "forcePushed"}, {"oid": "c42d9cdb39dda016b37754a2dae093952e4b3f9e", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/c42d9cdb39dda016b37754a2dae093952e4b3f9e", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-22T14:42:23Z", "type": "forcePushed"}, {"oid": "8f38cf875fcfa8c2a275bc5f38fd1782bce3c269", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/8f38cf875fcfa8c2a275bc5f38fd1782bce3c269", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-23T17:26:57Z", "type": "forcePushed"}, {"oid": "34ff698efbf4c1aa19008897dcfaaba18a3b5596", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/34ff698efbf4c1aa19008897dcfaaba18a3b5596", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-23T18:41:27Z", "type": "forcePushed"}, {"oid": "81f1268ab2d93b5505f2ea402ec93fc39cb3053c", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/81f1268ab2d93b5505f2ea402ec93fc39cb3053c", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-26T10:20:05Z", "type": "forcePushed"}, {"oid": "0d1dddbc5234e2dbf427989466eca1e4551e98b3", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/0d1dddbc5234e2dbf427989466eca1e4551e98b3", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-26T13:33:02Z", "type": "forcePushed"}, {"oid": "3bd246b0b13958748a0ef793ea34a08c7665d8cd", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/3bd246b0b13958748a0ef793ea34a08c7665d8cd", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-26T15:54:34Z", "type": "forcePushed"}, {"oid": "0081d4a8a9f0dd1e67a9543658a4a24dce8026bf", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/0081d4a8a9f0dd1e67a9543658a4a24dce8026bf", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-26T15:59:52Z", "type": "forcePushed"}, {"oid": "7f7818850b0c9eb4a87da0f7fec7cce1a97e767c", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/7f7818850b0c9eb4a87da0f7fec7cce1a97e767c", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-26T16:06:38Z", "type": "forcePushed"}, {"oid": "810d0ff22b517a4abd002d813b18888d4f4c98f1", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/810d0ff22b517a4abd002d813b18888d4f4c98f1", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-26T16:28:03Z", "type": "forcePushed"}, {"oid": "be3cb973510d04f8aabe096d1c4d01db934e719f", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/be3cb973510d04f8aabe096d1c4d01db934e719f", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-26T18:00:43Z", "type": "forcePushed"}, {"oid": "f3abdf0ed5d9cf68bae2eaa5e40b553d23af532a", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/f3abdf0ed5d9cf68bae2eaa5e40b553d23af532a", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-27T08:17:03Z", "type": "forcePushed"}, {"oid": "af57f54bddf624b72405dba9b935ead67b97fb89", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/af57f54bddf624b72405dba9b935ead67b97fb89", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-28T17:15:41Z", "type": "forcePushed"}, {"oid": "702567a869b50937e72c6dd7c121751cda401833", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/702567a869b50937e72c6dd7c121751cda401833", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-30T08:13:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3Mjk5Nw==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r514972997", "bodyText": "Maybe it's worth checking node is not null to avoid any potential NPE", "author": "afalhambra", "createdAt": "2020-10-30T09:37:07Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/CloudEvent.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+\n+class CloudEvent<T> {\n+\n+    private static ObjectMapper mapper = new ObjectMapper()\n+            .setDateFormat(new SimpleDateFormat(System.getProperty(\n+                    KafkaServerExtension.KAFKA_EXTENSION_PREFIX + \"json.date_format\", System.getProperty(\n+                            \"org.kie.server.json.date_format\",\n+                            \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"))));\n+\n+    private String specVersion;\n+    private Date time;\n+    private String id;\n+    private String type;\n+    private String source;\n+    private T data;\n+\n+    public static <T> CloudEvent<T> read(byte[] bytes, Class<T> type) throws IOException, ParseException {\n+        JsonNode node = mapper.readTree(bytes);\n+        CloudEvent<T> cloudEvent = new CloudEvent<>();\n+        if (node.has(\"id\")) {", "originalCommit": "702567a869b50937e72c6dd7c121751cda401833", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2OTIxMg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515169212", "bodyText": "I believe readTree hardly retursn null, hence there is not point in checking", "author": "fjtirado", "createdAt": "2020-10-30T15:10:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3Mjk5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk4OTE2Mg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r514989162", "bodyText": "Does it make to have it defined as public? As we already have a public method getExtensionName already defined? maybe it's useful to use it in a static context? just wondering.", "author": "afalhambra", "createdAt": "2020-10-30T10:06:15Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,307 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalType;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";", "originalCommit": "702567a869b50937e72c6dd7c121751cda401833", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE3MDE0MA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515170140", "bodyText": "Here Im just following what has been done in other extensions. It makes sense in static context, as for example, in the code it is used the EXTENSION_NAME of BPMNServerExtension", "author": "fjtirado", "createdAt": "2020-10-30T15:12:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk4OTE2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAwMTY4Mg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515001682", "bodyText": "Should we instead try to force user to specify a port number through properties? I mean, by removing this default port number?", "author": "afalhambra", "createdAt": "2020-10-30T10:29:56Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,307 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalType;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    private Consumer<String, byte[]> consumer;\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    private ScheduledExecutorService pollService;\n+    private ExecutorService notifyService;\n+\n+    private Map<String, SignalInfo> topic2Signal = new ConcurrentHashMap<>();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.getBoolean(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED);\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.getAndSet(true)) {\n+            return;\n+        }\n+        pollService = Executors.newSingleThreadScheduledExecutor();\n+        notifyService = Executors.newCachedThreadPool();\n+        consumer = getKafkaConsumer();\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            throw new IllegalStateException(\"Cannot find extension \" + JbpmKieServerExtension.EXTENSION_NAME);\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        pollService.shutdownNow();\n+        pollService = null;\n+        notifyService.shutdownNow();\n+        notifyService = null;\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+            deploymentService = null;\n+        }\n+        if (consumer != null) {\n+            consumer.close();\n+            consumer = null;\n+        }\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));", "originalCommit": "702567a869b50937e72c6dd7c121751cda401833", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE3MDUwOQ==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515170509", "bodyText": "for Kafka, host and port are specified together.", "author": "fjtirado", "createdAt": "2020-10-30T15:12:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAwMTY4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAwMTg1Nw==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515001857", "bodyText": "typo\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    // read only commited events\n          \n          \n            \n                    // read only committed events", "author": "afalhambra", "createdAt": "2020-10-30T10:30:17Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,307 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalType;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    private Consumer<String, byte[]> consumer;\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    private ScheduledExecutorService pollService;\n+    private ExecutorService notifyService;\n+\n+    private Map<String, SignalInfo> topic2Signal = new ConcurrentHashMap<>();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.getBoolean(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED);\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.getAndSet(true)) {\n+            return;\n+        }\n+        pollService = Executors.newSingleThreadScheduledExecutor();\n+        notifyService = Executors.newCachedThreadPool();\n+        consumer = getKafkaConsumer();\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            throw new IllegalStateException(\"Cannot find extension \" + JbpmKieServerExtension.EXTENSION_NAME);\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        pollService.shutdownNow();\n+        pollService = null;\n+        notifyService.shutdownNow();\n+        notifyService = null;\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+            deploymentService = null;\n+        }\n+        if (consumer != null) {\n+            consumer.close();\n+            consumer = null;\n+        }\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));\n+        // read only commited events", "originalCommit": "702567a869b50937e72c6dd7c121751cda401833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAyNDY4Ng==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515024686", "bodyText": "Red Hat copyright is missing.", "author": "afalhambra", "createdAt": "2020-10-30T11:15:48Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/test/java/org/kie/server/services/jbpm/kafka/KakfaServerExtensionTest.java", "diffHunk": "@@ -0,0 +1,149 @@\n+package org.kie.server.services.jbpm.kafka;", "originalCommit": "702567a869b50937e72c6dd7c121751cda401833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAyNTcyNg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515025726", "bodyText": "Can be replaced with empty diamonds\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    private MockConsumer<String, byte[]> consumer = new MockConsumer<String, byte[]>(OffsetResetStrategy.EARLIEST);\n          \n          \n            \n                    private MockConsumer<String, byte[]> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);", "author": "afalhambra", "createdAt": "2020-10-30T11:17:43Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/test/java/org/kie/server/services/jbpm/kafka/KakfaServerExtensionTest.java", "diffHunk": "@@ -0,0 +1,149 @@\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.MockConsumer;\n+import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n+import org.apache.kafka.common.TopicPartition;\n+import org.jbpm.bpmn2.core.Message;\n+import org.jbpm.bpmn2.core.Signal;\n+import org.jbpm.kie.services.impl.model.SignalDescImpl;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedUnit;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.mockito.Mockito;\n+import org.mockito.verification.VerificationMode;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.timeout;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.withSettings;\n+\n+public class KakfaServerExtensionTest {\n+\n+    private static class MockKafkaServerExtension extends KafkaServerExtension {\n+\n+        private MockConsumer<String, byte[]> consumer = new MockConsumer<String, byte[]>(OffsetResetStrategy.EARLIEST);", "originalCommit": "702567a869b50937e72c6dd7c121751cda401833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAyNzMxMA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515027310", "bodyText": "Typo\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class KakfaServerExtensionTest {\n          \n          \n            \n            public class KafkaServerExtensionTest {", "author": "afalhambra", "createdAt": "2020-10-30T11:20:22Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/test/java/org/kie/server/services/jbpm/kafka/KakfaServerExtensionTest.java", "diffHunk": "@@ -0,0 +1,149 @@\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.MockConsumer;\n+import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n+import org.apache.kafka.common.TopicPartition;\n+import org.jbpm.bpmn2.core.Message;\n+import org.jbpm.bpmn2.core.Signal;\n+import org.jbpm.kie.services.impl.model.SignalDescImpl;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedUnit;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.mockito.Mockito;\n+import org.mockito.verification.VerificationMode;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.timeout;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.withSettings;\n+\n+public class KakfaServerExtensionTest {", "originalCommit": "702567a869b50937e72c6dd7c121751cda401833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAyOTI5Ng==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515029296", "bodyText": "Checked exception is not thrown within this method\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void testKafkaServerExecutorMessage() throws InterruptedException {\n          \n          \n            \n                public void testKafkaServerExecutorMessage() {", "author": "afalhambra", "createdAt": "2020-10-30T11:24:06Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/test/java/org/kie/server/services/jbpm/kafka/KakfaServerExtensionTest.java", "diffHunk": "@@ -0,0 +1,149 @@\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.MockConsumer;\n+import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n+import org.apache.kafka.common.TopicPartition;\n+import org.jbpm.bpmn2.core.Message;\n+import org.jbpm.bpmn2.core.Signal;\n+import org.jbpm.kie.services.impl.model.SignalDescImpl;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedUnit;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.mockito.Mockito;\n+import org.mockito.verification.VerificationMode;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.timeout;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.withSettings;\n+\n+public class KakfaServerExtensionTest {\n+\n+    private static class MockKafkaServerExtension extends KafkaServerExtension {\n+\n+        private MockConsumer<String, byte[]> consumer = new MockConsumer<String, byte[]>(OffsetResetStrategy.EARLIEST);\n+\n+        protected Consumer<String, byte[]> getKafkaConsumer() {\n+            return consumer;\n+        }\n+\n+        public MockConsumer<String, byte[]> getMockConsumer() {\n+            return consumer;\n+        }\n+    }\n+\n+    private ProcessService processService;\n+    private MockKafkaServerExtension extension;\n+    private KieServerImpl server;\n+    private KieServerRegistry registry;\n+    private DeployedUnit deployedUnit;\n+    private ProcessDefinition processDefinition;\n+\n+    @Before\n+    public void setup() {\n+        extension = new MockKafkaServerExtension();\n+        server = mock(KieServerImpl.class);\n+        registry = mock(KieServerRegistry.class);\n+        KieServerExtension serverExtension = mock(KieServerExtension.class);\n+\n+        when(registry.getServerExtension(Mockito.anyString())).thenReturn(serverExtension);\n+        ListenerSupport deployService = mock(ListenerSupport.class, withSettings().extraInterfaces(\n+                DeploymentService.class));\n+        processService = mock(ProcessService.class);\n+        when(serverExtension.getServices()).thenReturn(Arrays.asList(deployService, processService));\n+        extension.init(server, registry);\n+        extension.serverStarted();\n+        deployedUnit = mock(DeployedUnit.class);\n+        processDefinition = mock(ProcessDefinition.class);\n+        when(deployedUnit.getDeployedAssets()).thenReturn(Collections.singletonList(processDefinition));\n+    }\n+\n+    @After\n+    public void close() {\n+        extension.destroy(server, registry);\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorSignal() throws InterruptedException {\n+        when(processDefinition.getSignalsMetadata()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"MySignal\", \"String\"))));\n+        when(deployedUnit.getDeployedAssets()).thenReturn(Collections.singletonList(processDefinition));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy\", deployedUnit));\n+        publishEvent(\"MySignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy\", \"MySignal\", \"javierito\");\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorMessage() throws InterruptedException {", "originalCommit": "702567a869b50937e72c6dd7c121751cda401833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAyOTUwNw==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515029507", "bodyText": "Checked exception is not thrown within this method\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void testKafkaServerExecutorMessageTopic() throws InterruptedException {\n          \n          \n            \n                public void testKafkaServerExecutorMessageTopic() {", "author": "afalhambra", "createdAt": "2020-10-30T11:24:27Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/test/java/org/kie/server/services/jbpm/kafka/KakfaServerExtensionTest.java", "diffHunk": "@@ -0,0 +1,149 @@\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.MockConsumer;\n+import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n+import org.apache.kafka.common.TopicPartition;\n+import org.jbpm.bpmn2.core.Message;\n+import org.jbpm.bpmn2.core.Signal;\n+import org.jbpm.kie.services.impl.model.SignalDescImpl;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedUnit;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.mockito.Mockito;\n+import org.mockito.verification.VerificationMode;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.timeout;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.withSettings;\n+\n+public class KakfaServerExtensionTest {\n+\n+    private static class MockKafkaServerExtension extends KafkaServerExtension {\n+\n+        private MockConsumer<String, byte[]> consumer = new MockConsumer<String, byte[]>(OffsetResetStrategy.EARLIEST);\n+\n+        protected Consumer<String, byte[]> getKafkaConsumer() {\n+            return consumer;\n+        }\n+\n+        public MockConsumer<String, byte[]> getMockConsumer() {\n+            return consumer;\n+        }\n+    }\n+\n+    private ProcessService processService;\n+    private MockKafkaServerExtension extension;\n+    private KieServerImpl server;\n+    private KieServerRegistry registry;\n+    private DeployedUnit deployedUnit;\n+    private ProcessDefinition processDefinition;\n+\n+    @Before\n+    public void setup() {\n+        extension = new MockKafkaServerExtension();\n+        server = mock(KieServerImpl.class);\n+        registry = mock(KieServerRegistry.class);\n+        KieServerExtension serverExtension = mock(KieServerExtension.class);\n+\n+        when(registry.getServerExtension(Mockito.anyString())).thenReturn(serverExtension);\n+        ListenerSupport deployService = mock(ListenerSupport.class, withSettings().extraInterfaces(\n+                DeploymentService.class));\n+        processService = mock(ProcessService.class);\n+        when(serverExtension.getServices()).thenReturn(Arrays.asList(deployService, processService));\n+        extension.init(server, registry);\n+        extension.serverStarted();\n+        deployedUnit = mock(DeployedUnit.class);\n+        processDefinition = mock(ProcessDefinition.class);\n+        when(deployedUnit.getDeployedAssets()).thenReturn(Collections.singletonList(processDefinition));\n+    }\n+\n+    @After\n+    public void close() {\n+        extension.destroy(server, registry);\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorSignal() throws InterruptedException {\n+        when(processDefinition.getSignalsMetadata()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"MySignal\", \"String\"))));\n+        when(deployedUnit.getDeployedAssets()).thenReturn(Collections.singletonList(processDefinition));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy\", deployedUnit));\n+        publishEvent(\"MySignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy\", \"MySignal\", \"javierito\");\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorMessage() throws InterruptedException {\n+\n+        Message msg = new Message(\"MyMessage\");\n+        msg.setName(\"Hello\");\n+        msg.setType(\"String\");\n+        when(processDefinition.getSignalsMetadata()).thenReturn(Collections.singletonList(SignalDescImpl.from(msg)));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy\", deployedUnit));\n+        publishEvent(\"Hello\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"pepe\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy\", \"Message-Hello\", \"pepe\");\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorMessageTopic() throws InterruptedException {", "originalCommit": "702567a869b50937e72c6dd7c121751cda401833", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE3MTY4MQ==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515171681", "bodyText": "good catch", "author": "fjtirado", "createdAt": "2020-10-30T15:14:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAyOTUwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAyOTY3MA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515029670", "bodyText": "Checked exception is not thrown within this method\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void testKafkaServerExecutorSignal() throws InterruptedException {\n          \n          \n            \n                public void testKafkaServerExecutorSignal() {", "author": "afalhambra", "createdAt": "2020-10-30T11:24:44Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/test/java/org/kie/server/services/jbpm/kafka/KakfaServerExtensionTest.java", "diffHunk": "@@ -0,0 +1,149 @@\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.MockConsumer;\n+import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n+import org.apache.kafka.common.TopicPartition;\n+import org.jbpm.bpmn2.core.Message;\n+import org.jbpm.bpmn2.core.Signal;\n+import org.jbpm.kie.services.impl.model.SignalDescImpl;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedUnit;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.mockito.Mockito;\n+import org.mockito.verification.VerificationMode;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.timeout;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.withSettings;\n+\n+public class KakfaServerExtensionTest {\n+\n+    private static class MockKafkaServerExtension extends KafkaServerExtension {\n+\n+        private MockConsumer<String, byte[]> consumer = new MockConsumer<String, byte[]>(OffsetResetStrategy.EARLIEST);\n+\n+        protected Consumer<String, byte[]> getKafkaConsumer() {\n+            return consumer;\n+        }\n+\n+        public MockConsumer<String, byte[]> getMockConsumer() {\n+            return consumer;\n+        }\n+    }\n+\n+    private ProcessService processService;\n+    private MockKafkaServerExtension extension;\n+    private KieServerImpl server;\n+    private KieServerRegistry registry;\n+    private DeployedUnit deployedUnit;\n+    private ProcessDefinition processDefinition;\n+\n+    @Before\n+    public void setup() {\n+        extension = new MockKafkaServerExtension();\n+        server = mock(KieServerImpl.class);\n+        registry = mock(KieServerRegistry.class);\n+        KieServerExtension serverExtension = mock(KieServerExtension.class);\n+\n+        when(registry.getServerExtension(Mockito.anyString())).thenReturn(serverExtension);\n+        ListenerSupport deployService = mock(ListenerSupport.class, withSettings().extraInterfaces(\n+                DeploymentService.class));\n+        processService = mock(ProcessService.class);\n+        when(serverExtension.getServices()).thenReturn(Arrays.asList(deployService, processService));\n+        extension.init(server, registry);\n+        extension.serverStarted();\n+        deployedUnit = mock(DeployedUnit.class);\n+        processDefinition = mock(ProcessDefinition.class);\n+        when(deployedUnit.getDeployedAssets()).thenReturn(Collections.singletonList(processDefinition));\n+    }\n+\n+    @After\n+    public void close() {\n+        extension.destroy(server, registry);\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorSignal() throws InterruptedException {", "originalCommit": "702567a869b50937e72c6dd7c121751cda401833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAzNjk2OQ==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515036969", "bodyText": "Can be replaced with empty diamonds\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    Map<TopicPartition, Long> partitionsBeginningMap = new HashMap<TopicPartition, Long>();\n          \n          \n            \n                    Map<TopicPartition, Long> partitionsEndMap = new HashMap<TopicPartition, Long>();\n          \n          \n            \n                    Map<TopicPartition, Long> partitionsBeginningMap = new HashMap<>();\n          \n          \n            \n                    Map<TopicPartition, Long> partitionsEndMap = new HashMap<>();", "author": "afalhambra", "createdAt": "2020-10-30T11:38:38Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/test/java/org/kie/server/services/jbpm/kafka/KakfaServerExtensionTest.java", "diffHunk": "@@ -0,0 +1,149 @@\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.MockConsumer;\n+import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n+import org.apache.kafka.common.TopicPartition;\n+import org.jbpm.bpmn2.core.Message;\n+import org.jbpm.bpmn2.core.Signal;\n+import org.jbpm.kie.services.impl.model.SignalDescImpl;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedUnit;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.mockito.Mockito;\n+import org.mockito.verification.VerificationMode;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.timeout;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.withSettings;\n+\n+public class KakfaServerExtensionTest {\n+\n+    private static class MockKafkaServerExtension extends KafkaServerExtension {\n+\n+        private MockConsumer<String, byte[]> consumer = new MockConsumer<String, byte[]>(OffsetResetStrategy.EARLIEST);\n+\n+        protected Consumer<String, byte[]> getKafkaConsumer() {\n+            return consumer;\n+        }\n+\n+        public MockConsumer<String, byte[]> getMockConsumer() {\n+            return consumer;\n+        }\n+    }\n+\n+    private ProcessService processService;\n+    private MockKafkaServerExtension extension;\n+    private KieServerImpl server;\n+    private KieServerRegistry registry;\n+    private DeployedUnit deployedUnit;\n+    private ProcessDefinition processDefinition;\n+\n+    @Before\n+    public void setup() {\n+        extension = new MockKafkaServerExtension();\n+        server = mock(KieServerImpl.class);\n+        registry = mock(KieServerRegistry.class);\n+        KieServerExtension serverExtension = mock(KieServerExtension.class);\n+\n+        when(registry.getServerExtension(Mockito.anyString())).thenReturn(serverExtension);\n+        ListenerSupport deployService = mock(ListenerSupport.class, withSettings().extraInterfaces(\n+                DeploymentService.class));\n+        processService = mock(ProcessService.class);\n+        when(serverExtension.getServices()).thenReturn(Arrays.asList(deployService, processService));\n+        extension.init(server, registry);\n+        extension.serverStarted();\n+        deployedUnit = mock(DeployedUnit.class);\n+        processDefinition = mock(ProcessDefinition.class);\n+        when(deployedUnit.getDeployedAssets()).thenReturn(Collections.singletonList(processDefinition));\n+    }\n+\n+    @After\n+    public void close() {\n+        extension.destroy(server, registry);\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorSignal() throws InterruptedException {\n+        when(processDefinition.getSignalsMetadata()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"MySignal\", \"String\"))));\n+        when(deployedUnit.getDeployedAssets()).thenReturn(Collections.singletonList(processDefinition));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy\", deployedUnit));\n+        publishEvent(\"MySignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy\", \"MySignal\", \"javierito\");\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorMessage() throws InterruptedException {\n+\n+        Message msg = new Message(\"MyMessage\");\n+        msg.setName(\"Hello\");\n+        msg.setType(\"String\");\n+        when(processDefinition.getSignalsMetadata()).thenReturn(Collections.singletonList(SignalDescImpl.from(msg)));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy\", deployedUnit));\n+        publishEvent(\"Hello\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"pepe\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy\", \"Message-Hello\", \"pepe\");\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorMessageTopic() throws InterruptedException {\n+\n+        final String topicProperty = KafkaServerExtension.TOPIC_PREFIX + \"Hello\";\n+        System.setProperty(topicProperty, \"MyTopic\");\n+        try {\n+            Message msg = new Message(\"MyMessage\");\n+            msg.setName(\"Hello\");\n+            msg.setType(\"String\");\n+            when(processDefinition.getSignalsMetadata()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                    msg)));\n+            extension.onDeploy(new DeploymentEvent(\"MyDeploy\", deployedUnit));\n+            publishEvent(\"MyTopic\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"pepe\\\"}\");\n+            verify(processService, getTimeout()).signalEvent(\"MyDeploy\", \"Message-Hello\", \"pepe\");\n+        } finally {\n+            System.clearProperty(topicProperty);\n+        }\n+    }\n+\n+    private VerificationMode getTimeout() {\n+        return timeout(Long.getLong(KafkaServerExtension.KAFKA_EXTENSION_PREFIX + \"poll.interval\", 1L) * 2000).times(1);\n+    }\n+\n+    private void publishEvent(String topic, String cloudEventText) {\n+        Set<String> topics = extension.getMockConsumer().subscription();\n+        assertTrue(\"Topic \" + topic + \" not found\", topics.contains(topic));\n+        List<TopicPartition> partitions = Collections.singletonList(new TopicPartition(topic, 0));\n+        extension.getMockConsumer().rebalance(partitions);\n+        Map<TopicPartition, Long> partitionsBeginningMap = new HashMap<TopicPartition, Long>();\n+        Map<TopicPartition, Long> partitionsEndMap = new HashMap<TopicPartition, Long>();", "originalCommit": "702567a869b50937e72c6dd7c121751cda401833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAzNzIyNA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515037224", "bodyText": "same as above\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    extension.getMockConsumer().addRecord(new ConsumerRecord<String, byte[]>(topic, 0, 0L, \"\",\n          \n          \n            \n                    extension.getMockConsumer().addRecord(new ConsumerRecord<>(topic, 0, 0L, \"\",", "author": "afalhambra", "createdAt": "2020-10-30T11:39:07Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/test/java/org/kie/server/services/jbpm/kafka/KakfaServerExtensionTest.java", "diffHunk": "@@ -0,0 +1,149 @@\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.MockConsumer;\n+import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n+import org.apache.kafka.common.TopicPartition;\n+import org.jbpm.bpmn2.core.Message;\n+import org.jbpm.bpmn2.core.Signal;\n+import org.jbpm.kie.services.impl.model.SignalDescImpl;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedUnit;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.mockito.Mockito;\n+import org.mockito.verification.VerificationMode;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.timeout;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.withSettings;\n+\n+public class KakfaServerExtensionTest {\n+\n+    private static class MockKafkaServerExtension extends KafkaServerExtension {\n+\n+        private MockConsumer<String, byte[]> consumer = new MockConsumer<String, byte[]>(OffsetResetStrategy.EARLIEST);\n+\n+        protected Consumer<String, byte[]> getKafkaConsumer() {\n+            return consumer;\n+        }\n+\n+        public MockConsumer<String, byte[]> getMockConsumer() {\n+            return consumer;\n+        }\n+    }\n+\n+    private ProcessService processService;\n+    private MockKafkaServerExtension extension;\n+    private KieServerImpl server;\n+    private KieServerRegistry registry;\n+    private DeployedUnit deployedUnit;\n+    private ProcessDefinition processDefinition;\n+\n+    @Before\n+    public void setup() {\n+        extension = new MockKafkaServerExtension();\n+        server = mock(KieServerImpl.class);\n+        registry = mock(KieServerRegistry.class);\n+        KieServerExtension serverExtension = mock(KieServerExtension.class);\n+\n+        when(registry.getServerExtension(Mockito.anyString())).thenReturn(serverExtension);\n+        ListenerSupport deployService = mock(ListenerSupport.class, withSettings().extraInterfaces(\n+                DeploymentService.class));\n+        processService = mock(ProcessService.class);\n+        when(serverExtension.getServices()).thenReturn(Arrays.asList(deployService, processService));\n+        extension.init(server, registry);\n+        extension.serverStarted();\n+        deployedUnit = mock(DeployedUnit.class);\n+        processDefinition = mock(ProcessDefinition.class);\n+        when(deployedUnit.getDeployedAssets()).thenReturn(Collections.singletonList(processDefinition));\n+    }\n+\n+    @After\n+    public void close() {\n+        extension.destroy(server, registry);\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorSignal() throws InterruptedException {\n+        when(processDefinition.getSignalsMetadata()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"MySignal\", \"String\"))));\n+        when(deployedUnit.getDeployedAssets()).thenReturn(Collections.singletonList(processDefinition));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy\", deployedUnit));\n+        publishEvent(\"MySignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy\", \"MySignal\", \"javierito\");\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorMessage() throws InterruptedException {\n+\n+        Message msg = new Message(\"MyMessage\");\n+        msg.setName(\"Hello\");\n+        msg.setType(\"String\");\n+        when(processDefinition.getSignalsMetadata()).thenReturn(Collections.singletonList(SignalDescImpl.from(msg)));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy\", deployedUnit));\n+        publishEvent(\"Hello\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"pepe\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy\", \"Message-Hello\", \"pepe\");\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorMessageTopic() throws InterruptedException {\n+\n+        final String topicProperty = KafkaServerExtension.TOPIC_PREFIX + \"Hello\";\n+        System.setProperty(topicProperty, \"MyTopic\");\n+        try {\n+            Message msg = new Message(\"MyMessage\");\n+            msg.setName(\"Hello\");\n+            msg.setType(\"String\");\n+            when(processDefinition.getSignalsMetadata()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                    msg)));\n+            extension.onDeploy(new DeploymentEvent(\"MyDeploy\", deployedUnit));\n+            publishEvent(\"MyTopic\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"pepe\\\"}\");\n+            verify(processService, getTimeout()).signalEvent(\"MyDeploy\", \"Message-Hello\", \"pepe\");\n+        } finally {\n+            System.clearProperty(topicProperty);\n+        }\n+    }\n+\n+    private VerificationMode getTimeout() {\n+        return timeout(Long.getLong(KafkaServerExtension.KAFKA_EXTENSION_PREFIX + \"poll.interval\", 1L) * 2000).times(1);\n+    }\n+\n+    private void publishEvent(String topic, String cloudEventText) {\n+        Set<String> topics = extension.getMockConsumer().subscription();\n+        assertTrue(\"Topic \" + topic + \" not found\", topics.contains(topic));\n+        List<TopicPartition> partitions = Collections.singletonList(new TopicPartition(topic, 0));\n+        extension.getMockConsumer().rebalance(partitions);\n+        Map<TopicPartition, Long> partitionsBeginningMap = new HashMap<TopicPartition, Long>();\n+        Map<TopicPartition, Long> partitionsEndMap = new HashMap<TopicPartition, Long>();\n+        long records = 10L;\n+        for (TopicPartition partition : partitions) {\n+            partitionsBeginningMap.put(partition, 0l);\n+            partitionsEndMap.put(partition, records);\n+        }\n+        extension.getMockConsumer().updateBeginningOffsets(partitionsBeginningMap);\n+        extension.getMockConsumer().updateEndOffsets(partitionsEndMap);\n+        extension.getMockConsumer().addRecord(new ConsumerRecord<String, byte[]>(topic, 0, 0L, \"\",", "originalCommit": "702567a869b50937e72c6dd7c121751cda401833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTAzNzQwNg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515037406", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        partitionsBeginningMap.put(partition, 0l);\n          \n          \n            \n                        partitionsBeginningMap.put(partition, 0L);", "author": "afalhambra", "createdAt": "2020-10-30T11:39:27Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/test/java/org/kie/server/services/jbpm/kafka/KakfaServerExtensionTest.java", "diffHunk": "@@ -0,0 +1,149 @@\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.MockConsumer;\n+import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n+import org.apache.kafka.common.TopicPartition;\n+import org.jbpm.bpmn2.core.Message;\n+import org.jbpm.bpmn2.core.Signal;\n+import org.jbpm.kie.services.impl.model.SignalDescImpl;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedUnit;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.mockito.Mockito;\n+import org.mockito.verification.VerificationMode;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.timeout;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.withSettings;\n+\n+public class KakfaServerExtensionTest {\n+\n+    private static class MockKafkaServerExtension extends KafkaServerExtension {\n+\n+        private MockConsumer<String, byte[]> consumer = new MockConsumer<String, byte[]>(OffsetResetStrategy.EARLIEST);\n+\n+        protected Consumer<String, byte[]> getKafkaConsumer() {\n+            return consumer;\n+        }\n+\n+        public MockConsumer<String, byte[]> getMockConsumer() {\n+            return consumer;\n+        }\n+    }\n+\n+    private ProcessService processService;\n+    private MockKafkaServerExtension extension;\n+    private KieServerImpl server;\n+    private KieServerRegistry registry;\n+    private DeployedUnit deployedUnit;\n+    private ProcessDefinition processDefinition;\n+\n+    @Before\n+    public void setup() {\n+        extension = new MockKafkaServerExtension();\n+        server = mock(KieServerImpl.class);\n+        registry = mock(KieServerRegistry.class);\n+        KieServerExtension serverExtension = mock(KieServerExtension.class);\n+\n+        when(registry.getServerExtension(Mockito.anyString())).thenReturn(serverExtension);\n+        ListenerSupport deployService = mock(ListenerSupport.class, withSettings().extraInterfaces(\n+                DeploymentService.class));\n+        processService = mock(ProcessService.class);\n+        when(serverExtension.getServices()).thenReturn(Arrays.asList(deployService, processService));\n+        extension.init(server, registry);\n+        extension.serverStarted();\n+        deployedUnit = mock(DeployedUnit.class);\n+        processDefinition = mock(ProcessDefinition.class);\n+        when(deployedUnit.getDeployedAssets()).thenReturn(Collections.singletonList(processDefinition));\n+    }\n+\n+    @After\n+    public void close() {\n+        extension.destroy(server, registry);\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorSignal() throws InterruptedException {\n+        when(processDefinition.getSignalsMetadata()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"MySignal\", \"String\"))));\n+        when(deployedUnit.getDeployedAssets()).thenReturn(Collections.singletonList(processDefinition));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy\", deployedUnit));\n+        publishEvent(\"MySignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy\", \"MySignal\", \"javierito\");\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorMessage() throws InterruptedException {\n+\n+        Message msg = new Message(\"MyMessage\");\n+        msg.setName(\"Hello\");\n+        msg.setType(\"String\");\n+        when(processDefinition.getSignalsMetadata()).thenReturn(Collections.singletonList(SignalDescImpl.from(msg)));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy\", deployedUnit));\n+        publishEvent(\"Hello\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"pepe\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy\", \"Message-Hello\", \"pepe\");\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorMessageTopic() throws InterruptedException {\n+\n+        final String topicProperty = KafkaServerExtension.TOPIC_PREFIX + \"Hello\";\n+        System.setProperty(topicProperty, \"MyTopic\");\n+        try {\n+            Message msg = new Message(\"MyMessage\");\n+            msg.setName(\"Hello\");\n+            msg.setType(\"String\");\n+            when(processDefinition.getSignalsMetadata()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                    msg)));\n+            extension.onDeploy(new DeploymentEvent(\"MyDeploy\", deployedUnit));\n+            publishEvent(\"MyTopic\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"pepe\\\"}\");\n+            verify(processService, getTimeout()).signalEvent(\"MyDeploy\", \"Message-Hello\", \"pepe\");\n+        } finally {\n+            System.clearProperty(topicProperty);\n+        }\n+    }\n+\n+    private VerificationMode getTimeout() {\n+        return timeout(Long.getLong(KafkaServerExtension.KAFKA_EXTENSION_PREFIX + \"poll.interval\", 1L) * 2000).times(1);\n+    }\n+\n+    private void publishEvent(String topic, String cloudEventText) {\n+        Set<String> topics = extension.getMockConsumer().subscription();\n+        assertTrue(\"Topic \" + topic + \" not found\", topics.contains(topic));\n+        List<TopicPartition> partitions = Collections.singletonList(new TopicPartition(topic, 0));\n+        extension.getMockConsumer().rebalance(partitions);\n+        Map<TopicPartition, Long> partitionsBeginningMap = new HashMap<TopicPartition, Long>();\n+        Map<TopicPartition, Long> partitionsEndMap = new HashMap<TopicPartition, Long>();\n+        long records = 10L;\n+        for (TopicPartition partition : partitions) {\n+            partitionsBeginningMap.put(partition, 0l);", "originalCommit": "702567a869b50937e72c6dd7c121751cda401833", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5162c4b335dc9d4e66a42d45755478723f9ecd40", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/5162c4b335dc9d4e66a42d45755478723f9ecd40", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-30T15:15:43Z", "type": "forcePushed"}, {"oid": "02a8f2440ed9ecc0abc0702a1a04c97f91f348f9", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/02a8f2440ed9ecc0abc0702a1a04c97f91f348f9", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-10-30T15:17:23Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkxNzk0Mg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515917942", "bodyText": "extract DeploymentEventListener (this is the topic manager) and Runnable (this is the event executor) from here.", "author": "elguardian", "createdAt": "2020-11-02T11:45:53Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,307 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalType;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {", "originalCommit": "02a8f2440ed9ecc0abc0702a1a04c97f91f348f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjY1MTE4Mg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r516651182", "bodyText": "I prefer to save object creation but making the extension implementing them", "author": "fjtirado", "createdAt": "2020-11-03T13:05:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkxNzk0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyMTEwOA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515921108", "bodyText": "test signal first and then messages. there is no such thing as signal type message (in spite it is implemented that way)\nif (there is no signal  defined should log a warn message like there is no way I sent this to jbpm or signal not bounded to any process in kjar bla bla bal\nif there is a parse exception should ignore the error (dont't log at that level) as you are doing content based routing (log as warn message) -> ignoring event by jbpm (you have the structure ref (if that is defined then use it) -> getClassData", "author": "elguardian", "createdAt": "2020-11-02T11:52:19Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,307 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalType;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    private Consumer<String, byte[]> consumer;\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    private ScheduledExecutorService pollService;\n+    private ExecutorService notifyService;\n+\n+    private Map<String, SignalInfo> topic2Signal = new ConcurrentHashMap<>();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.getBoolean(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED);\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.getAndSet(true)) {\n+            return;\n+        }\n+        pollService = Executors.newSingleThreadScheduledExecutor();\n+        notifyService = Executors.newCachedThreadPool();\n+        consumer = getKafkaConsumer();\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            throw new IllegalStateException(\"Cannot find extension \" + JbpmKieServerExtension.EXTENSION_NAME);\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        pollService.shutdownNow();\n+        pollService = null;\n+        notifyService.shutdownNow();\n+        notifyService = null;\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+            deploymentService = null;\n+        }\n+        if (consumer != null) {\n+            consumer.close();\n+            consumer = null;\n+        }\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));\n+        // read only committed events\n+        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase());\n+        // do not automatically create topics by default\n+        props.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, Boolean.getBoolean(\n+                KAFKA_EXTENSION_PREFIX + \"auto.create.topics\"));\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, System.getProperty(KAFKA_EXTENSION_PREFIX + \"group.id\",\n+                \"jbpm-consumer\"));\n+        return new KafkaConsumer<>(props, new StringDeserializer(), new ByteArrayDeserializer());\n+    }\n+\n+    @Override\n+    public void onDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onUnDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    @Override\n+    public void onActivate(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onDeactivate(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    private void updateTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        for (SignalDesc signal : processDefinition.getSignalsMetadata()) {\n+            topic2Signal.computeIfAbsent(topicFromSignal(signal), t -> new SignalInfo(deploymentId,\n+                    signal));\n+        }\n+    }\n+\n+    private void removeTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        for (SignalDesc signal : processDefinition.getSignalsMetadata()) {\n+            topic2Signal.remove(topicFromSignal(signal));\n+        }\n+    }\n+\n+    private void updateRegistration(DeploymentEvent event, BiConsumer<String, ProcessDefinition> updater) {\n+        if (consumer != null) {\n+            for (DeployedAsset asset : event.getDeployedUnit().getDeployedAssets()) {\n+                updater.accept(event.getDeploymentId(), (ProcessDefinition) asset);\n+            }\n+            synchronized (consumer) {\n+                consumer.subscribe(topic2Signal.keySet());\n+            }\n+        }\n+    }\n+\n+    private String topicFromSignal(SignalDesc signal) {\n+        return System.getProperty(TOPIC_PREFIX + signal.getName(), signal.getName());\n+    }\n+\n+    @Override\n+    public void serverStarted() {\n+        if (pollService != null) {\n+            long delay = Long.getLong(KAFKA_EXTENSION_PREFIX + \"poll.interval\", 1L);\n+            pollService.scheduleWithFixedDelay(this, delay, delay, TimeUnit.SECONDS);\n+        }\n+    }\n+\n+    @Override\n+    public void createContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public boolean isUpdateContainerAllowed(String id,\n+                                            KieContainerInstance kieContainerInstance,\n+                                            Map<String, Object> parameters) {\n+        return false;\n+    }\n+\n+    @Override\n+    public void prepareContainerUpdate(String id,\n+                                       KieContainerInstance kieContainerInstance,\n+                                       Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void updateContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void disposeContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public List<Object> getAppComponents(SupportedTransports type) {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public <T> T getAppComponents(Class<T> serviceType) {\n+        return null;\n+    }\n+\n+    @Override\n+    public String getImplementedCapability() {\n+        return KieServerConstants.CAPABILITY_BPM_KAFKA;\n+    }\n+\n+    @Override\n+    public List<Object> getServices() {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public String getExtensionName() {\n+        return EXTENSION_NAME;\n+    }\n+\n+    @Override\n+    public Integer getStartOrder() {\n+        return 20;\n+    }\n+\n+    @Override\n+    public void run() {\n+        if (consumer != null && !topic2Signal.isEmpty()) {\n+            ConsumerRecords<String, byte[]> events;\n+            synchronized (consumer) {\n+                events = consumer.subscription().isEmpty() ? ConsumerRecords.empty() : consumer.poll(Duration.ZERO);\n+            }\n+            for (ConsumerRecord<String, byte[]> event : events) {\n+                notifyService.submit(() -> processEvent(event));\n+            }\n+        }\n+    }\n+\n+    private void processEvent(ConsumerRecord<String, byte[]> event) {\n+        SignalInfo signalInfo = topic2Signal.get(event.topic());", "originalCommit": "02a8f2440ed9ecc0abc0702a1a04c97f91f348f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjY1MTg5NA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r516651894", "bodyText": "Implementation changed to deal with two different methods: getSignalsDesc and getMessagesDesc. This certainly complicates implementation here, but allow message and signal diverge on future (right now, they have the same data, thats why the implementation was done assuming identity between them)", "author": "fjtirado", "createdAt": "2020-11-03T13:06:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyMTEwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyMjA5Mw==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515922093", "bodyText": "the only thing missing here is the class loader. Maybe you can reuse the json marshaller (already has all this logic)\nAt least I cannot see how you can reach kjar class loader from here.", "author": "elguardian", "createdAt": "2020-11-02T11:54:22Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/CloudEvent.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+\n+class CloudEvent<T> {\n+\n+    private static ObjectMapper mapper = new ObjectMapper()\n+            .setDateFormat(new SimpleDateFormat(System.getProperty(\n+                    KafkaServerExtension.KAFKA_EXTENSION_PREFIX + \"json.date_format\", System.getProperty(\n+                            \"org.kie.server.json.date_format\",\n+                            \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"))));\n+\n+    private String specVersion;\n+    private Date time;\n+    private String id;\n+    private String type;\n+    private String source;\n+    private T data;\n+\n+    public static <T> CloudEvent<T> read(byte[] bytes, Class<T> type) throws IOException, ParseException {", "originalCommit": "02a8f2440ed9ecc0abc0702a1a04c97f91f348f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgxOTcxMw==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r516819713", "bodyText": "Good catch!. Using list of classes returned by DeployedEvent", "author": "fjtirado", "createdAt": "2020-11-03T17:00:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyMjA5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyMjY4MQ==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515922681", "bodyText": "this is not going to find any class at project leve AFAIK.", "author": "elguardian", "createdAt": "2020-11-02T11:55:31Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,307 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalType;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    private Consumer<String, byte[]> consumer;\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    private ScheduledExecutorService pollService;\n+    private ExecutorService notifyService;\n+\n+    private Map<String, SignalInfo> topic2Signal = new ConcurrentHashMap<>();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.getBoolean(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED);\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.getAndSet(true)) {\n+            return;\n+        }\n+        pollService = Executors.newSingleThreadScheduledExecutor();\n+        notifyService = Executors.newCachedThreadPool();\n+        consumer = getKafkaConsumer();\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            throw new IllegalStateException(\"Cannot find extension \" + JbpmKieServerExtension.EXTENSION_NAME);\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        pollService.shutdownNow();\n+        pollService = null;\n+        notifyService.shutdownNow();\n+        notifyService = null;\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+            deploymentService = null;\n+        }\n+        if (consumer != null) {\n+            consumer.close();\n+            consumer = null;\n+        }\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));\n+        // read only committed events\n+        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase());\n+        // do not automatically create topics by default\n+        props.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, Boolean.getBoolean(\n+                KAFKA_EXTENSION_PREFIX + \"auto.create.topics\"));\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, System.getProperty(KAFKA_EXTENSION_PREFIX + \"group.id\",\n+                \"jbpm-consumer\"));\n+        return new KafkaConsumer<>(props, new StringDeserializer(), new ByteArrayDeserializer());\n+    }\n+\n+    @Override\n+    public void onDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onUnDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    @Override\n+    public void onActivate(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onDeactivate(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    private void updateTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        for (SignalDesc signal : processDefinition.getSignalsMetadata()) {\n+            topic2Signal.computeIfAbsent(topicFromSignal(signal), t -> new SignalInfo(deploymentId,\n+                    signal));\n+        }\n+    }\n+\n+    private void removeTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        for (SignalDesc signal : processDefinition.getSignalsMetadata()) {\n+            topic2Signal.remove(topicFromSignal(signal));\n+        }\n+    }\n+\n+    private void updateRegistration(DeploymentEvent event, BiConsumer<String, ProcessDefinition> updater) {\n+        if (consumer != null) {\n+            for (DeployedAsset asset : event.getDeployedUnit().getDeployedAssets()) {\n+                updater.accept(event.getDeploymentId(), (ProcessDefinition) asset);\n+            }\n+            synchronized (consumer) {\n+                consumer.subscribe(topic2Signal.keySet());\n+            }\n+        }\n+    }\n+\n+    private String topicFromSignal(SignalDesc signal) {\n+        return System.getProperty(TOPIC_PREFIX + signal.getName(), signal.getName());\n+    }\n+\n+    @Override\n+    public void serverStarted() {\n+        if (pollService != null) {\n+            long delay = Long.getLong(KAFKA_EXTENSION_PREFIX + \"poll.interval\", 1L);\n+            pollService.scheduleWithFixedDelay(this, delay, delay, TimeUnit.SECONDS);\n+        }\n+    }\n+\n+    @Override\n+    public void createContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public boolean isUpdateContainerAllowed(String id,\n+                                            KieContainerInstance kieContainerInstance,\n+                                            Map<String, Object> parameters) {\n+        return false;\n+    }\n+\n+    @Override\n+    public void prepareContainerUpdate(String id,\n+                                       KieContainerInstance kieContainerInstance,\n+                                       Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void updateContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void disposeContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public List<Object> getAppComponents(SupportedTransports type) {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public <T> T getAppComponents(Class<T> serviceType) {\n+        return null;\n+    }\n+\n+    @Override\n+    public String getImplementedCapability() {\n+        return KieServerConstants.CAPABILITY_BPM_KAFKA;\n+    }\n+\n+    @Override\n+    public List<Object> getServices() {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public String getExtensionName() {\n+        return EXTENSION_NAME;\n+    }\n+\n+    @Override\n+    public Integer getStartOrder() {\n+        return 20;\n+    }\n+\n+    @Override\n+    public void run() {\n+        if (consumer != null && !topic2Signal.isEmpty()) {\n+            ConsumerRecords<String, byte[]> events;\n+            synchronized (consumer) {\n+                events = consumer.subscription().isEmpty() ? ConsumerRecords.empty() : consumer.poll(Duration.ZERO);\n+            }\n+            for (ConsumerRecord<String, byte[]> event : events) {\n+                notifyService.submit(() -> processEvent(event));\n+            }\n+        }\n+    }\n+\n+    private void processEvent(ConsumerRecord<String, byte[]> event) {\n+        SignalInfo signalInfo = topic2Signal.get(event.topic());\n+        if (signalInfo != null) {\n+            try {\n+                String type = signalInfo.getSignalDesc().getName();\n+                if (signalInfo.getSignalDesc().getSignalType() == SignalType.MESSAGE) {\n+                    type = \"Message-\" + type;\n+                }\n+                processService.signalEvent(signalInfo.getDeploymentId(), type, CloudEvent.read(event.value(),\n+                        getDataClass(signalInfo.getSignalDesc())).getData());\n+            } catch (IOException | ParseException e) {\n+                logger.error(\"Error deserializing event\", e);\n+            }\n+        }\n+    }\n+\n+    private Class<?> getDataClass(SignalDesc signalDesc) {", "originalCommit": "02a8f2440ed9ecc0abc0702a1a04c97f91f348f9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyNjA0Nw==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515926047", "bodyText": "If you are setting by default this extension as true\nhttps://github.com/kiegroup/droolsjbpm-integration/pull/2288/files#diff-821631efbb543afdf97147bcc31174b3fa3f48fc3b97159a0536a484468d0c4cR82\n(disable false)\nthis is not necessaraly true as this can be active without any kafka server in the environment. Have you checked ?", "author": "elguardian", "createdAt": "2020-11-02T12:02:45Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,307 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalType;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    private Consumer<String, byte[]> consumer;\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    private ScheduledExecutorService pollService;\n+    private ExecutorService notifyService;\n+\n+    private Map<String, SignalInfo> topic2Signal = new ConcurrentHashMap<>();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.getBoolean(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED);\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.getAndSet(true)) {\n+            return;\n+        }\n+        pollService = Executors.newSingleThreadScheduledExecutor();\n+        notifyService = Executors.newCachedThreadPool();\n+        consumer = getKafkaConsumer();\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            throw new IllegalStateException(\"Cannot find extension \" + JbpmKieServerExtension.EXTENSION_NAME);\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        pollService.shutdownNow();\n+        pollService = null;\n+        notifyService.shutdownNow();\n+        notifyService = null;\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+            deploymentService = null;\n+        }\n+        if (consumer != null) {\n+            consumer.close();\n+            consumer = null;\n+        }\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));", "originalCommit": "02a8f2440ed9ecc0abc0702a1a04c97f91f348f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjY2NTk3Mg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r516665972", "bodyText": "do we really need to check that? The kafka broker might be down when we start the server. I think this refers as if we want this extension to be active or not, regardless the status of the kafka broker", "author": "fjtirado", "createdAt": "2020-11-03T13:30:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyNjA0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc3NTAxMw==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r516775013", "bodyText": "Problem was that kie server wont start when extension is enabled and kafka broker down. Consumer is now lazily initialized to preven that. Deployment with signals/messages will fail if attempted when kafka broker is down (so user might retry)", "author": "fjtirado", "createdAt": "2020-11-03T15:57:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyNjA0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyOTExNA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515929114", "bodyText": "this is far to generic and it is not documented in the BAPL.\nsignal names within process does not necessarily mean the same among process I would argue this is a good idea in the general case.\nWhile I can see the benefit (and indeed you can leave it like this). You would need to search first for more particular signal definition\nThe most particular case (this should be mandatory at least)\nTOPIC_PREFIX...signalName=signalName\nmore general (by deployment)\nTOPIC_PREFIX..signalName = signalName\nor your case (by entire server -> this is ok too)\nTOPIC_PREFIX.signalName = signalName\nAlso requires to say that some signal is bounded. For instance is there is a signal registed by a process there is need to set an info kjar - process id - signal has been bounded to topic (whatever topic is mapped) through system property whaever.\nThe spec how to define the mapping should be part of the documentation of your BAPL as well.\nThis would add a lot of info regarding the mapping and allow diagnostics.", "author": "elguardian", "createdAt": "2020-11-02T12:09:27Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,307 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalType;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    private Consumer<String, byte[]> consumer;\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    private ScheduledExecutorService pollService;\n+    private ExecutorService notifyService;\n+\n+    private Map<String, SignalInfo> topic2Signal = new ConcurrentHashMap<>();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.getBoolean(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED);\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.getAndSet(true)) {\n+            return;\n+        }\n+        pollService = Executors.newSingleThreadScheduledExecutor();\n+        notifyService = Executors.newCachedThreadPool();\n+        consumer = getKafkaConsumer();\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            throw new IllegalStateException(\"Cannot find extension \" + JbpmKieServerExtension.EXTENSION_NAME);\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        pollService.shutdownNow();\n+        pollService = null;\n+        notifyService.shutdownNow();\n+        notifyService = null;\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+            deploymentService = null;\n+        }\n+        if (consumer != null) {\n+            consumer.close();\n+            consumer = null;\n+        }\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));\n+        // read only committed events\n+        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase());\n+        // do not automatically create topics by default\n+        props.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, Boolean.getBoolean(\n+                KAFKA_EXTENSION_PREFIX + \"auto.create.topics\"));\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, System.getProperty(KAFKA_EXTENSION_PREFIX + \"group.id\",\n+                \"jbpm-consumer\"));\n+        return new KafkaConsumer<>(props, new StringDeserializer(), new ByteArrayDeserializer());\n+    }\n+\n+    @Override\n+    public void onDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onUnDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    @Override\n+    public void onActivate(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onDeactivate(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    private void updateTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        for (SignalDesc signal : processDefinition.getSignalsMetadata()) {\n+            topic2Signal.computeIfAbsent(topicFromSignal(signal), t -> new SignalInfo(deploymentId,\n+                    signal));\n+        }\n+    }\n+\n+    private void removeTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        for (SignalDesc signal : processDefinition.getSignalsMetadata()) {\n+            topic2Signal.remove(topicFromSignal(signal));\n+        }\n+    }\n+\n+    private void updateRegistration(DeploymentEvent event, BiConsumer<String, ProcessDefinition> updater) {\n+        if (consumer != null) {\n+            for (DeployedAsset asset : event.getDeployedUnit().getDeployedAssets()) {\n+                updater.accept(event.getDeploymentId(), (ProcessDefinition) asset);\n+            }\n+            synchronized (consumer) {\n+                consumer.subscribe(topic2Signal.keySet());\n+            }\n+        }\n+    }\n+\n+    private String topicFromSignal(SignalDesc signal) {\n+        return System.getProperty(TOPIC_PREFIX + signal.getName(), signal.getName());", "originalCommit": "02a8f2440ed9ecc0abc0702a1a04c97f91f348f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg3NTY2Ng==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r516875666", "bodyText": "Leaving at it is for now. Opened https://issues.redhat.com/browse/JBPM-9455 to allow fine-grainer approach.", "author": "fjtirado", "createdAt": "2020-11-03T18:33:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyOTExNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyOTc5OA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515929798", "bodyText": "debug levels regarding what signal you received and where are you sending it.\ntrace would be the message content.", "author": "elguardian", "createdAt": "2020-11-02T12:10:55Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,307 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalType;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    private Consumer<String, byte[]> consumer;\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    private ScheduledExecutorService pollService;\n+    private ExecutorService notifyService;\n+\n+    private Map<String, SignalInfo> topic2Signal = new ConcurrentHashMap<>();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.getBoolean(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED);\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.getAndSet(true)) {\n+            return;\n+        }\n+        pollService = Executors.newSingleThreadScheduledExecutor();\n+        notifyService = Executors.newCachedThreadPool();\n+        consumer = getKafkaConsumer();\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            throw new IllegalStateException(\"Cannot find extension \" + JbpmKieServerExtension.EXTENSION_NAME);\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        pollService.shutdownNow();\n+        pollService = null;\n+        notifyService.shutdownNow();\n+        notifyService = null;\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+            deploymentService = null;\n+        }\n+        if (consumer != null) {\n+            consumer.close();\n+            consumer = null;\n+        }\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));\n+        // read only committed events\n+        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase());\n+        // do not automatically create topics by default\n+        props.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, Boolean.getBoolean(\n+                KAFKA_EXTENSION_PREFIX + \"auto.create.topics\"));\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, System.getProperty(KAFKA_EXTENSION_PREFIX + \"group.id\",\n+                \"jbpm-consumer\"));\n+        return new KafkaConsumer<>(props, new StringDeserializer(), new ByteArrayDeserializer());\n+    }\n+\n+    @Override\n+    public void onDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onUnDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    @Override\n+    public void onActivate(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onDeactivate(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    private void updateTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        for (SignalDesc signal : processDefinition.getSignalsMetadata()) {\n+            topic2Signal.computeIfAbsent(topicFromSignal(signal), t -> new SignalInfo(deploymentId,\n+                    signal));\n+        }\n+    }\n+\n+    private void removeTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        for (SignalDesc signal : processDefinition.getSignalsMetadata()) {\n+            topic2Signal.remove(topicFromSignal(signal));\n+        }\n+    }\n+\n+    private void updateRegistration(DeploymentEvent event, BiConsumer<String, ProcessDefinition> updater) {\n+        if (consumer != null) {\n+            for (DeployedAsset asset : event.getDeployedUnit().getDeployedAssets()) {\n+                updater.accept(event.getDeploymentId(), (ProcessDefinition) asset);\n+            }\n+            synchronized (consumer) {\n+                consumer.subscribe(topic2Signal.keySet());\n+            }\n+        }\n+    }\n+\n+    private String topicFromSignal(SignalDesc signal) {\n+        return System.getProperty(TOPIC_PREFIX + signal.getName(), signal.getName());\n+    }\n+\n+    @Override\n+    public void serverStarted() {\n+        if (pollService != null) {\n+            long delay = Long.getLong(KAFKA_EXTENSION_PREFIX + \"poll.interval\", 1L);\n+            pollService.scheduleWithFixedDelay(this, delay, delay, TimeUnit.SECONDS);\n+        }\n+    }\n+\n+    @Override\n+    public void createContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public boolean isUpdateContainerAllowed(String id,\n+                                            KieContainerInstance kieContainerInstance,\n+                                            Map<String, Object> parameters) {\n+        return false;\n+    }\n+\n+    @Override\n+    public void prepareContainerUpdate(String id,\n+                                       KieContainerInstance kieContainerInstance,\n+                                       Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void updateContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void disposeContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public List<Object> getAppComponents(SupportedTransports type) {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public <T> T getAppComponents(Class<T> serviceType) {\n+        return null;\n+    }\n+\n+    @Override\n+    public String getImplementedCapability() {\n+        return KieServerConstants.CAPABILITY_BPM_KAFKA;\n+    }\n+\n+    @Override\n+    public List<Object> getServices() {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public String getExtensionName() {\n+        return EXTENSION_NAME;\n+    }\n+\n+    @Override\n+    public Integer getStartOrder() {\n+        return 20;\n+    }\n+\n+    @Override\n+    public void run() {\n+        if (consumer != null && !topic2Signal.isEmpty()) {\n+            ConsumerRecords<String, byte[]> events;\n+            synchronized (consumer) {\n+                events = consumer.subscription().isEmpty() ? ConsumerRecords.empty() : consumer.poll(Duration.ZERO);\n+            }\n+            for (ConsumerRecord<String, byte[]> event : events) {\n+                notifyService.submit(() -> processEvent(event));\n+            }\n+        }\n+    }\n+\n+    private void processEvent(ConsumerRecord<String, byte[]> event) {\n+        SignalInfo signalInfo = topic2Signal.get(event.topic());\n+        if (signalInfo != null) {\n+            try {\n+                String type = signalInfo.getSignalDesc().getName();\n+                if (signalInfo.getSignalDesc().getSignalType() == SignalType.MESSAGE) {\n+                    type = \"Message-\" + type;\n+                }\n+                processService.signalEvent(signalInfo.getDeploymentId(), type, CloudEvent.read(event.value(),", "originalCommit": "02a8f2440ed9ecc0abc0702a1a04c97f91f348f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc3NTE0Nw==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r516775147", "bodyText": "done", "author": "fjtirado", "createdAt": "2020-11-03T15:57:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyOTc5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyOTkzNw==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515929937", "bodyText": "make this configurable (number of threads)", "author": "elguardian", "createdAt": "2020-11-02T12:11:13Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,307 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalType;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    private Consumer<String, byte[]> consumer;\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    private ScheduledExecutorService pollService;\n+    private ExecutorService notifyService;\n+\n+    private Map<String, SignalInfo> topic2Signal = new ConcurrentHashMap<>();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.getBoolean(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED);\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.getAndSet(true)) {\n+            return;\n+        }\n+        pollService = Executors.newSingleThreadScheduledExecutor();\n+        notifyService = Executors.newCachedThreadPool();", "originalCommit": "02a8f2440ed9ecc0abc0702a1a04c97f91f348f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjY2NDM0OQ==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r516664349", "bodyText": "I think the usage of newCachedThreadPool matches the use case, as per documentation (are you missing an upper limit?, me too ;), do we add it?)\n\nCreates a thread pool that creates new threads as needed, but\n\nwill reuse previously constructed threads when they are\navailable.  These pools will typically improve the performance\nof programs that execute many short-lived asynchronous tasks.\nCalls to {@code execute} will reuse previously constructed\nthreads if available. If no existing thread is available, a new\nthread will be created and added to the pool. Threads that have\nnot been used for sixty seconds are terminated and removed from\nthe cache. Thus, a pool that remains idle for long enough will\nnot consume any resources. Note that pools with similar\nproperties but different details (for example, timeout parameters)\nmay be created using {@link ThreadPoolExecutor} constructors.", "author": "fjtirado", "createdAt": "2020-11-03T13:27:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyOTkzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc3NTg2OA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r516775868", "bodyText": "Upper limit to avoid thread exhaustion set", "author": "fjtirado", "createdAt": "2020-11-03T15:58:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkyOTkzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkzMDQ2MA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r515930460", "bodyText": "if we have several deployments with the same signal name, wouldn't be overriding the deployment Id ?", "author": "elguardian", "createdAt": "2020-11-02T12:12:23Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,307 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalType;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    private Consumer<String, byte[]> consumer;\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    private ScheduledExecutorService pollService;\n+    private ExecutorService notifyService;\n+\n+    private Map<String, SignalInfo> topic2Signal = new ConcurrentHashMap<>();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.getBoolean(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED);\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.getAndSet(true)) {\n+            return;\n+        }\n+        pollService = Executors.newSingleThreadScheduledExecutor();\n+        notifyService = Executors.newCachedThreadPool();\n+        consumer = getKafkaConsumer();\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            throw new IllegalStateException(\"Cannot find extension \" + JbpmKieServerExtension.EXTENSION_NAME);\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        pollService.shutdownNow();\n+        pollService = null;\n+        notifyService.shutdownNow();\n+        notifyService = null;\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+            deploymentService = null;\n+        }\n+        if (consumer != null) {\n+            consumer.close();\n+            consumer = null;\n+        }\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));\n+        // read only committed events\n+        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase());\n+        // do not automatically create topics by default\n+        props.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, Boolean.getBoolean(\n+                KAFKA_EXTENSION_PREFIX + \"auto.create.topics\"));\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, System.getProperty(KAFKA_EXTENSION_PREFIX + \"group.id\",\n+                \"jbpm-consumer\"));\n+        return new KafkaConsumer<>(props, new StringDeserializer(), new ByteArrayDeserializer());\n+    }\n+\n+    @Override\n+    public void onDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onUnDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    @Override\n+    public void onActivate(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onDeactivate(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    private void updateTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        for (SignalDesc signal : processDefinition.getSignalsMetadata()) {\n+            topic2Signal.computeIfAbsent(topicFromSignal(signal), t -> new SignalInfo(deploymentId,", "originalCommit": "02a8f2440ed9ecc0abc0702a1a04c97f91f348f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjY1MjUwOA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r516652508", "bodyText": "Map structure changed to fix this", "author": "fjtirado", "createdAt": "2020-11-03T13:07:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkzMDQ2MA=="}], "type": "inlineReview"}, {"oid": "13c42b110a2fa2f9b6077c14c2094fea8dcc20b2", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/13c42b110a2fa2f9b6077c14c2094fea8dcc20b2", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-03T13:02:22Z", "type": "forcePushed"}, {"oid": "1ab6958fe0bfbaf2245278de193e40219f168da9", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/1ab6958fe0bfbaf2245278de193e40219f168da9", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-03T15:58:11Z", "type": "forcePushed"}, {"oid": "89ec48820033dc591dbdd8172b8ed967bee3bb4e", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/89ec48820033dc591dbdd8172b8ed967bee3bb4e", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-03T17:06:57Z", "type": "forcePushed"}, {"oid": "4b1cf7a3a0f88b8ed7f718439cec78f7b377aabf", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/4b1cf7a3a0f88b8ed7f718439cec78f7b377aabf", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-03T18:13:12Z", "type": "forcePushed"}, {"oid": "cd0ab2faf29f161a2e50e97ffaf74aaaefe2bbd2", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/cd0ab2faf29f161a2e50e97ffaf74aaaefe2bbd2", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-03T19:02:31Z", "type": "forcePushed"}, {"oid": "17d08ac91a960e2b2b0d0a0f4ede0dbacfa96398", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/17d08ac91a960e2b2b0d0a0f4ede0dbacfa96398", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-03T19:32:47Z", "type": "forcePushed"}, {"oid": "42f339a08f7d9eb123c506125fcf4aeb5a771dbe", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/42f339a08f7d9eb123c506125fcf4aeb5a771dbe", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-03T19:39:10Z", "type": "forcePushed"}, {"oid": "13cf2e3acbca0ecf0342470b14c72fcea677bcad", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/13cf2e3acbca0ecf0342470b14c72fcea677bcad", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-03T20:04:32Z", "type": "forcePushed"}, {"oid": "19edc3c68c6e445738877a4c3b0fa31aca0d6ae3", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/19edc3c68c6e445738877a4c3b0fa31aca0d6ae3", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-03T20:27:35Z", "type": "forcePushed"}, {"oid": "9cbed4f8d3f2fbe44ae127c8c27c2dd0a917abaa", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/9cbed4f8d3f2fbe44ae127c8c27c2dd0a917abaa", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-03T21:22:24Z", "type": "forcePushed"}, {"oid": "a1cb9ea4402f589c17d2793fb7e8e1c24a53f4bb", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/a1cb9ea4402f589c17d2793fb7e8e1c24a53f4bb", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-04T16:01:06Z", "type": "forcePushed"}, {"oid": "1a508f60083129fe949114e3c0b19f457c8587f9", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/1a508f60083129fe949114e3c0b19f457c8587f9", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-05T08:23:02Z", "type": "forcePushed"}, {"oid": "8b08b9dfe1cd5afb282fe684a2b9a5fa6a7ec7b0", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/8b08b9dfe1cd5afb282fe684a2b9a5fa6a7ec7b0", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-05T08:30:33Z", "type": "forcePushed"}, {"oid": "236d02e2b3dd570d51810a2d32af040e427aaa3f", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/236d02e2b3dd570d51810a2d32af040e427aaa3f", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-05T10:30:35Z", "type": "forcePushed"}, {"oid": "a95e2687e0fe78678090b8ee6dcb30f1a09dfdb6", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/a95e2687e0fe78678090b8ee6dcb30f1a09dfdb6", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-05T12:06:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODA5MjcwMg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518092702", "bodyText": "if we have consumer but not topics this loop does not sleep. we will eat CPU non stop without interruption.", "author": "elguardian", "createdAt": "2020-11-05T14:29:04Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,453 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.MessageDesc;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalDescBase;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    private AtomicBoolean consumerReady = new AtomicBoolean();\n+    private Consumer<String, byte[]> consumer;\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    private ExecutorService notifyService;\n+\n+    private Map<String, Map<SignalDesc, Collection<String>>> topic2Signal = new HashMap<>();\n+    private Map<String, Map<MessageDesc, Collection<String>>> topic2Message = new HashMap<>();\n+    private Map<String, Collection<Class<?>>> classes = new ConcurrentHashMap<>();\n+\n+    private Lock changeRegistrationLock = new ReentrantLock();\n+    private Lock consumerLock = new ReentrantLock();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.getBoolean(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED);\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.getAndSet(true)) {\n+            return;\n+        }\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            throw new IllegalStateException(\"Cannot find extension \" + JbpmKieServerExtension.EXTENSION_NAME);\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        classes.clear();\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+        }\n+        changeRegistrationLock.lock();\n+        try {\n+            topic2Signal.clear();\n+            topic2Message.clear();\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+        if (consumerReady.compareAndSet(true, false)) {\n+            consumer.wakeup();\n+            consumerLock.lock();\n+            try {\n+                notifyService.shutdownNow();\n+                consumer.close();\n+                notifyService = null;\n+                consumer = null;\n+            } finally {\n+                consumerLock.unlock();\n+            }\n+        }\n+        processService = null;\n+        deploymentService = null;\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));\n+        // read only committed events\n+        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase());\n+        // do not automatically create topics by default\n+        props.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, Boolean.getBoolean(\n+                KAFKA_EXTENSION_PREFIX + \"auto.create.topics\"));\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, System.getProperty(KAFKA_EXTENSION_PREFIX + \"group.id\",\n+                \"jbpm-consumer\"));\n+        return new KafkaConsumer<>(props, new StringDeserializer(), new ByteArrayDeserializer());\n+    }\n+\n+    @Override\n+    public void onDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onUnDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    @Override\n+    public void onActivate(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onDeactivate(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    private void updateTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        updateTopics(topic2Signal, deploymentId, processDefinition.getSignalsDesc());\n+        updateTopics(topic2Message, deploymentId, processDefinition.getMessagesDesc());\n+    }\n+\n+    private void removeTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        removeTopics(topic2Signal, deploymentId, processDefinition.getSignalsDesc());\n+        removeTopics(topic2Message, deploymentId, processDefinition.getMessagesDesc());\n+    }\n+\n+    private <T extends SignalDescBase> void updateTopics(Map<String, Map<T, Collection<String>>> topic2SignalBase,\n+                                                         String deploymentId,\n+                                                         Collection<T> signals) {\n+        for (T signal : signals) {\n+            topic2SignalBase.computeIfAbsent(topicFromSignal(signal), k -> new HashMap<>()).computeIfAbsent(\n+                    signal, k -> new ArrayList<>()).add(deploymentId);\n+        }\n+    }\n+\n+    private <T extends SignalDescBase> void removeTopics(Map<String, Map<T, Collection<String>>> topic2SignalBase,\n+                                                         String deploymentId,\n+                                                         Collection<T> signalsDesc) {\n+        for (T signal : signalsDesc) {\n+            String topic = topicFromSignal(signal);\n+            Map<T, Collection<String>> signals = topic2SignalBase.get(topic);\n+            if (signals != null) {\n+                Collection<String> deploymentIds = signals.get(signal);\n+                if (deploymentIds != null) {\n+                    deploymentIds.remove(deploymentId);\n+                    if (deploymentIds.isEmpty()) {\n+                        signals.remove(signal);\n+                        if (signals.isEmpty()) {\n+                            topic2SignalBase.remove(topic);\n+                        }\n+                    }\n+                }\n+\n+            }\n+        }\n+    }\n+\n+    private void updateRegistration(DeploymentEvent event, BiConsumer<String, ProcessDefinition> updater) {\n+        classes.put(event.getDeploymentId(), event.getDeployedUnit().getDeployedClasses());\n+        Set<String> topic2Register = new HashSet<>();\n+        changeRegistrationLock.lock();\n+        try {\n+            for (DeployedAsset asset : event.getDeployedUnit().getDeployedAssets()) {\n+                updater.accept(event.getDeploymentId(), (ProcessDefinition) asset);\n+            }\n+            topic2Register.addAll(topic2Signal.keySet());\n+            topic2Register.addAll(topic2Message.keySet());\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+\n+        if (consumerReady.compareAndSet(false, true)) {\n+            logger.trace(\"Creating kafka consumer\");\n+            consumer = getKafkaConsumer();\n+            subscribe(topic2Register);\n+            notifyService =\n+                    new ThreadPoolExecutor(1, Integer.getInteger(KAFKA_EXTENSION_PREFIX + \"maxNotifyThreads\", 10), 60L,\n+                            TimeUnit.SECONDS, new LinkedBlockingQueue<>());\n+            Thread pollService = new Thread(this);\n+            pollService.start();\n+        }\n+        else {\n+            consumer.wakeup();\n+            subscribe(topic2Register);\n+        }\n+\n+    }\n+\n+    private void subscribe(Set<String> topic2Register) {\n+        consumerLock.lock();\n+        try {\n+            consumer.subscribe(topic2Register);\n+        } finally {\n+            consumerLock.unlock();\n+        }\n+        logger.debug(\"Updated kafka subscription list to these topics {}\", topic2Register);\n+    }\n+\n+    private static <T extends SignalDescBase> String topicFromSignal(T signal) {\n+        return System.getProperty(TOPIC_PREFIX + signal.getName(), signal.getName());\n+    }\n+\n+    @Override\n+    public void serverStarted() {\n+        // will use lazy initialization for consumer\n+    }\n+\n+    @Override\n+    public void createContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public boolean isUpdateContainerAllowed(String id,\n+                                            KieContainerInstance kieContainerInstance,\n+                                            Map<String, Object> parameters) {\n+        return false;\n+    }\n+\n+    @Override\n+    public void prepareContainerUpdate(String id,\n+                                       KieContainerInstance kieContainerInstance,\n+                                       Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void updateContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void disposeContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public List<Object> getAppComponents(SupportedTransports type) {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public <T> T getAppComponents(Class<T> serviceType) {\n+        return null;\n+    }\n+\n+    @Override\n+    public String getImplementedCapability() {\n+        return KieServerConstants.CAPABILITY_BPM_KAFKA;\n+    }\n+\n+    @Override\n+    public List<Object> getServices() {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public String getExtensionName() {\n+        return EXTENSION_NAME;\n+    }\n+\n+    @Override\n+    public Integer getStartOrder() {\n+        return 20;\n+    }\n+\n+    @Override\n+    public void run() {\n+        Duration duration =\n+                Duration.ofSeconds(Long.getLong(KAFKA_EXTENSION_PREFIX + \"poll.interval\", 10L));\n+        logger.trace(\"Start polling kafka consumer every {} seconds\", duration.getSeconds());\n+        while (consumerReady.get()) {\n+            if (isSubscribed()) {\n+                ConsumerRecords<String, byte[]> events = ConsumerRecords.empty();\n+                consumerLock.lock();\n+                try {\n+                    try {\n+                        events = consumer.poll(duration);\n+                    } catch (WakeupException ex) {\n+                        logger.trace(\"Kafka wait interrupted\");\n+                    } catch (Exception ex) {\n+                        logger.error(\"Error polling kafka consumer\", ex);\n+                    }\n+                } finally {\n+                    consumerLock.unlock();\n+                }\n+                if (consumerReady.get() && !events.isEmpty()) {\n+                    if (logger.isDebugEnabled()) {\n+                        Map<String, Integer> eventsPerTopic = new HashMap<>();\n+                        for (ConsumerRecord<String, byte[]> event : events) {\n+                            eventsPerTopic.compute(event.topic(), (k, v) -> v == null ? 1 : v++);\n+                        }\n+                        logger.debug(\"Number of events received per topic {}\", eventsPerTopic);\n+                    }\n+                    for (ConsumerRecord<String, byte[]> event : events) {\n+                        notifyService.submit(() -> processEvent(event));\n+                    }\n+                }\n+            }\n+        }", "originalCommit": "a95e2687e0fe78678090b8ee6dcb30f1a09dfdb6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODE3NjQyMg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518176422", "bodyText": "good catch!! This bug was provoked by changing from ScheduledService to backgroung thread. Fixed used condition lock", "author": "fjtirado", "createdAt": "2020-11-05T16:16:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODA5MjcwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODE4NDAzOA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518184038", "bodyText": "good catch, fixed with a condition object", "author": "fjtirado", "createdAt": "2020-11-05T16:26:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODA5MjcwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODA5NDY3Mg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518094672", "bodyText": "this is ok but has a limitation. we won't support in kafka events typed json unmarshaller.\n{\n    map : {\n         \"key1\" : {\n               \"com.my.simple.type\" : {\n                       \"field\" : \"hello\"\n               }\n          }\n    }\n\n\n(for instance when we have Map, List so the values are not clear and require types being in the json.\nThis limitation should be in the BAPL.", "author": "elguardian", "createdAt": "2020-11-05T14:31:44Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,453 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.MessageDesc;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalDescBase;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    private AtomicBoolean consumerReady = new AtomicBoolean();\n+    private Consumer<String, byte[]> consumer;\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    private ExecutorService notifyService;\n+\n+    private Map<String, Map<SignalDesc, Collection<String>>> topic2Signal = new HashMap<>();\n+    private Map<String, Map<MessageDesc, Collection<String>>> topic2Message = new HashMap<>();\n+    private Map<String, Collection<Class<?>>> classes = new ConcurrentHashMap<>();\n+\n+    private Lock changeRegistrationLock = new ReentrantLock();\n+    private Lock consumerLock = new ReentrantLock();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.getBoolean(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED);\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.getAndSet(true)) {\n+            return;\n+        }\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            throw new IllegalStateException(\"Cannot find extension \" + JbpmKieServerExtension.EXTENSION_NAME);\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        classes.clear();\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+        }\n+        changeRegistrationLock.lock();\n+        try {\n+            topic2Signal.clear();\n+            topic2Message.clear();\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+        if (consumerReady.compareAndSet(true, false)) {\n+            consumer.wakeup();\n+            consumerLock.lock();\n+            try {\n+                notifyService.shutdownNow();\n+                consumer.close();\n+                notifyService = null;\n+                consumer = null;\n+            } finally {\n+                consumerLock.unlock();\n+            }\n+        }\n+        processService = null;\n+        deploymentService = null;\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));\n+        // read only committed events\n+        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase());\n+        // do not automatically create topics by default\n+        props.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, Boolean.getBoolean(\n+                KAFKA_EXTENSION_PREFIX + \"auto.create.topics\"));\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, System.getProperty(KAFKA_EXTENSION_PREFIX + \"group.id\",\n+                \"jbpm-consumer\"));\n+        return new KafkaConsumer<>(props, new StringDeserializer(), new ByteArrayDeserializer());\n+    }\n+\n+    @Override\n+    public void onDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onUnDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    @Override\n+    public void onActivate(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onDeactivate(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    private void updateTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        updateTopics(topic2Signal, deploymentId, processDefinition.getSignalsDesc());\n+        updateTopics(topic2Message, deploymentId, processDefinition.getMessagesDesc());\n+    }\n+\n+    private void removeTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        removeTopics(topic2Signal, deploymentId, processDefinition.getSignalsDesc());\n+        removeTopics(topic2Message, deploymentId, processDefinition.getMessagesDesc());\n+    }\n+\n+    private <T extends SignalDescBase> void updateTopics(Map<String, Map<T, Collection<String>>> topic2SignalBase,\n+                                                         String deploymentId,\n+                                                         Collection<T> signals) {\n+        for (T signal : signals) {\n+            topic2SignalBase.computeIfAbsent(topicFromSignal(signal), k -> new HashMap<>()).computeIfAbsent(\n+                    signal, k -> new ArrayList<>()).add(deploymentId);\n+        }\n+    }\n+\n+    private <T extends SignalDescBase> void removeTopics(Map<String, Map<T, Collection<String>>> topic2SignalBase,\n+                                                         String deploymentId,\n+                                                         Collection<T> signalsDesc) {\n+        for (T signal : signalsDesc) {\n+            String topic = topicFromSignal(signal);\n+            Map<T, Collection<String>> signals = topic2SignalBase.get(topic);\n+            if (signals != null) {\n+                Collection<String> deploymentIds = signals.get(signal);\n+                if (deploymentIds != null) {\n+                    deploymentIds.remove(deploymentId);\n+                    if (deploymentIds.isEmpty()) {\n+                        signals.remove(signal);\n+                        if (signals.isEmpty()) {\n+                            topic2SignalBase.remove(topic);\n+                        }\n+                    }\n+                }\n+\n+            }\n+        }\n+    }\n+\n+    private void updateRegistration(DeploymentEvent event, BiConsumer<String, ProcessDefinition> updater) {\n+        classes.put(event.getDeploymentId(), event.getDeployedUnit().getDeployedClasses());\n+        Set<String> topic2Register = new HashSet<>();\n+        changeRegistrationLock.lock();\n+        try {\n+            for (DeployedAsset asset : event.getDeployedUnit().getDeployedAssets()) {\n+                updater.accept(event.getDeploymentId(), (ProcessDefinition) asset);\n+            }\n+            topic2Register.addAll(topic2Signal.keySet());\n+            topic2Register.addAll(topic2Message.keySet());\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+\n+        if (consumerReady.compareAndSet(false, true)) {\n+            logger.trace(\"Creating kafka consumer\");\n+            consumer = getKafkaConsumer();\n+            subscribe(topic2Register);\n+            notifyService =\n+                    new ThreadPoolExecutor(1, Integer.getInteger(KAFKA_EXTENSION_PREFIX + \"maxNotifyThreads\", 10), 60L,\n+                            TimeUnit.SECONDS, new LinkedBlockingQueue<>());\n+            Thread pollService = new Thread(this);\n+            pollService.start();\n+        }\n+        else {\n+            consumer.wakeup();\n+            subscribe(topic2Register);\n+        }\n+\n+    }\n+\n+    private void subscribe(Set<String> topic2Register) {\n+        consumerLock.lock();\n+        try {\n+            consumer.subscribe(topic2Register);\n+        } finally {\n+            consumerLock.unlock();\n+        }\n+        logger.debug(\"Updated kafka subscription list to these topics {}\", topic2Register);\n+    }\n+\n+    private static <T extends SignalDescBase> String topicFromSignal(T signal) {\n+        return System.getProperty(TOPIC_PREFIX + signal.getName(), signal.getName());\n+    }\n+\n+    @Override\n+    public void serverStarted() {\n+        // will use lazy initialization for consumer\n+    }\n+\n+    @Override\n+    public void createContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public boolean isUpdateContainerAllowed(String id,\n+                                            KieContainerInstance kieContainerInstance,\n+                                            Map<String, Object> parameters) {\n+        return false;\n+    }\n+\n+    @Override\n+    public void prepareContainerUpdate(String id,\n+                                       KieContainerInstance kieContainerInstance,\n+                                       Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void updateContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void disposeContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public List<Object> getAppComponents(SupportedTransports type) {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public <T> T getAppComponents(Class<T> serviceType) {\n+        return null;\n+    }\n+\n+    @Override\n+    public String getImplementedCapability() {\n+        return KieServerConstants.CAPABILITY_BPM_KAFKA;\n+    }\n+\n+    @Override\n+    public List<Object> getServices() {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public String getExtensionName() {\n+        return EXTENSION_NAME;\n+    }\n+\n+    @Override\n+    public Integer getStartOrder() {\n+        return 20;\n+    }\n+\n+    @Override\n+    public void run() {\n+        Duration duration =\n+                Duration.ofSeconds(Long.getLong(KAFKA_EXTENSION_PREFIX + \"poll.interval\", 10L));\n+        logger.trace(\"Start polling kafka consumer every {} seconds\", duration.getSeconds());\n+        while (consumerReady.get()) {\n+            if (isSubscribed()) {\n+                ConsumerRecords<String, byte[]> events = ConsumerRecords.empty();\n+                consumerLock.lock();\n+                try {\n+                    try {\n+                        events = consumer.poll(duration);\n+                    } catch (WakeupException ex) {\n+                        logger.trace(\"Kafka wait interrupted\");\n+                    } catch (Exception ex) {\n+                        logger.error(\"Error polling kafka consumer\", ex);\n+                    }\n+                } finally {\n+                    consumerLock.unlock();\n+                }\n+                if (consumerReady.get() && !events.isEmpty()) {\n+                    if (logger.isDebugEnabled()) {\n+                        Map<String, Integer> eventsPerTopic = new HashMap<>();\n+                        for (ConsumerRecord<String, byte[]> event : events) {\n+                            eventsPerTopic.compute(event.topic(), (k, v) -> v == null ? 1 : v++);\n+                        }\n+                        logger.debug(\"Number of events received per topic {}\", eventsPerTopic);\n+                    }\n+                    for (ConsumerRecord<String, byte[]> event : events) {\n+                        notifyService.submit(() -> processEvent(event));\n+                    }\n+                }\n+            }\n+        }\n+        logger.trace(\"Kafka polling stopped\");\n+    }\n+\n+    private boolean isSubscribed() {\n+        changeRegistrationLock.lock();\n+        try {\n+            return !topic2Message.isEmpty() || !topic2Signal.isEmpty();\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+\n+    }\n+\n+    @FunctionalInterface\n+    private static interface Signaller {\n+        void signalEvent(String deploymentId, String signalName, Object data);\n+    }\n+\n+    private void processEvent(ConsumerRecord<String, byte[]> event) {\n+        changeRegistrationLock.lock();\n+        try {\n+            processEvent(topic2Signal, event, this::signalEvent);\n+            processEvent(topic2Message, event,\n+                    (deployment, signalName, data) -> signalEvent(deployment, \"Message-\" + signalName, data));\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+    }\n+    \n+    private void signalEvent  (String deployment, String signalName, Object data) {\n+        processService.signalEvent(deployment,signalName, data);\n+    }\n+\n+    private <T extends SignalDescBase> void processEvent(Map<String, Map<T, Collection<String>>> topic2SignalBase,\n+                                                         ConsumerRecord<String, byte[]> event,\n+                                                         Signaller signaller) {\n+        Map<T, Collection<String>> signalInfo = topic2SignalBase.get(event.topic());\n+        if (signalInfo != null) {\n+            for (Map.Entry<T, Collection<String>> entry : signalInfo.entrySet())\n+                try {\n+                    String signalName = entry.getKey().getName();\n+                    for (String deploymentId : entry.getValue()) {\n+                        CloudEvent<?> cloudEvent = CloudEvent.read(event.value(), getDataClass(deploymentId, entry\n+                                .getKey()));", "originalCommit": "a95e2687e0fe78678090b8ee6dcb30f1a09dfdb6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODE4MzYxNQ==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518183615", "bodyText": "Limitation docummented in bapl", "author": "fjtirado", "createdAt": "2020-11-05T16:25:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODA5NDY3Mg=="}], "type": "inlineReview"}, {"oid": "a3f08ded394724d463262a6e934d9be600fab800", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/a3f08ded394724d463262a6e934d9be600fab800", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-05T16:13:11Z", "type": "forcePushed"}, {"oid": "7dce599fbdcbb724217d646951918e2c2412074a", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/7dce599fbdcbb724217d646951918e2c2412074a", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-05T16:35:15Z", "type": "forcePushed"}, {"oid": "ad0cfb89fa112f2a98b9419495ae25c836094dba", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/ad0cfb89fa112f2a98b9419495ae25c836094dba", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-05T16:43:14Z", "type": "forcePushed"}, {"oid": "e9d0b7b6f2393d893b75e8f783e10163fd179c1b", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/e9d0b7b6f2393d893b75e8f783e10163fd179c1b", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-05T17:12:18Z", "type": "forcePushed"}, {"oid": "b3cdbcd530a752855d2657df035376a16d5b541c", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/b3cdbcd530a752855d2657df035376a16d5b541c", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-05T17:27:09Z", "type": "forcePushed"}, {"oid": "b383dc295882ea537dda989cb83658f1016326f8", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/b383dc295882ea537dda989cb83658f1016326f8", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-05T17:38:29Z", "type": "forcePushed"}, {"oid": "6b6c758a858eb67417179f99d9af440200aaeb14", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/6b6c758a858eb67417179f99d9af440200aaeb14", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-05T17:39:06Z", "type": "forcePushed"}, {"oid": "19526fb11d032f56e2c6a03171fe675b72b52056", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/19526fb11d032f56e2c6a03171fe675b72b52056", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-05T17:41:33Z", "type": "forcePushed"}, {"oid": "66bd82c671d96ca07b334754c8cf6985ef7bb72e", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/66bd82c671d96ca07b334754c8cf6985ef7bb72e", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-06T08:38:22Z", "type": "forcePushed"}, {"oid": "cac11f114b888e52e2f4f5bd0a3a9912f16ab332", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/cac11f114b888e52e2f4f5bd0a3a9912f16ab332", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-06T08:39:17Z", "type": "forcePushed"}, {"oid": "8be9b5aedcfd3fbc9ce840a9c3101598b5e22aa1", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/8be9b5aedcfd3fbc9ce840a9c3101598b5e22aa1", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-06T08:59:13Z", "type": "forcePushed"}, {"oid": "af43ecf49d1bad0773753abb586837ea7760d0dd", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/af43ecf49d1bad0773753abb586837ea7760d0dd", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-06T12:32:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODcxNjk0Mw==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518716943", "bodyText": "shouldn't we try to unsubscribe from topics before closing the consumer? wdyt?", "author": "afalhambra", "createdAt": "2020-11-06T12:21:15Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,492 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.MessageDesc;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalDescBase;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    // Kafka consumer\n+    private Consumer<String, byte[]> consumer;\n+    // JBPM services\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    // Executor thread for dispatching signals to jbpm core\n+    private AtomicReference<ExecutorService> notifyService = new AtomicReference<>();\n+    // registration and classes information\n+    private Map<String, Map<SignalDesc, Collection<String>>> topic2Signal = new HashMap<>();\n+    private Map<String, Map<MessageDesc, Collection<String>>> topic2Message = new HashMap<>();\n+    private Map<String, Collection<Class<?>>> classes = new ConcurrentHashMap<>();\n+    // synchronization variables\n+    private AtomicBoolean consumerReady = new AtomicBoolean();\n+    private Lock changeRegistrationLock = new ReentrantLock();\n+    private Lock consumerLock = new ReentrantLock();\n+    private Condition isSubscribedCond = changeRegistrationLock.newCondition();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.getBoolean(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED);\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.get()) {\n+            logger.warn(\"Kafka extension already initialized\");\n+            return;\n+        }\n+\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            logger.warn(\"Extension \" + JbpmKieServerExtension.EXTENSION_NAME + \" is required\");\n+            return;\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+        initialized.set(true);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+        }\n+        if (consumerReady.compareAndSet(true, false)) {\n+            notifyService.getAndSet(null).shutdownNow();\n+            consumer.wakeup();\n+            consumerLock.lock();\n+            try {\n+                consumer.close();", "originalCommit": "8be9b5aedcfd3fbc9ce840a9c3101598b5e22aa1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg1MjQ1OA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518852458", "bodyText": "Im not sure is required, but probably does not harm, so adding", "author": "fjtirado", "createdAt": "2020-11-06T16:11:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODcxNjk0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODcxNzg1OQ==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518717859", "bodyText": "would be beneficial to parameterized a timeout when closing consumer? wdyt?", "author": "afalhambra", "createdAt": "2020-11-06T12:23:11Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,492 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.MessageDesc;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalDescBase;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    // Kafka consumer\n+    private Consumer<String, byte[]> consumer;\n+    // JBPM services\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    // Executor thread for dispatching signals to jbpm core\n+    private AtomicReference<ExecutorService> notifyService = new AtomicReference<>();\n+    // registration and classes information\n+    private Map<String, Map<SignalDesc, Collection<String>>> topic2Signal = new HashMap<>();\n+    private Map<String, Map<MessageDesc, Collection<String>>> topic2Message = new HashMap<>();\n+    private Map<String, Collection<Class<?>>> classes = new ConcurrentHashMap<>();\n+    // synchronization variables\n+    private AtomicBoolean consumerReady = new AtomicBoolean();\n+    private Lock changeRegistrationLock = new ReentrantLock();\n+    private Lock consumerLock = new ReentrantLock();\n+    private Condition isSubscribedCond = changeRegistrationLock.newCondition();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.getBoolean(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED);\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.get()) {\n+            logger.warn(\"Kafka extension already initialized\");\n+            return;\n+        }\n+\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            logger.warn(\"Extension \" + JbpmKieServerExtension.EXTENSION_NAME + \" is required\");\n+            return;\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+        initialized.set(true);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+        }\n+        if (consumerReady.compareAndSet(true, false)) {\n+            notifyService.getAndSet(null).shutdownNow();\n+            consumer.wakeup();\n+            consumerLock.lock();\n+            try {\n+                consumer.close();", "originalCommit": "8be9b5aedcfd3fbc9ce840a9c3101598b5e22aa1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg2MDI4OQ==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518860289", "bodyText": "Property added, default 30 seconds, like no parameter close", "author": "fjtirado", "createdAt": "2020-11-06T16:24:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODcxNzg1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODczMjA3OQ==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518732079", "bodyText": "is there any chance for this to be null, if so, shouldn't we at least log warn message informing about an error while dispatching events? wdyt?", "author": "afalhambra", "createdAt": "2020-11-06T12:52:56Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,497 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.MessageDesc;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalDescBase;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    // Kafka consumer\n+    private Consumer<String, byte[]> consumer;\n+    // JBPM services\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    // Executor thread for dispatching signals to jbpm core\n+    private AtomicReference<ExecutorService> notifyService = new AtomicReference<>();\n+    // registration and classes information\n+    private Map<String, Map<SignalDesc, Collection<String>>> topic2Signal = new HashMap<>();\n+    private Map<String, Map<MessageDesc, Collection<String>>> topic2Message = new HashMap<>();\n+    private Map<String, Collection<Class<?>>> classes = new ConcurrentHashMap<>();\n+    // synchronization variables\n+    private AtomicBoolean consumerReady = new AtomicBoolean();\n+    private Lock changeRegistrationLock = new ReentrantLock();\n+    private Lock consumerLock = new ReentrantLock();\n+    private Condition isSubscribedCond = changeRegistrationLock.newCondition();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.parseBoolean(System.getProperty(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED, \"true\"));\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.get()) {\n+            logger.warn(\"Kafka extension already initialized\");\n+            return;\n+        }\n+\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            logger.warn(\"Extension \" + JbpmKieServerExtension.EXTENSION_NAME + \" is required\");\n+            return;\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+        initialized.set(true);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+        }\n+        if (consumerReady.compareAndSet(true, false)) {\n+            notifyService.getAndSet(null).shutdownNow();\n+            consumer.wakeup();\n+            consumerLock.lock();\n+            try {\n+                consumer.close();\n+            } finally {\n+                consumerLock.unlock();\n+            }\n+            consumer = null;\n+        }\n+        changeRegistrationLock.lock();\n+        try {\n+            topic2Signal.clear();\n+            topic2Message.clear();\n+            isSubscribedCond.signal();\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+        classes.clear();\n+        processService = null;\n+        deploymentService = null;\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));\n+        // read only committed events\n+        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase());\n+        // do not automatically create topics by default\n+        props.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, Boolean.getBoolean(\n+                KAFKA_EXTENSION_PREFIX + \"auto.create.topics\"));\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, System.getProperty(KAFKA_EXTENSION_PREFIX + \"group.id\",\n+                \"jbpm-consumer\"));\n+        return new KafkaConsumer<>(props, new StringDeserializer(), new ByteArrayDeserializer());\n+    }\n+\n+    @Override\n+    public void onDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onUnDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    @Override\n+    public void onActivate(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onDeactivate(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    private void updateTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        updateTopics(topic2Signal, deploymentId, processDefinition.getSignalsDesc());\n+        updateTopics(topic2Message, deploymentId, processDefinition.getMessagesDesc());\n+    }\n+\n+    private void removeTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        removeTopics(topic2Signal, deploymentId, processDefinition.getSignalsDesc());\n+        removeTopics(topic2Message, deploymentId, processDefinition.getMessagesDesc());\n+    }\n+\n+    private <T extends SignalDescBase> void updateTopics(Map<String, Map<T, Collection<String>>> topic2SignalBase,\n+                                                         String deploymentId,\n+                                                         Collection<T> signals) {\n+        for (T signal : signals) {\n+            topic2SignalBase.computeIfAbsent(topicFromSignal(signal), k -> new HashMap<>()).computeIfAbsent(\n+                    signal, k -> new ArrayList<>()).add(deploymentId);\n+        }\n+    }\n+\n+    private <T extends SignalDescBase> void removeTopics(Map<String, Map<T, Collection<String>>> topic2SignalBase,\n+                                                         String deploymentId,\n+                                                         Collection<T> signalsDesc) {\n+        for (T signal : signalsDesc) {\n+            String topic = topicFromSignal(signal);\n+            Map<T, Collection<String>> signals = topic2SignalBase.get(topic);\n+            if (signals != null) {\n+                Collection<String> deploymentIds = signals.get(signal);\n+                if (deploymentIds != null) {\n+                    deploymentIds.remove(deploymentId);\n+                    if (deploymentIds.isEmpty()) {\n+                        signals.remove(signal);\n+                        if (signals.isEmpty()) {\n+                            topic2SignalBase.remove(topic);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private void updateRegistration(DeploymentEvent event, BiConsumer<String, ProcessDefinition> updater) {\n+        classes.put(event.getDeploymentId(), event.getDeployedUnit().getDeployedClasses());\n+        Set<String> topic2Register = new HashSet<>();\n+        changeRegistrationLock.lock();\n+        try {\n+            for (DeployedAsset asset : event.getDeployedUnit().getDeployedAssets()) {\n+                updater.accept(event.getDeploymentId(), (ProcessDefinition) asset);\n+            }\n+            topic2Register.addAll(topic2Signal.keySet());\n+            topic2Register.addAll(topic2Message.keySet());\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+\n+        if (!consumerReady.get() && topic2Register.isEmpty()) {\n+            logger.debug(\"There are no topics to subscribe and consumer is not active yet, so skipping\");\n+            return;\n+        }\n+\n+        if (consumerReady.compareAndSet(false, true)) {\n+            logger.trace(\"Creating kafka consumer\");\n+            consumer = getKafkaConsumer();\n+            subscribe(topic2Register);\n+            notifyService.set(\n+                    new ThreadPoolExecutor(1, Integer.getInteger(KAFKA_EXTENSION_PREFIX + \"maxNotifyThreads\", 10), 60L,\n+                            TimeUnit.SECONDS, new LinkedBlockingQueue<>()));\n+            new Thread(this).start();\n+        } else {\n+            consumer.wakeup();\n+            subscribe(topic2Register);\n+            if (!topic2Register.isEmpty()) {\n+                changeRegistrationLock.lock();\n+                try {\n+                    isSubscribedCond.signal();\n+                } finally {\n+                    changeRegistrationLock.unlock();\n+                }\n+            }\n+        }\n+\n+    }\n+\n+    private void subscribe(Set<String> topic2Register) {\n+        consumerLock.lock();\n+        try {\n+            consumer.subscribe(topic2Register);\n+        } finally {\n+            consumerLock.unlock();\n+        }\n+        logger.debug(\"Updated kafka subscription list to these topics {}\", topic2Register);\n+    }\n+\n+    private static <T extends SignalDescBase> String topicFromSignal(T signal) {\n+        return System.getProperty(TOPIC_PREFIX + signal.getName(), signal.getName());\n+    }\n+\n+    @Override\n+    public void serverStarted() {\n+        // will use lazy initialization for consumer\n+    }\n+\n+    @Override\n+    public void createContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public boolean isUpdateContainerAllowed(String id,\n+                                            KieContainerInstance kieContainerInstance,\n+                                            Map<String, Object> parameters) {\n+        return true;\n+    }\n+\n+    @Override\n+    public void prepareContainerUpdate(String id,\n+                                       KieContainerInstance kieContainerInstance,\n+                                       Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void updateContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void disposeContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public List<Object> getAppComponents(SupportedTransports type) {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public <T> T getAppComponents(Class<T> serviceType) {\n+        return null;\n+    }\n+\n+    @Override\n+    public String getImplementedCapability() {\n+        return KieServerConstants.CAPABILITY_BPM_KAFKA;\n+    }\n+\n+    @Override\n+    public List<Object> getServices() {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public String getExtensionName() {\n+        return EXTENSION_NAME;\n+    }\n+\n+    @Override\n+    public Integer getStartOrder() {\n+        return 20;\n+    }\n+\n+    @Override\n+    public void run() {\n+        Duration duration =\n+                Duration.ofSeconds(Long.getLong(KAFKA_EXTENSION_PREFIX + \"poll.interval\", 10L));\n+        logger.trace(\"Start polling kafka consumer every {} seconds\", duration.getSeconds());\n+        try {\n+            while (consumerReady.get()) {\n+                checkSubscribed();\n+                dispatchEvents(pollEvents(duration));\n+            }\n+        } catch (InterruptedException e) {\n+            logger.warn(\"Polling thread interrupted\", e);\n+            Thread.currentThread().interrupt();\n+        } catch (Exception e) {\n+            logger.error(\"Polling thread unexpectedly finished\", e);\n+        }\n+        logger.trace(\"Kafka polling stopped\");\n+    }\n+\n+    private void checkSubscribed() throws InterruptedException {\n+        changeRegistrationLock.lock();\n+        try {\n+            while (consumerReady.get() && topic2Signal.isEmpty() && topic2Message.isEmpty()) {\n+                isSubscribedCond.await();\n+            }\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+    }\n+\n+    private ConsumerRecords<String, byte[]> pollEvents(Duration duration) {\n+        ConsumerRecords<String, byte[]> events = ConsumerRecords.empty();\n+        if (consumerReady.get()) {\n+            consumerLock.lock();\n+            try {\n+                events = consumer.poll(duration);\n+            } catch (WakeupException ex) {\n+                logger.trace(\"Kafka wait interrupted\");\n+            } catch (Exception ex) {\n+                logger.error(\"Error polling Kafka consumer\", ex);\n+            } finally {\n+                consumerLock.unlock();\n+            }\n+        }\n+        return events;\n+    }\n+\n+    private void dispatchEvents(ConsumerRecords<String, byte[]> events) {\n+        if (consumerReady.get() && !events.isEmpty()) {\n+            if (logger.isDebugEnabled()) {\n+                printEventsLog(events);\n+            }\n+            for (ConsumerRecord<String, byte[]> event : events) {\n+                if (notifyService.get() != null) {", "originalCommit": "af43ecf49d1bad0773753abb586837ea7760d0dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg2MDAwNw==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518860007", "bodyText": "this will be null only when destroying the extension. so Im removing the check and if the exception ever happen, it will catched and printed", "author": "fjtirado", "createdAt": "2020-11-06T16:24:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODczMjA3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODczMjUzMg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518732532", "bodyText": "static is redundant for inner interfaces\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static interface Signaller {\n          \n          \n            \n                private interface Signaller {", "author": "afalhambra", "createdAt": "2020-11-06T12:53:56Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,497 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.MessageDesc;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalDescBase;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    // Kafka consumer\n+    private Consumer<String, byte[]> consumer;\n+    // JBPM services\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    // Executor thread for dispatching signals to jbpm core\n+    private AtomicReference<ExecutorService> notifyService = new AtomicReference<>();\n+    // registration and classes information\n+    private Map<String, Map<SignalDesc, Collection<String>>> topic2Signal = new HashMap<>();\n+    private Map<String, Map<MessageDesc, Collection<String>>> topic2Message = new HashMap<>();\n+    private Map<String, Collection<Class<?>>> classes = new ConcurrentHashMap<>();\n+    // synchronization variables\n+    private AtomicBoolean consumerReady = new AtomicBoolean();\n+    private Lock changeRegistrationLock = new ReentrantLock();\n+    private Lock consumerLock = new ReentrantLock();\n+    private Condition isSubscribedCond = changeRegistrationLock.newCondition();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.parseBoolean(System.getProperty(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED, \"true\"));\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.get()) {\n+            logger.warn(\"Kafka extension already initialized\");\n+            return;\n+        }\n+\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            logger.warn(\"Extension \" + JbpmKieServerExtension.EXTENSION_NAME + \" is required\");\n+            return;\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+        initialized.set(true);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+        }\n+        if (consumerReady.compareAndSet(true, false)) {\n+            notifyService.getAndSet(null).shutdownNow();\n+            consumer.wakeup();\n+            consumerLock.lock();\n+            try {\n+                consumer.close();\n+            } finally {\n+                consumerLock.unlock();\n+            }\n+            consumer = null;\n+        }\n+        changeRegistrationLock.lock();\n+        try {\n+            topic2Signal.clear();\n+            topic2Message.clear();\n+            isSubscribedCond.signal();\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+        classes.clear();\n+        processService = null;\n+        deploymentService = null;\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));\n+        // read only committed events\n+        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase());\n+        // do not automatically create topics by default\n+        props.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, Boolean.getBoolean(\n+                KAFKA_EXTENSION_PREFIX + \"auto.create.topics\"));\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, System.getProperty(KAFKA_EXTENSION_PREFIX + \"group.id\",\n+                \"jbpm-consumer\"));\n+        return new KafkaConsumer<>(props, new StringDeserializer(), new ByteArrayDeserializer());\n+    }\n+\n+    @Override\n+    public void onDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onUnDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    @Override\n+    public void onActivate(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onDeactivate(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    private void updateTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        updateTopics(topic2Signal, deploymentId, processDefinition.getSignalsDesc());\n+        updateTopics(topic2Message, deploymentId, processDefinition.getMessagesDesc());\n+    }\n+\n+    private void removeTopics(String deploymentId, ProcessDefinition processDefinition) {\n+        removeTopics(topic2Signal, deploymentId, processDefinition.getSignalsDesc());\n+        removeTopics(topic2Message, deploymentId, processDefinition.getMessagesDesc());\n+    }\n+\n+    private <T extends SignalDescBase> void updateTopics(Map<String, Map<T, Collection<String>>> topic2SignalBase,\n+                                                         String deploymentId,\n+                                                         Collection<T> signals) {\n+        for (T signal : signals) {\n+            topic2SignalBase.computeIfAbsent(topicFromSignal(signal), k -> new HashMap<>()).computeIfAbsent(\n+                    signal, k -> new ArrayList<>()).add(deploymentId);\n+        }\n+    }\n+\n+    private <T extends SignalDescBase> void removeTopics(Map<String, Map<T, Collection<String>>> topic2SignalBase,\n+                                                         String deploymentId,\n+                                                         Collection<T> signalsDesc) {\n+        for (T signal : signalsDesc) {\n+            String topic = topicFromSignal(signal);\n+            Map<T, Collection<String>> signals = topic2SignalBase.get(topic);\n+            if (signals != null) {\n+                Collection<String> deploymentIds = signals.get(signal);\n+                if (deploymentIds != null) {\n+                    deploymentIds.remove(deploymentId);\n+                    if (deploymentIds.isEmpty()) {\n+                        signals.remove(signal);\n+                        if (signals.isEmpty()) {\n+                            topic2SignalBase.remove(topic);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private void updateRegistration(DeploymentEvent event, BiConsumer<String, ProcessDefinition> updater) {\n+        classes.put(event.getDeploymentId(), event.getDeployedUnit().getDeployedClasses());\n+        Set<String> topic2Register = new HashSet<>();\n+        changeRegistrationLock.lock();\n+        try {\n+            for (DeployedAsset asset : event.getDeployedUnit().getDeployedAssets()) {\n+                updater.accept(event.getDeploymentId(), (ProcessDefinition) asset);\n+            }\n+            topic2Register.addAll(topic2Signal.keySet());\n+            topic2Register.addAll(topic2Message.keySet());\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+\n+        if (!consumerReady.get() && topic2Register.isEmpty()) {\n+            logger.debug(\"There are no topics to subscribe and consumer is not active yet, so skipping\");\n+            return;\n+        }\n+\n+        if (consumerReady.compareAndSet(false, true)) {\n+            logger.trace(\"Creating kafka consumer\");\n+            consumer = getKafkaConsumer();\n+            subscribe(topic2Register);\n+            notifyService.set(\n+                    new ThreadPoolExecutor(1, Integer.getInteger(KAFKA_EXTENSION_PREFIX + \"maxNotifyThreads\", 10), 60L,\n+                            TimeUnit.SECONDS, new LinkedBlockingQueue<>()));\n+            new Thread(this).start();\n+        } else {\n+            consumer.wakeup();\n+            subscribe(topic2Register);\n+            if (!topic2Register.isEmpty()) {\n+                changeRegistrationLock.lock();\n+                try {\n+                    isSubscribedCond.signal();\n+                } finally {\n+                    changeRegistrationLock.unlock();\n+                }\n+            }\n+        }\n+\n+    }\n+\n+    private void subscribe(Set<String> topic2Register) {\n+        consumerLock.lock();\n+        try {\n+            consumer.subscribe(topic2Register);\n+        } finally {\n+            consumerLock.unlock();\n+        }\n+        logger.debug(\"Updated kafka subscription list to these topics {}\", topic2Register);\n+    }\n+\n+    private static <T extends SignalDescBase> String topicFromSignal(T signal) {\n+        return System.getProperty(TOPIC_PREFIX + signal.getName(), signal.getName());\n+    }\n+\n+    @Override\n+    public void serverStarted() {\n+        // will use lazy initialization for consumer\n+    }\n+\n+    @Override\n+    public void createContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public boolean isUpdateContainerAllowed(String id,\n+                                            KieContainerInstance kieContainerInstance,\n+                                            Map<String, Object> parameters) {\n+        return true;\n+    }\n+\n+    @Override\n+    public void prepareContainerUpdate(String id,\n+                                       KieContainerInstance kieContainerInstance,\n+                                       Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void updateContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public void disposeContainer(String id, KieContainerInstance kieContainerInstance, Map<String, Object> parameters) {\n+        // will be done by listener\n+    }\n+\n+    @Override\n+    public List<Object> getAppComponents(SupportedTransports type) {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public <T> T getAppComponents(Class<T> serviceType) {\n+        return null;\n+    }\n+\n+    @Override\n+    public String getImplementedCapability() {\n+        return KieServerConstants.CAPABILITY_BPM_KAFKA;\n+    }\n+\n+    @Override\n+    public List<Object> getServices() {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public String getExtensionName() {\n+        return EXTENSION_NAME;\n+    }\n+\n+    @Override\n+    public Integer getStartOrder() {\n+        return 20;\n+    }\n+\n+    @Override\n+    public void run() {\n+        Duration duration =\n+                Duration.ofSeconds(Long.getLong(KAFKA_EXTENSION_PREFIX + \"poll.interval\", 10L));\n+        logger.trace(\"Start polling kafka consumer every {} seconds\", duration.getSeconds());\n+        try {\n+            while (consumerReady.get()) {\n+                checkSubscribed();\n+                dispatchEvents(pollEvents(duration));\n+            }\n+        } catch (InterruptedException e) {\n+            logger.warn(\"Polling thread interrupted\", e);\n+            Thread.currentThread().interrupt();\n+        } catch (Exception e) {\n+            logger.error(\"Polling thread unexpectedly finished\", e);\n+        }\n+        logger.trace(\"Kafka polling stopped\");\n+    }\n+\n+    private void checkSubscribed() throws InterruptedException {\n+        changeRegistrationLock.lock();\n+        try {\n+            while (consumerReady.get() && topic2Signal.isEmpty() && topic2Message.isEmpty()) {\n+                isSubscribedCond.await();\n+            }\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+    }\n+\n+    private ConsumerRecords<String, byte[]> pollEvents(Duration duration) {\n+        ConsumerRecords<String, byte[]> events = ConsumerRecords.empty();\n+        if (consumerReady.get()) {\n+            consumerLock.lock();\n+            try {\n+                events = consumer.poll(duration);\n+            } catch (WakeupException ex) {\n+                logger.trace(\"Kafka wait interrupted\");\n+            } catch (Exception ex) {\n+                logger.error(\"Error polling Kafka consumer\", ex);\n+            } finally {\n+                consumerLock.unlock();\n+            }\n+        }\n+        return events;\n+    }\n+\n+    private void dispatchEvents(ConsumerRecords<String, byte[]> events) {\n+        if (consumerReady.get() && !events.isEmpty()) {\n+            if (logger.isDebugEnabled()) {\n+                printEventsLog(events);\n+            }\n+            for (ConsumerRecord<String, byte[]> event : events) {\n+                if (notifyService.get() != null) {\n+                    notifyService.get().submit(() -> processEvent(event));\n+                }\n+            }\n+        }\n+    }\n+\n+    private void printEventsLog(ConsumerRecords<String, byte[]> events) {\n+        Map<String, Integer> eventsPerTopic = new HashMap<>();\n+        for (ConsumerRecord<String, byte[]> event : events) {\n+            eventsPerTopic.compute(event.topic(), (k, v) -> v == null ? 1 : v++);\n+        }\n+        logger.debug(\"Number of events received per topic {}\", eventsPerTopic);\n+    }\n+\n+    @FunctionalInterface\n+    private static interface Signaller {", "originalCommit": "af43ecf49d1bad0773753abb586837ea7760d0dd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc0NzE1NA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518747154", "bodyText": "Shouldn't we also stop consumer when undeploying? just wondering, wdyt?", "author": "afalhambra", "createdAt": "2020-11-06T13:22:35Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,497 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.MessageDesc;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalDescBase;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    // Kafka consumer\n+    private Consumer<String, byte[]> consumer;\n+    // JBPM services\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    // Executor thread for dispatching signals to jbpm core\n+    private AtomicReference<ExecutorService> notifyService = new AtomicReference<>();\n+    // registration and classes information\n+    private Map<String, Map<SignalDesc, Collection<String>>> topic2Signal = new HashMap<>();\n+    private Map<String, Map<MessageDesc, Collection<String>>> topic2Message = new HashMap<>();\n+    private Map<String, Collection<Class<?>>> classes = new ConcurrentHashMap<>();\n+    // synchronization variables\n+    private AtomicBoolean consumerReady = new AtomicBoolean();\n+    private Lock changeRegistrationLock = new ReentrantLock();\n+    private Lock consumerLock = new ReentrantLock();\n+    private Condition isSubscribedCond = changeRegistrationLock.newCondition();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.parseBoolean(System.getProperty(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED, \"true\"));\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.get()) {\n+            logger.warn(\"Kafka extension already initialized\");\n+            return;\n+        }\n+\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            logger.warn(\"Extension \" + JbpmKieServerExtension.EXTENSION_NAME + \" is required\");\n+            return;\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+        initialized.set(true);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+        }\n+        if (consumerReady.compareAndSet(true, false)) {\n+            notifyService.getAndSet(null).shutdownNow();\n+            consumer.wakeup();\n+            consumerLock.lock();\n+            try {\n+                consumer.close();\n+            } finally {\n+                consumerLock.unlock();\n+            }\n+            consumer = null;\n+        }\n+        changeRegistrationLock.lock();\n+        try {\n+            topic2Signal.clear();\n+            topic2Message.clear();\n+            isSubscribedCond.signal();\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+        classes.clear();\n+        processService = null;\n+        deploymentService = null;\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));\n+        // read only committed events\n+        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase());\n+        // do not automatically create topics by default\n+        props.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, Boolean.getBoolean(\n+                KAFKA_EXTENSION_PREFIX + \"auto.create.topics\"));\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, System.getProperty(KAFKA_EXTENSION_PREFIX + \"group.id\",\n+                \"jbpm-consumer\"));\n+        return new KafkaConsumer<>(props, new StringDeserializer(), new ByteArrayDeserializer());\n+    }\n+\n+    @Override\n+    public void onDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onUnDeploy(DeploymentEvent event) {", "originalCommit": "af43ecf49d1bad0773753abb586837ea7760d0dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg1NTQ1Nw==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518855457", "bodyText": "No, we are undeploying one container, there might be other containers listening for events", "author": "fjtirado", "createdAt": "2020-11-06T16:16:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc0NzE1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc0NzMyNg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518747326", "bodyText": "same as above", "author": "afalhambra", "createdAt": "2020-11-06T13:22:57Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/main/java/org/kie/server/services/jbpm/kafka/KafkaServerExtension.java", "diffHunk": "@@ -0,0 +1,497 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+*/\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.BiConsumer;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.IsolationLevel;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentEventListener;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedAsset;\n+import org.jbpm.services.api.model.MessageDesc;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.jbpm.services.api.model.SignalDesc;\n+import org.jbpm.services.api.model.SignalDescBase;\n+import org.kie.server.api.KieServerConstants;\n+import org.kie.server.services.api.KieContainerInstance;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.api.SupportedTransports;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.kie.server.services.jbpm.JbpmKieServerExtension;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaServerExtension implements KieServerExtension, DeploymentEventListener, Runnable {\n+\n+    public static final String EXTENSION_NAME = \"Kafka\";\n+    static final String KAFKA_EXTENSION_PREFIX = \"org.kie.server.jbpm-kafka.ext.\";\n+    static final String TOPIC_PREFIX = KAFKA_EXTENSION_PREFIX + \"topics.\";\n+    private static final Logger logger = LoggerFactory.getLogger(KafkaServerExtension.class);\n+\n+    private AtomicBoolean initialized = new AtomicBoolean();\n+    // Kafka consumer\n+    private Consumer<String, byte[]> consumer;\n+    // JBPM services\n+    private ListenerSupport deploymentService;\n+    private ProcessService processService;\n+    // Executor thread for dispatching signals to jbpm core\n+    private AtomicReference<ExecutorService> notifyService = new AtomicReference<>();\n+    // registration and classes information\n+    private Map<String, Map<SignalDesc, Collection<String>>> topic2Signal = new HashMap<>();\n+    private Map<String, Map<MessageDesc, Collection<String>>> topic2Message = new HashMap<>();\n+    private Map<String, Collection<Class<?>>> classes = new ConcurrentHashMap<>();\n+    // synchronization variables\n+    private AtomicBoolean consumerReady = new AtomicBoolean();\n+    private Lock changeRegistrationLock = new ReentrantLock();\n+    private Lock consumerLock = new ReentrantLock();\n+    private Condition isSubscribedCond = changeRegistrationLock.newCondition();\n+\n+    @Override\n+    public boolean isInitialized() {\n+        return initialized.get();\n+    }\n+\n+    @Override\n+    public boolean isActive() {\n+        return !Boolean.parseBoolean(System.getProperty(KieServerConstants.KIE_KAFKA_SERVER_EXT_DISABLED, \"true\"));\n+    }\n+\n+    @Override\n+    public void init(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (initialized.get()) {\n+            logger.warn(\"Kafka extension already initialized\");\n+            return;\n+        }\n+\n+        KieServerExtension jbpmExt = registry.getServerExtension(JbpmKieServerExtension.EXTENSION_NAME);\n+        if (jbpmExt == null) {\n+            logger.warn(\"Extension \" + JbpmKieServerExtension.EXTENSION_NAME + \" is required\");\n+            return;\n+        }\n+\n+        for (Object service : jbpmExt.getServices()) {\n+            if (deploymentService == null && DeploymentService.class.isAssignableFrom(service.getClass())) {\n+                deploymentService = (ListenerSupport) service;\n+            } else if (processService == null && ProcessService.class.isAssignableFrom(service.getClass())) {\n+                processService = (ProcessService) service;\n+            }\n+            if (deploymentService != null && processService != null) {\n+                break;\n+            }\n+        }\n+        if (deploymentService == null) {\n+            throw new IllegalStateException(\"Cannot find deployment service\");\n+        }\n+        if (processService == null) {\n+            throw new IllegalStateException(\"Cannot find process service\");\n+        }\n+        deploymentService.addListener(this);\n+        initialized.set(true);\n+    }\n+\n+    @Override\n+    public void destroy(KieServerImpl kieServer, KieServerRegistry registry) {\n+        if (deploymentService != null) {\n+            deploymentService.removeListener(this);\n+        }\n+        if (consumerReady.compareAndSet(true, false)) {\n+            notifyService.getAndSet(null).shutdownNow();\n+            consumer.wakeup();\n+            consumerLock.lock();\n+            try {\n+                consumer.close();\n+            } finally {\n+                consumerLock.unlock();\n+            }\n+            consumer = null;\n+        }\n+        changeRegistrationLock.lock();\n+        try {\n+            topic2Signal.clear();\n+            topic2Message.clear();\n+            isSubscribedCond.signal();\n+        } finally {\n+            changeRegistrationLock.unlock();\n+        }\n+        classes.clear();\n+        processService = null;\n+        deploymentService = null;\n+        initialized.set(false);\n+    }\n+\n+    protected Consumer<String, byte[]> getKafkaConsumer() {\n+        Map<String, Object> props = new HashMap<>();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, System.getProperty(\n+                KAFKA_EXTENSION_PREFIX + \"boopstrap.servers\", \"localhost:9092\"));\n+        // read only committed events\n+        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase());\n+        // do not automatically create topics by default\n+        props.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, Boolean.getBoolean(\n+                KAFKA_EXTENSION_PREFIX + \"auto.create.topics\"));\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, System.getProperty(KAFKA_EXTENSION_PREFIX + \"group.id\",\n+                \"jbpm-consumer\"));\n+        return new KafkaConsumer<>(props, new StringDeserializer(), new ByteArrayDeserializer());\n+    }\n+\n+    @Override\n+    public void onDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onUnDeploy(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);\n+    }\n+\n+    @Override\n+    public void onActivate(DeploymentEvent event) {\n+        updateRegistration(event, this::updateTopics);\n+    }\n+\n+    @Override\n+    public void onDeactivate(DeploymentEvent event) {\n+        updateRegistration(event, this::removeTopics);", "originalCommit": "af43ecf49d1bad0773753abb586837ea7760d0dd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwNDIzNg==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518804236", "bodyText": "Long.getLong(KafkaServerExtension.KAFKA_EXTENSION_PREFIX + \"poll.interval\", 1L) * 2000 will always return 2 seconds as the property org.kie.server.jbpm-kafka.ext.poll.interval is not defined anywhere in the test.\nNot an issue, but I was thinking about testing all these different sets of properties and make sure it behaves properly", "author": "afalhambra", "createdAt": "2020-11-06T14:57:10Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/test/java/org/kie/server/services/jbpm/kafka/KafkaServerExtensionTest.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.MockConsumer;\n+import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n+import org.apache.kafka.common.TopicPartition;\n+import org.jbpm.bpmn2.core.Message;\n+import org.jbpm.bpmn2.core.Signal;\n+import org.jbpm.kie.services.impl.model.MessageDescImpl;\n+import org.jbpm.kie.services.impl.model.SignalDescImpl;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedUnit;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.mockito.Mockito;\n+import org.mockito.verification.VerificationMode;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.timeout;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.withSettings;\n+\n+public class KafkaServerExtensionTest {\n+\n+    private static class MockKafkaServerExtension extends KafkaServerExtension {\n+\n+        private MockConsumer<String, byte[]> consumer;\n+\n+        public MockKafkaServerExtension(MockConsumer<String, byte[]> consumer) {\n+            this.consumer = consumer;\n+        }\n+\n+        protected Consumer<String, byte[]> getKafkaConsumer() {\n+            return consumer;\n+        }\n+\n+        public void setKafkaConsumer(MockConsumer<String, byte[]> consumer) {\n+            this.consumer = consumer;\n+        }\n+    }\n+\n+    private ProcessService processService;\n+    private MockKafkaServerExtension extension;\n+    private KieServerImpl server;\n+    private KieServerRegistry registry;\n+    private DeployedUnit deployedUnit;\n+    private ProcessDefinition processDefinition;\n+    private MockConsumer<String, byte[]> mockConsumer;\n+    private static Logger logger = LoggerFactory.getLogger(KafkaServerExtensionTest.class);\n+\n+\n+    @Before\n+    public void setup() {\n+        mockConsumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);\n+        extension = new MockKafkaServerExtension(mockConsumer);\n+        server = mock(KieServerImpl.class);\n+        registry = mock(KieServerRegistry.class);\n+        KieServerExtension serverExtension = mock(KieServerExtension.class);\n+        when(registry.getServerExtension(Mockito.anyString())).thenReturn(serverExtension);\n+        ListenerSupport deployService = mock(ListenerSupport.class, withSettings().extraInterfaces(\n+                DeploymentService.class));\n+        processService = mock(ProcessService.class);\n+        when(serverExtension.getServices()).thenReturn(Arrays.asList(deployService, processService));\n+        deployedUnit = mock(DeployedUnit.class);\n+        processDefinition = mock(ProcessDefinition.class);\n+        when(deployedUnit.getDeployedAssets()).thenReturn(Collections.singletonList(processDefinition));\n+        extension.init(server, registry);\n+        extension.serverStarted();\n+    }\n+\n+    @After\n+    public void close() {\n+        extension.destroy(server, registry);\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorSignal() {\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"MySignal\", \"String\"))));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        publishEvent(\"MySignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy1\", \"MySignal\", \"javierito\");\n+    }\n+\n+    @Test\n+    public void testKafkaSubscriptionChange() {\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"MySignal\", \"String\"))));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        publishEvent(\"MySignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy1\", \"MySignal\", \"javierito\");\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"ChangedSignal\", \"ChangedSignal\", \"String\"))));\n+        extension.onActivate(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        publishEvent(\"ChangedSignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy1\", \"ChangedSignal\", \"javierito\");\n+    }\n+\n+    @Test\n+    public void testKafkaSubscriptionEmpty() {\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"MySignal\", \"String\"))));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        publishEvent(\"MySignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy1\", \"MySignal\", \"javierito\");\n+        extension.onUnDeploy(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.emptyList());\n+        extension.onActivate(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        publishEventWithoutTopicCheck(\"ChangedSignal\",\n+                \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout(0)).signalEvent(\"MyDeploy1\", \"ChangedSignal\", \"javierito\");\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"NewSignal\", \"String\"))));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        publishEvent(\"NewSignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy1\", \"NewSignal\", \"javierito\");\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorMessage() {\n+        Message msg = new Message(\"MyMessage\");\n+        msg.setName(\"Hello\");\n+        msg.setType(\"String\");\n+        when(processDefinition.getMessagesDesc()).thenReturn(Collections.singletonList(MessageDescImpl.from(msg)));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy2\", deployedUnit));\n+        publishEvent(\"Hello\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"pepe\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy2\", \"Message-Hello\", \"pepe\");\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorMessageTopic() {\n+        final String topicProperty = KafkaServerExtension.TOPIC_PREFIX + \"Hello\";\n+        System.setProperty(topicProperty, \"MyTopic\");\n+        try {\n+            Message msg = new Message(\"MyMessage\");\n+            msg.setName(\"Hello\");\n+            msg.setType(\"String\");\n+            when(processDefinition.getMessagesDesc()).thenReturn(Collections.singletonList(MessageDescImpl.from(\n+                    msg)));\n+            extension.onDeploy(new DeploymentEvent(\"MyDeploy3\", deployedUnit));\n+            publishEvent(\"MyTopic\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"pepe\\\"}\");\n+            verify(processService, getTimeout()).signalEvent(\"MyDeploy3\", \"Message-Hello\", \"pepe\");\n+        } finally {\n+            System.clearProperty(topicProperty);\n+        }\n+    }\n+\n+    @Test\n+    public void testWithDestroy() {\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"MySignal2\", \"String\"))));\n+        Message msg = new Message(\"MyMessage\");\n+        msg.setName(\"Hello2\");\n+        msg.setType(\"String\");\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy4\", deployedUnit));\n+        publishEvent(\"MySignal2\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy4\", \"MySignal2\", \"javierito\");\n+        extension.destroy(server, registry);\n+        mockConsumer = new MockConsumer<String, byte[]>(OffsetResetStrategy.EARLIEST);\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.emptyList());\n+        when(processDefinition.getMessagesDesc()).thenReturn(Collections.singletonList(MessageDescImpl.from(\n+                msg)));\n+        extension.setKafkaConsumer(mockConsumer);\n+        extension.init(server, registry);\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy5\", deployedUnit));\n+        publishEvent(\"Hello2\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"pepe\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy5\", \"Message-Hello2\", \"pepe\");\n+    }\n+\n+\n+    private VerificationMode getTimeout() {\n+        return getTimeout(1);\n+    }\n+\n+    private VerificationMode getTimeout(int times) {\n+        return timeout(Long.getLong(KafkaServerExtension.KAFKA_EXTENSION_PREFIX + \"poll.interval\", 1L) * 2000).times(", "originalCommit": "af43ecf49d1bad0773753abb586837ea7760d0dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg2MDU1MQ==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518860551", "bodyText": "Ill test that one, other I think should be tested by integration test", "author": "fjtirado", "createdAt": "2020-11-06T16:24:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwNDIzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgyOTQ4Mw==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518829483", "bodyText": "It would be nice to check some other data structure for the cloudEvent rather than always a String.", "author": "afalhambra", "createdAt": "2020-11-06T15:34:42Z", "path": "kie-server-parent/kie-server-services/kie-server-services-kafka/src/test/java/org/kie/server/services/jbpm/kafka/KafkaServerExtensionTest.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Copyright 2020 Red Hat, Inc. and/or its affiliates.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.kie.server.services.jbpm.kafka;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.MockConsumer;\n+import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n+import org.apache.kafka.common.TopicPartition;\n+import org.jbpm.bpmn2.core.Message;\n+import org.jbpm.bpmn2.core.Signal;\n+import org.jbpm.kie.services.impl.model.MessageDescImpl;\n+import org.jbpm.kie.services.impl.model.SignalDescImpl;\n+import org.jbpm.services.api.DeploymentEvent;\n+import org.jbpm.services.api.DeploymentService;\n+import org.jbpm.services.api.ListenerSupport;\n+import org.jbpm.services.api.ProcessService;\n+import org.jbpm.services.api.model.DeployedUnit;\n+import org.jbpm.services.api.model.ProcessDefinition;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.kie.server.services.api.KieServerExtension;\n+import org.kie.server.services.api.KieServerRegistry;\n+import org.kie.server.services.impl.KieServerImpl;\n+import org.mockito.Mockito;\n+import org.mockito.verification.VerificationMode;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.timeout;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.mockito.Mockito.withSettings;\n+\n+public class KafkaServerExtensionTest {\n+\n+    private static class MockKafkaServerExtension extends KafkaServerExtension {\n+\n+        private MockConsumer<String, byte[]> consumer;\n+\n+        public MockKafkaServerExtension(MockConsumer<String, byte[]> consumer) {\n+            this.consumer = consumer;\n+        }\n+\n+        protected Consumer<String, byte[]> getKafkaConsumer() {\n+            return consumer;\n+        }\n+\n+        public void setKafkaConsumer(MockConsumer<String, byte[]> consumer) {\n+            this.consumer = consumer;\n+        }\n+    }\n+\n+    private ProcessService processService;\n+    private MockKafkaServerExtension extension;\n+    private KieServerImpl server;\n+    private KieServerRegistry registry;\n+    private DeployedUnit deployedUnit;\n+    private ProcessDefinition processDefinition;\n+    private MockConsumer<String, byte[]> mockConsumer;\n+    private static Logger logger = LoggerFactory.getLogger(KafkaServerExtensionTest.class);\n+\n+\n+    @Before\n+    public void setup() {\n+        mockConsumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);\n+        extension = new MockKafkaServerExtension(mockConsumer);\n+        server = mock(KieServerImpl.class);\n+        registry = mock(KieServerRegistry.class);\n+        KieServerExtension serverExtension = mock(KieServerExtension.class);\n+        when(registry.getServerExtension(Mockito.anyString())).thenReturn(serverExtension);\n+        ListenerSupport deployService = mock(ListenerSupport.class, withSettings().extraInterfaces(\n+                DeploymentService.class));\n+        processService = mock(ProcessService.class);\n+        when(serverExtension.getServices()).thenReturn(Arrays.asList(deployService, processService));\n+        deployedUnit = mock(DeployedUnit.class);\n+        processDefinition = mock(ProcessDefinition.class);\n+        when(deployedUnit.getDeployedAssets()).thenReturn(Collections.singletonList(processDefinition));\n+        extension.init(server, registry);\n+        extension.serverStarted();\n+    }\n+\n+    @After\n+    public void close() {\n+        extension.destroy(server, registry);\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorSignal() {\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"MySignal\", \"String\"))));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        publishEvent(\"MySignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy1\", \"MySignal\", \"javierito\");\n+    }\n+\n+    @Test\n+    public void testKafkaSubscriptionChange() {\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"MySignal\", \"String\"))));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        publishEvent(\"MySignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy1\", \"MySignal\", \"javierito\");\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"ChangedSignal\", \"ChangedSignal\", \"String\"))));\n+        extension.onActivate(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        publishEvent(\"ChangedSignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy1\", \"ChangedSignal\", \"javierito\");\n+    }\n+\n+    @Test\n+    public void testKafkaSubscriptionEmpty() {\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"MySignal\", \"String\"))));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        publishEvent(\"MySignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy1\", \"MySignal\", \"javierito\");\n+        extension.onUnDeploy(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.emptyList());\n+        extension.onActivate(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        publishEventWithoutTopicCheck(\"ChangedSignal\",\n+                \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout(0)).signalEvent(\"MyDeploy1\", \"ChangedSignal\", \"javierito\");\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"NewSignal\", \"String\"))));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy1\", deployedUnit));\n+        publishEvent(\"NewSignal\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy1\", \"NewSignal\", \"javierito\");\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorMessage() {\n+        Message msg = new Message(\"MyMessage\");\n+        msg.setName(\"Hello\");\n+        msg.setType(\"String\");\n+        when(processDefinition.getMessagesDesc()).thenReturn(Collections.singletonList(MessageDescImpl.from(msg)));\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy2\", deployedUnit));\n+        publishEvent(\"Hello\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"pepe\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy2\", \"Message-Hello\", \"pepe\");\n+    }\n+\n+    @Test\n+    public void testKafkaServerExecutorMessageTopic() {\n+        final String topicProperty = KafkaServerExtension.TOPIC_PREFIX + \"Hello\";\n+        System.setProperty(topicProperty, \"MyTopic\");\n+        try {\n+            Message msg = new Message(\"MyMessage\");\n+            msg.setName(\"Hello\");\n+            msg.setType(\"String\");\n+            when(processDefinition.getMessagesDesc()).thenReturn(Collections.singletonList(MessageDescImpl.from(\n+                    msg)));\n+            extension.onDeploy(new DeploymentEvent(\"MyDeploy3\", deployedUnit));\n+            publishEvent(\"MyTopic\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"pepe\\\"}\");\n+            verify(processService, getTimeout()).signalEvent(\"MyDeploy3\", \"Message-Hello\", \"pepe\");\n+        } finally {\n+            System.clearProperty(topicProperty);\n+        }\n+    }\n+\n+    @Test\n+    public void testWithDestroy() {\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.singletonList(SignalDescImpl.from(\n+                new Signal(\"MySignal\", \"MySignal2\", \"String\"))));\n+        Message msg = new Message(\"MyMessage\");\n+        msg.setName(\"Hello2\");\n+        msg.setType(\"String\");\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy4\", deployedUnit));\n+        publishEvent(\"MySignal2\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"javierito\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy4\", \"MySignal2\", \"javierito\");\n+        extension.destroy(server, registry);\n+        mockConsumer = new MockConsumer<String, byte[]>(OffsetResetStrategy.EARLIEST);\n+        when(processDefinition.getSignalsDesc()).thenReturn(Collections.emptyList());\n+        when(processDefinition.getMessagesDesc()).thenReturn(Collections.singletonList(MessageDescImpl.from(\n+                msg)));\n+        extension.setKafkaConsumer(mockConsumer);\n+        extension.init(server, registry);\n+        extension.onDeploy(new DeploymentEvent(\"MyDeploy5\", deployedUnit));\n+        publishEvent(\"Hello2\", \"{\\\"id\\\":\\\"javi\\\",\\\"type\\\":\\\"one\\\",\\\"source\\\":\\\"pepe\\\",\\\"data\\\":\\\"pepe\\\"}\");\n+        verify(processService, getTimeout()).signalEvent(\"MyDeploy5\", \"Message-Hello2\", \"pepe\");", "originalCommit": "af43ecf49d1bad0773753abb586837ea7760d0dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg1NTg0MA==", "url": "https://github.com/kiegroup/droolsjbpm-integration/pull/2288#discussion_r518855840", "bodyText": "Sure, test added", "author": "fjtirado", "createdAt": "2020-11-06T16:17:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgyOTQ4Mw=="}], "type": "inlineReview"}, {"oid": "d63411f76bc17e334fe358f8c70b90e4dd1ac424", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/d63411f76bc17e334fe358f8c70b90e4dd1ac424", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-06T16:50:53Z", "type": "forcePushed"}, {"oid": "4efcbabbeadb197f6e8fb1a651bb1db181cf0079", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/4efcbabbeadb197f6e8fb1a651bb1db181cf0079", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-10T10:14:14Z", "type": "forcePushed"}, {"oid": "95e21534d050bff56d13074948950be9df939956", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/95e21534d050bff56d13074948950be9df939956", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-12T10:22:47Z", "type": "forcePushed"}, {"oid": "95b49e57d4cffeeb11e2932450b8e238634c0922", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/95b49e57d4cffeeb11e2932450b8e238634c0922", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-12T10:24:52Z", "type": "forcePushed"}, {"oid": "945d353841d850912e6eefcc695cd7d4d91389cc", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/945d353841d850912e6eefcc695cd7d4d91389cc", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-12T10:28:49Z", "type": "forcePushed"}, {"oid": "3f70db10544811cf5d8d5efcd041deec80f2b89b", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/3f70db10544811cf5d8d5efcd041deec80f2b89b", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-12T15:41:58Z", "type": "forcePushed"}, {"oid": "8979388bc0e800adb102dfb3c4a9fbae2a868590", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/8979388bc0e800adb102dfb3c4a9fbae2a868590", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-12T17:38:02Z", "type": "commit"}, {"oid": "8979388bc0e800adb102dfb3c4a9fbae2a868590", "url": "https://github.com/kiegroup/droolsjbpm-integration/commit/8979388bc0e800adb102dfb3c4a9fbae2a868590", "message": "[JBPM-9436] AMQ Streams (Kafka) listener functionality / consumer\n\nAdding Kafka extension", "committedDate": "2020-11-12T17:38:02Z", "type": "forcePushed"}]}