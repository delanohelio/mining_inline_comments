{"pr_number": 666, "pr_title": "Warp 10 Accelerator", "pr_createdAt": "2020-02-14T15:20:31Z", "pr_url": "https://github.com/senx/warp10-platform/pull/666", "timeline": [{"oid": "8f5eff53eb9f5d20c10a5888ae4a104cbdacc981", "url": "https://github.com/senx/warp10-platform/commit/8f5eff53eb9f5d20c10a5888ae4a104cbdacc981", "message": "Initial commit of Warp 10 Accelerator", "committedDate": "2020-02-13T13:56:44Z", "type": "commit"}, {"oid": "f689ff02507d8510cf33d73893464a8409f055f8", "url": "https://github.com/senx/warp10-platform/commit/f689ff02507d8510cf33d73893464a8409f055f8", "message": "Minor fixes for ephemeral mode", "committedDate": "2020-02-14T15:19:33Z", "type": "commit"}, {"oid": "d9d0d5c0025d4372d21ae9cf55b4693f905c08ab", "url": "https://github.com/senx/warp10-platform/commit/d9d0d5c0025d4372d21ae9cf55b4693f905c08ab", "message": "Added support for ephemeral cache. Corrected end timestamp", "committedDate": "2020-02-14T15:19:59Z", "type": "commit"}, {"oid": "803d927c1ec456def5de4f9d1fdd070690aa6b08", "url": "https://github.com/senx/warp10-platform/commit/803d927c1ec456def5de4f9d1fdd070690aa6b08", "message": "Fixed gcperiod wait", "committedDate": "2020-02-19T16:14:49Z", "type": "commit"}, {"oid": "1af764344bbb72df775ba764fb5c6dddc5897940", "url": "https://github.com/senx/warp10-platform/commit/1af764344bbb72df775ba764fb5c6dddc5897940", "message": "Corrected count parameter", "committedDate": "2020-02-19T16:15:37Z", "type": "commit"}, {"oid": "a0ecf5f2d20bec7df69563a15a8e7cd8716542ac", "url": "https://github.com/senx/warp10-platform/commit/a0ecf5f2d20bec7df69563a15a8e7cd8716542ac", "message": "Added support for nocache/nopersist configs to tune Accelerator usage", "committedDate": "2020-02-24T16:37:00Z", "type": "commit"}, {"oid": "c08dc126ef709435fb20476836555f8f2ddf6214", "url": "https://github.com/senx/warp10-platform/commit/c08dc126ef709435fb20476836555f8f2ddf6214", "message": "Added handling of nocache/nopersist in fetch", "committedDate": "2020-02-24T16:53:25Z", "type": "commit"}, {"oid": "487e3ab10be53bcdc38614e528c007190c74e8a4", "url": "https://github.com/senx/warp10-platform/commit/487e3ab10be53bcdc38614e528c007190c74e8a4", "message": "Merge branch 'master' into inmem-accelerator", "committedDate": "2020-03-02T11:29:50Z", "type": "commit"}, {"oid": "7ac05bcccdd85e81ed01d7a840fe3537e913008c", "url": "https://github.com/senx/warp10-platform/commit/7ac05bcccdd85e81ed01d7a840fe3537e913008c", "message": "Added support for specifying data store (cache/persist) in FETCH and reporting of cache usage post FETCH.", "committedDate": "2020-03-02T11:48:16Z", "type": "commit"}, {"oid": "be4ca0a988393d4f9ffbfa9b3baa9533d2c5a8e9", "url": "https://github.com/senx/warp10-platform/commit/be4ca0a988393d4f9ffbfa9b3baa9533d2c5a8e9", "message": "Added support for specifying data store (cache/persist) in FETCH and reporting of cache usage post FETCH.", "committedDate": "2020-03-02T11:48:40Z", "type": "commit"}, {"oid": "0cc5249f912650f67b1605f475e90937754ed0c9", "url": "https://github.com/senx/warp10-platform/commit/0cc5249f912650f67b1605f475e90937754ed0c9", "message": "Added support for nocache/nopersist configuration in /api/v0/fetch", "committedDate": "2020-03-02T12:42:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTE2Nzc0MQ==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r385167741", "bodyText": "I see some >= for chunkstart checking (line 575 for instance), can you double-check if it's really chunkends[i] > end + chunklen?", "author": "ftence", "createdAt": "2020-02-27T14:57:01Z", "path": "warp10/src/main/java/io/warp10/standalone/InMemoryChunkSet.java", "diffHunk": "@@ -839,4 +854,41 @@ long optimize(CapacityExtractorOutputStream out, long now, AtomicLong allocation\n     \n     return reclaimed;\n   }\n+  \n+  public long delete(long start, long end) {\n+    long count = 0L;\n+    \n+    for (int i = 0; i < chunks.length; i++) {\n+      if (!this.ephemeral && (chunkends[i] < start || chunkends[i] > end + chunklen)) {", "originalCommit": "c08dc126ef709435fb20476836555f8f2ddf6214", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzAzODIxMQ==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r387038211", "bodyText": "chunkends[] keeps track of the last timestamp in a chunk. Each chunk is chunklen time units wide, so we are checking here if the ith chunk ends more than chunklen timeunits after end, making the whole chunk past 'end'", "author": "hbs", "createdAt": "2020-03-03T14:02:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTE2Nzc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ2ODEyMg==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r396468122", "bodyText": "I'm pretty sure it should be chunkends[i] >= end + chunklen because:\nchunkends[i] > end + chunklen\n= chunkends[i] - chunklen > end\n= chunkends[i - 1] > end\n= chunkstarts[i] - 1 > end (if chunkstarts was defined, first included tick for each chunk)\nThus, if end == chunkstarts[i] - 1, the i-th chunk will be considered although it does not contain end.", "author": "ftence", "createdAt": "2020-03-23T13:55:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTE2Nzc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjU1Njc2Ng==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r396556766", "bodyText": "indeed", "author": "hbs", "createdAt": "2020-03-23T15:52:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTE2Nzc0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTE3MDI4OQ==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r385170289", "bodyText": "Can you add a comment to tell if both start and end are included?", "author": "ftence", "createdAt": "2020-02-27T15:01:02Z", "path": "warp10/src/main/java/io/warp10/standalone/InMemoryChunkSet.java", "diffHunk": "@@ -839,4 +854,41 @@ long optimize(CapacityExtractorOutputStream out, long now, AtomicLong allocation\n     \n     return reclaimed;\n   }\n+  \n+  public long delete(long start, long end) {", "originalCommit": "c08dc126ef709435fb20476836555f8f2ddf6214", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjM2MTk2Ng==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r386361966", "bodyText": "As Jetty will use a ThreadPool to manage the handlers threads, thus the threads running the WarpScript, the values will be kept form previous runs. Not sure of this, but seems likely to me.", "author": "ftence", "createdAt": "2020-03-02T12:24:14Z", "path": "warp10/src/main/java/io/warp10/standalone/StandaloneAcceleratedStoreClient.java", "diffHunk": "@@ -0,0 +1,298 @@\n+package io.warp10.standalone;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.LockSupport;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import io.warp10.WarpConfig;\n+import io.warp10.continuum.Configuration;\n+import io.warp10.continuum.TimeSource;\n+import io.warp10.continuum.gts.GTSDecoder;\n+import io.warp10.continuum.gts.GTSEncoder;\n+import io.warp10.continuum.store.Constants;\n+import io.warp10.continuum.store.DirectoryClient;\n+import io.warp10.continuum.store.GTSDecoderIterator;\n+import io.warp10.continuum.store.MetadataIterator;\n+import io.warp10.continuum.store.StoreClient;\n+import io.warp10.continuum.store.thrift.data.DirectoryRequest;\n+import io.warp10.continuum.store.thrift.data.Metadata;\n+import io.warp10.quasar.token.thrift.data.ReadToken;\n+import io.warp10.quasar.token.thrift.data.WriteToken;\n+\n+public class StandaloneAcceleratedStoreClient implements StoreClient {\n+  \n+  private static final Logger LOG = LoggerFactory.getLogger(StandaloneAcceleratedStoreClient.class);\n+  \n+  private final StoreClient persistent;\n+  private final StandaloneChunkedMemoryStore cache;\n+  private final boolean ephemeral;\n+\n+  public static final String ATTR_REPORT = \"accel.report\";\n+  public static final String ATTR_NOCACHE = \"accel.nocache\";\n+  public static final String ATTR_NOPERSIST = \"accel.nopersist\";\n+  \n+  public static final String NOCACHE = \"nocache\";\n+  public static final String NOPERSIST = \"nopersist\";\n+  \n+  public static final String ACCELERATOR_HEADER = \"X-Warp10-Accelerator\";\n+  \n+  /**\n+   * Was the last FETCH accelerated for the given Thread?\n+   */\n+  private static final ThreadLocal<Boolean> accelerated = new ThreadLocal<Boolean>() {\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    };\n+  };\n+  \n+  private static final ThreadLocal<Boolean> nocache = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final ThreadLocal<Boolean> nopersist = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };", "originalCommit": "be4ca0a988393d4f9ffbfa9b3baa9533d2c5a8e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzA1NDgwMA==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r387054800", "bodyText": "Indeed, that is why both FETCH and EgressFetchHandler call either cache/nocache and persist/nopersist prior to calling fetch. The only case which should be a concern is when the StoreClient is exposed via 'egress.clients.expose' set to true, as the use of the StoreClient should be preceded by the same logic of calling cache/nocache/persist/nopersist.\nAdding a warning in the configuration.", "author": "hbs", "createdAt": "2020-03-03T14:28:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjM2MTk2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjM2NTk4MA==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r386365980", "bodyText": "Build the String once.", "author": "ftence", "createdAt": "2020-03-02T12:33:51Z", "path": "warp10/src/main/java/io/warp10/standalone/StandaloneAcceleratedStoreClient.java", "diffHunk": "@@ -0,0 +1,298 @@\n+package io.warp10.standalone;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.LockSupport;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import io.warp10.WarpConfig;\n+import io.warp10.continuum.Configuration;\n+import io.warp10.continuum.TimeSource;\n+import io.warp10.continuum.gts.GTSDecoder;\n+import io.warp10.continuum.gts.GTSEncoder;\n+import io.warp10.continuum.store.Constants;\n+import io.warp10.continuum.store.DirectoryClient;\n+import io.warp10.continuum.store.GTSDecoderIterator;\n+import io.warp10.continuum.store.MetadataIterator;\n+import io.warp10.continuum.store.StoreClient;\n+import io.warp10.continuum.store.thrift.data.DirectoryRequest;\n+import io.warp10.continuum.store.thrift.data.Metadata;\n+import io.warp10.quasar.token.thrift.data.ReadToken;\n+import io.warp10.quasar.token.thrift.data.WriteToken;\n+\n+public class StandaloneAcceleratedStoreClient implements StoreClient {\n+  \n+  private static final Logger LOG = LoggerFactory.getLogger(StandaloneAcceleratedStoreClient.class);\n+  \n+  private final StoreClient persistent;\n+  private final StandaloneChunkedMemoryStore cache;\n+  private final boolean ephemeral;\n+\n+  public static final String ATTR_REPORT = \"accel.report\";\n+  public static final String ATTR_NOCACHE = \"accel.nocache\";\n+  public static final String ATTR_NOPERSIST = \"accel.nopersist\";\n+  \n+  public static final String NOCACHE = \"nocache\";\n+  public static final String NOPERSIST = \"nopersist\";\n+  \n+  public static final String ACCELERATOR_HEADER = \"X-Warp10-Accelerator\";\n+  \n+  /**\n+   * Was the last FETCH accelerated for the given Thread?\n+   */\n+  private static final ThreadLocal<Boolean> accelerated = new ThreadLocal<Boolean>() {\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    };\n+  };\n+  \n+  private static final ThreadLocal<Boolean> nocache = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final ThreadLocal<Boolean> nopersist = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final AtomicBoolean instantiated = new AtomicBoolean(false);\n+  \n+  public StandaloneAcceleratedStoreClient(DirectoryClient dir, StoreClient persistentStore) {\n+    \n+    instantiated.set(true);\n+    \n+    this.persistent = persistentStore;\n+    this.cache = new StandaloneChunkedMemoryStore(WarpConfig.getProperties(), Warp.getKeyStore());\n+\n+    this.ephemeral = \"true\".equals(WarpConfig.getProperty(Configuration.IN_MEMORY_EPHEMERAL)); \n+    \n+    //\n+    // Preload the cache\n+    //\n+    \n+    long nanos = System.nanoTime();\n+    \n+    DirectoryRequest request = new DirectoryRequest();\n+    request.addToClassSelectors(\"~.*\");\n+    Map<String,String> labelselectors = new HashMap<String,String>();\n+    labelselectors.put(Constants.APPLICATION_LABEL, \"~.*\");\n+    labelselectors.put(Constants.PRODUCER_LABEL, \"~.*\");\n+    labelselectors.put(Constants.OWNER_LABEL, \"~.*\");\n+    request.addToLabelsSelectors(labelselectors);\n+    \n+    long end;\n+    long start;\n+    long n = -1L;\n+    \n+    if (this.ephemeral) {\n+      end = Long.MAX_VALUE;\n+      start = Long.MIN_VALUE;\n+      n = 1L;\n+    } else {\n+      end = InMemoryChunkSet.chunkEnd(TimeSource.getTime(), this.cache.getChunkSpan());\n+      start = end - this.cache.getChunkCount() * this.cache.getChunkSpan() + 1;\n+      n = -1L;\n+    }\n+    \n+    final long now = end;\n+    final long then = start;\n+    final long count = n;\n+    \n+    if (\"true\".equals(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_ACTIVITY))) {\n+      long activityWindow = Long.parseLong(WarpConfig.getProperty(Configuration.INGRESS_ACTIVITY_WINDOW, \"-1\"));\n+      if (activityWindow > 0) {\n+        request.setActiveAfter(then / Constants.TIME_UNITS_PER_MS - activityWindow - 1L);\n+      }\n+    }\n+    \n+    try {\n+      final AtomicLong datapoints = new AtomicLong();\n+      \n+      MetadataIterator iter = dir.iterator(request);\n+      \n+      int BATCH_SIZE = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_BATCHSIZE, \"1000\"));\n+      List<Metadata> batch = new ArrayList<Metadata>(BATCH_SIZE);\n+            \n+      int nthreads = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_POOLSIZE, \"8\"));\n+      \n+      ThreadPoolExecutor exec = new ThreadPoolExecutor(nthreads, nthreads, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(nthreads));\n+      final AtomicReference<Throwable> error = new AtomicReference<Throwable>();\n+      \n+      while(iter.hasNext()) {\n+        batch.add(iter.next());\n+        \n+        if (null != error.get()) {\n+          throw new RuntimeException(\"Error populating the accelerator\", error.get());\n+        }\n+        \n+        if (BATCH_SIZE == batch.size()) {\n+          \n+          final List<Metadata> fbatch = batch;\n+          \n+          Runnable runnable = new Runnable() {            \n+            @Override\n+            public void run() {\n+              try {\n+                GTSDecoderIterator decoders = persistent.fetch(null, fbatch, now, then, count, 0, 1.0D, false, 0, 0);\n+                \n+                while(decoders.hasNext()) {\n+                  GTSDecoder decoder = decoders.next();\n+                  decoder.next();\n+                  GTSEncoder encoder = decoder.getEncoder(true);\n+                  cache.store(encoder);\n+                  datapoints.addAndGet(decoder.getCount());\n+                }                              \n+              } catch (Exception e) {\n+                error.set(e);\n+                throw new RuntimeException(e);\n+              }\n+            }\n+          };\n+          \n+          boolean submitted = false;\n+          while(!submitted) {\n+            try {\n+              exec.execute(runnable);\n+              submitted = true;\n+            } catch (RejectedExecutionException re) {\n+              LockSupport.parkNanos(100000000L);\n+            }\n+          }\n+          \n+          batch = new ArrayList<Metadata>(BATCH_SIZE);\n+        }\n+      }\n+\n+      if (!batch.isEmpty()) {          \n+        GTSDecoderIterator decoders = this.persistent.fetch(null, batch, now, then, count, 0, 0.0, false, 0, 0);\n+        \n+        while(decoders.hasNext()) {\n+          GTSDecoder decoder = decoders.next();\n+          decoder.next();\n+          GTSEncoder encoder = decoder.getEncoder(true);\n+          this.cache.store(encoder);\n+          datapoints.addAndGet(decoder.getCount());\n+        }\n+      }\n+\n+      exec.shutdown();\n+      \n+      System.out.println(\"Preloaded accelerator with \" + datapoints + \" datapoints from \" + this.cache.getGTSCount() + \" Geo Time Series in \" + ((System.nanoTime() - nanos) / 1000000.0D) + \" ms.\");\n+      LOG.info(\"Preloaded accelerator with \" + datapoints + \" datapoints from \" + this.cache.getGTSCount() + \" Geo Time Series in \" + ((System.nanoTime() - nanos) / 1000000.0D) + \" ms.\");", "originalCommit": "be4ca0a988393d4f9ffbfa9b3baa9533d2c5a8e9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjM3NjE3OQ==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r386376179", "bodyText": "Shouldn't cacheend be computed using TimeSource.getTime() instead of now?", "author": "ftence", "createdAt": "2020-03-02T12:56:44Z", "path": "warp10/src/main/java/io/warp10/standalone/StandaloneAcceleratedStoreClient.java", "diffHunk": "@@ -0,0 +1,298 @@\n+package io.warp10.standalone;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.LockSupport;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import io.warp10.WarpConfig;\n+import io.warp10.continuum.Configuration;\n+import io.warp10.continuum.TimeSource;\n+import io.warp10.continuum.gts.GTSDecoder;\n+import io.warp10.continuum.gts.GTSEncoder;\n+import io.warp10.continuum.store.Constants;\n+import io.warp10.continuum.store.DirectoryClient;\n+import io.warp10.continuum.store.GTSDecoderIterator;\n+import io.warp10.continuum.store.MetadataIterator;\n+import io.warp10.continuum.store.StoreClient;\n+import io.warp10.continuum.store.thrift.data.DirectoryRequest;\n+import io.warp10.continuum.store.thrift.data.Metadata;\n+import io.warp10.quasar.token.thrift.data.ReadToken;\n+import io.warp10.quasar.token.thrift.data.WriteToken;\n+\n+public class StandaloneAcceleratedStoreClient implements StoreClient {\n+  \n+  private static final Logger LOG = LoggerFactory.getLogger(StandaloneAcceleratedStoreClient.class);\n+  \n+  private final StoreClient persistent;\n+  private final StandaloneChunkedMemoryStore cache;\n+  private final boolean ephemeral;\n+\n+  public static final String ATTR_REPORT = \"accel.report\";\n+  public static final String ATTR_NOCACHE = \"accel.nocache\";\n+  public static final String ATTR_NOPERSIST = \"accel.nopersist\";\n+  \n+  public static final String NOCACHE = \"nocache\";\n+  public static final String NOPERSIST = \"nopersist\";\n+  \n+  public static final String ACCELERATOR_HEADER = \"X-Warp10-Accelerator\";\n+  \n+  /**\n+   * Was the last FETCH accelerated for the given Thread?\n+   */\n+  private static final ThreadLocal<Boolean> accelerated = new ThreadLocal<Boolean>() {\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    };\n+  };\n+  \n+  private static final ThreadLocal<Boolean> nocache = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final ThreadLocal<Boolean> nopersist = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final AtomicBoolean instantiated = new AtomicBoolean(false);\n+  \n+  public StandaloneAcceleratedStoreClient(DirectoryClient dir, StoreClient persistentStore) {\n+    \n+    instantiated.set(true);\n+    \n+    this.persistent = persistentStore;\n+    this.cache = new StandaloneChunkedMemoryStore(WarpConfig.getProperties(), Warp.getKeyStore());\n+\n+    this.ephemeral = \"true\".equals(WarpConfig.getProperty(Configuration.IN_MEMORY_EPHEMERAL)); \n+    \n+    //\n+    // Preload the cache\n+    //\n+    \n+    long nanos = System.nanoTime();\n+    \n+    DirectoryRequest request = new DirectoryRequest();\n+    request.addToClassSelectors(\"~.*\");\n+    Map<String,String> labelselectors = new HashMap<String,String>();\n+    labelselectors.put(Constants.APPLICATION_LABEL, \"~.*\");\n+    labelselectors.put(Constants.PRODUCER_LABEL, \"~.*\");\n+    labelselectors.put(Constants.OWNER_LABEL, \"~.*\");\n+    request.addToLabelsSelectors(labelselectors);\n+    \n+    long end;\n+    long start;\n+    long n = -1L;\n+    \n+    if (this.ephemeral) {\n+      end = Long.MAX_VALUE;\n+      start = Long.MIN_VALUE;\n+      n = 1L;\n+    } else {\n+      end = InMemoryChunkSet.chunkEnd(TimeSource.getTime(), this.cache.getChunkSpan());\n+      start = end - this.cache.getChunkCount() * this.cache.getChunkSpan() + 1;\n+      n = -1L;\n+    }\n+    \n+    final long now = end;\n+    final long then = start;\n+    final long count = n;\n+    \n+    if (\"true\".equals(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_ACTIVITY))) {\n+      long activityWindow = Long.parseLong(WarpConfig.getProperty(Configuration.INGRESS_ACTIVITY_WINDOW, \"-1\"));\n+      if (activityWindow > 0) {\n+        request.setActiveAfter(then / Constants.TIME_UNITS_PER_MS - activityWindow - 1L);\n+      }\n+    }\n+    \n+    try {\n+      final AtomicLong datapoints = new AtomicLong();\n+      \n+      MetadataIterator iter = dir.iterator(request);\n+      \n+      int BATCH_SIZE = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_BATCHSIZE, \"1000\"));\n+      List<Metadata> batch = new ArrayList<Metadata>(BATCH_SIZE);\n+            \n+      int nthreads = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_POOLSIZE, \"8\"));\n+      \n+      ThreadPoolExecutor exec = new ThreadPoolExecutor(nthreads, nthreads, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(nthreads));\n+      final AtomicReference<Throwable> error = new AtomicReference<Throwable>();\n+      \n+      while(iter.hasNext()) {\n+        batch.add(iter.next());\n+        \n+        if (null != error.get()) {\n+          throw new RuntimeException(\"Error populating the accelerator\", error.get());\n+        }\n+        \n+        if (BATCH_SIZE == batch.size()) {\n+          \n+          final List<Metadata> fbatch = batch;\n+          \n+          Runnable runnable = new Runnable() {            \n+            @Override\n+            public void run() {\n+              try {\n+                GTSDecoderIterator decoders = persistent.fetch(null, fbatch, now, then, count, 0, 1.0D, false, 0, 0);\n+                \n+                while(decoders.hasNext()) {\n+                  GTSDecoder decoder = decoders.next();\n+                  decoder.next();\n+                  GTSEncoder encoder = decoder.getEncoder(true);\n+                  cache.store(encoder);\n+                  datapoints.addAndGet(decoder.getCount());\n+                }                              \n+              } catch (Exception e) {\n+                error.set(e);\n+                throw new RuntimeException(e);\n+              }\n+            }\n+          };\n+          \n+          boolean submitted = false;\n+          while(!submitted) {\n+            try {\n+              exec.execute(runnable);\n+              submitted = true;\n+            } catch (RejectedExecutionException re) {\n+              LockSupport.parkNanos(100000000L);\n+            }\n+          }\n+          \n+          batch = new ArrayList<Metadata>(BATCH_SIZE);\n+        }\n+      }\n+\n+      if (!batch.isEmpty()) {          \n+        GTSDecoderIterator decoders = this.persistent.fetch(null, batch, now, then, count, 0, 0.0, false, 0, 0);\n+        \n+        while(decoders.hasNext()) {\n+          GTSDecoder decoder = decoders.next();\n+          decoder.next();\n+          GTSEncoder encoder = decoder.getEncoder(true);\n+          this.cache.store(encoder);\n+          datapoints.addAndGet(decoder.getCount());\n+        }\n+      }\n+\n+      exec.shutdown();\n+      \n+      System.out.println(\"Preloaded accelerator with \" + datapoints + \" datapoints from \" + this.cache.getGTSCount() + \" Geo Time Series in \" + ((System.nanoTime() - nanos) / 1000000.0D) + \" ms.\");\n+      LOG.info(\"Preloaded accelerator with \" + datapoints + \" datapoints from \" + this.cache.getGTSCount() + \" Geo Time Series in \" + ((System.nanoTime() - nanos) / 1000000.0D) + \" ms.\");\n+    } catch (IOException ioe) {\n+      throw new RuntimeException(\"Error populating cache.\", ioe);\n+    }\n+  }\n+  \n+  @Override\n+  public void addPlasmaHandler(StandalonePlasmaHandlerInterface handler) {\n+    this.persistent.addPlasmaHandler(handler);\n+  }\n+  \n+  @Override\n+  public long delete(WriteToken token, Metadata metadata, long start, long end) throws IOException {\n+    cache.delete(token, metadata, start, end);\n+    persistent.delete(token, metadata, start, end);\n+    return 0;\n+  }\n+  \n+  @Override\n+  public GTSDecoderIterator fetch(ReadToken token, List<Metadata> metadatas, long now, long then, long count, long skip, double sample, boolean writeTimestamp, int preBoundary, int postBoundary) throws IOException {\n+    //\n+    // If the fetch has both a time range that is larger than the cache range, we will only use\n+    // the persistent backend to ensure a correct fetch. Same goes with boundaries which could extend outside the\n+    // cache.\n+    //\n+    // Note that this is a heuristic which could still lead to missing datapoints as data with timestamps within\n+    // the current cache time range could very well have been written to the persistent store and not preloaded\n+    // at cache startup if they were not in an active chunk of the cache.\n+    //\n+    \n+    long cacheend = InMemoryChunkSet.chunkEnd(now, this.cache.getChunkSpan());", "originalCommit": "0cc5249f912650f67b1605f475e90937754ed0c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjM4NzQ1Ng==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r386387456", "bodyText": "Is there a reason for not fetching in both persist and cache by spiting the time window according to cachestart and cacheend?", "author": "ftence", "createdAt": "2020-03-02T13:20:50Z", "path": "warp10/src/main/java/io/warp10/standalone/StandaloneAcceleratedStoreClient.java", "diffHunk": "@@ -0,0 +1,298 @@\n+package io.warp10.standalone;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.LockSupport;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import io.warp10.WarpConfig;\n+import io.warp10.continuum.Configuration;\n+import io.warp10.continuum.TimeSource;\n+import io.warp10.continuum.gts.GTSDecoder;\n+import io.warp10.continuum.gts.GTSEncoder;\n+import io.warp10.continuum.store.Constants;\n+import io.warp10.continuum.store.DirectoryClient;\n+import io.warp10.continuum.store.GTSDecoderIterator;\n+import io.warp10.continuum.store.MetadataIterator;\n+import io.warp10.continuum.store.StoreClient;\n+import io.warp10.continuum.store.thrift.data.DirectoryRequest;\n+import io.warp10.continuum.store.thrift.data.Metadata;\n+import io.warp10.quasar.token.thrift.data.ReadToken;\n+import io.warp10.quasar.token.thrift.data.WriteToken;\n+\n+public class StandaloneAcceleratedStoreClient implements StoreClient {\n+  \n+  private static final Logger LOG = LoggerFactory.getLogger(StandaloneAcceleratedStoreClient.class);\n+  \n+  private final StoreClient persistent;\n+  private final StandaloneChunkedMemoryStore cache;\n+  private final boolean ephemeral;\n+\n+  public static final String ATTR_REPORT = \"accel.report\";\n+  public static final String ATTR_NOCACHE = \"accel.nocache\";\n+  public static final String ATTR_NOPERSIST = \"accel.nopersist\";\n+  \n+  public static final String NOCACHE = \"nocache\";\n+  public static final String NOPERSIST = \"nopersist\";\n+  \n+  public static final String ACCELERATOR_HEADER = \"X-Warp10-Accelerator\";\n+  \n+  /**\n+   * Was the last FETCH accelerated for the given Thread?\n+   */\n+  private static final ThreadLocal<Boolean> accelerated = new ThreadLocal<Boolean>() {\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    };\n+  };\n+  \n+  private static final ThreadLocal<Boolean> nocache = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final ThreadLocal<Boolean> nopersist = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final AtomicBoolean instantiated = new AtomicBoolean(false);\n+  \n+  public StandaloneAcceleratedStoreClient(DirectoryClient dir, StoreClient persistentStore) {\n+    \n+    instantiated.set(true);\n+    \n+    this.persistent = persistentStore;\n+    this.cache = new StandaloneChunkedMemoryStore(WarpConfig.getProperties(), Warp.getKeyStore());\n+\n+    this.ephemeral = \"true\".equals(WarpConfig.getProperty(Configuration.IN_MEMORY_EPHEMERAL)); \n+    \n+    //\n+    // Preload the cache\n+    //\n+    \n+    long nanos = System.nanoTime();\n+    \n+    DirectoryRequest request = new DirectoryRequest();\n+    request.addToClassSelectors(\"~.*\");\n+    Map<String,String> labelselectors = new HashMap<String,String>();\n+    labelselectors.put(Constants.APPLICATION_LABEL, \"~.*\");\n+    labelselectors.put(Constants.PRODUCER_LABEL, \"~.*\");\n+    labelselectors.put(Constants.OWNER_LABEL, \"~.*\");\n+    request.addToLabelsSelectors(labelselectors);\n+    \n+    long end;\n+    long start;\n+    long n = -1L;\n+    \n+    if (this.ephemeral) {\n+      end = Long.MAX_VALUE;\n+      start = Long.MIN_VALUE;\n+      n = 1L;\n+    } else {\n+      end = InMemoryChunkSet.chunkEnd(TimeSource.getTime(), this.cache.getChunkSpan());\n+      start = end - this.cache.getChunkCount() * this.cache.getChunkSpan() + 1;\n+      n = -1L;\n+    }\n+    \n+    final long now = end;\n+    final long then = start;\n+    final long count = n;\n+    \n+    if (\"true\".equals(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_ACTIVITY))) {\n+      long activityWindow = Long.parseLong(WarpConfig.getProperty(Configuration.INGRESS_ACTIVITY_WINDOW, \"-1\"));\n+      if (activityWindow > 0) {\n+        request.setActiveAfter(then / Constants.TIME_UNITS_PER_MS - activityWindow - 1L);\n+      }\n+    }\n+    \n+    try {\n+      final AtomicLong datapoints = new AtomicLong();\n+      \n+      MetadataIterator iter = dir.iterator(request);\n+      \n+      int BATCH_SIZE = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_BATCHSIZE, \"1000\"));\n+      List<Metadata> batch = new ArrayList<Metadata>(BATCH_SIZE);\n+            \n+      int nthreads = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_POOLSIZE, \"8\"));\n+      \n+      ThreadPoolExecutor exec = new ThreadPoolExecutor(nthreads, nthreads, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(nthreads));\n+      final AtomicReference<Throwable> error = new AtomicReference<Throwable>();\n+      \n+      while(iter.hasNext()) {\n+        batch.add(iter.next());\n+        \n+        if (null != error.get()) {\n+          throw new RuntimeException(\"Error populating the accelerator\", error.get());\n+        }\n+        \n+        if (BATCH_SIZE == batch.size()) {\n+          \n+          final List<Metadata> fbatch = batch;\n+          \n+          Runnable runnable = new Runnable() {            \n+            @Override\n+            public void run() {\n+              try {\n+                GTSDecoderIterator decoders = persistent.fetch(null, fbatch, now, then, count, 0, 1.0D, false, 0, 0);\n+                \n+                while(decoders.hasNext()) {\n+                  GTSDecoder decoder = decoders.next();\n+                  decoder.next();\n+                  GTSEncoder encoder = decoder.getEncoder(true);\n+                  cache.store(encoder);\n+                  datapoints.addAndGet(decoder.getCount());\n+                }                              \n+              } catch (Exception e) {\n+                error.set(e);\n+                throw new RuntimeException(e);\n+              }\n+            }\n+          };\n+          \n+          boolean submitted = false;\n+          while(!submitted) {\n+            try {\n+              exec.execute(runnable);\n+              submitted = true;\n+            } catch (RejectedExecutionException re) {\n+              LockSupport.parkNanos(100000000L);\n+            }\n+          }\n+          \n+          batch = new ArrayList<Metadata>(BATCH_SIZE);\n+        }\n+      }\n+\n+      if (!batch.isEmpty()) {          \n+        GTSDecoderIterator decoders = this.persistent.fetch(null, batch, now, then, count, 0, 0.0, false, 0, 0);\n+        \n+        while(decoders.hasNext()) {\n+          GTSDecoder decoder = decoders.next();\n+          decoder.next();\n+          GTSEncoder encoder = decoder.getEncoder(true);\n+          this.cache.store(encoder);\n+          datapoints.addAndGet(decoder.getCount());\n+        }\n+      }\n+\n+      exec.shutdown();\n+      \n+      System.out.println(\"Preloaded accelerator with \" + datapoints + \" datapoints from \" + this.cache.getGTSCount() + \" Geo Time Series in \" + ((System.nanoTime() - nanos) / 1000000.0D) + \" ms.\");\n+      LOG.info(\"Preloaded accelerator with \" + datapoints + \" datapoints from \" + this.cache.getGTSCount() + \" Geo Time Series in \" + ((System.nanoTime() - nanos) / 1000000.0D) + \" ms.\");\n+    } catch (IOException ioe) {\n+      throw new RuntimeException(\"Error populating cache.\", ioe);\n+    }\n+  }\n+  \n+  @Override\n+  public void addPlasmaHandler(StandalonePlasmaHandlerInterface handler) {\n+    this.persistent.addPlasmaHandler(handler);\n+  }\n+  \n+  @Override\n+  public long delete(WriteToken token, Metadata metadata, long start, long end) throws IOException {\n+    cache.delete(token, metadata, start, end);\n+    persistent.delete(token, metadata, start, end);\n+    return 0;\n+  }\n+  \n+  @Override\n+  public GTSDecoderIterator fetch(ReadToken token, List<Metadata> metadatas, long now, long then, long count, long skip, double sample, boolean writeTimestamp, int preBoundary, int postBoundary) throws IOException {\n+    //\n+    // If the fetch has both a time range that is larger than the cache range, we will only use\n+    // the persistent backend to ensure a correct fetch. Same goes with boundaries which could extend outside the\n+    // cache.\n+    //\n+    // Note that this is a heuristic which could still lead to missing datapoints as data with timestamps within\n+    // the current cache time range could very well have been written to the persistent store and not preloaded\n+    // at cache startup if they were not in an active chunk of the cache.\n+    //\n+    \n+    long cacheend = InMemoryChunkSet.chunkEnd(now, this.cache.getChunkSpan());\n+    long cachestart = cacheend - this.cache.getChunkCount() * this.cache.getChunkSpan() + 1;\n+\n+    //\n+    // If fetching a single value from Long.MAX_VALUE with an ephemeral cache, always use the cache\n+    // unless ACCEL.NOCACHE was called.\n+    //\n+    if (this.ephemeral && 1 == count && Long.MAX_VALUE == now && !nocache.get()) {\n+      accelerated.set(Boolean.TRUE);\n+      return this.cache.fetch(token, metadatas, now, then, count, skip, sample, writeTimestamp, preBoundary, postBoundary);      \n+    }\n+    \n+    // Use the persistent store unless ACCEL.NOPERSIST was called \n+    if (((now > cacheend || then < cachestart) || preBoundary > 0 || postBoundary > 0 || nocache.get()) && !nopersist.get()) {\n+      accelerated.set(Boolean.FALSE);\n+      return this.persistent.fetch(token, metadatas, now, then, count, skip, sample, writeTimestamp, preBoundary, postBoundary);\n+    }\n+    \n+    // Last resort, use the cache\n+    accelerated.set(Boolean.TRUE);\n+    return this.cache.fetch(token, metadatas, now, then, count, skip, sample, writeTimestamp, preBoundary, postBoundary);", "originalCommit": "0cc5249f912650f67b1605f475e90937754ed0c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjM4ODgwOA==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r386388808", "bodyText": "Empty if.", "author": "ftence", "createdAt": "2020-03-02T13:23:35Z", "path": "warp10/src/main/java/io/warp10/standalone/StandaloneChunkedMemoryStore.java", "diffHunk": "@@ -451,7 +459,7 @@ public void run() {\n   @Override\n   public long delete(WriteToken token, Metadata metadata, long start, long end) throws IOException {\n     if (Long.MIN_VALUE != start || Long.MAX_VALUE != end) {\n-      throw new IOException(\"MemoryStore only supports deleting complete Geo Time Series.\");\n+      //throw new IOException(\"MemoryStore only supports deleting complete Geo Time Series.\");", "originalCommit": "0cc5249f912650f67b1605f475e90937754ed0c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njg4ODAwOQ==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r386888009", "bodyText": "Should the result be deduped to have consistent result between cache and persist?\nOr modify the cache so it does not store duplicate ticks.\nAlso the result will not be sorted using the cache but will be using persist. I don't think this is as problematic as the duplicate ticks is.", "author": "ftence", "createdAt": "2020-03-03T09:19:54Z", "path": "warp10/src/main/java/io/warp10/standalone/StandaloneAcceleratedStoreClient.java", "diffHunk": "@@ -0,0 +1,298 @@\n+package io.warp10.standalone;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.LockSupport;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import io.warp10.WarpConfig;\n+import io.warp10.continuum.Configuration;\n+import io.warp10.continuum.TimeSource;\n+import io.warp10.continuum.gts.GTSDecoder;\n+import io.warp10.continuum.gts.GTSEncoder;\n+import io.warp10.continuum.store.Constants;\n+import io.warp10.continuum.store.DirectoryClient;\n+import io.warp10.continuum.store.GTSDecoderIterator;\n+import io.warp10.continuum.store.MetadataIterator;\n+import io.warp10.continuum.store.StoreClient;\n+import io.warp10.continuum.store.thrift.data.DirectoryRequest;\n+import io.warp10.continuum.store.thrift.data.Metadata;\n+import io.warp10.quasar.token.thrift.data.ReadToken;\n+import io.warp10.quasar.token.thrift.data.WriteToken;\n+\n+public class StandaloneAcceleratedStoreClient implements StoreClient {\n+  \n+  private static final Logger LOG = LoggerFactory.getLogger(StandaloneAcceleratedStoreClient.class);\n+  \n+  private final StoreClient persistent;\n+  private final StandaloneChunkedMemoryStore cache;\n+  private final boolean ephemeral;\n+\n+  public static final String ATTR_REPORT = \"accel.report\";\n+  public static final String ATTR_NOCACHE = \"accel.nocache\";\n+  public static final String ATTR_NOPERSIST = \"accel.nopersist\";\n+  \n+  public static final String NOCACHE = \"nocache\";\n+  public static final String NOPERSIST = \"nopersist\";\n+  \n+  public static final String ACCELERATOR_HEADER = \"X-Warp10-Accelerator\";\n+  \n+  /**\n+   * Was the last FETCH accelerated for the given Thread?\n+   */\n+  private static final ThreadLocal<Boolean> accelerated = new ThreadLocal<Boolean>() {\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    };\n+  };\n+  \n+  private static final ThreadLocal<Boolean> nocache = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final ThreadLocal<Boolean> nopersist = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final AtomicBoolean instantiated = new AtomicBoolean(false);\n+  \n+  public StandaloneAcceleratedStoreClient(DirectoryClient dir, StoreClient persistentStore) {\n+    \n+    instantiated.set(true);\n+    \n+    this.persistent = persistentStore;\n+    this.cache = new StandaloneChunkedMemoryStore(WarpConfig.getProperties(), Warp.getKeyStore());\n+\n+    this.ephemeral = \"true\".equals(WarpConfig.getProperty(Configuration.IN_MEMORY_EPHEMERAL)); \n+    \n+    //\n+    // Preload the cache\n+    //\n+    \n+    long nanos = System.nanoTime();\n+    \n+    DirectoryRequest request = new DirectoryRequest();\n+    request.addToClassSelectors(\"~.*\");\n+    Map<String,String> labelselectors = new HashMap<String,String>();\n+    labelselectors.put(Constants.APPLICATION_LABEL, \"~.*\");\n+    labelselectors.put(Constants.PRODUCER_LABEL, \"~.*\");\n+    labelselectors.put(Constants.OWNER_LABEL, \"~.*\");\n+    request.addToLabelsSelectors(labelselectors);\n+    \n+    long end;\n+    long start;\n+    long n = -1L;\n+    \n+    if (this.ephemeral) {\n+      end = Long.MAX_VALUE;\n+      start = Long.MIN_VALUE;\n+      n = 1L;\n+    } else {\n+      end = InMemoryChunkSet.chunkEnd(TimeSource.getTime(), this.cache.getChunkSpan());\n+      start = end - this.cache.getChunkCount() * this.cache.getChunkSpan() + 1;\n+      n = -1L;\n+    }\n+    \n+    final long now = end;\n+    final long then = start;\n+    final long count = n;\n+    \n+    if (\"true\".equals(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_ACTIVITY))) {\n+      long activityWindow = Long.parseLong(WarpConfig.getProperty(Configuration.INGRESS_ACTIVITY_WINDOW, \"-1\"));\n+      if (activityWindow > 0) {\n+        request.setActiveAfter(then / Constants.TIME_UNITS_PER_MS - activityWindow - 1L);\n+      }\n+    }\n+    \n+    try {\n+      final AtomicLong datapoints = new AtomicLong();\n+      \n+      MetadataIterator iter = dir.iterator(request);\n+      \n+      int BATCH_SIZE = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_BATCHSIZE, \"1000\"));\n+      List<Metadata> batch = new ArrayList<Metadata>(BATCH_SIZE);\n+            \n+      int nthreads = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_POOLSIZE, \"8\"));\n+      \n+      ThreadPoolExecutor exec = new ThreadPoolExecutor(nthreads, nthreads, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(nthreads));\n+      final AtomicReference<Throwable> error = new AtomicReference<Throwable>();\n+      \n+      while(iter.hasNext()) {\n+        batch.add(iter.next());\n+        \n+        if (null != error.get()) {\n+          throw new RuntimeException(\"Error populating the accelerator\", error.get());\n+        }\n+        \n+        if (BATCH_SIZE == batch.size()) {\n+          \n+          final List<Metadata> fbatch = batch;\n+          \n+          Runnable runnable = new Runnable() {            \n+            @Override\n+            public void run() {\n+              try {\n+                GTSDecoderIterator decoders = persistent.fetch(null, fbatch, now, then, count, 0, 1.0D, false, 0, 0);\n+                \n+                while(decoders.hasNext()) {\n+                  GTSDecoder decoder = decoders.next();\n+                  decoder.next();\n+                  GTSEncoder encoder = decoder.getEncoder(true);\n+                  cache.store(encoder);\n+                  datapoints.addAndGet(decoder.getCount());\n+                }                              \n+              } catch (Exception e) {\n+                error.set(e);\n+                throw new RuntimeException(e);\n+              }\n+            }\n+          };\n+          \n+          boolean submitted = false;\n+          while(!submitted) {\n+            try {\n+              exec.execute(runnable);\n+              submitted = true;\n+            } catch (RejectedExecutionException re) {\n+              LockSupport.parkNanos(100000000L);\n+            }\n+          }\n+          \n+          batch = new ArrayList<Metadata>(BATCH_SIZE);\n+        }\n+      }\n+\n+      if (!batch.isEmpty()) {          \n+        GTSDecoderIterator decoders = this.persistent.fetch(null, batch, now, then, count, 0, 0.0, false, 0, 0);\n+        \n+        while(decoders.hasNext()) {\n+          GTSDecoder decoder = decoders.next();\n+          decoder.next();\n+          GTSEncoder encoder = decoder.getEncoder(true);\n+          this.cache.store(encoder);\n+          datapoints.addAndGet(decoder.getCount());\n+        }\n+      }\n+\n+      exec.shutdown();\n+      \n+      System.out.println(\"Preloaded accelerator with \" + datapoints + \" datapoints from \" + this.cache.getGTSCount() + \" Geo Time Series in \" + ((System.nanoTime() - nanos) / 1000000.0D) + \" ms.\");\n+      LOG.info(\"Preloaded accelerator with \" + datapoints + \" datapoints from \" + this.cache.getGTSCount() + \" Geo Time Series in \" + ((System.nanoTime() - nanos) / 1000000.0D) + \" ms.\");\n+    } catch (IOException ioe) {\n+      throw new RuntimeException(\"Error populating cache.\", ioe);\n+    }\n+  }\n+  \n+  @Override\n+  public void addPlasmaHandler(StandalonePlasmaHandlerInterface handler) {\n+    this.persistent.addPlasmaHandler(handler);\n+  }\n+  \n+  @Override\n+  public long delete(WriteToken token, Metadata metadata, long start, long end) throws IOException {\n+    cache.delete(token, metadata, start, end);\n+    persistent.delete(token, metadata, start, end);\n+    return 0;\n+  }\n+  \n+  @Override\n+  public GTSDecoderIterator fetch(ReadToken token, List<Metadata> metadatas, long now, long then, long count, long skip, double sample, boolean writeTimestamp, int preBoundary, int postBoundary) throws IOException {\n+    //\n+    // If the fetch has both a time range that is larger than the cache range, we will only use\n+    // the persistent backend to ensure a correct fetch. Same goes with boundaries which could extend outside the\n+    // cache.\n+    //\n+    // Note that this is a heuristic which could still lead to missing datapoints as data with timestamps within\n+    // the current cache time range could very well have been written to the persistent store and not preloaded\n+    // at cache startup if they were not in an active chunk of the cache.\n+    //\n+    \n+    long cacheend = InMemoryChunkSet.chunkEnd(now, this.cache.getChunkSpan());\n+    long cachestart = cacheend - this.cache.getChunkCount() * this.cache.getChunkSpan() + 1;\n+\n+    //\n+    // If fetching a single value from Long.MAX_VALUE with an ephemeral cache, always use the cache\n+    // unless ACCEL.NOCACHE was called.\n+    //\n+    if (this.ephemeral && 1 == count && Long.MAX_VALUE == now && !nocache.get()) {\n+      accelerated.set(Boolean.TRUE);\n+      return this.cache.fetch(token, metadatas, now, then, count, skip, sample, writeTimestamp, preBoundary, postBoundary);      \n+    }\n+    \n+    // Use the persistent store unless ACCEL.NOPERSIST was called \n+    if (((now > cacheend || then < cachestart) || preBoundary > 0 || postBoundary > 0 || nocache.get()) && !nopersist.get()) {\n+      accelerated.set(Boolean.FALSE);\n+      return this.persistent.fetch(token, metadatas, now, then, count, skip, sample, writeTimestamp, preBoundary, postBoundary);\n+    }\n+    \n+    // Last resort, use the cache\n+    accelerated.set(Boolean.TRUE);\n+    return this.cache.fetch(token, metadatas, now, then, count, skip, sample, writeTimestamp, preBoundary, postBoundary);", "originalCommit": "0cc5249f912650f67b1605f475e90937754ed0c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzA0OTExNg==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r387049116", "bodyText": "The semantics are those of the in-memory store, duplication is not handled at the store level, for the rare cases where you want to avoid duplicates datapoints, simply call DEDUP after FETCH if it was against the in-memory store.", "author": "hbs", "createdAt": "2020-03-03T14:19:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Njg4ODAwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjkwMTMwOA==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r386901308", "bodyText": "You have to wait for the tasks to complete after that. If you want to log the waiting, you can do:\nwhile (!exec.awaitTermination(30, TimeUnit.SECONDS)) {\n  LOG.info(\"Awaiting completion of accelerator preloading.\");\n}", "author": "ftence", "createdAt": "2020-03-03T09:42:57Z", "path": "warp10/src/main/java/io/warp10/standalone/StandaloneAcceleratedStoreClient.java", "diffHunk": "@@ -0,0 +1,298 @@\n+package io.warp10.standalone;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.LockSupport;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import io.warp10.WarpConfig;\n+import io.warp10.continuum.Configuration;\n+import io.warp10.continuum.TimeSource;\n+import io.warp10.continuum.gts.GTSDecoder;\n+import io.warp10.continuum.gts.GTSEncoder;\n+import io.warp10.continuum.store.Constants;\n+import io.warp10.continuum.store.DirectoryClient;\n+import io.warp10.continuum.store.GTSDecoderIterator;\n+import io.warp10.continuum.store.MetadataIterator;\n+import io.warp10.continuum.store.StoreClient;\n+import io.warp10.continuum.store.thrift.data.DirectoryRequest;\n+import io.warp10.continuum.store.thrift.data.Metadata;\n+import io.warp10.quasar.token.thrift.data.ReadToken;\n+import io.warp10.quasar.token.thrift.data.WriteToken;\n+\n+public class StandaloneAcceleratedStoreClient implements StoreClient {\n+  \n+  private static final Logger LOG = LoggerFactory.getLogger(StandaloneAcceleratedStoreClient.class);\n+  \n+  private final StoreClient persistent;\n+  private final StandaloneChunkedMemoryStore cache;\n+  private final boolean ephemeral;\n+\n+  public static final String ATTR_REPORT = \"accel.report\";\n+  public static final String ATTR_NOCACHE = \"accel.nocache\";\n+  public static final String ATTR_NOPERSIST = \"accel.nopersist\";\n+  \n+  public static final String NOCACHE = \"nocache\";\n+  public static final String NOPERSIST = \"nopersist\";\n+  \n+  public static final String ACCELERATOR_HEADER = \"X-Warp10-Accelerator\";\n+  \n+  /**\n+   * Was the last FETCH accelerated for the given Thread?\n+   */\n+  private static final ThreadLocal<Boolean> accelerated = new ThreadLocal<Boolean>() {\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    };\n+  };\n+  \n+  private static final ThreadLocal<Boolean> nocache = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final ThreadLocal<Boolean> nopersist = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final AtomicBoolean instantiated = new AtomicBoolean(false);\n+  \n+  public StandaloneAcceleratedStoreClient(DirectoryClient dir, StoreClient persistentStore) {\n+    \n+    instantiated.set(true);\n+    \n+    this.persistent = persistentStore;\n+    this.cache = new StandaloneChunkedMemoryStore(WarpConfig.getProperties(), Warp.getKeyStore());\n+\n+    this.ephemeral = \"true\".equals(WarpConfig.getProperty(Configuration.IN_MEMORY_EPHEMERAL)); \n+    \n+    //\n+    // Preload the cache\n+    //\n+    \n+    long nanos = System.nanoTime();\n+    \n+    DirectoryRequest request = new DirectoryRequest();\n+    request.addToClassSelectors(\"~.*\");\n+    Map<String,String> labelselectors = new HashMap<String,String>();\n+    labelselectors.put(Constants.APPLICATION_LABEL, \"~.*\");\n+    labelselectors.put(Constants.PRODUCER_LABEL, \"~.*\");\n+    labelselectors.put(Constants.OWNER_LABEL, \"~.*\");\n+    request.addToLabelsSelectors(labelselectors);\n+    \n+    long end;\n+    long start;\n+    long n = -1L;\n+    \n+    if (this.ephemeral) {\n+      end = Long.MAX_VALUE;\n+      start = Long.MIN_VALUE;\n+      n = 1L;\n+    } else {\n+      end = InMemoryChunkSet.chunkEnd(TimeSource.getTime(), this.cache.getChunkSpan());\n+      start = end - this.cache.getChunkCount() * this.cache.getChunkSpan() + 1;\n+      n = -1L;\n+    }\n+    \n+    final long now = end;\n+    final long then = start;\n+    final long count = n;\n+    \n+    if (\"true\".equals(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_ACTIVITY))) {\n+      long activityWindow = Long.parseLong(WarpConfig.getProperty(Configuration.INGRESS_ACTIVITY_WINDOW, \"-1\"));\n+      if (activityWindow > 0) {\n+        request.setActiveAfter(then / Constants.TIME_UNITS_PER_MS - activityWindow - 1L);\n+      }\n+    }\n+    \n+    try {\n+      final AtomicLong datapoints = new AtomicLong();\n+      \n+      MetadataIterator iter = dir.iterator(request);\n+      \n+      int BATCH_SIZE = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_BATCHSIZE, \"1000\"));\n+      List<Metadata> batch = new ArrayList<Metadata>(BATCH_SIZE);\n+            \n+      int nthreads = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_POOLSIZE, \"8\"));\n+      \n+      ThreadPoolExecutor exec = new ThreadPoolExecutor(nthreads, nthreads, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(nthreads));\n+      final AtomicReference<Throwable> error = new AtomicReference<Throwable>();\n+      \n+      while(iter.hasNext()) {\n+        batch.add(iter.next());\n+        \n+        if (null != error.get()) {\n+          throw new RuntimeException(\"Error populating the accelerator\", error.get());\n+        }\n+        \n+        if (BATCH_SIZE == batch.size()) {\n+          \n+          final List<Metadata> fbatch = batch;\n+          \n+          Runnable runnable = new Runnable() {            \n+            @Override\n+            public void run() {\n+              try {\n+                GTSDecoderIterator decoders = persistent.fetch(null, fbatch, now, then, count, 0, 1.0D, false, 0, 0);\n+                \n+                while(decoders.hasNext()) {\n+                  GTSDecoder decoder = decoders.next();\n+                  decoder.next();\n+                  GTSEncoder encoder = decoder.getEncoder(true);\n+                  cache.store(encoder);\n+                  datapoints.addAndGet(decoder.getCount());\n+                }                              \n+              } catch (Exception e) {\n+                error.set(e);\n+                throw new RuntimeException(e);\n+              }\n+            }\n+          };\n+          \n+          boolean submitted = false;\n+          while(!submitted) {\n+            try {\n+              exec.execute(runnable);\n+              submitted = true;\n+            } catch (RejectedExecutionException re) {\n+              LockSupport.parkNanos(100000000L);\n+            }\n+          }\n+          \n+          batch = new ArrayList<Metadata>(BATCH_SIZE);\n+        }\n+      }\n+\n+      if (!batch.isEmpty()) {          \n+        GTSDecoderIterator decoders = this.persistent.fetch(null, batch, now, then, count, 0, 0.0, false, 0, 0);\n+        \n+        while(decoders.hasNext()) {\n+          GTSDecoder decoder = decoders.next();\n+          decoder.next();\n+          GTSEncoder encoder = decoder.getEncoder(true);\n+          this.cache.store(encoder);\n+          datapoints.addAndGet(decoder.getCount());\n+        }\n+      }\n+\n+      exec.shutdown();", "originalCommit": "0cc5249f912650f67b1605f475e90937754ed0c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjkwMjI0NQ==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r386902245", "bodyText": "You could  add || !iter.hasNext to avoid adding the if (!batch.isEmpty()) after.", "author": "ftence", "createdAt": "2020-03-03T09:44:33Z", "path": "warp10/src/main/java/io/warp10/standalone/StandaloneAcceleratedStoreClient.java", "diffHunk": "@@ -0,0 +1,298 @@\n+package io.warp10.standalone;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.LockSupport;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import io.warp10.WarpConfig;\n+import io.warp10.continuum.Configuration;\n+import io.warp10.continuum.TimeSource;\n+import io.warp10.continuum.gts.GTSDecoder;\n+import io.warp10.continuum.gts.GTSEncoder;\n+import io.warp10.continuum.store.Constants;\n+import io.warp10.continuum.store.DirectoryClient;\n+import io.warp10.continuum.store.GTSDecoderIterator;\n+import io.warp10.continuum.store.MetadataIterator;\n+import io.warp10.continuum.store.StoreClient;\n+import io.warp10.continuum.store.thrift.data.DirectoryRequest;\n+import io.warp10.continuum.store.thrift.data.Metadata;\n+import io.warp10.quasar.token.thrift.data.ReadToken;\n+import io.warp10.quasar.token.thrift.data.WriteToken;\n+\n+public class StandaloneAcceleratedStoreClient implements StoreClient {\n+  \n+  private static final Logger LOG = LoggerFactory.getLogger(StandaloneAcceleratedStoreClient.class);\n+  \n+  private final StoreClient persistent;\n+  private final StandaloneChunkedMemoryStore cache;\n+  private final boolean ephemeral;\n+\n+  public static final String ATTR_REPORT = \"accel.report\";\n+  public static final String ATTR_NOCACHE = \"accel.nocache\";\n+  public static final String ATTR_NOPERSIST = \"accel.nopersist\";\n+  \n+  public static final String NOCACHE = \"nocache\";\n+  public static final String NOPERSIST = \"nopersist\";\n+  \n+  public static final String ACCELERATOR_HEADER = \"X-Warp10-Accelerator\";\n+  \n+  /**\n+   * Was the last FETCH accelerated for the given Thread?\n+   */\n+  private static final ThreadLocal<Boolean> accelerated = new ThreadLocal<Boolean>() {\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    };\n+  };\n+  \n+  private static final ThreadLocal<Boolean> nocache = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final ThreadLocal<Boolean> nopersist = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final AtomicBoolean instantiated = new AtomicBoolean(false);\n+  \n+  public StandaloneAcceleratedStoreClient(DirectoryClient dir, StoreClient persistentStore) {\n+    \n+    instantiated.set(true);\n+    \n+    this.persistent = persistentStore;\n+    this.cache = new StandaloneChunkedMemoryStore(WarpConfig.getProperties(), Warp.getKeyStore());\n+\n+    this.ephemeral = \"true\".equals(WarpConfig.getProperty(Configuration.IN_MEMORY_EPHEMERAL)); \n+    \n+    //\n+    // Preload the cache\n+    //\n+    \n+    long nanos = System.nanoTime();\n+    \n+    DirectoryRequest request = new DirectoryRequest();\n+    request.addToClassSelectors(\"~.*\");\n+    Map<String,String> labelselectors = new HashMap<String,String>();\n+    labelselectors.put(Constants.APPLICATION_LABEL, \"~.*\");\n+    labelselectors.put(Constants.PRODUCER_LABEL, \"~.*\");\n+    labelselectors.put(Constants.OWNER_LABEL, \"~.*\");\n+    request.addToLabelsSelectors(labelselectors);\n+    \n+    long end;\n+    long start;\n+    long n = -1L;\n+    \n+    if (this.ephemeral) {\n+      end = Long.MAX_VALUE;\n+      start = Long.MIN_VALUE;\n+      n = 1L;\n+    } else {\n+      end = InMemoryChunkSet.chunkEnd(TimeSource.getTime(), this.cache.getChunkSpan());\n+      start = end - this.cache.getChunkCount() * this.cache.getChunkSpan() + 1;\n+      n = -1L;\n+    }\n+    \n+    final long now = end;\n+    final long then = start;\n+    final long count = n;\n+    \n+    if (\"true\".equals(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_ACTIVITY))) {\n+      long activityWindow = Long.parseLong(WarpConfig.getProperty(Configuration.INGRESS_ACTIVITY_WINDOW, \"-1\"));\n+      if (activityWindow > 0) {\n+        request.setActiveAfter(then / Constants.TIME_UNITS_PER_MS - activityWindow - 1L);\n+      }\n+    }\n+    \n+    try {\n+      final AtomicLong datapoints = new AtomicLong();\n+      \n+      MetadataIterator iter = dir.iterator(request);\n+      \n+      int BATCH_SIZE = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_BATCHSIZE, \"1000\"));\n+      List<Metadata> batch = new ArrayList<Metadata>(BATCH_SIZE);\n+            \n+      int nthreads = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_POOLSIZE, \"8\"));\n+      \n+      ThreadPoolExecutor exec = new ThreadPoolExecutor(nthreads, nthreads, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(nthreads));\n+      final AtomicReference<Throwable> error = new AtomicReference<Throwable>();\n+      \n+      while(iter.hasNext()) {\n+        batch.add(iter.next());\n+        \n+        if (null != error.get()) {\n+          throw new RuntimeException(\"Error populating the accelerator\", error.get());\n+        }\n+        \n+        if (BATCH_SIZE == batch.size()) {", "originalCommit": "0cc5249f912650f67b1605f475e90937754ed0c9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4bd5567dd84718bdc3213e0edc44c4d7196b3f04", "url": "https://github.com/senx/warp10-platform/commit/4bd5567dd84718bdc3213e0edc44c4d7196b3f04", "message": "Addressed PR comments", "committedDate": "2020-03-03T14:27:40Z", "type": "commit"}, {"oid": "e1a53592505cb936ebe863da29f065afe207a412", "url": "https://github.com/senx/warp10-platform/commit/e1a53592505cb936ebe863da29f065afe207a412", "message": "Added nocache/nopersist parameters to /delete to allow for the deletion of the inmemory cache only or persisted data only", "committedDate": "2020-03-10T09:36:44Z", "type": "commit"}, {"oid": "056a5d9c86d038617e66efd1425374c436310d03", "url": "https://github.com/senx/warp10-platform/commit/056a5d9c86d038617e66efd1425374c436310d03", "message": "Added nocache/nopersist parameters to call to /delete performed by DELETE", "committedDate": "2020-03-10T09:47:45Z", "type": "commit"}, {"oid": "b50343faa81f91c875907314523ef92d54183cd0", "url": "https://github.com/senx/warp10-platform/commit/b50343faa81f91c875907314523ef92d54183cd0", "message": "Made time range mandatory when specifying nocache or nopersist", "committedDate": "2020-03-10T09:50:54Z", "type": "commit"}, {"oid": "77714884b3668f52e2f248939f0d90b071eebcc9", "url": "https://github.com/senx/warp10-platform/commit/77714884b3668f52e2f248939f0d90b071eebcc9", "message": "Added config to skip preloading", "committedDate": "2020-03-10T10:04:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgyNDc4Mw==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r392824783", "bodyText": "Is it a debug function or not ? If so, it should report more than the last fetch status. Maybe a map with :\n\naccelerator activation status\naccelerator preload status\naccelerator max time depth\nlast fetch was accelerated\nnumber of gts in the accelerator (help for debug remote customers too) (as directory is not affected, FIND won't make any difference when called after ACCEL.NOPERSIST)", "author": "pi-r-p", "createdAt": "2020-03-16T07:20:07Z", "path": "warp10/src/main/java/io/warp10/script/functions/ACCELREPORT.java", "diffHunk": "@@ -0,0 +1,41 @@\n+//\n+//   Copyright 2020  SenX S.A.S.\n+//\n+//   Licensed under the Apache License, Version 2.0 (the \"License\");\n+//   you may not use this file except in compliance with the License.\n+//   You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+//   Unless required by applicable law or agreed to in writing, software\n+//   distributed under the License is distributed on an \"AS IS\" BASIS,\n+//   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+//   See the License for the specific language governing permissions and\n+//   limitations under the License.\n+//\n+\n+package io.warp10.script.functions;\n+\n+import io.warp10.script.NamedWarpScriptFunction;\n+import io.warp10.script.WarpScriptStackFunction;\n+import io.warp10.standalone.StandaloneAcceleratedStoreClient;\n+import io.warp10.script.WarpScriptException;\n+import io.warp10.script.WarpScriptStack;\n+\n+public class ACCELREPORT extends NamedWarpScriptFunction implements WarpScriptStackFunction {\n+  ", "originalCommit": "77714884b3668f52e2f248939f0d90b071eebcc9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgyODcyNw==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r392828727", "bodyText": "So it must be a chunked memory store. Warp 10 must fail or warn if not properly configured.\nMaybe the most clean will be to have accelerator.chunk.count and accelerator.chunk.length configurations.", "author": "pi-r-p", "createdAt": "2020-03-16T07:31:48Z", "path": "warp10/src/main/java/io/warp10/standalone/StandaloneAcceleratedStoreClient.java", "diffHunk": "@@ -0,0 +1,304 @@\n+package io.warp10.standalone;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.LockSupport;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import io.warp10.WarpConfig;\n+import io.warp10.continuum.Configuration;\n+import io.warp10.continuum.TimeSource;\n+import io.warp10.continuum.gts.GTSDecoder;\n+import io.warp10.continuum.gts.GTSEncoder;\n+import io.warp10.continuum.store.Constants;\n+import io.warp10.continuum.store.DirectoryClient;\n+import io.warp10.continuum.store.GTSDecoderIterator;\n+import io.warp10.continuum.store.MetadataIterator;\n+import io.warp10.continuum.store.StoreClient;\n+import io.warp10.continuum.store.thrift.data.DirectoryRequest;\n+import io.warp10.continuum.store.thrift.data.Metadata;\n+import io.warp10.quasar.token.thrift.data.ReadToken;\n+import io.warp10.quasar.token.thrift.data.WriteToken;\n+\n+public class StandaloneAcceleratedStoreClient implements StoreClient {\n+  \n+  private static final Logger LOG = LoggerFactory.getLogger(StandaloneAcceleratedStoreClient.class);\n+  \n+  private final StoreClient persistent;\n+  private final StandaloneChunkedMemoryStore cache;", "originalCommit": "77714884b3668f52e2f248939f0d90b071eebcc9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1ca8bb665a3bc058d11c7ccd67e2c143d058c41c", "url": "https://github.com/senx/warp10-platform/commit/1ca8bb665a3bc058d11c7ccd67e2c143d058c41c", "message": "Addressed PR comments", "committedDate": "2020-03-19T08:22:46Z", "type": "commit"}, {"oid": "57677ee0feb921cf7a73c69695a2a897a191e091", "url": "https://github.com/senx/warp10-platform/commit/57677ee0feb921cf7a73c69695a2a897a191e091", "message": "Made setting chunk count and length mandatory for accelerator", "committedDate": "2020-03-19T08:47:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjMwODcxMA==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r396308710", "bodyText": "Why accelerator.chunk.count and accelerator.chunk.length are mandatory whereas  in.memory.chunk.count and in.memory.chunk.length have default values?", "author": "ftence", "createdAt": "2020-03-23T09:22:12Z", "path": "warp10/src/main/java/io/warp10/standalone/StandaloneAcceleratedStoreClient.java", "diffHunk": "@@ -72,11 +89,29 @@ protected Boolean initialValue() {\n     }\n   };\n \n-  private static final AtomicBoolean instantiated = new AtomicBoolean(false);\n-  \n   public StandaloneAcceleratedStoreClient(DirectoryClient dir, StoreClient persistentStore) {\n+        \n+    if (null != instance) {\n+      throw new RuntimeException(StandaloneAcceleratedStoreClient.class.getName() + \" can only be instantiated once.\");\n+    }\n+            \n+    //\n+    // Force accelerator parameters to be replicated on inmemory ones and clear other in memory params\n+    //\n+    \n+    if (null == WarpConfig.getProperty(Configuration.ACCELERATOR_CHUNK_COUNT)\n+        || null == WarpConfig.getProperty(Configuration.ACCELERATOR_CHUNK_LENGTH)) {\n+      throw new RuntimeException(\"Missing configuration key '\" + Configuration.ACCELERATOR_CHUNK_COUNT + \"' or '\" + Configuration.ACCELERATOR_CHUNK_LENGTH + \"'\");\n+    }", "originalCommit": "57677ee0feb921cf7a73c69695a2a897a191e091", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjU0OTcyMw==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r396549723", "bodyText": "The accelerator will load its in-memory cache with data from disk upon startup, having default values similar to those of in.memory.xxx would lead to massive fetch ops, so better not have default values and force the user to choose wisely.", "author": "hbs", "createdAt": "2020-03-23T15:43:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjMwODcxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ4MTM0OA==", "url": "https://github.com/senx/warp10-platform/pull/666#discussion_r396481348", "bodyText": "Shouldn't an error be thrown if nocache.get()?", "author": "ftence", "createdAt": "2020-03-23T14:14:12Z", "path": "warp10/src/main/java/io/warp10/standalone/StandaloneAcceleratedStoreClient.java", "diffHunk": "@@ -0,0 +1,377 @@\n+//\n+//   Copyright 2020  SenX S.A.S.\n+//\n+//   Licensed under the Apache License, Version 2.0 (the \"License\");\n+//   you may not use this file except in compliance with the License.\n+//   You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+//   Unless required by applicable law or agreed to in writing, software\n+//   distributed under the License is distributed on an \"AS IS\" BASIS,\n+//   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+//   See the License for the specific language governing permissions and\n+//   limitations under the License.\n+//\n+\n+package io.warp10.standalone;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.LockSupport;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import io.warp10.WarpConfig;\n+import io.warp10.continuum.Configuration;\n+import io.warp10.continuum.TimeSource;\n+import io.warp10.continuum.gts.GTSDecoder;\n+import io.warp10.continuum.gts.GTSEncoder;\n+import io.warp10.continuum.store.Constants;\n+import io.warp10.continuum.store.DirectoryClient;\n+import io.warp10.continuum.store.GTSDecoderIterator;\n+import io.warp10.continuum.store.MetadataIterator;\n+import io.warp10.continuum.store.StoreClient;\n+import io.warp10.continuum.store.thrift.data.DirectoryRequest;\n+import io.warp10.continuum.store.thrift.data.Metadata;\n+import io.warp10.quasar.token.thrift.data.ReadToken;\n+import io.warp10.quasar.token.thrift.data.WriteToken;\n+\n+public class StandaloneAcceleratedStoreClient implements StoreClient {\n+  \n+  private static final Logger LOG = LoggerFactory.getLogger(StandaloneAcceleratedStoreClient.class);\n+  \n+  private final StoreClient persistent;\n+  private final StandaloneChunkedMemoryStore cache;\n+  private final boolean ephemeral;\n+\n+  public static final String ATTR_REPORT = \"accel.report\";\n+  public static final String ATTR_NOCACHE = \"accel.nocache\";\n+  public static final String ATTR_NOPERSIST = \"accel.nopersist\";\n+  \n+  public static final String NOCACHE = \"nocache\";\n+  public static final String NOPERSIST = \"nopersist\";\n+  \n+  public static final String ACCELERATOR_HEADER = \"X-Warp10-Accelerator\";\n+  \n+  private static StandaloneAcceleratedStoreClient instance = null;\n+  \n+  /**\n+   * Was the last FETCH accelerated for the given Thread?\n+   */\n+  private static final ThreadLocal<Boolean> accelerated = new ThreadLocal<Boolean>() {\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    };\n+  };\n+  \n+  private static final ThreadLocal<Boolean> nocache = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  private static final ThreadLocal<Boolean> nopersist = new ThreadLocal<Boolean>() {\n+    @Override\n+    protected Boolean initialValue() {\n+      return Boolean.FALSE;\n+    }\n+  };\n+\n+  public StandaloneAcceleratedStoreClient(DirectoryClient dir, StoreClient persistentStore) {\n+        \n+    if (null != instance) {\n+      throw new RuntimeException(StandaloneAcceleratedStoreClient.class.getName() + \" can only be instantiated once.\");\n+    }\n+            \n+    //\n+    // Force accelerator parameters to be replicated on inmemory ones and clear other in memory params\n+    //\n+    \n+    if (null == WarpConfig.getProperty(Configuration.ACCELERATOR_CHUNK_COUNT)\n+        || null == WarpConfig.getProperty(Configuration.ACCELERATOR_CHUNK_LENGTH)) {\n+      throw new RuntimeException(\"Missing configuration key '\" + Configuration.ACCELERATOR_CHUNK_COUNT + \"' or '\" + Configuration.ACCELERATOR_CHUNK_LENGTH + \"'\");\n+    }\n+    \n+    WarpConfig.setProperty(Configuration.IN_MEMORY_CHUNK_COUNT, WarpConfig.getProperty(Configuration.ACCELERATOR_CHUNK_COUNT));\n+    WarpConfig.setProperty(Configuration.IN_MEMORY_CHUNK_LENGTH, WarpConfig.getProperty(Configuration.ACCELERATOR_CHUNK_LENGTH));\n+    WarpConfig.setProperty(Configuration.IN_MEMORY_EPHEMERAL, WarpConfig.getProperty(Configuration.ACCELERATOR_EPHEMERAL));\n+    WarpConfig.setProperty(Configuration.STANDALONE_MEMORY_GC_PERIOD, WarpConfig.getProperty(Configuration.ACCELERATOR_GC_PERIOD));\n+    WarpConfig.setProperty(Configuration.STANDALONE_MEMORY_GC_MAXALLOC, WarpConfig.getProperty(Configuration.ACCELERATOR_GC_MAXALLOC));\n+    \n+    WarpConfig.setProperty(Configuration.STANDALONE_MEMORY_STORE_LOAD, null);\n+    WarpConfig.setProperty(Configuration.STANDALONE_MEMORY_STORE_DUMP, null);\n+    \n+    this.persistent = persistentStore;\n+    this.cache = new StandaloneChunkedMemoryStore(WarpConfig.getProperties(), Warp.getKeyStore());\n+\n+    this.ephemeral = \"true\".equals(WarpConfig.getProperty(Configuration.IN_MEMORY_EPHEMERAL)); \n+    \n+    //\n+    // Preload the cache\n+    //\n+    \n+    long nanos = System.nanoTime();\n+    \n+    DirectoryRequest request = new DirectoryRequest();\n+    request.addToClassSelectors(\"~.*\");\n+    Map<String,String> labelselectors = new HashMap<String,String>();\n+    labelselectors.put(Constants.APPLICATION_LABEL, \"~.*\");\n+    labelselectors.put(Constants.PRODUCER_LABEL, \"~.*\");\n+    labelselectors.put(Constants.OWNER_LABEL, \"~.*\");\n+    request.addToLabelsSelectors(labelselectors);\n+    \n+    long end;\n+    long start;\n+    long n = -1L;\n+    \n+    if (this.ephemeral) {\n+      end = Long.MAX_VALUE;\n+      start = Long.MIN_VALUE;\n+      n = 1L;\n+    } else {\n+      end = InMemoryChunkSet.chunkEnd(TimeSource.getTime(), this.cache.getChunkSpan());\n+      start = end - this.cache.getChunkCount() * this.cache.getChunkSpan() + 1;\n+      n = -1L;\n+    }\n+    \n+    final long now = end;\n+    final long then = start;\n+    final long count = n;\n+    \n+    boolean preload = \"true\".equals(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD));\n+    \n+    if (\"true\".equals(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_ACTIVITY))) {\n+      long activityWindow = Long.parseLong(WarpConfig.getProperty(Configuration.INGRESS_ACTIVITY_WINDOW, \"-1\"));\n+      if (activityWindow > 0) {\n+        request.setActiveAfter(then / Constants.TIME_UNITS_PER_MS - activityWindow - 1L);\n+      }\n+    }\n+    \n+    if (preload) {\n+      try {\n+        final AtomicLong datapoints = new AtomicLong();\n+        \n+        MetadataIterator iter = dir.iterator(request);\n+        \n+        int BATCH_SIZE = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_BATCHSIZE, \"1000\"));\n+        List<Metadata> batch = new ArrayList<Metadata>(BATCH_SIZE);\n+              \n+        int nthreads = Integer.parseInt(WarpConfig.getProperty(Configuration.ACCELERATOR_PRELOAD_POOLSIZE, \"8\"));\n+        \n+        ThreadPoolExecutor exec = new ThreadPoolExecutor(nthreads, nthreads, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(nthreads));\n+        final AtomicReference<Throwable> error = new AtomicReference<Throwable>();\n+        \n+        while(iter.hasNext()) {\n+          batch.add(iter.next());\n+          \n+          if (null != error.get()) {\n+            throw new RuntimeException(\"Error populating the accelerator\", error.get());\n+          }\n+          \n+          if (BATCH_SIZE == batch.size() || !iter.hasNext()) {\n+            \n+            final List<Metadata> fbatch = batch;\n+            \n+            Runnable runnable = new Runnable() {            \n+              @Override\n+              public void run() {\n+                try {\n+                  GTSDecoderIterator decoders = persistent.fetch(null, fbatch, now, then, count, 0, 1.0D, false, 0, 0);\n+                  \n+                  while(decoders.hasNext()) {\n+                    GTSDecoder decoder = decoders.next();\n+                    decoder.next();\n+                    GTSEncoder encoder = decoder.getEncoder(true);\n+                    cache.store(encoder);\n+                    datapoints.addAndGet(decoder.getCount());\n+                  }                              \n+                } catch (Exception e) {\n+                  error.set(e);\n+                  throw new RuntimeException(e);\n+                }\n+              }\n+            };\n+            \n+            boolean submitted = false;\n+            while(!submitted) {\n+              try {\n+                exec.execute(runnable);\n+                submitted = true;\n+              } catch (RejectedExecutionException re) {\n+                LockSupport.parkNanos(100000000L);\n+              }\n+            }\n+            \n+            batch = new ArrayList<Metadata>(BATCH_SIZE);\n+          }\n+        }\n+\n+        exec.shutdown();\n+        \n+        while(true) {\n+          try {\n+            if (exec.awaitTermination(30, TimeUnit.SECONDS)) {\n+              break;\n+            }\n+          } catch (InterruptedException ie) {          \n+          }\n+        }\n+\n+        LOG.info(\"Preloaded accelerator with \" + datapoints + \" datapoints from \" + this.cache.getGTSCount() + \" Geo Time Series in \" + ((System.nanoTime() - nanos) / 1000000.0D) + \" ms.\");\n+      } catch (IOException ioe) {\n+        throw new RuntimeException(\"Error populating cache.\", ioe);\n+      }      \n+    } else {\n+      LOG.info(\"Skipping accelerator preloading.\");\n+    }\n+    \n+    instance = this;\n+  }\n+  \n+  @Override\n+  public void addPlasmaHandler(StandalonePlasmaHandlerInterface handler) {\n+    this.persistent.addPlasmaHandler(handler);\n+  }\n+  \n+  @Override\n+  public long delete(WriteToken token, Metadata metadata, long start, long end) throws IOException {\n+    if (!nocache.get()) {\n+      cache.delete(token, metadata, start, end);\n+    }\n+    if (!nopersist.get()) {\n+      persistent.delete(token, metadata, start, end);\n+    }\n+    return 0;\n+  }\n+  \n+  @Override\n+  public GTSDecoderIterator fetch(ReadToken token, List<Metadata> metadatas, long now, long then, long count, long skip, double sample, boolean writeTimestamp, int preBoundary, int postBoundary) throws IOException {\n+    //\n+    // If the fetch has both a time range that is larger than the cache range, we will only use\n+    // the persistent backend to ensure a correct fetch. Same goes with boundaries which could extend outside the\n+    // cache.\n+    //\n+    // Note that this is a heuristic which could still lead to missing datapoints as data with timestamps within\n+    // the current cache time range could very well have been written to the persistent store and not preloaded\n+    // at cache startup if they were not in an active chunk of the cache.\n+    //\n+    \n+    long cacheend = InMemoryChunkSet.chunkEnd(TimeSource.getTime(), this.cache.getChunkSpan());\n+    long cachestart = cacheend - this.cache.getChunkCount() * this.cache.getChunkSpan() + 1;\n+\n+    //\n+    // If fetching a single value from Long.MAX_VALUE with an ephemeral cache, always use the cache\n+    // unless ACCEL.NOCACHE was called.\n+    //\n+    if (this.ephemeral && 1 == count && Long.MAX_VALUE == now && !nocache.get()) {\n+      accelerated.set(Boolean.TRUE);\n+      return this.cache.fetch(token, metadatas, now, then, count, skip, sample, writeTimestamp, preBoundary, postBoundary);      \n+    }\n+    \n+    // Use the persistent store unless ACCEL.NOPERSIST was called \n+    if (((now > cacheend || then < cachestart) || preBoundary > 0 || postBoundary > 0 || nocache.get()) && !nopersist.get()) {\n+      accelerated.set(Boolean.FALSE);\n+      return this.persistent.fetch(token, metadatas, now, then, count, skip, sample, writeTimestamp, preBoundary, postBoundary);\n+    }\n+    \n+    // Last resort, use the cache\n+    accelerated.set(Boolean.TRUE);\n+    return this.cache.fetch(token, metadatas, now, then, count, skip, sample, writeTimestamp, preBoundary, postBoundary);", "originalCommit": "57677ee0feb921cf7a73c69695a2a897a191e091", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3bb0a8b9f326e0487e17898269acb60bc69b03a4", "url": "https://github.com/senx/warp10-platform/commit/3bb0a8b9f326e0487e17898269acb60bc69b03a4", "message": "Addressed PR comments", "committedDate": "2020-03-23T15:53:37Z", "type": "commit"}, {"oid": "608fd2139451a06cbdea0f1b42a4c5235aab111e", "url": "https://github.com/senx/warp10-platform/commit/608fd2139451a06cbdea0f1b42a4c5235aab111e", "message": "Fixed comparison", "committedDate": "2020-03-23T16:38:48Z", "type": "commit"}]}