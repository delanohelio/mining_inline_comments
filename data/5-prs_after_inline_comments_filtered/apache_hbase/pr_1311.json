{"pr_number": 1311, "pr_title": "HBASE-23984 [Flakey Tests] TestMasterAbortAndRSGotKilled fails in tea\u2026", "pr_createdAt": "2020-03-19T18:25:07Z", "pr_url": "https://github.com/apache/hbase/pull/1311", "timeline": [{"oid": "4e7c530a77bc09f3157d855ac7e36de89c581c44", "url": "https://github.com/apache/hbase/commit/4e7c530a77bc09f3157d855ac7e36de89c581c44", "message": "HBASE-23984 [Flakey Tests] TestMasterAbortAndRSGotKilled fails in teardown\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\n Change parameter name and add javadoc to make it more clear what the\n param actually is.\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/AssignRegionHandler.java\n Move postOpenDeployTasks so if it fails to talk to the Master -- which\n can happen on cluster shutdown -- then we will do cleanup of state;\n without this the RS can get stuck and won't go down.\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java\n Add handleException so CRH looks more like UnassignRegionHandler and\n AssignRegionHandler around exception handling. Add a bit of doc on\n why CRH.\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/UnassignRegionHandler.java\n Right shift most of the body of process so can add in a finally\n that cleans up rs.getRegionsInTransitionInRS is on exception\n (otherwise outstanding entries can stop a RS going down on cluster\n shutdown)", "committedDate": "2020-03-19T18:25:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMwNzM1Ng==", "url": "https://github.com/apache/hbase/pull/1311#discussion_r395307356", "bodyText": "Yikes! Yeah, this seems better here. Good.", "author": "ndimiduk", "createdAt": "2020-03-19T20:41:17Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/AssignRegionHandler.java", "diffHunk": "@@ -132,11 +131,11 @@ public void process() throws IOException {\n       // opening can not be interrupted by a close request any more.\n       region = HRegion.openHRegion(regionInfo, htd, rs.getWAL(regionInfo), rs.getConfiguration(),\n         rs, null);\n+      rs.postOpenDeployTasks(new PostOpenDeployContext(region, openProcId, masterSystemTime));", "originalCommit": "4e7c530a77bc09f3157d855ac7e36de89c581c44", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQwNjQ1OA==", "url": "https://github.com/apache/hbase/pull/1311#discussion_r395406458", "bodyText": "No...\nIIRC, the design here is that, postOpenDeployTasks is the PONR, if we arrive here, then we can not revert back, the only way to address the exception is to abort the region server.\nThe fact is that, if we haven't told master anything, it is fine for us to close the region and tell master the failure, but once we have already called master with the succeeded message, even if the rpc call fails, we do not know whether the other side(the master) has received and processed the request already, so the only way is to retry for ever, and if this can not be done, the only way is to abort ourselves...", "author": "Apache9", "createdAt": "2020-03-20T01:45:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMwNzM1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQzNDQ3OA==", "url": "https://github.com/apache/hbase/pull/1311#discussion_r395434478", "bodyText": "bq. IIRC, the design here is that, postOpenDeployTasks is the PONR, if we arrive here, then we can not revert back, the only way to address the exception is to abort the region server.\nOk. That helps. Let me add above as comment and ensure the above happens and that I get my fix in.", "author": "saintstack", "createdAt": "2020-03-20T04:26:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMwNzM1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMwOTEyOQ==", "url": "https://github.com/apache/hbase/pull/1311#discussion_r395309129", "bodyText": "helpful observation.", "author": "ndimiduk", "createdAt": "2020-03-19T20:44:54Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java", "diffHunk": "@@ -38,9 +37,13 @@\n /**\n  * Handles closing of a region on a region server.\n  * <p/>\n- * Now for regular close region request, we will use {@link UnassignRegionHandler} instead. But when\n- * shutting down the region server, will also close regions and the related methods still use this\n- * class so we keep it here.\n+ * In normal operation, we use {@link UnassignRegionHandler} closing Regions but when shutting down\n+ * the region server and closing out Regions, we use this handler instead; it does not expect to\n+ * be able to communicate the close back to the Master.\n+ * <p>Expects that the close has been registered in the hosting RegionServer before\n+ * submitting this Handler; i.e. <code>rss.getRegionsInTransitionInRS().putIfAbsent(\n+ * this.regionInfo.getEncodedNameAsBytes(), Boolean.FALSE);</code> has been called first.\n+ * In here when done, we do the deregister.</p>", "originalCommit": "4e7c530a77bc09f3157d855ac7e36de89c581c44", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMxMjI4NA==", "url": "https://github.com/apache/hbase/pull/1311#discussion_r395312284", "bodyText": "Maybe it's just because I'm new to the *Handler code, but it's not clear to me why one would handle exceptions locally vs. handle them from this handleException method. I guess it's all hooks for operating within the confines of a Runnable off on a thread pool somewhere.", "author": "ndimiduk", "createdAt": "2020-03-19T20:51:21Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java", "diffHunk": "@@ -133,4 +123,10 @@ public void process() {\n         remove(this.regionInfo.getEncodedNameAsBytes(), Boolean.FALSE);\n     }\n   }\n+\n+  @Override protected void handleException(Throwable t) {", "originalCommit": "4e7c530a77bc09f3157d855ac7e36de89c581c44", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMyNTgyOA==", "url": "https://github.com/apache/hbase/pull/1311#discussion_r395325828", "bodyText": "Yes, inconsistently used. Here trying to keep w/ the herd.", "author": "saintstack", "createdAt": "2020-03-19T21:19:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMxMjI4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMxNjcxOA==", "url": "https://github.com/apache/hbase/pull/1311#discussion_r395316718", "bodyText": "After reading the above comment and seeing you discarded the throwing of this exception, I initially choked. But reading through the actual use of these Handlers in the ExecutorService instance hanging off of HRegionServer, and HMaster I can only conclude that the above throw was only wishful thinking. There's even a comment (emphasis mine):\n\nStart up all services. If any of these threads gets an unhandled exception\nthen they just die with a logged message.  This should be fine because\nin general, we do not expect the master to get such unhandled exceptions\nas OOMEs; it should be lightly loaded. See what HRegionServer does if\nneed to install an unexpected exception handler.\n\nThe author of the above comment speaks wistfully of what i can only assume is HRegionServer#uncaughtExceptionHandler. However, it doesn't appear that this is threaded down into the executor service, which means this line's throw statement is simply logged and ignored.\nSo yes, I think removing the throw is the right choice. It removes the false sense of handling this error condition correctly. It's really the abort that protects the content of the memstore.\nAlso, why is there not a named exception thrown by the memstore when it cannot flush? Seems like a useful point in that data structure's API.", "author": "ndimiduk", "createdAt": "2020-03-19T21:00:12Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java", "diffHunk": "@@ -106,20 +105,11 @@ public void process() {\n       }\n \n       // Close the region\n-      try {\n-        if (region.close(abort) == null) {\n-          // This region got closed.  Most likely due to a split.\n-          // The split message will clean up the master state.\n-          LOG.warn(\"Can't close region {}, was already closed during close()\", name);\n-          return;\n-        }\n-      } catch (IOException ioe) {\n-        // An IOException here indicates that we couldn't successfully flush the\n-        // memstore before closing. So, we need to abort the server and allow\n-        // the master to split our logs in order to recover the data.\n-        server.abort(\"Unrecoverable exception while closing region \" +\n-          regionInfo.getRegionNameAsString() + \", still finishing close\", ioe);\n-        throw new RuntimeException(ioe);", "originalCommit": "4e7c530a77bc09f3157d855ac7e36de89c581c44", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMxODA2MQ==", "url": "https://github.com/apache/hbase/pull/1311#discussion_r395318061", "bodyText": "good.", "author": "ndimiduk", "createdAt": "2020-03-19T21:03:03Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/UnassignRegionHandler.java", "diffHunk": "@@ -94,40 +93,42 @@ public void process() throws IOException {\n       }\n       return;\n     }\n-    HRegion region = rs.getRegion(encodedName);\n-    if (region == null) {\n-      LOG.debug(\n-        \"Received CLOSE for a region {} which is not online, and we're not opening/closing.\",\n-        encodedName);\n-      rs.getRegionsInTransitionInRS().remove(encodedNameBytes, Boolean.FALSE);\n-      return;\n-    }\n-    String regionName = region.getRegionInfo().getEncodedName();\n-    LOG.info(\"Close {}\", regionName);\n-    if (region.getCoprocessorHost() != null) {\n-      // XXX: The behavior is a bit broken. At master side there is no FAILED_CLOSE state, so if\n-      // there are exception thrown from the CP, we can not report the error to master, and if here\n-      // we just return without calling reportRegionStateTransition, the TRSP at master side will\n-      // hang there for ever. So here if the CP throws an exception out, the only way is to abort\n-      // the RS...\n-      region.getCoprocessorHost().preClose(abort);\n-    }\n-    if (region.close(abort) == null) {\n-      // XXX: Is this still possible? The old comment says about split, but now split is done at\n-      // master side, so...\n-      LOG.warn(\"Can't close region {}, was already closed during close()\", regionName);\n+    try {\n+      HRegion region = rs.getRegion(encodedName);\n+      if (region == null) {\n+        LOG.debug(\n+          \"Received CLOSE for a region {} which is not online, and we're not opening/closing.\",\n+          encodedName);\n+        return;\n+      }\n+      String regionName = region.getRegionInfo().getEncodedName();\n+      LOG.info(\"Close {}\", regionName);\n+      if (region.getCoprocessorHost() != null) {\n+        // XXX: The behavior is a bit broken. At master side there is no FAILED_CLOSE state, so if\n+        // there are exception thrown from the CP, we can not report the error to master, and if\n+        // here we just return without calling reportRegionStateTransition, the TRSP at master side\n+        // will hang there for ever. So here if the CP throws an exception out, the only way is to\n+        // abort the RS...\n+        region.getCoprocessorHost().preClose(abort);\n+      }\n+      if (region.close(abort) == null) {\n+        // XXX: Is this still possible? The old comment says about split, but now split is done at\n+        // master side, so...\n+        LOG.warn(\"Can't close region {}, was already closed during close()\", regionName);\n+        return;\n+      }\n+      rs.removeRegion(region, destination);\n+      if (!rs.reportRegionStateTransition(\n+        new RegionStateTransitionContext(TransitionCode.CLOSED, HConstants.NO_SEQNUM, closeProcId,\n+          -1, region.getRegionInfo()))) {\n+        throw new IOException(\"Failed to report close to master: \" + regionName);\n+      }\n+      // Cache the close region procedure id after report region transition succeed.\n+      rs.finishRegionProcedure(closeProcId);\n+      LOG.info(\"Closed {}\", regionName);\n+    } finally {\n       rs.getRegionsInTransitionInRS().remove(encodedNameBytes, Boolean.FALSE);", "originalCommit": "4e7c530a77bc09f3157d855ac7e36de89c581c44", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQwNzQ4NA==", "url": "https://github.com/apache/hbase/pull/1311#discussion_r395407484", "bodyText": "So this is the actual fix here?\nIf you really want to do this to let the test pass, I suggest you add the removal in the handleException method, and add a FIXME or TODO comment to say that this is just for making test pass, should be addressed later.", "author": "Apache9", "createdAt": "2020-03-20T01:50:33Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/UnassignRegionHandler.java", "diffHunk": "@@ -94,40 +93,42 @@ public void process() throws IOException {\n       }\n       return;\n     }\n-    HRegion region = rs.getRegion(encodedName);\n-    if (region == null) {\n-      LOG.debug(\n-        \"Received CLOSE for a region {} which is not online, and we're not opening/closing.\",\n-        encodedName);\n-      rs.getRegionsInTransitionInRS().remove(encodedNameBytes, Boolean.FALSE);\n-      return;\n-    }\n-    String regionName = region.getRegionInfo().getEncodedName();\n-    LOG.info(\"Close {}\", regionName);\n-    if (region.getCoprocessorHost() != null) {\n-      // XXX: The behavior is a bit broken. At master side there is no FAILED_CLOSE state, so if\n-      // there are exception thrown from the CP, we can not report the error to master, and if here\n-      // we just return without calling reportRegionStateTransition, the TRSP at master side will\n-      // hang there for ever. So here if the CP throws an exception out, the only way is to abort\n-      // the RS...\n-      region.getCoprocessorHost().preClose(abort);\n-    }\n-    if (region.close(abort) == null) {\n-      // XXX: Is this still possible? The old comment says about split, but now split is done at\n-      // master side, so...\n-      LOG.warn(\"Can't close region {}, was already closed during close()\", regionName);\n+    try {\n+      HRegion region = rs.getRegion(encodedName);\n+      if (region == null) {\n+        LOG.debug(\n+          \"Received CLOSE for a region {} which is not online, and we're not opening/closing.\",\n+          encodedName);\n+        return;\n+      }\n+      String regionName = region.getRegionInfo().getEncodedName();\n+      LOG.info(\"Close {}\", regionName);\n+      if (region.getCoprocessorHost() != null) {\n+        // XXX: The behavior is a bit broken. At master side there is no FAILED_CLOSE state, so if\n+        // there are exception thrown from the CP, we can not report the error to master, and if\n+        // here we just return without calling reportRegionStateTransition, the TRSP at master side\n+        // will hang there for ever. So here if the CP throws an exception out, the only way is to\n+        // abort the RS...\n+        region.getCoprocessorHost().preClose(abort);\n+      }\n+      if (region.close(abort) == null) {\n+        // XXX: Is this still possible? The old comment says about split, but now split is done at\n+        // master side, so...\n+        LOG.warn(\"Can't close region {}, was already closed during close()\", regionName);\n+        return;\n+      }\n+      rs.removeRegion(region, destination);\n+      if (!rs.reportRegionStateTransition(\n+        new RegionStateTransitionContext(TransitionCode.CLOSED, HConstants.NO_SEQNUM, closeProcId,\n+          -1, region.getRegionInfo()))) {\n+        throw new IOException(\"Failed to report close to master: \" + regionName);\n+      }\n+      // Cache the close region procedure id after report region transition succeed.\n+      rs.finishRegionProcedure(closeProcId);\n+      LOG.info(\"Closed {}\", regionName);\n+    } finally {", "originalCommit": "4e7c530a77bc09f3157d855ac7e36de89c581c44", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1df4e23ae18637055695bf9bdb7598ba811d399a", "url": "https://github.com/apache/hbase/commit/1df4e23ae18637055695bf9bdb7598ba811d399a", "message": "HBASE-23984 [Flakey Tests] TestMasterAbortAndRSGotKilled fails in teardown\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\n Change parameter name and add javadoc to make it more clear what the\n param actually is.\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/AssignRegionHandler.java\n Move postOpenDeployTasks so if it fails to talk to the Master -- which\n can happen on cluster shutdown -- then we will do cleanup of state;\n without this the RS can get stuck and won't go down.\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java\n Add handleException so CRH looks more like UnassignRegionHandler and\n AssignRegionHandler around exception handling. Add a bit of doc on\n why CRH.\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/UnassignRegionHandler.java\n Right shift most of the body of process so can add in a finally\n that cleans up rs.getRegionsInTransitionInRS is on exception\n (otherwise outstanding entries can stop a RS going down on cluster\n shutdown)", "committedDate": "2020-03-20T05:05:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQ1Mjk1NA==", "url": "https://github.com/apache/hbase/pull/1311#discussion_r395452954", "bodyText": "LOG.debug(\"Closed {}\", region.getRegionInfo().getRegionNameAsString());", "author": "Apache9", "createdAt": "2020-03-20T06:14:09Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java", "diffHunk": "@@ -92,45 +91,41 @@ public RegionInfo getRegionInfo() {\n   }\n \n   @Override\n-  public void process() {\n-    try {\n-      String name = regionInfo.getEncodedName();\n-      LOG.trace(\"Processing close of {}\", name);\n-      String encodedRegionName = regionInfo.getEncodedName();\n-      // Check that this region is being served here\n-      HRegion region = (HRegion)rsServices.getRegion(encodedRegionName);\n-      if (region == null) {\n-        LOG.warn(\"Received CLOSE for region {} but currently not serving - ignoring\", name);\n-        // TODO: do better than a simple warning\n-        return;\n-      }\n+  public void process() throws IOException {\n+    String name = regionInfo.getEncodedName();\n+    LOG.trace(\"Processing close of {}\", name);\n+    String encodedRegionName = regionInfo.getEncodedName();\n+    // Check that this region is being served here\n+    HRegion region = (HRegion)rsServices.getRegion(encodedRegionName);\n+    if (region == null) {\n+      LOG.warn(\"Received CLOSE for region {} but currently not serving - ignoring\", name);\n+      // TODO: do better than a simple warning\n+      return;\n+    }\n+\n+    // Close the region\n+    if (region.close(abort) == null) {\n+      // This region got closed.  Most likely due to a split.\n+      // The split message will clean up the master state.\n+      LOG.warn(\"Can't close region {}, was already closed during close()\", name);\n+      return;\n+    }\n \n-      // Close the region\n-      try {\n-        if (region.close(abort) == null) {\n-          // This region got closed.  Most likely due to a split.\n-          // The split message will clean up the master state.\n-          LOG.warn(\"Can't close region {}, was already closed during close()\", name);\n-          return;\n-        }\n-      } catch (IOException ioe) {\n-        // An IOException here indicates that we couldn't successfully flush the\n-        // memstore before closing. So, we need to abort the server and allow\n-        // the master to split our logs in order to recover the data.\n-        server.abort(\"Unrecoverable exception while closing region \" +\n-          regionInfo.getRegionNameAsString() + \", still finishing close\", ioe);\n-        throw new RuntimeException(ioe);\n-      }\n+    this.rsServices.removeRegion(region, destination);\n+    rsServices.reportRegionStateTransition(new RegionStateTransitionContext(TransitionCode.CLOSED,\n+      HConstants.NO_SEQNUM, Procedure.NO_PROC_ID, -1, regionInfo));\n \n-      this.rsServices.removeRegion(region, destination);\n-      rsServices.reportRegionStateTransition(new RegionStateTransitionContext(TransitionCode.CLOSED,\n-        HConstants.NO_SEQNUM, Procedure.NO_PROC_ID, -1, regionInfo));\n+    // Done!  Region is closed on this RS\n+    this.rsServices.getRegionsInTransitionInRS().\n+      remove(this.regionInfo.getEncodedNameAsBytes(), Boolean.FALSE);\n+    LOG.debug(\"Closed \" + region.getRegionInfo().getRegionNameAsString());", "originalCommit": "1df4e23ae18637055695bf9bdb7598ba811d399a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "096a8bc47ce91500fa6260da44051a7cda430ac7", "url": "https://github.com/apache/hbase/commit/096a8bc47ce91500fa6260da44051a7cda430ac7", "message": "HBASE-23984 [Flakey Tests] TestMasterAbortAndRSGotKilled fails in teardown\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\n Change parameter name and add javadoc to make it more clear what the\n param actually is.\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/AssignRegionHandler.java\n Move postOpenDeployTasks so if it fails to talk to the Master -- which\n can happen on cluster shutdown -- then we will do cleanup of state;\n without this the RS can get stuck and won't go down.\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java\n Add handleException so CRH looks more like UnassignRegionHandler and\n AssignRegionHandler around exception handling. Add a bit of doc on\n why CRH.\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/UnassignRegionHandler.java\n Right shift most of the body of process so can add in a finally\n that cleans up rs.getRegionsInTransitionInRS is on exception\n (otherwise outstanding entries can stop a RS going down on cluster\n shutdown)", "committedDate": "2020-03-20T19:05:53Z", "type": "commit"}, {"oid": "096a8bc47ce91500fa6260da44051a7cda430ac7", "url": "https://github.com/apache/hbase/commit/096a8bc47ce91500fa6260da44051a7cda430ac7", "message": "HBASE-23984 [Flakey Tests] TestMasterAbortAndRSGotKilled fails in teardown\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\n Change parameter name and add javadoc to make it more clear what the\n param actually is.\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/AssignRegionHandler.java\n Move postOpenDeployTasks so if it fails to talk to the Master -- which\n can happen on cluster shutdown -- then we will do cleanup of state;\n without this the RS can get stuck and won't go down.\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java\n Add handleException so CRH looks more like UnassignRegionHandler and\n AssignRegionHandler around exception handling. Add a bit of doc on\n why CRH.\n\nhbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/UnassignRegionHandler.java\n Right shift most of the body of process so can add in a finally\n that cleans up rs.getRegionsInTransitionInRS is on exception\n (otherwise outstanding entries can stop a RS going down on cluster\n shutdown)", "committedDate": "2020-03-20T19:05:53Z", "type": "forcePushed"}]}